{
    "id": "lziv5yjgedtmp3dz23rszmo3mo3rh43y",
    "title": "Learning CRFs with Hierarchical Features: An Application to Go",
    "info": {
        "author": [
            "Scott Sanner, University of Toronto"
        ],
        "published": "June 24, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Structured Data",
            "Top->Mathematics->Game Theory"
        ]
    },
    "url": "http://videolectures.net/icml07_sanner_lch/",
    "segmentation": [
        [
            "OK, I did this work with Tor, grape or offer rich and humming at Microsoft Research.",
            "David Stern provide a lot of the software and Michael Christopher gave me a lot of the Latex code to draw the nice diagrams you'll see in the talk.",
            "We can't hear me just let me know.",
            "An empirical talk and I just want to mainly talk about the tradeoffs involved in learning CRF's when inferencing in that CRF is intractable."
        ],
        [
            "OK, I'm applying this to go, so let me just quickly introduce the game just so you have an idea of what I'm trying to do.",
            "About 4000 years ago in China, 60 million players, two players, black and white.",
            "The boards 1919 grid looks like this if you're familiar with oh OK, you play stones of the vertices of the board like this one.",
            "Player places a stone and every turn there's a capture rule, so if you can surround your opponent on the North East, Southwest positions, then you can effectively capture that.",
            "So here Black is moved to position one and is captured the white stone and the name of the game is to capture territory by surrounding it.",
            "So at the end of the game.",
            "You get this board in 99 go, then black would get all the territories surrounded and white would get all the territory that it is surrounded OK. Of course you want to maximize."
        ],
        [
            "Pretore OK.",
            "In this talk I want to talk about territory prediction go so given a board configuration on the left, I want to predict the expected territory that you're going to get for each vertices on the board.",
            "Of course, the rectangles indicate the color of the territory that you expect in this in the strength of it.",
            "So let's talk about ways to do this.",
            "Monte Carlo is a very popular way to do it, actually works very well in practice.",
            "Some of the best go players use Monte Carlo to predict territory.",
            "The problem is that it takes 100 moves per turn.",
            "Or there's an average of 180 miss returning, though typically games or are more than 100 moves, so averaging out to the end of the game, or, well, rolling out to the end of the game is extremely costly.",
            "So what I wanted to see if we can predict as well as Monte Carlo or better, but using a factor graphical model to make this territory predictions."
        ],
        [
            "OK, so we have lots of tools that we can use for doing this.",
            "I just want to quickly cover this.",
            "In the beginning we have higher cool pattern features commonly used in go.",
            "I'll discuss those briefly.",
            "the Dallas model you could use as an independent pattern based classifier, but it's worth comparing to this to see when this works and when it doesn't.",
            "OK will cover more expressive CRF models and they have coupling factors.",
            "Now realize that they go board is 19 by 19, so we have 3 or 60 one of these nodes.",
            "So inference is intractable, and so if you have to learn CRS when the inference is intractable, it's not quite clear what's the best method.",
            "I'm going to export the trade offs there.",
            "I've lost experimental data and conclusions."
        ],
        [
            "Girl.",
            "OK, so the higher patterns we use their sent around a single position, their hierarchical and that is we start with a pattern size of 1 which is just the center position pattern.",
            "Size 2 expands out a bit three so on we only use up to 8 nested pattern templates.",
            "We mind the most frequent patterns from 5000 expert games inside that gives us about 3.8 million mind patterns that will use.",
            "OK, so when I talk about patterns, they're centered on the position for a single vertex and up to 8 can match it at a single vertex, we don't."
        ],
        [
            "Well, 14.",
            "OK, so the models here again the Independent party classifiers.",
            "We just point out the nodes.",
            "Here are the vertex variables, both the configuration CI and the score of the territory.",
            "SI now the configuration can be black, white or empty in the scores.",
            "Obviously this black or white.",
            "OK, anytime you see a factor hanging off a unary AAAA unary variable, it will be a pattern factor.",
            "It will have weights Lambda, foreach pattern pie, and again we have 3.8 million lambdas to learn.",
            "OK, the coupling factors are much more simple, they just take into account the correlation between neighboring nodes.",
            "Here there are 36 possible configurations of the neighboring territory and configurations for couple of nodes."
        ],
        [
            "Enter.",
            "So we just quickly go over."
        ],
        [
            "The interface classifiers.",
            "These are fairly obviously very quick, so you have 8 patterns that may match it at A at a given node.",
            "Questions which pattern you use, you could use a small pattern.",
            "Obviously have a lot of data there, but it's not very productive.",
            "Use largest pattern you have less data, it can be more productive, but the variance of that pattern and the predictions can kill you, so you probably will look at ways you can combine all patterns.",
            "Logistic regression is an obvious way to use all patterns an independent model.",
            "Also discussed Bayesian model averaging, which is a neat trick and is very efficient and it works very well in some cases."
        ],
        [
            "As I will show so the case, it's an obvious idea.",
            "Most people probably know it.",
            "You want to predict the score of the territory for a given vertex given the configuration of the board and the data.",
            "OK, you have a lot of different models.",
            "Have a model for each pattern, so I'll call my models Talia pricey wine in a second.",
            "I'm just going to introduce that model variable in marginalized out.",
            "OK, so this is the prediction of the vertex territory.",
            "Given my particular model, and it's weighted by the.",
            "Probably that model given the data.",
            "So what is?",
            "This is just turns out with Bayes rule is just the likelihood the normalized likelihood of the data given the speaker model by some prior on your models.",
            "OK, very obvious.",
            "The only issue here is that the model must apply to all the data.",
            "An caveat with using patterns patterns only apply to a subset of the data.",
            "And there's a very simple test."
        ],
        [
            "And then we can do not show that next.",
            "So if you have a pattern pie and you wanted to give you predictions for all data, you can just make a simple tree model that says well if \u03c0 matches then you get some really distribution for the data there.",
            "And if I didn't match, they get some billing dispute with the data there.",
            "Now, how do we incorporate the pattern hierarchy where we can build tree models?",
            "So if Pi 2 subsumes Pi one and we say well, part one matches well whatever Part 2 also matches and you get paranoid distributions there.",
            "And if \u03c0 three.",
            "To this party empire one you can build this remodel OK and what this gives you effectively engage model averaging is you can efficiently compute the likelihoods of each of these three models and a strong likelihood or a high likelihood will indicate a.",
            "But a better model, obviously.",
            "So this is sort of a beige and white to do back off model used T3, but I predict if it's not predictable back off to T2 or T1 or the most predictive models and ice basean way to trade off the pattern heart pattern hierarchy and compared to logistic regression.",
            "Training here is closed form is just counts at the leaves it."
        ],
        [
            "Extremely efficient.",
            "OK, so this is where the independent model just quickly cover the options we have with CRF, which are just going to have the coupling factors and patterns which have couplings in the patterns the three."
        ],
        [
            "8 million patterns.",
            "OK partner for 19119 nineteen grid, I don't want to say inferences interact well, you can do.",
            "It takes about 2 minutes.",
            "It's too slow for training.",
            "We have 5000 games, three 100 to 200 per game.",
            "You can train with these exact inference.",
            "Loopy belief propagation is faster, but as will show its bias and sometimes that can hurt you.",
            "Sampling is unbiased, but slower than loopy, so there's no clear obvious win win situation for how to do inference in the CRF for training, OK?",
            "If you look at Max likelihood training, everyone knows this requires inference, so you have to do the inference.",
            "For every element of training data, OK, you can't do exact.",
            "You can use libpcap.",
            "It's very slow, so we're going to look at some other proxy methods that don't require you to do this inference in every."
        ],
        [
            "Training sample first.",
            "First ones obviously Sitter likelihood, so you're standing log likelihood is over the full model Sitter likelihood.",
            "We look at the edge based variant so you're just going to summer.",
            "Each factor in the local likelihood.",
            "For each factor clamped with respect to this Markov blanket.",
            "So this makes inference purely local.",
            "The hope is that you do catch the long range effects.",
            "Upper CRF in the data.",
            "So with infinite data under certain conditions you can show this will.",
            "Converge to the Max likelihood model wouldn't do it.",
            "Probably here, because we probably capture the model distribution exactly, but at least it indicates the likelihood.",
            "Can work well first, now that it's very fast inference, but only for training.",
            "You can't do this at runtime 'cause you acquired the labeled data for the."
        ],
        [
            "Call blanket OK. Local training introduced by Charles Sutton into McCallum, I think you do a 2005 OK nice simple idea.",
            "Just break the CRF into its component models.",
            "Training each of these components by Max likelihood.",
            "Charleston Tomica this past summer introduced shared unary piecewise extension.",
            "Very simple.",
            "Just share the unary variables among the higher order factors.",
            "In this way you can not waste your time with the higher order factors of modeling within areas.",
            "Can model and will see this as a."
        ],
        [
            "A little bit better.",
            "OK, so those are models and cover quickly cover them again and."
        ],
        [
            "Over some of the results.",
            "OK so I miss my models.",
            "The algorithms as the model I used is it independent is at Sierra for the pattern CRF.",
            "What's the training algorithm and what's the inference algorithm if not obvious?",
            "So for all the CRM going loopy BP, unless I specify otherwise, so first models we have are the independent models false largest pattern?",
            "We have base model averaging of trees we can use to priors for the trees.",
            "Uniform exponential, exponential was originally suggested by Oliver in hand when they introduced based model averaging treason that does work better as we'll see.",
            "Progression is independent model.",
            "OK for the CRF we can look at training with loopy BP and doing inference will be P or Spencer Wang sampling, which is unbiased sampler.",
            "For the pattern CRF'S.",
            "We can look at training with pseudo likelihood or just training the edges via pseudo likelihood using logistic regression.",
            "For the unary factors.",
            "And for piecewise and we can also look at using piecewise for themselves by Libby, BP.",
            "Inference on the on the full battery CR didn't work well.",
            "We had three point 8 million parameters and exact inference, and it just never gave us a good results.",
            "I don't include these results here.",
            "An Monte Carlo, which of course is sort of the gold standard, but."
        ],
        [
            "A bit slow.",
            "Such as cooking training time.",
            "Whether these are, you know.",
            "It depends on your application essay as to which wich wich model might be appropriate terms of training time.",
            "The independent models obviously very quick.",
            "They did just accounts for the data piece wise in our case is closed form given the way that we type parameters.",
            "So it's very fast training for the CRF logistic regression requires greater sense, little bit slower.",
            "Five hours city likelihood requires gradient descent for the pattern.",
            "CRF takes about 1/2 a day, and training with loopy BP on the CRF took.",
            "Well over 2 days."
        ],
        [
            "If you look at inference time independent models are obviously very efficient in generating all patterns.",
            "Isn't isn't too much slower than just looking at a single pattern.",
            "Libby, BP.",
            "On the plane CRF without patterns, 600 milliseconds had patterns to 2 milliseconds.",
            "These are both reasonable, especially at runtime.",
            "Monte Carlo takes almost 3 seconds to do inference, so it's a little bit slow if you want to do a lot of valuations, and we're trying to be Monte Carlo.",
            "Obvious fencing isn't a way to do it.",
            "It has unbiased inference, but it's taking 11 seconds, so you might as well use Monte Carlo because."
        ],
        [
            "It's already good algorithm.",
            "OK, the first thing to look at our vertex error, which is essentially just for each vertex.",
            "What's the fraction of Misclassifications?",
            "The net error or essentially the score error?",
            "Sum the expectations for every vertex and see how that differs from the actual score in absolute value and log likelihood obviously measures the model."
        ],
        [
            "OK so I will get trade offs so I just averaged a ton of data down to single points here.",
            "And given all the algorithms that I discussed earlier, I'm going to discuss these.",
            "I swear their class so Monte Carlo you'll notice is in lower left.",
            "By the way.",
            "Lower on net error is good.",
            "Lower in vertex errors.",
            "Good Monte Carlo can see probably does just about the best, but like I say it's a bit slow so we went right about that.",
            "All the independent models are in blue except for logistic regression.",
            "Now if we use the smallest pattern, obviously we don't get very good classification error.",
            "If we go to the largest pattern here, when we get better classification air.",
            "But if we can trade off all the patterns we get even better.",
            "So this does show that base model averaging it is a good idea for these sorts of models.",
            "If you look at logistic regression, you actually do a little bit better on the vertex air a little worse on the net error.",
            "OK, that's just appeared observation.",
            "I can't quite explain that.",
            "If you look at the CRF models, this is sort of interesting.",
            "So here we have for the square we have CRF, inference, training and inference.",
            "At runtime, we're done with OBP.",
            "The diamond is training with Lube because it's a little bit faster, but doing inference with the more expensive but unbiased Spencer Wang, and so we see that loopy BP.",
            "And this is for the company is doing pretty well on vertex error.",
            "It's only got 36 parameters and is beating our 3.8 million parameters so.",
            "This is a very nice result for CRF's.",
            "The only thing it's not doing well is the net error.",
            "It's much more biased or often compared to the independent models, and so if you I'll cover this in a second with some graphics as to why this occurs.",
            "But intuitively the probably because it's biased errors will be correlated, and when you're summing up the expectations, those errors in there in the expectations will be in the same sign and will cause more bias in the sum.",
            "Now and you can see this if you go to exact unbiased inference.",
            "Well, because of this stuff between training and runtime inference, you're going to see a little bit worse classification error, but you do see with less bias and less correlated error in the inference you do get a lower net error.",
            "OK, again, it may not be clear here, but I'm going to show you some more slides that make us more clear.",
            "OK, and if we look at the pattern CRF models that we trained with our approximate inference methods, we see that select it actually gets a very nice quick classification of vertex air.",
            "But because of the bias in the parameters in the inference of loopy, again we see a pretty bad net error so."
        ],
        [
            "Examine that a little more in depth.",
            "OK, so why is the vertex error the classification error better for the CRF?",
            "So if you look at, say, one of our best independent models, you see that OK.",
            "It makes a lot of independent errors.",
            "There's no real reason to predict that this should be black territory, given that it's closer to white than it is to black.",
            "OK, so based model averaging is making these sort of random errors.",
            "If you look at Libby, BP.",
            "The coupling factors smooth out these these errors so that you just you see a nice smooth distribution.",
            "This is sort of the more.",
            "Believable territory distribution given the stones here that are placed on the board.",
            "OK, so the coupling factors in the CRF really do help a lot, and that's why we get much better classification error."
        ],
        [
            "The CRF's OK, but why do we get worse net error for the CRF's?",
            "Well, OK, here is unbiased inference.",
            "So this is more or less what you should get.",
            "OK, and you see that out here where we're so far from any stones we don't really know what to predict.",
            "At least the unbiased doesn't know what to predict, but if you look at the errors in Lupien correlated error, it just constantly constantly reinforces this belief that this is going to be white.",
            "Based on these two groups of stones.",
            "OK, but this is a bias.",
            "It's not.",
            "It's not good inference here.",
            "And if you sum up these expectations then clearly going to be huge biased towards white.",
            "When in the exact inference you can see, you can see that you shouldn't have that bias, so loopy BP can be bad for certain metrics like netero or score and go."
        ],
        [
            "OK, and let's look at the last thing here is the bias of local training.",
            "So here I've trained this with maximum likelihood, not rain.",
            "This with piecewise.",
            "I'm doing inference with low BP.",
            "What you'll see is that piecewise tends to be much more saturated, is giving the same predictions or the same sign as Max likelihood, but because each factor has to independently count for the data, it's sort of overestimates its parameters.",
            "And when you combine that with Lib P. You see very biased inference.",
            "OK, so is this bad?",
            "Well, for vertex error classification there now it's not bad because it's still getting the sign right?",
            "That's fine, but if you want to look like the model fit or the net error the score error, you can see that you're going to get more bias when you use the exact."
        ],
        [
            "Training methods OK, let's analyze that just a little bit more.",
            "So here we see on the X on the Y axis the net error and we're trading this off versus the log likelihood or the negative like that again, being lower left is better.",
            "OK, so we can see, not surprisingly, that all the.",
            "Of the pattern, CRF models that were trained with with the exact inference had the port net error because their primitive biased in loopy amplifies that they report log likelihood because they weren't optimizing the likelihood directly.",
            "OK, it's not surprising we actually get nice little log likelihoods in that errors for the independent models showing these actually are.",
            "A slightly better fit to the data in terms of their honesty, and they're in their uncertainty.",
            "If we look at the roof models again, loopy CR Luby's is square and exact inference or unbiased inference for the diamonds, we see that.",
            "Again.",
            "If you.",
            "If you use the unbiased inference, you'll get a lower net error, but because of the mismatch between the training inference, you won't do so good on the log likelihood.",
            "OK, so just there's a lot of data here, and if you really want to look at CF training with any with with intractable models, you probably like them or more in depth.",
            "I'm not going to cover all the."
        ],
        [
            "So collisions you could draw, but just maybe two general messages.",
            "If you're looking at the CRC versus the independent models, theoretically the patterns have should do better, OK?",
            "However, the time cost of the inference is high for training an runtime, and you can save time with approx in training and inference, which is what we had to do here.",
            "OK, but on certain metrics here office may perform worse than the independent classifiers.",
            "So if you have a metric like net error, you went good predictions for go, then in fact the impending classifier similar may work better than pattern CRF.",
            "So just be aware of that trade off if you want to classification error that you probably do want to use CRF.",
            "OK, and finally, for the Internet models, we simply showed that the problem choosing the appropriate neighborhood in terms of the pattern size and we finished by Bayesian model averaging.",
            "It's nice little trick, you should have your toolbox is closed form, it's very fast and for our.",
            "We can make with respect to go is that if you want to do good score predictions for go then you want to use an independent model based model.",
            "Averaging this turned out to be the best with respect to our evaluation.",
            "OK that's it.",
            "Thank you.",
            "Very basic question, but the original problem formulation given a partial board instantiation and the training data is final territory, right?",
            "Right, so this comes from expert data and then we did some labelings with respect the expert data and we didn't for the for the.",
            "In the other data, because we don't have the agreed territory from the experts, we just have the final word Federation.",
            "If the final territory was not clear from the data, then we just didn't use that for training.",
            "So.",
            "Typically, how many steps were there between York?",
            "So you take a snapshot of the board and then trying to break the final outcome?",
            "And how many gameplays weather in between those?",
            "Wherein we are we trained from the first play to the end which is around 150 to 200 moves.",
            "So still even in the best case you're still 150 moves or so from the end.",
            "Configuration is huge.",
            "And it's very easy.",
            "So I would have thought you might need a different set of balls anymore halfway through the game.",
            "To take into account.",
            "OK, So what would you suggest?",
            "I'm just curious in terms of.",
            "OK, OK. That's why I'm trying to understand.",
            "Very good suggestion.",
            "Thanks.",
            "I was also just a basic question, so when you find out this neighborhood sizes for the local or the independent models.",
            "But what are we looking at?",
            "Their need the neighborhood is with respect to the patterns here.",
            "So so you have a niche 14 possible patterns.",
            "We have update patterns that we mined from the data and the large so the size 8 patterns give a larger neighborhood around the node upon which they condition on condition on the presence or absence.",
            "Well, I guess the color or not filled with each of those squares.",
            "Yeah, yeah, exactly figuration.",
            "Some.",
            "Right?",
            "OK.",
            "Patterns show up in how many training games do you have?",
            "We have 5000 training games.",
            "We typically matched, usually up to size of a pattern size 4 on every node.",
            "Whether whether you got up to aid.",
            "Obviously the beginning in the end you matched larger patterns than you do in the middle.",
            "So that's I mean, I initially had much more data, but it was hard for the for anyone to read, so we just collapse it down to the single predictions, averaged over all points in the game.",
            "So use this map for playing.",
            "Look ahead or something, he said.",
            "That's what idea I mean for us.",
            "It didn't work well myself in the original goal.",
            "This was used in a reinforcement learning.",
            "We just didn't have time to really evaluate that what we and.",
            "Just using the base model, averaging score prediction alone didn't work very well.",
            "The problem with go, especially on 1919 boards, it typically takes a lot of different approaches working together to make a good player.",
            "So just good score prediction alone isn't going to do it.",
            "But you think it will be useful.",
            "It could be a source component for other techniques that use that in their inference.",
            "It seems to me.",
            "Think about what happens in the next few steps rather than at the end of the game.",
            "Yeah, because football end of the game is far away and it's hard to predict.",
            "Secondly, maybe the current move depends more on immediate.",
            "Future.",
            "I mean, I have to move and it seems to me that.",
            "I would choose a mode that gives me some immediate benefits rather than.",
            "At low probability of success, in the end, right?",
            "So I would agree.",
            "I'm not ago expert but just empirically where is not fellow.",
            "You know game tree.",
            "Search to some depth works very well with cut offs.",
            "It doesn't work very well and go and empirically the best methods are the ones that roll out to the end and try to look at the final territory predictions.",
            "And tour is not here.",
            "Tours and exporting go.",
            "He could probably tell you why that is.",
            "I can't tell you why."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I did this work with Tor, grape or offer rich and humming at Microsoft Research.",
                    "label": 0
                },
                {
                    "sent": "David Stern provide a lot of the software and Michael Christopher gave me a lot of the Latex code to draw the nice diagrams you'll see in the talk.",
                    "label": 0
                },
                {
                    "sent": "We can't hear me just let me know.",
                    "label": 0
                },
                {
                    "sent": "An empirical talk and I just want to mainly talk about the tradeoffs involved in learning CRF's when inferencing in that CRF is intractable.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm applying this to go, so let me just quickly introduce the game just so you have an idea of what I'm trying to do.",
                    "label": 0
                },
                {
                    "sent": "About 4000 years ago in China, 60 million players, two players, black and white.",
                    "label": 1
                },
                {
                    "sent": "The boards 1919 grid looks like this if you're familiar with oh OK, you play stones of the vertices of the board like this one.",
                    "label": 0
                },
                {
                    "sent": "Player places a stone and every turn there's a capture rule, so if you can surround your opponent on the North East, Southwest positions, then you can effectively capture that.",
                    "label": 1
                },
                {
                    "sent": "So here Black is moved to position one and is captured the white stone and the name of the game is to capture territory by surrounding it.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the game.",
                    "label": 0
                },
                {
                    "sent": "You get this board in 99 go, then black would get all the territories surrounded and white would get all the territory that it is surrounded OK. Of course you want to maximize.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pretore OK.",
                    "label": 0
                },
                {
                    "sent": "In this talk I want to talk about territory prediction go so given a board configuration on the left, I want to predict the expected territory that you're going to get for each vertices on the board.",
                    "label": 0
                },
                {
                    "sent": "Of course, the rectangles indicate the color of the territory that you expect in this in the strength of it.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about ways to do this.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo is a very popular way to do it, actually works very well in practice.",
                    "label": 0
                },
                {
                    "sent": "Some of the best go players use Monte Carlo to predict territory.",
                    "label": 1
                },
                {
                    "sent": "The problem is that it takes 100 moves per turn.",
                    "label": 0
                },
                {
                    "sent": "Or there's an average of 180 miss returning, though typically games or are more than 100 moves, so averaging out to the end of the game, or, well, rolling out to the end of the game is extremely costly.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to see if we can predict as well as Monte Carlo or better, but using a factor graphical model to make this territory predictions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have lots of tools that we can use for doing this.",
                    "label": 0
                },
                {
                    "sent": "I just want to quickly cover this.",
                    "label": 0
                },
                {
                    "sent": "In the beginning we have higher cool pattern features commonly used in go.",
                    "label": 0
                },
                {
                    "sent": "I'll discuss those briefly.",
                    "label": 0
                },
                {
                    "sent": "the Dallas model you could use as an independent pattern based classifier, but it's worth comparing to this to see when this works and when it doesn't.",
                    "label": 0
                },
                {
                    "sent": "OK will cover more expressive CRF models and they have coupling factors.",
                    "label": 1
                },
                {
                    "sent": "Now realize that they go board is 19 by 19, so we have 3 or 60 one of these nodes.",
                    "label": 0
                },
                {
                    "sent": "So inference is intractable, and so if you have to learn CRS when the inference is intractable, it's not quite clear what's the best method.",
                    "label": 0
                },
                {
                    "sent": "I'm going to export the trade offs there.",
                    "label": 0
                },
                {
                    "sent": "I've lost experimental data and conclusions.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Girl.",
                    "label": 0
                },
                {
                    "sent": "OK, so the higher patterns we use their sent around a single position, their hierarchical and that is we start with a pattern size of 1 which is just the center position pattern.",
                    "label": 0
                },
                {
                    "sent": "Size 2 expands out a bit three so on we only use up to 8 nested pattern templates.",
                    "label": 1
                },
                {
                    "sent": "We mind the most frequent patterns from 5000 expert games inside that gives us about 3.8 million mind patterns that will use.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I talk about patterns, they're centered on the position for a single vertex and up to 8 can match it at a single vertex, we don't.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, 14.",
                    "label": 0
                },
                {
                    "sent": "OK, so the models here again the Independent party classifiers.",
                    "label": 0
                },
                {
                    "sent": "We just point out the nodes.",
                    "label": 0
                },
                {
                    "sent": "Here are the vertex variables, both the configuration CI and the score of the territory.",
                    "label": 1
                },
                {
                    "sent": "SI now the configuration can be black, white or empty in the scores.",
                    "label": 0
                },
                {
                    "sent": "Obviously this black or white.",
                    "label": 1
                },
                {
                    "sent": "OK, anytime you see a factor hanging off a unary AAAA unary variable, it will be a pattern factor.",
                    "label": 0
                },
                {
                    "sent": "It will have weights Lambda, foreach pattern pie, and again we have 3.8 million lambdas to learn.",
                    "label": 1
                },
                {
                    "sent": "OK, the coupling factors are much more simple, they just take into account the correlation between neighboring nodes.",
                    "label": 0
                },
                {
                    "sent": "Here there are 36 possible configurations of the neighboring territory and configurations for couple of nodes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "So we just quickly go over.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The interface classifiers.",
                    "label": 0
                },
                {
                    "sent": "These are fairly obviously very quick, so you have 8 patterns that may match it at A at a given node.",
                    "label": 0
                },
                {
                    "sent": "Questions which pattern you use, you could use a small pattern.",
                    "label": 0
                },
                {
                    "sent": "Obviously have a lot of data there, but it's not very productive.",
                    "label": 0
                },
                {
                    "sent": "Use largest pattern you have less data, it can be more productive, but the variance of that pattern and the predictions can kill you, so you probably will look at ways you can combine all patterns.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression is an obvious way to use all patterns an independent model.",
                    "label": 0
                },
                {
                    "sent": "Also discussed Bayesian model averaging, which is a neat trick and is very efficient and it works very well in some cases.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I will show so the case, it's an obvious idea.",
                    "label": 0
                },
                {
                    "sent": "Most people probably know it.",
                    "label": 0
                },
                {
                    "sent": "You want to predict the score of the territory for a given vertex given the configuration of the board and the data.",
                    "label": 0
                },
                {
                    "sent": "OK, you have a lot of different models.",
                    "label": 0
                },
                {
                    "sent": "Have a model for each pattern, so I'll call my models Talia pricey wine in a second.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to introduce that model variable in marginalized out.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the prediction of the vertex territory.",
                    "label": 0
                },
                {
                    "sent": "Given my particular model, and it's weighted by the.",
                    "label": 0
                },
                {
                    "sent": "Probably that model given the data.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                },
                {
                    "sent": "This is just turns out with Bayes rule is just the likelihood the normalized likelihood of the data given the speaker model by some prior on your models.",
                    "label": 0
                },
                {
                    "sent": "OK, very obvious.",
                    "label": 0
                },
                {
                    "sent": "The only issue here is that the model must apply to all the data.",
                    "label": 1
                },
                {
                    "sent": "An caveat with using patterns patterns only apply to a subset of the data.",
                    "label": 0
                },
                {
                    "sent": "And there's a very simple test.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we can do not show that next.",
                    "label": 0
                },
                {
                    "sent": "So if you have a pattern pie and you wanted to give you predictions for all data, you can just make a simple tree model that says well if \u03c0 matches then you get some really distribution for the data there.",
                    "label": 0
                },
                {
                    "sent": "And if I didn't match, they get some billing dispute with the data there.",
                    "label": 0
                },
                {
                    "sent": "Now, how do we incorporate the pattern hierarchy where we can build tree models?",
                    "label": 1
                },
                {
                    "sent": "So if Pi 2 subsumes Pi one and we say well, part one matches well whatever Part 2 also matches and you get paranoid distributions there.",
                    "label": 0
                },
                {
                    "sent": "And if \u03c0 three.",
                    "label": 0
                },
                {
                    "sent": "To this party empire one you can build this remodel OK and what this gives you effectively engage model averaging is you can efficiently compute the likelihoods of each of these three models and a strong likelihood or a high likelihood will indicate a.",
                    "label": 0
                },
                {
                    "sent": "But a better model, obviously.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a beige and white to do back off model used T3, but I predict if it's not predictable back off to T2 or T1 or the most predictive models and ice basean way to trade off the pattern heart pattern hierarchy and compared to logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Training here is closed form is just counts at the leaves it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extremely efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is where the independent model just quickly cover the options we have with CRF, which are just going to have the coupling factors and patterns which have couplings in the patterns the three.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "8 million patterns.",
                    "label": 0
                },
                {
                    "sent": "OK partner for 19119 nineteen grid, I don't want to say inferences interact well, you can do.",
                    "label": 0
                },
                {
                    "sent": "It takes about 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "It's too slow for training.",
                    "label": 0
                },
                {
                    "sent": "We have 5000 games, three 100 to 200 per game.",
                    "label": 0
                },
                {
                    "sent": "You can train with these exact inference.",
                    "label": 0
                },
                {
                    "sent": "Loopy belief propagation is faster, but as will show its bias and sometimes that can hurt you.",
                    "label": 0
                },
                {
                    "sent": "Sampling is unbiased, but slower than loopy, so there's no clear obvious win win situation for how to do inference in the CRF for training, OK?",
                    "label": 1
                },
                {
                    "sent": "If you look at Max likelihood training, everyone knows this requires inference, so you have to do the inference.",
                    "label": 0
                },
                {
                    "sent": "For every element of training data, OK, you can't do exact.",
                    "label": 0
                },
                {
                    "sent": "You can use libpcap.",
                    "label": 0
                },
                {
                    "sent": "It's very slow, so we're going to look at some other proxy methods that don't require you to do this inference in every.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training sample first.",
                    "label": 0
                },
                {
                    "sent": "First ones obviously Sitter likelihood, so you're standing log likelihood is over the full model Sitter likelihood.",
                    "label": 0
                },
                {
                    "sent": "We look at the edge based variant so you're just going to summer.",
                    "label": 0
                },
                {
                    "sent": "Each factor in the local likelihood.",
                    "label": 0
                },
                {
                    "sent": "For each factor clamped with respect to this Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "So this makes inference purely local.",
                    "label": 1
                },
                {
                    "sent": "The hope is that you do catch the long range effects.",
                    "label": 1
                },
                {
                    "sent": "Upper CRF in the data.",
                    "label": 0
                },
                {
                    "sent": "So with infinite data under certain conditions you can show this will.",
                    "label": 0
                },
                {
                    "sent": "Converge to the Max likelihood model wouldn't do it.",
                    "label": 1
                },
                {
                    "sent": "Probably here, because we probably capture the model distribution exactly, but at least it indicates the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Can work well first, now that it's very fast inference, but only for training.",
                    "label": 0
                },
                {
                    "sent": "You can't do this at runtime 'cause you acquired the labeled data for the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Call blanket OK. Local training introduced by Charles Sutton into McCallum, I think you do a 2005 OK nice simple idea.",
                    "label": 0
                },
                {
                    "sent": "Just break the CRF into its component models.",
                    "label": 0
                },
                {
                    "sent": "Training each of these components by Max likelihood.",
                    "label": 0
                },
                {
                    "sent": "Charleston Tomica this past summer introduced shared unary piecewise extension.",
                    "label": 1
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "Just share the unary variables among the higher order factors.",
                    "label": 0
                },
                {
                    "sent": "In this way you can not waste your time with the higher order factors of modeling within areas.",
                    "label": 0
                },
                {
                    "sent": "Can model and will see this as a.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit better.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are models and cover quickly cover them again and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over some of the results.",
                    "label": 0
                },
                {
                    "sent": "OK so I miss my models.",
                    "label": 0
                },
                {
                    "sent": "The algorithms as the model I used is it independent is at Sierra for the pattern CRF.",
                    "label": 0
                },
                {
                    "sent": "What's the training algorithm and what's the inference algorithm if not obvious?",
                    "label": 1
                },
                {
                    "sent": "So for all the CRM going loopy BP, unless I specify otherwise, so first models we have are the independent models false largest pattern?",
                    "label": 0
                },
                {
                    "sent": "We have base model averaging of trees we can use to priors for the trees.",
                    "label": 0
                },
                {
                    "sent": "Uniform exponential, exponential was originally suggested by Oliver in hand when they introduced based model averaging treason that does work better as we'll see.",
                    "label": 0
                },
                {
                    "sent": "Progression is independent model.",
                    "label": 0
                },
                {
                    "sent": "OK for the CRF we can look at training with loopy BP and doing inference will be P or Spencer Wang sampling, which is unbiased sampler.",
                    "label": 0
                },
                {
                    "sent": "For the pattern CRF'S.",
                    "label": 0
                },
                {
                    "sent": "We can look at training with pseudo likelihood or just training the edges via pseudo likelihood using logistic regression.",
                    "label": 0
                },
                {
                    "sent": "For the unary factors.",
                    "label": 0
                },
                {
                    "sent": "And for piecewise and we can also look at using piecewise for themselves by Libby, BP.",
                    "label": 0
                },
                {
                    "sent": "Inference on the on the full battery CR didn't work well.",
                    "label": 0
                },
                {
                    "sent": "We had three point 8 million parameters and exact inference, and it just never gave us a good results.",
                    "label": 0
                },
                {
                    "sent": "I don't include these results here.",
                    "label": 0
                },
                {
                    "sent": "An Monte Carlo, which of course is sort of the gold standard, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A bit slow.",
                    "label": 0
                },
                {
                    "sent": "Such as cooking training time.",
                    "label": 1
                },
                {
                    "sent": "Whether these are, you know.",
                    "label": 0
                },
                {
                    "sent": "It depends on your application essay as to which wich wich model might be appropriate terms of training time.",
                    "label": 0
                },
                {
                    "sent": "The independent models obviously very quick.",
                    "label": 0
                },
                {
                    "sent": "They did just accounts for the data piece wise in our case is closed form given the way that we type parameters.",
                    "label": 0
                },
                {
                    "sent": "So it's very fast training for the CRF logistic regression requires greater sense, little bit slower.",
                    "label": 0
                },
                {
                    "sent": "Five hours city likelihood requires gradient descent for the pattern.",
                    "label": 1
                },
                {
                    "sent": "CRF takes about 1/2 a day, and training with loopy BP on the CRF took.",
                    "label": 0
                },
                {
                    "sent": "Well over 2 days.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at inference time independent models are obviously very efficient in generating all patterns.",
                    "label": 0
                },
                {
                    "sent": "Isn't isn't too much slower than just looking at a single pattern.",
                    "label": 0
                },
                {
                    "sent": "Libby, BP.",
                    "label": 0
                },
                {
                    "sent": "On the plane CRF without patterns, 600 milliseconds had patterns to 2 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "These are both reasonable, especially at runtime.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo takes almost 3 seconds to do inference, so it's a little bit slow if you want to do a lot of valuations, and we're trying to be Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Obvious fencing isn't a way to do it.",
                    "label": 0
                },
                {
                    "sent": "It has unbiased inference, but it's taking 11 seconds, so you might as well use Monte Carlo because.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's already good algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, the first thing to look at our vertex error, which is essentially just for each vertex.",
                    "label": 1
                },
                {
                    "sent": "What's the fraction of Misclassifications?",
                    "label": 0
                },
                {
                    "sent": "The net error or essentially the score error?",
                    "label": 1
                },
                {
                    "sent": "Sum the expectations for every vertex and see how that differs from the actual score in absolute value and log likelihood obviously measures the model.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I will get trade offs so I just averaged a ton of data down to single points here.",
                    "label": 0
                },
                {
                    "sent": "And given all the algorithms that I discussed earlier, I'm going to discuss these.",
                    "label": 0
                },
                {
                    "sent": "I swear their class so Monte Carlo you'll notice is in lower left.",
                    "label": 1
                },
                {
                    "sent": "By the way.",
                    "label": 0
                },
                {
                    "sent": "Lower on net error is good.",
                    "label": 1
                },
                {
                    "sent": "Lower in vertex errors.",
                    "label": 0
                },
                {
                    "sent": "Good Monte Carlo can see probably does just about the best, but like I say it's a bit slow so we went right about that.",
                    "label": 0
                },
                {
                    "sent": "All the independent models are in blue except for logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Now if we use the smallest pattern, obviously we don't get very good classification error.",
                    "label": 0
                },
                {
                    "sent": "If we go to the largest pattern here, when we get better classification air.",
                    "label": 0
                },
                {
                    "sent": "But if we can trade off all the patterns we get even better.",
                    "label": 0
                },
                {
                    "sent": "So this does show that base model averaging it is a good idea for these sorts of models.",
                    "label": 0
                },
                {
                    "sent": "If you look at logistic regression, you actually do a little bit better on the vertex air a little worse on the net error.",
                    "label": 0
                },
                {
                    "sent": "OK, that's just appeared observation.",
                    "label": 0
                },
                {
                    "sent": "I can't quite explain that.",
                    "label": 0
                },
                {
                    "sent": "If you look at the CRF models, this is sort of interesting.",
                    "label": 0
                },
                {
                    "sent": "So here we have for the square we have CRF, inference, training and inference.",
                    "label": 0
                },
                {
                    "sent": "At runtime, we're done with OBP.",
                    "label": 0
                },
                {
                    "sent": "The diamond is training with Lube because it's a little bit faster, but doing inference with the more expensive but unbiased Spencer Wang, and so we see that loopy BP.",
                    "label": 0
                },
                {
                    "sent": "And this is for the company is doing pretty well on vertex error.",
                    "label": 0
                },
                {
                    "sent": "It's only got 36 parameters and is beating our 3.8 million parameters so.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice result for CRF's.",
                    "label": 1
                },
                {
                    "sent": "The only thing it's not doing well is the net error.",
                    "label": 0
                },
                {
                    "sent": "It's much more biased or often compared to the independent models, and so if you I'll cover this in a second with some graphics as to why this occurs.",
                    "label": 0
                },
                {
                    "sent": "But intuitively the probably because it's biased errors will be correlated, and when you're summing up the expectations, those errors in there in the expectations will be in the same sign and will cause more bias in the sum.",
                    "label": 0
                },
                {
                    "sent": "Now and you can see this if you go to exact unbiased inference.",
                    "label": 0
                },
                {
                    "sent": "Well, because of this stuff between training and runtime inference, you're going to see a little bit worse classification error, but you do see with less bias and less correlated error in the inference you do get a lower net error.",
                    "label": 0
                },
                {
                    "sent": "OK, again, it may not be clear here, but I'm going to show you some more slides that make us more clear.",
                    "label": 0
                },
                {
                    "sent": "OK, and if we look at the pattern CRF models that we trained with our approximate inference methods, we see that select it actually gets a very nice quick classification of vertex air.",
                    "label": 0
                },
                {
                    "sent": "But because of the bias in the parameters in the inference of loopy, again we see a pretty bad net error so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examine that a little more in depth.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is the vertex error the classification error better for the CRF?",
                    "label": 1
                },
                {
                    "sent": "So if you look at, say, one of our best independent models, you see that OK.",
                    "label": 0
                },
                {
                    "sent": "It makes a lot of independent errors.",
                    "label": 0
                },
                {
                    "sent": "There's no real reason to predict that this should be black territory, given that it's closer to white than it is to black.",
                    "label": 0
                },
                {
                    "sent": "OK, so based model averaging is making these sort of random errors.",
                    "label": 0
                },
                {
                    "sent": "If you look at Libby, BP.",
                    "label": 0
                },
                {
                    "sent": "The coupling factors smooth out these these errors so that you just you see a nice smooth distribution.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the more.",
                    "label": 0
                },
                {
                    "sent": "Believable territory distribution given the stones here that are placed on the board.",
                    "label": 0
                },
                {
                    "sent": "OK, so the coupling factors in the CRF really do help a lot, and that's why we get much better classification error.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The CRF's OK, but why do we get worse net error for the CRF's?",
                    "label": 1
                },
                {
                    "sent": "Well, OK, here is unbiased inference.",
                    "label": 1
                },
                {
                    "sent": "So this is more or less what you should get.",
                    "label": 0
                },
                {
                    "sent": "OK, and you see that out here where we're so far from any stones we don't really know what to predict.",
                    "label": 0
                },
                {
                    "sent": "At least the unbiased doesn't know what to predict, but if you look at the errors in Lupien correlated error, it just constantly constantly reinforces this belief that this is going to be white.",
                    "label": 0
                },
                {
                    "sent": "Based on these two groups of stones.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is a bias.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not good inference here.",
                    "label": 1
                },
                {
                    "sent": "And if you sum up these expectations then clearly going to be huge biased towards white.",
                    "label": 0
                },
                {
                    "sent": "When in the exact inference you can see, you can see that you shouldn't have that bias, so loopy BP can be bad for certain metrics like netero or score and go.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and let's look at the last thing here is the bias of local training.",
                    "label": 1
                },
                {
                    "sent": "So here I've trained this with maximum likelihood, not rain.",
                    "label": 0
                },
                {
                    "sent": "This with piecewise.",
                    "label": 0
                },
                {
                    "sent": "I'm doing inference with low BP.",
                    "label": 0
                },
                {
                    "sent": "What you'll see is that piecewise tends to be much more saturated, is giving the same predictions or the same sign as Max likelihood, but because each factor has to independently count for the data, it's sort of overestimates its parameters.",
                    "label": 1
                },
                {
                    "sent": "And when you combine that with Lib P. You see very biased inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so is this bad?",
                    "label": 0
                },
                {
                    "sent": "Well, for vertex error classification there now it's not bad because it's still getting the sign right?",
                    "label": 0
                },
                {
                    "sent": "That's fine, but if you want to look like the model fit or the net error the score error, you can see that you're going to get more bias when you use the exact.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training methods OK, let's analyze that just a little bit more.",
                    "label": 0
                },
                {
                    "sent": "So here we see on the X on the Y axis the net error and we're trading this off versus the log likelihood or the negative like that again, being lower left is better.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can see, not surprisingly, that all the.",
                    "label": 0
                },
                {
                    "sent": "Of the pattern, CRF models that were trained with with the exact inference had the port net error because their primitive biased in loopy amplifies that they report log likelihood because they weren't optimizing the likelihood directly.",
                    "label": 1
                },
                {
                    "sent": "OK, it's not surprising we actually get nice little log likelihoods in that errors for the independent models showing these actually are.",
                    "label": 0
                },
                {
                    "sent": "A slightly better fit to the data in terms of their honesty, and they're in their uncertainty.",
                    "label": 0
                },
                {
                    "sent": "If we look at the roof models again, loopy CR Luby's is square and exact inference or unbiased inference for the diamonds, we see that.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 1
                },
                {
                    "sent": "If you use the unbiased inference, you'll get a lower net error, but because of the mismatch between the training inference, you won't do so good on the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so just there's a lot of data here, and if you really want to look at CF training with any with with intractable models, you probably like them or more in depth.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to cover all the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So collisions you could draw, but just maybe two general messages.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at the CRC versus the independent models, theoretically the patterns have should do better, OK?",
                    "label": 0
                },
                {
                    "sent": "However, the time cost of the inference is high for training an runtime, and you can save time with approx in training and inference, which is what we had to do here.",
                    "label": 1
                },
                {
                    "sent": "OK, but on certain metrics here office may perform worse than the independent classifiers.",
                    "label": 1
                },
                {
                    "sent": "So if you have a metric like net error, you went good predictions for go, then in fact the impending classifier similar may work better than pattern CRF.",
                    "label": 1
                },
                {
                    "sent": "So just be aware of that trade off if you want to classification error that you probably do want to use CRF.",
                    "label": 0
                },
                {
                    "sent": "OK, and finally, for the Internet models, we simply showed that the problem choosing the appropriate neighborhood in terms of the pattern size and we finished by Bayesian model averaging.",
                    "label": 0
                },
                {
                    "sent": "It's nice little trick, you should have your toolbox is closed form, it's very fast and for our.",
                    "label": 0
                },
                {
                    "sent": "We can make with respect to go is that if you want to do good score predictions for go then you want to use an independent model based model.",
                    "label": 0
                },
                {
                    "sent": "Averaging this turned out to be the best with respect to our evaluation.",
                    "label": 0
                },
                {
                    "sent": "OK that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Very basic question, but the original problem formulation given a partial board instantiation and the training data is final territory, right?",
                    "label": 0
                },
                {
                    "sent": "Right, so this comes from expert data and then we did some labelings with respect the expert data and we didn't for the for the.",
                    "label": 0
                },
                {
                    "sent": "In the other data, because we don't have the agreed territory from the experts, we just have the final word Federation.",
                    "label": 0
                },
                {
                    "sent": "If the final territory was not clear from the data, then we just didn't use that for training.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Typically, how many steps were there between York?",
                    "label": 0
                },
                {
                    "sent": "So you take a snapshot of the board and then trying to break the final outcome?",
                    "label": 0
                },
                {
                    "sent": "And how many gameplays weather in between those?",
                    "label": 0
                },
                {
                    "sent": "Wherein we are we trained from the first play to the end which is around 150 to 200 moves.",
                    "label": 0
                },
                {
                    "sent": "So still even in the best case you're still 150 moves or so from the end.",
                    "label": 0
                },
                {
                    "sent": "Configuration is huge.",
                    "label": 0
                },
                {
                    "sent": "And it's very easy.",
                    "label": 0
                },
                {
                    "sent": "So I would have thought you might need a different set of balls anymore halfway through the game.",
                    "label": 0
                },
                {
                    "sent": "To take into account.",
                    "label": 0
                },
                {
                    "sent": "OK, So what would you suggest?",
                    "label": 0
                },
                {
                    "sent": "I'm just curious in terms of.",
                    "label": 0
                },
                {
                    "sent": "OK, OK. That's why I'm trying to understand.",
                    "label": 0
                },
                {
                    "sent": "Very good suggestion.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "I was also just a basic question, so when you find out this neighborhood sizes for the local or the independent models.",
                    "label": 0
                },
                {
                    "sent": "But what are we looking at?",
                    "label": 0
                },
                {
                    "sent": "Their need the neighborhood is with respect to the patterns here.",
                    "label": 0
                },
                {
                    "sent": "So so you have a niche 14 possible patterns.",
                    "label": 0
                },
                {
                    "sent": "We have update patterns that we mined from the data and the large so the size 8 patterns give a larger neighborhood around the node upon which they condition on condition on the presence or absence.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess the color or not filled with each of those squares.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly figuration.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Patterns show up in how many training games do you have?",
                    "label": 0
                },
                {
                    "sent": "We have 5000 training games.",
                    "label": 0
                },
                {
                    "sent": "We typically matched, usually up to size of a pattern size 4 on every node.",
                    "label": 0
                },
                {
                    "sent": "Whether whether you got up to aid.",
                    "label": 0
                },
                {
                    "sent": "Obviously the beginning in the end you matched larger patterns than you do in the middle.",
                    "label": 0
                },
                {
                    "sent": "So that's I mean, I initially had much more data, but it was hard for the for anyone to read, so we just collapse it down to the single predictions, averaged over all points in the game.",
                    "label": 0
                },
                {
                    "sent": "So use this map for playing.",
                    "label": 0
                },
                {
                    "sent": "Look ahead or something, he said.",
                    "label": 0
                },
                {
                    "sent": "That's what idea I mean for us.",
                    "label": 0
                },
                {
                    "sent": "It didn't work well myself in the original goal.",
                    "label": 0
                },
                {
                    "sent": "This was used in a reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We just didn't have time to really evaluate that what we and.",
                    "label": 0
                },
                {
                    "sent": "Just using the base model, averaging score prediction alone didn't work very well.",
                    "label": 0
                },
                {
                    "sent": "The problem with go, especially on 1919 boards, it typically takes a lot of different approaches working together to make a good player.",
                    "label": 0
                },
                {
                    "sent": "So just good score prediction alone isn't going to do it.",
                    "label": 0
                },
                {
                    "sent": "But you think it will be useful.",
                    "label": 0
                },
                {
                    "sent": "It could be a source component for other techniques that use that in their inference.",
                    "label": 0
                },
                {
                    "sent": "It seems to me.",
                    "label": 0
                },
                {
                    "sent": "Think about what happens in the next few steps rather than at the end of the game.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because football end of the game is far away and it's hard to predict.",
                    "label": 0
                },
                {
                    "sent": "Secondly, maybe the current move depends more on immediate.",
                    "label": 0
                },
                {
                    "sent": "Future.",
                    "label": 0
                },
                {
                    "sent": "I mean, I have to move and it seems to me that.",
                    "label": 0
                },
                {
                    "sent": "I would choose a mode that gives me some immediate benefits rather than.",
                    "label": 0
                },
                {
                    "sent": "At low probability of success, in the end, right?",
                    "label": 0
                },
                {
                    "sent": "So I would agree.",
                    "label": 0
                },
                {
                    "sent": "I'm not ago expert but just empirically where is not fellow.",
                    "label": 0
                },
                {
                    "sent": "You know game tree.",
                    "label": 0
                },
                {
                    "sent": "Search to some depth works very well with cut offs.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work very well and go and empirically the best methods are the ones that roll out to the end and try to look at the final territory predictions.",
                    "label": 0
                },
                {
                    "sent": "And tour is not here.",
                    "label": 0
                },
                {
                    "sent": "Tours and exporting go.",
                    "label": 0
                },
                {
                    "sent": "He could probably tell you why that is.",
                    "label": 0
                },
                {
                    "sent": "I can't tell you why.",
                    "label": 0
                }
            ]
        }
    }
}