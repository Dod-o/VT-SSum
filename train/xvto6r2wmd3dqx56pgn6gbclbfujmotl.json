{
    "id": "xvto6r2wmd3dqx56pgn6gbclbfujmotl",
    "title": "Computational and Statistical Tradeoffs via Convex Relaxation",
    "info": {
        "author": [
            "Venkat Chandrasekaran, Department of Computing and Mathematical Sciences, California Institute of Technology (Caltech)"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods->Convex Optimization"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_chandrasekaran_convex_relaxation/",
    "segmentation": [
        [
            "This is joint work with them.",
            "Michael Jordan at Berkeley at his work added as a postdoc last year.",
            "And it's about trying to understand tradeoffs between computational complexity and sample complexity and statistical inference, especially high dimensional statistical inference an it's not a traditional, I guess discrete optimization talk, although towards the end I will interface with some sort of traditional classical ideas and discrete optimization and show perhaps how maybe an inference viewpoint suggests that one should take a somewhat different view of discrete optimization convex relaxations for the screen optimization.",
            "As opposed to what?",
            "1 viewpoint one takes with with traditional?",
            "Traditional classical relaxations for discrete optimization.",
            "So somehow the inference viewpoint suggests a very different, fundamentally different viewpoint to these sorts of relaxations."
        ],
        [
            "So this talk is about statistical inference problems that arise with high dimensional data.",
            "And so the idea here is that the number of random variables, the number of parameters in the statistical model are very large and we would like to somehow do inference model selection with these kinds of problems with these kinds of models.",
            "So you can imagine Gene microarray analysis problems where maybe the number of random variables is on the order of 30 thousand 40,000 right?",
            "Then there are image processing problems involving climate studies and global weather modeling, where the number of variables may even be on the order of several millions.",
            "Then if the Netflix problem.",
            "Which many of us are familiar with, which you know is a problem involving sort of 100 million variables or more.",
            "And see if data living in very high dimensional spaces and we would like to do informed, reasoned statistical inference in such case."
        ],
        [
            "And one of the big success stories over the last say 10 to 15 years or so.",
            "Is to do this sort of statistical inference and settings where we have limited data where the number of variables P may be very large.",
            "This is what I mean by high dimensional.",
            "So P is very large, but the number of samples N is much smaller relative to pee.",
            "So in the gene microarray analysis experiment that I just mentioned, P is on the order of 30 to 40,000 and maybe on the order of a few hundreds.",
            "So we have very few data samples, very few observations relative to the number of random variables over which we wish to do inference.",
            "Any major success story over the last few years is that you know when you have such another posed problem.",
            "Somehow you need structured statistical models, so you're able to somehow exploit low dimensional structure to be able to reason or unformed inference with very very few data samples.",
            "And so this role of structure sparsity, low rank ranking problems, other sorts of problems as Peter really fundamental role and allied to this role of structure is the idea that we have very sophisticated computational methods.",
            "Methods, for instance based on L1 optimization or nuclear norm relaxations to solve these sorts of problems to exploit structure in these sorts of statistical models and really get away with very limited number of data observations and still do reason statistical inference when P is much much larger than N. But he's somewhat."
        ],
        [
            "Your challenge you see sort of articles on this.",
            "You know every week.",
            "Perhaps in the New York Times of The Economist or something, right?",
            "They call it big data and so on.",
            "Big data.",
            "Massive data is where we have way more data than what these fundamental limit suggests.",
            "So lots more data, somehow, way more data than we even know what to do with.",
            "And you know this is a setting where P is large, but then is also large.",
            "Social data is a setting where this comes up.",
            "Financial modeling is a setting where this comes up, and you know there are very interesting high energy physics experiments at CERN where they generate something on the order of hundreds of petabytes of data every year.",
            "So massive amounts of data, and we wish to do reason informed inference in such a setting.",
            "And we have way more data somehow than some of these fundamental limits suggest, and so the challenge is really computational way more than it is statistical.",
            "We have lots more data.",
            "How do we deal with this computationally?",
            "How do we exploit this data computation?"
        ],
        [
            "This large data set computation so consider sort of atypical inference scenario, and when I say typical inference in, I mean sort of a typical parameter estimation problem you wish to estimate some high dimensional parameter.",
            "P is very large and so you have N = 5000 samples to be able to process.",
            "Somehow an estimate this parameter.",
            "And so you know, run your computer for an hour and you'll get a risk.",
            "A statistical error of about .03.",
            "Now if I give you 500,000 data points and you run the same algorithm, sort of classical inference theory tells you you will reduce your risk, so it will go down to something much smaller, but you'll spend maybe 20 days on your computer, right?",
            "I've just cooked up these numbers, but it's sort of suggestive of these sorts of things we face in practice.",
            "But suppose we don't care about such small improvements in risk.",
            "You know these sorts of parameter estimation problems correspond to, you know statistical models that are very high dimensional and these parameters are sort of approximations to reality.",
            "These sorts of statistical models are only approximations to reality aren't reality, and so we maybe don't care about estimating these sorts of parameters to a very, very high degree of accuracy.",
            "Maybe something reasonable is good enough for us.",
            "So if I tell you that a risk of .03 is good enough for me and I don't care about improving the risk beyond some fixed level like that.",
            "What should your answer be?",
            "Your first answer should be.",
            "Well, I should never, you know.",
            "5000 data points are enough for me.",
            "If you give me 500,000 data points and they're all IID or exchangeable somehow.",
            "I can just take 5000 points out of this 500,000 large data set, run my computer for an hour and get to a risk of .03.",
            "That's certainly one answer.",
            "And that answer suggests that your runtime should never be more than an hour if your desired risk is .03.",
            "You should never have to run your computer for more than an hour, right?",
            "But that then suggests the next sort, which is.",
            "Can I exploit this larger data set to perhaps reduce runtime to perhaps say that you know if I have one MB of data I'm going to run my algorithm for an hour if I have one GB of data I'm going to run a different algorithm that maybe runs in half an hour or one terabyte of data.",
            "I'll run it, and even an even simpler algorithm that maybe runs in five minutes but gets me the same fixed risk.",
            "The idea is that I'm going to fix the desired accuracy, fix the desired risk, and then.",
            "And then try and obtain these sorts of tradeoffs.",
            "So can we do something like this in some sort of reasonable way?"
        ],
        [
            "And this sort of suggests at least my view, a fundamental tension if you will, between what we think of as computer or computational science and statistics.",
            "So in statistics you know if you open sort of any math, statistics, Journal, or textbook, these sorts of tradeoffs people talk about, or between number of data points or number of samples versus error.",
            "The more data you have, the small layer error.",
            "And how exactly this error scales as a function of the size of the data set.",
            "In computer science, or at least in numerical computation, we think of tradeoffs between sort of runtime and error.",
            "The more iterations are run, some iterative algorithm for.",
            "The smaller my error will be and there are no interesting tradeoffs there as well.",
            "But in computer science, we don't think of sort of data.",
            "Number of data points as a resource in the same way that we think of time as a resource in the same way that we think of space is a resource, right?",
            "More time is always good in computer science, but more data somehow not viewed as being a good resource, analogous to more time.",
            "So we'd like to be able to think of more data as actually being a resource, and this talk will sort of be trying to understand this 2D plane if you will trying to fix the error at some desired level and try and understand tradeoffs between runtime and amount of data.",
            "That's the spirit in which I'll be talking about this.",
            "There are many others.",
            "Mike Jordan, for instance, prefers to think about this slightly differently, where he prefers to fix the runtime, fix a computational budget if you are, or fix a runtime really, and then analyze tradeoffs between amount of data and error.",
            "He says you have a fixed.",
            "Computational budget or fixed runtime of an hour.",
            "And no matter how much data you have, as you get more and more data somehow, you should be reducing your error subject to only running your computer for an hour.",
            "This talk will be about, you know, fixing the error at some desired level and talking about tradeoffs between runtime and amount of data.",
            "These are analogous, obviously, but you know, depending on depending on one's viewpoint, someone's tastes, you can take, you can take a different take on these things."
        ],
        [
            "So even before I get to something precise, a very precise statistical inference problem where I'll describe these tradeoffs, I want to talk about conceptual level about what one can even expect from trying to study time data tradeoffs.",
            "What should we possibly expect and trying to write theorems alright, somewhat formal statements when we talk about time, data, tradeoffs, and then I'll go on to sort of a simple high dimensional estimation problem.",
            "And I'll talk about how one can write down convex programming based estimators to solve these statistical inference problems.",
            "And finally how by exploiting this convex programming structure and specifically by using the idea of convex relaxation, how one can actually obtain time data trade offs and I'll also touch upon as we go along some Allied work that people in machine learning people in computer science have done over the last 10 or 15 years trying to understand time data, tradeoffs in different settings.",
            "OK, so."
        ],
        [
            "I am going to in this talk.",
            "This is sort of a toy figure.",
            "It sort of informal, but I think it's very informative.",
            "It's I'm going to view inference procedures as points in a 2D plot, so runtime is 1 axis.",
            "Number of samples is another axis, and I'm going to imagine that the risk the desired risk in my parameter estimation problem is fixed.",
            "And I'm going to plot procedures as requiring a particular certain amount of runtime and a certain number of samples in this in this 2D plot.",
            "OK, and you know if I have points here that means I'll need an amount of runtime and that number of samples to achieve a desired a desired risk.",
            "So what do?"
        ],
        [
            "Even at a very basic level, anytime you have two D plots right there, trying to understand tradeoffs.",
            "The first thing you should ask about the horizontal and vertical lines mean.",
            "So what is a vertical line mean here?",
            "A vertical line on this plot means no matter sort of what I'm telling you is.",
            "Your sort of computational budget is unbounded.",
            "You can run your computer for as long as you want, but to achieve this fixed risk there is going to be a minimum number of samples you need no matter what.",
            "Right, and this is the domain of classical estimation theory.",
            "Estimation theory tells me this is a lower bound on the sample complexity to achieve a desired risk.",
            "And this is the domain of sort of estimation theory, and many Max theory in statistics, and this is relatively well understood that are interesting open problems here.",
            "But this is quite well understood and has been sort of has been well understood for awhile.",
            "You can."
        ],
        [
            "And sort of talk about horizontal lines, right?",
            "What do horizontal lines mean in this plot?",
            "What they mean?",
            "A complexity theoretic lower bounds?",
            "What they mean are for my inference problem.",
            "To obtain a fixed accuracy.",
            "I'm willing to give you however many samples you want.",
            "What's a lower bound on runtime?",
            "What's the minimum number of flops or operations you need to run on your computer?",
            "To be able to achieve this fixed risk given are unbound magic later.",
            "These are much, much harder to obtain complexity theory.",
            "Lower bounds are sort of these central problem in CS theory these days have been for awhile, and some of the most Canonical hard problems that we think of in CS such as satisfiability and so on.",
            "We don't have very good lower bounds.",
            "The best known lower bounds for the satisfiability problem, a linear and problem size.",
            "Where is the best?",
            "Known algorithms require exponential time in problem size, and so we don't have very good lower bounds.",
            "This also sort of depends on the kind of computational model.",
            "If you restrict yourself to say optimization based procedures, or if you restrict itself to some other class of procedures, then also plays a very important role.",
            "And so this is sort of much, much less well understood, and this sort of landscape.",
            "This research landscape will inform the kinds of statements were possibly going to be able to make, right?",
            "So when."
        ],
        [
            "I talk about tradeoffs.",
            "This is the sense in which I'll be talking about trade offs.",
            "I'll plot like I said, inference procedures as points in this plot.",
            "It is clear that as I give you more and more data, you shouldn't be using the same algorithm that you used when I gave you a small amount of data as I give you more data, you should switch to a different algorithm.",
            "If you use the same algorithm, your runtime is actually going to increase, right?",
            "So you should be using a different algorithm.",
            "In some sense.",
            "A weaker algorithm that processes larger datasets more coarsely.",
            "That's something you need to be able to do, that's one thing.",
            "The second sort of very basic, very elementary statement you can make is if I give you 1 trillion trillion trillion data points at some point it makes sense to just throw away data, right?",
            "If your goal is a fixed risk.",
            "At some point, if there is a computational cost, associated processing or touching every single data point, it just makes sense to throw away excess data.",
            "And so you will see things like this at some point, you'll plateau out.",
            "It makes no sense to sort of touch additional data points, you just throw it away.",
            "But the main point that I wish to make is that when I talk about tradeoffs here, I'm going to be talking about tradeoffs involving runtime upper bounds.",
            "So as you give me more data, I'll produce a different algorithm that has a smaller runtime upper bound.",
            "Not smaller runtime lower bound, because obtaining lower bounds like I mentioned is very, very hard.",
            "So if you give me different larger amount of data, I'll produce a weaker algorithm.",
            "That has a smaller runtime, but smaller worst case runtime.",
            "Worst case is measured in the alphabounce OK."
        ],
        [
            "So let's be a bit more precise.",
            "I'm you know when I thought about these things, I wanted to consider sort of the simplest possible problem.",
            "The simplest estimation problem where I could still talk in a reasonable way about about tradeoffs, and this, I think, is about as simple as one could get.",
            "So I have just a basic denoising problem where I have a signal X star that lives in some set in RP.",
            "The set I mean I've put bounded here, but actually doesn't need to be bounded.",
            "The noise is Gaussian.",
            "0 means they have variance Sigma squared.",
            "And this is the observation model I have.",
            "Why?",
            "That's the signal corrupted by noise and what I get RN IID samples drawn from Crown from the distribution of Y.",
            "Another example is why I.",
            "So what I'm going to assume is, of course that the signal is unknown to me.",
            "That's what I wish to estimate.",
            "But the signal set the set S from which X star comes is actually known to me.",
            "And this is not perhaps unreasonable in many, many applications we think of.",
            "For instance, you know these signals set S is perhaps being some sort of sparse vectors of sparse models.",
            "This is very reasonable in a lot of estimation problems.",
            "We may think of these sentences being some class of low rank matrices.",
            "This is also a reasonable assumption, so any sort of structural assumption, any sort of prior assumption you have on your model or on your space of parameters would be incorporated in the status, but text or some unknown signal belonging to this known set is."
        ],
        [
            "So it's then clear that the sample mean is a sufficient statistic, so you have to compute the sample mean that requires a certain number of operations, and any estimator would just be based on the sample mean.",
            "A natural estimator that you could write down is to compute the closest point to the sample mean.",
            "And the closest point in South to the sample.",
            "The most basic thing one could write down.",
            "But these sets S that we typically think of.",
            "Think of some friends in some sort of sparse vectors are low rank matrices or something like this.",
            "A typically very, very hard to project onto.",
            "A very complicated nonconvex and we in general will not be able to compute this even and then a reasonable amount of time, especially when P. The problem dimension is very large.",
            "And So what we consider a convex programming based estimators based on constraints ET C. Where C is any convex set that contains S inside of it.",
            "OK. And so the argument inside this estimator is X at Sabin goes from excess amount of excess event of C. OK, and this philosophy, by the way, of taking sort of a complicated set S and writing down its convex Hull.",
            "For instance, some convex outer approximation.",
            "This is exactly the philosophy that we adopt when we use the L1 norm as Sarah get for sparsity, or when we use the nuclear norm and trace norm as a surrogate for low rank matrices.",
            "Exactly the same idea that we've adopted and all these other settings.",
            "And so before I'm going to be able to.",
            "Tell you the statistical performance of this estimator."
        ],
        [
            "I need a couple of definitions.",
            "The first definition is an idea from sort of a concept from convex analysis of convex geometry, and that's what's called the Kona feasible directions.",
            "So, given a point X star and given a convex set C. I'm going to take the set of all directions into see from XR.",
            "This is the Kona feasible directions.",
            "Add X star with respect to the convex sets.",
            "And so think for instance of the set S is looking something like this something hideous?",
            "This is just a toy picture in 2D, that's one convex outer approximation as being something like this of just in this case taking the convex Hull.",
            "And then the corner feasible directions and text are is just the set of those directions routed to?",
            "Next are all the different ways I can go into the convex set next.",
            "This is 1 definition.",
            "We need the second definition we need."
        ],
        [
            "Is something close to the Gaussian complexity, but there's an X squared in it, which is why I put the square and parentheses here.",
            "But the Gaussian complexity of a cone is going to be defined as follows.",
            "It's just the standard definition of Gaussian or outer market complexity that we're all familiar with Gaussian complexity.",
            "In particular, where we take the expectation of the soup of a standard Gaussian random variable Z with respect to points in this cone.",
            "Because a cone sort of goes off to Infinity, I need to sort of normalize things, so normalized with respect to the Euclidean distance.",
            "Have that be less than or equal to 1.",
            "And so this is the Gaussian squared complexity.",
            "Gaussian complexity of account.",
            "So if the corner feasible directions and the Gaussian complexity of such cones, given these two ideas."
        ],
        [
            "This is the risk of the estimator that I just described that basic convex programming estimated.",
            "So the expected value of the mean squared error between next are.",
            "At this point the Sigma squared the variance divided by N. This is a reasonable quantity to show times.",
            "The Gaussian squared complexity of the corner feasible directions.",
            "The text are with respect to see.",
            "Very elementary proof 3 or 4 lines.",
            "Just applying the optimality conditions of your convex program.",
            "And the basic idea is this.",
            "I mean I only need to consider those error directions that take me into the convex set, right?",
            "Because that's the only set in which I can move from X time.",
            "Because that's a strong constraint in my in my convex program, so only need to consider the error directions in this corner feasible direction.",
            "So."
        ],
        [
            "So if in fact you used a very trivial convex set as an outer approximation to your signal set S, suppose you just use all of our P. Then we just recover what you typically get with the sample mean, which is a Sigma squared over N * P. One can also generalize this proposition in several ways, and I'm happy to talk about it offline.",
            "You can obtain better bias, variance tradeoffs, or similar sorts of results for non Gaussian noise, but I'm not going to go into this proposition is sort of all we need as we go along."
        ],
        [
            "OK.",
            "So rewriting that proposition here, the risk of that estimator is this so far now wish to obtain a risk of at most one.",
            "Remember going back to our original few slides.",
            "We said we'll fix the accuracy of fixed the risk and suppose I wish to only have a risk of at most one, then how many samples do you need.",
            "So I want this right hand side to be at most one flipping things around.",
            "I require that end should be greater than or equal to this quantity.",
            "That Sigma squared times the Gaussian squared complexity of that and recover.",
            "OK.",
            "So."
        ],
        [
            "Does this suggest this Gaussian squared complexity observation number one is that this gas is going complexity is monotonic and see if I have a larger convex set.",
            "see I have a larger Kona feasible directions from Xtar into see.",
            "That's monotonic again.",
            "The definition of Gaussian squared complexities monotonic in the argument, and so this, as I have larger and larger set see this number is going to be larger and larger.",
            "So if I give you some very large NA, very large number of data points in.",
            "You can use as logic convex set C. Such that this number times Sigma squared is still less than right.",
            "You can use any convex at sea.",
            "That satisfies this constraint to obtain a risk of at most one.",
            "And in particular, you can use the largest such 1, right?",
            "So if you have access and factory very large data set, you can use larger and larger convex sets."
        ],
        [
            "In this sense, every kind of algorithm weakening mechanism, right?",
            "What I can potentially do?",
            "Is I mean just like just like I showed a few slides ago.",
            "Imagine this being your signal set S. This great thing is exactly the convex Hull of S. That's the set C. And I can potentially if I have a very large number, very large number of samples NI can potentially use a weaker set.",
            "See a weaker approximation to CC prime?",
            "That's something like this.",
            "And it will have a larger cone, obviously of a feasible directions.",
            "Now let's in fact see Prime.",
            "Is easier to deal with computationally than C, then in fact, we've obtained a time data trade off right as you have more data, you use a larger convex Etsy.",
            "And if I can convince you that projecting onto larger convex Etsy is easier.",
            "Then you're in fact in good shape, right?",
            "OK."
        ],
        [
            "That's going to be sort of the idea in the next 2 slides, so one of the very nice things in the convex optimization literature, especially in linear programming and in summer, different programming over the last last 10 to 15 years or so has been this very, very principled way of obtaining outer convex approximations.",
            "The interesting looking interesting looking set.",
            "So if my set S is suitably algebraic and I'm not going to go into the technical aspects of that, a lot of sets that we imagine as representing.",
            "Parameter space is the model classes in machine learning tend to be algebraic.",
            "These are sets defined by systems of polynomial equations and inequality's.",
            "Then one can obtain a family of convex outer approximations to this convex Hull.",
            "With the following very interesting property that C3 will be easier to deal with computationally.",
            "And by that I mean easier to project onto.",
            "Then C4 and so on.",
            "I should rather say C2 will be easier to deal with than C3 and C1 will be easier to deal with than Cito and easier by easier to deal with.",
            "I mean it will be easier to project onto optimizing a linear functional over C1 will be easier than optimizing a linear function over C2 and so on and so forth.",
            "So you have a family of convex outer approximations ordered by approximation quality.",
            "But also ordered by computational complexity.",
            "And so both these both.",
            "These points are very, very important.",
            "You can obtain families of polyhedral relaxations.",
            "And that was the work of Sherali and Adams.",
            "You can obtain semi definite relaxations.",
            "Let's see work of Pablo Parillo and John Lasserre.",
            "You can obtain hyperbolic relaxations as the work of guarding and renigar.",
            "And the way in which you obtain weaker and weaker relaxations is based on this notion of lift and project.",
            "So one of these central ideas in convex optimization is to represent a set as the projection of some set upstairs as a prediction projection of some set in higher dimensional space.",
            "And the kinds of Canonical representations of sets that we consider a sets that are representable as the intersection of a cone and affine space.",
            "So if my cone is the orthant and I have an affine space intersecting it, such sets.",
            "A linear programming representable sets and the analogous optimization problems are called linear programming.",
            "If I have sets where the cone is the PSD cone, the cone of positive semi definite matrices in the sector of the defined spaces.",
            "Such sets are called semi definite representable sets, and the analogous optimization problems, semi definite programming problems, and so on and so forth.",
            "And the idea is that I have some convex set in P dimensional space.",
            "And I represent obtain increasingly better approximations to it by taking larger and larger sized lifts of this convex set.",
            "But taking semi definite representable sets and larger and larger dimensions by taking linear programming representable sets in larger and larger dimensions and projecting back down.",
            "So the dimension to which I left is actually what controls the complexity of projection onto my convex set.",
            "And so as I take larger and larger lifting dimensions.",
            "I'm going to have more and more difficult things to project onto and so this is something that's ordered just like I said by computational complexity, as well as by approximation quality."
        ],
        [
            "And so this just reiterates what I said in the largest in the previous slide.",
            "So these larger dimension lifts off a better approximation.",
            "But they also come at a greater computational cost.",
            "Anne.",
            "This just this just mentions the same point again.",
            "OK, so at."
        ],
        [
            "This point I would like to contrast what I've talked about, so will give."
        ],
        [
            "A few examples of how these this idea, this philosophy actually leads to very concrete time data tradeoffs in the coming slides, but I wish to sort of."
        ],
        [
            "Contrast with some previous work, so there's been some work sort of starting even in the CS theory learning theory side of things, and the late 90s and binary classifier learning where folks talked about time, data, tradeoffs, and trying to learn classifiers.",
            "There's also work by Naughty Serebro, and she'll have shorts few years ago, and I CML, where they use stochastic gradient descent to talk about some of these ideas.",
            "And there's been more recent work.",
            "This is actually work in computational biology where these guys were interested in time data tradeoffs from a very different perspective.",
            "An one of the ideas that one of the one of the common themes, at least that I noticed in some of these papers, if not all of them, is that a lot of extra data is required to be able to use weaker and weaker algorithms.",
            "And one thing you'll see in our examples is that in many cases not all just a modest amount of extra data is required to be able to use very simple, very simple algorithms.",
            "A body of work that's actually but closer in philosophy to some of what we've talked about is some work and sparse PCA and clustering, which also shows up in problems in network inference and so on by Artie sings Group at CMU and Martin Wainwright in the student, or Russia meaning where they compared few different algorithms for these sparse PCA problem.",
            "At least, Martin and Rash talked about semi definite programming versus thresholding thresholding is a very simple procedure to apply and requires.",
            "Requires more data than a more sophisticated procedure based on semi definite programming, but in contrast to sort of all this previous work, one thing that I think our work tries to really emphasizes this notion of algorithm weakening.",
            "This is fundamental if you want to obtain any kind of kind data trade off an.",
            "My view is that convex relaxation provides a very principled and very general way to do this.",
            "The sort of philosophy that I described of lift and project representing convex sets is actually very general, very broadly applicable.",
            "And I think it is potentially applicable in many settings."
        ],
        [
            "Before we get 2 examples, I want to talk about a couple of things very quickly.",
            "First thing is, how do we calculate runtime of these kind of these procedures?",
            "There are two components to computing runtime right.",
            "One is the.",
            "Complexity of computing the sample mean I have N data points and aggregate them.",
            "If these data points live in P dimensional space, that's N * P. Plus then the number of operations to project onto my convex set.",
            "This is the total runtime.",
            "As you get more and more data, this term is going to go up because N is larger.",
            "But the hope and the expectation in fact what we show is that this term goes down by much more.",
            "OK, so this time will go up because then becomes larger.",
            "But because you have a larger end, you're going to be able to use a much weaker convex set.",
            "So the game that you get the reduction in the number of operations for projection is actually much more, and that's how we'll obtain time data tradeoffs."
        ],
        [
            "The second thing is, how do we actually estimate Gaussian complexity or Gaussian squared complexity like I talked about a few slides ago.",
            "This is something we need to do to be able to understand the statistical performance of.",
            "Of our estimators, there are very interesting general techniques, some of which I've actually been very prevalent in machine learning as well.",
            "You can go all the way back to Dudley's paper from 1967, and these are very general and as a result they're not particularly sharp.",
            "In our case, sharpen the censored, you'll lose frequently log factors of polylog factors even, and estimating Gaussian complexity.",
            "The special case that we care about is estimating Gaussian complexities of convex cones.",
            "And in this particular case you can say something very specific.",
            "And very sharp.",
            "So if the Conti the convex County has a dual which has a relative volume of mu.",
            "So when I say relative volume I mean sort of the fraction of the space that's covered by the dual account.",
            "Dual is sort of, I guess, a.",
            "The standard idea from from convex geometry, convex analysis, but the idea is that if I have a larger County, it's dual becomes smaller and smaller.",
            "That's sort of all you need to know as you appreciate this, then the Gaussian squared complexity of this County is 20 times log of one over formula.",
            "So if I have a larger convex County.",
            "Its dual would become smaller and smaller, so mu would become smaller and smaller.",
            "Then one of 'em you becomes larger and so G FT will become larger.",
            "So things do go in the right direction.",
            "They sort of make sense intuitively as we expect them to make sense.",
            "And the proof of this appeals to Gaussian isoperimetry rather than dealing with all possible convex cones where they do as a relative volume of mu.",
            "If you appeal a Gaussian isoperimetry, you only need to be able to say that.",
            "You only need to really study is spherical cone.",
            "With dual having relative volume, you.",
            "Because in some sense, that's the extreme account, and once you do that you just appeal.",
            "You just sort of.",
            "Compute this quantity for these verical cone and then you're done.",
            "OK, so let's go."
        ],
        [
            "So some examples.",
            "So the first example will be something sort of motivate.",
            "These are stylized examples motivated by, I think, interesting applications, but still stylized.",
            "First example is where these signals and S consists of what I call cut matrices or rank one matrices.",
            "Where the factors consist of just plus minus ones.",
            "This is of interest in collaborative filtering and as well as in clustering actually.",
            "And so this is going to be my signal set.",
            "I have a signal from the set corrupted by noise and I wish to denoise now one point.",
            "I wish to make here is that I'm going to view the signal set as belonging or living in RP.",
            "And so the fact array is going to sort of be of dimension square root P. So you shouldn't worry about what happens inside.",
            "You should just always think of these signals set as living in RP, even though it's so this is a signal set of matrices that will live in P dimensional space.",
            "OK.",
            "The best possible convex relaxation you could write down is the convex Hull of S. That's known as the cut polytope and combinatorial optimization in computer science.",
            "It's an intractable polytope to be able to project onto.",
            "If you could project onto attractively, you'd be able to solve Max cut tractably.",
            "And this polytope requires runtime.",
            "That's not much, but if in fact you could project onto it, then the number of samples that you need to be able to get to a risk of one is some constant Times Square root P. A weaker set to project onto.",
            "There's something called the laptop.",
            "This is the standard.",
            "When I say these standard, the Canonical relaxation of the cut polytope that folks in CS theory user and combinatorially optimization use based on semi definite programming.",
            "This requires number of samples to get a risk of one of some other constant C2 that's larger than C1 Times Square root P, But the runtime goes from something that super polynomial to Peter the 2.25.",
            "The reason I have strange looking exponents here is because I'm viewing S is living in P dimensional space rather than, you know.",
            "K by K Dimension space is something like this space of matrices.",
            "So that's the reason for the strange exponent.",
            "You could also then observe that the elements of S are in fact rank one matrices, so you could adopt the philosophy that a lot of folks in machine learning have adopted that.",
            "Maybe I could use the nuclear norm as a relaxation of this, and if you do using nuclear norm, you can use SVD based ideas and projections and requires Peter the 1.5 operations and will require a larger constant C three Times Square root P operations to be able to project.",
            "OK."
        ],
        [
            "Another example is based on sort of banding estimators.",
            "This sort of shows up more in in covariance estimation in statistics.",
            "And a lot of a lot of interesting ideas for estimating very high dimensional covariance matrices are based on the idea that banding a covariance matrix is a good thing to do in the high dimension setting.",
            "By banding I mean thresholding entries of the covariance matrix.",
            "There are certain distance away from the diagonal, but this sort of method only makes sense if you have an ordering for your variables to begin with, right?",
            "This is frequently the case in many applications, but if you don't have an ordering for your variables.",
            "What do you do?",
            "You'd like to then jointly estimate an ordering.",
            "Getting matrix that's as nicely bendable as possible and then bandit, right?",
            "And so based on this sort of idea, a stylized problem we can consider is let MBA known tridiagonal matrix and think of them as being a covariance matrix and my signal set S is going to be the set of all possible Pie Empire transposes where pisy permutation matrix sort of all possible shufflings of the rows and columns event.",
            "M itself is a known tridiagonal matrix, but the signal set consists in order.",
            "This permutation is unknown to me and so I wish to be able to estimate somehow, jointly both the ordering as well as the covariance matrix.",
            "In this case, if I compute the convex Hull events, this is also intractable to project onto, it corresponds to the longest path problem in computer science.",
            "And you know, to get to a risk of one, that's how many samples you need.",
            "You then make the observation well if M is tridiagonal, it's sparse, so I could use the L1 norm.",
            "Projecting onto the L1 norm is trivial and you get just a constant factor.",
            "Larger number of samples required for projection.",
            "But with a much smaller runtime.",
            "A third example."
        ],
        [
            "Is where the signal set S consists of the set of all perfect matchings in a complete graph.",
            "Matchings are subsets of the edges where every edge is incident to exactly 1 node.",
            "At least that's what perfect matchings are.",
            "And every node only participates in one edge.",
            "And these sorts of problems show up in network inference where you have some unknown matching and you wish to estimate it given some corrupted observations.",
            "In this case, in fact, in contrast to the previous two cases, the convex Hull of is the best possible convex set that you could write down.",
            "Is in fact polynomial time projectable so it requires P to the five amount of time.",
            "It's based on Edmonds blossom algorithm.",
            "And you couple that with the ellipsoid method to be able to get to this.",
            "Requires that run those many samples to get to a risk of 1.",
            "You then make the observation that the signal said yes.",
            "The elements of these signals, set S, consists of matrices that are zeros and ones.",
            "But the property of each such matrix in the set is that they contain exactly the same number of ones in exactly the same number of zeros.",
            "So you could take the convex Hull.",
            "Now is the set of matrices that have that number of ones in that number of zeros.",
            "This is what's called the hyper simplex and polyhedral theory, or in combinatorics or in convex geometry, much, much easier to project onto and requires again just a constant factor more samples.",
            "The point I wanted to make in showing this example is that it's of interest not just to go from sort of exponential super polynomial runtime down to something simpler.",
            "It is of interest to go from some large polynomial to some small.",
            "Small polynomial too.",
            "Relation within the constant and we just see this inequality.",
            "Yeah, I'll I'll show one example where it's more than just a constant factor too.",
            "But in general all I'm able to say here is that these constants are larger.",
            "I have some estimates for these constants in specific cases, but I don't have sort of a generic way of producing estimates other than to say that it is monotonically increasing and there is a sharp difference between the two.",
            "Um?"
        ],
        [
            "So a fourth example, very much motivated by this, this point is going to be an example where there will be more than just a constant factor difference.",
            "In the sample complexity.",
            "And this is an example where the set S consists of adjacency matrices corresponding to planted cliques.",
            "In my in my graph.",
            "So this is going to be an adjacency matrix of a click on square root of the nodes of the size of the graph.",
            "The reason I've picked square root is because that's of interest in the planted Cleek problem, and many other settings, but that's going to be the signal set S. This is of interest in the sparse PCA problem and gene expression patterns.",
            "Network inference shows up in many interesting settings.",
            "In this case, computing the convex Hull of S is again intractable, but requires.",
            "Number of samples to produce a risk of one that's on the order of 4th root of P times log P. You make the observation that adjacency matrices of cliques.",
            "If you put ones on the diagonal, suitably rank one matrices you again using nuclear norm ball as a relaxation, much easier to project onto.",
            "But this requires qualitatively more more than just a constant factor number of cents.",
            "OK you can.",
            "I mean I've tried playing around with relaxations in between this and this that require a much larger runtime, but I haven't been able to move too much away from square root P in terms of in terms of number of samples.",
            "So this is an example where it does seem like there is a qualitative gap beyond just beyond just a a constant factor."
        ],
        [
            "One thing I want to point out is just with this example.",
            "It's a very very simple point, but I think it points to something something deeper is what if we use an even weaker relaxation of using convex Hull of S?"
        ],
        [
            "Which is the convex Hull of all these sorts of adjacency matrices of cliques.",
            "If."
        ],
        [
            "Use a nuclear numble.",
            "What if I use an even weaker relaxation to project onto?",
            "I can use the Euclidean ball.",
            "It properly scaled Euclidean ball to project onto.",
            "That's even more trivial to project onto the nuclear norm, alright?",
            "Why did I not list that as a third as the third algorithm?",
            "Sorry.",
            "Yeah, yeah.",
            "So if in fact used."
        ],
        [
            "Euclidean ball.",
            "The number of samples that I need to be able to produce a risk one estimate is order P, so it goes from four through the P log P sqrt P to something that's in the order of P. If I have order P samples.",
            "What's the runtime?",
            "The runtime is N * P?",
            "For the sample mean plus just order P operations to project onto the Euclidean norm Ball Euclidean unit Euclidean number.",
            "Or some scale version of it, but the runtime depends on the number of data points, and if N is on the order of P, you'll get a total runtime of order P squared.",
            "So you're using such a weak algorithm.",
            "That the number of data points you need to be able to produce a reasonable estimate is so large that even preprocessing such a large data set requires an enormous run.",
            "And so this sort of in this case it actually makes sense to throw away data, right?",
            "If in fact I give you order P samples and.",
            "This is a very not very careful statement.",
            "If you have order P samples and your choices between nuclear norm projection and Euclidean norm projection, just these two algorithms.",
            "Then it actually makes sense to throw away all the data beyond square root P samples and actually just use nuclear norm projections."
        ],
        [
            "And the sort of points of goes back to the this idea that we had in the plot before that at some point it makes sense to throw away data that point very much depends on the space of algorithms you're willing to consider.",
            "I've only talked about very specific convex optimization based process.",
            "But if this is the space of procedures you restrict yourself to.",
            "Then, beyond square root P samples it does make sense to throw away extra data.",
            "That's again the reason why I'm only going to talk about runtime upper bounds, so I mean, when I you know anytime one talks about tradeoffs it.",
            "It's sort of tempting to sort of talk.",
            "I think of tradeoffs in physics.",
            "For instance, it's always you know something like the uncertainty principle.",
            "Write something times something bigger than or equal to something.",
            "I'm not making statements like that, and I'm not going to be able to.",
            "I think given I think the current state of the art in complexity theory, right?",
            "So I know that the worst case runtime of nuclear non projection is Peter the 1.5."
        ],
        [
            "I know that the worst case runtime of Euclidean norm projection is order P, and that's what I'm going to stick to.",
            "So yes, that is a limitation and sort of any sort of attempt to study time data tradeoffs very much.",
            "OK."
        ],
        [
            "So there so I think interesting questions that arise with these with these examples, and many of these examples, you don't need too many extra data samples or just a constant factor more data samples to be able to use really simple algorithms.",
            "And one thing I wish to point out this and this is a point that sort of interfaces with more traditional ideas and discrete optimization.",
            "Is that some of these relaxations that I used a really terrible from an approximation ratio point of view?",
            "And approximation ratio is sort of the.",
            "The main thing you wish to measure if you're using these convex relaxations to solve discrete optimization problems if the underlying problem is somehow discrete.",
            "But these are these approximation ratios being bad somehow doesn't seem to matter as much if my objective is statistical inference or parameter estimation in the sense that I've described.",
            "And so it's sort of a very active area of research in theoretical computer science.",
            "Over the last few years have been to understand these approximation ratios of these hierarchies of linear programming.",
            "The semantic programming relaxations as a way to obtain approximation algorithms for hard problems in computer science.",
            "And I think it's of interest to understand not approximation ratios of these hierarchies, but rather Gaussian complexities or Gaussian squared complexities in the sense that I've described, because that's what's relevant for statistical inference, and in many cases these complexities are much nicely behaved relative to approximation ratios."
        ],
        [
            "Nicely behaved relative to approximation ratios.",
            "And this makes exactly the same point, but more graphically.",
            "So think of having two convex sets.",
            "You know the great thing, and then another approximation to it.",
            "Using this using this red lines.",
            "In computer science, when I say approximation theory, what they're really interested in the sort of measuring a few, will.",
            "Somehow the gap, roughly speaking between that and that, the worst case, such gap in all directions roughly speak what we care about.",
            "If you care about statistical inference of parameter estimation, is the Gaussian complexity of the gas and squared complexity of these tangent counts, or these cones are feasible directions?",
            "How does how does that sort of scale as you use weaker and weaker relaxations?",
            "And these can be very, very differently behaved.",
            "Like I said in some of these examples, approximation ratios are terrible.",
            "But measured from the viewpoint of Gaussian complexity, you just need a constant factor.",
            "So the difference in Gaussian complexity ratio is just a constant factor.",
            "The difference in approximation ratio is something that depends on problem dimension.",
            "OK."
        ],
        [
            "So, just to summarize, I talked about some of these challenges with massive datasets with very large datasets.",
            "And I talked about a very simple denoising problem in the high dimensional setting where one can talk about interesting time data tradeoffs via convex relaxation.",
            "I really used convex relaxation as an algorithm weakening mechanism.",
            "As you get larger and larger datasets.",
            "I think there are many interesting.",
            "Potential avenues for future work.",
            "Potentially other methods to weaken algorithms and machine learning.",
            "We can think of sort of dimensionality reduction as an algorithm weakening mechanism as I get more and more data project on the lower and lower dimensional spaces.",
            "Perhaps clustering as an algorithm weakening mechanism.",
            "There are very interesting research ideas and computer hardware actually, where there's this idea somehow that if I want to do arithmetic operations, so I want to add 100 and 100.",
            "You know the answer is 200, but if I'm willing to tolerate an error of say, 1%.",
            "So I'm happy for some answer between 198 and 202.",
            "Then I'm able to do these things.",
            "Much faster and with a much less power consumption in hardware.",
            "So far allow myself these small arithmetic and accuracies.",
            "I'm able to get away with much lower power consumption and much, much faster computations in hardware, and I think these sorts of ideas are also potentially relevant as algorithm weakening mechanisms and obviously also getting to more complex statistical inference problems beyond the denoising one that I've considered.",
            "We have a paper up on archive and it's also on my website and as well as in Mike Jordan's website.",
            "It's been up for about a few weeks or a month or so.",
            "Thank you.",
            "So in the case when NPR, both very very large, in the case where it's not even possible to store all of the data at one site in particular, sent like the distributed computing setting where something like computing the statistical estimation procedure sampling is extraordinarily easy to parallelize, might it not be simpler to use some of these simple projection methods like examples of P squared?",
            "My practically achievable yeah case that doing spherical projection is very, very easy because it's more amenable to distributed computation.",
            "Absolutely.",
            "No, but I mean the sense in which I considered computational resource with just a single computer number of operations.",
            "But if you bring in other dimensions to how computational complexity plays out based on whatever other constraints when we have in a problem like you know, not being able to store the data in anyone location.",
            "Certainly number of processors, number of locations, all these sorts of ideas come in.",
            "I haven't considered these, but I think that's very interesting and very relevant.",
            "Absolutely, and in those cases some of these.",
            "You know the way you'd measure runtime should be should also take into account how amenable it is to your particular computational infrastructure, absolutely.",
            "Quick question.",
            "What about step up?",
            "Going back to your non convex sets and then also depend on how you relax the set.",
            "Yeah, that's true.",
            "That's a good question I've.",
            "Sort of, I'm not able to say something in general there.",
            "So for instance, if you said S is somehow some set of sparse vectors, that's the case where we understand how we can go back from these solutions to relax problems to somehow that original set is in fact, there's a great literature saying that even if you use these convex relaxations, you will recover vectors that are on facts bars, and you can say the same thing with the trace norm with low rank matrices and so on.",
            "But that's sort of very specialized case by case analysis, and at least I don't know if a way to say something more general than that sort of uniformly for all sets S and so on.",
            "OK, thank you very much.",
            "I think there will be more time for discussions in the break and let's take this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is joint work with them.",
                    "label": 1
                },
                {
                    "sent": "Michael Jordan at Berkeley at his work added as a postdoc last year.",
                    "label": 0
                },
                {
                    "sent": "And it's about trying to understand tradeoffs between computational complexity and sample complexity and statistical inference, especially high dimensional statistical inference an it's not a traditional, I guess discrete optimization talk, although towards the end I will interface with some sort of traditional classical ideas and discrete optimization and show perhaps how maybe an inference viewpoint suggests that one should take a somewhat different view of discrete optimization convex relaxations for the screen optimization.",
                    "label": 0
                },
                {
                    "sent": "As opposed to what?",
                    "label": 0
                },
                {
                    "sent": "1 viewpoint one takes with with traditional?",
                    "label": 0
                },
                {
                    "sent": "Traditional classical relaxations for discrete optimization.",
                    "label": 0
                },
                {
                    "sent": "So somehow the inference viewpoint suggests a very different, fundamentally different viewpoint to these sorts of relaxations.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this talk is about statistical inference problems that arise with high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "And so the idea here is that the number of random variables, the number of parameters in the statistical model are very large and we would like to somehow do inference model selection with these kinds of problems with these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine Gene microarray analysis problems where maybe the number of random variables is on the order of 30 thousand 40,000 right?",
                    "label": 1
                },
                {
                    "sent": "Then there are image processing problems involving climate studies and global weather modeling, where the number of variables may even be on the order of several millions.",
                    "label": 0
                },
                {
                    "sent": "Then if the Netflix problem.",
                    "label": 0
                },
                {
                    "sent": "Which many of us are familiar with, which you know is a problem involving sort of 100 million variables or more.",
                    "label": 0
                },
                {
                    "sent": "And see if data living in very high dimensional spaces and we would like to do informed, reasoned statistical inference in such case.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the big success stories over the last say 10 to 15 years or so.",
                    "label": 0
                },
                {
                    "sent": "Is to do this sort of statistical inference and settings where we have limited data where the number of variables P may be very large.",
                    "label": 1
                },
                {
                    "sent": "This is what I mean by high dimensional.",
                    "label": 1
                },
                {
                    "sent": "So P is very large, but the number of samples N is much smaller relative to pee.",
                    "label": 0
                },
                {
                    "sent": "So in the gene microarray analysis experiment that I just mentioned, P is on the order of 30 to 40,000 and maybe on the order of a few hundreds.",
                    "label": 1
                },
                {
                    "sent": "So we have very few data samples, very few observations relative to the number of random variables over which we wish to do inference.",
                    "label": 0
                },
                {
                    "sent": "Any major success story over the last few years is that you know when you have such another posed problem.",
                    "label": 0
                },
                {
                    "sent": "Somehow you need structured statistical models, so you're able to somehow exploit low dimensional structure to be able to reason or unformed inference with very very few data samples.",
                    "label": 0
                },
                {
                    "sent": "And so this role of structure sparsity, low rank ranking problems, other sorts of problems as Peter really fundamental role and allied to this role of structure is the idea that we have very sophisticated computational methods.",
                    "label": 1
                },
                {
                    "sent": "Methods, for instance based on L1 optimization or nuclear norm relaxations to solve these sorts of problems to exploit structure in these sorts of statistical models and really get away with very limited number of data observations and still do reason statistical inference when P is much much larger than N. But he's somewhat.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your challenge you see sort of articles on this.",
                    "label": 0
                },
                {
                    "sent": "You know every week.",
                    "label": 0
                },
                {
                    "sent": "Perhaps in the New York Times of The Economist or something, right?",
                    "label": 0
                },
                {
                    "sent": "They call it big data and so on.",
                    "label": 0
                },
                {
                    "sent": "Big data.",
                    "label": 0
                },
                {
                    "sent": "Massive data is where we have way more data than what these fundamental limit suggests.",
                    "label": 0
                },
                {
                    "sent": "So lots more data, somehow, way more data than we even know what to do with.",
                    "label": 0
                },
                {
                    "sent": "And you know this is a setting where P is large, but then is also large.",
                    "label": 0
                },
                {
                    "sent": "Social data is a setting where this comes up.",
                    "label": 0
                },
                {
                    "sent": "Financial modeling is a setting where this comes up, and you know there are very interesting high energy physics experiments at CERN where they generate something on the order of hundreds of petabytes of data every year.",
                    "label": 0
                },
                {
                    "sent": "So massive amounts of data, and we wish to do reason informed inference in such a setting.",
                    "label": 0
                },
                {
                    "sent": "And we have way more data somehow than some of these fundamental limits suggest, and so the challenge is really computational way more than it is statistical.",
                    "label": 0
                },
                {
                    "sent": "We have lots more data.",
                    "label": 0
                },
                {
                    "sent": "How do we deal with this computationally?",
                    "label": 0
                },
                {
                    "sent": "How do we exploit this data computation?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This large data set computation so consider sort of atypical inference scenario, and when I say typical inference in, I mean sort of a typical parameter estimation problem you wish to estimate some high dimensional parameter.",
                    "label": 0
                },
                {
                    "sent": "P is very large and so you have N = 5000 samples to be able to process.",
                    "label": 0
                },
                {
                    "sent": "Somehow an estimate this parameter.",
                    "label": 0
                },
                {
                    "sent": "And so you know, run your computer for an hour and you'll get a risk.",
                    "label": 0
                },
                {
                    "sent": "A statistical error of about .03.",
                    "label": 0
                },
                {
                    "sent": "Now if I give you 500,000 data points and you run the same algorithm, sort of classical inference theory tells you you will reduce your risk, so it will go down to something much smaller, but you'll spend maybe 20 days on your computer, right?",
                    "label": 0
                },
                {
                    "sent": "I've just cooked up these numbers, but it's sort of suggestive of these sorts of things we face in practice.",
                    "label": 0
                },
                {
                    "sent": "But suppose we don't care about such small improvements in risk.",
                    "label": 1
                },
                {
                    "sent": "You know these sorts of parameter estimation problems correspond to, you know statistical models that are very high dimensional and these parameters are sort of approximations to reality.",
                    "label": 1
                },
                {
                    "sent": "These sorts of statistical models are only approximations to reality aren't reality, and so we maybe don't care about estimating these sorts of parameters to a very, very high degree of accuracy.",
                    "label": 0
                },
                {
                    "sent": "Maybe something reasonable is good enough for us.",
                    "label": 0
                },
                {
                    "sent": "So if I tell you that a risk of .03 is good enough for me and I don't care about improving the risk beyond some fixed level like that.",
                    "label": 0
                },
                {
                    "sent": "What should your answer be?",
                    "label": 0
                },
                {
                    "sent": "Your first answer should be.",
                    "label": 0
                },
                {
                    "sent": "Well, I should never, you know.",
                    "label": 0
                },
                {
                    "sent": "5000 data points are enough for me.",
                    "label": 0
                },
                {
                    "sent": "If you give me 500,000 data points and they're all IID or exchangeable somehow.",
                    "label": 0
                },
                {
                    "sent": "I can just take 5000 points out of this 500,000 large data set, run my computer for an hour and get to a risk of .03.",
                    "label": 0
                },
                {
                    "sent": "That's certainly one answer.",
                    "label": 0
                },
                {
                    "sent": "And that answer suggests that your runtime should never be more than an hour if your desired risk is .03.",
                    "label": 0
                },
                {
                    "sent": "You should never have to run your computer for more than an hour, right?",
                    "label": 0
                },
                {
                    "sent": "But that then suggests the next sort, which is.",
                    "label": 0
                },
                {
                    "sent": "Can I exploit this larger data set to perhaps reduce runtime to perhaps say that you know if I have one MB of data I'm going to run my algorithm for an hour if I have one GB of data I'm going to run a different algorithm that maybe runs in half an hour or one terabyte of data.",
                    "label": 0
                },
                {
                    "sent": "I'll run it, and even an even simpler algorithm that maybe runs in five minutes but gets me the same fixed risk.",
                    "label": 0
                },
                {
                    "sent": "The idea is that I'm going to fix the desired accuracy, fix the desired risk, and then.",
                    "label": 0
                },
                {
                    "sent": "And then try and obtain these sorts of tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "So can we do something like this in some sort of reasonable way?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this sort of suggests at least my view, a fundamental tension if you will, between what we think of as computer or computational science and statistics.",
                    "label": 0
                },
                {
                    "sent": "So in statistics you know if you open sort of any math, statistics, Journal, or textbook, these sorts of tradeoffs people talk about, or between number of data points or number of samples versus error.",
                    "label": 0
                },
                {
                    "sent": "The more data you have, the small layer error.",
                    "label": 0
                },
                {
                    "sent": "And how exactly this error scales as a function of the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "In computer science, or at least in numerical computation, we think of tradeoffs between sort of runtime and error.",
                    "label": 0
                },
                {
                    "sent": "The more iterations are run, some iterative algorithm for.",
                    "label": 0
                },
                {
                    "sent": "The smaller my error will be and there are no interesting tradeoffs there as well.",
                    "label": 0
                },
                {
                    "sent": "But in computer science, we don't think of sort of data.",
                    "label": 1
                },
                {
                    "sent": "Number of data points as a resource in the same way that we think of time as a resource in the same way that we think of space is a resource, right?",
                    "label": 0
                },
                {
                    "sent": "More time is always good in computer science, but more data somehow not viewed as being a good resource, analogous to more time.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to be able to think of more data as actually being a resource, and this talk will sort of be trying to understand this 2D plane if you will trying to fix the error at some desired level and try and understand tradeoffs between runtime and amount of data.",
                    "label": 0
                },
                {
                    "sent": "That's the spirit in which I'll be talking about this.",
                    "label": 0
                },
                {
                    "sent": "There are many others.",
                    "label": 0
                },
                {
                    "sent": "Mike Jordan, for instance, prefers to think about this slightly differently, where he prefers to fix the runtime, fix a computational budget if you are, or fix a runtime really, and then analyze tradeoffs between amount of data and error.",
                    "label": 0
                },
                {
                    "sent": "He says you have a fixed.",
                    "label": 0
                },
                {
                    "sent": "Computational budget or fixed runtime of an hour.",
                    "label": 0
                },
                {
                    "sent": "And no matter how much data you have, as you get more and more data somehow, you should be reducing your error subject to only running your computer for an hour.",
                    "label": 0
                },
                {
                    "sent": "This talk will be about, you know, fixing the error at some desired level and talking about tradeoffs between runtime and amount of data.",
                    "label": 0
                },
                {
                    "sent": "These are analogous, obviously, but you know, depending on depending on one's viewpoint, someone's tastes, you can take, you can take a different take on these things.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So even before I get to something precise, a very precise statistical inference problem where I'll describe these tradeoffs, I want to talk about conceptual level about what one can even expect from trying to study time data tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "What should we possibly expect and trying to write theorems alright, somewhat formal statements when we talk about time, data, tradeoffs, and then I'll go on to sort of a simple high dimensional estimation problem.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about how one can write down convex programming based estimators to solve these statistical inference problems.",
                    "label": 1
                },
                {
                    "sent": "And finally how by exploiting this convex programming structure and specifically by using the idea of convex relaxation, how one can actually obtain time data trade offs and I'll also touch upon as we go along some Allied work that people in machine learning people in computer science have done over the last 10 or 15 years trying to understand time data, tradeoffs in different settings.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am going to in this talk.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a toy figure.",
                    "label": 0
                },
                {
                    "sent": "It sort of informal, but I think it's very informative.",
                    "label": 0
                },
                {
                    "sent": "It's I'm going to view inference procedures as points in a 2D plot, so runtime is 1 axis.",
                    "label": 1
                },
                {
                    "sent": "Number of samples is another axis, and I'm going to imagine that the risk the desired risk in my parameter estimation problem is fixed.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to plot procedures as requiring a particular certain amount of runtime and a certain number of samples in this in this 2D plot.",
                    "label": 0
                },
                {
                    "sent": "OK, and you know if I have points here that means I'll need an amount of runtime and that number of samples to achieve a desired a desired risk.",
                    "label": 0
                },
                {
                    "sent": "So what do?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even at a very basic level, anytime you have two D plots right there, trying to understand tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "The first thing you should ask about the horizontal and vertical lines mean.",
                    "label": 0
                },
                {
                    "sent": "So what is a vertical line mean here?",
                    "label": 0
                },
                {
                    "sent": "A vertical line on this plot means no matter sort of what I'm telling you is.",
                    "label": 0
                },
                {
                    "sent": "Your sort of computational budget is unbounded.",
                    "label": 0
                },
                {
                    "sent": "You can run your computer for as long as you want, but to achieve this fixed risk there is going to be a minimum number of samples you need no matter what.",
                    "label": 1
                },
                {
                    "sent": "Right, and this is the domain of classical estimation theory.",
                    "label": 1
                },
                {
                    "sent": "Estimation theory tells me this is a lower bound on the sample complexity to achieve a desired risk.",
                    "label": 0
                },
                {
                    "sent": "And this is the domain of sort of estimation theory, and many Max theory in statistics, and this is relatively well understood that are interesting open problems here.",
                    "label": 0
                },
                {
                    "sent": "But this is quite well understood and has been sort of has been well understood for awhile.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And sort of talk about horizontal lines, right?",
                    "label": 1
                },
                {
                    "sent": "What do horizontal lines mean in this plot?",
                    "label": 0
                },
                {
                    "sent": "What they mean?",
                    "label": 1
                },
                {
                    "sent": "A complexity theoretic lower bounds?",
                    "label": 1
                },
                {
                    "sent": "What they mean are for my inference problem.",
                    "label": 0
                },
                {
                    "sent": "To obtain a fixed accuracy.",
                    "label": 0
                },
                {
                    "sent": "I'm willing to give you however many samples you want.",
                    "label": 0
                },
                {
                    "sent": "What's a lower bound on runtime?",
                    "label": 1
                },
                {
                    "sent": "What's the minimum number of flops or operations you need to run on your computer?",
                    "label": 0
                },
                {
                    "sent": "To be able to achieve this fixed risk given are unbound magic later.",
                    "label": 0
                },
                {
                    "sent": "These are much, much harder to obtain complexity theory.",
                    "label": 0
                },
                {
                    "sent": "Lower bounds are sort of these central problem in CS theory these days have been for awhile, and some of the most Canonical hard problems that we think of in CS such as satisfiability and so on.",
                    "label": 0
                },
                {
                    "sent": "We don't have very good lower bounds.",
                    "label": 0
                },
                {
                    "sent": "The best known lower bounds for the satisfiability problem, a linear and problem size.",
                    "label": 0
                },
                {
                    "sent": "Where is the best?",
                    "label": 0
                },
                {
                    "sent": "Known algorithms require exponential time in problem size, and so we don't have very good lower bounds.",
                    "label": 0
                },
                {
                    "sent": "This also sort of depends on the kind of computational model.",
                    "label": 1
                },
                {
                    "sent": "If you restrict yourself to say optimization based procedures, or if you restrict itself to some other class of procedures, then also plays a very important role.",
                    "label": 0
                },
                {
                    "sent": "And so this is sort of much, much less well understood, and this sort of landscape.",
                    "label": 0
                },
                {
                    "sent": "This research landscape will inform the kinds of statements were possibly going to be able to make, right?",
                    "label": 0
                },
                {
                    "sent": "So when.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I talk about tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "This is the sense in which I'll be talking about trade offs.",
                    "label": 0
                },
                {
                    "sent": "I'll plot like I said, inference procedures as points in this plot.",
                    "label": 0
                },
                {
                    "sent": "It is clear that as I give you more and more data, you shouldn't be using the same algorithm that you used when I gave you a small amount of data as I give you more data, you should switch to a different algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you use the same algorithm, your runtime is actually going to increase, right?",
                    "label": 0
                },
                {
                    "sent": "So you should be using a different algorithm.",
                    "label": 0
                },
                {
                    "sent": "In some sense.",
                    "label": 0
                },
                {
                    "sent": "A weaker algorithm that processes larger datasets more coarsely.",
                    "label": 1
                },
                {
                    "sent": "That's something you need to be able to do, that's one thing.",
                    "label": 1
                },
                {
                    "sent": "The second sort of very basic, very elementary statement you can make is if I give you 1 trillion trillion trillion data points at some point it makes sense to just throw away data, right?",
                    "label": 0
                },
                {
                    "sent": "If your goal is a fixed risk.",
                    "label": 1
                },
                {
                    "sent": "At some point, if there is a computational cost, associated processing or touching every single data point, it just makes sense to throw away excess data.",
                    "label": 0
                },
                {
                    "sent": "And so you will see things like this at some point, you'll plateau out.",
                    "label": 0
                },
                {
                    "sent": "It makes no sense to sort of touch additional data points, you just throw it away.",
                    "label": 0
                },
                {
                    "sent": "But the main point that I wish to make is that when I talk about tradeoffs here, I'm going to be talking about tradeoffs involving runtime upper bounds.",
                    "label": 0
                },
                {
                    "sent": "So as you give me more data, I'll produce a different algorithm that has a smaller runtime upper bound.",
                    "label": 1
                },
                {
                    "sent": "Not smaller runtime lower bound, because obtaining lower bounds like I mentioned is very, very hard.",
                    "label": 0
                },
                {
                    "sent": "So if you give me different larger amount of data, I'll produce a weaker algorithm.",
                    "label": 0
                },
                {
                    "sent": "That has a smaller runtime, but smaller worst case runtime.",
                    "label": 0
                },
                {
                    "sent": "Worst case is measured in the alphabounce OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's be a bit more precise.",
                    "label": 0
                },
                {
                    "sent": "I'm you know when I thought about these things, I wanted to consider sort of the simplest possible problem.",
                    "label": 0
                },
                {
                    "sent": "The simplest estimation problem where I could still talk in a reasonable way about about tradeoffs, and this, I think, is about as simple as one could get.",
                    "label": 0
                },
                {
                    "sent": "So I have just a basic denoising problem where I have a signal X star that lives in some set in RP.",
                    "label": 0
                },
                {
                    "sent": "The set I mean I've put bounded here, but actually doesn't need to be bounded.",
                    "label": 0
                },
                {
                    "sent": "The noise is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "0 means they have variance Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "And this is the observation model I have.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 1
                },
                {
                    "sent": "That's the signal corrupted by noise and what I get RN IID samples drawn from Crown from the distribution of Y.",
                    "label": 0
                },
                {
                    "sent": "Another example is why I.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to assume is, of course that the signal is unknown to me.",
                    "label": 0
                },
                {
                    "sent": "That's what I wish to estimate.",
                    "label": 0
                },
                {
                    "sent": "But the signal set the set S from which X star comes is actually known to me.",
                    "label": 0
                },
                {
                    "sent": "And this is not perhaps unreasonable in many, many applications we think of.",
                    "label": 0
                },
                {
                    "sent": "For instance, you know these signals set S is perhaps being some sort of sparse vectors of sparse models.",
                    "label": 0
                },
                {
                    "sent": "This is very reasonable in a lot of estimation problems.",
                    "label": 0
                },
                {
                    "sent": "We may think of these sentences being some class of low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "This is also a reasonable assumption, so any sort of structural assumption, any sort of prior assumption you have on your model or on your space of parameters would be incorporated in the status, but text or some unknown signal belonging to this known set is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's then clear that the sample mean is a sufficient statistic, so you have to compute the sample mean that requires a certain number of operations, and any estimator would just be based on the sample mean.",
                    "label": 0
                },
                {
                    "sent": "A natural estimator that you could write down is to compute the closest point to the sample mean.",
                    "label": 0
                },
                {
                    "sent": "And the closest point in South to the sample.",
                    "label": 0
                },
                {
                    "sent": "The most basic thing one could write down.",
                    "label": 0
                },
                {
                    "sent": "But these sets S that we typically think of.",
                    "label": 0
                },
                {
                    "sent": "Think of some friends in some sort of sparse vectors are low rank matrices or something like this.",
                    "label": 0
                },
                {
                    "sent": "A typically very, very hard to project onto.",
                    "label": 0
                },
                {
                    "sent": "A very complicated nonconvex and we in general will not be able to compute this even and then a reasonable amount of time, especially when P. The problem dimension is very large.",
                    "label": 0
                },
                {
                    "sent": "And So what we consider a convex programming based estimators based on constraints ET C. Where C is any convex set that contains S inside of it.",
                    "label": 1
                },
                {
                    "sent": "OK. And so the argument inside this estimator is X at Sabin goes from excess amount of excess event of C. OK, and this philosophy, by the way, of taking sort of a complicated set S and writing down its convex Hull.",
                    "label": 0
                },
                {
                    "sent": "For instance, some convex outer approximation.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the philosophy that we adopt when we use the L1 norm as Sarah get for sparsity, or when we use the nuclear norm and trace norm as a surrogate for low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same idea that we've adopted and all these other settings.",
                    "label": 0
                },
                {
                    "sent": "And so before I'm going to be able to.",
                    "label": 0
                },
                {
                    "sent": "Tell you the statistical performance of this estimator.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I need a couple of definitions.",
                    "label": 0
                },
                {
                    "sent": "The first definition is an idea from sort of a concept from convex analysis of convex geometry, and that's what's called the Kona feasible directions.",
                    "label": 0
                },
                {
                    "sent": "So, given a point X star and given a convex set C. I'm going to take the set of all directions into see from XR.",
                    "label": 1
                },
                {
                    "sent": "This is the Kona feasible directions.",
                    "label": 0
                },
                {
                    "sent": "Add X star with respect to the convex sets.",
                    "label": 0
                },
                {
                    "sent": "And so think for instance of the set S is looking something like this something hideous?",
                    "label": 0
                },
                {
                    "sent": "This is just a toy picture in 2D, that's one convex outer approximation as being something like this of just in this case taking the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "And then the corner feasible directions and text are is just the set of those directions routed to?",
                    "label": 0
                },
                {
                    "sent": "Next are all the different ways I can go into the convex set next.",
                    "label": 0
                },
                {
                    "sent": "This is 1 definition.",
                    "label": 0
                },
                {
                    "sent": "We need the second definition we need.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is something close to the Gaussian complexity, but there's an X squared in it, which is why I put the square and parentheses here.",
                    "label": 0
                },
                {
                    "sent": "But the Gaussian complexity of a cone is going to be defined as follows.",
                    "label": 1
                },
                {
                    "sent": "It's just the standard definition of Gaussian or outer market complexity that we're all familiar with Gaussian complexity.",
                    "label": 0
                },
                {
                    "sent": "In particular, where we take the expectation of the soup of a standard Gaussian random variable Z with respect to points in this cone.",
                    "label": 0
                },
                {
                    "sent": "Because a cone sort of goes off to Infinity, I need to sort of normalize things, so normalized with respect to the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "Have that be less than or equal to 1.",
                    "label": 1
                },
                {
                    "sent": "And so this is the Gaussian squared complexity.",
                    "label": 0
                },
                {
                    "sent": "Gaussian complexity of account.",
                    "label": 0
                },
                {
                    "sent": "So if the corner feasible directions and the Gaussian complexity of such cones, given these two ideas.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the risk of the estimator that I just described that basic convex programming estimated.",
                    "label": 1
                },
                {
                    "sent": "So the expected value of the mean squared error between next are.",
                    "label": 0
                },
                {
                    "sent": "At this point the Sigma squared the variance divided by N. This is a reasonable quantity to show times.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian squared complexity of the corner feasible directions.",
                    "label": 0
                },
                {
                    "sent": "The text are with respect to see.",
                    "label": 0
                },
                {
                    "sent": "Very elementary proof 3 or 4 lines.",
                    "label": 1
                },
                {
                    "sent": "Just applying the optimality conditions of your convex program.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is this.",
                    "label": 0
                },
                {
                    "sent": "I mean I only need to consider those error directions that take me into the convex set, right?",
                    "label": 0
                },
                {
                    "sent": "Because that's the only set in which I can move from X time.",
                    "label": 0
                },
                {
                    "sent": "Because that's a strong constraint in my in my convex program, so only need to consider the error directions in this corner feasible direction.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if in fact you used a very trivial convex set as an outer approximation to your signal set S, suppose you just use all of our P. Then we just recover what you typically get with the sample mean, which is a Sigma squared over N * P. One can also generalize this proposition in several ways, and I'm happy to talk about it offline.",
                    "label": 0
                },
                {
                    "sent": "You can obtain better bias, variance tradeoffs, or similar sorts of results for non Gaussian noise, but I'm not going to go into this proposition is sort of all we need as we go along.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So rewriting that proposition here, the risk of that estimator is this so far now wish to obtain a risk of at most one.",
                    "label": 1
                },
                {
                    "sent": "Remember going back to our original few slides.",
                    "label": 0
                },
                {
                    "sent": "We said we'll fix the accuracy of fixed the risk and suppose I wish to only have a risk of at most one, then how many samples do you need.",
                    "label": 0
                },
                {
                    "sent": "So I want this right hand side to be at most one flipping things around.",
                    "label": 0
                },
                {
                    "sent": "I require that end should be greater than or equal to this quantity.",
                    "label": 0
                },
                {
                    "sent": "That Sigma squared times the Gaussian squared complexity of that and recover.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does this suggest this Gaussian squared complexity observation number one is that this gas is going complexity is monotonic and see if I have a larger convex set.",
                    "label": 0
                },
                {
                    "sent": "see I have a larger Kona feasible directions from Xtar into see.",
                    "label": 0
                },
                {
                    "sent": "That's monotonic again.",
                    "label": 0
                },
                {
                    "sent": "The definition of Gaussian squared complexities monotonic in the argument, and so this, as I have larger and larger set see this number is going to be larger and larger.",
                    "label": 0
                },
                {
                    "sent": "So if I give you some very large NA, very large number of data points in.",
                    "label": 0
                },
                {
                    "sent": "You can use as logic convex set C. Such that this number times Sigma squared is still less than right.",
                    "label": 0
                },
                {
                    "sent": "You can use any convex at sea.",
                    "label": 0
                },
                {
                    "sent": "That satisfies this constraint to obtain a risk of at most one.",
                    "label": 1
                },
                {
                    "sent": "And in particular, you can use the largest such 1, right?",
                    "label": 1
                },
                {
                    "sent": "So if you have access and factory very large data set, you can use larger and larger convex sets.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this sense, every kind of algorithm weakening mechanism, right?",
                    "label": 0
                },
                {
                    "sent": "What I can potentially do?",
                    "label": 0
                },
                {
                    "sent": "Is I mean just like just like I showed a few slides ago.",
                    "label": 0
                },
                {
                    "sent": "Imagine this being your signal set S. This great thing is exactly the convex Hull of S. That's the set C. And I can potentially if I have a very large number, very large number of samples NI can potentially use a weaker set.",
                    "label": 0
                },
                {
                    "sent": "See a weaker approximation to CC prime?",
                    "label": 0
                },
                {
                    "sent": "That's something like this.",
                    "label": 0
                },
                {
                    "sent": "And it will have a larger cone, obviously of a feasible directions.",
                    "label": 0
                },
                {
                    "sent": "Now let's in fact see Prime.",
                    "label": 0
                },
                {
                    "sent": "Is easier to deal with computationally than C, then in fact, we've obtained a time data trade off right as you have more data, you use a larger convex Etsy.",
                    "label": 0
                },
                {
                    "sent": "And if I can convince you that projecting onto larger convex Etsy is easier.",
                    "label": 0
                },
                {
                    "sent": "Then you're in fact in good shape, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's going to be sort of the idea in the next 2 slides, so one of the very nice things in the convex optimization literature, especially in linear programming and in summer, different programming over the last last 10 to 15 years or so has been this very, very principled way of obtaining outer convex approximations.",
                    "label": 0
                },
                {
                    "sent": "The interesting looking interesting looking set.",
                    "label": 0
                },
                {
                    "sent": "So if my set S is suitably algebraic and I'm not going to go into the technical aspects of that, a lot of sets that we imagine as representing.",
                    "label": 0
                },
                {
                    "sent": "Parameter space is the model classes in machine learning tend to be algebraic.",
                    "label": 0
                },
                {
                    "sent": "These are sets defined by systems of polynomial equations and inequality's.",
                    "label": 0
                },
                {
                    "sent": "Then one can obtain a family of convex outer approximations to this convex Hull.",
                    "label": 1
                },
                {
                    "sent": "With the following very interesting property that C3 will be easier to deal with computationally.",
                    "label": 0
                },
                {
                    "sent": "And by that I mean easier to project onto.",
                    "label": 0
                },
                {
                    "sent": "Then C4 and so on.",
                    "label": 0
                },
                {
                    "sent": "I should rather say C2 will be easier to deal with than C3 and C1 will be easier to deal with than Cito and easier by easier to deal with.",
                    "label": 0
                },
                {
                    "sent": "I mean it will be easier to project onto optimizing a linear functional over C1 will be easier than optimizing a linear function over C2 and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you have a family of convex outer approximations ordered by approximation quality.",
                    "label": 1
                },
                {
                    "sent": "But also ordered by computational complexity.",
                    "label": 0
                },
                {
                    "sent": "And so both these both.",
                    "label": 0
                },
                {
                    "sent": "These points are very, very important.",
                    "label": 0
                },
                {
                    "sent": "You can obtain families of polyhedral relaxations.",
                    "label": 0
                },
                {
                    "sent": "And that was the work of Sherali and Adams.",
                    "label": 0
                },
                {
                    "sent": "You can obtain semi definite relaxations.",
                    "label": 0
                },
                {
                    "sent": "Let's see work of Pablo Parillo and John Lasserre.",
                    "label": 0
                },
                {
                    "sent": "You can obtain hyperbolic relaxations as the work of guarding and renigar.",
                    "label": 0
                },
                {
                    "sent": "And the way in which you obtain weaker and weaker relaxations is based on this notion of lift and project.",
                    "label": 0
                },
                {
                    "sent": "So one of these central ideas in convex optimization is to represent a set as the projection of some set upstairs as a prediction projection of some set in higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And the kinds of Canonical representations of sets that we consider a sets that are representable as the intersection of a cone and affine space.",
                    "label": 0
                },
                {
                    "sent": "So if my cone is the orthant and I have an affine space intersecting it, such sets.",
                    "label": 0
                },
                {
                    "sent": "A linear programming representable sets and the analogous optimization problems are called linear programming.",
                    "label": 0
                },
                {
                    "sent": "If I have sets where the cone is the PSD cone, the cone of positive semi definite matrices in the sector of the defined spaces.",
                    "label": 0
                },
                {
                    "sent": "Such sets are called semi definite representable sets, and the analogous optimization problems, semi definite programming problems, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that I have some convex set in P dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And I represent obtain increasingly better approximations to it by taking larger and larger sized lifts of this convex set.",
                    "label": 0
                },
                {
                    "sent": "But taking semi definite representable sets and larger and larger dimensions by taking linear programming representable sets in larger and larger dimensions and projecting back down.",
                    "label": 0
                },
                {
                    "sent": "So the dimension to which I left is actually what controls the complexity of projection onto my convex set.",
                    "label": 0
                },
                {
                    "sent": "And so as I take larger and larger lifting dimensions.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have more and more difficult things to project onto and so this is something that's ordered just like I said by computational complexity, as well as by approximation quality.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so this just reiterates what I said in the largest in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So these larger dimension lifts off a better approximation.",
                    "label": 1
                },
                {
                    "sent": "But they also come at a greater computational cost.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This just this just mentions the same point again.",
                    "label": 0
                },
                {
                    "sent": "OK, so at.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This point I would like to contrast what I've talked about, so will give.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A few examples of how these this idea, this philosophy actually leads to very concrete time data tradeoffs in the coming slides, but I wish to sort of.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contrast with some previous work, so there's been some work sort of starting even in the CS theory learning theory side of things, and the late 90s and binary classifier learning where folks talked about time, data, tradeoffs, and trying to learn classifiers.",
                    "label": 0
                },
                {
                    "sent": "There's also work by Naughty Serebro, and she'll have shorts few years ago, and I CML, where they use stochastic gradient descent to talk about some of these ideas.",
                    "label": 0
                },
                {
                    "sent": "And there's been more recent work.",
                    "label": 0
                },
                {
                    "sent": "This is actually work in computational biology where these guys were interested in time data tradeoffs from a very different perspective.",
                    "label": 0
                },
                {
                    "sent": "An one of the ideas that one of the one of the common themes, at least that I noticed in some of these papers, if not all of them, is that a lot of extra data is required to be able to use weaker and weaker algorithms.",
                    "label": 0
                },
                {
                    "sent": "And one thing you'll see in our examples is that in many cases not all just a modest amount of extra data is required to be able to use very simple, very simple algorithms.",
                    "label": 1
                },
                {
                    "sent": "A body of work that's actually but closer in philosophy to some of what we've talked about is some work and sparse PCA and clustering, which also shows up in problems in network inference and so on by Artie sings Group at CMU and Martin Wainwright in the student, or Russia meaning where they compared few different algorithms for these sparse PCA problem.",
                    "label": 0
                },
                {
                    "sent": "At least, Martin and Rash talked about semi definite programming versus thresholding thresholding is a very simple procedure to apply and requires.",
                    "label": 0
                },
                {
                    "sent": "Requires more data than a more sophisticated procedure based on semi definite programming, but in contrast to sort of all this previous work, one thing that I think our work tries to really emphasizes this notion of algorithm weakening.",
                    "label": 1
                },
                {
                    "sent": "This is fundamental if you want to obtain any kind of kind data trade off an.",
                    "label": 0
                },
                {
                    "sent": "My view is that convex relaxation provides a very principled and very general way to do this.",
                    "label": 1
                },
                {
                    "sent": "The sort of philosophy that I described of lift and project representing convex sets is actually very general, very broadly applicable.",
                    "label": 0
                },
                {
                    "sent": "And I think it is potentially applicable in many settings.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before we get 2 examples, I want to talk about a couple of things very quickly.",
                    "label": 1
                },
                {
                    "sent": "First thing is, how do we calculate runtime of these kind of these procedures?",
                    "label": 1
                },
                {
                    "sent": "There are two components to computing runtime right.",
                    "label": 0
                },
                {
                    "sent": "One is the.",
                    "label": 0
                },
                {
                    "sent": "Complexity of computing the sample mean I have N data points and aggregate them.",
                    "label": 0
                },
                {
                    "sent": "If these data points live in P dimensional space, that's N * P. Plus then the number of operations to project onto my convex set.",
                    "label": 1
                },
                {
                    "sent": "This is the total runtime.",
                    "label": 0
                },
                {
                    "sent": "As you get more and more data, this term is going to go up because N is larger.",
                    "label": 0
                },
                {
                    "sent": "But the hope and the expectation in fact what we show is that this term goes down by much more.",
                    "label": 0
                },
                {
                    "sent": "OK, so this time will go up because then becomes larger.",
                    "label": 0
                },
                {
                    "sent": "But because you have a larger end, you're going to be able to use a much weaker convex set.",
                    "label": 0
                },
                {
                    "sent": "So the game that you get the reduction in the number of operations for projection is actually much more, and that's how we'll obtain time data tradeoffs.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second thing is, how do we actually estimate Gaussian complexity or Gaussian squared complexity like I talked about a few slides ago.",
                    "label": 0
                },
                {
                    "sent": "This is something we need to do to be able to understand the statistical performance of.",
                    "label": 0
                },
                {
                    "sent": "Of our estimators, there are very interesting general techniques, some of which I've actually been very prevalent in machine learning as well.",
                    "label": 0
                },
                {
                    "sent": "You can go all the way back to Dudley's paper from 1967, and these are very general and as a result they're not particularly sharp.",
                    "label": 0
                },
                {
                    "sent": "In our case, sharpen the censored, you'll lose frequently log factors of polylog factors even, and estimating Gaussian complexity.",
                    "label": 1
                },
                {
                    "sent": "The special case that we care about is estimating Gaussian complexities of convex cones.",
                    "label": 0
                },
                {
                    "sent": "And in this particular case you can say something very specific.",
                    "label": 0
                },
                {
                    "sent": "And very sharp.",
                    "label": 0
                },
                {
                    "sent": "So if the Conti the convex County has a dual which has a relative volume of mu.",
                    "label": 1
                },
                {
                    "sent": "So when I say relative volume I mean sort of the fraction of the space that's covered by the dual account.",
                    "label": 0
                },
                {
                    "sent": "Dual is sort of, I guess, a.",
                    "label": 0
                },
                {
                    "sent": "The standard idea from from convex geometry, convex analysis, but the idea is that if I have a larger County, it's dual becomes smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "That's sort of all you need to know as you appreciate this, then the Gaussian squared complexity of this County is 20 times log of one over formula.",
                    "label": 0
                },
                {
                    "sent": "So if I have a larger convex County.",
                    "label": 0
                },
                {
                    "sent": "Its dual would become smaller and smaller, so mu would become smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "Then one of 'em you becomes larger and so G FT will become larger.",
                    "label": 0
                },
                {
                    "sent": "So things do go in the right direction.",
                    "label": 0
                },
                {
                    "sent": "They sort of make sense intuitively as we expect them to make sense.",
                    "label": 0
                },
                {
                    "sent": "And the proof of this appeals to Gaussian isoperimetry rather than dealing with all possible convex cones where they do as a relative volume of mu.",
                    "label": 0
                },
                {
                    "sent": "If you appeal a Gaussian isoperimetry, you only need to be able to say that.",
                    "label": 0
                },
                {
                    "sent": "You only need to really study is spherical cone.",
                    "label": 0
                },
                {
                    "sent": "With dual having relative volume, you.",
                    "label": 0
                },
                {
                    "sent": "Because in some sense, that's the extreme account, and once you do that you just appeal.",
                    "label": 0
                },
                {
                    "sent": "You just sort of.",
                    "label": 0
                },
                {
                    "sent": "Compute this quantity for these verical cone and then you're done.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some examples.",
                    "label": 0
                },
                {
                    "sent": "So the first example will be something sort of motivate.",
                    "label": 0
                },
                {
                    "sent": "These are stylized examples motivated by, I think, interesting applications, but still stylized.",
                    "label": 0
                },
                {
                    "sent": "First example is where these signals and S consists of what I call cut matrices or rank one matrices.",
                    "label": 1
                },
                {
                    "sent": "Where the factors consist of just plus minus ones.",
                    "label": 1
                },
                {
                    "sent": "This is of interest in collaborative filtering and as well as in clustering actually.",
                    "label": 0
                },
                {
                    "sent": "And so this is going to be my signal set.",
                    "label": 0
                },
                {
                    "sent": "I have a signal from the set corrupted by noise and I wish to denoise now one point.",
                    "label": 0
                },
                {
                    "sent": "I wish to make here is that I'm going to view the signal set as belonging or living in RP.",
                    "label": 0
                },
                {
                    "sent": "And so the fact array is going to sort of be of dimension square root P. So you shouldn't worry about what happens inside.",
                    "label": 0
                },
                {
                    "sent": "You should just always think of these signals set as living in RP, even though it's so this is a signal set of matrices that will live in P dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The best possible convex relaxation you could write down is the convex Hull of S. That's known as the cut polytope and combinatorial optimization in computer science.",
                    "label": 0
                },
                {
                    "sent": "It's an intractable polytope to be able to project onto.",
                    "label": 0
                },
                {
                    "sent": "If you could project onto attractively, you'd be able to solve Max cut tractably.",
                    "label": 0
                },
                {
                    "sent": "And this polytope requires runtime.",
                    "label": 0
                },
                {
                    "sent": "That's not much, but if in fact you could project onto it, then the number of samples that you need to be able to get to a risk of one is some constant Times Square root P. A weaker set to project onto.",
                    "label": 0
                },
                {
                    "sent": "There's something called the laptop.",
                    "label": 0
                },
                {
                    "sent": "This is the standard.",
                    "label": 0
                },
                {
                    "sent": "When I say these standard, the Canonical relaxation of the cut polytope that folks in CS theory user and combinatorially optimization use based on semi definite programming.",
                    "label": 0
                },
                {
                    "sent": "This requires number of samples to get a risk of one of some other constant C2 that's larger than C1 Times Square root P, But the runtime goes from something that super polynomial to Peter the 2.25.",
                    "label": 0
                },
                {
                    "sent": "The reason I have strange looking exponents here is because I'm viewing S is living in P dimensional space rather than, you know.",
                    "label": 0
                },
                {
                    "sent": "K by K Dimension space is something like this space of matrices.",
                    "label": 0
                },
                {
                    "sent": "So that's the reason for the strange exponent.",
                    "label": 0
                },
                {
                    "sent": "You could also then observe that the elements of S are in fact rank one matrices, so you could adopt the philosophy that a lot of folks in machine learning have adopted that.",
                    "label": 0
                },
                {
                    "sent": "Maybe I could use the nuclear norm as a relaxation of this, and if you do using nuclear norm, you can use SVD based ideas and projections and requires Peter the 1.5 operations and will require a larger constant C three Times Square root P operations to be able to project.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example is based on sort of banding estimators.",
                    "label": 1
                },
                {
                    "sent": "This sort of shows up more in in covariance estimation in statistics.",
                    "label": 0
                },
                {
                    "sent": "And a lot of a lot of interesting ideas for estimating very high dimensional covariance matrices are based on the idea that banding a covariance matrix is a good thing to do in the high dimension setting.",
                    "label": 0
                },
                {
                    "sent": "By banding I mean thresholding entries of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "There are certain distance away from the diagonal, but this sort of method only makes sense if you have an ordering for your variables to begin with, right?",
                    "label": 0
                },
                {
                    "sent": "This is frequently the case in many applications, but if you don't have an ordering for your variables.",
                    "label": 0
                },
                {
                    "sent": "What do you do?",
                    "label": 0
                },
                {
                    "sent": "You'd like to then jointly estimate an ordering.",
                    "label": 0
                },
                {
                    "sent": "Getting matrix that's as nicely bendable as possible and then bandit, right?",
                    "label": 0
                },
                {
                    "sent": "And so based on this sort of idea, a stylized problem we can consider is let MBA known tridiagonal matrix and think of them as being a covariance matrix and my signal set S is going to be the set of all possible Pie Empire transposes where pisy permutation matrix sort of all possible shufflings of the rows and columns event.",
                    "label": 0
                },
                {
                    "sent": "M itself is a known tridiagonal matrix, but the signal set consists in order.",
                    "label": 1
                },
                {
                    "sent": "This permutation is unknown to me and so I wish to be able to estimate somehow, jointly both the ordering as well as the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "In this case, if I compute the convex Hull events, this is also intractable to project onto, it corresponds to the longest path problem in computer science.",
                    "label": 0
                },
                {
                    "sent": "And you know, to get to a risk of one, that's how many samples you need.",
                    "label": 0
                },
                {
                    "sent": "You then make the observation well if M is tridiagonal, it's sparse, so I could use the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "Projecting onto the L1 norm is trivial and you get just a constant factor.",
                    "label": 0
                },
                {
                    "sent": "Larger number of samples required for projection.",
                    "label": 0
                },
                {
                    "sent": "But with a much smaller runtime.",
                    "label": 0
                },
                {
                    "sent": "A third example.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is where the signal set S consists of the set of all perfect matchings in a complete graph.",
                    "label": 1
                },
                {
                    "sent": "Matchings are subsets of the edges where every edge is incident to exactly 1 node.",
                    "label": 0
                },
                {
                    "sent": "At least that's what perfect matchings are.",
                    "label": 0
                },
                {
                    "sent": "And every node only participates in one edge.",
                    "label": 0
                },
                {
                    "sent": "And these sorts of problems show up in network inference where you have some unknown matching and you wish to estimate it given some corrupted observations.",
                    "label": 0
                },
                {
                    "sent": "In this case, in fact, in contrast to the previous two cases, the convex Hull of is the best possible convex set that you could write down.",
                    "label": 0
                },
                {
                    "sent": "Is in fact polynomial time projectable so it requires P to the five amount of time.",
                    "label": 0
                },
                {
                    "sent": "It's based on Edmonds blossom algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you couple that with the ellipsoid method to be able to get to this.",
                    "label": 0
                },
                {
                    "sent": "Requires that run those many samples to get to a risk of 1.",
                    "label": 0
                },
                {
                    "sent": "You then make the observation that the signal said yes.",
                    "label": 0
                },
                {
                    "sent": "The elements of these signals, set S, consists of matrices that are zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "But the property of each such matrix in the set is that they contain exactly the same number of ones in exactly the same number of zeros.",
                    "label": 0
                },
                {
                    "sent": "So you could take the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "Now is the set of matrices that have that number of ones in that number of zeros.",
                    "label": 0
                },
                {
                    "sent": "This is what's called the hyper simplex and polyhedral theory, or in combinatorics or in convex geometry, much, much easier to project onto and requires again just a constant factor more samples.",
                    "label": 0
                },
                {
                    "sent": "The point I wanted to make in showing this example is that it's of interest not just to go from sort of exponential super polynomial runtime down to something simpler.",
                    "label": 0
                },
                {
                    "sent": "It is of interest to go from some large polynomial to some small.",
                    "label": 0
                },
                {
                    "sent": "Small polynomial too.",
                    "label": 0
                },
                {
                    "sent": "Relation within the constant and we just see this inequality.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll I'll show one example where it's more than just a constant factor too.",
                    "label": 0
                },
                {
                    "sent": "But in general all I'm able to say here is that these constants are larger.",
                    "label": 0
                },
                {
                    "sent": "I have some estimates for these constants in specific cases, but I don't have sort of a generic way of producing estimates other than to say that it is monotonically increasing and there is a sharp difference between the two.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a fourth example, very much motivated by this, this point is going to be an example where there will be more than just a constant factor difference.",
                    "label": 0
                },
                {
                    "sent": "In the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "And this is an example where the set S consists of adjacency matrices corresponding to planted cliques.",
                    "label": 1
                },
                {
                    "sent": "In my in my graph.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be an adjacency matrix of a click on square root of the nodes of the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "The reason I've picked square root is because that's of interest in the planted Cleek problem, and many other settings, but that's going to be the signal set S. This is of interest in the sparse PCA problem and gene expression patterns.",
                    "label": 0
                },
                {
                    "sent": "Network inference shows up in many interesting settings.",
                    "label": 0
                },
                {
                    "sent": "In this case, computing the convex Hull of S is again intractable, but requires.",
                    "label": 0
                },
                {
                    "sent": "Number of samples to produce a risk of one that's on the order of 4th root of P times log P. You make the observation that adjacency matrices of cliques.",
                    "label": 0
                },
                {
                    "sent": "If you put ones on the diagonal, suitably rank one matrices you again using nuclear norm ball as a relaxation, much easier to project onto.",
                    "label": 0
                },
                {
                    "sent": "But this requires qualitatively more more than just a constant factor number of cents.",
                    "label": 0
                },
                {
                    "sent": "OK you can.",
                    "label": 0
                },
                {
                    "sent": "I mean I've tried playing around with relaxations in between this and this that require a much larger runtime, but I haven't been able to move too much away from square root P in terms of in terms of number of samples.",
                    "label": 0
                },
                {
                    "sent": "So this is an example where it does seem like there is a qualitative gap beyond just beyond just a a constant factor.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing I want to point out is just with this example.",
                    "label": 0
                },
                {
                    "sent": "It's a very very simple point, but I think it points to something something deeper is what if we use an even weaker relaxation of using convex Hull of S?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is the convex Hull of all these sorts of adjacency matrices of cliques.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use a nuclear numble.",
                    "label": 0
                },
                {
                    "sent": "What if I use an even weaker relaxation to project onto?",
                    "label": 1
                },
                {
                    "sent": "I can use the Euclidean ball.",
                    "label": 1
                },
                {
                    "sent": "It properly scaled Euclidean ball to project onto.",
                    "label": 0
                },
                {
                    "sent": "That's even more trivial to project onto the nuclear norm, alright?",
                    "label": 0
                },
                {
                    "sent": "Why did I not list that as a third as the third algorithm?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So if in fact used.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Euclidean ball.",
                    "label": 0
                },
                {
                    "sent": "The number of samples that I need to be able to produce a risk one estimate is order P, so it goes from four through the P log P sqrt P to something that's in the order of P. If I have order P samples.",
                    "label": 0
                },
                {
                    "sent": "What's the runtime?",
                    "label": 0
                },
                {
                    "sent": "The runtime is N * P?",
                    "label": 0
                },
                {
                    "sent": "For the sample mean plus just order P operations to project onto the Euclidean norm Ball Euclidean unit Euclidean number.",
                    "label": 0
                },
                {
                    "sent": "Or some scale version of it, but the runtime depends on the number of data points, and if N is on the order of P, you'll get a total runtime of order P squared.",
                    "label": 0
                },
                {
                    "sent": "So you're using such a weak algorithm.",
                    "label": 0
                },
                {
                    "sent": "That the number of data points you need to be able to produce a reasonable estimate is so large that even preprocessing such a large data set requires an enormous run.",
                    "label": 0
                },
                {
                    "sent": "And so this sort of in this case it actually makes sense to throw away data, right?",
                    "label": 1
                },
                {
                    "sent": "If in fact I give you order P samples and.",
                    "label": 0
                },
                {
                    "sent": "This is a very not very careful statement.",
                    "label": 0
                },
                {
                    "sent": "If you have order P samples and your choices between nuclear norm projection and Euclidean norm projection, just these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then it actually makes sense to throw away all the data beyond square root P samples and actually just use nuclear norm projections.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the sort of points of goes back to the this idea that we had in the plot before that at some point it makes sense to throw away data that point very much depends on the space of algorithms you're willing to consider.",
                    "label": 1
                },
                {
                    "sent": "I've only talked about very specific convex optimization based process.",
                    "label": 0
                },
                {
                    "sent": "But if this is the space of procedures you restrict yourself to.",
                    "label": 0
                },
                {
                    "sent": "Then, beyond square root P samples it does make sense to throw away extra data.",
                    "label": 0
                },
                {
                    "sent": "That's again the reason why I'm only going to talk about runtime upper bounds, so I mean, when I you know anytime one talks about tradeoffs it.",
                    "label": 0
                },
                {
                    "sent": "It's sort of tempting to sort of talk.",
                    "label": 0
                },
                {
                    "sent": "I think of tradeoffs in physics.",
                    "label": 0
                },
                {
                    "sent": "For instance, it's always you know something like the uncertainty principle.",
                    "label": 0
                },
                {
                    "sent": "Write something times something bigger than or equal to something.",
                    "label": 0
                },
                {
                    "sent": "I'm not making statements like that, and I'm not going to be able to.",
                    "label": 0
                },
                {
                    "sent": "I think given I think the current state of the art in complexity theory, right?",
                    "label": 0
                },
                {
                    "sent": "So I know that the worst case runtime of nuclear non projection is Peter the 1.5.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know that the worst case runtime of Euclidean norm projection is order P, and that's what I'm going to stick to.",
                    "label": 0
                },
                {
                    "sent": "So yes, that is a limitation and sort of any sort of attempt to study time data tradeoffs very much.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there so I think interesting questions that arise with these with these examples, and many of these examples, you don't need too many extra data samples or just a constant factor more data samples to be able to use really simple algorithms.",
                    "label": 1
                },
                {
                    "sent": "And one thing I wish to point out this and this is a point that sort of interfaces with more traditional ideas and discrete optimization.",
                    "label": 0
                },
                {
                    "sent": "Is that some of these relaxations that I used a really terrible from an approximation ratio point of view?",
                    "label": 1
                },
                {
                    "sent": "And approximation ratio is sort of the.",
                    "label": 0
                },
                {
                    "sent": "The main thing you wish to measure if you're using these convex relaxations to solve discrete optimization problems if the underlying problem is somehow discrete.",
                    "label": 1
                },
                {
                    "sent": "But these are these approximation ratios being bad somehow doesn't seem to matter as much if my objective is statistical inference or parameter estimation in the sense that I've described.",
                    "label": 0
                },
                {
                    "sent": "And so it's sort of a very active area of research in theoretical computer science.",
                    "label": 0
                },
                {
                    "sent": "Over the last few years have been to understand these approximation ratios of these hierarchies of linear programming.",
                    "label": 0
                },
                {
                    "sent": "The semantic programming relaxations as a way to obtain approximation algorithms for hard problems in computer science.",
                    "label": 0
                },
                {
                    "sent": "And I think it's of interest to understand not approximation ratios of these hierarchies, but rather Gaussian complexities or Gaussian squared complexities in the sense that I've described, because that's what's relevant for statistical inference, and in many cases these complexities are much nicely behaved relative to approximation ratios.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nicely behaved relative to approximation ratios.",
                    "label": 0
                },
                {
                    "sent": "And this makes exactly the same point, but more graphically.",
                    "label": 0
                },
                {
                    "sent": "So think of having two convex sets.",
                    "label": 0
                },
                {
                    "sent": "You know the great thing, and then another approximation to it.",
                    "label": 0
                },
                {
                    "sent": "Using this using this red lines.",
                    "label": 0
                },
                {
                    "sent": "In computer science, when I say approximation theory, what they're really interested in the sort of measuring a few, will.",
                    "label": 0
                },
                {
                    "sent": "Somehow the gap, roughly speaking between that and that, the worst case, such gap in all directions roughly speak what we care about.",
                    "label": 0
                },
                {
                    "sent": "If you care about statistical inference of parameter estimation, is the Gaussian complexity of the gas and squared complexity of these tangent counts, or these cones are feasible directions?",
                    "label": 0
                },
                {
                    "sent": "How does how does that sort of scale as you use weaker and weaker relaxations?",
                    "label": 0
                },
                {
                    "sent": "And these can be very, very differently behaved.",
                    "label": 0
                },
                {
                    "sent": "Like I said in some of these examples, approximation ratios are terrible.",
                    "label": 0
                },
                {
                    "sent": "But measured from the viewpoint of Gaussian complexity, you just need a constant factor.",
                    "label": 0
                },
                {
                    "sent": "So the difference in Gaussian complexity ratio is just a constant factor.",
                    "label": 1
                },
                {
                    "sent": "The difference in approximation ratio is something that depends on problem dimension.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to summarize, I talked about some of these challenges with massive datasets with very large datasets.",
                    "label": 1
                },
                {
                    "sent": "And I talked about a very simple denoising problem in the high dimensional setting where one can talk about interesting time data tradeoffs via convex relaxation.",
                    "label": 1
                },
                {
                    "sent": "I really used convex relaxation as an algorithm weakening mechanism.",
                    "label": 0
                },
                {
                    "sent": "As you get larger and larger datasets.",
                    "label": 1
                },
                {
                    "sent": "I think there are many interesting.",
                    "label": 0
                },
                {
                    "sent": "Potential avenues for future work.",
                    "label": 0
                },
                {
                    "sent": "Potentially other methods to weaken algorithms and machine learning.",
                    "label": 1
                },
                {
                    "sent": "We can think of sort of dimensionality reduction as an algorithm weakening mechanism as I get more and more data project on the lower and lower dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "Perhaps clustering as an algorithm weakening mechanism.",
                    "label": 0
                },
                {
                    "sent": "There are very interesting research ideas and computer hardware actually, where there's this idea somehow that if I want to do arithmetic operations, so I want to add 100 and 100.",
                    "label": 0
                },
                {
                    "sent": "You know the answer is 200, but if I'm willing to tolerate an error of say, 1%.",
                    "label": 0
                },
                {
                    "sent": "So I'm happy for some answer between 198 and 202.",
                    "label": 0
                },
                {
                    "sent": "Then I'm able to do these things.",
                    "label": 0
                },
                {
                    "sent": "Much faster and with a much less power consumption in hardware.",
                    "label": 0
                },
                {
                    "sent": "So far allow myself these small arithmetic and accuracies.",
                    "label": 0
                },
                {
                    "sent": "I'm able to get away with much lower power consumption and much, much faster computations in hardware, and I think these sorts of ideas are also potentially relevant as algorithm weakening mechanisms and obviously also getting to more complex statistical inference problems beyond the denoising one that I've considered.",
                    "label": 0
                },
                {
                    "sent": "We have a paper up on archive and it's also on my website and as well as in Mike Jordan's website.",
                    "label": 0
                },
                {
                    "sent": "It's been up for about a few weeks or a month or so.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So in the case when NPR, both very very large, in the case where it's not even possible to store all of the data at one site in particular, sent like the distributed computing setting where something like computing the statistical estimation procedure sampling is extraordinarily easy to parallelize, might it not be simpler to use some of these simple projection methods like examples of P squared?",
                    "label": 0
                },
                {
                    "sent": "My practically achievable yeah case that doing spherical projection is very, very easy because it's more amenable to distributed computation.",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "No, but I mean the sense in which I considered computational resource with just a single computer number of operations.",
                    "label": 0
                },
                {
                    "sent": "But if you bring in other dimensions to how computational complexity plays out based on whatever other constraints when we have in a problem like you know, not being able to store the data in anyone location.",
                    "label": 0
                },
                {
                    "sent": "Certainly number of processors, number of locations, all these sorts of ideas come in.",
                    "label": 0
                },
                {
                    "sent": "I haven't considered these, but I think that's very interesting and very relevant.",
                    "label": 0
                },
                {
                    "sent": "Absolutely, and in those cases some of these.",
                    "label": 0
                },
                {
                    "sent": "You know the way you'd measure runtime should be should also take into account how amenable it is to your particular computational infrastructure, absolutely.",
                    "label": 0
                },
                {
                    "sent": "Quick question.",
                    "label": 0
                },
                {
                    "sent": "What about step up?",
                    "label": 0
                },
                {
                    "sent": "Going back to your non convex sets and then also depend on how you relax the set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's a good question I've.",
                    "label": 0
                },
                {
                    "sent": "Sort of, I'm not able to say something in general there.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you said S is somehow some set of sparse vectors, that's the case where we understand how we can go back from these solutions to relax problems to somehow that original set is in fact, there's a great literature saying that even if you use these convex relaxations, you will recover vectors that are on facts bars, and you can say the same thing with the trace norm with low rank matrices and so on.",
                    "label": 0
                },
                {
                    "sent": "But that's sort of very specialized case by case analysis, and at least I don't know if a way to say something more general than that sort of uniformly for all sets S and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I think there will be more time for discussions in the break and let's take this.",
                    "label": 0
                }
            ]
        }
    }
}