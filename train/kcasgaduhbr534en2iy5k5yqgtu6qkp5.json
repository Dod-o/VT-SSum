{
    "id": "kcasgaduhbr534en2iy5k5yqgtu6qkp5",
    "title": "TSE-NER: An Iterative Approach for Long-Tail Entity Extraction in Scientific Publications",
    "info": {
        "author": [
            "Christoph Lofi, Delft University of Technology (TU Delft)"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_lofi_tse_ner_iterative/",
    "segmentation": [
        [
            "Well, welcome everybody.",
            "My name is Krista, Fluffy from Tia Delft in the Netherlands.",
            "So unfortunately I'm only the third or so of that paper because my Iranian first also and Mexican second also had like a really harsh visa problem, so yeah, must be me.",
            "Who's giving that talk and I'm going well basically what I'm Irish."
        ],
        [
            "I'd really like to do a lot, as is dealing with metadata and basically seeing what people would like to do with certain items.",
            "What metadata do we have around and where the gap between what people like to do and what we can do with the data available?",
            "Now if you look at digital libraries, especially scientific digital libraries, they have a very very strong tradition and having very well curated metadata.",
            "Unfortunately, a lot of that metadata is.",
            "Let's call it very factual.",
            "If you have like metadata like titles, authors, the year, the conference venue, maybe citations, maybe it's classified into some kind of taxonomy or ontology.",
            "Very rarely have deep metadata.",
            "So basically meta data describing what the actual content is about or what things they discussed here, what we try to do for this paper is is looking at named entity recognition and named entity linking, but most is a recognition part.",
            "In order to recognize entities in scientific publications.",
            "Now the problem is, or the goal is a long term goal."
        ],
        [
            "That with that is to have some queries which we can later on do something like which methods are commonly applied to the movielens data set.",
            "You might know that thing and you do collaborative filtering and recommendations or which treatments have been researched to treat Alzheimer's if your medicine.",
            "Order to do these things, you would need to know for each paper which treatments it does contain or which datasets it would need to contain.",
            "Now the problem is if you want to extract these things using and or."
        ],
        [
            "Named entity recognizer technique.",
            "You can quickly run into problem because a lot of these entity types are super rare, so I like to call these things long tail entities, so most frame works you might encounter so far."
        ],
        [
            "Example, but a lot of people like to do is just using out of the box named entity recognizers.",
            "So something like the Stanford named entity recognizer, which is totally awesome.",
            "You can just download that thing, put it on your document and it works its magic and it's very good at recognizing stuff like people, locations, maybe companies or something like that.",
            "And it basically is similar thing applies to most of these out of the boxing is mightytext tracer or even IBM Watson and all these services they provide.",
            "But if you go to a very specific domain and say like hey, I am in the semantic web domain and I'm super interested in recognizing let's say ontologies instead of people or places, this will be very very hard.",
            "So basically what you can do then is out of the box.",
            "Any art doesn't really help you that much.",
            "What you can do now is you can train your own air classifier, which is also super easy.",
            "You can just download the algorithms for that.",
            "So for example stand for any error has very nice toolbox of algorithms.",
            "Which you don't have to use out of the box, but you can train it yourself.",
            "And then do that.",
            "Now bunch of domains actually did that.",
            "So for example in bio medicine and biology chemistry, alot of people in the digital library domain spend a lot of time, effort and especially money on training these classifiers, building ontologies, building repository's of extracted entities because, well, there's a lot of money to be made in a lot of money involved in these domains.",
            "Now if we want to do that, let's say in computer science, or even worse, if you go to.",
            "Digital humanities or people who notoriously don't have money.",
            "Well, this is a little bit harder, So what we try to do with our paper here is."
        ],
        [
            "Is looking at the problem of how we can train our own named entity classifiers for Super Rare entity types with very minimal human supervision and superhumans provision.",
            "Here translates into, well, time and money spent.",
            "So basically, how can you train in any are for super cheap?",
            "This is not necessarily a paper about how to build, you know, like an intellige or taxonomy or how to build a better any our algorithms.",
            "I mean, that's something people in natural language processing would do.",
            "So our paper here is about a process of training any kind of named entity recognition algorithms there might be."
        ],
        [
            "And we call that thing GSR.",
            "So we will later see that this becomes relevant.",
            "So we gave our algorithm and name.",
            "And what we want to do is we want to have an iterative training process where we only provide a small set of heat examples.",
            "So for example, in the domain of computer science, I might argue important entity types are datasets, algorithms, methods, maybe software packages, and for each of these I just provide like 5 examples, 10 examples, 50 examples, something like that, and another domain like biologic biology or chemistry.",
            "I might talk about.",
            "I don't know jeans or whatever is important to these people, right?",
            "And then have an iterative approach training a traditional any are."
        ],
        [
            "And this kind of looks like that.",
            "So we start with a C term list.",
            "There's a saying it's really nasty as light.",
            "That is a manually provided list that some kind of domain expert provided.",
            "Or maybe we just got it from some kind of knowledge base.",
            "And then we take our whole document corpus and try to extract sentences where these known instances of an entity type are contained.",
            "Now after we extract example sentences with these entity instances in it, then the juristically expand.",
            "Either the sentences I do get to that later again, or I can expand the actual seat set using some kind of expansion touristics and then create a new training data set.",
            "So basically what I'm doing next as search matches.",
            "I'm annotating using these expanded either seed sets or sentence sets.",
            "I'm just annotating a well, very, very standard corpus and then just training name.",
            "Entity classifier as I would do if I do it for the manual.",
            "So basically just take like in our case it will stand for any R and just let it rain on our annotated training data set.",
            "Now after I trained an entity classifier, I can apply to the whole corpus to extract all the entities of the types and interested in which creates a rather huge list of potentially new instances of that type.",
            "Now the problem is typically when you train.",
            "And then entity classifier in a more traditional way.",
            "So you basically just take a text and manually go through it and annotate each single entity like this has this type.",
            "This has another type.",
            "This is Type 3 and so on and so on.",
            "Then this would work very, very well.",
            "Now we don't do that.",
            "What we did is we just started with the list of examples and then used heuristics to annotate the corpus.",
            "Because of that we created a lot of noise, so a lot of these recognized entity instances which they end up here.",
            "Are well most likely false positives.",
            "Or maybe we miss a lot of them.",
            "So what we do next is the filter.",
            "These lists down in order to exclude or recognized named entities which shouldn't be there, so we have a bunch of heuristics and saying like, OK, this definitely cannot be a named entity of this type because it looks off and if you remove those, whatever remains then is a new list of entities which we can treat as a new seat term list and just start the whole process.",
            "Again, so basically it's just a process of having a list of examples like sponding it using different heuristics, annotating a corpus training, and normal any R, whatever it might be, extracting entities using that, any R and then filtering down the results and doing it again and again and again.",
            "Good now, does that work well, so unfortunately I removed the slightest, explained a little bit further.",
            "Sorry for that.",
            "So what are these expansion juristic San filter heuristics?",
            "Well for the expansion heuristics, we basically have two.",
            "One is expanding and the instance examples and the other one is expanding.",
            "The sentence is so for example if I provide as a data set name like.",
            "I don't know DB pedia, right?",
            "What I could do now is I could argue that everything which is semantically similar to DB Pedia is probably also a data set.",
            "And how can I capture semantically similar now?",
            "For example, I could use a vert embedding like vert avec or something like that and could argue OK. What is similar in route avec is likely also similar to the data set type, so I can just use vert avec in order to find new datasets.",
            "This is very very 40, of course it it really introduces a lot of noise.",
            "This is right later on we have to filter face to remove that noise again.",
            "Now the second thing I could do for expansion is finding more sentences which might also be negative examples.",
            "So I could argue if I have a sentence where I talk about the data set, for example sentence saying like oh DB pedia is avail known data set.",
            "This following properties I could find another sentence which is similar like blah blah data set is a very well known data set with similar properties.",
            "I could also include sentences like selves.",
            "And most likely the sentences also contain datasets, but maybe they don't.",
            "So depending on what it is, it could serve as a negative example.",
            "So in the end it just increases the size of the training corpus.",
            "Now for the filtering heuristics to get rid of words it shouldn't be there.",
            "We had a set of four heuristics.",
            "One of them is the cause we interested in super long term entities.",
            "So one was arguing, and that's rather silly one.",
            "Actually, if you can find it on Wikipedia, then it's probably not the thing we're looking for because we're in computer science and everybody, including myself, invents their own tiny little algorithm.",
            "So if it ends up in Wikipedia, well, then it's wrong one here I mean, of course, it only works in this particular domain.",
            "It probably wouldn't work for jeans or something like that.",
            "So this you risztics are little bit adaptable.",
            "So another heuristic which worked indeed rather well is there be handcrafted a small set of rules and saying like OK when people talk about data set, it typically looks like that and then measured the sentence distance to that using PMI.",
            "So that worked very well.",
            "However, it also forced us to craft rules which was not very nice.",
            "Heuristic, which also worked somewhat well, is arguing that we clustered all these entities in the end, also using their vert avec similarity and something which is too far off of a cluster of known instances.",
            "That is something we exclude and this heuristics.",
            "I mean, that's something you can play around with.",
            "But we evaluated four of them and."
        ],
        [
            "Results are Kenzie here, so how did the."
        ],
        [
            "Evaluate that, so we took 16,000 papers which we crawled from various digital libraries and most of them is somehow related to data, so it was like papers here from my SWC or from WDP, so something talking about data and algorithms and we only focused on two entity types which is data set and message and we tried to annotate this data set for training, especially evaluation, which was a very unfun experience because we forced our.",
            "Well, postdocs and PhD students, and unfortunately also assistant professors to sit down for long time and annotate sentences.",
            "Also turns out that this is a very hard thing to do because we had a lot of disagreement.",
            "For example, if something like B tree is that an algorithm and somebody argue it's a data structure, it's not an algorithm, so we didn't really always have an agreement here.",
            "I think our Inter annotator agreement with something like 0 to fix or something like that, which is showing how tricky that is.",
            "So all these numbers need to be taken with some grain of salt and also.",
            "People have problems."
        ],
        [
            "Bing.",
            "Now yes, wonderful tables which you can probably not."
        ],
        [
            "See that I have a larger version here.",
            "Then we can see you a little bit of results here.",
            "I don't have alot of time, but what we can see is if you don't use any expansion strategies that is this one here.",
            "No expansion strategies and basically just use the named entity classifiers as is out of the box.",
            "We get somehow decent precision, so something like 8483% or 0.84.",
            "Very little recall, which is not surprising because we just trained that thing.",
            "This is Super Tiny set of examples.",
            "And therefore also somewhat small esscor here.",
            "However, as soon as I start using the expansion strategies, which are very heuristic in their nature.",
            "The currency or as precision decreases it a little bit because these things basically just guess what might be more entities of that type, but the recall increases quite tremendously as we can see here that the recall goes from something like 0.5.",
            "Right, yes, maybe 0.15 to 30, so it more than doubles or more than like 2 1/2 * 3 times of that just by having more expansion strategies.",
            "What you can do here is a little bit by choosing which filter strategies in which expands the strategies you want to use.",
            "You can fine tune the type of results you want, and we can trade precision for recall or something like that.",
            "Also, here you have like lines for five 2500, that is a number of seed terms we are using here now.",
            "So for example, here's a 25 languages 25 examples of datasets, 25 examples methods.",
            "That's not a lot, it's really a tiny list, it takes like.",
            "Half an hour to compile that list.",
            "Still we get a precision of like 80% here when we use the sentence expansion.",
            "Heuristic, so it's not true that.",
            "So if you look at these results and compare them, maybe if you work yourself on named entity recognition, you might say like, yeah, well, these numbers are not that super awesome.",
            "And I agree to that, but this was super cheap to create right?",
            "So the manual work in order to create that beyond implementing and writing the paper is basically like I don't know an hour and all the other effort we put in there was for doing this evaluation, not for trainings in any R. And that is basically big takeaway may."
        ],
        [
            "So you can train and named entity recognizer for very cheap if you use this technique, it might not be perfect.",
            "It might have flaws, but it won't cost you a lot."
        ],
        [
            "So here I show a little bit how is it different number of iterations might increase the performance, so you don't really need that many iterations.",
            "So with our heuristics we're using, it kinda stops giving additional improvement."
        ],
        [
            "After his three iterations.",
            "So this might look different with different heuristics.",
            "So for the sake."
        ],
        [
            "Time, let's just go a little bit to the results."
        ],
        [
            "Yeah, you also did a small test in the biomedical domain which showed more or less comperable numbers.",
            "This was very easy to do, so we just took an existing corpus.",
            "We extracted a bunch of protein names from Wikipedia, fed it into the system, and very, very quickly we had an end entity recognizer which missed a lot of things but was very cheap to train."
        ],
        [
            "So much cheaper than something else now.",
            "Let's go to one of the later things.",
            "We are currently doing is involving humans or the actual users of a digital library, and the whole process and arguing.",
            "While if you're something like IEEE Digital library or something like that, people might look at your documents online.",
            "And how about we just ask?"
        ],
        [
            "Then why they're reading in the PDF like oh?",
            "Is this entity correctly classified?",
            "Yes or no.",
            "So basically just use their constant feedback to tune the whole thing.",
            "And if you do that right, it does not annoy people a lot.",
            "Ending increases the quality by quite a bit."
        ],
        [
            "Good then of course.",
            "And yeah, you can have wonderful."
        ],
        [
            "This realization of that whole stuff I just have this image because our media department's very happy when we do this.",
            "Images is totally meaningless, but it basically shows that you can take these entities, cluster them, color them, and then you can maybe see something that's also something you can do with this approach.",
            "You can impress people with colorful graphics."
        ],
        [
            "So conclusions what I propose here is an iterative training for named entity recognizer using expansion heuristics and filtering realistics in order to decrease the costs which canvas training any our classifiers little bit, sacrificing performance but not too much.",
            "So in the end I think it's a rather good trade off of performance and costs, which should make people in less popular domains quite happy.",
            "So thanks."
        ],
        [
            "That and that.",
            "Interest.",
            "Spent three years.",
            "Building any armor.",
            "Search engine queries and come talking about where is the lowest garbled syntax.",
            "Very short.",
            "And it didn't quite work, and you know my question for you was.",
            "How many years did you have or like that?",
            "Lower case stuff you know no syntactic or whatsoever.",
            "Those actually are the natural queries you get from production quicker.",
            "Case.",
            "Glad to be fair.",
            "We didn't really have a query corpora.",
            "We had a document corpora, so we just trained for any ours.",
            "We did not have a user queries with all that, well 40.",
            "Not yet, no.",
            "As of right now, it's like we have documents.",
            "Let's find all the entities mentioned in these documents, which have a particular type.",
            "So working with queries is probably the next step.",
            "I would venture to say.",
            "Yeah.",
            "Yeah.",
            "So how I envisioned this thing to be used is mostly for navigational interface, where we extract the named entities from the documents and then allow people like a navigational access to it and saying like, OK, you have a paper here which uses a certain data set.",
            "How about this paper here which uses the same data set or more papers which have the same algorithm and you just can navigate there, because I personally would avoid exactly the problem you just mentioned that people are totally unable to have queries which are proper.",
            "Well formed.",
            "And mostly work on documents which are.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, welcome everybody.",
                    "label": 0
                },
                {
                    "sent": "My name is Krista, Fluffy from Tia Delft in the Netherlands.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately I'm only the third or so of that paper because my Iranian first also and Mexican second also had like a really harsh visa problem, so yeah, must be me.",
                    "label": 0
                },
                {
                    "sent": "Who's giving that talk and I'm going well basically what I'm Irish.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd really like to do a lot, as is dealing with metadata and basically seeing what people would like to do with certain items.",
                    "label": 0
                },
                {
                    "sent": "What metadata do we have around and where the gap between what people like to do and what we can do with the data available?",
                    "label": 0
                },
                {
                    "sent": "Now if you look at digital libraries, especially scientific digital libraries, they have a very very strong tradition and having very well curated metadata.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, a lot of that metadata is.",
                    "label": 0
                },
                {
                    "sent": "Let's call it very factual.",
                    "label": 0
                },
                {
                    "sent": "If you have like metadata like titles, authors, the year, the conference venue, maybe citations, maybe it's classified into some kind of taxonomy or ontology.",
                    "label": 0
                },
                {
                    "sent": "Very rarely have deep metadata.",
                    "label": 0
                },
                {
                    "sent": "So basically meta data describing what the actual content is about or what things they discussed here, what we try to do for this paper is is looking at named entity recognition and named entity linking, but most is a recognition part.",
                    "label": 0
                },
                {
                    "sent": "In order to recognize entities in scientific publications.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is, or the goal is a long term goal.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That with that is to have some queries which we can later on do something like which methods are commonly applied to the movielens data set.",
                    "label": 0
                },
                {
                    "sent": "You might know that thing and you do collaborative filtering and recommendations or which treatments have been researched to treat Alzheimer's if your medicine.",
                    "label": 0
                },
                {
                    "sent": "Order to do these things, you would need to know for each paper which treatments it does contain or which datasets it would need to contain.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is if you want to extract these things using and or.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Named entity recognizer technique.",
                    "label": 0
                },
                {
                    "sent": "You can quickly run into problem because a lot of these entity types are super rare, so I like to call these things long tail entities, so most frame works you might encounter so far.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, but a lot of people like to do is just using out of the box named entity recognizers.",
                    "label": 0
                },
                {
                    "sent": "So something like the Stanford named entity recognizer, which is totally awesome.",
                    "label": 0
                },
                {
                    "sent": "You can just download that thing, put it on your document and it works its magic and it's very good at recognizing stuff like people, locations, maybe companies or something like that.",
                    "label": 0
                },
                {
                    "sent": "And it basically is similar thing applies to most of these out of the boxing is mightytext tracer or even IBM Watson and all these services they provide.",
                    "label": 0
                },
                {
                    "sent": "But if you go to a very specific domain and say like hey, I am in the semantic web domain and I'm super interested in recognizing let's say ontologies instead of people or places, this will be very very hard.",
                    "label": 0
                },
                {
                    "sent": "So basically what you can do then is out of the box.",
                    "label": 0
                },
                {
                    "sent": "Any art doesn't really help you that much.",
                    "label": 0
                },
                {
                    "sent": "What you can do now is you can train your own air classifier, which is also super easy.",
                    "label": 0
                },
                {
                    "sent": "You can just download the algorithms for that.",
                    "label": 0
                },
                {
                    "sent": "So for example stand for any error has very nice toolbox of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Which you don't have to use out of the box, but you can train it yourself.",
                    "label": 0
                },
                {
                    "sent": "And then do that.",
                    "label": 0
                },
                {
                    "sent": "Now bunch of domains actually did that.",
                    "label": 0
                },
                {
                    "sent": "So for example in bio medicine and biology chemistry, alot of people in the digital library domain spend a lot of time, effort and especially money on training these classifiers, building ontologies, building repository's of extracted entities because, well, there's a lot of money to be made in a lot of money involved in these domains.",
                    "label": 0
                },
                {
                    "sent": "Now if we want to do that, let's say in computer science, or even worse, if you go to.",
                    "label": 0
                },
                {
                    "sent": "Digital humanities or people who notoriously don't have money.",
                    "label": 0
                },
                {
                    "sent": "Well, this is a little bit harder, So what we try to do with our paper here is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is looking at the problem of how we can train our own named entity classifiers for Super Rare entity types with very minimal human supervision and superhumans provision.",
                    "label": 0
                },
                {
                    "sent": "Here translates into, well, time and money spent.",
                    "label": 0
                },
                {
                    "sent": "So basically, how can you train in any are for super cheap?",
                    "label": 0
                },
                {
                    "sent": "This is not necessarily a paper about how to build, you know, like an intellige or taxonomy or how to build a better any our algorithms.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's something people in natural language processing would do.",
                    "label": 0
                },
                {
                    "sent": "So our paper here is about a process of training any kind of named entity recognition algorithms there might be.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we call that thing GSR.",
                    "label": 0
                },
                {
                    "sent": "So we will later see that this becomes relevant.",
                    "label": 0
                },
                {
                    "sent": "So we gave our algorithm and name.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is we want to have an iterative training process where we only provide a small set of heat examples.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the domain of computer science, I might argue important entity types are datasets, algorithms, methods, maybe software packages, and for each of these I just provide like 5 examples, 10 examples, 50 examples, something like that, and another domain like biologic biology or chemistry.",
                    "label": 0
                },
                {
                    "sent": "I might talk about.",
                    "label": 0
                },
                {
                    "sent": "I don't know jeans or whatever is important to these people, right?",
                    "label": 0
                },
                {
                    "sent": "And then have an iterative approach training a traditional any are.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this kind of looks like that.",
                    "label": 0
                },
                {
                    "sent": "So we start with a C term list.",
                    "label": 1
                },
                {
                    "sent": "There's a saying it's really nasty as light.",
                    "label": 0
                },
                {
                    "sent": "That is a manually provided list that some kind of domain expert provided.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we just got it from some kind of knowledge base.",
                    "label": 1
                },
                {
                    "sent": "And then we take our whole document corpus and try to extract sentences where these known instances of an entity type are contained.",
                    "label": 0
                },
                {
                    "sent": "Now after we extract example sentences with these entity instances in it, then the juristically expand.",
                    "label": 0
                },
                {
                    "sent": "Either the sentences I do get to that later again, or I can expand the actual seat set using some kind of expansion touristics and then create a new training data set.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm doing next as search matches.",
                    "label": 0
                },
                {
                    "sent": "I'm annotating using these expanded either seed sets or sentence sets.",
                    "label": 0
                },
                {
                    "sent": "I'm just annotating a well, very, very standard corpus and then just training name.",
                    "label": 1
                },
                {
                    "sent": "Entity classifier as I would do if I do it for the manual.",
                    "label": 0
                },
                {
                    "sent": "So basically just take like in our case it will stand for any R and just let it rain on our annotated training data set.",
                    "label": 0
                },
                {
                    "sent": "Now after I trained an entity classifier, I can apply to the whole corpus to extract all the entities of the types and interested in which creates a rather huge list of potentially new instances of that type.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is typically when you train.",
                    "label": 0
                },
                {
                    "sent": "And then entity classifier in a more traditional way.",
                    "label": 0
                },
                {
                    "sent": "So you basically just take a text and manually go through it and annotate each single entity like this has this type.",
                    "label": 0
                },
                {
                    "sent": "This has another type.",
                    "label": 0
                },
                {
                    "sent": "This is Type 3 and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "Then this would work very, very well.",
                    "label": 0
                },
                {
                    "sent": "Now we don't do that.",
                    "label": 0
                },
                {
                    "sent": "What we did is we just started with the list of examples and then used heuristics to annotate the corpus.",
                    "label": 0
                },
                {
                    "sent": "Because of that we created a lot of noise, so a lot of these recognized entity instances which they end up here.",
                    "label": 0
                },
                {
                    "sent": "Are well most likely false positives.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we miss a lot of them.",
                    "label": 0
                },
                {
                    "sent": "So what we do next is the filter.",
                    "label": 0
                },
                {
                    "sent": "These lists down in order to exclude or recognized named entities which shouldn't be there, so we have a bunch of heuristics and saying like, OK, this definitely cannot be a named entity of this type because it looks off and if you remove those, whatever remains then is a new list of entities which we can treat as a new seat term list and just start the whole process.",
                    "label": 0
                },
                {
                    "sent": "Again, so basically it's just a process of having a list of examples like sponding it using different heuristics, annotating a corpus training, and normal any R, whatever it might be, extracting entities using that, any R and then filtering down the results and doing it again and again and again.",
                    "label": 0
                },
                {
                    "sent": "Good now, does that work well, so unfortunately I removed the slightest, explained a little bit further.",
                    "label": 0
                },
                {
                    "sent": "Sorry for that.",
                    "label": 0
                },
                {
                    "sent": "So what are these expansion juristic San filter heuristics?",
                    "label": 0
                },
                {
                    "sent": "Well for the expansion heuristics, we basically have two.",
                    "label": 0
                },
                {
                    "sent": "One is expanding and the instance examples and the other one is expanding.",
                    "label": 0
                },
                {
                    "sent": "The sentence is so for example if I provide as a data set name like.",
                    "label": 0
                },
                {
                    "sent": "I don't know DB pedia, right?",
                    "label": 0
                },
                {
                    "sent": "What I could do now is I could argue that everything which is semantically similar to DB Pedia is probably also a data set.",
                    "label": 0
                },
                {
                    "sent": "And how can I capture semantically similar now?",
                    "label": 0
                },
                {
                    "sent": "For example, I could use a vert embedding like vert avec or something like that and could argue OK. What is similar in route avec is likely also similar to the data set type, so I can just use vert avec in order to find new datasets.",
                    "label": 0
                },
                {
                    "sent": "This is very very 40, of course it it really introduces a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "This is right later on we have to filter face to remove that noise again.",
                    "label": 0
                },
                {
                    "sent": "Now the second thing I could do for expansion is finding more sentences which might also be negative examples.",
                    "label": 0
                },
                {
                    "sent": "So I could argue if I have a sentence where I talk about the data set, for example sentence saying like oh DB pedia is avail known data set.",
                    "label": 0
                },
                {
                    "sent": "This following properties I could find another sentence which is similar like blah blah data set is a very well known data set with similar properties.",
                    "label": 0
                },
                {
                    "sent": "I could also include sentences like selves.",
                    "label": 0
                },
                {
                    "sent": "And most likely the sentences also contain datasets, but maybe they don't.",
                    "label": 0
                },
                {
                    "sent": "So depending on what it is, it could serve as a negative example.",
                    "label": 0
                },
                {
                    "sent": "So in the end it just increases the size of the training corpus.",
                    "label": 0
                },
                {
                    "sent": "Now for the filtering heuristics to get rid of words it shouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "We had a set of four heuristics.",
                    "label": 0
                },
                {
                    "sent": "One of them is the cause we interested in super long term entities.",
                    "label": 0
                },
                {
                    "sent": "So one was arguing, and that's rather silly one.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you can find it on Wikipedia, then it's probably not the thing we're looking for because we're in computer science and everybody, including myself, invents their own tiny little algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if it ends up in Wikipedia, well, then it's wrong one here I mean, of course, it only works in this particular domain.",
                    "label": 0
                },
                {
                    "sent": "It probably wouldn't work for jeans or something like that.",
                    "label": 0
                },
                {
                    "sent": "So this you risztics are little bit adaptable.",
                    "label": 0
                },
                {
                    "sent": "So another heuristic which worked indeed rather well is there be handcrafted a small set of rules and saying like OK when people talk about data set, it typically looks like that and then measured the sentence distance to that using PMI.",
                    "label": 0
                },
                {
                    "sent": "So that worked very well.",
                    "label": 0
                },
                {
                    "sent": "However, it also forced us to craft rules which was not very nice.",
                    "label": 0
                },
                {
                    "sent": "Heuristic, which also worked somewhat well, is arguing that we clustered all these entities in the end, also using their vert avec similarity and something which is too far off of a cluster of known instances.",
                    "label": 0
                },
                {
                    "sent": "That is something we exclude and this heuristics.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's something you can play around with.",
                    "label": 0
                },
                {
                    "sent": "But we evaluated four of them and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results are Kenzie here, so how did the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluate that, so we took 16,000 papers which we crawled from various digital libraries and most of them is somehow related to data, so it was like papers here from my SWC or from WDP, so something talking about data and algorithms and we only focused on two entity types which is data set and message and we tried to annotate this data set for training, especially evaluation, which was a very unfun experience because we forced our.",
                    "label": 0
                },
                {
                    "sent": "Well, postdocs and PhD students, and unfortunately also assistant professors to sit down for long time and annotate sentences.",
                    "label": 0
                },
                {
                    "sent": "Also turns out that this is a very hard thing to do because we had a lot of disagreement.",
                    "label": 0
                },
                {
                    "sent": "For example, if something like B tree is that an algorithm and somebody argue it's a data structure, it's not an algorithm, so we didn't really always have an agreement here.",
                    "label": 0
                },
                {
                    "sent": "I think our Inter annotator agreement with something like 0 to fix or something like that, which is showing how tricky that is.",
                    "label": 0
                },
                {
                    "sent": "So all these numbers need to be taken with some grain of salt and also.",
                    "label": 0
                },
                {
                    "sent": "People have problems.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing.",
                    "label": 0
                },
                {
                    "sent": "Now yes, wonderful tables which you can probably not.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See that I have a larger version here.",
                    "label": 0
                },
                {
                    "sent": "Then we can see you a little bit of results here.",
                    "label": 0
                },
                {
                    "sent": "I don't have alot of time, but what we can see is if you don't use any expansion strategies that is this one here.",
                    "label": 0
                },
                {
                    "sent": "No expansion strategies and basically just use the named entity classifiers as is out of the box.",
                    "label": 0
                },
                {
                    "sent": "We get somehow decent precision, so something like 8483% or 0.84.",
                    "label": 0
                },
                {
                    "sent": "Very little recall, which is not surprising because we just trained that thing.",
                    "label": 0
                },
                {
                    "sent": "This is Super Tiny set of examples.",
                    "label": 0
                },
                {
                    "sent": "And therefore also somewhat small esscor here.",
                    "label": 0
                },
                {
                    "sent": "However, as soon as I start using the expansion strategies, which are very heuristic in their nature.",
                    "label": 0
                },
                {
                    "sent": "The currency or as precision decreases it a little bit because these things basically just guess what might be more entities of that type, but the recall increases quite tremendously as we can see here that the recall goes from something like 0.5.",
                    "label": 0
                },
                {
                    "sent": "Right, yes, maybe 0.15 to 30, so it more than doubles or more than like 2 1/2 * 3 times of that just by having more expansion strategies.",
                    "label": 0
                },
                {
                    "sent": "What you can do here is a little bit by choosing which filter strategies in which expands the strategies you want to use.",
                    "label": 0
                },
                {
                    "sent": "You can fine tune the type of results you want, and we can trade precision for recall or something like that.",
                    "label": 0
                },
                {
                    "sent": "Also, here you have like lines for five 2500, that is a number of seed terms we are using here now.",
                    "label": 0
                },
                {
                    "sent": "So for example, here's a 25 languages 25 examples of datasets, 25 examples methods.",
                    "label": 0
                },
                {
                    "sent": "That's not a lot, it's really a tiny list, it takes like.",
                    "label": 0
                },
                {
                    "sent": "Half an hour to compile that list.",
                    "label": 0
                },
                {
                    "sent": "Still we get a precision of like 80% here when we use the sentence expansion.",
                    "label": 0
                },
                {
                    "sent": "Heuristic, so it's not true that.",
                    "label": 0
                },
                {
                    "sent": "So if you look at these results and compare them, maybe if you work yourself on named entity recognition, you might say like, yeah, well, these numbers are not that super awesome.",
                    "label": 0
                },
                {
                    "sent": "And I agree to that, but this was super cheap to create right?",
                    "label": 0
                },
                {
                    "sent": "So the manual work in order to create that beyond implementing and writing the paper is basically like I don't know an hour and all the other effort we put in there was for doing this evaluation, not for trainings in any R. And that is basically big takeaway may.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can train and named entity recognizer for very cheap if you use this technique, it might not be perfect.",
                    "label": 0
                },
                {
                    "sent": "It might have flaws, but it won't cost you a lot.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I show a little bit how is it different number of iterations might increase the performance, so you don't really need that many iterations.",
                    "label": 0
                },
                {
                    "sent": "So with our heuristics we're using, it kinda stops giving additional improvement.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After his three iterations.",
                    "label": 0
                },
                {
                    "sent": "So this might look different with different heuristics.",
                    "label": 0
                },
                {
                    "sent": "So for the sake.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time, let's just go a little bit to the results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, you also did a small test in the biomedical domain which showed more or less comperable numbers.",
                    "label": 0
                },
                {
                    "sent": "This was very easy to do, so we just took an existing corpus.",
                    "label": 0
                },
                {
                    "sent": "We extracted a bunch of protein names from Wikipedia, fed it into the system, and very, very quickly we had an end entity recognizer which missed a lot of things but was very cheap to train.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So much cheaper than something else now.",
                    "label": 0
                },
                {
                    "sent": "Let's go to one of the later things.",
                    "label": 0
                },
                {
                    "sent": "We are currently doing is involving humans or the actual users of a digital library, and the whole process and arguing.",
                    "label": 0
                },
                {
                    "sent": "While if you're something like IEEE Digital library or something like that, people might look at your documents online.",
                    "label": 0
                },
                {
                    "sent": "And how about we just ask?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then why they're reading in the PDF like oh?",
                    "label": 0
                },
                {
                    "sent": "Is this entity correctly classified?",
                    "label": 0
                },
                {
                    "sent": "Yes or no.",
                    "label": 0
                },
                {
                    "sent": "So basically just use their constant feedback to tune the whole thing.",
                    "label": 0
                },
                {
                    "sent": "And if you do that right, it does not annoy people a lot.",
                    "label": 0
                },
                {
                    "sent": "Ending increases the quality by quite a bit.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good then of course.",
                    "label": 0
                },
                {
                    "sent": "And yeah, you can have wonderful.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This realization of that whole stuff I just have this image because our media department's very happy when we do this.",
                    "label": 0
                },
                {
                    "sent": "Images is totally meaningless, but it basically shows that you can take these entities, cluster them, color them, and then you can maybe see something that's also something you can do with this approach.",
                    "label": 0
                },
                {
                    "sent": "You can impress people with colorful graphics.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So conclusions what I propose here is an iterative training for named entity recognizer using expansion heuristics and filtering realistics in order to decrease the costs which canvas training any our classifiers little bit, sacrificing performance but not too much.",
                    "label": 0
                },
                {
                    "sent": "So in the end I think it's a rather good trade off of performance and costs, which should make people in less popular domains quite happy.",
                    "label": 0
                },
                {
                    "sent": "So thanks.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That and that.",
                    "label": 0
                },
                {
                    "sent": "Interest.",
                    "label": 0
                },
                {
                    "sent": "Spent three years.",
                    "label": 0
                },
                {
                    "sent": "Building any armor.",
                    "label": 0
                },
                {
                    "sent": "Search engine queries and come talking about where is the lowest garbled syntax.",
                    "label": 0
                },
                {
                    "sent": "Very short.",
                    "label": 0
                },
                {
                    "sent": "And it didn't quite work, and you know my question for you was.",
                    "label": 0
                },
                {
                    "sent": "How many years did you have or like that?",
                    "label": 0
                },
                {
                    "sent": "Lower case stuff you know no syntactic or whatsoever.",
                    "label": 0
                },
                {
                    "sent": "Those actually are the natural queries you get from production quicker.",
                    "label": 0
                },
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "Glad to be fair.",
                    "label": 0
                },
                {
                    "sent": "We didn't really have a query corpora.",
                    "label": 0
                },
                {
                    "sent": "We had a document corpora, so we just trained for any ours.",
                    "label": 0
                },
                {
                    "sent": "We did not have a user queries with all that, well 40.",
                    "label": 0
                },
                {
                    "sent": "Not yet, no.",
                    "label": 0
                },
                {
                    "sent": "As of right now, it's like we have documents.",
                    "label": 0
                },
                {
                    "sent": "Let's find all the entities mentioned in these documents, which have a particular type.",
                    "label": 0
                },
                {
                    "sent": "So working with queries is probably the next step.",
                    "label": 0
                },
                {
                    "sent": "I would venture to say.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So how I envisioned this thing to be used is mostly for navigational interface, where we extract the named entities from the documents and then allow people like a navigational access to it and saying like, OK, you have a paper here which uses a certain data set.",
                    "label": 0
                },
                {
                    "sent": "How about this paper here which uses the same data set or more papers which have the same algorithm and you just can navigate there, because I personally would avoid exactly the problem you just mentioned that people are totally unable to have queries which are proper.",
                    "label": 0
                },
                {
                    "sent": "Well formed.",
                    "label": 0
                },
                {
                    "sent": "And mostly work on documents which are.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}