{
    "id": "qxfmsjplk2bdbxtilt6leoyjwutpvzvx",
    "title": "Uncorrelated Multilinear Principal Component Analysis through Successive Variance Maximization",
    "info": {
        "author": [
            "Haiping Lu, Department of Computer Science, University of Toronto"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis"
        ]
    },
    "url": "http://videolectures.net/icml08_lu_ump/",
    "segmentation": [
        [
            "My name is hiking loop and from University of Toronto and the paper I'm going to present is uncorrelated, multilinear principal component analysis through successes variance maximization."
        ],
        [
            "So here is an outline of my talk.",
            "A star is the motivations of this works, and then I propose the uncorrelated multilinear PCA algorithm.",
            "And next, the experimental evaluations will be presented.",
            "And finally I will draw the conclusions."
        ],
        [
            "So here is the data that we consider in this work.",
            "So it is tensorial data.",
            "Fees tend to have various definitions in various field.",
            "The tensor in this work we refer to multidimensional arrays and if your generalization of vector and matrix concept.",
            "So if there are any modes then we call it an NTH order tensor.",
            "So vector either 1st order tensor and matrix.",
            "Either 2nd order tensor.",
            "And tensorial data?",
            "It has a wide range of applications such as application involving images, video sequences and streaming data or data mining data.",
            "And here I show the two example of tensors.",
            "The first one is a second order tensor.",
            "It has two modes.",
            "The column spatial columns and spatial rumors.",
            "So this is a typical grade level 6 image and the second one.",
            "It's the 3rd order tensor, which is a binary silhouette sequence is a gauge sequence, and has three modes.",
            "The spatial column, spatial role and the time modes right?",
            "So this is 1/3 or."
        ],
        [
            "Tensor.",
            "So the problem we're going to solve is dimensionality reduction.",
            "So why do we want to reduce the dimensionality the tensor objects?",
            "They are usually very high dimensional.",
            "This leads to the problem of curse of dimensionality.",
            "So why this is a curse?",
            "'cause it is this kind of high dimension?",
            "Data is computationally very expensive to handle.",
            "An many classifiers we perform very poorly in high dimensional space.",
            "Given the small number of training samples, which is often true in practice."
        ],
        [
            "But in fact, if we look at a class of tensor objects, they're mostly highly constrained to a subspace manifold of intrinsically low dimension.",
            "Like the face, the gate.",
            "We are also very constrained, right?",
            "And so dimensionality the popular method.",
            "And there's also some kind of feature extraction.",
            "Dimensionality will transform these high dimensional data into a low dimensional space.",
            "While doing this transformation with the objective, trying to retain most of the underlying structure."
        ],
        [
            "And the focus of this work is unsupervised dimensionality reduction, and in particular PC based algorithms.",
            "So PC is a well known linear method read, produce uncorrelated features.",
            "It retain as much as possible the variations in the original data.",
            "But there's one.",
            "One thing we should be aware that PC is a linear method, so we need to reshape this tensor no matter is 2 D 3D or even higher.",
            "They need to receive them into vectors.",
            "So this will result in very high dimensionality and lead to high computational and memory demand.",
            "And another thing is that this kind of reshaping the usually briefly natural structure in the original data."
        ],
        [
            "So this kind of observation lead to the development of multilinear algorithms.",
            "They extract features directly from the tensor objects from the tensor representation.",
            "So here are list of the existing work, the tensor rank one.",
            "Decomposition is probably in 2001 and the two dimensional PCA is in 2004 and the generalized low rank approximation of matrices, which is I think it's the best.",
            "The student of wallpaper in XML 2005 and also the generalized PCA and the other one is a code concurrent subspace analysis is proposed in 2005 and another one.",
            "Is multilinear PCA property in 2008 with Luismi?"
        ],
        [
            "So there's a difference between this."
        ],
        [
            "So.",
            "Existing mounting method as a linear methods.",
            "That one property that this kind of multilinear matter they do not share with the original classical PC derivation that is the original PC derives uncorrelated features.",
            "But none of these methods they produce."
        ],
        [
            "Related features.",
            "So why I would prefer uncorrelated features if it costs on correlated features?",
            "The resulting minimum redundancy the ensure linear independence among features, and we simplify the classification task, follow the feature extraction process.",
            "So the question we would like to ask is can we extract uncorrelated features?",
            "Directly from tensor objects and in an unsupervised way, and the answer is yes."
        ],
        [
            "Advise my people will not get accepted right?",
            "So I will propose the Unpc algorithm now.",
            "So before going to the algorithm we need to I need to introduce the projection first."
        ],
        [
            "Answer To vector projection and before we introduce the projection, I think it's better to introduce the notations so that it's easier to follow because it's multilinear algebra, so we have some kind of conventions and further vector we use lowercase policies and four matches.",
            "We use uppercase boldface an for 10s of years, calligraphic to later, and I use superscript key to use as a feature index and.",
            "Subscript subscript M is the training center in jest and superscript N is the mode, so it's 50 is the transpose and this kind of operation is mode multiplication."
        ],
        [
            "So here is the basic problem we need to consider.",
            "So how can we project the tensor without vectorization into a scalar?",
            "This can be achieved through what I called elementary multilinear projection is elementary projection.",
            "2 Project you use this third order tensor example project this tensor into a scalar.",
            "We need 3 vectors so we use the first vector to project the first mode right project each of these so called mode vector and we get a matrix.",
            "So the order from three to two and then we use the 2nd.",
            "Vector projected matches.",
            "Into a vector, we get a first order tensor, right?",
            "And we use the third projection author projected vector to further protect this vector into scalar to achieve."
        ],
        [
            "Sir, to scalar projection and how about tensor to vector projection projection?",
            "So imagine we have.",
            "A number of amps we have P of them.",
            "Then we get the scalar.",
            "So this piece killer the former vector.",
            "So we have 10 through 12."
        ],
        [
            "Vector projection."
        ],
        [
            "So here is the unpc problem formulation.",
            "The input is tensorial training samples, and we also need to specify the desired dimensionality."
        ],
        [
            "And the objective is same as PC where you want to what we want to maximize either total scatter.",
            "It is a major party measure of the variance and it is defined based on scalar in the projective space is a projective sample, right?",
            "This is a scalar.",
            "It's defined."
        ],
        [
            "Based on scalars.",
            "And we also need to enforce the constraint.",
            "The first constraint is to make sure that this kind of scatter will not be sensitive to scaling and the certain constraint is the title.",
            "In my paper 'cause we want to enforce the constraint of zero correlation.",
            "So we want it to be uncorrelated.",
            "It is defined."
        ],
        [
            "Please down the.",
            "Calling it vector, which is a projection from the piece.",
            "Yeah."
        ],
        [
            "Npi so the output of this algorithm is a tensor to vector projections that satisfy this objective.",
            "So this is this is."
        ],
        [
            "Problem formulation so how to solve this problem?",
            "The approach I have take is success is maximization.",
            "We saw first the first EMP without any constraint and trying to maximize the projection.",
            "The scatter of the first projection and then we continue to determine the second EMP.",
            "By math, maybe the schedule of the second projection, but subject to the constraint, there's a second set of features will be uncorrelated with the first set, and then we continue to the third one, maximizing the scatter and something that constraint that the third one will be uncorrelated first one and."
        ],
        [
            "The second one.",
            "And how to solve each individual in case we take the approach of alternating projection?",
            "So this is a iterative solution.",
            "So why do we need the iterative solution?",
            "Visit cost for each NP.",
            "If you remember we have North set of projects in vectors to solve so, but to simultaneously determine this end set of parameters is currently in physical.",
            "So it is, uh, the approach of alternating projection.",
            "So we solve one set of parameters with all the other parameters fixed and then iterate.",
            "This is originated from the alternating least square.",
            "A method proposing the."
        ],
        [
            "Eventis so here is how it works.",
            "We assume that except a mode and star or the project vectors are given.",
            "And we project the training centers into this minus one one mode, so we get.",
            "Vectors right?",
            "We get em vectors.",
            "Then we determine a projection vector that can project this.",
            "These vectors underline that will maximize the variance and subject to the constraint of zero correlation.",
            "So this problem is.",
            "Either classical pspa with the input of the vectors Y and the the corresponding respective scatter matches is also defined based on this."
        ],
        [
            "Vectors of Y.",
            "And so to solve this problem for the case of equal to 180 DZ right, there's no constraint, so we can just maximize the objective function.",
            "So the first projection projection projection is obtained by setting the projector equal to the unit.",
            "Eigenvector of ST1 theater right and associated with the largest eigenvalue."
        ],
        [
            "How about for P greater than one?",
            "We formally liking PC right?",
            "We form a data metrics by stacking this an vectors and."
        ],
        [
            "We reformulate the original problem, so this maximization problem is formulated in terms of the scatter metrics defined based on Y and the."
        ],
        [
            "Constraint is also revised accordingly.",
            "This one key kept unchanged, and this the constraint is revised with according to the data Matrix Y an now."
        ],
        [
            "To solve this problem we propose we propose a theorem in the paper.",
            "This theorem states that the solution to this unpc problem that typically for P greater than one is the unit length eigenvector corresponding to the largest eigenvalue of this.",
            "Our grammatic problem here, this symbol.",
            "Fee is."
        ],
        [
            "Find.",
            "As follows, the definition is basically based on the data metrics right and the coordinate?",
            "The metrics G consists of the coordinate vectors.",
            "I'm not going to give the details of it."
        ],
        [
            "Vision and you can read the paper.",
            "I'll come to the poster just outside of these thought so.",
            "So by now I have proposed the Unpc algorithm, so it's time to evaluate the property the propose."
        ],
        [
            "The algorithm.",
            "So this is the experimental setup.",
            "We choose a fair face recognition problem.",
            "The unsupervised face recognition problem.",
            "The database we have chooses.",
            "The fair database and we choose a subset so that the maximum post variation is 15 degrees.",
            "And the minimum number of 50 images per subject is 8."
        ],
        [
            "So this results in 700 and 2150 images from 70 subjects and we are focusing on the feature extraction so we do pre processing by menu cropping and alignment and it is all the images are normalized to 80 by 80 pixels with two 156th grade grade levels per pixel."
        ],
        [
            "And further clarification, we focus on features and so we use simple classification method, nearest neighbor classifier, Euclidean distance measure and for the performance evaluation of classification we use the report.",
            "The rank one identification rich."
        ],
        [
            "So."
        ],
        [
            "So here are the results.",
            "First, for LT1 L here you know the number of training samples perceptive, so L you could.",
            "1 means that only one thing example per subject.",
            "This so this is a extremely small sample size scenario, so it's very difficult and.",
            "The actual mentioned that the supervised method cannot solve the conventional supervised later cannot solve this problem, because with only one sample you cannot estimate the weaving class together and hear that results.",
            "The first one is for dimensionality from one to 10, the right trigger it from 15 to up to 80.",
            "And the proposed method is marked by diamond, so expensive diamonds and from the result we have.",
            "Seeing that this proposed you empty 8 outperforms the other three methods.",
            "PC, NPC and TR."
        ],
        [
            "Key.",
            "And we further examine the results for L equal to 7.",
            "And here are the results similarly shown.",
            "And we have the similar observation.",
            "The Unpc outperform the other three method as well, and for the other results from two to six is outside of this also."
        ],
        [
            "You can come to the poster.",
            "And since we this method maximized variation."
        ],
        [
            "And actually, there's an observation from the previous result.",
            "We observe that the unpc it saturates.",
            "Earlier than other methods at the performance."
        ],
        [
            "Fat rich earlier around 2030 factory very fast so."
        ],
        [
            "This motivated us to examine the variance captured by each method.",
            "So here are the results.",
            "We plot the variation captured, measured by the scatter in log scales, visa log scales, and these 4X1 leq 2.",
            "And again, the proper method is marked by diamonds, and we can also from this variations plot that the variation captured by unpc is much lower than the other method, right?",
            "And this is due to the very constraint solution.",
            "So 'cause we have constant zero correlation and we also have the country of it must be a tensor to vector direct projection and this too low variation is explained.",
            "The saturation, because when it's too low, the variation it you're limited contribution."
        ],
        [
            "Either recognition so we clean these, uncorrelated with mathematical proof.",
            "Here are some simulation studies.",
            "We applaud the correlation average correlation between pairwise features and here are the results.",
            "Obvious PC and unpc the option on quality features.",
            "The other two methods.",
            "Their features are correlated."
        ],
        [
            "So here, then a summary of my talk.",
            "So I propose a unpc algorithm.",
            "80 thread uncorrelated features directly from tensor objects through a tensor vector projection.",
            "And the solution to this unpc problem is through successes.",
            "Various mathematician and alternating projection method and further evaluation.",
            "We have observed that the Untz outperformed PC and PC and cloudy in unsupervised face recognition task and especially is very effective in lower dimension."
        ],
        [
            "So I would like to conclude with the future works.",
            "So this is a new algorithm an would like to see its application to other unsupervised learning tasks such as clustering.",
            "And we would also like to investigate the design issues such as initialization, protection order, antenna termination, which I did not mention in the talk.",
            "And finally it's interesting to study the combination of unpc features with PC and PCR TRD features.",
            "Thank you just a little bit over.",
            "Yes, traditionally the people actually extended the multilinear extensions of the single ability.",
            "Composition is a parafac.",
            "Yeah, there's a fact, right?",
            "Yeah, yeah, this kind of thing.",
            "They are, they are.",
            "They are that's in the 70s.",
            "We have extensively studied this factor in the factorization settings, but there are some.",
            "Difference between subsidy learning and factorization.",
            "I would like to summarize there are three main difference.",
            "The first one is that.",
            "3D artworks by.",
            "There are previous work, like tensor phase.",
            "It's very popular paper, very well cited.",
            "It's also multilinear setting, but it will treat the tensors no matter if it image oh gate, no matter the dimension, they will still vectorize it.",
            "But we will form the tensor through various modality.",
            "Or like the factors involved in the formation in the formation of the data, right?",
            "So it's a it's dealing with different problem and this kind of factorization.",
            "We have.",
            "Another key difference is that they require a lot of data.",
            "For training we have to find the data to form the kind of if we have 35 illumination, three posts and 10 people.",
            "We have to find so many samples for each subject, right?",
            "So they require lots of samples and correspondingly there.",
            "American is very huge.",
            "The so the processing cost is higher than we can kind of subsidy learning and we think it also have limitation in is good in modeling and understanding the problem, but it may not be well practical because in practice usually you have only a few samples and you do not have luxury to form this kind of.",
            "Tensors with various factors from each person."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is hiking loop and from University of Toronto and the paper I'm going to present is uncorrelated, multilinear principal component analysis through successes variance maximization.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "A star is the motivations of this works, and then I propose the uncorrelated multilinear PCA algorithm.",
                    "label": 1
                },
                {
                    "sent": "And next, the experimental evaluations will be presented.",
                    "label": 0
                },
                {
                    "sent": "And finally I will draw the conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the data that we consider in this work.",
                    "label": 0
                },
                {
                    "sent": "So it is tensorial data.",
                    "label": 1
                },
                {
                    "sent": "Fees tend to have various definitions in various field.",
                    "label": 0
                },
                {
                    "sent": "The tensor in this work we refer to multidimensional arrays and if your generalization of vector and matrix concept.",
                    "label": 1
                },
                {
                    "sent": "So if there are any modes then we call it an NTH order tensor.",
                    "label": 0
                },
                {
                    "sent": "So vector either 1st order tensor and matrix.",
                    "label": 0
                },
                {
                    "sent": "Either 2nd order tensor.",
                    "label": 0
                },
                {
                    "sent": "And tensorial data?",
                    "label": 0
                },
                {
                    "sent": "It has a wide range of applications such as application involving images, video sequences and streaming data or data mining data.",
                    "label": 1
                },
                {
                    "sent": "And here I show the two example of tensors.",
                    "label": 0
                },
                {
                    "sent": "The first one is a second order tensor.",
                    "label": 0
                },
                {
                    "sent": "It has two modes.",
                    "label": 0
                },
                {
                    "sent": "The column spatial columns and spatial rumors.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical grade level 6 image and the second one.",
                    "label": 0
                },
                {
                    "sent": "It's the 3rd order tensor, which is a binary silhouette sequence is a gauge sequence, and has three modes.",
                    "label": 0
                },
                {
                    "sent": "The spatial column, spatial role and the time modes right?",
                    "label": 0
                },
                {
                    "sent": "So this is 1/3 or.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tensor.",
                    "label": 0
                },
                {
                    "sent": "So the problem we're going to solve is dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "So why do we want to reduce the dimensionality the tensor objects?",
                    "label": 0
                },
                {
                    "sent": "They are usually very high dimensional.",
                    "label": 1
                },
                {
                    "sent": "This leads to the problem of curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So why this is a curse?",
                    "label": 0
                },
                {
                    "sent": "'cause it is this kind of high dimension?",
                    "label": 1
                },
                {
                    "sent": "Data is computationally very expensive to handle.",
                    "label": 1
                },
                {
                    "sent": "An many classifiers we perform very poorly in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Given the small number of training samples, which is often true in practice.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in fact, if we look at a class of tensor objects, they're mostly highly constrained to a subspace manifold of intrinsically low dimension.",
                    "label": 1
                },
                {
                    "sent": "Like the face, the gate.",
                    "label": 0
                },
                {
                    "sent": "We are also very constrained, right?",
                    "label": 0
                },
                {
                    "sent": "And so dimensionality the popular method.",
                    "label": 0
                },
                {
                    "sent": "And there's also some kind of feature extraction.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality will transform these high dimensional data into a low dimensional space.",
                    "label": 1
                },
                {
                    "sent": "While doing this transformation with the objective, trying to retain most of the underlying structure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the focus of this work is unsupervised dimensionality reduction, and in particular PC based algorithms.",
                    "label": 0
                },
                {
                    "sent": "So PC is a well known linear method read, produce uncorrelated features.",
                    "label": 1
                },
                {
                    "sent": "It retain as much as possible the variations in the original data.",
                    "label": 1
                },
                {
                    "sent": "But there's one.",
                    "label": 1
                },
                {
                    "sent": "One thing we should be aware that PC is a linear method, so we need to reshape this tensor no matter is 2 D 3D or even higher.",
                    "label": 0
                },
                {
                    "sent": "They need to receive them into vectors.",
                    "label": 1
                },
                {
                    "sent": "So this will result in very high dimensionality and lead to high computational and memory demand.",
                    "label": 0
                },
                {
                    "sent": "And another thing is that this kind of reshaping the usually briefly natural structure in the original data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this kind of observation lead to the development of multilinear algorithms.",
                    "label": 0
                },
                {
                    "sent": "They extract features directly from the tensor objects from the tensor representation.",
                    "label": 0
                },
                {
                    "sent": "So here are list of the existing work, the tensor rank one.",
                    "label": 0
                },
                {
                    "sent": "Decomposition is probably in 2001 and the two dimensional PCA is in 2004 and the generalized low rank approximation of matrices, which is I think it's the best.",
                    "label": 1
                },
                {
                    "sent": "The student of wallpaper in XML 2005 and also the generalized PCA and the other one is a code concurrent subspace analysis is proposed in 2005 and another one.",
                    "label": 1
                },
                {
                    "sent": "Is multilinear PCA property in 2008 with Luismi?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a difference between this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Existing mounting method as a linear methods.",
                    "label": 0
                },
                {
                    "sent": "That one property that this kind of multilinear matter they do not share with the original classical PC derivation that is the original PC derives uncorrelated features.",
                    "label": 0
                },
                {
                    "sent": "But none of these methods they produce.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Related features.",
                    "label": 0
                },
                {
                    "sent": "So why I would prefer uncorrelated features if it costs on correlated features?",
                    "label": 0
                },
                {
                    "sent": "The resulting minimum redundancy the ensure linear independence among features, and we simplify the classification task, follow the feature extraction process.",
                    "label": 1
                },
                {
                    "sent": "So the question we would like to ask is can we extract uncorrelated features?",
                    "label": 0
                },
                {
                    "sent": "Directly from tensor objects and in an unsupervised way, and the answer is yes.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Advise my people will not get accepted right?",
                    "label": 0
                },
                {
                    "sent": "So I will propose the Unpc algorithm now.",
                    "label": 0
                },
                {
                    "sent": "So before going to the algorithm we need to I need to introduce the projection first.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Answer To vector projection and before we introduce the projection, I think it's better to introduce the notations so that it's easier to follow because it's multilinear algebra, so we have some kind of conventions and further vector we use lowercase policies and four matches.",
                    "label": 0
                },
                {
                    "sent": "We use uppercase boldface an for 10s of years, calligraphic to later, and I use superscript key to use as a feature index and.",
                    "label": 0
                },
                {
                    "sent": "Subscript subscript M is the training center in jest and superscript N is the mode, so it's 50 is the transpose and this kind of operation is mode multiplication.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the basic problem we need to consider.",
                    "label": 0
                },
                {
                    "sent": "So how can we project the tensor without vectorization into a scalar?",
                    "label": 0
                },
                {
                    "sent": "This can be achieved through what I called elementary multilinear projection is elementary projection.",
                    "label": 1
                },
                {
                    "sent": "2 Project you use this third order tensor example project this tensor into a scalar.",
                    "label": 0
                },
                {
                    "sent": "We need 3 vectors so we use the first vector to project the first mode right project each of these so called mode vector and we get a matrix.",
                    "label": 0
                },
                {
                    "sent": "So the order from three to two and then we use the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Vector projected matches.",
                    "label": 0
                },
                {
                    "sent": "Into a vector, we get a first order tensor, right?",
                    "label": 0
                },
                {
                    "sent": "And we use the third projection author projected vector to further protect this vector into scalar to achieve.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sir, to scalar projection and how about tensor to vector projection projection?",
                    "label": 0
                },
                {
                    "sent": "So imagine we have.",
                    "label": 0
                },
                {
                    "sent": "A number of amps we have P of them.",
                    "label": 0
                },
                {
                    "sent": "Then we get the scalar.",
                    "label": 0
                },
                {
                    "sent": "So this piece killer the former vector.",
                    "label": 0
                },
                {
                    "sent": "So we have 10 through 12.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector projection.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the unpc problem formulation.",
                    "label": 0
                },
                {
                    "sent": "The input is tensorial training samples, and we also need to specify the desired dimensionality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the objective is same as PC where you want to what we want to maximize either total scatter.",
                    "label": 0
                },
                {
                    "sent": "It is a major party measure of the variance and it is defined based on scalar in the projective space is a projective sample, right?",
                    "label": 0
                },
                {
                    "sent": "This is a scalar.",
                    "label": 0
                },
                {
                    "sent": "It's defined.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on scalars.",
                    "label": 0
                },
                {
                    "sent": "And we also need to enforce the constraint.",
                    "label": 0
                },
                {
                    "sent": "The first constraint is to make sure that this kind of scatter will not be sensitive to scaling and the certain constraint is the title.",
                    "label": 0
                },
                {
                    "sent": "In my paper 'cause we want to enforce the constraint of zero correlation.",
                    "label": 0
                },
                {
                    "sent": "So we want it to be uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "It is defined.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please down the.",
                    "label": 0
                },
                {
                    "sent": "Calling it vector, which is a projection from the piece.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Npi so the output of this algorithm is a tensor to vector projections that satisfy this objective.",
                    "label": 0
                },
                {
                    "sent": "So this is this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem formulation so how to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "The approach I have take is success is maximization.",
                    "label": 0
                },
                {
                    "sent": "We saw first the first EMP without any constraint and trying to maximize the projection.",
                    "label": 1
                },
                {
                    "sent": "The scatter of the first projection and then we continue to determine the second EMP.",
                    "label": 0
                },
                {
                    "sent": "By math, maybe the schedule of the second projection, but subject to the constraint, there's a second set of features will be uncorrelated with the first set, and then we continue to the third one, maximizing the scatter and something that constraint that the third one will be uncorrelated first one and.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second one.",
                    "label": 0
                },
                {
                    "sent": "And how to solve each individual in case we take the approach of alternating projection?",
                    "label": 0
                },
                {
                    "sent": "So this is a iterative solution.",
                    "label": 1
                },
                {
                    "sent": "So why do we need the iterative solution?",
                    "label": 0
                },
                {
                    "sent": "Visit cost for each NP.",
                    "label": 0
                },
                {
                    "sent": "If you remember we have North set of projects in vectors to solve so, but to simultaneously determine this end set of parameters is currently in physical.",
                    "label": 1
                },
                {
                    "sent": "So it is, uh, the approach of alternating projection.",
                    "label": 0
                },
                {
                    "sent": "So we solve one set of parameters with all the other parameters fixed and then iterate.",
                    "label": 1
                },
                {
                    "sent": "This is originated from the alternating least square.",
                    "label": 0
                },
                {
                    "sent": "A method proposing the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eventis so here is how it works.",
                    "label": 0
                },
                {
                    "sent": "We assume that except a mode and star or the project vectors are given.",
                    "label": 0
                },
                {
                    "sent": "And we project the training centers into this minus one one mode, so we get.",
                    "label": 0
                },
                {
                    "sent": "Vectors right?",
                    "label": 0
                },
                {
                    "sent": "We get em vectors.",
                    "label": 0
                },
                {
                    "sent": "Then we determine a projection vector that can project this.",
                    "label": 0
                },
                {
                    "sent": "These vectors underline that will maximize the variance and subject to the constraint of zero correlation.",
                    "label": 0
                },
                {
                    "sent": "So this problem is.",
                    "label": 0
                },
                {
                    "sent": "Either classical pspa with the input of the vectors Y and the the corresponding respective scatter matches is also defined based on this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vectors of Y.",
                    "label": 0
                },
                {
                    "sent": "And so to solve this problem for the case of equal to 180 DZ right, there's no constraint, so we can just maximize the objective function.",
                    "label": 0
                },
                {
                    "sent": "So the first projection projection projection is obtained by setting the projector equal to the unit.",
                    "label": 0
                },
                {
                    "sent": "Eigenvector of ST1 theater right and associated with the largest eigenvalue.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How about for P greater than one?",
                    "label": 0
                },
                {
                    "sent": "We formally liking PC right?",
                    "label": 0
                },
                {
                    "sent": "We form a data metrics by stacking this an vectors and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We reformulate the original problem, so this maximization problem is formulated in terms of the scatter metrics defined based on Y and the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraint is also revised accordingly.",
                    "label": 0
                },
                {
                    "sent": "This one key kept unchanged, and this the constraint is revised with according to the data Matrix Y an now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve this problem we propose we propose a theorem in the paper.",
                    "label": 0
                },
                {
                    "sent": "This theorem states that the solution to this unpc problem that typically for P greater than one is the unit length eigenvector corresponding to the largest eigenvalue of this.",
                    "label": 1
                },
                {
                    "sent": "Our grammatic problem here, this symbol.",
                    "label": 0
                },
                {
                    "sent": "Fee is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find.",
                    "label": 0
                },
                {
                    "sent": "As follows, the definition is basically based on the data metrics right and the coordinate?",
                    "label": 0
                },
                {
                    "sent": "The metrics G consists of the coordinate vectors.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to give the details of it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vision and you can read the paper.",
                    "label": 0
                },
                {
                    "sent": "I'll come to the poster just outside of these thought so.",
                    "label": 0
                },
                {
                    "sent": "So by now I have proposed the Unpc algorithm, so it's time to evaluate the property the propose.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is the experimental setup.",
                    "label": 1
                },
                {
                    "sent": "We choose a fair face recognition problem.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised face recognition problem.",
                    "label": 0
                },
                {
                    "sent": "The database we have chooses.",
                    "label": 1
                },
                {
                    "sent": "The fair database and we choose a subset so that the maximum post variation is 15 degrees.",
                    "label": 0
                },
                {
                    "sent": "And the minimum number of 50 images per subject is 8.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this results in 700 and 2150 images from 70 subjects and we are focusing on the feature extraction so we do pre processing by menu cropping and alignment and it is all the images are normalized to 80 by 80 pixels with two 156th grade grade levels per pixel.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And further clarification, we focus on features and so we use simple classification method, nearest neighbor classifier, Euclidean distance measure and for the performance evaluation of classification we use the report.",
                    "label": 0
                },
                {
                    "sent": "The rank one identification rich.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "First, for LT1 L here you know the number of training samples perceptive, so L you could.",
                    "label": 1
                },
                {
                    "sent": "1 means that only one thing example per subject.",
                    "label": 0
                },
                {
                    "sent": "This so this is a extremely small sample size scenario, so it's very difficult and.",
                    "label": 1
                },
                {
                    "sent": "The actual mentioned that the supervised method cannot solve the conventional supervised later cannot solve this problem, because with only one sample you cannot estimate the weaving class together and hear that results.",
                    "label": 0
                },
                {
                    "sent": "The first one is for dimensionality from one to 10, the right trigger it from 15 to up to 80.",
                    "label": 1
                },
                {
                    "sent": "And the proposed method is marked by diamond, so expensive diamonds and from the result we have.",
                    "label": 0
                },
                {
                    "sent": "Seeing that this proposed you empty 8 outperforms the other three methods.",
                    "label": 0
                },
                {
                    "sent": "PC, NPC and TR.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Key.",
                    "label": 0
                },
                {
                    "sent": "And we further examine the results for L equal to 7.",
                    "label": 1
                },
                {
                    "sent": "And here are the results similarly shown.",
                    "label": 0
                },
                {
                    "sent": "And we have the similar observation.",
                    "label": 0
                },
                {
                    "sent": "The Unpc outperform the other three method as well, and for the other results from two to six is outside of this also.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can come to the poster.",
                    "label": 0
                },
                {
                    "sent": "And since we this method maximized variation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, there's an observation from the previous result.",
                    "label": 0
                },
                {
                    "sent": "We observe that the unpc it saturates.",
                    "label": 0
                },
                {
                    "sent": "Earlier than other methods at the performance.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fat rich earlier around 2030 factory very fast so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This motivated us to examine the variance captured by each method.",
                    "label": 0
                },
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "We plot the variation captured, measured by the scatter in log scales, visa log scales, and these 4X1 leq 2.",
                    "label": 0
                },
                {
                    "sent": "And again, the proper method is marked by diamonds, and we can also from this variations plot that the variation captured by unpc is much lower than the other method, right?",
                    "label": 1
                },
                {
                    "sent": "And this is due to the very constraint solution.",
                    "label": 0
                },
                {
                    "sent": "So 'cause we have constant zero correlation and we also have the country of it must be a tensor to vector direct projection and this too low variation is explained.",
                    "label": 0
                },
                {
                    "sent": "The saturation, because when it's too low, the variation it you're limited contribution.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Either recognition so we clean these, uncorrelated with mathematical proof.",
                    "label": 0
                },
                {
                    "sent": "Here are some simulation studies.",
                    "label": 0
                },
                {
                    "sent": "We applaud the correlation average correlation between pairwise features and here are the results.",
                    "label": 0
                },
                {
                    "sent": "Obvious PC and unpc the option on quality features.",
                    "label": 0
                },
                {
                    "sent": "The other two methods.",
                    "label": 0
                },
                {
                    "sent": "Their features are correlated.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, then a summary of my talk.",
                    "label": 0
                },
                {
                    "sent": "So I propose a unpc algorithm.",
                    "label": 0
                },
                {
                    "sent": "80 thread uncorrelated features directly from tensor objects through a tensor vector projection.",
                    "label": 1
                },
                {
                    "sent": "And the solution to this unpc problem is through successes.",
                    "label": 1
                },
                {
                    "sent": "Various mathematician and alternating projection method and further evaluation.",
                    "label": 0
                },
                {
                    "sent": "We have observed that the Untz outperformed PC and PC and cloudy in unsupervised face recognition task and especially is very effective in lower dimension.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to conclude with the future works.",
                    "label": 1
                },
                {
                    "sent": "So this is a new algorithm an would like to see its application to other unsupervised learning tasks such as clustering.",
                    "label": 1
                },
                {
                    "sent": "And we would also like to investigate the design issues such as initialization, protection order, antenna termination, which I did not mention in the talk.",
                    "label": 1
                },
                {
                    "sent": "And finally it's interesting to study the combination of unpc features with PC and PCR TRD features.",
                    "label": 0
                },
                {
                    "sent": "Thank you just a little bit over.",
                    "label": 0
                },
                {
                    "sent": "Yes, traditionally the people actually extended the multilinear extensions of the single ability.",
                    "label": 0
                },
                {
                    "sent": "Composition is a parafac.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a fact, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "They are, they are.",
                    "label": 0
                },
                {
                    "sent": "They are that's in the 70s.",
                    "label": 0
                },
                {
                    "sent": "We have extensively studied this factor in the factorization settings, but there are some.",
                    "label": 0
                },
                {
                    "sent": "Difference between subsidy learning and factorization.",
                    "label": 0
                },
                {
                    "sent": "I would like to summarize there are three main difference.",
                    "label": 0
                },
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "3D artworks by.",
                    "label": 0
                },
                {
                    "sent": "There are previous work, like tensor phase.",
                    "label": 0
                },
                {
                    "sent": "It's very popular paper, very well cited.",
                    "label": 0
                },
                {
                    "sent": "It's also multilinear setting, but it will treat the tensors no matter if it image oh gate, no matter the dimension, they will still vectorize it.",
                    "label": 0
                },
                {
                    "sent": "But we will form the tensor through various modality.",
                    "label": 0
                },
                {
                    "sent": "Or like the factors involved in the formation in the formation of the data, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a it's dealing with different problem and this kind of factorization.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Another key difference is that they require a lot of data.",
                    "label": 0
                },
                {
                    "sent": "For training we have to find the data to form the kind of if we have 35 illumination, three posts and 10 people.",
                    "label": 0
                },
                {
                    "sent": "We have to find so many samples for each subject, right?",
                    "label": 0
                },
                {
                    "sent": "So they require lots of samples and correspondingly there.",
                    "label": 0
                },
                {
                    "sent": "American is very huge.",
                    "label": 0
                },
                {
                    "sent": "The so the processing cost is higher than we can kind of subsidy learning and we think it also have limitation in is good in modeling and understanding the problem, but it may not be well practical because in practice usually you have only a few samples and you do not have luxury to form this kind of.",
                    "label": 0
                },
                {
                    "sent": "Tensors with various factors from each person.",
                    "label": 0
                }
            ]
        }
    }
}