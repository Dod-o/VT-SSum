{
    "id": "y36rsspi3isdc5c6wvctrnd637phn54i",
    "title": "Abductive Plan Recognition by Extending Bayesian Logic Programs",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Sindhu Raghavan, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Logic"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_raghavan_abductive/",
    "segmentation": [
        [
            "Hello everyone, I'm Cinderella one and I will be talking about extending basic logic programs for abductor plan recognition.",
            "This is joint work with my advisor Emoni who's in the audience today.",
            "This work was done at the University of Texas at all."
        ],
        [
            "So next must understand the problem of land recognition.",
            "Plan recognition is the task of predicting in agents top level plans based on its observed actions.",
            "You can think of this as an abductive reasoning task.",
            "When you're trying to infer, the cause is based on the effects.",
            "So in this case, your top level plants are going to be the causes and the observed actions are going to be the effects.",
            "In this talk we will see plan recognition being applied to several applications, particularly story understanding, strategic planning and intelligent user interface."
        ],
        [
            "So let's take a concrete example of land recognition from intelligent user interfaces.",
            "Here you see a user that's working at a desktop system.",
            "Let's assume that this is a Linux based system and he starting these commands change to test directly.",
            "Copy test one dot T XT to my directory and remove.",
            "Test one dot T XT.",
            "So here is the plan.",
            "Recognition task involves finding out the top level plan, all the tasks that the user is performing.",
            "Is he trying to move a file?",
            "Is he copying a file, removing the file?",
            "Given that you figure out what he's doing, you also want to figure out which files and directories are involved in this task.",
            "So you would ask why would we care about having planned recognition and intelligent user interfaces?",
            "Now if you look at the example, it looks like the user is trying to move a file, and if you're aware of Linux Linux based system, you know that you know you can just use the move command to move a file.",
            "You don't have to copy a file from the source directory to the Destination directory and then remove it from this source directory.",
            "So if you had a plan recognition system in place that could predict what the user was doing now the intelligent desktop or the user interface could tell the user hey, why don't you just use the MOVE command while using copy and remove command?",
            "So if you look at this example, you see that the data is structured or relational in nature there are several entities in the forms of in the form of files and directories, and there are several different relations between them.",
            "This is typical of most plan recognition."
        ],
        [
            "So previously people have used first order logic based approaches for plant recognition.",
            "You compile a knowledge base of actions and plans and then you either use logical abduction or default reasoning to predict the plan that best explains the observations.",
            "Now there is one limitation with this approach.",
            "It does not deal with uncertainty in the data or it cannot compute likelihood for alternative explanations.",
            "The automated set of approaches involves using probabilistic graphical models like Bayesian networks, hidden Markov models, statistical ngram models.",
            "But you know that these approaches can deal with uncertainty in the data, but they cannot handle relational structured data.",
            "So the third category of approaches fall in the area of statistical relational learning.",
            "You some formalism from statistical relational learning, where which basically combines these trends of both 1st order logic and probabilistic graphical models.",
            "So in the cloud category, Markov logic networks have been used for planned recognition by party and mooneyhan, singular and money.",
            "In our people who compare their approach to result from cartoon movies, Emelin approach for plan recognition.",
            "But earlier this year of singular and only published another approach of using evidence for plan recognition in triple AI.",
            "So in this talk you will see our approach being compared to the latest results.",
            "We will also compare our approach to axle or first order logic based system by Indian movie and statistical ngram model based approach.",
            "Plan recognition by Blaylock analysis."
        ],
        [
            "So I'd approach in was extending Bayesian logic programs for plan recognition.",
            "Basic logic programs are BMP Zara, formalism of that integrate, 1st order logic and Bayesian networks.",
            "You might ask why we're please, why not any other formalism?",
            "So we found that the MPs have a very efficient grounding mechanism and they include only those variables that are relevant to the query in the network.",
            "Secondly, they're very easy to extend, which you will see in a couple of minutes.",
            "You can incorporate any type of logical inference to construct the networks, and finally, because of the directed nature of their well suited for capturing causal relations in data."
        ],
        [
            "So I have actually given you the motivation of the talk in the next couple of slides.",
            "I'll cover some background on logical abduction and BLTS.",
            "Then I'll describe our DLP based approach to plan recognition.",
            "Finally, I'll show some experimental results and conclude."
        ],
        [
            "So abduction is the process of finding the best explanation for a given set of observations in first order logic you have a knowledge base, typically represented in the form of hard clauses, and you have the observations given as atomic fax.",
            "Now logical abduction involves finding a hypothesis or a set of assumptions that logically entail the observations given the theory.",
            "Sometimes you find that multiple hypothesis can explain the given set of observations.",
            "In such cases, we pick the hypothesis that has the least number of assumptions."
        ],
        [
            "So what I believe I told you earlier that beer please integrate 1st order logic and Bayesian networks.",
            "More formally, VIP program is a set of basic clauses.",
            "These are definite clauses that are universally quantified and range restricted and associated with each.",
            "Based in class is the conditional probability table, which basically tells you the probability of the head item taking several values given different combination of body of values for the atoms in the.",
            "Class body.",
            "Now each of these predicates are called Bayesian predicates because they can take finite domains as opposed to a logical predicate which can take two values, true or false, and associated with each basin predicate.",
            "Is the combining rule of which is needed to map multiple CPT's into a single CPT.",
            "I will be giving an example in the next few slides and in that example it becomes evident why we need a combining rule."
        ],
        [
            "So how do we at least construct a network, can do probabilistic inference?",
            "And if you're given a query BLTS use SLD resolution to get the logical proofs.",
            "And once you have the logic and proofs you can construct Bayesian networks.",
            "You include each logical at each ground item in the logical proof as her random variable and your Bayesian network.",
            "Then you add edges.",
            "You add edges from ground items in the body of the class to the ground item in the head, and then you use the CPT's that are associated with the corresponding Bayesian clauses to specify the problem listing parameters.",
            "Once you have the structure and the parameters specified, you can perform probabilistic inference.",
            "You can either get the marginal probability for inquiry given the evidence, or constructing most probable every explanation."
        ],
        [
            "So how are we going to use BLTS for plant recognition?",
            "Just in the previous slide, I said that BLTS used SLD resolution to construct their network.",
            "Now SND resolution is a type of deductive inference and it can be used if you want to predict your observations based on your top level plans.",
            "But the problem that we're trying to solve here as planned recognition, which is abductive in nature.",
            "We are trying to do Rivers.",
            "Given your observations, you want to figure out what caused those observations.",
            "The top level plans.",
            "So we found that VIPs cannot be used for plan recognition as is."
        ],
        [
            "So we extend their piece by using logical abduction to get our logical proofs instead of slef resolution VR, the resulting model, Bayesian abductive logic programs."
        ],
        [
            "So if you're given a knowledge base auto BRB, understand of observation literals, we compute abductive proofs using stickers abduction algorithm.",
            "So we watching on each observation literal until it can be proved or assumed.",
            "So we say that a little is proved, or it can be true if it unifies with the fact in the knowledge base or if it unifies with the head of some rules in the knowledge base.",
            "Otherwise we end up assuming the literal.",
            "Once you have your logical.",
            "Doctor Bruce, we construct the Bayesian network using the standard mechanism that's used in the LP's."
        ],
        [
            "Someone explain this algorithm or this entire process with an example.",
            "We'll go back to our example from intelligent user interfaces, so we have three top level plan predicates.",
            "Copy file, move file and remove file and two action predicates.",
            "Copy and remove commands.",
            "Now the knowledge base has four rules.",
            "It basically says that the copy command can be executed when the user is trying to either copy a file or move a file and the remove command is executed when the user is trying to move a file or remove file.",
            "Let's say we observe copy test one dot T XT to my directory, remove test one dot EX."
        ],
        [
            "So we did the first observation and be batching on it using the route shown here.",
            "We find that it results in a subgroup copying files.",
            "Test one dot T XT might retreat.",
            "We find that we cannot prove this subgroup because it does not unify with any fact in the knowledge base, and it also does not unify with the head of any rule in the knowledge base.",
            "So we have to assume this little."
        ],
        [
            "Similarly, back chain on the same observation using an alternate room.",
            "Here again, we find that move file cannot be proved, so we end up assuming."
        ],
        [
            "We then go to the next observation, the Remove Command.",
            "So when you back soon on the remove command we end up getting a subgoal move file, and we will have to assume it because we cannot prove it.",
            "But instead of creating a new assumption we had already created the assumption in the previous step, so we end up reusing or matching to that."
        ],
        [
            "Finally, back chain and remove command using the 4th root to get remove file as a sub goal which ends up being getting assume because we cannot prove."
        ],
        [
            "Now this becomes the structure of the Bayesian network."
        ],
        [
            "Now we need to specify the probabilistic parameters.",
            "Need to combine the evidence coming from the conjunction.",
            "The body of the clause.",
            "We use the noisy and model.",
            "But in some cases, like you saw in the example, you can have the same observation of.",
            "Being generated or being observed.",
            "Because of two or more, a top level plans.",
            "In such cases we need to use the noisy or model to model the explaining away phenomenon.",
            "People actually useful CPT is to specify these parameters, but if you end up using CPT's you will have many more parameters to estimate from the data by using the noisy or noisey and models you end up with parameters that are linear in the number of parents."
        ],
        [
            "So in this example, as you can see, popping command is observed as part of copy file or MOV file.",
            "So here we use the noisy or combining rule.",
            "Tomorrow, this similarly or for the remove command as well use ISR to combine the evidence coming from two different classes or two different plans."
        ],
        [
            "So once we have the structure and the parameters specified, people formed probabilistic evidence of probabilistic inference to get the best set of plans.",
            "So in some domains we notice that there are several plans that can explain the set of observations in such domains, we compute most probable explanation given given the evidence.",
            "So most probable explanation basically will give you truth values for all the unobserved nodes in the network.",
            "Once you specify the evidence.",
            "But in certain other domains, we know that there's a single correct land that best explains the observations.",
            "In such domains, we compute marginal probability for plan instances and then pick the plan that has the highest probability.",
            "We found that we could not use exact inference for some domains because it was intractable infest domains.",
            "We used sample search and approximate sampling algorithm that is specifically designed for graphical models with deterministic constraints."
        ],
        [
            "So here."
        ],
        [
            "Your network you specify your evidence."
        ],
        [
            "You specify equating radials."
        ],
        [
            "And perform problems with big influence.",
            "Now based on me."
        ],
        [
            "We we find this move file is the plan that best explains these observe."
        ],
        [
            "So we can learn these noisy or noisy and parameters using the EM algorithm that has been adapted for beardies, blacklisting and direct.",
            "We found that in plan recognition is always partial observability.",
            "During learning you have evidence for the observations and evidence for the top level plans, but then there are these nodes which represent subgoals noisy or noisey, and loads for which you don't have any evidence available.",
            "So for this reason we felt that EM was suitable for learning these parameters.",
            "We have a simplified our problem by learning only the noisy or parameters, so to combine the evidence coming from the conjunction, the body of the clause.",
            "We use the deterministic logical and model and stuff noisy and."
        ],
        [
            "People bad, we experimentally evaluated our approach on three different datasets.",
            "Monroe, coming through strategic planning Linux from intelligent user interfaces and stories."
        ],
        [
            "Someone who I'm Linux.",
            "Our datasets created by Layla and Alan Monroe is an artificially generated data set, and in this domain the task involves recognizing the top level plans in an emergency response domain.",
            "Now, Linux is a data set that was collected based on all the Linux commands that were executed by real human users, so this is a real data set and here the task involves finding the top level plan.",
            "Based on these Linux commands like how we signed the example.",
            "And in both these domains, the single correct plan in each example.",
            "So here's a quick summary of these datasets.",
            "Monroe being an artificially generated data set, you have many more examples, and there are more observations, for example.",
            "However, there are fewer top level plan predicates and fewer observed action predicates."
        ],
        [
            "So the methodology for both these domains, we manually encoded the knowledge base.",
            "We learned the parameters using the EM algorithm and we computed marginal probability for plan instances because there was a single correct plan for each example.",
            "We compare bars with Emily and Hedge can.",
            "That's the latest Eminem based approach for plan recognition and the system by Blaylock and added which is based on engram statistical N gram models.",
            "Views conversion score of this is a metric to find out later canal and this basically measures the fraction of examples for which the plant predicate was predicted correctly.",
            "Keep in mind that this metric does not really account for predicting the argument."
        ],
        [
            "Select so in Monroe based on convergence cool, we find that buys outperforms both immigrants and laylock and added start indicates that the differences are statistically SIG."
        ],
        [
            "Similarly, on Linux we find that balance is outperforming these two."
        ],
        [
            "Great, but we find that the convergence code is not perfect.",
            "Two reasons.",
            "One, it does not account for predicting the arguments correctly.",
            "Typically in a plan recognition system is not sufficient if you just predict the top level plan predicate correctly.",
            "You would also want to know the arguments and the convergence core does not capture that.",
            "Secondly, this does not test or this cannot be used to evaluate the ability of a plan recognition system to perform early plan recognition.",
            "That is, when the plan recognition system has seen fewer observations early on.",
            "Because in order to use the convergence Corda Plan definition, system has to see all the observations.",
            "So For these reasons, we design these additional experiments for early plan recognition.",
            "Specifically, we performed plan recognition after seeing the 1st 20, five, 5075 and 100% of the observations and they.",
            "Then we computed accuracy.",
            "So what we did was we first saw if the plan predicate was predicted correctly.",
            "Given that the plan predicate is predicted correctly now, you see if a subset of these arguments are predicted correctly and we assign partial credit.",
            "Now if both the plan predicate and all the arguments are predicted correctly, then you get an accuracy of 100% or one depending on how you want to assign it.",
            "We compared bots with Emily and hedge Cam or I would like to point out that some of these results were entered in his camp are not published yet, but we have access to the latest results and we thought we might as well compare them to the latest results.",
            "We could not compare our results with the the system badly Lochan Island because of the way they had conducted there."
        ],
        [
            "So I'm on the road.",
            "We find that when when the systems have seen just 25% of the observations, you find that endurance, which is the red line eminence, have an advantage over balance.",
            "But very quickly you see that Valves catches up with millions, and then we've seen all observations.",
            "Bugs have slightly.",
            "They have they have a little advantage over eminence, not a significant advantage, but flight."
        ],
        [
            "Bondage however, on Linux we find that throughout bars have a significant advantage over ambulance."
        ],
        [
            "So I'll quickly read through the 3rd and the final data set.",
            "A better story understanding here.",
            "The task involves predicting the agents top level plans based on the actions described in the narrative text.",
            "This is an example of a domain that has multiple top level plans and this is a much smaller data set with 25 examples in the development."
        ],
        [
            "Detested.",
            "He will be included.",
            "We got a knowledge base that was created for access by Indian money and we could not learn the parameters because there were very few examples in the development set.",
            "So we set them manually.",
            "We we computed MPE to get the best set of plans.",
            "We compared bars with MLNH Cam and access.",
            "Axle has two different heuristics simplicity which explains that have the least least number of assumptions and actual guidance is a domain specific heuristic which is designed specifically for story under."
        ],
        [
            "So based on story understanding, define that bass is outperforming.",
            "Everyone has Cameron eccentricity which are both general book was plan recognition systems with respect to precision recall and F mission.",
            "But then when you look at actual currency, actual coherence outperforms all these systems.",
            "But again, I'm still occurrences domain specific and we don't have the best incorporated in any of these."
        ],
        [
            "So do any quickly summarize.",
            "I have presented Bayesian abductive logic programs, an extension of beer, please for abductor plan recognition.",
            "We've demonstrated that we can learn the model parameters automatically from data using here and finally our empirical results suggest that bags have advantages over existing approaches for planter."
        ],
        [
            "Mission.",
            "In future we would like to learn the abducted knowledge bases automatically from data and compare balls to other probabilistic logics like problem prison and pulls on abduction."
        ],
        [
            "Questions.",
            "So if I understand correctly, your system doesn't have the notion of time sequence, yes, so in your example, what really makes you believe that the user wants to move something is the specific sequence, right?",
            "Ideally you would like to have the time information.",
            "This is where it all started, but definitely agree that going forward there should be a way in which we can incorporate the time dimension.",
            "Into our system, maybe you know use event calculus or some kind of representation that can capture time and then be able to do this.",
            "Place.",
            "In some cases.",
            "Most cases.",
            "Do you?",
            "Do you have any questions?",
            "So for the most part, we find that bags are outperforming MLN's.",
            "But the only yeah, the only place where we found some.",
            "Significant advantage is when you see very few observations.",
            "Unfortunately at this point or we don't have a clear intuition for why one is performing better than the other, because we're not using the same inference algorithm or the same learning and the models.",
            "Even though MLN hedge Cam and valves have pretty much similar models, there are still you know differences.",
            "But yeah, let's get open question and we are actually working on that.",
            "If I understand correctly, in order to perform this action you have two or less numerate.",
            "Many different possibilities through the adaptive knowledge base.",
            "How does this compare in size to a forward?",
            "For detection and.",
            "For larger more difficult tasks.",
            "So if I understand your question correctly, I think that I'd like to pass right one is.",
            "How do you come up with alternative explanations?",
            "Is that correct?",
            "That's True Detective.",
            "So right now we are trying.",
            "We come up with all possible abductor walkthroughs and we let the probabilistic inference engine figure out.",
            "So in case of like in the third domain we computed NP so we can actually compute CNP, wherein we get K best explanations and when you are computing marginal probabilities, you can just compute marginal probability for all plan instances and just taking the plant that has the highest probability.",
            "You can also look at, you know.",
            "Plants that have slightly lower probability, so in that way you can compute alternative explanations.",
            "And I thank you.",
            "Second question was with respect to how would you use deduction to do this?",
            "Based on what you said, if there if you had a very large model space with many unobservable variables.",
            "So for that reason, you cannot use deduction or SMT resolution because you cannot prove prove in case of unobserved data you cannot prove those things, so for that reason use abduction and wherever we cannot prove a particular fact we end up assuming it.",
            "OK, thank you and running out."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, I'm Cinderella one and I will be talking about extending basic logic programs for abductor plan recognition.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my advisor Emoni who's in the audience today.",
                    "label": 0
                },
                {
                    "sent": "This work was done at the University of Texas at all.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next must understand the problem of land recognition.",
                    "label": 0
                },
                {
                    "sent": "Plan recognition is the task of predicting in agents top level plans based on its observed actions.",
                    "label": 1
                },
                {
                    "sent": "You can think of this as an abductive reasoning task.",
                    "label": 0
                },
                {
                    "sent": "When you're trying to infer, the cause is based on the effects.",
                    "label": 0
                },
                {
                    "sent": "So in this case, your top level plants are going to be the causes and the observed actions are going to be the effects.",
                    "label": 0
                },
                {
                    "sent": "In this talk we will see plan recognition being applied to several applications, particularly story understanding, strategic planning and intelligent user interface.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take a concrete example of land recognition from intelligent user interfaces.",
                    "label": 1
                },
                {
                    "sent": "Here you see a user that's working at a desktop system.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that this is a Linux based system and he starting these commands change to test directly.",
                    "label": 0
                },
                {
                    "sent": "Copy test one dot T XT to my directory and remove.",
                    "label": 0
                },
                {
                    "sent": "Test one dot T XT.",
                    "label": 0
                },
                {
                    "sent": "So here is the plan.",
                    "label": 1
                },
                {
                    "sent": "Recognition task involves finding out the top level plan, all the tasks that the user is performing.",
                    "label": 0
                },
                {
                    "sent": "Is he trying to move a file?",
                    "label": 0
                },
                {
                    "sent": "Is he copying a file, removing the file?",
                    "label": 0
                },
                {
                    "sent": "Given that you figure out what he's doing, you also want to figure out which files and directories are involved in this task.",
                    "label": 1
                },
                {
                    "sent": "So you would ask why would we care about having planned recognition and intelligent user interfaces?",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the example, it looks like the user is trying to move a file, and if you're aware of Linux Linux based system, you know that you know you can just use the move command to move a file.",
                    "label": 1
                },
                {
                    "sent": "You don't have to copy a file from the source directory to the Destination directory and then remove it from this source directory.",
                    "label": 0
                },
                {
                    "sent": "So if you had a plan recognition system in place that could predict what the user was doing now the intelligent desktop or the user interface could tell the user hey, why don't you just use the MOVE command while using copy and remove command?",
                    "label": 0
                },
                {
                    "sent": "So if you look at this example, you see that the data is structured or relational in nature there are several entities in the forms of in the form of files and directories, and there are several different relations between them.",
                    "label": 1
                },
                {
                    "sent": "This is typical of most plan recognition.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So previously people have used first order logic based approaches for plant recognition.",
                    "label": 1
                },
                {
                    "sent": "You compile a knowledge base of actions and plans and then you either use logical abduction or default reasoning to predict the plan that best explains the observations.",
                    "label": 1
                },
                {
                    "sent": "Now there is one limitation with this approach.",
                    "label": 0
                },
                {
                    "sent": "It does not deal with uncertainty in the data or it cannot compute likelihood for alternative explanations.",
                    "label": 1
                },
                {
                    "sent": "The automated set of approaches involves using probabilistic graphical models like Bayesian networks, hidden Markov models, statistical ngram models.",
                    "label": 1
                },
                {
                    "sent": "But you know that these approaches can deal with uncertainty in the data, but they cannot handle relational structured data.",
                    "label": 0
                },
                {
                    "sent": "So the third category of approaches fall in the area of statistical relational learning.",
                    "label": 0
                },
                {
                    "sent": "You some formalism from statistical relational learning, where which basically combines these trends of both 1st order logic and probabilistic graphical models.",
                    "label": 0
                },
                {
                    "sent": "So in the cloud category, Markov logic networks have been used for planned recognition by party and mooneyhan, singular and money.",
                    "label": 0
                },
                {
                    "sent": "In our people who compare their approach to result from cartoon movies, Emelin approach for plan recognition.",
                    "label": 1
                },
                {
                    "sent": "But earlier this year of singular and only published another approach of using evidence for plan recognition in triple AI.",
                    "label": 0
                },
                {
                    "sent": "So in this talk you will see our approach being compared to the latest results.",
                    "label": 0
                },
                {
                    "sent": "We will also compare our approach to axle or first order logic based system by Indian movie and statistical ngram model based approach.",
                    "label": 0
                },
                {
                    "sent": "Plan recognition by Blaylock analysis.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd approach in was extending Bayesian logic programs for plan recognition.",
                    "label": 1
                },
                {
                    "sent": "Basic logic programs are BMP Zara, formalism of that integrate, 1st order logic and Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "You might ask why we're please, why not any other formalism?",
                    "label": 0
                },
                {
                    "sent": "So we found that the MPs have a very efficient grounding mechanism and they include only those variables that are relevant to the query in the network.",
                    "label": 1
                },
                {
                    "sent": "Secondly, they're very easy to extend, which you will see in a couple of minutes.",
                    "label": 0
                },
                {
                    "sent": "You can incorporate any type of logical inference to construct the networks, and finally, because of the directed nature of their well suited for capturing causal relations in data.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have actually given you the motivation of the talk in the next couple of slides.",
                    "label": 0
                },
                {
                    "sent": "I'll cover some background on logical abduction and BLTS.",
                    "label": 1
                },
                {
                    "sent": "Then I'll describe our DLP based approach to plan recognition.",
                    "label": 0
                },
                {
                    "sent": "Finally, I'll show some experimental results and conclude.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So abduction is the process of finding the best explanation for a given set of observations in first order logic you have a knowledge base, typically represented in the form of hard clauses, and you have the observations given as atomic fax.",
                    "label": 1
                },
                {
                    "sent": "Now logical abduction involves finding a hypothesis or a set of assumptions that logically entail the observations given the theory.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you find that multiple hypothesis can explain the given set of observations.",
                    "label": 0
                },
                {
                    "sent": "In such cases, we pick the hypothesis that has the least number of assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I believe I told you earlier that beer please integrate 1st order logic and Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "More formally, VIP program is a set of basic clauses.",
                    "label": 1
                },
                {
                    "sent": "These are definite clauses that are universally quantified and range restricted and associated with each.",
                    "label": 1
                },
                {
                    "sent": "Based in class is the conditional probability table, which basically tells you the probability of the head item taking several values given different combination of body of values for the atoms in the.",
                    "label": 0
                },
                {
                    "sent": "Class body.",
                    "label": 0
                },
                {
                    "sent": "Now each of these predicates are called Bayesian predicates because they can take finite domains as opposed to a logical predicate which can take two values, true or false, and associated with each basin predicate.",
                    "label": 0
                },
                {
                    "sent": "Is the combining rule of which is needed to map multiple CPT's into a single CPT.",
                    "label": 1
                },
                {
                    "sent": "I will be giving an example in the next few slides and in that example it becomes evident why we need a combining rule.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we at least construct a network, can do probabilistic inference?",
                    "label": 0
                },
                {
                    "sent": "And if you're given a query BLTS use SLD resolution to get the logical proofs.",
                    "label": 1
                },
                {
                    "sent": "And once you have the logic and proofs you can construct Bayesian networks.",
                    "label": 1
                },
                {
                    "sent": "You include each logical at each ground item in the logical proof as her random variable and your Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "Then you add edges.",
                    "label": 0
                },
                {
                    "sent": "You add edges from ground items in the body of the class to the ground item in the head, and then you use the CPT's that are associated with the corresponding Bayesian clauses to specify the problem listing parameters.",
                    "label": 1
                },
                {
                    "sent": "Once you have the structure and the parameters specified, you can perform probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "You can either get the marginal probability for inquiry given the evidence, or constructing most probable every explanation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how are we going to use BLTS for plant recognition?",
                    "label": 0
                },
                {
                    "sent": "Just in the previous slide, I said that BLTS used SLD resolution to construct their network.",
                    "label": 0
                },
                {
                    "sent": "Now SND resolution is a type of deductive inference and it can be used if you want to predict your observations based on your top level plans.",
                    "label": 0
                },
                {
                    "sent": "But the problem that we're trying to solve here as planned recognition, which is abductive in nature.",
                    "label": 1
                },
                {
                    "sent": "We are trying to do Rivers.",
                    "label": 0
                },
                {
                    "sent": "Given your observations, you want to figure out what caused those observations.",
                    "label": 0
                },
                {
                    "sent": "The top level plans.",
                    "label": 0
                },
                {
                    "sent": "So we found that VIPs cannot be used for plan recognition as is.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we extend their piece by using logical abduction to get our logical proofs instead of slef resolution VR, the resulting model, Bayesian abductive logic programs.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you're given a knowledge base auto BRB, understand of observation literals, we compute abductive proofs using stickers abduction algorithm.",
                    "label": 1
                },
                {
                    "sent": "So we watching on each observation literal until it can be proved or assumed.",
                    "label": 0
                },
                {
                    "sent": "So we say that a little is proved, or it can be true if it unifies with the fact in the knowledge base or if it unifies with the head of some rules in the knowledge base.",
                    "label": 1
                },
                {
                    "sent": "Otherwise we end up assuming the literal.",
                    "label": 0
                },
                {
                    "sent": "Once you have your logical.",
                    "label": 0
                },
                {
                    "sent": "Doctor Bruce, we construct the Bayesian network using the standard mechanism that's used in the LP's.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Someone explain this algorithm or this entire process with an example.",
                    "label": 0
                },
                {
                    "sent": "We'll go back to our example from intelligent user interfaces, so we have three top level plan predicates.",
                    "label": 1
                },
                {
                    "sent": "Copy file, move file and remove file and two action predicates.",
                    "label": 0
                },
                {
                    "sent": "Copy and remove commands.",
                    "label": 1
                },
                {
                    "sent": "Now the knowledge base has four rules.",
                    "label": 0
                },
                {
                    "sent": "It basically says that the copy command can be executed when the user is trying to either copy a file or move a file and the remove command is executed when the user is trying to move a file or remove file.",
                    "label": 0
                },
                {
                    "sent": "Let's say we observe copy test one dot T XT to my directory, remove test one dot EX.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did the first observation and be batching on it using the route shown here.",
                    "label": 0
                },
                {
                    "sent": "We find that it results in a subgroup copying files.",
                    "label": 0
                },
                {
                    "sent": "Test one dot T XT might retreat.",
                    "label": 0
                },
                {
                    "sent": "We find that we cannot prove this subgroup because it does not unify with any fact in the knowledge base, and it also does not unify with the head of any rule in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So we have to assume this little.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, back chain on the same observation using an alternate room.",
                    "label": 0
                },
                {
                    "sent": "Here again, we find that move file cannot be proved, so we end up assuming.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then go to the next observation, the Remove Command.",
                    "label": 0
                },
                {
                    "sent": "So when you back soon on the remove command we end up getting a subgoal move file, and we will have to assume it because we cannot prove it.",
                    "label": 0
                },
                {
                    "sent": "But instead of creating a new assumption we had already created the assumption in the previous step, so we end up reusing or matching to that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, back chain and remove command using the 4th root to get remove file as a sub goal which ends up being getting assume because we cannot prove.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this becomes the structure of the Bayesian network.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we need to specify the probabilistic parameters.",
                    "label": 0
                },
                {
                    "sent": "Need to combine the evidence coming from the conjunction.",
                    "label": 0
                },
                {
                    "sent": "The body of the clause.",
                    "label": 1
                },
                {
                    "sent": "We use the noisy and model.",
                    "label": 0
                },
                {
                    "sent": "But in some cases, like you saw in the example, you can have the same observation of.",
                    "label": 0
                },
                {
                    "sent": "Being generated or being observed.",
                    "label": 0
                },
                {
                    "sent": "Because of two or more, a top level plans.",
                    "label": 0
                },
                {
                    "sent": "In such cases we need to use the noisy or model to model the explaining away phenomenon.",
                    "label": 0
                },
                {
                    "sent": "People actually useful CPT is to specify these parameters, but if you end up using CPT's you will have many more parameters to estimate from the data by using the noisy or noisey and models you end up with parameters that are linear in the number of parents.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this example, as you can see, popping command is observed as part of copy file or MOV file.",
                    "label": 0
                },
                {
                    "sent": "So here we use the noisy or combining rule.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow, this similarly or for the remove command as well use ISR to combine the evidence coming from two different classes or two different plans.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have the structure and the parameters specified, people formed probabilistic evidence of probabilistic inference to get the best set of plans.",
                    "label": 0
                },
                {
                    "sent": "So in some domains we notice that there are several plans that can explain the set of observations in such domains, we compute most probable explanation given given the evidence.",
                    "label": 0
                },
                {
                    "sent": "So most probable explanation basically will give you truth values for all the unobserved nodes in the network.",
                    "label": 1
                },
                {
                    "sent": "Once you specify the evidence.",
                    "label": 0
                },
                {
                    "sent": "But in certain other domains, we know that there's a single correct land that best explains the observations.",
                    "label": 1
                },
                {
                    "sent": "In such domains, we compute marginal probability for plan instances and then pick the plan that has the highest probability.",
                    "label": 0
                },
                {
                    "sent": "We found that we could not use exact inference for some domains because it was intractable infest domains.",
                    "label": 0
                },
                {
                    "sent": "We used sample search and approximate sampling algorithm that is specifically designed for graphical models with deterministic constraints.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your network you specify your evidence.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You specify equating radials.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And perform problems with big influence.",
                    "label": 0
                },
                {
                    "sent": "Now based on me.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we find this move file is the plan that best explains these observe.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can learn these noisy or noisy and parameters using the EM algorithm that has been adapted for beardies, blacklisting and direct.",
                    "label": 1
                },
                {
                    "sent": "We found that in plan recognition is always partial observability.",
                    "label": 0
                },
                {
                    "sent": "During learning you have evidence for the observations and evidence for the top level plans, but then there are these nodes which represent subgoals noisy or noisey, and loads for which you don't have any evidence available.",
                    "label": 1
                },
                {
                    "sent": "So for this reason we felt that EM was suitable for learning these parameters.",
                    "label": 0
                },
                {
                    "sent": "We have a simplified our problem by learning only the noisy or parameters, so to combine the evidence coming from the conjunction, the body of the clause.",
                    "label": 0
                },
                {
                    "sent": "We use the deterministic logical and model and stuff noisy and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People bad, we experimentally evaluated our approach on three different datasets.",
                    "label": 0
                },
                {
                    "sent": "Monroe, coming through strategic planning Linux from intelligent user interfaces and stories.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Someone who I'm Linux.",
                    "label": 0
                },
                {
                    "sent": "Our datasets created by Layla and Alan Monroe is an artificially generated data set, and in this domain the task involves recognizing the top level plans in an emergency response domain.",
                    "label": 1
                },
                {
                    "sent": "Now, Linux is a data set that was collected based on all the Linux commands that were executed by real human users, so this is a real data set and here the task involves finding the top level plan.",
                    "label": 1
                },
                {
                    "sent": "Based on these Linux commands like how we signed the example.",
                    "label": 1
                },
                {
                    "sent": "And in both these domains, the single correct plan in each example.",
                    "label": 0
                },
                {
                    "sent": "So here's a quick summary of these datasets.",
                    "label": 0
                },
                {
                    "sent": "Monroe being an artificially generated data set, you have many more examples, and there are more observations, for example.",
                    "label": 0
                },
                {
                    "sent": "However, there are fewer top level plan predicates and fewer observed action predicates.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the methodology for both these domains, we manually encoded the knowledge base.",
                    "label": 1
                },
                {
                    "sent": "We learned the parameters using the EM algorithm and we computed marginal probability for plan instances because there was a single correct plan for each example.",
                    "label": 1
                },
                {
                    "sent": "We compare bars with Emily and Hedge can.",
                    "label": 0
                },
                {
                    "sent": "That's the latest Eminem based approach for plan recognition and the system by Blaylock and added which is based on engram statistical N gram models.",
                    "label": 0
                },
                {
                    "sent": "Views conversion score of this is a metric to find out later canal and this basically measures the fraction of examples for which the plant predicate was predicted correctly.",
                    "label": 1
                },
                {
                    "sent": "Keep in mind that this metric does not really account for predicting the argument.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Select so in Monroe based on convergence cool, we find that buys outperforms both immigrants and laylock and added start indicates that the differences are statistically SIG.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, on Linux we find that balance is outperforming these two.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great, but we find that the convergence code is not perfect.",
                    "label": 0
                },
                {
                    "sent": "Two reasons.",
                    "label": 0
                },
                {
                    "sent": "One, it does not account for predicting the arguments correctly.",
                    "label": 1
                },
                {
                    "sent": "Typically in a plan recognition system is not sufficient if you just predict the top level plan predicate correctly.",
                    "label": 0
                },
                {
                    "sent": "You would also want to know the arguments and the convergence core does not capture that.",
                    "label": 0
                },
                {
                    "sent": "Secondly, this does not test or this cannot be used to evaluate the ability of a plan recognition system to perform early plan recognition.",
                    "label": 0
                },
                {
                    "sent": "That is, when the plan recognition system has seen fewer observations early on.",
                    "label": 0
                },
                {
                    "sent": "Because in order to use the convergence Corda Plan definition, system has to see all the observations.",
                    "label": 1
                },
                {
                    "sent": "So For these reasons, we design these additional experiments for early plan recognition.",
                    "label": 1
                },
                {
                    "sent": "Specifically, we performed plan recognition after seeing the 1st 20, five, 5075 and 100% of the observations and they.",
                    "label": 0
                },
                {
                    "sent": "Then we computed accuracy.",
                    "label": 1
                },
                {
                    "sent": "So what we did was we first saw if the plan predicate was predicted correctly.",
                    "label": 0
                },
                {
                    "sent": "Given that the plan predicate is predicted correctly now, you see if a subset of these arguments are predicted correctly and we assign partial credit.",
                    "label": 0
                },
                {
                    "sent": "Now if both the plan predicate and all the arguments are predicted correctly, then you get an accuracy of 100% or one depending on how you want to assign it.",
                    "label": 0
                },
                {
                    "sent": "We compared bots with Emily and hedge Cam or I would like to point out that some of these results were entered in his camp are not published yet, but we have access to the latest results and we thought we might as well compare them to the latest results.",
                    "label": 0
                },
                {
                    "sent": "We could not compare our results with the the system badly Lochan Island because of the way they had conducted there.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm on the road.",
                    "label": 0
                },
                {
                    "sent": "We find that when when the systems have seen just 25% of the observations, you find that endurance, which is the red line eminence, have an advantage over balance.",
                    "label": 0
                },
                {
                    "sent": "But very quickly you see that Valves catches up with millions, and then we've seen all observations.",
                    "label": 0
                },
                {
                    "sent": "Bugs have slightly.",
                    "label": 0
                },
                {
                    "sent": "They have they have a little advantage over eminence, not a significant advantage, but flight.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bondage however, on Linux we find that throughout bars have a significant advantage over ambulance.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll quickly read through the 3rd and the final data set.",
                    "label": 0
                },
                {
                    "sent": "A better story understanding here.",
                    "label": 1
                },
                {
                    "sent": "The task involves predicting the agents top level plans based on the actions described in the narrative text.",
                    "label": 1
                },
                {
                    "sent": "This is an example of a domain that has multiple top level plans and this is a much smaller data set with 25 examples in the development.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detested.",
                    "label": 0
                },
                {
                    "sent": "He will be included.",
                    "label": 0
                },
                {
                    "sent": "We got a knowledge base that was created for access by Indian money and we could not learn the parameters because there were very few examples in the development set.",
                    "label": 1
                },
                {
                    "sent": "So we set them manually.",
                    "label": 0
                },
                {
                    "sent": "We we computed MPE to get the best set of plans.",
                    "label": 1
                },
                {
                    "sent": "We compared bars with MLNH Cam and access.",
                    "label": 0
                },
                {
                    "sent": "Axle has two different heuristics simplicity which explains that have the least least number of assumptions and actual guidance is a domain specific heuristic which is designed specifically for story under.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based on story understanding, define that bass is outperforming.",
                    "label": 1
                },
                {
                    "sent": "Everyone has Cameron eccentricity which are both general book was plan recognition systems with respect to precision recall and F mission.",
                    "label": 0
                },
                {
                    "sent": "But then when you look at actual currency, actual coherence outperforms all these systems.",
                    "label": 0
                },
                {
                    "sent": "But again, I'm still occurrences domain specific and we don't have the best incorporated in any of these.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So do any quickly summarize.",
                    "label": 0
                },
                {
                    "sent": "I have presented Bayesian abductive logic programs, an extension of beer, please for abductor plan recognition.",
                    "label": 1
                },
                {
                    "sent": "We've demonstrated that we can learn the model parameters automatically from data using here and finally our empirical results suggest that bags have advantages over existing approaches for planter.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "In future we would like to learn the abducted knowledge bases automatically from data and compare balls to other probabilistic logics like problem prison and pulls on abduction.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So if I understand correctly, your system doesn't have the notion of time sequence, yes, so in your example, what really makes you believe that the user wants to move something is the specific sequence, right?",
                    "label": 0
                },
                {
                    "sent": "Ideally you would like to have the time information.",
                    "label": 0
                },
                {
                    "sent": "This is where it all started, but definitely agree that going forward there should be a way in which we can incorporate the time dimension.",
                    "label": 0
                },
                {
                    "sent": "Into our system, maybe you know use event calculus or some kind of representation that can capture time and then be able to do this.",
                    "label": 0
                },
                {
                    "sent": "Place.",
                    "label": 0
                },
                {
                    "sent": "In some cases.",
                    "label": 0
                },
                {
                    "sent": "Most cases.",
                    "label": 0
                },
                {
                    "sent": "Do you?",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "So for the most part, we find that bags are outperforming MLN's.",
                    "label": 0
                },
                {
                    "sent": "But the only yeah, the only place where we found some.",
                    "label": 0
                },
                {
                    "sent": "Significant advantage is when you see very few observations.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately at this point or we don't have a clear intuition for why one is performing better than the other, because we're not using the same inference algorithm or the same learning and the models.",
                    "label": 0
                },
                {
                    "sent": "Even though MLN hedge Cam and valves have pretty much similar models, there are still you know differences.",
                    "label": 0
                },
                {
                    "sent": "But yeah, let's get open question and we are actually working on that.",
                    "label": 0
                },
                {
                    "sent": "If I understand correctly, in order to perform this action you have two or less numerate.",
                    "label": 0
                },
                {
                    "sent": "Many different possibilities through the adaptive knowledge base.",
                    "label": 0
                },
                {
                    "sent": "How does this compare in size to a forward?",
                    "label": 0
                },
                {
                    "sent": "For detection and.",
                    "label": 0
                },
                {
                    "sent": "For larger more difficult tasks.",
                    "label": 0
                },
                {
                    "sent": "So if I understand your question correctly, I think that I'd like to pass right one is.",
                    "label": 0
                },
                {
                    "sent": "How do you come up with alternative explanations?",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "That's True Detective.",
                    "label": 0
                },
                {
                    "sent": "So right now we are trying.",
                    "label": 0
                },
                {
                    "sent": "We come up with all possible abductor walkthroughs and we let the probabilistic inference engine figure out.",
                    "label": 0
                },
                {
                    "sent": "So in case of like in the third domain we computed NP so we can actually compute CNP, wherein we get K best explanations and when you are computing marginal probabilities, you can just compute marginal probability for all plan instances and just taking the plant that has the highest probability.",
                    "label": 0
                },
                {
                    "sent": "You can also look at, you know.",
                    "label": 0
                },
                {
                    "sent": "Plants that have slightly lower probability, so in that way you can compute alternative explanations.",
                    "label": 0
                },
                {
                    "sent": "And I thank you.",
                    "label": 0
                },
                {
                    "sent": "Second question was with respect to how would you use deduction to do this?",
                    "label": 0
                },
                {
                    "sent": "Based on what you said, if there if you had a very large model space with many unobservable variables.",
                    "label": 0
                },
                {
                    "sent": "So for that reason, you cannot use deduction or SMT resolution because you cannot prove prove in case of unobserved data you cannot prove those things, so for that reason use abduction and wherever we cannot prove a particular fact we end up assuming it.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you and running out.",
                    "label": 0
                }
            ]
        }
    }
}