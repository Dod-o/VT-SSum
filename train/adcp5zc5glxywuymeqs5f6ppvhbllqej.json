{
    "id": "adcp5zc5glxywuymeqs5f6ppvhbllqej",
    "title": "The Offset Tree for Learning with Partial Labels",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_langford_otlpl/",
    "segmentation": [
        [
            "OK, so this is the.",
            "Second half of the session, which is not too late in the first half of session, but I hope they find interesting anyways, so this is about the offset tree for learning with partial."
        ],
        [
            "So first of all, I need to tell you what a partial label is.",
            "So at Yahoo we have this kind of structure of what happens.",
            "We have some sort of search engine, for example and user with some interests comes to Yahoo and makes a query and then Yahoo chooses some ads display.",
            "Maybe it's actually multiple ads will just think about one for this talk and then the user is either quick something adder or does not, right?",
            "And if they click on the admin, Yahoo makes money right?",
            "So it's important to click on the ad.",
            "Other things, tricky courses that the users don't fill out a questionnaire or something saying what they're interested in before they come to Yahoo."
        ],
        [
            "So maybe one with a mathematical description of what goes on.",
            "So we're going to think about a world which chooses some features that could be the query.",
            "Or it could be something more complex in the query and also some some rewards for each of the different possible ads.",
            "So you have the reward of add one reward of add two and and so forth.",
            "And I guess you want to think about simply then you can just imagine that the rewards are either one or zero, right?",
            "Depending on whether not something we click on the ad.",
            "And then you have some policy which is going to choose a particular ad.",
            "And then.",
            "The world is going to reveal the reward of that particular ad.",
            "Right?",
            "OK, so this is a very simple, straightforward thing."
        ],
        [
            "And now our goal is going to be to find the policy which maximizes the expected reward.",
            "Right?",
            "OK, so now it's important to kind of understand where this model of the world lies between different things.",
            "So first of all, this is not supervised learning because we don't.",
            "The world does not reveal the rewards of all the different ADS world revealed.",
            "The reward of all the different ads is to just be kostiv classification and we would have some techniques and we would know how to deal with this."
        ],
        [
            "The other thing is that.",
            "This is not just a simple bandit problem because we have this X right?",
            "So this is this technology that people have developed for K armed bandits, where where there's no X otherwise looks exactly the same as this.",
            "And if you tried to apply that technology it would fail miserably because.",
            "You really need something which can, which isn't a new instance for every single set of every single feature value, right?",
            "Maybe if you were doing supervised learning, it would be just crazy for us too.",
            "For every value of the features, try to memorize with.",
            "The right solution is and that's what using abandoned solution might suggest.",
            "OK, so the other thing to think about is reinforcement learning.",
            "This is a simpler problem then reinforcement learning, so reinforcement learning is sort of a super general problem and this is much simpler because we're not imagining their actions actually affect our future observations.",
            "And maybe that's a fair approximation here because the ads that I see maybe don't affect ads that your behavior when you see ads, right?"
        ],
        [
            "OK, so.",
            "There's a core trick, which is is central to the paper, which is this offsetting trick.",
            "So what we're going to do is we're going to take a."
        ],
        [
            "Oh, actually go back one.",
            "Right so.",
            "Because we only see the reward of the action that we take, it is essential that we explore over the different actions right?",
            "And you can think about this in the extreme case, suppose you always just played one ad right then your hope for learning how to display or that you display some other ad is.",
            "Not going to be achieved if you only have experience with one ad you cannot really hope to predict how you're going to do well in another ad, so exploration is essential.",
            "An just for simplicity.",
            "For this talk, I'm going to imagine the exploration is done uniformly at random over the different actions, right?",
            "So you can use do other things as well, but.",
            "But it's simpler to talk about uniform exploration.",
            "OK, so whenever you see an ad chosen.",
            "The purpose of learning they give it as being chosen uniformly."
        ],
        [
            "OK, so it's easiest to explain things just when you have two actions.",
            "So suppose that we have two ads.",
            "So we're going to have a partial label sample, which consists of the features, the action that was taken, and the reward of that action.",
            "Right, and we don't know the reward of the other action.",
            "It could be one, or it could be 0.",
            "And now we're going to create a binary importance weighted.",
            "Supervised example right so?",
            "This is also going to be a three to pull, but the meaning of the tuples different, so we're going to have the features this is we had before.",
            "And then if the reward is greater than 1/2.",
            "We're going to have.",
            "We're going to go with the action that we saw.",
            "And.",
            "We're going to have an importance weight equal to difference between the reward and a half.",
            "And if the reward is less than 1/2, they're going to go with the other action.",
            "We're going to.",
            "The other action is the right action.",
            "And we're going to have an importance weight equal to 1/2 minus the reward.",
            "So this is the offsetting trick.",
            "The offset here is 1/2.",
            "And the claim is that this is a mechanism for reducing two importance weighted binary classification.",
            "So."
        ],
        [
            "Which we can then reduce to just binary classification using rejection sampling techniques.",
            "For example, right so important webinar classification is just like binary classification, except that we're just saying that you know some examples are a little bit more important to get correct than others, and we measure that just by waiting the examples in the sum to measure your accuracy according to the importance weights.",
            "Alright, so.",
            "So that's the offsetting trick.",
            "And maybe it's worth."
        ],
        [
            "Well too.",
            "So this gives us a mechanism for transforming examples from the partial label problem to a binary importance weight problem and then to a binary classification problem, and then we can apply our favorite binary classification algorithm now."
        ],
        [
            "Maybe it's important to think a little bit about.",
            "But what this means?",
            "So we have a distribution.",
            "Creating these binary examples right so.",
            "The way we draw from the distribution is that we first draw from the partial label problem, and then we choose a random action A.",
            "And then with some probability.",
            "We're going to either go with a binary example X, A, or going to be example X, a bar.",
            "OK, so the claim is that using this offset.",
            "Helps remove noise from the problem."
        ],
        [
            "And then there's a few examples which I think really help exercise your intuition for this.",
            "So suppose that the reward of.",
            "One action was a half and the order the other action was one, so let me say that I'm imagining that all rewards are in the interval from zero to 1, just as a initial normalization.",
            "OK, so if it's if the reward of the first action is a half a second example action is 1.",
            "Then examples from class one are going to have weight 0.",
            "Examples from Class 2 are going to have weight.",
            "Well, it will be 1/2 but after we normalize it will be one.",
            "So what that's saying is that.",
            "Every example that you create.",
            "For your binary classification, will say that you should go with Class 2.",
            "Right?",
            "And then suppose that the reward of class one is 0.",
            "Then reward a Class 2 is 1.",
            "So this is this is going to mention case.",
            "So all examples again are going to say that you are that you should go with Class 2.",
            "Right, because if the random action is Class 2, then you say you should go with Class 2 and if the random action is class one, then you're going to be.",
            "In this case here and it's going to say that you should go with Class 2.",
            "OK, so now this is kind of in between case here.",
            "What were you actually creating a noisy problem?",
            "So.",
            "The reward of class one is .75.",
            "The reward of Class 2 is 1.",
            "Then the induced distribution on label one versus 2 is going to be 1/3 in class one and 2/3 in Class 2.",
            "So if you made the offset be zero, which you could, you could certainly do.",
            "It would be a valid thing to do.",
            "Then you'd have a noisier problem.",
            "Noisier induced problem here.",
            "A noisier induced problem here and you would have a problem here with no noise again, but you would have half as many examples.",
            "So we're sort of for free out of this offsetting trick, doubling our number of examples in this case."
        ],
        [
            "OK, so.",
            "We would like to analyze this.",
            "We want to know that we're doing something sane.",
            "The way that we tried to analyze this is using a reduction style analysis.",
            "So we have this induced distribution D prime.",
            "And we're going to look at the regret, which is how well we do in comparison to how well we could have done, right?",
            "So we have the 01 classification loss.",
            "Respected the prime overlearned classifier minus the best possible 01 classification loss.",
            "And then we also are interested in how well it's possible to perform on our original problem, right?",
            "So we're going to look at the regret for the policy that we learn.",
            "So regret for the policy that we learn is going to be the difference in rewards between the best possible policy, F star and our policy F. OK, so the claim that you can prove is that.",
            "For every two label 2 Action partial label problem D and every binary classifier F. The 01 regret upper bounds of policy regret.",
            "And this is sort of the best you can imagine being possible because.",
            "Because you can prove that you can't do better than that, and it's somewhat amazing that we don't have.",
            "We don't have a constant here, we don't have or some sort of mean.",
            "This is a very good theorem."
        ],
        [
            "OK, so.",
            "It's gone, so suppose that we have more than two actions.",
            "Then the question is how do we?",
            "How do we use this trick in order to deal with more than two actions?",
            "And the answer is that you can use it in a tree like structure, which also has some very nice computational properties as well.",
            "So we can create a tournament over the different actions and we can.",
            "Predict we can learn a classifier at each node in the tournament according to that offsetting trick, which I just described.",
            "So.",
            "The one thing which is tricky is that we need to train the internal nodes of this tree.",
            "To predict the winner of three versus 4 versus the winner of 1 versus 2.",
            "So we need to be a little bit careful in how we actually train these internal nodes, and I'll explain how we can be careful in training the intern."
        ],
        [
            "All notes.",
            "So suppose that we took action three randomly.",
            "Then we got some particular reward.",
            "Then we would create an example for this node here first.",
            "And then we would train at that node."
        ],
        [
            "And then.",
            "Conditioned on this classifier choosing action three, we create a new example here.",
            "OK, so that conditioning part is important because if you don't have a conditioning part, this method is not even consistent.",
            "And that should be a little bit intuitive, because if you don't condition on 3 versus 4."
        ],
        [
            "Then you end up not comparing the winner of 1 versus 2 to the winner of three versus 4, but rather you compare.",
            "One versus 2.",
            "2 three versus 4 and you don't want to know, sort of the average reward of going One Direction versus the other.",
            "What you want to know is that the reward that you will actually achieve if you go One Direction versus the other."
        ],
        [
            "OK, so so you have this little conditioning check here, and if you don't if this classifier doesn't actually agree with direction 3, then then you just you don't pass on the exam."
        ],
        [
            "And then you do the same thing at the root.",
            "OK, so this is a.",
            "Basic simple trick which makes sure that you have a consistent.",
            "Method for training to predict one of K different actions and.",
            "You can think about this as either working with a batch or an online classifier.",
            "It's kind of orthogonal to the whole batch versus online distinction, right?",
            "Because you could use a batch classifier at the nodes and you just train at the leaf words nodes, and then if the nodes next and the nodes next.",
            "Or you could do an online classifier and you could be doing this whole process online."
        ],
        [
            "OK, So what can we prove about this?",
            "OK, so we have a more complex distribution over binary examples that we create.",
            "Right and then?"
        ],
        [
            "You can kind of imagine how that happens.",
            "We create a binary example at the first internal node as described before.",
            "And then conditioned on what happens, we may create one here and then we create one there and then we draw uniform random from those binary examples."
        ],
        [
            "OK, so now we're going to have a binary classifier.",
            "Each of the nodes.",
            "And then we're going to have an induced offset tree policy.",
            "Its price of ETH.",
            "And then what you can prove is that.",
            "Uh huh.",
            "K -- 1 the number of actions minus one.",
            "Times the average binary regret.",
            "Upper bound your policy regret.",
            "And at this point we kind of go darn because because we have this depends on K -- 1.",
            "We would not like to depend upon the number of actions.",
            "So we scratch our heads and we think for a while and it turns out that you can actually do better.",
            "You can prove that no reduction has a better analysis.",
            "And this this K -- 1 is.",
            "If you look at the lower bound you see that it's inherently related to the fact that we're only observing the reward of 1 out of K -- 1 choices.",
            "So we have optimally used the information which is available when we randomly.",
            "We only have one of these partial label problems."
        ],
        [
            "OK, so I've described you are method for doing this.",
            "There's a couple of other methods which you should be aware of.",
            "This is a very simple one, which is you just condition on the action.",
            "Predict the expected reward.",
            "Right, you can just trainer aggressor to do that and then you predict, according to an argmax policy.",
            "Argmax over the aggressor and other approaches in Bianca's thesis, Nadia, there is that you reduce to importance weighted multiclass classification and then you know how to solve importance.",
            "Weighted multiclass classification.",
            "So you're going to do an analysis for these approaches similar to the one that we did for the."
        ],
        [
            "Upset tree and when you do you get get some so these are.",
            "These are the upper bounds on the policy.",
            "Regret.",
            "OK, So what does this say?",
            "So this is the offset tree.",
            "You have K -- 1 times the binary regret bounds of policy regret.",
            "And then there's the importance weighted classification approach, where you have 4K times the binary regret bounds of policy regret.",
            "Is so four a little bit worse than one, but it's not so much worse.",
            "And then for argmax regression, you have two K times the binary regret bounds policy, regret.",
            "OK, so some of you are thinking that square root is good, but actually it's bad.",
            "Because remember, we're thinking about rewards which are in the interval from zero to 1.",
            "Alright, so if this quantity here is larger than one, that means that we lossed already.",
            "Right, so when we're less than one, then of course covered increases the size of the number, so this is this is a worst analysis then these down here."
        ],
        [
            "So we can go and we can compare these different approaches.",
            "Anne Anne.",
            "Alright, So what happened so?",
            "What we did was we grabbed a bunch of UCI datasets.",
            "And.",
            "These are zero multiclass datasets, so we're going to pretend that we just have a partial label problem.",
            "So the basic difficulty with research in this area is that.",
            "Is it?",
            "It's rather difficult to actually.",
            "On the one hand, it's very clear that a lot of problems are partial problems, but on the other hand, because it partially problems, it's pretty hard to get ahold of data that you need in order to do in the direct evaluation of a method.",
            "So we're going to do in valuation by simulation.",
            "Simulation is we have a multi class data set and we would just blow away.",
            "We pretend that it's a partial label data set, meaning that we can randomly pro for the loss of a particular choice on each example right?",
            "So instead of knowing that the label is 3?",
            "We know that the loss of label four is 1.",
            "On a particular example.",
            "OK, so.",
            "On the training set we treated this way and then on the test that we just evaluate to see what our average losses.",
            "And I guess we can try because what I described to you is a reduction.",
            "It composes with different mechanisms for regression or classification and we can try different mechanisms for regression classification and you can see that.",
            "So this far graph is the comparison between the argmax approach.",
            "And the offset tree approach.",
            "So conclusion.",
            "We do much better here.",
            "See everything below the line.",
            "Little bit more mixed here."
        ],
        [
            "There was another algorithm released called the Bandit ran a little while ago, which is just for the realizable case, where one the choice one of the choices reward one and the rest of reward 0.",
            "And we do much better there as well, so you should compare this to this."
        ],
        [
            "Thank you.",
            "Look for quick question.",
            "So your analysis here was assuming the rewards were in the 01.",
            "Yeah, so you can have some negative reports.",
            "Or for example you display a silly ad, you can add the user paying attention to ads, right?",
            "Does it go through with the negative rewards as well?",
            "Yeah, so in general you can adjust the rewards by any affine transformation.",
            "So is there any hope for extending it to the case of material selection?",
            "So in the case where we have to play multiple ads basically seems like you need to modify both the tournament and the regret bounds.",
            "Yeah, there's no mathematical problem with extending to accommodate Oriel Dingus latest interaction.",
            "Rather than add the difficulty is dependent on K, right?",
            "So and that's a challenging aspect of things, you need to think about ways to reduce the dependence on K. The text."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                },
                {
                    "sent": "Second half of the session, which is not too late in the first half of session, but I hope they find interesting anyways, so this is about the offset tree for learning with partial.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, I need to tell you what a partial label is.",
                    "label": 0
                },
                {
                    "sent": "So at Yahoo we have this kind of structure of what happens.",
                    "label": 0
                },
                {
                    "sent": "We have some sort of search engine, for example and user with some interests comes to Yahoo and makes a query and then Yahoo chooses some ads display.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's actually multiple ads will just think about one for this talk and then the user is either quick something adder or does not, right?",
                    "label": 0
                },
                {
                    "sent": "And if they click on the admin, Yahoo makes money right?",
                    "label": 0
                },
                {
                    "sent": "So it's important to click on the ad.",
                    "label": 0
                },
                {
                    "sent": "Other things, tricky courses that the users don't fill out a questionnaire or something saying what they're interested in before they come to Yahoo.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe one with a mathematical description of what goes on.",
                    "label": 1
                },
                {
                    "sent": "So we're going to think about a world which chooses some features that could be the query.",
                    "label": 0
                },
                {
                    "sent": "Or it could be something more complex in the query and also some some rewards for each of the different possible ads.",
                    "label": 0
                },
                {
                    "sent": "So you have the reward of add one reward of add two and and so forth.",
                    "label": 0
                },
                {
                    "sent": "And I guess you want to think about simply then you can just imagine that the rewards are either one or zero, right?",
                    "label": 0
                },
                {
                    "sent": "Depending on whether not something we click on the ad.",
                    "label": 0
                },
                {
                    "sent": "And then you have some policy which is going to choose a particular ad.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 1
                },
                {
                    "sent": "The world is going to reveal the reward of that particular ad.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very simple, straightforward thing.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now our goal is going to be to find the policy which maximizes the expected reward.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so now it's important to kind of understand where this model of the world lies between different things.",
                    "label": 0
                },
                {
                    "sent": "So first of all, this is not supervised learning because we don't.",
                    "label": 1
                },
                {
                    "sent": "The world does not reveal the rewards of all the different ADS world revealed.",
                    "label": 0
                },
                {
                    "sent": "The reward of all the different ads is to just be kostiv classification and we would have some techniques and we would know how to deal with this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing is that.",
                    "label": 0
                },
                {
                    "sent": "This is not just a simple bandit problem because we have this X right?",
                    "label": 0
                },
                {
                    "sent": "So this is this technology that people have developed for K armed bandits, where where there's no X otherwise looks exactly the same as this.",
                    "label": 0
                },
                {
                    "sent": "And if you tried to apply that technology it would fail miserably because.",
                    "label": 0
                },
                {
                    "sent": "You really need something which can, which isn't a new instance for every single set of every single feature value, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe if you were doing supervised learning, it would be just crazy for us too.",
                    "label": 0
                },
                {
                    "sent": "For every value of the features, try to memorize with.",
                    "label": 0
                },
                {
                    "sent": "The right solution is and that's what using abandoned solution might suggest.",
                    "label": 0
                },
                {
                    "sent": "OK, so the other thing to think about is reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "This is a simpler problem then reinforcement learning, so reinforcement learning is sort of a super general problem and this is much simpler because we're not imagining their actions actually affect our future observations.",
                    "label": 0
                },
                {
                    "sent": "And maybe that's a fair approximation here because the ads that I see maybe don't affect ads that your behavior when you see ads, right?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's a core trick, which is is central to the paper, which is this offsetting trick.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to take a.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, actually go back one.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Because we only see the reward of the action that we take, it is essential that we explore over the different actions right?",
                    "label": 0
                },
                {
                    "sent": "And you can think about this in the extreme case, suppose you always just played one ad right then your hope for learning how to display or that you display some other ad is.",
                    "label": 0
                },
                {
                    "sent": "Not going to be achieved if you only have experience with one ad you cannot really hope to predict how you're going to do well in another ad, so exploration is essential.",
                    "label": 0
                },
                {
                    "sent": "An just for simplicity.",
                    "label": 0
                },
                {
                    "sent": "For this talk, I'm going to imagine the exploration is done uniformly at random over the different actions, right?",
                    "label": 0
                },
                {
                    "sent": "So you can use do other things as well, but.",
                    "label": 0
                },
                {
                    "sent": "But it's simpler to talk about uniform exploration.",
                    "label": 0
                },
                {
                    "sent": "OK, so whenever you see an ad chosen.",
                    "label": 0
                },
                {
                    "sent": "The purpose of learning they give it as being chosen uniformly.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's easiest to explain things just when you have two actions.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we have two ads.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have a partial label sample, which consists of the features, the action that was taken, and the reward of that action.",
                    "label": 1
                },
                {
                    "sent": "Right, and we don't know the reward of the other action.",
                    "label": 0
                },
                {
                    "sent": "It could be one, or it could be 0.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to create a binary importance weighted.",
                    "label": 1
                },
                {
                    "sent": "Supervised example right so?",
                    "label": 0
                },
                {
                    "sent": "This is also going to be a three to pull, but the meaning of the tuples different, so we're going to have the features this is we had before.",
                    "label": 0
                },
                {
                    "sent": "And then if the reward is greater than 1/2.",
                    "label": 0
                },
                {
                    "sent": "We're going to have.",
                    "label": 0
                },
                {
                    "sent": "We're going to go with the action that we saw.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We're going to have an importance weight equal to difference between the reward and a half.",
                    "label": 0
                },
                {
                    "sent": "And if the reward is less than 1/2, they're going to go with the other action.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "The other action is the right action.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have an importance weight equal to 1/2 minus the reward.",
                    "label": 1
                },
                {
                    "sent": "So this is the offsetting trick.",
                    "label": 0
                },
                {
                    "sent": "The offset here is 1/2.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that this is a mechanism for reducing two importance weighted binary classification.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which we can then reduce to just binary classification using rejection sampling techniques.",
                    "label": 1
                },
                {
                    "sent": "For example, right so important webinar classification is just like binary classification, except that we're just saying that you know some examples are a little bit more important to get correct than others, and we measure that just by waiting the examples in the sum to measure your accuracy according to the importance weights.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So that's the offsetting trick.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's worth.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well too.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a mechanism for transforming examples from the partial label problem to a binary importance weight problem and then to a binary classification problem, and then we can apply our favorite binary classification algorithm now.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe it's important to think a little bit about.",
                    "label": 0
                },
                {
                    "sent": "But what this means?",
                    "label": 0
                },
                {
                    "sent": "So we have a distribution.",
                    "label": 0
                },
                {
                    "sent": "Creating these binary examples right so.",
                    "label": 0
                },
                {
                    "sent": "The way we draw from the distribution is that we first draw from the partial label problem, and then we choose a random action A.",
                    "label": 0
                },
                {
                    "sent": "And then with some probability.",
                    "label": 0
                },
                {
                    "sent": "We're going to either go with a binary example X, A, or going to be example X, a bar.",
                    "label": 1
                },
                {
                    "sent": "OK, so the claim is that using this offset.",
                    "label": 0
                },
                {
                    "sent": "Helps remove noise from the problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's a few examples which I think really help exercise your intuition for this.",
                    "label": 0
                },
                {
                    "sent": "So suppose that the reward of.",
                    "label": 0
                },
                {
                    "sent": "One action was a half and the order the other action was one, so let me say that I'm imagining that all rewards are in the interval from zero to 1, just as a initial normalization.",
                    "label": 0
                },
                {
                    "sent": "OK, so if it's if the reward of the first action is a half a second example action is 1.",
                    "label": 0
                },
                {
                    "sent": "Then examples from class one are going to have weight 0.",
                    "label": 0
                },
                {
                    "sent": "Examples from Class 2 are going to have weight.",
                    "label": 0
                },
                {
                    "sent": "Well, it will be 1/2 but after we normalize it will be one.",
                    "label": 0
                },
                {
                    "sent": "So what that's saying is that.",
                    "label": 0
                },
                {
                    "sent": "Every example that you create.",
                    "label": 0
                },
                {
                    "sent": "For your binary classification, will say that you should go with Class 2.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And then suppose that the reward of class one is 0.",
                    "label": 0
                },
                {
                    "sent": "Then reward a Class 2 is 1.",
                    "label": 0
                },
                {
                    "sent": "So this is this is going to mention case.",
                    "label": 0
                },
                {
                    "sent": "So all examples again are going to say that you are that you should go with Class 2.",
                    "label": 0
                },
                {
                    "sent": "Right, because if the random action is Class 2, then you say you should go with Class 2 and if the random action is class one, then you're going to be.",
                    "label": 0
                },
                {
                    "sent": "In this case here and it's going to say that you should go with Class 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this is kind of in between case here.",
                    "label": 0
                },
                {
                    "sent": "What were you actually creating a noisy problem?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The reward of class one is .75.",
                    "label": 0
                },
                {
                    "sent": "The reward of Class 2 is 1.",
                    "label": 0
                },
                {
                    "sent": "Then the induced distribution on label one versus 2 is going to be 1/3 in class one and 2/3 in Class 2.",
                    "label": 0
                },
                {
                    "sent": "So if you made the offset be zero, which you could, you could certainly do.",
                    "label": 0
                },
                {
                    "sent": "It would be a valid thing to do.",
                    "label": 0
                },
                {
                    "sent": "Then you'd have a noisier problem.",
                    "label": 0
                },
                {
                    "sent": "Noisier induced problem here.",
                    "label": 0
                },
                {
                    "sent": "A noisier induced problem here and you would have a problem here with no noise again, but you would have half as many examples.",
                    "label": 0
                },
                {
                    "sent": "So we're sort of for free out of this offsetting trick, doubling our number of examples in this case.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We would like to analyze this.",
                    "label": 0
                },
                {
                    "sent": "We want to know that we're doing something sane.",
                    "label": 0
                },
                {
                    "sent": "The way that we tried to analyze this is using a reduction style analysis.",
                    "label": 0
                },
                {
                    "sent": "So we have this induced distribution D prime.",
                    "label": 0
                },
                {
                    "sent": "And we're going to look at the regret, which is how well we do in comparison to how well we could have done, right?",
                    "label": 0
                },
                {
                    "sent": "So we have the 01 classification loss.",
                    "label": 0
                },
                {
                    "sent": "Respected the prime overlearned classifier minus the best possible 01 classification loss.",
                    "label": 0
                },
                {
                    "sent": "And then we also are interested in how well it's possible to perform on our original problem, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to look at the regret for the policy that we learn.",
                    "label": 0
                },
                {
                    "sent": "So regret for the policy that we learn is going to be the difference in rewards between the best possible policy, F star and our policy F. OK, so the claim that you can prove is that.",
                    "label": 0
                },
                {
                    "sent": "For every two label 2 Action partial label problem D and every binary classifier F. The 01 regret upper bounds of policy regret.",
                    "label": 1
                },
                {
                    "sent": "And this is sort of the best you can imagine being possible because.",
                    "label": 0
                },
                {
                    "sent": "Because you can prove that you can't do better than that, and it's somewhat amazing that we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have a constant here, we don't have or some sort of mean.",
                    "label": 0
                },
                {
                    "sent": "This is a very good theorem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "It's gone, so suppose that we have more than two actions.",
                    "label": 0
                },
                {
                    "sent": "Then the question is how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we use this trick in order to deal with more than two actions?",
                    "label": 0
                },
                {
                    "sent": "And the answer is that you can use it in a tree like structure, which also has some very nice computational properties as well.",
                    "label": 0
                },
                {
                    "sent": "So we can create a tournament over the different actions and we can.",
                    "label": 0
                },
                {
                    "sent": "Predict we can learn a classifier at each node in the tournament according to that offsetting trick, which I just described.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "The one thing which is tricky is that we need to train the internal nodes of this tree.",
                    "label": 0
                },
                {
                    "sent": "To predict the winner of three versus 4 versus the winner of 1 versus 2.",
                    "label": 1
                },
                {
                    "sent": "So we need to be a little bit careful in how we actually train these internal nodes, and I'll explain how we can be careful in training the intern.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All notes.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we took action three randomly.",
                    "label": 0
                },
                {
                    "sent": "Then we got some particular reward.",
                    "label": 0
                },
                {
                    "sent": "Then we would create an example for this node here first.",
                    "label": 0
                },
                {
                    "sent": "And then we would train at that node.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on this classifier choosing action three, we create a new example here.",
                    "label": 1
                },
                {
                    "sent": "OK, so that conditioning part is important because if you don't have a conditioning part, this method is not even consistent.",
                    "label": 0
                },
                {
                    "sent": "And that should be a little bit intuitive, because if you don't condition on 3 versus 4.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then you end up not comparing the winner of 1 versus 2 to the winner of three versus 4, but rather you compare.",
                    "label": 1
                },
                {
                    "sent": "One versus 2.",
                    "label": 0
                },
                {
                    "sent": "2 three versus 4 and you don't want to know, sort of the average reward of going One Direction versus the other.",
                    "label": 0
                },
                {
                    "sent": "What you want to know is that the reward that you will actually achieve if you go One Direction versus the other.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so you have this little conditioning check here, and if you don't if this classifier doesn't actually agree with direction 3, then then you just you don't pass on the exam.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you do the same thing at the root.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a.",
                    "label": 0
                },
                {
                    "sent": "Basic simple trick which makes sure that you have a consistent.",
                    "label": 0
                },
                {
                    "sent": "Method for training to predict one of K different actions and.",
                    "label": 0
                },
                {
                    "sent": "You can think about this as either working with a batch or an online classifier.",
                    "label": 0
                },
                {
                    "sent": "It's kind of orthogonal to the whole batch versus online distinction, right?",
                    "label": 0
                },
                {
                    "sent": "Because you could use a batch classifier at the nodes and you just train at the leaf words nodes, and then if the nodes next and the nodes next.",
                    "label": 0
                },
                {
                    "sent": "Or you could do an online classifier and you could be doing this whole process online.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what can we prove about this?",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a more complex distribution over binary examples that we create.",
                    "label": 0
                },
                {
                    "sent": "Right and then?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can kind of imagine how that happens.",
                    "label": 0
                },
                {
                    "sent": "We create a binary example at the first internal node as described before.",
                    "label": 0
                },
                {
                    "sent": "And then conditioned on what happens, we may create one here and then we create one there and then we draw uniform random from those binary examples.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we're going to have a binary classifier.",
                    "label": 0
                },
                {
                    "sent": "Each of the nodes.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have an induced offset tree policy.",
                    "label": 1
                },
                {
                    "sent": "Its price of ETH.",
                    "label": 0
                },
                {
                    "sent": "And then what you can prove is that.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "K -- 1 the number of actions minus one.",
                    "label": 0
                },
                {
                    "sent": "Times the average binary regret.",
                    "label": 0
                },
                {
                    "sent": "Upper bound your policy regret.",
                    "label": 0
                },
                {
                    "sent": "And at this point we kind of go darn because because we have this depends on K -- 1.",
                    "label": 0
                },
                {
                    "sent": "We would not like to depend upon the number of actions.",
                    "label": 0
                },
                {
                    "sent": "So we scratch our heads and we think for a while and it turns out that you can actually do better.",
                    "label": 1
                },
                {
                    "sent": "You can prove that no reduction has a better analysis.",
                    "label": 1
                },
                {
                    "sent": "And this this K -- 1 is.",
                    "label": 0
                },
                {
                    "sent": "If you look at the lower bound you see that it's inherently related to the fact that we're only observing the reward of 1 out of K -- 1 choices.",
                    "label": 0
                },
                {
                    "sent": "So we have optimally used the information which is available when we randomly.",
                    "label": 1
                },
                {
                    "sent": "We only have one of these partial label problems.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I've described you are method for doing this.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of other methods which you should be aware of.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple one, which is you just condition on the action.",
                    "label": 0
                },
                {
                    "sent": "Predict the expected reward.",
                    "label": 0
                },
                {
                    "sent": "Right, you can just trainer aggressor to do that and then you predict, according to an argmax policy.",
                    "label": 0
                },
                {
                    "sent": "Argmax over the aggressor and other approaches in Bianca's thesis, Nadia, there is that you reduce to importance weighted multiclass classification and then you know how to solve importance.",
                    "label": 0
                },
                {
                    "sent": "Weighted multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "So you're going to do an analysis for these approaches similar to the one that we did for the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Upset tree and when you do you get get some so these are.",
                    "label": 1
                },
                {
                    "sent": "These are the upper bounds on the policy.",
                    "label": 0
                },
                {
                    "sent": "Regret.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does this say?",
                    "label": 0
                },
                {
                    "sent": "So this is the offset tree.",
                    "label": 1
                },
                {
                    "sent": "You have K -- 1 times the binary regret bounds of policy regret.",
                    "label": 0
                },
                {
                    "sent": "And then there's the importance weighted classification approach, where you have 4K times the binary regret bounds of policy regret.",
                    "label": 0
                },
                {
                    "sent": "Is so four a little bit worse than one, but it's not so much worse.",
                    "label": 0
                },
                {
                    "sent": "And then for argmax regression, you have two K times the binary regret bounds policy, regret.",
                    "label": 1
                },
                {
                    "sent": "OK, so some of you are thinking that square root is good, but actually it's bad.",
                    "label": 0
                },
                {
                    "sent": "Because remember, we're thinking about rewards which are in the interval from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if this quantity here is larger than one, that means that we lossed already.",
                    "label": 0
                },
                {
                    "sent": "Right, so when we're less than one, then of course covered increases the size of the number, so this is this is a worst analysis then these down here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can go and we can compare these different approaches.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what happened so?",
                    "label": 0
                },
                {
                    "sent": "What we did was we grabbed a bunch of UCI datasets.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These are zero multiclass datasets, so we're going to pretend that we just have a partial label problem.",
                    "label": 0
                },
                {
                    "sent": "So the basic difficulty with research in this area is that.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "It's rather difficult to actually.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, it's very clear that a lot of problems are partial problems, but on the other hand, because it partially problems, it's pretty hard to get ahold of data that you need in order to do in the direct evaluation of a method.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do in valuation by simulation.",
                    "label": 1
                },
                {
                    "sent": "Simulation is we have a multi class data set and we would just blow away.",
                    "label": 0
                },
                {
                    "sent": "We pretend that it's a partial label data set, meaning that we can randomly pro for the loss of a particular choice on each example right?",
                    "label": 0
                },
                {
                    "sent": "So instead of knowing that the label is 3?",
                    "label": 0
                },
                {
                    "sent": "We know that the loss of label four is 1.",
                    "label": 0
                },
                {
                    "sent": "On a particular example.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "On the training set we treated this way and then on the test that we just evaluate to see what our average losses.",
                    "label": 0
                },
                {
                    "sent": "And I guess we can try because what I described to you is a reduction.",
                    "label": 0
                },
                {
                    "sent": "It composes with different mechanisms for regression or classification and we can try different mechanisms for regression classification and you can see that.",
                    "label": 0
                },
                {
                    "sent": "So this far graph is the comparison between the argmax approach.",
                    "label": 0
                },
                {
                    "sent": "And the offset tree approach.",
                    "label": 1
                },
                {
                    "sent": "So conclusion.",
                    "label": 0
                },
                {
                    "sent": "We do much better here.",
                    "label": 0
                },
                {
                    "sent": "See everything below the line.",
                    "label": 0
                },
                {
                    "sent": "Little bit more mixed here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There was another algorithm released called the Bandit ran a little while ago, which is just for the realizable case, where one the choice one of the choices reward one and the rest of reward 0.",
                    "label": 0
                },
                {
                    "sent": "And we do much better there as well, so you should compare this to this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Look for quick question.",
                    "label": 0
                },
                {
                    "sent": "So your analysis here was assuming the rewards were in the 01.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can have some negative reports.",
                    "label": 0
                },
                {
                    "sent": "Or for example you display a silly ad, you can add the user paying attention to ads, right?",
                    "label": 0
                },
                {
                    "sent": "Does it go through with the negative rewards as well?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in general you can adjust the rewards by any affine transformation.",
                    "label": 0
                },
                {
                    "sent": "So is there any hope for extending it to the case of material selection?",
                    "label": 0
                },
                {
                    "sent": "So in the case where we have to play multiple ads basically seems like you need to modify both the tournament and the regret bounds.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's no mathematical problem with extending to accommodate Oriel Dingus latest interaction.",
                    "label": 0
                },
                {
                    "sent": "Rather than add the difficulty is dependent on K, right?",
                    "label": 0
                },
                {
                    "sent": "So and that's a challenging aspect of things, you need to think about ways to reduce the dependence on K. The text.",
                    "label": 0
                }
            ]
        }
    }
}