{
    "id": "2fj6tmos2jlmbufoh5czqt4ys4phxfug",
    "title": "TermPicker: Enabling the Reuse of Vocabulary Terms by Exploiting Data from the Linked Open Data Cloud",
    "info": {
        "author": [
            "Johann Schaible, GESIS - Leibniz Institute for the Social Sciences"
        ],
        "published": "July 28, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_schaible_open_data/",
    "segmentation": [
        [
            "Our presenting our work on term picker it's collaborative work with colleague Thomas Got Ron who is now at the innovation lab of Shufa and UNSCR share, proves my supervisor at the Lightness Institute CBW and what temperature is all about is that we want to enable the reuse of vocabulary terms when modeling open data by exploiting linked open data itself, hence provide vocabulary term recommendations.",
            "Go."
        ],
        [
            "Want to back to the problem statement would be one question where we say well.",
            "When modeling linked open data, we do this.",
            "A customer is best practice.",
            "It advised to us as linked open data modelers to reuse existing RDF vocabularies before inventing new ones right, but.",
            "As we all know, the most of us maybe, or the ones who already model linked open data.",
            "It is quite a challenging task.",
            "You have to find vocabularies you have to to evaluate it, whether it suits your domain, whether it's already popular, whether it contains enough classes and properties to represent all your data.",
            "Ann.",
            "This is quite challenging.",
            "So for example, we do have a data model.",
            "Maybe she has a few resources and she looks for vocabulary using.",
            "Tools like linked, open vocabularies, vocab, Sissy others allow these stats.",
            "There are various tools that provide or alleviate the situation for the link data modeler for providing basics.",
            "Best ring match searches and so she searches for the resources saying I want to have this publication so she finds the SW receive vocabulary with publication as a class and she annotates those resources of type publication.",
            "Then she also has different various other.",
            "Resources and she annotates them with.",
            "Person.",
            "So now we want to connect this.",
            "At some point an now we gotta ask ourselves, do we have to make this process all over again?",
            "For every vocabulary term, do we have to find the data to use the system to type it in?",
            "We have a publication we have a person now.",
            "What is the connection between them?",
            "We want to specify that this person is the author of the publication, so we have already again like to say is it an author or is it the Creator?",
            "Is it a maker?",
            "So at some point we do have problems finding the right property also right?",
            "So it's not even just finding one.",
            "But even if you find like four of them like which should we use?",
            "Should we use?",
            "Creator from the Dublin core elements vocabulary or from the Dublin core terms vocabulary.",
            "Should we use feltmaker or should we stick to the same vocabulary that we already use for publication in person?",
            "So these are the kind of challenges that we have when we model linked open data and try to reuse vocabulary terms.",
            "So at some point we say well.",
            "It would be good to have a recommendation system that recommends your vocabulary terms in some sort of way, right?",
            "So but"
        ],
        [
            "Talking about recommendations would be also the questions what the recommendations should be based on.",
            "For example, we can say, well, I want to have recommendations based on the RDF's domain and range information of properties and classes, right?",
            "So what is it exactly that we would try to model an but these informations there are encoded in the vocabularies themselves.",
            "So sometimes if ontology engineer is, well, not quite pedantic about building the ontology.",
            "Might forget to add those kind of things to vocabulary, and even if there in one vocabulary there also in this locality.",
            "But maybe they also want to be used between vocabulary.",
            "Slightly interlinking different vocabularies via equivalent properties, equivalent classes and stuff like that.",
            "And sometimes it does exist in vocabularies, sometimes it does not, so this is kind of like a challenge to only basically recommendations on that because sometimes vocabularies do not contain such information.",
            "So if we go to linked open data and we want to extract.",
            "Vocabulary terms from linked open data itself.",
            "The question would be here also based on what should we recommend these terms simple popularity?",
            "Like for example, 4th is the most popular vocabulary to annotate persons and their relations.",
            "So should we use that?",
            "Maybe, maybe not.",
            "Maybe for publications there's also established received person.",
            "Is it better to use it for scientific person or is it better still to use fourth person?",
            "So what gives us the popularity at some point?",
            "The other feature would be to say, well, we already use one vocabulary.",
            "Why not stick to this one vocabulary so if we use SWC maybe we should use as much terms from this vocabulary as possible.",
            "It contains person.",
            "Yes four person might be more popular but it might be better to reuse the person class from SWC because we already using this vocabulary.",
            "So this is also open question and there might be also various other.",
            "Yeah, features that can be used for recommendations here.",
            "What we wanted to look at is.",
            "To ask well which vocabulary terms did other data providers on the linked open Data Cloud also use in a similar scenario that I am currently right now as modeling some sort of data.",
            "This is my scenario, I'm in it right now.",
            "So what did the others do in my situation?",
            "Right now modeling this data from that domain.",
            "So this is what we try to.",
            "Yeah, apply here for the record."
        ],
        [
            "Nations, but how do we capture such a scenario?",
            "How do you capture scenario?",
            "Building some linked open data, and we say that we define the scenario by the vocabulary terms that are used for only a part of the entire model of your data, right?",
            "So only a little part of your model is basically a scenario that you are currently and that you are as a model are currently focusing on modeling.",
            "Alright, so these are patterns on schema level and we define them as schema level parents.",
            "I've been basically talking about schema level patterns the entire week already, so I might continue as well here.",
            "An schema level patterns is nothing else but a tuple describing the connection between two Type 2 type of two sets of RDF types, which I connected via properties right?",
            "So for example we do have such a schema level patterns.",
            "Now what does it tell us?",
            "So the first set is the set of classes that I used for a resource in subject position of an RDF triple.",
            "The second set is the outgoing property of those resources, and the third set in the tuple is the RDF type or the class of the resources in an object position of an RDF triple.",
            "So saying this schema level pattern it would be saying well resources of type publication are connected to resources of type person via the DC Creator property.",
            "And we can extract those schema level patterns from linked open data, thus exploiting linked open data to gain these schema level patterns and to gain information how the how they others used vocabulary terms to model their data.",
            "In general, there would be like the subject type, set the property set and the object type set now using."
        ],
        [
            "Information how does our recommendation system work?",
            "Let's say this is the black box, right?",
            "So the black box recommender system and we do have various vocabulary terms X one to XN, which were extracted from some link data data set.",
            "For example, the billing triple Challenge data set 2014.",
            "You can harvest all the vocabulary terms in there and justice are the set of possible recommendation candidates that we can recommend.",
            "So as a first step for the recommender we have to provide an input and it's not just a query based on.",
            "One single vocabulary term, but the query is actual schema level parent that we have just introduced.",
            "This is the part of your model that you are currently modeling.",
            "So for example you have some resources and you say, well these resources are of type solo music artists.",
            "And that's it.",
            "That's only the class describing your resources.",
            "You don't have anything else yet, but what you want to know is what the others have used.",
            "Also, in that kind of situation or scenario.",
            "So this is the input at.",
            "What are recommended?",
            "This is still a black box I'll come to that in to detail it.",
            "Takes this input and computes a set of features.",
            "Based on the input schema level patterns.",
            "So the query SLP and the recommendation candidate XI, for example right.",
            "After we compute, for each recommendation candidata feature values, we do have ranking model based on learning to rank.",
            "I'll come to that also in the detail that provides a rank score for each vocabulary term that can be recommended.",
            "So for each recommendation candidate and once it does that, it ranks them in descending order from most from biggest rank score from the lowest rank score.",
            "And what it provides an output are three sets of vocabulary terms.",
            "One set would be further classes that you can use for the resource in subject position.",
            "Right now, for example, would be only music artists or even fourth person, meaning that other people have used these classes in conjunction with solo music artists as an outgoing property, you have like recommendations like 4th Mates or member of and for Resources and object Position you would have recommendations like record or music group.",
            "Alright, so this is the basic principle, how recommend."
        ],
        [
            "System works, and we've seen that, and now we're going to come up to."
        ],
        [
            "The feature computation what we do there and the schema level penance that we use is not only for querying the recommender system, but also to define specific feature value for a given vocabulary recommendation.",
            "So how do we compute this SLP feature an we say the set of all SLP's counted from linked open data?",
            "Is basically representing every kind of scenario that somebody some data modeler at some point of the time was when they were modeling their data is linked open data.",
            "So for example, let's say we do have these three schema level patterns extracted from linked open data, so these are the three.",
            "For example, just one soul music, artist music, artist, member of Music Pen.",
            "So we see.",
            "This is how others modeled their resources, basically.",
            "And now we do have again.",
            "US as a link data modeler right now and we do have the same query.",
            "The Soul Music Artists and what it basically does is recommender feature for the soap feature.",
            "We go through each extracted schema level parent from the linked open Data Cloud.",
            "And if it is a non if the query SLP is a nonempty subset.",
            "Of one of these extracted schema level patterns.",
            "Then we can provide recommendations by just using all the further terms that are included in the other schema level patterns.",
            "So in this case, for example for the first SLP over there therapy, every vocabulary term besides the solo music artist.",
            "So we could recommendations, music, artists, Member of Music Band and the last.",
            "Schema level pattern over here.",
            "Would not be used for recommendations because it's not a subset because the schema level pattern that we used for query is not a subset of that one.",
            "Alright, so this is the SLP feature that we do and it's basically collaborative filtering.",
            "What do we have and what did others also have?",
            "Right?",
            "So this is the schema level pattern feature that we use."
        ],
        [
            "And also we do have a state of the art set of features that we elaborated in a survey.",
            "We published this paper on that I think 2000 2014.",
            "It says 21,000 four, but yeah.",
            "Not yet this WC, but two years ago we published this paper where we like we said what are the current state of the art features that link open data modelers use for a reusing vocabularies and we had four of them.",
            "F1 to F3.",
            "So feature one to have three.",
            "Would be saying we want to reuse popular vocabularies and the question was always in what is a popular vocabulary.",
            "So we define it in the three sort of ways from the survey.",
            "Also extracted pad once number of datasets using the vocabulary term on the linked open data cloud.",
            "Second number of datasets using just the vocabulary of the vocabulary term and the third one is the total number of total occurrence over the linked open data cloud.",
            "Off that vocabulary term across datasets, basically so this by these three feature values we define the popularity of a recommendation candidate and feature four specifies whether a recommendation candidate is from a vocabulary that we already have used going to the same direction while you have a domain specific vocabulary, use as much terms from that vocabulary as possible, so these are the features that we have used for REC."
        ],
        [
            "Mentation system an now.",
            "How is our ranking model trying to solve the problem?",
            "How to use this feature values?"
        ],
        [
            "For example.",
            "We have four recommendation candidates X 12X4.",
            "Like 4 vocabulary terms.",
            "And the feature values for them.",
            "In conjunction with the schema level pen that's provided as input, the query basically as follows.",
            "And the question would be how should we rank?",
            "How should we rank X1 to X4 in a descending order from most appropriate to least appropriate based on these features?",
            "Well, we should have a weighting function that waits each feature value how much it goes into detail, how much it weights the exact value of the feature, and so this is like how to wait.",
            "The values would be the main question, and doing it manually would be not feasable.",
            "So what we used is used learning to rank algorithms.",
            "Which is basically a family of supervised machine learning algorithms that.",
            "Compute generalized ranking function over a set of features, which is exactly our case.",
            "This is exactly what we need it.",
            "We want to have overrides generalized functions saying what these are.",
            "The features we waited that way, and we can apply it to future non existing not previously trained data.",
            "And.",
            "Yeah so, but learning to rank does iterates over and over and over again until it can find a weighting function such that the relevant vocabulary terms, the relevant recommendation is end up as much as high on the top of the list.",
            "So this is how we."
        ],
        [
            "Compute the ranking model using different learning to rank algorithms now."
        ],
        [
            "We have output data.",
            "We have a list of recommendations.",
            "How do we calculate it?",
            "Because how is it possible to?",
            "I mean, what is the evaluation also about what we wanted to see in a valuation for?",
            "And I mean you have various different kinds like we can look at the learning to rank.",
            "Algorithm was the best well, but we focus on the question what is the benefit of our proposed SLP feature?",
            "Actually, what is the benefit of it?",
            "And we provide like the two baselines we use for that is the popular baseline where we say like we used for learning to rank.",
            "Only features one to three saying the popularity of a vocabulary term.",
            "This is baseline number one and based on number 2 would be adding the 4th features saying we also want to reuse vocabulary terms from the same vocabulary.",
            "These are the two baselines and our approach also adds the schema level pattern feature, and we wanted to see how much of a boost, how much of an improvement of various like precision values does give our doesn't give the SLP feature, so this is what we want to evaluate and the procedure what we do."
        ],
        [
            "It is an offline evaluation with hidden information.",
            "There's always the question, how do you assess the quality of a recommendation list?",
            "How do you say whether recommendation candidate is relevant or not?",
            "And a typical case in offline evaluation is to take for example, for our case, one query schema level pattern.",
            "And we hide randomly height one of the terms.",
            "Of this query input, for example, right here we hide 4th mate the property then.",
            "We use R recommendation system and then we get a result list and then we can look inside the result list where is the hidden information that we actually previously extracted.",
            "Is it at the 1st place and the 3rd place?",
            "And based on that we can calculate precision, recall mean, average precision and so on.",
            "What we do to measure the quality of the recommendation is that we use mean average precision and reciprocal rank at the 1st five position, simply because precision and recall do not tell us anything about the ranking position inside the list.",
            "It just says how many of the relevant terms are included in the list.",
            "But as we have recommendations, we're not going to browse like through hundreds vocabulary terms.",
            "You want to have them in the top five from the top 10.",
            "So This is why we use mean average precision.",
            "I mean me reciprocal rank because they also give a value about which ranking position are relevant vocabulary term is and to do so, the evaluation we use the Rank Clip library which provided us various learning to rank algorithms for once and also various.",
            "Possibilities to measure the quality of the result list including mean average precision and the mean reciprocal rank.",
            "The data we."
        ],
        [
            "I've used for revelation.",
            "We have two evaluations based one on the billing Triple Challenge data set 2014 and one space on Dildo, and they want one contain we've built in circumstances that contains a lot of triples, but we use only 34 million of those triples just to reduce the overhead of computing.",
            "The recommendations, because it's still a prototype system.",
            "We do not have a full functioning indexing set yet, so This is why we reduced the number of triples.",
            "And but it contains still contains 3500 pay level domains which basically like dbpedia.org or data that goes up a level domains an this triple challenge data set in 34 million triples.",
            "We do still have 3.5 thousand of those.",
            "It also includes about 5.5 million different vocabulary terms that we can recommend from 1005 vocabularies, and we can compute 220,000 different schema level patterns representing how others have modeled the data set.",
            "Those little smaller but still enough to calculate valuable recommendations.",
            "And what we did is we did a 10 cross tenfold leave one out validation based on the pay level domains.",
            "Meaning that from these pay level domains for each data set we extracted 10.",
            "Based on two measures, one measure was we wanted to have in the pay level domain that uses a lot of different vocabulary terms.",
            "Otherwise we could only extract a few ones using a lot of different ones.",
            "We can extract a lot of schema level patterns.",
            "This was the first criteria on the second one.",
            "We had like a peer level domain for the temple leave.",
            "An application should have also a lot of reuse vocabulary terms because taken one pay level domain out which has only self defined terms.",
            "You cannot recommend anything based on that because nobody else uses them.",
            "Right?"
        ],
        [
            "And to our results.",
            "We used this box mechanism.",
            "So what do you see here is first.",
            "This is the baseline using popular vocabularies only.",
            "The different roles are the three best learning to rank algorithms in the Rank library or Rank Clip Library and the three left box plots are based on the billion triple Challenge data set and the three on the right ones are based on the dildo data set and whether it is recommended a Type 4 subjects type for objects or property.",
            "So this is what are the box plus R and we can see that like using only based on popular recommendations.",
            "We achieve about like an average mean average precision of about 0.32.",
            "Which is OK, but not too good basically.",
            "So what we did for the second baseline, we added the feature same vocabulary and it didn't improve too much.",
            "As we can see here.",
            "So it's about a little yeah.",
            "0.4 so that improved a little bit, so the improvement was not significant.",
            "By adding this feature to provide recommendations by adding our schema level pattern feature, we can already see that I'm building on a billion triple challenge data set.",
            "It worked very, very well, but on a dealer that I said it doesn't work too good.",
            "Still, there is a high improvement such that the mean average precision was about like 0.75 to 0.8.",
            "And the mean reciprocal rank.",
            "Is exactly looking that way, so it's also about 0.7 to 0.8 of miracle rank at the 1st five position."
        ],
        [
            "Now, was it the way?",
            "First of all, why is it like that using same vocabulary features not significantly better?",
            "Well, we looked into the data and we saw that we, from the training data set and the test data set all the schema level patterns.",
            "They only a few cases where people used terms from actually the same vocabulary.",
            "So sometimes they use something from DB pedia.",
            "Then again from 4th and again from this WR C and so on and so on and so on.",
            "So that is why.",
            "The learning to rank algorithm could not learn by saying, well, this is an interesting feature.",
            "This terms from the same vocabulary should be ranked higher.",
            "This is why I didn't do it.",
            "Again using aesopi.",
            "Feature there was a significant improvement about about like 35% better mean average precision.",
            "Why does it happen?",
            "We also looked into the data and we saw that well, I mean, as a kind of also like the discussion question.",
            "Maybe also like we can save maybe already happening now that people, not even without without a recommender system, they try to look how did others model the data.",
            "They look into the data set and they use these vocabulary terms.",
            "So the learning to rank algorithms.",
            "Actually learned this is a valuable feature.",
            "And the question why is also better performing on the billion Triple Challenge data set is, well, we did have more data in the building triple challenge, so we looked way far into it and we saw that in 37% more cases.",
            "The recommendation candidates that were relevant had also an SLP feature value above 0.",
            "So basically the learning to rank algorithms learned based on the data of BTC.",
            "Yep, this is a relevant feature.",
            "Are rank it higher is assign it a better wait because the relevant terms at the top.",
            "Using the low data, it's learned that the relevant candidates had SLP feature value of 0 or 7037% less.",
            "A zero and saying learning that this feature is not as relevant providing not as good results.",
            "So."
        ],
        [
            "What can we conclude from our work here is for once that, yes, using US or piece or using the schema level pattern feature, we can say that relevant documentations are ranked significantly higher in the result list and that it can aid as we said, like already now linked open data modelers tried.",
            "So we look at what others did and this one.",
            "This kind of approach could aid the engineer even more.",
            "Looking at what others did and provide recommendations.",
            "And of course, as we said, like a.",
            "To rank it's machine learning, you given training data and if one feature is aligning with the relevance value of elliptical Cavaleri term then of course this feature gets pushed a lot, gets a lot of weight and this way can be used very good.",
            "Yes, so this is may conclusion, but however it's an offline evaluation, we could not see how the users actually behaved while doing that.",
            "So also we needed an online evaluation.",
            "And I presented that one yesterday, but I will put up this license slide share.",
            "Well, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our presenting our work on term picker it's collaborative work with colleague Thomas Got Ron who is now at the innovation lab of Shufa and UNSCR share, proves my supervisor at the Lightness Institute CBW and what temperature is all about is that we want to enable the reuse of vocabulary terms when modeling open data by exploiting linked open data itself, hence provide vocabulary term recommendations.",
                    "label": 0
                },
                {
                    "sent": "Go.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to back to the problem statement would be one question where we say well.",
                    "label": 0
                },
                {
                    "sent": "When modeling linked open data, we do this.",
                    "label": 0
                },
                {
                    "sent": "A customer is best practice.",
                    "label": 0
                },
                {
                    "sent": "It advised to us as linked open data modelers to reuse existing RDF vocabularies before inventing new ones right, but.",
                    "label": 0
                },
                {
                    "sent": "As we all know, the most of us maybe, or the ones who already model linked open data.",
                    "label": 0
                },
                {
                    "sent": "It is quite a challenging task.",
                    "label": 0
                },
                {
                    "sent": "You have to find vocabularies you have to to evaluate it, whether it suits your domain, whether it's already popular, whether it contains enough classes and properties to represent all your data.",
                    "label": 0
                },
                {
                    "sent": "Ann.",
                    "label": 0
                },
                {
                    "sent": "This is quite challenging.",
                    "label": 0
                },
                {
                    "sent": "So for example, we do have a data model.",
                    "label": 0
                },
                {
                    "sent": "Maybe she has a few resources and she looks for vocabulary using.",
                    "label": 0
                },
                {
                    "sent": "Tools like linked, open vocabularies, vocab, Sissy others allow these stats.",
                    "label": 0
                },
                {
                    "sent": "There are various tools that provide or alleviate the situation for the link data modeler for providing basics.",
                    "label": 0
                },
                {
                    "sent": "Best ring match searches and so she searches for the resources saying I want to have this publication so she finds the SW receive vocabulary with publication as a class and she annotates those resources of type publication.",
                    "label": 0
                },
                {
                    "sent": "Then she also has different various other.",
                    "label": 0
                },
                {
                    "sent": "Resources and she annotates them with.",
                    "label": 0
                },
                {
                    "sent": "Person.",
                    "label": 0
                },
                {
                    "sent": "So now we want to connect this.",
                    "label": 0
                },
                {
                    "sent": "At some point an now we gotta ask ourselves, do we have to make this process all over again?",
                    "label": 0
                },
                {
                    "sent": "For every vocabulary term, do we have to find the data to use the system to type it in?",
                    "label": 0
                },
                {
                    "sent": "We have a publication we have a person now.",
                    "label": 0
                },
                {
                    "sent": "What is the connection between them?",
                    "label": 0
                },
                {
                    "sent": "We want to specify that this person is the author of the publication, so we have already again like to say is it an author or is it the Creator?",
                    "label": 0
                },
                {
                    "sent": "Is it a maker?",
                    "label": 0
                },
                {
                    "sent": "So at some point we do have problems finding the right property also right?",
                    "label": 0
                },
                {
                    "sent": "So it's not even just finding one.",
                    "label": 0
                },
                {
                    "sent": "But even if you find like four of them like which should we use?",
                    "label": 0
                },
                {
                    "sent": "Should we use?",
                    "label": 0
                },
                {
                    "sent": "Creator from the Dublin core elements vocabulary or from the Dublin core terms vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Should we use feltmaker or should we stick to the same vocabulary that we already use for publication in person?",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of challenges that we have when we model linked open data and try to reuse vocabulary terms.",
                    "label": 0
                },
                {
                    "sent": "So at some point we say well.",
                    "label": 0
                },
                {
                    "sent": "It would be good to have a recommendation system that recommends your vocabulary terms in some sort of way, right?",
                    "label": 0
                },
                {
                    "sent": "So but",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talking about recommendations would be also the questions what the recommendations should be based on.",
                    "label": 0
                },
                {
                    "sent": "For example, we can say, well, I want to have recommendations based on the RDF's domain and range information of properties and classes, right?",
                    "label": 0
                },
                {
                    "sent": "So what is it exactly that we would try to model an but these informations there are encoded in the vocabularies themselves.",
                    "label": 0
                },
                {
                    "sent": "So sometimes if ontology engineer is, well, not quite pedantic about building the ontology.",
                    "label": 0
                },
                {
                    "sent": "Might forget to add those kind of things to vocabulary, and even if there in one vocabulary there also in this locality.",
                    "label": 0
                },
                {
                    "sent": "But maybe they also want to be used between vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Slightly interlinking different vocabularies via equivalent properties, equivalent classes and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And sometimes it does exist in vocabularies, sometimes it does not, so this is kind of like a challenge to only basically recommendations on that because sometimes vocabularies do not contain such information.",
                    "label": 0
                },
                {
                    "sent": "So if we go to linked open data and we want to extract.",
                    "label": 0
                },
                {
                    "sent": "Vocabulary terms from linked open data itself.",
                    "label": 0
                },
                {
                    "sent": "The question would be here also based on what should we recommend these terms simple popularity?",
                    "label": 0
                },
                {
                    "sent": "Like for example, 4th is the most popular vocabulary to annotate persons and their relations.",
                    "label": 0
                },
                {
                    "sent": "So should we use that?",
                    "label": 0
                },
                {
                    "sent": "Maybe, maybe not.",
                    "label": 0
                },
                {
                    "sent": "Maybe for publications there's also established received person.",
                    "label": 0
                },
                {
                    "sent": "Is it better to use it for scientific person or is it better still to use fourth person?",
                    "label": 0
                },
                {
                    "sent": "So what gives us the popularity at some point?",
                    "label": 0
                },
                {
                    "sent": "The other feature would be to say, well, we already use one vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Why not stick to this one vocabulary so if we use SWC maybe we should use as much terms from this vocabulary as possible.",
                    "label": 0
                },
                {
                    "sent": "It contains person.",
                    "label": 0
                },
                {
                    "sent": "Yes four person might be more popular but it might be better to reuse the person class from SWC because we already using this vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So this is also open question and there might be also various other.",
                    "label": 0
                },
                {
                    "sent": "Yeah, features that can be used for recommendations here.",
                    "label": 0
                },
                {
                    "sent": "What we wanted to look at is.",
                    "label": 0
                },
                {
                    "sent": "To ask well which vocabulary terms did other data providers on the linked open Data Cloud also use in a similar scenario that I am currently right now as modeling some sort of data.",
                    "label": 0
                },
                {
                    "sent": "This is my scenario, I'm in it right now.",
                    "label": 0
                },
                {
                    "sent": "So what did the others do in my situation?",
                    "label": 0
                },
                {
                    "sent": "Right now modeling this data from that domain.",
                    "label": 0
                },
                {
                    "sent": "So this is what we try to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, apply here for the record.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations, but how do we capture such a scenario?",
                    "label": 0
                },
                {
                    "sent": "How do you capture scenario?",
                    "label": 0
                },
                {
                    "sent": "Building some linked open data, and we say that we define the scenario by the vocabulary terms that are used for only a part of the entire model of your data, right?",
                    "label": 0
                },
                {
                    "sent": "So only a little part of your model is basically a scenario that you are currently and that you are as a model are currently focusing on modeling.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these are patterns on schema level and we define them as schema level parents.",
                    "label": 0
                },
                {
                    "sent": "I've been basically talking about schema level patterns the entire week already, so I might continue as well here.",
                    "label": 0
                },
                {
                    "sent": "An schema level patterns is nothing else but a tuple describing the connection between two Type 2 type of two sets of RDF types, which I connected via properties right?",
                    "label": 0
                },
                {
                    "sent": "So for example we do have such a schema level patterns.",
                    "label": 0
                },
                {
                    "sent": "Now what does it tell us?",
                    "label": 0
                },
                {
                    "sent": "So the first set is the set of classes that I used for a resource in subject position of an RDF triple.",
                    "label": 0
                },
                {
                    "sent": "The second set is the outgoing property of those resources, and the third set in the tuple is the RDF type or the class of the resources in an object position of an RDF triple.",
                    "label": 0
                },
                {
                    "sent": "So saying this schema level pattern it would be saying well resources of type publication are connected to resources of type person via the DC Creator property.",
                    "label": 0
                },
                {
                    "sent": "And we can extract those schema level patterns from linked open data, thus exploiting linked open data to gain these schema level patterns and to gain information how the how they others used vocabulary terms to model their data.",
                    "label": 0
                },
                {
                    "sent": "In general, there would be like the subject type, set the property set and the object type set now using.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information how does our recommendation system work?",
                    "label": 0
                },
                {
                    "sent": "Let's say this is the black box, right?",
                    "label": 0
                },
                {
                    "sent": "So the black box recommender system and we do have various vocabulary terms X one to XN, which were extracted from some link data data set.",
                    "label": 0
                },
                {
                    "sent": "For example, the billing triple Challenge data set 2014.",
                    "label": 0
                },
                {
                    "sent": "You can harvest all the vocabulary terms in there and justice are the set of possible recommendation candidates that we can recommend.",
                    "label": 0
                },
                {
                    "sent": "So as a first step for the recommender we have to provide an input and it's not just a query based on.",
                    "label": 0
                },
                {
                    "sent": "One single vocabulary term, but the query is actual schema level parent that we have just introduced.",
                    "label": 0
                },
                {
                    "sent": "This is the part of your model that you are currently modeling.",
                    "label": 0
                },
                {
                    "sent": "So for example you have some resources and you say, well these resources are of type solo music artists.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "That's only the class describing your resources.",
                    "label": 0
                },
                {
                    "sent": "You don't have anything else yet, but what you want to know is what the others have used.",
                    "label": 0
                },
                {
                    "sent": "Also, in that kind of situation or scenario.",
                    "label": 0
                },
                {
                    "sent": "So this is the input at.",
                    "label": 0
                },
                {
                    "sent": "What are recommended?",
                    "label": 0
                },
                {
                    "sent": "This is still a black box I'll come to that in to detail it.",
                    "label": 0
                },
                {
                    "sent": "Takes this input and computes a set of features.",
                    "label": 0
                },
                {
                    "sent": "Based on the input schema level patterns.",
                    "label": 0
                },
                {
                    "sent": "So the query SLP and the recommendation candidate XI, for example right.",
                    "label": 0
                },
                {
                    "sent": "After we compute, for each recommendation candidata feature values, we do have ranking model based on learning to rank.",
                    "label": 1
                },
                {
                    "sent": "I'll come to that also in the detail that provides a rank score for each vocabulary term that can be recommended.",
                    "label": 0
                },
                {
                    "sent": "So for each recommendation candidate and once it does that, it ranks them in descending order from most from biggest rank score from the lowest rank score.",
                    "label": 0
                },
                {
                    "sent": "And what it provides an output are three sets of vocabulary terms.",
                    "label": 1
                },
                {
                    "sent": "One set would be further classes that you can use for the resource in subject position.",
                    "label": 0
                },
                {
                    "sent": "Right now, for example, would be only music artists or even fourth person, meaning that other people have used these classes in conjunction with solo music artists as an outgoing property, you have like recommendations like 4th Mates or member of and for Resources and object Position you would have recommendations like record or music group.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is the basic principle, how recommend.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System works, and we've seen that, and now we're going to come up to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The feature computation what we do there and the schema level penance that we use is not only for querying the recommender system, but also to define specific feature value for a given vocabulary recommendation.",
                    "label": 0
                },
                {
                    "sent": "So how do we compute this SLP feature an we say the set of all SLP's counted from linked open data?",
                    "label": 0
                },
                {
                    "sent": "Is basically representing every kind of scenario that somebody some data modeler at some point of the time was when they were modeling their data is linked open data.",
                    "label": 0
                },
                {
                    "sent": "So for example, let's say we do have these three schema level patterns extracted from linked open data, so these are the three.",
                    "label": 0
                },
                {
                    "sent": "For example, just one soul music, artist music, artist, member of Music Pen.",
                    "label": 0
                },
                {
                    "sent": "So we see.",
                    "label": 0
                },
                {
                    "sent": "This is how others modeled their resources, basically.",
                    "label": 0
                },
                {
                    "sent": "And now we do have again.",
                    "label": 0
                },
                {
                    "sent": "US as a link data modeler right now and we do have the same query.",
                    "label": 0
                },
                {
                    "sent": "The Soul Music Artists and what it basically does is recommender feature for the soap feature.",
                    "label": 0
                },
                {
                    "sent": "We go through each extracted schema level parent from the linked open Data Cloud.",
                    "label": 0
                },
                {
                    "sent": "And if it is a non if the query SLP is a nonempty subset.",
                    "label": 0
                },
                {
                    "sent": "Of one of these extracted schema level patterns.",
                    "label": 0
                },
                {
                    "sent": "Then we can provide recommendations by just using all the further terms that are included in the other schema level patterns.",
                    "label": 0
                },
                {
                    "sent": "So in this case, for example for the first SLP over there therapy, every vocabulary term besides the solo music artist.",
                    "label": 0
                },
                {
                    "sent": "So we could recommendations, music, artists, Member of Music Band and the last.",
                    "label": 0
                },
                {
                    "sent": "Schema level pattern over here.",
                    "label": 0
                },
                {
                    "sent": "Would not be used for recommendations because it's not a subset because the schema level pattern that we used for query is not a subset of that one.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is the SLP feature that we do and it's basically collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "What do we have and what did others also have?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So this is the schema level pattern feature that we use.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also we do have a state of the art set of features that we elaborated in a survey.",
                    "label": 0
                },
                {
                    "sent": "We published this paper on that I think 2000 2014.",
                    "label": 0
                },
                {
                    "sent": "It says 21,000 four, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Not yet this WC, but two years ago we published this paper where we like we said what are the current state of the art features that link open data modelers use for a reusing vocabularies and we had four of them.",
                    "label": 0
                },
                {
                    "sent": "F1 to F3.",
                    "label": 0
                },
                {
                    "sent": "So feature one to have three.",
                    "label": 0
                },
                {
                    "sent": "Would be saying we want to reuse popular vocabularies and the question was always in what is a popular vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So we define it in the three sort of ways from the survey.",
                    "label": 0
                },
                {
                    "sent": "Also extracted pad once number of datasets using the vocabulary term on the linked open data cloud.",
                    "label": 1
                },
                {
                    "sent": "Second number of datasets using just the vocabulary of the vocabulary term and the third one is the total number of total occurrence over the linked open data cloud.",
                    "label": 1
                },
                {
                    "sent": "Off that vocabulary term across datasets, basically so this by these three feature values we define the popularity of a recommendation candidate and feature four specifies whether a recommendation candidate is from a vocabulary that we already have used going to the same direction while you have a domain specific vocabulary, use as much terms from that vocabulary as possible, so these are the features that we have used for REC.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mentation system an now.",
                    "label": 0
                },
                {
                    "sent": "How is our ranking model trying to solve the problem?",
                    "label": 0
                },
                {
                    "sent": "How to use this feature values?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "We have four recommendation candidates X 12X4.",
                    "label": 0
                },
                {
                    "sent": "Like 4 vocabulary terms.",
                    "label": 0
                },
                {
                    "sent": "And the feature values for them.",
                    "label": 0
                },
                {
                    "sent": "In conjunction with the schema level pen that's provided as input, the query basically as follows.",
                    "label": 0
                },
                {
                    "sent": "And the question would be how should we rank?",
                    "label": 0
                },
                {
                    "sent": "How should we rank X1 to X4 in a descending order from most appropriate to least appropriate based on these features?",
                    "label": 0
                },
                {
                    "sent": "Well, we should have a weighting function that waits each feature value how much it goes into detail, how much it weights the exact value of the feature, and so this is like how to wait.",
                    "label": 0
                },
                {
                    "sent": "The values would be the main question, and doing it manually would be not feasable.",
                    "label": 0
                },
                {
                    "sent": "So what we used is used learning to rank algorithms.",
                    "label": 0
                },
                {
                    "sent": "Which is basically a family of supervised machine learning algorithms that.",
                    "label": 0
                },
                {
                    "sent": "Compute generalized ranking function over a set of features, which is exactly our case.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what we need it.",
                    "label": 0
                },
                {
                    "sent": "We want to have overrides generalized functions saying what these are.",
                    "label": 0
                },
                {
                    "sent": "The features we waited that way, and we can apply it to future non existing not previously trained data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah so, but learning to rank does iterates over and over and over again until it can find a weighting function such that the relevant vocabulary terms, the relevant recommendation is end up as much as high on the top of the list.",
                    "label": 0
                },
                {
                    "sent": "So this is how we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute the ranking model using different learning to rank algorithms now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have output data.",
                    "label": 0
                },
                {
                    "sent": "We have a list of recommendations.",
                    "label": 0
                },
                {
                    "sent": "How do we calculate it?",
                    "label": 0
                },
                {
                    "sent": "Because how is it possible to?",
                    "label": 0
                },
                {
                    "sent": "I mean, what is the evaluation also about what we wanted to see in a valuation for?",
                    "label": 0
                },
                {
                    "sent": "And I mean you have various different kinds like we can look at the learning to rank.",
                    "label": 0
                },
                {
                    "sent": "Algorithm was the best well, but we focus on the question what is the benefit of our proposed SLP feature?",
                    "label": 0
                },
                {
                    "sent": "Actually, what is the benefit of it?",
                    "label": 0
                },
                {
                    "sent": "And we provide like the two baselines we use for that is the popular baseline where we say like we used for learning to rank.",
                    "label": 0
                },
                {
                    "sent": "Only features one to three saying the popularity of a vocabulary term.",
                    "label": 0
                },
                {
                    "sent": "This is baseline number one and based on number 2 would be adding the 4th features saying we also want to reuse vocabulary terms from the same vocabulary.",
                    "label": 0
                },
                {
                    "sent": "These are the two baselines and our approach also adds the schema level pattern feature, and we wanted to see how much of a boost, how much of an improvement of various like precision values does give our doesn't give the SLP feature, so this is what we want to evaluate and the procedure what we do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is an offline evaluation with hidden information.",
                    "label": 0
                },
                {
                    "sent": "There's always the question, how do you assess the quality of a recommendation list?",
                    "label": 0
                },
                {
                    "sent": "How do you say whether recommendation candidate is relevant or not?",
                    "label": 0
                },
                {
                    "sent": "And a typical case in offline evaluation is to take for example, for our case, one query schema level pattern.",
                    "label": 0
                },
                {
                    "sent": "And we hide randomly height one of the terms.",
                    "label": 0
                },
                {
                    "sent": "Of this query input, for example, right here we hide 4th mate the property then.",
                    "label": 0
                },
                {
                    "sent": "We use R recommendation system and then we get a result list and then we can look inside the result list where is the hidden information that we actually previously extracted.",
                    "label": 0
                },
                {
                    "sent": "Is it at the 1st place and the 3rd place?",
                    "label": 0
                },
                {
                    "sent": "And based on that we can calculate precision, recall mean, average precision and so on.",
                    "label": 0
                },
                {
                    "sent": "What we do to measure the quality of the recommendation is that we use mean average precision and reciprocal rank at the 1st five position, simply because precision and recall do not tell us anything about the ranking position inside the list.",
                    "label": 0
                },
                {
                    "sent": "It just says how many of the relevant terms are included in the list.",
                    "label": 0
                },
                {
                    "sent": "But as we have recommendations, we're not going to browse like through hundreds vocabulary terms.",
                    "label": 0
                },
                {
                    "sent": "You want to have them in the top five from the top 10.",
                    "label": 0
                },
                {
                    "sent": "So This is why we use mean average precision.",
                    "label": 0
                },
                {
                    "sent": "I mean me reciprocal rank because they also give a value about which ranking position are relevant vocabulary term is and to do so, the evaluation we use the Rank Clip library which provided us various learning to rank algorithms for once and also various.",
                    "label": 0
                },
                {
                    "sent": "Possibilities to measure the quality of the result list including mean average precision and the mean reciprocal rank.",
                    "label": 0
                },
                {
                    "sent": "The data we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've used for revelation.",
                    "label": 0
                },
                {
                    "sent": "We have two evaluations based one on the billing Triple Challenge data set 2014 and one space on Dildo, and they want one contain we've built in circumstances that contains a lot of triples, but we use only 34 million of those triples just to reduce the overhead of computing.",
                    "label": 0
                },
                {
                    "sent": "The recommendations, because it's still a prototype system.",
                    "label": 0
                },
                {
                    "sent": "We do not have a full functioning indexing set yet, so This is why we reduced the number of triples.",
                    "label": 0
                },
                {
                    "sent": "And but it contains still contains 3500 pay level domains which basically like dbpedia.org or data that goes up a level domains an this triple challenge data set in 34 million triples.",
                    "label": 0
                },
                {
                    "sent": "We do still have 3.5 thousand of those.",
                    "label": 0
                },
                {
                    "sent": "It also includes about 5.5 million different vocabulary terms that we can recommend from 1005 vocabularies, and we can compute 220,000 different schema level patterns representing how others have modeled the data set.",
                    "label": 0
                },
                {
                    "sent": "Those little smaller but still enough to calculate valuable recommendations.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we did a 10 cross tenfold leave one out validation based on the pay level domains.",
                    "label": 0
                },
                {
                    "sent": "Meaning that from these pay level domains for each data set we extracted 10.",
                    "label": 0
                },
                {
                    "sent": "Based on two measures, one measure was we wanted to have in the pay level domain that uses a lot of different vocabulary terms.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we could only extract a few ones using a lot of different ones.",
                    "label": 0
                },
                {
                    "sent": "We can extract a lot of schema level patterns.",
                    "label": 0
                },
                {
                    "sent": "This was the first criteria on the second one.",
                    "label": 0
                },
                {
                    "sent": "We had like a peer level domain for the temple leave.",
                    "label": 0
                },
                {
                    "sent": "An application should have also a lot of reuse vocabulary terms because taken one pay level domain out which has only self defined terms.",
                    "label": 0
                },
                {
                    "sent": "You cannot recommend anything based on that because nobody else uses them.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to our results.",
                    "label": 0
                },
                {
                    "sent": "We used this box mechanism.",
                    "label": 0
                },
                {
                    "sent": "So what do you see here is first.",
                    "label": 0
                },
                {
                    "sent": "This is the baseline using popular vocabularies only.",
                    "label": 0
                },
                {
                    "sent": "The different roles are the three best learning to rank algorithms in the Rank library or Rank Clip Library and the three left box plots are based on the billion triple Challenge data set and the three on the right ones are based on the dildo data set and whether it is recommended a Type 4 subjects type for objects or property.",
                    "label": 0
                },
                {
                    "sent": "So this is what are the box plus R and we can see that like using only based on popular recommendations.",
                    "label": 0
                },
                {
                    "sent": "We achieve about like an average mean average precision of about 0.32.",
                    "label": 0
                },
                {
                    "sent": "Which is OK, but not too good basically.",
                    "label": 0
                },
                {
                    "sent": "So what we did for the second baseline, we added the feature same vocabulary and it didn't improve too much.",
                    "label": 0
                },
                {
                    "sent": "As we can see here.",
                    "label": 0
                },
                {
                    "sent": "So it's about a little yeah.",
                    "label": 0
                },
                {
                    "sent": "0.4 so that improved a little bit, so the improvement was not significant.",
                    "label": 0
                },
                {
                    "sent": "By adding this feature to provide recommendations by adding our schema level pattern feature, we can already see that I'm building on a billion triple challenge data set.",
                    "label": 0
                },
                {
                    "sent": "It worked very, very well, but on a dealer that I said it doesn't work too good.",
                    "label": 0
                },
                {
                    "sent": "Still, there is a high improvement such that the mean average precision was about like 0.75 to 0.8.",
                    "label": 0
                },
                {
                    "sent": "And the mean reciprocal rank.",
                    "label": 0
                },
                {
                    "sent": "Is exactly looking that way, so it's also about 0.7 to 0.8 of miracle rank at the 1st five position.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, was it the way?",
                    "label": 0
                },
                {
                    "sent": "First of all, why is it like that using same vocabulary features not significantly better?",
                    "label": 0
                },
                {
                    "sent": "Well, we looked into the data and we saw that we, from the training data set and the test data set all the schema level patterns.",
                    "label": 0
                },
                {
                    "sent": "They only a few cases where people used terms from actually the same vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So sometimes they use something from DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Then again from 4th and again from this WR C and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So that is why.",
                    "label": 0
                },
                {
                    "sent": "The learning to rank algorithm could not learn by saying, well, this is an interesting feature.",
                    "label": 0
                },
                {
                    "sent": "This terms from the same vocabulary should be ranked higher.",
                    "label": 0
                },
                {
                    "sent": "This is why I didn't do it.",
                    "label": 0
                },
                {
                    "sent": "Again using aesopi.",
                    "label": 0
                },
                {
                    "sent": "Feature there was a significant improvement about about like 35% better mean average precision.",
                    "label": 0
                },
                {
                    "sent": "Why does it happen?",
                    "label": 0
                },
                {
                    "sent": "We also looked into the data and we saw that well, I mean, as a kind of also like the discussion question.",
                    "label": 0
                },
                {
                    "sent": "Maybe also like we can save maybe already happening now that people, not even without without a recommender system, they try to look how did others model the data.",
                    "label": 0
                },
                {
                    "sent": "They look into the data set and they use these vocabulary terms.",
                    "label": 0
                },
                {
                    "sent": "So the learning to rank algorithms.",
                    "label": 0
                },
                {
                    "sent": "Actually learned this is a valuable feature.",
                    "label": 0
                },
                {
                    "sent": "And the question why is also better performing on the billion Triple Challenge data set is, well, we did have more data in the building triple challenge, so we looked way far into it and we saw that in 37% more cases.",
                    "label": 0
                },
                {
                    "sent": "The recommendation candidates that were relevant had also an SLP feature value above 0.",
                    "label": 0
                },
                {
                    "sent": "So basically the learning to rank algorithms learned based on the data of BTC.",
                    "label": 0
                },
                {
                    "sent": "Yep, this is a relevant feature.",
                    "label": 0
                },
                {
                    "sent": "Are rank it higher is assign it a better wait because the relevant terms at the top.",
                    "label": 0
                },
                {
                    "sent": "Using the low data, it's learned that the relevant candidates had SLP feature value of 0 or 7037% less.",
                    "label": 0
                },
                {
                    "sent": "A zero and saying learning that this feature is not as relevant providing not as good results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What can we conclude from our work here is for once that, yes, using US or piece or using the schema level pattern feature, we can say that relevant documentations are ranked significantly higher in the result list and that it can aid as we said, like already now linked open data modelers tried.",
                    "label": 0
                },
                {
                    "sent": "So we look at what others did and this one.",
                    "label": 0
                },
                {
                    "sent": "This kind of approach could aid the engineer even more.",
                    "label": 0
                },
                {
                    "sent": "Looking at what others did and provide recommendations.",
                    "label": 0
                },
                {
                    "sent": "And of course, as we said, like a.",
                    "label": 0
                },
                {
                    "sent": "To rank it's machine learning, you given training data and if one feature is aligning with the relevance value of elliptical Cavaleri term then of course this feature gets pushed a lot, gets a lot of weight and this way can be used very good.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is may conclusion, but however it's an offline evaluation, we could not see how the users actually behaved while doing that.",
                    "label": 0
                },
                {
                    "sent": "So also we needed an online evaluation.",
                    "label": 0
                },
                {
                    "sent": "And I presented that one yesterday, but I will put up this license slide share.",
                    "label": 0
                },
                {
                    "sent": "Well, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}