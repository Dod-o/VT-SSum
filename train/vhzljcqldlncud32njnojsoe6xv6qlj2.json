{
    "id": "vhzljcqldlncud32njnojsoe6xv6qlj2",
    "title": "Online gradient descent for LS regression: Non-asymptotic bounds and application to bandits",
    "info": {
        "author": [
            "Prashanth. L.A., SequeL lab, INRIA Lille - Nord Europe"
        ],
        "published": "Nov. 7, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Decision Support",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/lsoldm2013_prashanth_ls_regression/",
    "segmentation": [
        [
            "The previous speaker is talking about news recommendation and the need for giving recommendations really fast, and so this work is one step in that direction so."
        ],
        [
            "So the point is what we want to do is we want to solve normal least squares problems or least squares.",
            "And some of the well known complexities are listed there.",
            "It's D is the dimension of the, say X the.",
            "X vectors and.",
            "So this is the dimension that usually no and.",
            "Some of the well known procedures, one well known alternative to solving using Sherman Morrison or some of the other algorithms, is to use stochastic gradient descent.",
            "And this is quite computationally efficient, so the complexity comes down to order of D and.",
            "And this has been well known so far, but what has not been known as in this work, what we do is we develop high priority bonds.",
            "Doing some finite time analysis and we give out some explicit constants.",
            "And some part of this is not fully known yet and which we derive."
        ],
        [
            "And the motivation for doing this is really to use this in higher machine level machine learning algorithms.",
            "And one algorithm that we show we show the usefulness of these schemes.",
            "The online gradient descent schemes is a linear bandit algorithm, so typical structure of a linear bandit algorithm is like.",
            "Are your first computer least squares estimate based on the arms that you have chosen so far and actually the losses that you got so far, and then you construct the ellipsoid with this with the least squares solution as the center, and then you pick pick an action which gives the best reward within this ellipsoid.",
            "Best, sorry, best loss and there's a typo.",
            "It's observed the loss and what we do here is that we replace this step one.",
            "The ordinary list questing with online gradient descent solution.",
            "And so that brings down the complexity and the question now is does it impact the regret?",
            "So we studied this in two 2 popular linear bandit algorithms.",
            "The first algorithm is called the pig algorithm, and there you assume that the arms come from is set, which is strongly convex.",
            "We will make this precise later, and in this case we see that using online gradient descent we don't have any loss in the regret bearing lock factors.",
            "The rate is still, it's the regret is still order of square root of N whereas in the other case using a confidence ball algorithm.",
            "Our current analysis we could get.",
            "I mean a loss of order of empower one, verify deterioration in the regret.",
            "So this is in addition to the square root N. So."
        ],
        [
            "This is pretty much the outline of the talk and.",
            "So we'll start with the.",
            "The strongly convex arms case.",
            "So first we'll give the algorithm that solves least squares.",
            "Sing a random online procedure and then will apply to the PG algorithm and show what the record points are like and then we'll move on to a setting where the armstone come from a strongly convex set and we develop, and in this case we have to explicitly regularize when we do the gradient descent, and so we regularize the problem.",
            "We develop a variant of the algorithm random online with regularization, and then we applied to the confidence by logarithm, which is a well known linear.",
            "Bandit algorithm.",
            "Unsure what the regret points are like.",
            "Linda."
        ],
        [
            "So so.",
            "So this is the algorithm, So what you want to do is.",
            "You have a fixed data setting, so for example a time step and you have your given you're given a set of samples X11 thrill till XNYN.",
            "And what we do is we pick one random sample uniformly uniformly from one to N, and then descend using just that one sample.",
            "So effectively the algorithm is just doing one step, one step at a time, and the sample that it takes is actually a random sample.",
            "It's like using the empirical distribution you take 1 sample do are decent.",
            "And with this, with this scheme simple scheme.",
            "So the idea of this is like for example in linear bandit you picnic, you choose an arm, you get a loss, you observe a loss and then you go on to the next step you choose another arm, you will get another loss.",
            "So what you want to do is you want you want this recursion one to track the corresponding least squares solution.",
            "So this data and we would like to be very close to the data hat which is like data and hat, which is the least squares solution using that set of samples.",
            "But what we do is we take only one step.",
            "Here we pick one random sample.",
            "We do one step here and under some under these assumptions at least the first or standard we assume bounded noise and bounded field features and the last one is just saying that the covariance matrix is.",
            "Minimum eigenvalue, which is something like strong convexity.",
            "This may look like a strong assumption, but in the bandit application that we use, this assumption is indeed satisfied in this first part, whereas in the second part this is not satisfied.",
            "This assumption A3 and we have to explicitly regular regularize the problem and make it strongly convex, so at least for this part we do this and given this so.",
            "We develop in high priority bound this way, so we're just qualifying the L2 norm.",
            "Dayton Hat is the least squares solution and detain is our iterate, random, unlimited, and so.",
            "Using a step size of C by N. Where she has to be chosen carefully so that she has to be greater than one by two mu where mu is coming from E3.",
            "So this is."
        ],
        [
            "Related to strong convexity, and so we have this following high priority bound, and so I think the constant.",
            "The advantage of this high priority mode is that all the constants are explicit, which is quite necessary when we use it in a higher level algorithm to construct their lips, or we need all this constant.",
            "So and there's a there's a problem in the sense that for this optimum rate like 1 by root N. So to get the optimal order of 1 by routine rate, you need to choose the step size is that she has to be greater than C is greater than one by two mu, and so you need to know mu.",
            "But there's a neat trick in stochastic optimization literature which is like doing the Polyak report averaging.",
            "So when you so the idea is instead of taking one by and steps.",
            "What you do is you take bigger steps and then you average the iterate.",
            "So it's like you take one by steps as one by Empire Alpha where Alpha is between half and one not one, and then you average data rate and then it can be shown that you get order on by routine convergence still.",
            "Without I mean for free without, then we thought that condition on the C, but we stick to this bound on 2 becausw in the bandit application that we show next, we know mu actually, so we will make that clear so."
        ],
        [
            "So, given this error bound for the random online algorithm.",
            "We apply it to a simple linear bandit setting a linear bandit setting from from this paper by cyclists and the first author is hard to come on, so and so.",
            "So the idea is that you assume the answer from a compact set and you are given a basis for this set.",
            "So B1 through BD is the basis set of basis vectors and the losses the assumption on the losses being linear is standard in linear bandits.",
            "And note that data star is not known.",
            "To the algorithm.",
            "So the aim of the linear bandit algorithm is to minimize this regret.",
            "So you are evaluated against benchmark, which is like the best point of that space or best army that compact space and the extra assumption in comparison to.",
            "Edit algorithm of the second part where we show.",
            "Non strongly convex case is this last assumption, where we assume that this function G of data.",
            "Is smoothing data.",
            "So this actually translates to some kind of strong convexity assumption in which helps.",
            "Is a random online algorithm and achieve the same order of regret.",
            "So in the second part of the talk will not get rid of this assumption and still try to get some good regret bounds so.",
            "I think I'm going to go ahead with the algorithm."
        ],
        [
            "So the algorithm works this way.",
            "So what you do is each, so the algorithm is.",
            "Of it works over cycles and eat in each cycle you have exploration phase followed by exploitation phase.",
            "So in the exploration phase you pull the basis you pull all the basis vectors.",
            "Once we went through PD and then you compute the least squares solution.",
            "But our our proposal is to use the random online scheme here instead of the least cost solution and then so after this you construct you pick, pick a action, you pick the arm.",
            "Greedy so that X whichever arm you choose speak greedily using this G function, and then you choose this mtime so The thing is so the way it works is first time you pick you pick all the basis vectors once and you pull the greedy and once in the second time again the explosion phase is the same.",
            "You pick all the greedy you pick all the basis vectors once and you pull the gradient twice and so on.",
            "So the exploitation phase is going to grow symbiotically and the idea is that eventually you hit the best time and you keep pulling the best time.",
            "And so the crucial thing is you are given the basis vectors.",
            "So the point is, given the basis vectors, you can compute the minimum eigenvalue and we use it to tune.",
            "Put the analysis of the data records.",
            "And so the question now is, given this.",
            "So the original PG algorithm used the least squares solution in step two of exploration and we replace it with a random online iterate.",
            "And so effectively we're doing this one step there.",
            "For each exploration step, and."
        ],
        [
            "The point is, how does the regret look like?",
            "So the regret of the PG algorithm without online gradient descent was order of square root N, and we established that.",
            "The regret is still the same order.",
            "Barring some log factor, so the log factors are all hidden in history of Earth, and the assumptions that are necessary.",
            "So even and data are saying that the features are bounded and the noise is bounded.",
            "So the two assumptions are directly from the sexiest paper where we say that basis vector basis set of basis vectors are known, and we have this function G which is Lipschitz Lipschitz in Tita.",
            "And some comment on this assumption is that this assumption is satisfied if the set D where the answer located are is like a unit sphere, unit ball and but it's not satisfied.",
            "Usually if you have a discrete set of arms.",
            "If you have a finite set of arms, and so this in particular is really a strong assumption for the arms.",
            "For example, if you take news recommendation, you have a finite set of particles.",
            "Your set of particles and come from a compact set.",
            "So don't come from a unit sphere, and so the second part of this talk is to try developing an algorithm that.",
            "That the tries to solve the least squares by regularising the problem and then using it in an algorithm which does not require this assumption here for."
        ],
        [
            "So.",
            "So so that.",
            "So to reiterate, the problematic assumption is that you assume this.",
            "The covariance matrix is regular in some sense, or it has a minimum value and.",
            "The point is, this has to hold for all in.",
            "As you see, more and more samples.",
            "So where standard solution is to regularize the problem.",
            "So instead of solving the ordinary least squares, you at the regularization constant and we show that this Lambda in which is, we adaptively regularize the problem, and we show that we cannot use a fixed regular, so we cannot have Lambda in.",
            "As like say, some constant or in and with the analysis we saw that we need.",
            "The step size into this Lambda in to be not summable, but if you use one way and it gets becomes summable, and so really so really, the problem is that we have to regularize more more than what we would like.",
            "What we usually do, and then given this problem so the algorithm random online with regular list is is like the random online algorithm with this regulation thrown in, so you pick one random sample out of one through N and then.",
            "Your descend into using this update."
        ],
        [
            "And so yeah, here is so the point.",
            "So The thing is, we develop a bound on how good this solution is determined to Akita star.",
            "So in the end the reason we do enormous for because in linear bandits application we need abounding in Norman this can be brought down to the bounding Aaltonen quite easily by changing the norm in the inequality and here.",
            "Here the point is that we have all the constraints explicit here, and this pretty much helps.",
            "How we construct the ellipsoid in the linear bandit algorithm.",
            "And so.",
            "I'm going to the linear bandit."
        ],
        [
            "So the way the so the algorithm is from this paper by or should any at all.",
            "So the data and the way it works is you construct ellipse are around the.",
            "Least squares solution, but in our case it's about around the iterate random online iterate and then you pick an arm.",
            "The train Step 2.",
            "And you update the least squares solution.",
            "So what we're really doing is that.",
            "We are trying to replace the original confidence ball algorithms least quest step with our random online regularize Detroit.",
            "And.",
            "So the point is, for constructing the ellipsoid.",
            "If you see the constants cap, ion and beaten are coming from this bound.",
            "So this effectively is saying that if you remember this data star is really this assumption.",
            "This data star is.",
            "The gardening, the whole the losses are right so so The thing is what?"
        ],
        [
            "You're saying here is that with high probability, our lips are.",
            "If you construct ellipse sort of size cap iron plus be turned around the turn, the unknown parameter T test our results in the ellipsoid.",
            "So this is really the reason why we have we have to develop a high priority bound with all the explicit constants, because this gets."
        ],
        [
            "Used in the step one of the linear bounded confidence ball algorithm where we construct the ellipsoid and then we chosen our action that gives the minimum loss or all possible vectors in this ellipsoid and.",
            "And so The thing is, the Step 4 is like you update the iterate and if you compare the complexity really is.",
            "If you replace step four with the least squares, normally square solution.",
            "It's like order of the squares and if you run the linear bandit algorithm for N steps.",
            "You have a complexity of order of D squared times here and what we're doing here is that.",
            "You replacing step four with our gradient decent rule.",
            "So which is of order D. We do only one step in each iteration of linear receiver, so it's like an end order of the times in where N is the number of steps of the linear bandit algorithm.",
            "So we have got gaining complexity and this complexity and holds for the previous PG algorithm also and the good part previously was that we had a nice strong convexity assumption and hence we didn't lose anything in the regret.",
            "But here unfortunately."
        ],
        [
            "We cannot.",
            "We cannot match the regret bound of the original confidence ball algorithm.",
            "So when we replace the least squares solution with the onion gradient descent rule, you can see that there's a deeper one by lurking there.",
            "So we have a loss of power on by 5 extra loss in the regret.",
            "It's not the optimum, so the optimum bond was shown in the layer Paper to be order of karate and they gave us the confidence ball algorithm achieves this optimal bond of square root T and the best we could get with this adaptive regularised version is this where we have a loss of power 1 by 5 and.",
            "But, uh, but again, it's.",
            "It's like a trade off and it's like you gain a lot in complexity, but we lose, we lose something in the regret, and probably it's some of any of future work to see if we can improve on this and.",
            "Achieve the best bound restricted mode optimal regret bound while using an efficient solution so.",
            "Looks pretty much what I have."
        ],
        [
            "And just include a saying that we propose two schemes really for solving ordinary least squares and regression version.",
            "And the first algorithm assumes strong convexity, while the second uses adaptive regularization are to get around it, and we developed bounds in expectation finite time bounds both in expectation and high probability, and all of these feed into higher level machine learning algorithms and one such algorithm that we picked us the linear bandit algorithm.",
            "So we picked.",
            "We use, we showed the usefulness in two particular settings and in both settings we gain a lot in terms of complexity, order of the reduction in complexity.",
            "Well and first case for the PGA algorithm, we enclose much in the regret, barring lock factors, the rest for confidence Ball Wizard iteration of order in Power BI fee.",
            "And the question really, for the future work is whether this gap can be improved upon for the conference by logarithm, and we are in the process of getting some experiments done for a newsfeed application.",
            "Where we try to use a linear see based model is the model known algorithm and we use linear recipe with online gradient descent and so far the results are encouraging and should be able to share soon coming soon and.",
            "Yeah, I'll close it.",
            "Something about Albert.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The previous speaker is talking about news recommendation and the need for giving recommendations really fast, and so this work is one step in that direction so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the point is what we want to do is we want to solve normal least squares problems or least squares.",
                    "label": 1
                },
                {
                    "sent": "And some of the well known complexities are listed there.",
                    "label": 0
                },
                {
                    "sent": "It's D is the dimension of the, say X the.",
                    "label": 0
                },
                {
                    "sent": "X vectors and.",
                    "label": 0
                },
                {
                    "sent": "So this is the dimension that usually no and.",
                    "label": 0
                },
                {
                    "sent": "Some of the well known procedures, one well known alternative to solving using Sherman Morrison or some of the other algorithms, is to use stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And this is quite computationally efficient, so the complexity comes down to order of D and.",
                    "label": 0
                },
                {
                    "sent": "And this has been well known so far, but what has not been known as in this work, what we do is we develop high priority bonds.",
                    "label": 1
                },
                {
                    "sent": "Doing some finite time analysis and we give out some explicit constants.",
                    "label": 1
                },
                {
                    "sent": "And some part of this is not fully known yet and which we derive.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the motivation for doing this is really to use this in higher machine level machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And one algorithm that we show we show the usefulness of these schemes.",
                    "label": 0
                },
                {
                    "sent": "The online gradient descent schemes is a linear bandit algorithm, so typical structure of a linear bandit algorithm is like.",
                    "label": 1
                },
                {
                    "sent": "Are your first computer least squares estimate based on the arms that you have chosen so far and actually the losses that you got so far, and then you construct the ellipsoid with this with the least squares solution as the center, and then you pick pick an action which gives the best reward within this ellipsoid.",
                    "label": 1
                },
                {
                    "sent": "Best, sorry, best loss and there's a typo.",
                    "label": 0
                },
                {
                    "sent": "It's observed the loss and what we do here is that we replace this step one.",
                    "label": 0
                },
                {
                    "sent": "The ordinary list questing with online gradient descent solution.",
                    "label": 0
                },
                {
                    "sent": "And so that brings down the complexity and the question now is does it impact the regret?",
                    "label": 0
                },
                {
                    "sent": "So we studied this in two 2 popular linear bandit algorithms.",
                    "label": 0
                },
                {
                    "sent": "The first algorithm is called the pig algorithm, and there you assume that the arms come from is set, which is strongly convex.",
                    "label": 0
                },
                {
                    "sent": "We will make this precise later, and in this case we see that using online gradient descent we don't have any loss in the regret bearing lock factors.",
                    "label": 0
                },
                {
                    "sent": "The rate is still, it's the regret is still order of square root of N whereas in the other case using a confidence ball algorithm.",
                    "label": 1
                },
                {
                    "sent": "Our current analysis we could get.",
                    "label": 0
                },
                {
                    "sent": "I mean a loss of order of empower one, verify deterioration in the regret.",
                    "label": 0
                },
                {
                    "sent": "So this is in addition to the square root N. So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is pretty much the outline of the talk and.",
                    "label": 0
                },
                {
                    "sent": "So we'll start with the.",
                    "label": 0
                },
                {
                    "sent": "The strongly convex arms case.",
                    "label": 1
                },
                {
                    "sent": "So first we'll give the algorithm that solves least squares.",
                    "label": 0
                },
                {
                    "sent": "Sing a random online procedure and then will apply to the PG algorithm and show what the record points are like and then we'll move on to a setting where the armstone come from a strongly convex set and we develop, and in this case we have to explicitly regularize when we do the gradient descent, and so we regularize the problem.",
                    "label": 0
                },
                {
                    "sent": "We develop a variant of the algorithm random online with regularization, and then we applied to the confidence by logarithm, which is a well known linear.",
                    "label": 0
                },
                {
                    "sent": "Bandit algorithm.",
                    "label": 0
                },
                {
                    "sent": "Unsure what the regret points are like.",
                    "label": 0
                },
                {
                    "sent": "Linda.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm, So what you want to do is.",
                    "label": 0
                },
                {
                    "sent": "You have a fixed data setting, so for example a time step and you have your given you're given a set of samples X11 thrill till XNYN.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we pick one random sample uniformly uniformly from one to N, and then descend using just that one sample.",
                    "label": 0
                },
                {
                    "sent": "So effectively the algorithm is just doing one step, one step at a time, and the sample that it takes is actually a random sample.",
                    "label": 0
                },
                {
                    "sent": "It's like using the empirical distribution you take 1 sample do are decent.",
                    "label": 0
                },
                {
                    "sent": "And with this, with this scheme simple scheme.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this is like for example in linear bandit you picnic, you choose an arm, you get a loss, you observe a loss and then you go on to the next step you choose another arm, you will get another loss.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is you want you want this recursion one to track the corresponding least squares solution.",
                    "label": 0
                },
                {
                    "sent": "So this data and we would like to be very close to the data hat which is like data and hat, which is the least squares solution using that set of samples.",
                    "label": 0
                },
                {
                    "sent": "But what we do is we take only one step.",
                    "label": 0
                },
                {
                    "sent": "Here we pick one random sample.",
                    "label": 0
                },
                {
                    "sent": "We do one step here and under some under these assumptions at least the first or standard we assume bounded noise and bounded field features and the last one is just saying that the covariance matrix is.",
                    "label": 0
                },
                {
                    "sent": "Minimum eigenvalue, which is something like strong convexity.",
                    "label": 0
                },
                {
                    "sent": "This may look like a strong assumption, but in the bandit application that we use, this assumption is indeed satisfied in this first part, whereas in the second part this is not satisfied.",
                    "label": 0
                },
                {
                    "sent": "This assumption A3 and we have to explicitly regular regularize the problem and make it strongly convex, so at least for this part we do this and given this so.",
                    "label": 0
                },
                {
                    "sent": "We develop in high priority bound this way, so we're just qualifying the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Dayton Hat is the least squares solution and detain is our iterate, random, unlimited, and so.",
                    "label": 0
                },
                {
                    "sent": "Using a step size of C by N. Where she has to be chosen carefully so that she has to be greater than one by two mu where mu is coming from E3.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Related to strong convexity, and so we have this following high priority bound, and so I think the constant.",
                    "label": 0
                },
                {
                    "sent": "The advantage of this high priority mode is that all the constants are explicit, which is quite necessary when we use it in a higher level algorithm to construct their lips, or we need all this constant.",
                    "label": 0
                },
                {
                    "sent": "So and there's a there's a problem in the sense that for this optimum rate like 1 by root N. So to get the optimal order of 1 by routine rate, you need to choose the step size is that she has to be greater than C is greater than one by two mu, and so you need to know mu.",
                    "label": 0
                },
                {
                    "sent": "But there's a neat trick in stochastic optimization literature which is like doing the Polyak report averaging.",
                    "label": 0
                },
                {
                    "sent": "So when you so the idea is instead of taking one by and steps.",
                    "label": 0
                },
                {
                    "sent": "What you do is you take bigger steps and then you average the iterate.",
                    "label": 0
                },
                {
                    "sent": "So it's like you take one by steps as one by Empire Alpha where Alpha is between half and one not one, and then you average data rate and then it can be shown that you get order on by routine convergence still.",
                    "label": 0
                },
                {
                    "sent": "Without I mean for free without, then we thought that condition on the C, but we stick to this bound on 2 becausw in the bandit application that we show next, we know mu actually, so we will make that clear so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given this error bound for the random online algorithm.",
                    "label": 0
                },
                {
                    "sent": "We apply it to a simple linear bandit setting a linear bandit setting from from this paper by cyclists and the first author is hard to come on, so and so.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you assume the answer from a compact set and you are given a basis for this set.",
                    "label": 1
                },
                {
                    "sent": "So B1 through BD is the basis set of basis vectors and the losses the assumption on the losses being linear is standard in linear bandits.",
                    "label": 0
                },
                {
                    "sent": "And note that data star is not known.",
                    "label": 0
                },
                {
                    "sent": "To the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the aim of the linear bandit algorithm is to minimize this regret.",
                    "label": 1
                },
                {
                    "sent": "So you are evaluated against benchmark, which is like the best point of that space or best army that compact space and the extra assumption in comparison to.",
                    "label": 0
                },
                {
                    "sent": "Edit algorithm of the second part where we show.",
                    "label": 1
                },
                {
                    "sent": "Non strongly convex case is this last assumption, where we assume that this function G of data.",
                    "label": 0
                },
                {
                    "sent": "Is smoothing data.",
                    "label": 0
                },
                {
                    "sent": "So this actually translates to some kind of strong convexity assumption in which helps.",
                    "label": 1
                },
                {
                    "sent": "Is a random online algorithm and achieve the same order of regret.",
                    "label": 0
                },
                {
                    "sent": "So in the second part of the talk will not get rid of this assumption and still try to get some good regret bounds so.",
                    "label": 0
                },
                {
                    "sent": "I think I'm going to go ahead with the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the algorithm works this way.",
                    "label": 0
                },
                {
                    "sent": "So what you do is each, so the algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Of it works over cycles and eat in each cycle you have exploration phase followed by exploitation phase.",
                    "label": 1
                },
                {
                    "sent": "So in the exploration phase you pull the basis you pull all the basis vectors.",
                    "label": 0
                },
                {
                    "sent": "Once we went through PD and then you compute the least squares solution.",
                    "label": 0
                },
                {
                    "sent": "But our our proposal is to use the random online scheme here instead of the least cost solution and then so after this you construct you pick, pick a action, you pick the arm.",
                    "label": 0
                },
                {
                    "sent": "Greedy so that X whichever arm you choose speak greedily using this G function, and then you choose this mtime so The thing is so the way it works is first time you pick you pick all the basis vectors once and you pull the greedy and once in the second time again the explosion phase is the same.",
                    "label": 0
                },
                {
                    "sent": "You pick all the greedy you pick all the basis vectors once and you pull the gradient twice and so on.",
                    "label": 0
                },
                {
                    "sent": "So the exploitation phase is going to grow symbiotically and the idea is that eventually you hit the best time and you keep pulling the best time.",
                    "label": 0
                },
                {
                    "sent": "And so the crucial thing is you are given the basis vectors.",
                    "label": 0
                },
                {
                    "sent": "So the point is, given the basis vectors, you can compute the minimum eigenvalue and we use it to tune.",
                    "label": 0
                },
                {
                    "sent": "Put the analysis of the data records.",
                    "label": 0
                },
                {
                    "sent": "And so the question now is, given this.",
                    "label": 0
                },
                {
                    "sent": "So the original PG algorithm used the least squares solution in step two of exploration and we replace it with a random online iterate.",
                    "label": 0
                },
                {
                    "sent": "And so effectively we're doing this one step there.",
                    "label": 1
                },
                {
                    "sent": "For each exploration step, and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point is, how does the regret look like?",
                    "label": 0
                },
                {
                    "sent": "So the regret of the PG algorithm without online gradient descent was order of square root N, and we established that.",
                    "label": 0
                },
                {
                    "sent": "The regret is still the same order.",
                    "label": 0
                },
                {
                    "sent": "Barring some log factor, so the log factors are all hidden in history of Earth, and the assumptions that are necessary.",
                    "label": 0
                },
                {
                    "sent": "So even and data are saying that the features are bounded and the noise is bounded.",
                    "label": 0
                },
                {
                    "sent": "So the two assumptions are directly from the sexiest paper where we say that basis vector basis set of basis vectors are known, and we have this function G which is Lipschitz Lipschitz in Tita.",
                    "label": 0
                },
                {
                    "sent": "And some comment on this assumption is that this assumption is satisfied if the set D where the answer located are is like a unit sphere, unit ball and but it's not satisfied.",
                    "label": 0
                },
                {
                    "sent": "Usually if you have a discrete set of arms.",
                    "label": 0
                },
                {
                    "sent": "If you have a finite set of arms, and so this in particular is really a strong assumption for the arms.",
                    "label": 0
                },
                {
                    "sent": "For example, if you take news recommendation, you have a finite set of particles.",
                    "label": 0
                },
                {
                    "sent": "Your set of particles and come from a compact set.",
                    "label": 0
                },
                {
                    "sent": "So don't come from a unit sphere, and so the second part of this talk is to try developing an algorithm that.",
                    "label": 0
                },
                {
                    "sent": "That the tries to solve the least squares by regularising the problem and then using it in an algorithm which does not require this assumption here for.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So so that.",
                    "label": 0
                },
                {
                    "sent": "So to reiterate, the problematic assumption is that you assume this.",
                    "label": 0
                },
                {
                    "sent": "The covariance matrix is regular in some sense, or it has a minimum value and.",
                    "label": 0
                },
                {
                    "sent": "The point is, this has to hold for all in.",
                    "label": 0
                },
                {
                    "sent": "As you see, more and more samples.",
                    "label": 1
                },
                {
                    "sent": "So where standard solution is to regularize the problem.",
                    "label": 0
                },
                {
                    "sent": "So instead of solving the ordinary least squares, you at the regularization constant and we show that this Lambda in which is, we adaptively regularize the problem, and we show that we cannot use a fixed regular, so we cannot have Lambda in.",
                    "label": 0
                },
                {
                    "sent": "As like say, some constant or in and with the analysis we saw that we need.",
                    "label": 0
                },
                {
                    "sent": "The step size into this Lambda in to be not summable, but if you use one way and it gets becomes summable, and so really so really, the problem is that we have to regularize more more than what we would like.",
                    "label": 0
                },
                {
                    "sent": "What we usually do, and then given this problem so the algorithm random online with regular list is is like the random online algorithm with this regulation thrown in, so you pick one random sample out of one through N and then.",
                    "label": 0
                },
                {
                    "sent": "Your descend into using this update.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so yeah, here is so the point.",
                    "label": 0
                },
                {
                    "sent": "So The thing is, we develop a bound on how good this solution is determined to Akita star.",
                    "label": 0
                },
                {
                    "sent": "So in the end the reason we do enormous for because in linear bandits application we need abounding in Norman this can be brought down to the bounding Aaltonen quite easily by changing the norm in the inequality and here.",
                    "label": 0
                },
                {
                    "sent": "Here the point is that we have all the constraints explicit here, and this pretty much helps.",
                    "label": 0
                },
                {
                    "sent": "How we construct the ellipsoid in the linear bandit algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to the linear bandit.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way the so the algorithm is from this paper by or should any at all.",
                    "label": 0
                },
                {
                    "sent": "So the data and the way it works is you construct ellipse are around the.",
                    "label": 0
                },
                {
                    "sent": "Least squares solution, but in our case it's about around the iterate random online iterate and then you pick an arm.",
                    "label": 0
                },
                {
                    "sent": "The train Step 2.",
                    "label": 0
                },
                {
                    "sent": "And you update the least squares solution.",
                    "label": 0
                },
                {
                    "sent": "So what we're really doing is that.",
                    "label": 0
                },
                {
                    "sent": "We are trying to replace the original confidence ball algorithms least quest step with our random online regularize Detroit.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the point is, for constructing the ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "If you see the constants cap, ion and beaten are coming from this bound.",
                    "label": 0
                },
                {
                    "sent": "So this effectively is saying that if you remember this data star is really this assumption.",
                    "label": 0
                },
                {
                    "sent": "This data star is.",
                    "label": 0
                },
                {
                    "sent": "The gardening, the whole the losses are right so so The thing is what?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're saying here is that with high probability, our lips are.",
                    "label": 0
                },
                {
                    "sent": "If you construct ellipse sort of size cap iron plus be turned around the turn, the unknown parameter T test our results in the ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So this is really the reason why we have we have to develop a high priority bound with all the explicit constants, because this gets.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Used in the step one of the linear bounded confidence ball algorithm where we construct the ellipsoid and then we chosen our action that gives the minimum loss or all possible vectors in this ellipsoid and.",
                    "label": 1
                },
                {
                    "sent": "And so The thing is, the Step 4 is like you update the iterate and if you compare the complexity really is.",
                    "label": 0
                },
                {
                    "sent": "If you replace step four with the least squares, normally square solution.",
                    "label": 1
                },
                {
                    "sent": "It's like order of the squares and if you run the linear bandit algorithm for N steps.",
                    "label": 0
                },
                {
                    "sent": "You have a complexity of order of D squared times here and what we're doing here is that.",
                    "label": 0
                },
                {
                    "sent": "You replacing step four with our gradient decent rule.",
                    "label": 0
                },
                {
                    "sent": "So which is of order D. We do only one step in each iteration of linear receiver, so it's like an end order of the times in where N is the number of steps of the linear bandit algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have got gaining complexity and this complexity and holds for the previous PG algorithm also and the good part previously was that we had a nice strong convexity assumption and hence we didn't lose anything in the regret.",
                    "label": 0
                },
                {
                    "sent": "But here unfortunately.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot match the regret bound of the original confidence ball algorithm.",
                    "label": 1
                },
                {
                    "sent": "So when we replace the least squares solution with the onion gradient descent rule, you can see that there's a deeper one by lurking there.",
                    "label": 1
                },
                {
                    "sent": "So we have a loss of power on by 5 extra loss in the regret.",
                    "label": 0
                },
                {
                    "sent": "It's not the optimum, so the optimum bond was shown in the layer Paper to be order of karate and they gave us the confidence ball algorithm achieves this optimal bond of square root T and the best we could get with this adaptive regularised version is this where we have a loss of power 1 by 5 and.",
                    "label": 0
                },
                {
                    "sent": "But, uh, but again, it's.",
                    "label": 0
                },
                {
                    "sent": "It's like a trade off and it's like you gain a lot in complexity, but we lose, we lose something in the regret, and probably it's some of any of future work to see if we can improve on this and.",
                    "label": 0
                },
                {
                    "sent": "Achieve the best bound restricted mode optimal regret bound while using an efficient solution so.",
                    "label": 0
                },
                {
                    "sent": "Looks pretty much what I have.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just include a saying that we propose two schemes really for solving ordinary least squares and regression version.",
                    "label": 1
                },
                {
                    "sent": "And the first algorithm assumes strong convexity, while the second uses adaptive regularization are to get around it, and we developed bounds in expectation finite time bounds both in expectation and high probability, and all of these feed into higher level machine learning algorithms and one such algorithm that we picked us the linear bandit algorithm.",
                    "label": 1
                },
                {
                    "sent": "So we picked.",
                    "label": 0
                },
                {
                    "sent": "We use, we showed the usefulness in two particular settings and in both settings we gain a lot in terms of complexity, order of the reduction in complexity.",
                    "label": 1
                },
                {
                    "sent": "Well and first case for the PGA algorithm, we enclose much in the regret, barring lock factors, the rest for confidence Ball Wizard iteration of order in Power BI fee.",
                    "label": 0
                },
                {
                    "sent": "And the question really, for the future work is whether this gap can be improved upon for the conference by logarithm, and we are in the process of getting some experiments done for a newsfeed application.",
                    "label": 0
                },
                {
                    "sent": "Where we try to use a linear see based model is the model known algorithm and we use linear recipe with online gradient descent and so far the results are encouraging and should be able to share soon coming soon and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll close it.",
                    "label": 0
                },
                {
                    "sent": "Something about Albert.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}