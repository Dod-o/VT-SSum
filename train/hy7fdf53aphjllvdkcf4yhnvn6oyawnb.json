{
    "id": "hy7fdf53aphjllvdkcf4yhnvn6oyawnb",
    "title": "Stationary Subspace Analysis",
    "info": {
        "author": [
            "Paul von Bunau, TU Berlin",
            "Frank C. Meinecke, TU Berlin",
            "Klaus-Robert M\u00fcller, Department of Software Engineering and Theoretical Computer Science, Technische Universit\u00e4t Berlin"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aml08_bunau_ssa/",
    "segmentation": [
        [
            "Stationary subspace analysis.",
            "Thank you very much for the introduction.",
            "I simply assume that you're not intimidated by this title.",
            "So I will present our work on stationary subspace analysis and you might ask yourself why am I talking about this in a workshop on algebraic methods, and the reason is that we use a small trick to come up with an efficient optimization procedure over the set of orthogonal matrices, which helps us to derive an efficient algorithm.",
            "So what?"
        ],
        [
            "The present now will be very down to Earth, but on the other hand it works very well in practice so.",
            "I hope you enjoy it and maybe.",
            "Can wind down a little bit before we go skiing.",
            "So this is the overview I will start with a brief motivation of why we might be interested to do stationary subspace nalysis at all, then more formalized the problem, but tell you about the inherent symmetries and invariances in the problem itself.",
            "An then we go about the practical part, how we optimize, how we measure stationarity.",
            "Then we have some empirical evaluation which includes an application to brain computer interface."
        ],
        [
            "So as we might all know, real data that we get from any kind of sources might very often be.",
            "Be affected by some non stationary non stationary phenomenon.",
            "So even though in machine learning will always assume that we can in a straightforward manner generalize from the training set to the test set, in practice that is often not true.",
            "'cause perhaps the data generating system changes overtime, or we might just not be able to collect data that for the training phase that represents our test domain exactly, or it might be very costly."
        ],
        [
            "However, there is hope.",
            "So we have good reason to believe that for many data generating systems there is a smaller part which remains stationary while only some parts of it are nonstationary.",
            "We will see this later in our brain computer interfacing application.",
            "So if that is the case then we may be able to simply get rid of the nonstationary part and confine our classical learning systems to the stationary part or apart from these applications we might just also be interested in understanding.",
            "What is going on in the data overtime?",
            "What are the nonstationarity's and for these two things, stationary subspace analysis should be helpful."
        ],
        [
            "So let's start off with the most simple possible example.",
            "So here we have a non stationary source and a stationary source and this is the time series that we have divided into 4 approx and for every epoch we have shown here the scatter plot.",
            "So the first epoch there was drunkenly correlation between stationary and non stationary component.",
            "In the second epoch there is an anti correlation and here we have very little correlation but the variance of the non stationary source changes get smaller and here there's some correlation again and the variance of the nonstationary part gets gets larger.",
            "So if we now have like whatever any type of mixture of the stationary and non stationary source then we want to separate them.",
            "Then from this simple example, we can see that in principle, PCA or ISA could not perform these tasks.",
            "First of all, because we have some correlations between the stationary in the nonstationary part and these correlations they also change overtime.",
            "Could not just classical methods for that."
        ],
        [
            "OK, so the assumption behind our approach is, as I said before, that the nonstationarity is confined to a lower dimensional subspace of the data, so we have some."
        ],
        [
            "Stationary source signals.",
            "And the rest."
        ],
        [
            "The source signals are non station."
        ],
        [
            "And what we observe is some linear mixture of the stationary and non stationary sources.",
            "So this IIS spends the stationary subspace and the N spends the nonstationary subspace.",
            "We see only the mixed version of the sources."
        ],
        [
            "And what we want to do, given only the X we want to estimate the mixing matrix such that we find the dimmest mixing matrix behat, which allows us to separate the stationary from the non stationary sources.",
            "So we want to have some estimated stationary sources which are stationary and all the nonstationarity's should be confined to the estimated non stationary sources."
        ],
        [
            "This is our goal.",
            "But before we start talking about the concrete methods which achieves this, let's first have a look at up to which symmetries we can identify the solution.",
            "So clearly, of course having the correct mixing matrix would provide a solution, but there may be other solutions."
        ],
        [
            "So in order to best investigate this, let us express the true mixing matrices as a linear combination of the estimated mixing matrices.",
            "Having formulated in this way."
        ],
        [
            "We can write down the composite transformation, so make through mixing followed by estimated mixing to relat the true sources to the estimated sources, and I remember our goal was to estimate the stationary sources so the stationary sources estimated stationary sources should be stationary.",
            "So we require this M3 here to be 0, so there should not be any non stationary component.",
            "In the estimated stationary sources."
        ],
        [
            "OK, so this is what we want, but.",
            "The M1M2 and M4 remain unconstrained here."
        ],
        [
            "And since they remain unconstrained, we can choose them in such a way that the.",
            "That the stationary subspace is orthogonal to the non stationary subspace."
        ],
        [
            "And Moreover, we can.",
            "Also we have the freedom to choose orthogonal basis within the non stationary and the same stationary subspace.",
            "And if we do these two things, then we have effectively restricted ourselves to the estimation of orthogonal mixing and mixing matrices without losing any general."
        ],
        [
            "So this this is a nice thing and we will later see how we can.",
            "Optimize efficiently over the set of orthogonal matrices.",
            "This is the algebraic trick here."
        ],
        [
            "So.",
            "Anne.",
            "For our purpose.",
            "What we will do is we will be given N datasets and we will.",
            "Consider D estimated sources as stationary if their joint distribution of all the end data sets is the same.",
            "This is our concept."
        ],
        [
            "Stationarity?",
            "And in order to measure this, we will simply use the pairwise cuckoo Clock divergance between the distributions in the stationary subspace.",
            "Overall, the end datasets."
        ],
        [
            "And Moreover, for practical reasons we will confine ourselves to comparing distributions only in the first 2 moments.",
            "And then we can simply use a Gaussian approximation for the densities and compute the kullback Leibler divergent between Gaussians, which can be evaluated readily."
        ],
        [
            "So.",
            "In order to stay within the feasible set of orthogonal matrices, we employ multiplicative update scheme.",
            "So we start with the identity as a demixing matrix and then at every step rotate the demixing matrix by some some rotation R."
        ],
        [
            "The loss function that we want to minimize is now there.",
            "The sum of all pairs of data sets where we sum up the kullback.",
            "Leiber divergent between the estimated normal distributions of the two.",
            "Data set so the data set I in the data set J and So what we just do is that we take the estimated mean and the estimated covariance matrix and transform them accordingly and simply take the first D components which correspond to the stationary part.",
            "So here we will only only only the.",
            "Only the stationary part of the data enters the last function.",
            "It's a lower dimensional problem."
        ],
        [
            "So.",
            "What we do effectively is that we optimize over the manifold of all orthogonal matrices."
        ],
        [
            "And in a very cartoony way, we can think about the manifold of orthogonal.",
            "Of rotations as a as a curve, so every element on this curve corresponds to some rotation and."
        ],
        [
            "Luckily what we exploit is that we know from group theory that every rotation, so every orthogonal matrix corresponds to some element in the associated Lee Group, which is that the tangent space and this Lee Group is the set of skew symmetric matrices.",
            "So what we will do effectively is that we make a step here.",
            "In the Lee algebra skew symmetric matrices and then map back from here to the orthogonal group using the exponential map.",
            "By doing this, we will never leave."
        ],
        [
            "Defeasible set.",
            "OK.",
            "So formally."
        ],
        [
            "We can express the rotation R that we seek in every step as the matrix of exponential sum, skew symmetric matrix, and then we can do the optimization in the set of all EMS and these skew symmetric matrices also have a nice interpretation.",
            "So the element image can be interpreted as the rotation between the generalized rotation angle between the axis I and the X is Jay.",
            "And later on we will also be able to preserve the nice form of the.",
            "Of the gradient, which we can readily compute from the gradient of the loss function by R."
        ],
        [
            "The from the.",
            "From the form of the grade, from the algebraic expression of the gradient, we can easily see that the gradient has this form.",
            "So the zero block here corresponds to all rotations within the two subspaces, because clearly if we do some rotation in the non stationary or the stationary subspace, that should not change.",
            "The should not change the solution.",
            "The only rotations that matter are the rotations that.",
            "That rotate between the two subspaces.",
            "So we can effectively reduce the number of parameters to the elements of the set."
        ],
        [
            "And what we do then in practice is that we do conjugate gradient optimization in this.",
            "Angle space here."
        ],
        [
            "OK.",
            "So now let us turn to the empirical evaluation, so we will first evaluate the practical utility on some synthetic data where we have just chosen some covariance matrices and means for the first the components are stationary and then we have mixed them with some randomly sampled, well behaved mixing matrix and then we apply essay in order to see whether we can recover the true known stationary subspace from the data.",
            "And we do this for.",
            "Varying numbers of observed data sets and dimensionality's of the stationary subspace."
        ],
        [
            "So these are the results.",
            "So for different dimensionalities of the stationary subspace, we get.",
            "Some.",
            "Yeah, we try to recover it and as a measure of as a performance measure, we employ the angle between the found subspace to the true known stationary subspace.",
            "So for example, if we have 25 datasets and our non stationary or stationary subspace is 9 dimensional, then the median angle to the ground truth is around 55 degrees.",
            "So what we can see here by just looking at the green curve is that the performance scales with the degrees of freedom, which is what we would expect.",
            "We can also see that it is highly beneficial to have more datasets.",
            "So if we have only observed 8 datasets for instance.",
            "Then it can easily happen that there are some seemingly stationary directions in the data, which are in fact not the true stationary.",
            "Stationary parts of the data generating system, on the other hand, it will never happen, never happened that we mistake a stationary direction for non stationary.",
            "So that's why these curves are not symmetric."
        ],
        [
            "OK. Now let's turn to our application to brain computer interfacing.",
            "So in the BCI paradigm that we are interested in, the task is usually to distinguish movements of the left imagined movements of the left and right hand from e.g signals.",
            "However, unfortunately there is also some strong non task related activity going on in the brain which.",
            "It is connected to the state of the brain.",
            "So for example, the tiredness of the subject if we're talking about the Alpha activity and during the calibration phase this brain state can be different from the feedback phase.",
            "So the calibration phase we we train our classifier and then maybe in the feedback phase the subject should perform some task like Play Tetris or Control the pinball machine or whatever and.",
            "These two phases are usually very different in nature of the brain.",
            "State can change and if we have trained here on some data where say there is a high Alpha activity then we may get here in the feedback phase.",
            "We have low Alpha activity that it may be difficult to get good classification results."
        ],
        [
            "So what we're trying now experience to be invariant towards these type of nonstationarity's.",
            "And in order to conduct conduct controlled experiment, we.",
            "We measure some nonstationarity's name here.",
            "The algorithm in a separate artifact measurement and then add these nonstationarity's to the test portion of the data in varying varying strengths."
        ],
        [
            "So we have.",
            "We will divide our data into three parts, so we have the training set with some level of Alpha activity and we have the test set with a different level of ultra Alpha activity and then we will chop off the first thirty trials from the from the test set as an adaptation part and over the adaptation part in the training set we will use SSA to estimate a stationary subspace and within this stationary subspace we will then confine or do our train our classifier.",
            "And then see whether by doing this, we can be robust towards the nonstationarity's that we have induced here.",
            "But first, let's have a relative."
        ],
        [
            "Look at how the stationary and non stationary subspace look like.",
            "So this is a Scout plot.",
            "So every cross here corresponds to an electrode electrode on the head.",
            "And what we see here is the relative power differences between the training and test set for every electrode.",
            "For the stationary and non stationary subspace.",
            "So we can see that most of the changes most of the nonstationarity's are actually really confined to the non stationary subspace, while there is little change in the stationary subspace.",
            "So this gives us some hope that we have actually found some.",
            "Some subspace, which awesome factorization which well separates stationarity's from non stationarity."
        ],
        [
            "Now, now let's have a look how this impacts the classification performance.",
            "So here every these four boxplot correspond to varying levels of nonstationarity's that we have induced.",
            "So.",
            "And the dashed black line is the performance of a baseline method where we just trained on all the components.",
            "And as we can see with increasing nonstationarity's the performance gets worse.",
            "In here we have almost no no reasonable classification performance left at all.",
            "However, if we confine the learning algorithms to a 45 dimensional stationary subspace out of 49 input dimension or electrodes, we can actually retain the good performance.",
            "Even under very strong nonstationarity's.",
            "Anne.",
            "But of course, as we start to remove more information from the data, this will also impact on the classification performance, so it will have maybe some shape like this if we investigate more dimensionalities for this office stationary subspace."
        ],
        [
            "OK, so to sum up, yeah we have presented an algorithm for doing stationary subspace analysis, factoring in time series is stationary and non stationary part and we have shown that we can restrict our search for the demixing matrix to the set of orthogonal matrices without losing any generality and we have exploited the underlyingly group structure of the feasible sets to come up with an efficient optimization and our application both in simulated data and brain computer data showed good results.",
            "Thank you."
        ],
        [
            "Questions.",
            "I have a question about the loss function without using yes.",
            "Right?",
            "Right?",
            "If you compute the KL divergent.",
            "On the other way round.",
            "Performance is optimization OK?",
            "We haven't tried that, actually.",
            "I don't know, yeah, but that's a good point.",
            "Sure, yeah.",
            "Yes.",
            "On manifolds, do you face any problems with local?",
            "Yes yeah there are.",
            "There are local minima definitely, but in practice if we just use a small number of restarts, that's not a big problem and the optimization itself is very fast.",
            "So we have all these experiments.",
            "I haven't set this.",
            "I'm sorry.",
            "We always use five restarts and that's normal.",
            "Not a big, not a big deal, but yes, there are local minima, definitely.",
            "Second last slide the 2nd.",
            "That was the this one.",
            "OK, So what happens if you don't?"
        ],
        [
            "If you don't have try to estimate the non stationary subspace at all and you just.",
            "Take.",
            "This many components, then we get the.",
            "This is the baseline.",
            "That's the baseline exactly exactly.",
            "Yeah, and these box plots are here over just repetitions of the optimization.",
            "So here here gap between the dotted line and the red line is the improvement exactly.",
            "Exactly and and also what I haven't said here.",
            "The blue triangle corresponds to.",
            "That stationary switch with the lowest minimum objective function value.",
            "Yep.",
            "Sorry I missed the first show.",
            "This way.",
            "How do you define stationary OK?",
            "Depends on the time scale you're talking about.",
            "With that somehow determines by you pop priority.",
            "Or yeah, we're just using a very simple notion of stationarity, and that is the that we're given M datasets here, and we say that some components are stationary if their joint distributions are the same.",
            "Overall, the datasets that we have, and we measure this using the collaborator versions.",
            "It is very practical straightforward.",
            "Type of concept, yeah.",
            "Right?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stationary subspace analysis.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for the introduction.",
                    "label": 0
                },
                {
                    "sent": "I simply assume that you're not intimidated by this title.",
                    "label": 0
                },
                {
                    "sent": "So I will present our work on stationary subspace analysis and you might ask yourself why am I talking about this in a workshop on algebraic methods, and the reason is that we use a small trick to come up with an efficient optimization procedure over the set of orthogonal matrices, which helps us to derive an efficient algorithm.",
                    "label": 1
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The present now will be very down to Earth, but on the other hand it works very well in practice so.",
                    "label": 0
                },
                {
                    "sent": "I hope you enjoy it and maybe.",
                    "label": 0
                },
                {
                    "sent": "Can wind down a little bit before we go skiing.",
                    "label": 0
                },
                {
                    "sent": "So this is the overview I will start with a brief motivation of why we might be interested to do stationary subspace nalysis at all, then more formalized the problem, but tell you about the inherent symmetries and invariances in the problem itself.",
                    "label": 1
                },
                {
                    "sent": "An then we go about the practical part, how we optimize, how we measure stationarity.",
                    "label": 1
                },
                {
                    "sent": "Then we have some empirical evaluation which includes an application to brain computer interface.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as we might all know, real data that we get from any kind of sources might very often be.",
                    "label": 0
                },
                {
                    "sent": "Be affected by some non stationary non stationary phenomenon.",
                    "label": 0
                },
                {
                    "sent": "So even though in machine learning will always assume that we can in a straightforward manner generalize from the training set to the test set, in practice that is often not true.",
                    "label": 0
                },
                {
                    "sent": "'cause perhaps the data generating system changes overtime, or we might just not be able to collect data that for the training phase that represents our test domain exactly, or it might be very costly.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, there is hope.",
                    "label": 0
                },
                {
                    "sent": "So we have good reason to believe that for many data generating systems there is a smaller part which remains stationary while only some parts of it are nonstationary.",
                    "label": 1
                },
                {
                    "sent": "We will see this later in our brain computer interfacing application.",
                    "label": 0
                },
                {
                    "sent": "So if that is the case then we may be able to simply get rid of the nonstationary part and confine our classical learning systems to the stationary part or apart from these applications we might just also be interested in understanding.",
                    "label": 1
                },
                {
                    "sent": "What is going on in the data overtime?",
                    "label": 1
                },
                {
                    "sent": "What are the nonstationarity's and for these two things, stationary subspace analysis should be helpful.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start off with the most simple possible example.",
                    "label": 0
                },
                {
                    "sent": "So here we have a non stationary source and a stationary source and this is the time series that we have divided into 4 approx and for every epoch we have shown here the scatter plot.",
                    "label": 0
                },
                {
                    "sent": "So the first epoch there was drunkenly correlation between stationary and non stationary component.",
                    "label": 1
                },
                {
                    "sent": "In the second epoch there is an anti correlation and here we have very little correlation but the variance of the non stationary source changes get smaller and here there's some correlation again and the variance of the nonstationary part gets gets larger.",
                    "label": 1
                },
                {
                    "sent": "So if we now have like whatever any type of mixture of the stationary and non stationary source then we want to separate them.",
                    "label": 0
                },
                {
                    "sent": "Then from this simple example, we can see that in principle, PCA or ISA could not perform these tasks.",
                    "label": 0
                },
                {
                    "sent": "First of all, because we have some correlations between the stationary in the nonstationary part and these correlations they also change overtime.",
                    "label": 0
                },
                {
                    "sent": "Could not just classical methods for that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the assumption behind our approach is, as I said before, that the nonstationarity is confined to a lower dimensional subspace of the data, so we have some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stationary source signals.",
                    "label": 0
                },
                {
                    "sent": "And the rest.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The source signals are non station.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we observe is some linear mixture of the stationary and non stationary sources.",
                    "label": 1
                },
                {
                    "sent": "So this IIS spends the stationary subspace and the N spends the nonstationary subspace.",
                    "label": 0
                },
                {
                    "sent": "We see only the mixed version of the sources.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we want to do, given only the X we want to estimate the mixing matrix such that we find the dimmest mixing matrix behat, which allows us to separate the stationary from the non stationary sources.",
                    "label": 0
                },
                {
                    "sent": "So we want to have some estimated stationary sources which are stationary and all the nonstationarity's should be confined to the estimated non stationary sources.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our goal.",
                    "label": 0
                },
                {
                    "sent": "But before we start talking about the concrete methods which achieves this, let's first have a look at up to which symmetries we can identify the solution.",
                    "label": 0
                },
                {
                    "sent": "So clearly, of course having the correct mixing matrix would provide a solution, but there may be other solutions.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to best investigate this, let us express the true mixing matrices as a linear combination of the estimated mixing matrices.",
                    "label": 0
                },
                {
                    "sent": "Having formulated in this way.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can write down the composite transformation, so make through mixing followed by estimated mixing to relat the true sources to the estimated sources, and I remember our goal was to estimate the stationary sources so the stationary sources estimated stationary sources should be stationary.",
                    "label": 1
                },
                {
                    "sent": "So we require this M3 here to be 0, so there should not be any non stationary component.",
                    "label": 0
                },
                {
                    "sent": "In the estimated stationary sources.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what we want, but.",
                    "label": 0
                },
                {
                    "sent": "The M1M2 and M4 remain unconstrained here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And since they remain unconstrained, we can choose them in such a way that the.",
                    "label": 0
                },
                {
                    "sent": "That the stationary subspace is orthogonal to the non stationary subspace.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Moreover, we can.",
                    "label": 0
                },
                {
                    "sent": "Also we have the freedom to choose orthogonal basis within the non stationary and the same stationary subspace.",
                    "label": 1
                },
                {
                    "sent": "And if we do these two things, then we have effectively restricted ourselves to the estimation of orthogonal mixing and mixing matrices without losing any general.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this is a nice thing and we will later see how we can.",
                    "label": 0
                },
                {
                    "sent": "Optimize efficiently over the set of orthogonal matrices.",
                    "label": 0
                },
                {
                    "sent": "This is the algebraic trick here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "For our purpose.",
                    "label": 0
                },
                {
                    "sent": "What we will do is we will be given N datasets and we will.",
                    "label": 0
                },
                {
                    "sent": "Consider D estimated sources as stationary if their joint distribution of all the end data sets is the same.",
                    "label": 1
                },
                {
                    "sent": "This is our concept.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stationarity?",
                    "label": 0
                },
                {
                    "sent": "And in order to measure this, we will simply use the pairwise cuckoo Clock divergance between the distributions in the stationary subspace.",
                    "label": 1
                },
                {
                    "sent": "Overall, the end datasets.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Moreover, for practical reasons we will confine ourselves to comparing distributions only in the first 2 moments.",
                    "label": 0
                },
                {
                    "sent": "And then we can simply use a Gaussian approximation for the densities and compute the kullback Leibler divergent between Gaussians, which can be evaluated readily.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In order to stay within the feasible set of orthogonal matrices, we employ multiplicative update scheme.",
                    "label": 1
                },
                {
                    "sent": "So we start with the identity as a demixing matrix and then at every step rotate the demixing matrix by some some rotation R.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The loss function that we want to minimize is now there.",
                    "label": 1
                },
                {
                    "sent": "The sum of all pairs of data sets where we sum up the kullback.",
                    "label": 0
                },
                {
                    "sent": "Leiber divergent between the estimated normal distributions of the two.",
                    "label": 0
                },
                {
                    "sent": "Data set so the data set I in the data set J and So what we just do is that we take the estimated mean and the estimated covariance matrix and transform them accordingly and simply take the first D components which correspond to the stationary part.",
                    "label": 1
                },
                {
                    "sent": "So here we will only only only the.",
                    "label": 0
                },
                {
                    "sent": "Only the stationary part of the data enters the last function.",
                    "label": 0
                },
                {
                    "sent": "It's a lower dimensional problem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we do effectively is that we optimize over the manifold of all orthogonal matrices.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in a very cartoony way, we can think about the manifold of orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Of rotations as a as a curve, so every element on this curve corresponds to some rotation and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Luckily what we exploit is that we know from group theory that every rotation, so every orthogonal matrix corresponds to some element in the associated Lee Group, which is that the tangent space and this Lee Group is the set of skew symmetric matrices.",
                    "label": 1
                },
                {
                    "sent": "So what we will do effectively is that we make a step here.",
                    "label": 1
                },
                {
                    "sent": "In the Lee algebra skew symmetric matrices and then map back from here to the orthogonal group using the exponential map.",
                    "label": 0
                },
                {
                    "sent": "By doing this, we will never leave.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Defeasible set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So formally.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can express the rotation R that we seek in every step as the matrix of exponential sum, skew symmetric matrix, and then we can do the optimization in the set of all EMS and these skew symmetric matrices also have a nice interpretation.",
                    "label": 1
                },
                {
                    "sent": "So the element image can be interpreted as the rotation between the generalized rotation angle between the axis I and the X is Jay.",
                    "label": 0
                },
                {
                    "sent": "And later on we will also be able to preserve the nice form of the.",
                    "label": 1
                },
                {
                    "sent": "Of the gradient, which we can readily compute from the gradient of the loss function by R.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The from the.",
                    "label": 0
                },
                {
                    "sent": "From the form of the grade, from the algebraic expression of the gradient, we can easily see that the gradient has this form.",
                    "label": 1
                },
                {
                    "sent": "So the zero block here corresponds to all rotations within the two subspaces, because clearly if we do some rotation in the non stationary or the stationary subspace, that should not change.",
                    "label": 1
                },
                {
                    "sent": "The should not change the solution.",
                    "label": 0
                },
                {
                    "sent": "The only rotations that matter are the rotations that.",
                    "label": 0
                },
                {
                    "sent": "That rotate between the two subspaces.",
                    "label": 0
                },
                {
                    "sent": "So we can effectively reduce the number of parameters to the elements of the set.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we do then in practice is that we do conjugate gradient optimization in this.",
                    "label": 0
                },
                {
                    "sent": "Angle space here.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now let us turn to the empirical evaluation, so we will first evaluate the practical utility on some synthetic data where we have just chosen some covariance matrices and means for the first the components are stationary and then we have mixed them with some randomly sampled, well behaved mixing matrix and then we apply essay in order to see whether we can recover the true known stationary subspace from the data.",
                    "label": 1
                },
                {
                    "sent": "And we do this for.",
                    "label": 0
                },
                {
                    "sent": "Varying numbers of observed data sets and dimensionality's of the stationary subspace.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are the results.",
                    "label": 0
                },
                {
                    "sent": "So for different dimensionalities of the stationary subspace, we get.",
                    "label": 1
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we try to recover it and as a measure of as a performance measure, we employ the angle between the found subspace to the true known stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have 25 datasets and our non stationary or stationary subspace is 9 dimensional, then the median angle to the ground truth is around 55 degrees.",
                    "label": 1
                },
                {
                    "sent": "So what we can see here by just looking at the green curve is that the performance scales with the degrees of freedom, which is what we would expect.",
                    "label": 0
                },
                {
                    "sent": "We can also see that it is highly beneficial to have more datasets.",
                    "label": 1
                },
                {
                    "sent": "So if we have only observed 8 datasets for instance.",
                    "label": 0
                },
                {
                    "sent": "Then it can easily happen that there are some seemingly stationary directions in the data, which are in fact not the true stationary.",
                    "label": 0
                },
                {
                    "sent": "Stationary parts of the data generating system, on the other hand, it will never happen, never happened that we mistake a stationary direction for non stationary.",
                    "label": 0
                },
                {
                    "sent": "So that's why these curves are not symmetric.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now let's turn to our application to brain computer interfacing.",
                    "label": 1
                },
                {
                    "sent": "So in the BCI paradigm that we are interested in, the task is usually to distinguish movements of the left imagined movements of the left and right hand from e.g signals.",
                    "label": 0
                },
                {
                    "sent": "However, unfortunately there is also some strong non task related activity going on in the brain which.",
                    "label": 0
                },
                {
                    "sent": "It is connected to the state of the brain.",
                    "label": 0
                },
                {
                    "sent": "So for example, the tiredness of the subject if we're talking about the Alpha activity and during the calibration phase this brain state can be different from the feedback phase.",
                    "label": 1
                },
                {
                    "sent": "So the calibration phase we we train our classifier and then maybe in the feedback phase the subject should perform some task like Play Tetris or Control the pinball machine or whatever and.",
                    "label": 0
                },
                {
                    "sent": "These two phases are usually very different in nature of the brain.",
                    "label": 0
                },
                {
                    "sent": "State can change and if we have trained here on some data where say there is a high Alpha activity then we may get here in the feedback phase.",
                    "label": 0
                },
                {
                    "sent": "We have low Alpha activity that it may be difficult to get good classification results.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're trying now experience to be invariant towards these type of nonstationarity's.",
                    "label": 0
                },
                {
                    "sent": "And in order to conduct conduct controlled experiment, we.",
                    "label": 1
                },
                {
                    "sent": "We measure some nonstationarity's name here.",
                    "label": 0
                },
                {
                    "sent": "The algorithm in a separate artifact measurement and then add these nonstationarity's to the test portion of the data in varying varying strengths.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We will divide our data into three parts, so we have the training set with some level of Alpha activity and we have the test set with a different level of ultra Alpha activity and then we will chop off the first thirty trials from the from the test set as an adaptation part and over the adaptation part in the training set we will use SSA to estimate a stationary subspace and within this stationary subspace we will then confine or do our train our classifier.",
                    "label": 1
                },
                {
                    "sent": "And then see whether by doing this, we can be robust towards the nonstationarity's that we have induced here.",
                    "label": 0
                },
                {
                    "sent": "But first, let's have a relative.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at how the stationary and non stationary subspace look like.",
                    "label": 0
                },
                {
                    "sent": "So this is a Scout plot.",
                    "label": 0
                },
                {
                    "sent": "So every cross here corresponds to an electrode electrode on the head.",
                    "label": 0
                },
                {
                    "sent": "And what we see here is the relative power differences between the training and test set for every electrode.",
                    "label": 1
                },
                {
                    "sent": "For the stationary and non stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "So we can see that most of the changes most of the nonstationarity's are actually really confined to the non stationary subspace, while there is little change in the stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "So this gives us some hope that we have actually found some.",
                    "label": 0
                },
                {
                    "sent": "Some subspace, which awesome factorization which well separates stationarity's from non stationarity.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, now let's have a look how this impacts the classification performance.",
                    "label": 0
                },
                {
                    "sent": "So here every these four boxplot correspond to varying levels of nonstationarity's that we have induced.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the dashed black line is the performance of a baseline method where we just trained on all the components.",
                    "label": 1
                },
                {
                    "sent": "And as we can see with increasing nonstationarity's the performance gets worse.",
                    "label": 0
                },
                {
                    "sent": "In here we have almost no no reasonable classification performance left at all.",
                    "label": 0
                },
                {
                    "sent": "However, if we confine the learning algorithms to a 45 dimensional stationary subspace out of 49 input dimension or electrodes, we can actually retain the good performance.",
                    "label": 0
                },
                {
                    "sent": "Even under very strong nonstationarity's.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "But of course, as we start to remove more information from the data, this will also impact on the classification performance, so it will have maybe some shape like this if we investigate more dimensionalities for this office stationary subspace.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to sum up, yeah we have presented an algorithm for doing stationary subspace analysis, factoring in time series is stationary and non stationary part and we have shown that we can restrict our search for the demixing matrix to the set of orthogonal matrices without losing any generality and we have exploited the underlyingly group structure of the feasible sets to come up with an efficient optimization and our application both in simulated data and brain computer data showed good results.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "I have a question about the loss function without using yes.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "If you compute the KL divergent.",
                    "label": 0
                },
                {
                    "sent": "On the other way round.",
                    "label": 0
                },
                {
                    "sent": "Performance is optimization OK?",
                    "label": 0
                },
                {
                    "sent": "We haven't tried that, actually.",
                    "label": 0
                },
                {
                    "sent": "I don't know, yeah, but that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Sure, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "On manifolds, do you face any problems with local?",
                    "label": 0
                },
                {
                    "sent": "Yes yeah there are.",
                    "label": 0
                },
                {
                    "sent": "There are local minima definitely, but in practice if we just use a small number of restarts, that's not a big problem and the optimization itself is very fast.",
                    "label": 0
                },
                {
                    "sent": "So we have all these experiments.",
                    "label": 0
                },
                {
                    "sent": "I haven't set this.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "We always use five restarts and that's normal.",
                    "label": 0
                },
                {
                    "sent": "Not a big, not a big deal, but yes, there are local minima, definitely.",
                    "label": 0
                },
                {
                    "sent": "Second last slide the 2nd.",
                    "label": 0
                },
                {
                    "sent": "That was the this one.",
                    "label": 0
                },
                {
                    "sent": "OK, So what happens if you don't?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you don't have try to estimate the non stationary subspace at all and you just.",
                    "label": 1
                },
                {
                    "sent": "Take.",
                    "label": 0
                },
                {
                    "sent": "This many components, then we get the.",
                    "label": 0
                },
                {
                    "sent": "This is the baseline.",
                    "label": 0
                },
                {
                    "sent": "That's the baseline exactly exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and these box plots are here over just repetitions of the optimization.",
                    "label": 0
                },
                {
                    "sent": "So here here gap between the dotted line and the red line is the improvement exactly.",
                    "label": 0
                },
                {
                    "sent": "Exactly and and also what I haven't said here.",
                    "label": 0
                },
                {
                    "sent": "The blue triangle corresponds to.",
                    "label": 0
                },
                {
                    "sent": "That stationary switch with the lowest minimum objective function value.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sorry I missed the first show.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "How do you define stationary OK?",
                    "label": 0
                },
                {
                    "sent": "Depends on the time scale you're talking about.",
                    "label": 0
                },
                {
                    "sent": "With that somehow determines by you pop priority.",
                    "label": 0
                },
                {
                    "sent": "Or yeah, we're just using a very simple notion of stationarity, and that is the that we're given M datasets here, and we say that some components are stationary if their joint distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "Overall, the datasets that we have, and we measure this using the collaborator versions.",
                    "label": 0
                },
                {
                    "sent": "It is very practical straightforward.",
                    "label": 0
                },
                {
                    "sent": "Type of concept, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}