{
    "id": "sad7hmkx6vzyod4ov34eccrvuyvavffv",
    "title": "An RKHS for Multi-View Learning and Manifold Co-Regularization",
    "info": {
        "author": [
            "David S. Rosenberg, Department of Statistics, UC Berkeley"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Manifold Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_rosenberg_rkhs/",
    "segmentation": [
        [
            "Target function ETA of X.",
            "This is a function that we believe does well in the data predicting Y from X.",
            "Now, if we assume that there's a good function in each of the two classes F&G, so in F star Energy Star that both do well, there's no division there.",
            "OK.",
            "So if we assume they both do well, then F star is approximately equal to the target ETA.",
            "Angie Stars also approximately equal to ETA.",
            "And So what that tells us is that F star and G star approximately equal to each other.",
            "OK, so under this assumption, what we're looking for is a pair of functions, one in each space that are similar that are approximately equal pointwise, say.",
            "Now, this isn't a powerful assumption."
        ],
        [
            "So try to illustrate on this next slide.",
            "So here's a cartoon depiction of two function classes.",
            "FNG OK, now under this assumption that each class has a good prediction function were justified in pruning out from each of these classes.",
            "FNG any pair of functions that don't agree because the pair were looking for must agree.",
            "So when we've shown here, is this light blue region in the middle of the two function spaces.",
            "Those comprise the functions that have agreed that are agreeing pairs, and by pruning out all the other functions in these spaces, we come up with a problem that's much easier.",
            "Statistically we need fewer data points.",
            "You are labeled data points to find the right functions in these smaller classes than bigger classes.",
            "Essentially, we potentially reduce the estimation error in this type of problem.",
            "Since the motivation, this is the motivation for Multiview Semi supervised learning."
        ],
        [
            "So next we'll look at one particular instantiation of this idea we saw with two R2 views are two RKS is FNG an objective function?",
            "Now recall we're interested for now in predicting with the average of FNG, so we have this labeled loss functional on.",
            "Evaluates the performance of FNG their average on the labeled data.",
            "The usual norm penalties now one for F and one for G and finally that last term in the objective function.",
            "Looks at the L2 disagreement of FNG Ann here.",
            "I've summed over the unlabeled data so we penalize for pairs of FNG that don't agree.",
            "OK.",
            "So solving this optimization problem is straightforward using by the Representer theorem.",
            "It amounts to a QP depending on the loss function, or it can be a quadratic program, or just solving quadratic.",
            "So so so experimental work has been done on this and some theoretical work has been done and what we do here is we reformulate this problem in a similar way with a single."
        ],
        [
            "Arcade chess like this.",
            "So the final production is the final prediction.",
            "Is this point rise average left orange star.",
            "So let's introduce a function space comprising those pointwise averages H~ is F + G / 2 for FNG in those in those views.",
            "Fine so we can show very very simply just some rearrangement that H star.",
            "This average of F star G star.",
            "Can be written as the minimum of the same loss functional plus some new new norm on this function space HO which is defined.",
            "As this is essentially the last three regular.",
            "It's essentially the last two regularization terms plus a minimum to over FNG that average to the H, and so I pose this as a proposed Norman.",
            "It's straightforward to show that this is in fact the norm, but it's much more than just a norm.",
            "It turns out it's the norm for a Hilbert space corresponding to inner product in Hilbert space, and Moreover, it's reproducing kernel Hobart space."
        ],
        [
            "For which we have a closed form Colonel.",
            "So here's the theorem which I've just paraphrased, so there exists a an inner product on that space of average functions, FG over two which H~ is in Ark.",
            "HS an importantly, we have a nice closed form for the kernel for this space.",
            "So this kernel K~ is some kind of some of the kernels of each of the original spaces minus some quadratic form that D prime HD.",
            "And all the unlabeled data, all the information about agreement, that's.",
            "Contained in that quadratic form at the end.",
            "OK."
        ],
        [
            "OK, So what have we gained by this?",
            "We've taken the original problem and come up with a completely equivalent formulation where instead of looking for an F&G and the cross product of two spaces, we're looking for agents single space.",
            "So from a practical perspective, I think this just immediately opens the doors too.",
            "Any number of multiview, semi, supervised learning algorithms you could take your this new kernel from the new RKHS and Plug it into your favorite supervised kernel algorithm.",
            "An immediately convert it into a multiview semi supervised algorithm.",
            "So I think it's nice from a practical point of view, from a theoretical point of view.",
            "It's pretty great because you can immediately apply much of this theory developed for standard supervised kernel learning.",
            "Because we've reformulated this multiview, yes.",
            "Kind of based on the data and then there would be some difficulty fair enough so to act so in practice.",
            "So when you apply these theorems.",
            "If you want to apply them directly without any change, you save conditional on the unlabeled data Bob goes through, so yeah.",
            "So for example, we can just applying directly a standard result that the Rademacher complexity of a ball in Arcgis is.",
            "Is bounded in terms of the trace of the empirical kernel matrix.",
            "We can reproduce the result that was.",
            "Quite a bit of work to do by bare hands."
        ],
        [
            "OK.",
            "So I want to focus in on this result because it's.",
            "Some interesting interpretation and motivates application is manifold Co regularization, so I don't want to write the details out, but you can measure the complexity of a function class.",
            "In many ways, there's VC dimension an what we're interested in here is Rademacher complexity.",
            "And.",
            "Well, we have this result, which is just a direct application of supervised learning arcade chess results that the Rademacher complexity of.",
            "I've written a J, but I should have been the H~ That's a typo.",
            "The Rademacher complexity of the H~ is bounded by this you over out where you.",
            "Is in terms of the traces of the kernel matrices of the two views less this Delta of Lambda, where Lambda is the parameter for the agreement constraint.",
            "An indult of lambdas where all the unlabeled data action takes place.",
            "So the question.",
            "So one question is when we introduced this agreement constraint, how much smaller does the Rademacher complexity get?",
            "OK, remember this pictures of the purple and the blue we shrunk down the spaces.",
            "That was the reduction in the complexity of the class, who ruled out lots of functions that don't agree, and the question is, how much smaller do the function classes get?",
            "So all that reduction is containing the Delta of Lambda term inside the square root, which is always non negative."
        ],
        [
            "And for Lambda equals 0.",
            "There's no reduction 'cause there's no curricula stations to make sense for Lambda bigger than zero.",
            "We can interpret this reduction carefully slowly, so Delta Lambda it's a sum over label points one to LD is a distance metric that will define a second, so it's the squared distance between KF.",
            "Something cagey something.",
            "So what those things in the distance metric are are column vectors.",
            "CFX, I unload this is a column vector of the kernel inner products between.",
            "The ice label point and each of the unlabeled data points in the cagey is a similar column vector.",
            "An I interpret these common vectors as representing the ice label point in two different ways, one as an inner product with the unlabeled in the first kernel and the other is inner product with the unlabeled's in the second kernel.",
            "So two different representations of the single label Point XI.",
            "And then what's this metric?",
            "The metric is defined in terms of a quadratic form that M in the middle.",
            "And M is just the sum of the two unlabeled data kernel matrices.",
            "OK.",
            "So to paraphrase this, we're looking at how far apart the two representations of each label points are in some metric that's determined by the unlabeled data.",
            "So somehow the unlabeled data defines.",
            "Of the.",
            "The structure of the marginal distribution is incorporated into this distance metric.",
            "OK, so kind of the take home message is that the amount of the reduction grows with the amount of difference between the two different representations.",
            "So this motivates one to think about 2 views that are as different as possible.",
            "OK."
        ],
        [
            "So this got us thinking about this manifold.",
            "How do you come up with two views and one thing that's already out there?",
            "Is this manifold regularization idea so?",
            "In manifold regularization, OK.",
            "Thank you and manifold regularization.",
            "You have the idea that you're unless your data lie on a low dimensional manifold in a high dimensional space.",
            "So this is a cartoon example where in R2, but the data represented by this green line.",
            "Is essentially on a 1 dimensional manifold.",
            "And the assumption that is this manifold regularization is that the final function we'd like to predict with should change slowly along the manifold.",
            "So for instance, if the function is the prediction function, F should have similar values at A&B but not as similar values at ANC, because they're also close in the ambient space.",
            "They're far along the manifold.",
            "So there's been a lot of work with this sort of idea.",
            "Some just to extract the manifold and some that you specifically try to regularize the choice of functions to be smooth with respect to the manifold."
        ],
        [
            "So one particular implementation of this idea was this manifold regularization framework by Belkin at all from a few years ago.",
            "And there they have a looking for function F that's first of all smooth with respect to some ambient space.",
            "So maybe have an RBF kernel on our Dee Ann.",
            "You look at this, that's your ambient arche age.",
            "So you want to be smooth with respect to that norm.",
            "But Additionally, they have you add this quadratic form, which encodes.",
            "The type of smoothness you want on the manifold, so it's a quadratic form because you don't actually know the manifold, you only know the unlabeled.",
            "You only know the data points which are discrete, so you ask that, at least on these data points, the function very smoothly.",
            "This amounts to a.",
            "You can write.",
            "This norm is just a quadratic form, it's all finite and discrete.",
            "OK, so that's one approach.",
            "They directly penalize F in two ways at the same time.",
            "We can think about it.",
            "Another approach to this, let's do.",
            "We're looking for 2 views, so let's take the first view.",
            "As trying to find a function that smooth on the ambient space and the 2nd just find another function that smooth with respect to the.",
            "Data points the manifold and asked if these two different functions, essentially in different spaces.",
            "Although they intersect that they agree.",
            "And then finally, we would.",
            "You could ask to predict the average, or you find some way to come up with the final prediction function.",
            "So that's this two view.",
            "Approach to this manifold regularization so called manifold Co regularization and then you see you're looking for FNG.",
            "Each smooth in their respective manifolds and that they agree.",
            "OK.",
            "So one thing that's interesting is.",
            "So Lambda controls how much we want F&G to agree the two different views.",
            "So what happens if we send Lambda to Infinity, OK?",
            "So then we're asking that F&G agree exactly right.",
            "Well then then this reduces to manifold regularization.",
            "Assuming our architectures are big enough etc.",
            "But?",
            "Loosely speaking, manifold regularization is seems to be a limiting case of this manifold Co regularization, or we require exact agreement.",
            "OK."
        ],
        [
            "So thanks so we ran some quick experiments, few datasets.",
            "Of different sizes."
        ],
        [
            "And.",
            "Some interesting results.",
            "This Co regular sized manifold regularization seems to do.",
            "Fair bit better in much in many situations.",
            "And I have some guesses to why, but nothing, nothing precise.",
            "Like come back to this.",
            "Firstly, that's the main.",
            "That's that's what I had to say about the Manifold Co regularization and I just want."
        ],
        [
            "Presents a somewhat of a generalization of the framework I gave before, so everything I wrote was I've been saying multiview, but it's essentially been for two views and then I was saying we are combining our functions from each view by taking the pointwise average OK, but this can be generalized.",
            "Let's start with M views and let's fix an arbitrary linear combination of the functions from the views.",
            "And then everything goes through us before you can you have this?",
            "Objective function we're looking for a single function that's the correct linear combination.",
            "And the other part of the generalization is that we don't.",
            "We can go beyond just asking for agreement in this L2 sense.",
            "We can actually penalize.",
            "With some arbitrary quadratic form, that's what that F transpose MF is.",
            "And everything goes through."
        ],
        [
            "Before just a little bit more complicated, you can get an arc HSA, single arc HS such that you can express this type of multiview.",
            "I would call it pointcloud regularization.",
            "An analogy with some earlier work of I think sanjuanita certainly was an author.",
            "OK."
        ],
        [
            "OK, so let me just give a quick recap.",
            "We presented this arc H this single arc HS for the.",
            "If you learning approach, which is nice because you can easily convert any supervised RKS method to a multi view semi supervised method.",
            "It also allows you to leverage all this theory that already exists for supervised kernel learning.",
            "Then we used the interpretation of the balance to motivate this manifold Co regularization.",
            "There's two view approach version of Manifold Regularization which seems to give improved empirical results with that prior requires some further study to understand.",
            "And finally, at the end, the generalization to multiple views with arbitrary linear combinations and going beyond this agreement coupling term.",
            "Thanks.",
            "How do you create?",
            "2nd.",
            "So there's a lot.",
            "OK, so this one traditional way is if your views come as vectors, for example, in Rd you split the input features.",
            "You split the input coordinates one way.",
            "Another way is used to like I was thinking you have two RKS is used, two different kernels.",
            "Maybe they have different properties.",
            "There's no reason to split the features necessarily.",
            "They could be overlapping.",
            "You can have multiple.",
            "You could take, you know, M random subsets of features and end up with M views.",
            "Translator that's interesting, I'm not sure.",
            "It's actually not so related.",
            "We heard some talks early today.",
            "Their learning.",
            "Do you think that this revised formulation where you're formulating it is a single arc HS as opposed to adding realization terms can help?",
            "If you're want to combine the organization together with probability, so curl learning how to combine kernels.",
            "Examples combination of kernels OK.",
            "Yes, certainly as I mean."
        ],
        [
            "As an outer loop, learning, for example, a one through am.",
            "What's the right linear combination?",
            "Yeah, that would be an interesting extension, so this is fairly tightly related to learning these essentially doing the model selection on these gammas the regularization parameters.",
            "This seems, but.",
            "Yeah, I'd be an interesting extension.",
            "Yeah.",
            "Training.",
            "Features or not.",
            "User as independent as possible so you know framework.",
            "Maybe with the functions are functions that I have energy and the finance team input space, but I'm just wondering whether some kind of independence Oregon correlation between these functions stats are required in order to answer really statistical results.",
            "No, I don't think so.",
            "I mean, somehow it's really sufficient to have this agreement.",
            "This sort of thing.",
            "If you believe there's are good function in each of your function classes.",
            "In a way that they are very close to each other, you lose nothing by this type of pruning.",
            "This type of regularization and you gain an estimation.",
            "So the only fear is that you're losing an approximation error when you prune out looking for agreement, but in fact there's nothing that's good in these classes or in one of these classes to agree with that could be hurting you, but independence doesn't enter into this argument.",
            "Yeah, just a question.",
            "Are there any degeneracy for pop out with both function classes are the same so it just reduces it, just it's as though you were not doing any.",
            "Co regulation at all.",
            "It's as though you're doing supervised learning in one space.",
            "Because asking for agreement, you can always find trivially use the same function twice.",
            "They agree exactly, so it's essentially no matter what the unlabeled data are, so all the unlabeled data fall out.",
            "It's just doing.",
            "Supervised learning and one in one view.",
            "So we still would like to see the question of optical training, so it seems to me that if the two targets F&G are kind of specialized in different regions, yes.",
            "Enforcing agreements will actually block with.",
            "Yeah, this is an issue, so it would be great if you could have different combinations of F&G in different regions of your space.",
            "That's very interesting, but.",
            "This the motivation that we know works.",
            "That's like just self evident that it motivates this method that's doesn't fit there.",
            "So if F is great in one part of space but bad somewhere else and G is great in that bad bar space but good everywhere else, no good.",
            "This system, I mean in practice and maybe OK, but.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Target function ETA of X.",
                    "label": 0
                },
                {
                    "sent": "This is a function that we believe does well in the data predicting Y from X.",
                    "label": 0
                },
                {
                    "sent": "Now, if we assume that there's a good function in each of the two classes F&G, so in F star Energy Star that both do well, there's no division there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if we assume they both do well, then F star is approximately equal to the target ETA.",
                    "label": 1
                },
                {
                    "sent": "Angie Stars also approximately equal to ETA.",
                    "label": 0
                },
                {
                    "sent": "And So what that tells us is that F star and G star approximately equal to each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so under this assumption, what we're looking for is a pair of functions, one in each space that are similar that are approximately equal pointwise, say.",
                    "label": 0
                },
                {
                    "sent": "Now, this isn't a powerful assumption.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So try to illustrate on this next slide.",
                    "label": 0
                },
                {
                    "sent": "So here's a cartoon depiction of two function classes.",
                    "label": 0
                },
                {
                    "sent": "FNG OK, now under this assumption that each class has a good prediction function were justified in pruning out from each of these classes.",
                    "label": 0
                },
                {
                    "sent": "FNG any pair of functions that don't agree because the pair were looking for must agree.",
                    "label": 0
                },
                {
                    "sent": "So when we've shown here, is this light blue region in the middle of the two function spaces.",
                    "label": 0
                },
                {
                    "sent": "Those comprise the functions that have agreed that are agreeing pairs, and by pruning out all the other functions in these spaces, we come up with a problem that's much easier.",
                    "label": 0
                },
                {
                    "sent": "Statistically we need fewer data points.",
                    "label": 0
                },
                {
                    "sent": "You are labeled data points to find the right functions in these smaller classes than bigger classes.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we potentially reduce the estimation error in this type of problem.",
                    "label": 0
                },
                {
                    "sent": "Since the motivation, this is the motivation for Multiview Semi supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next we'll look at one particular instantiation of this idea we saw with two R2 views are two RKS is FNG an objective function?",
                    "label": 0
                },
                {
                    "sent": "Now recall we're interested for now in predicting with the average of FNG, so we have this labeled loss functional on.",
                    "label": 0
                },
                {
                    "sent": "Evaluates the performance of FNG their average on the labeled data.",
                    "label": 0
                },
                {
                    "sent": "The usual norm penalties now one for F and one for G and finally that last term in the objective function.",
                    "label": 0
                },
                {
                    "sent": "Looks at the L2 disagreement of FNG Ann here.",
                    "label": 0
                },
                {
                    "sent": "I've summed over the unlabeled data so we penalize for pairs of FNG that don't agree.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So solving this optimization problem is straightforward using by the Representer theorem.",
                    "label": 0
                },
                {
                    "sent": "It amounts to a QP depending on the loss function, or it can be a quadratic program, or just solving quadratic.",
                    "label": 1
                },
                {
                    "sent": "So so so experimental work has been done on this and some theoretical work has been done and what we do here is we reformulate this problem in a similar way with a single.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arcade chess like this.",
                    "label": 0
                },
                {
                    "sent": "So the final production is the final prediction.",
                    "label": 0
                },
                {
                    "sent": "Is this point rise average left orange star.",
                    "label": 0
                },
                {
                    "sent": "So let's introduce a function space comprising those pointwise averages H~ is F + G / 2 for FNG in those in those views.",
                    "label": 1
                },
                {
                    "sent": "Fine so we can show very very simply just some rearrangement that H star.",
                    "label": 1
                },
                {
                    "sent": "This average of F star G star.",
                    "label": 0
                },
                {
                    "sent": "Can be written as the minimum of the same loss functional plus some new new norm on this function space HO which is defined.",
                    "label": 0
                },
                {
                    "sent": "As this is essentially the last three regular.",
                    "label": 0
                },
                {
                    "sent": "It's essentially the last two regularization terms plus a minimum to over FNG that average to the H, and so I pose this as a proposed Norman.",
                    "label": 0
                },
                {
                    "sent": "It's straightforward to show that this is in fact the norm, but it's much more than just a norm.",
                    "label": 0
                },
                {
                    "sent": "It turns out it's the norm for a Hilbert space corresponding to inner product in Hilbert space, and Moreover, it's reproducing kernel Hobart space.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For which we have a closed form Colonel.",
                    "label": 0
                },
                {
                    "sent": "So here's the theorem which I've just paraphrased, so there exists a an inner product on that space of average functions, FG over two which H~ is in Ark.",
                    "label": 1
                },
                {
                    "sent": "HS an importantly, we have a nice closed form for the kernel for this space.",
                    "label": 0
                },
                {
                    "sent": "So this kernel K~ is some kind of some of the kernels of each of the original spaces minus some quadratic form that D prime HD.",
                    "label": 0
                },
                {
                    "sent": "And all the unlabeled data, all the information about agreement, that's.",
                    "label": 0
                },
                {
                    "sent": "Contained in that quadratic form at the end.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what have we gained by this?",
                    "label": 1
                },
                {
                    "sent": "We've taken the original problem and come up with a completely equivalent formulation where instead of looking for an F&G and the cross product of two spaces, we're looking for agents single space.",
                    "label": 0
                },
                {
                    "sent": "So from a practical perspective, I think this just immediately opens the doors too.",
                    "label": 0
                },
                {
                    "sent": "Any number of multiview, semi, supervised learning algorithms you could take your this new kernel from the new RKHS and Plug it into your favorite supervised kernel algorithm.",
                    "label": 0
                },
                {
                    "sent": "An immediately convert it into a multiview semi supervised algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I think it's nice from a practical point of view, from a theoretical point of view.",
                    "label": 0
                },
                {
                    "sent": "It's pretty great because you can immediately apply much of this theory developed for standard supervised kernel learning.",
                    "label": 1
                },
                {
                    "sent": "Because we've reformulated this multiview, yes.",
                    "label": 0
                },
                {
                    "sent": "Kind of based on the data and then there would be some difficulty fair enough so to act so in practice.",
                    "label": 0
                },
                {
                    "sent": "So when you apply these theorems.",
                    "label": 0
                },
                {
                    "sent": "If you want to apply them directly without any change, you save conditional on the unlabeled data Bob goes through, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can just applying directly a standard result that the Rademacher complexity of a ball in Arcgis is.",
                    "label": 0
                },
                {
                    "sent": "Is bounded in terms of the trace of the empirical kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "We can reproduce the result that was.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit of work to do by bare hands.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I want to focus in on this result because it's.",
                    "label": 0
                },
                {
                    "sent": "Some interesting interpretation and motivates application is manifold Co regularization, so I don't want to write the details out, but you can measure the complexity of a function class.",
                    "label": 0
                },
                {
                    "sent": "In many ways, there's VC dimension an what we're interested in here is Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, we have this result, which is just a direct application of supervised learning arcade chess results that the Rademacher complexity of.",
                    "label": 0
                },
                {
                    "sent": "I've written a J, but I should have been the H~ That's a typo.",
                    "label": 0
                },
                {
                    "sent": "The Rademacher complexity of the H~ is bounded by this you over out where you.",
                    "label": 1
                },
                {
                    "sent": "Is in terms of the traces of the kernel matrices of the two views less this Delta of Lambda, where Lambda is the parameter for the agreement constraint.",
                    "label": 0
                },
                {
                    "sent": "An indult of lambdas where all the unlabeled data action takes place.",
                    "label": 0
                },
                {
                    "sent": "So the question.",
                    "label": 0
                },
                {
                    "sent": "So one question is when we introduced this agreement constraint, how much smaller does the Rademacher complexity get?",
                    "label": 0
                },
                {
                    "sent": "OK, remember this pictures of the purple and the blue we shrunk down the spaces.",
                    "label": 0
                },
                {
                    "sent": "That was the reduction in the complexity of the class, who ruled out lots of functions that don't agree, and the question is, how much smaller do the function classes get?",
                    "label": 0
                },
                {
                    "sent": "So all that reduction is containing the Delta of Lambda term inside the square root, which is always non negative.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for Lambda equals 0.",
                    "label": 0
                },
                {
                    "sent": "There's no reduction 'cause there's no curricula stations to make sense for Lambda bigger than zero.",
                    "label": 0
                },
                {
                    "sent": "We can interpret this reduction carefully slowly, so Delta Lambda it's a sum over label points one to LD is a distance metric that will define a second, so it's the squared distance between KF.",
                    "label": 0
                },
                {
                    "sent": "Something cagey something.",
                    "label": 0
                },
                {
                    "sent": "So what those things in the distance metric are are column vectors.",
                    "label": 0
                },
                {
                    "sent": "CFX, I unload this is a column vector of the kernel inner products between.",
                    "label": 0
                },
                {
                    "sent": "The ice label point and each of the unlabeled data points in the cagey is a similar column vector.",
                    "label": 0
                },
                {
                    "sent": "An I interpret these common vectors as representing the ice label point in two different ways, one as an inner product with the unlabeled in the first kernel and the other is inner product with the unlabeled's in the second kernel.",
                    "label": 0
                },
                {
                    "sent": "So two different representations of the single label Point XI.",
                    "label": 0
                },
                {
                    "sent": "And then what's this metric?",
                    "label": 0
                },
                {
                    "sent": "The metric is defined in terms of a quadratic form that M in the middle.",
                    "label": 0
                },
                {
                    "sent": "And M is just the sum of the two unlabeled data kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So to paraphrase this, we're looking at how far apart the two representations of each label points are in some metric that's determined by the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So somehow the unlabeled data defines.",
                    "label": 0
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "The structure of the marginal distribution is incorporated into this distance metric.",
                    "label": 0
                },
                {
                    "sent": "OK, so kind of the take home message is that the amount of the reduction grows with the amount of difference between the two different representations.",
                    "label": 0
                },
                {
                    "sent": "So this motivates one to think about 2 views that are as different as possible.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this got us thinking about this manifold.",
                    "label": 0
                },
                {
                    "sent": "How do you come up with two views and one thing that's already out there?",
                    "label": 0
                },
                {
                    "sent": "Is this manifold regularization idea so?",
                    "label": 0
                },
                {
                    "sent": "In manifold regularization, OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you and manifold regularization.",
                    "label": 1
                },
                {
                    "sent": "You have the idea that you're unless your data lie on a low dimensional manifold in a high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "So this is a cartoon example where in R2, but the data represented by this green line.",
                    "label": 0
                },
                {
                    "sent": "Is essentially on a 1 dimensional manifold.",
                    "label": 1
                },
                {
                    "sent": "And the assumption that is this manifold regularization is that the final function we'd like to predict with should change slowly along the manifold.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if the function is the prediction function, F should have similar values at A&B but not as similar values at ANC, because they're also close in the ambient space.",
                    "label": 1
                },
                {
                    "sent": "They're far along the manifold.",
                    "label": 1
                },
                {
                    "sent": "So there's been a lot of work with this sort of idea.",
                    "label": 0
                },
                {
                    "sent": "Some just to extract the manifold and some that you specifically try to regularize the choice of functions to be smooth with respect to the manifold.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one particular implementation of this idea was this manifold regularization framework by Belkin at all from a few years ago.",
                    "label": 0
                },
                {
                    "sent": "And there they have a looking for function F that's first of all smooth with respect to some ambient space.",
                    "label": 0
                },
                {
                    "sent": "So maybe have an RBF kernel on our Dee Ann.",
                    "label": 0
                },
                {
                    "sent": "You look at this, that's your ambient arche age.",
                    "label": 0
                },
                {
                    "sent": "So you want to be smooth with respect to that norm.",
                    "label": 0
                },
                {
                    "sent": "But Additionally, they have you add this quadratic form, which encodes.",
                    "label": 0
                },
                {
                    "sent": "The type of smoothness you want on the manifold, so it's a quadratic form because you don't actually know the manifold, you only know the unlabeled.",
                    "label": 0
                },
                {
                    "sent": "You only know the data points which are discrete, so you ask that, at least on these data points, the function very smoothly.",
                    "label": 0
                },
                {
                    "sent": "This amounts to a.",
                    "label": 0
                },
                {
                    "sent": "You can write.",
                    "label": 0
                },
                {
                    "sent": "This norm is just a quadratic form, it's all finite and discrete.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one approach.",
                    "label": 0
                },
                {
                    "sent": "They directly penalize F in two ways at the same time.",
                    "label": 0
                },
                {
                    "sent": "We can think about it.",
                    "label": 0
                },
                {
                    "sent": "Another approach to this, let's do.",
                    "label": 0
                },
                {
                    "sent": "We're looking for 2 views, so let's take the first view.",
                    "label": 0
                },
                {
                    "sent": "As trying to find a function that smooth on the ambient space and the 2nd just find another function that smooth with respect to the.",
                    "label": 0
                },
                {
                    "sent": "Data points the manifold and asked if these two different functions, essentially in different spaces.",
                    "label": 0
                },
                {
                    "sent": "Although they intersect that they agree.",
                    "label": 0
                },
                {
                    "sent": "And then finally, we would.",
                    "label": 0
                },
                {
                    "sent": "You could ask to predict the average, or you find some way to come up with the final prediction function.",
                    "label": 0
                },
                {
                    "sent": "So that's this two view.",
                    "label": 0
                },
                {
                    "sent": "Approach to this manifold regularization so called manifold Co regularization and then you see you're looking for FNG.",
                    "label": 0
                },
                {
                    "sent": "Each smooth in their respective manifolds and that they agree.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one thing that's interesting is.",
                    "label": 0
                },
                {
                    "sent": "So Lambda controls how much we want F&G to agree the two different views.",
                    "label": 0
                },
                {
                    "sent": "So what happens if we send Lambda to Infinity, OK?",
                    "label": 0
                },
                {
                    "sent": "So then we're asking that F&G agree exactly right.",
                    "label": 0
                },
                {
                    "sent": "Well then then this reduces to manifold regularization.",
                    "label": 0
                },
                {
                    "sent": "Assuming our architectures are big enough etc.",
                    "label": 0
                },
                {
                    "sent": "But?",
                    "label": 0
                },
                {
                    "sent": "Loosely speaking, manifold regularization is seems to be a limiting case of this manifold Co regularization, or we require exact agreement.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks so we ran some quick experiments, few datasets.",
                    "label": 0
                },
                {
                    "sent": "Of different sizes.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Some interesting results.",
                    "label": 0
                },
                {
                    "sent": "This Co regular sized manifold regularization seems to do.",
                    "label": 0
                },
                {
                    "sent": "Fair bit better in much in many situations.",
                    "label": 0
                },
                {
                    "sent": "And I have some guesses to why, but nothing, nothing precise.",
                    "label": 0
                },
                {
                    "sent": "Like come back to this.",
                    "label": 0
                },
                {
                    "sent": "Firstly, that's the main.",
                    "label": 0
                },
                {
                    "sent": "That's that's what I had to say about the Manifold Co regularization and I just want.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Presents a somewhat of a generalization of the framework I gave before, so everything I wrote was I've been saying multiview, but it's essentially been for two views and then I was saying we are combining our functions from each view by taking the pointwise average OK, but this can be generalized.",
                    "label": 0
                },
                {
                    "sent": "Let's start with M views and let's fix an arbitrary linear combination of the functions from the views.",
                    "label": 1
                },
                {
                    "sent": "And then everything goes through us before you can you have this?",
                    "label": 0
                },
                {
                    "sent": "Objective function we're looking for a single function that's the correct linear combination.",
                    "label": 0
                },
                {
                    "sent": "And the other part of the generalization is that we don't.",
                    "label": 0
                },
                {
                    "sent": "We can go beyond just asking for agreement in this L2 sense.",
                    "label": 0
                },
                {
                    "sent": "We can actually penalize.",
                    "label": 1
                },
                {
                    "sent": "With some arbitrary quadratic form, that's what that F transpose MF is.",
                    "label": 0
                },
                {
                    "sent": "And everything goes through.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before just a little bit more complicated, you can get an arc HSA, single arc HS such that you can express this type of multiview.",
                    "label": 0
                },
                {
                    "sent": "I would call it pointcloud regularization.",
                    "label": 0
                },
                {
                    "sent": "An analogy with some earlier work of I think sanjuanita certainly was an author.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just give a quick recap.",
                    "label": 0
                },
                {
                    "sent": "We presented this arc H this single arc HS for the.",
                    "label": 0
                },
                {
                    "sent": "If you learning approach, which is nice because you can easily convert any supervised RKS method to a multi view semi supervised method.",
                    "label": 0
                },
                {
                    "sent": "It also allows you to leverage all this theory that already exists for supervised kernel learning.",
                    "label": 0
                },
                {
                    "sent": "Then we used the interpretation of the balance to motivate this manifold Co regularization.",
                    "label": 0
                },
                {
                    "sent": "There's two view approach version of Manifold Regularization which seems to give improved empirical results with that prior requires some further study to understand.",
                    "label": 0
                },
                {
                    "sent": "And finally, at the end, the generalization to multiple views with arbitrary linear combinations and going beyond this agreement coupling term.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "How do you create?",
                    "label": 0
                },
                {
                    "sent": "2nd.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one traditional way is if your views come as vectors, for example, in Rd you split the input features.",
                    "label": 0
                },
                {
                    "sent": "You split the input coordinates one way.",
                    "label": 0
                },
                {
                    "sent": "Another way is used to like I was thinking you have two RKS is used, two different kernels.",
                    "label": 0
                },
                {
                    "sent": "Maybe they have different properties.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to split the features necessarily.",
                    "label": 0
                },
                {
                    "sent": "They could be overlapping.",
                    "label": 0
                },
                {
                    "sent": "You can have multiple.",
                    "label": 0
                },
                {
                    "sent": "You could take, you know, M random subsets of features and end up with M views.",
                    "label": 0
                },
                {
                    "sent": "Translator that's interesting, I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "It's actually not so related.",
                    "label": 0
                },
                {
                    "sent": "We heard some talks early today.",
                    "label": 0
                },
                {
                    "sent": "Their learning.",
                    "label": 0
                },
                {
                    "sent": "Do you think that this revised formulation where you're formulating it is a single arc HS as opposed to adding realization terms can help?",
                    "label": 0
                },
                {
                    "sent": "If you're want to combine the organization together with probability, so curl learning how to combine kernels.",
                    "label": 0
                },
                {
                    "sent": "Examples combination of kernels OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, certainly as I mean.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As an outer loop, learning, for example, a one through am.",
                    "label": 0
                },
                {
                    "sent": "What's the right linear combination?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be an interesting extension, so this is fairly tightly related to learning these essentially doing the model selection on these gammas the regularization parameters.",
                    "label": 0
                },
                {
                    "sent": "This seems, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'd be an interesting extension.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "Features or not.",
                    "label": 0
                },
                {
                    "sent": "User as independent as possible so you know framework.",
                    "label": 0
                },
                {
                    "sent": "Maybe with the functions are functions that I have energy and the finance team input space, but I'm just wondering whether some kind of independence Oregon correlation between these functions stats are required in order to answer really statistical results.",
                    "label": 0
                },
                {
                    "sent": "No, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "I mean, somehow it's really sufficient to have this agreement.",
                    "label": 0
                },
                {
                    "sent": "This sort of thing.",
                    "label": 0
                },
                {
                    "sent": "If you believe there's are good function in each of your function classes.",
                    "label": 0
                },
                {
                    "sent": "In a way that they are very close to each other, you lose nothing by this type of pruning.",
                    "label": 0
                },
                {
                    "sent": "This type of regularization and you gain an estimation.",
                    "label": 0
                },
                {
                    "sent": "So the only fear is that you're losing an approximation error when you prune out looking for agreement, but in fact there's nothing that's good in these classes or in one of these classes to agree with that could be hurting you, but independence doesn't enter into this argument.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just a question.",
                    "label": 0
                },
                {
                    "sent": "Are there any degeneracy for pop out with both function classes are the same so it just reduces it, just it's as though you were not doing any.",
                    "label": 0
                },
                {
                    "sent": "Co regulation at all.",
                    "label": 0
                },
                {
                    "sent": "It's as though you're doing supervised learning in one space.",
                    "label": 0
                },
                {
                    "sent": "Because asking for agreement, you can always find trivially use the same function twice.",
                    "label": 0
                },
                {
                    "sent": "They agree exactly, so it's essentially no matter what the unlabeled data are, so all the unlabeled data fall out.",
                    "label": 0
                },
                {
                    "sent": "It's just doing.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning and one in one view.",
                    "label": 0
                },
                {
                    "sent": "So we still would like to see the question of optical training, so it seems to me that if the two targets F&G are kind of specialized in different regions, yes.",
                    "label": 0
                },
                {
                    "sent": "Enforcing agreements will actually block with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is an issue, so it would be great if you could have different combinations of F&G in different regions of your space.",
                    "label": 0
                },
                {
                    "sent": "That's very interesting, but.",
                    "label": 0
                },
                {
                    "sent": "This the motivation that we know works.",
                    "label": 0
                },
                {
                    "sent": "That's like just self evident that it motivates this method that's doesn't fit there.",
                    "label": 0
                },
                {
                    "sent": "So if F is great in one part of space but bad somewhere else and G is great in that bad bar space but good everywhere else, no good.",
                    "label": 0
                },
                {
                    "sent": "This system, I mean in practice and maybe OK, but.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}