{
    "id": "m4pt5bdli5savphsyjvzcedbzsanjugq",
    "title": "Semantic multimedia remixing",
    "info": {
        "author": [
            "Rapha\u00ebl Troncy, EURECOM",
            "Benoit Huet, EURECOM"
        ],
        "published": "Feb. 10, 2014",
        "recorded": "January 2014",
        "category": [
            "Top->Computer Science->Semantic Web->Annotation",
            "Top->Computer Science->Semantic Web->Applications",
            "Top->Computer Science->Software and Tools"
        ]
    },
    "url": "http://videolectures.net/wmpa2014_huet_troncy_semantic_multimedia/",
    "segmentation": [
        [
            "The title of this lecture now is semantic multimedia remixing, and the first part will be about deep linking into media assets at the fragment level."
        ],
        [
            "I don't know the ones who are familiar with content management systems in the room, so it's EMS to manage your website, and in particular so those are like WordPress or Drupal, and for the ones who are using group or within the in the middle of July last year Drupal eight was announcing that they were supporting particulare vocabulary semantic vocabulary for marking up web pages managed by this content management system.",
            "So this was this announcement from Drupal 8, but actually Drupal is pretty user ready to play."
        ],
        [
            "Semantic technologies.",
            "And this is a much older blog post.",
            "So it's dated from 2010 and here it was Drupal six I think, or perhaps seven was announced and they were already trying to support semantic vocabularies for indexing pages.",
            "And if you read this very serious blog post which is published in our technical Journal, you can read this.",
            "So I will read for you.",
            "I'm not sure you can read it from the back, but basically it tells you you know semantic content management, consuming and producing RDF in report is possible.",
            "And if you're really interesting into this, you should click on this link of this video recording session and then you should specifically go between 9 minutes and annual segments between minutes 44 two minutes, 42.",
            "So it's very precise instructions you have to click on this link.",
            "Sick to the minute 44?",
            "Stop watching at minutes 42 because this is those 9 minutes that are really interesting for you.",
            "If you're interested in this topic now, you could wonder there must be a better way of sharing.",
            "Watch across sequence of a video on the web without relying on those lengthy instructions that you as a human have to interpret in order to be able to play this particular fragment."
        ],
        [
            "So what about so?",
            "This is really as leading to watch ever since the beginning of the morning media fragmente.",
            "So what we really would like to do is be able to publish on Facebook, on Twitter or bookmark or whatever, but direct link which contains this media fragments and when you click on it, you just play the requested sequence and not before and not after.",
            "So this is now possible with double VC recommendation.",
            "And there is an example here that if you click on this particular tweet that I have made, you will just play this video between seconds.",
            "Here 22 to 32.",
            "So Twitter is skating the link so I have put it in bigger here.",
            "But you see this is normal link with hash sign T equal and then two value, separated.",
            "And this is the definitions of the video sequence.",
            "I want to read.",
            "So you can see here I'm using the hash sign for the limiting their, imitating the FRAGMENTA specification, and it's used to highlights.",
            "In this case the temporal sequence, but it could also be used to highlight a special region within a frame."
        ],
        [
            "So let me just bring a bit more historical perspective on how that happened.",
            "Back in 2007, we come back in history.",
            "The words just the fact that video at that time was really not really a first class citizen on the web to play video back in 2007.",
            "You needed to have a plugin in your browser.",
            "It could be a flash silver plug-in or Silverlight, QuickTime, whatever.",
            "But you need a nut source.",
            "Outside program within your web browser to be able to decode and play a video.",
            "The browser couldn't play natively a video and it was very hard to find videos on the web.",
            "Yeah, you add YouTube already, but it was.",
            "I mean, the indexing was not that good and so on so.",
            "Based on this fact and the fact that we anticipate video playing a major role on the web, double officials organized two workshop in Brussels and in the San Jose in California in order to bring all interesting partners to build the stone up that would make video more used on the web.",
            "And as you can see, there's been a lot of papers submitted coming from many organizations.",
            "You can read this in the web archive."
        ],
        [
            "But I'm just summarizing what our domain topics discussed in these two workshop addressing we wanted to have global identifiers for identifying special and temporal clips for for exactly what I'm just showing deep linking, bookmarking, caching and indexing.",
            "Smaller subset of video content.",
            "We wanted to have good metadata standards in order to be able to search and discover new content.",
            "There was a lot of papers about video codec.",
            "Codec is quite a nightmare.",
            "There is a lot of patterns around and there is no.",
            "Open video correct for the World Wide Web.",
            "So many people were arguing we need one so this one has been leftover.",
            "WCC will not summarize any codec, but it was an issue, pointed out.",
            "And finally there was the problem of content protection.",
            "Managing digital rights associated with the media is key and we did.",
            "The roof is issued work into metadata for digital rights.",
            "It has been slow for doing this, but it's now coming up again more and more and actually just after this lecture, Roberto will exactly talk about this.",
            "Topic, so in the chair I will talk about addressing in meta data with burner and later on you will have content protection covered with Roberto.",
            "So Media Mixer is going through exactly the research agenda of this workshop at that time."
        ],
        [
            "So this workshop has led to the creation of an activity within GCP called video on the web for which the goal was to make the video of 1st Class Citizen the same way that image was a first class citizen on the web, and you can really see that with HTML5 where you have the video and audio elements, but enables the browser to natively decode audio and video files."
        ],
        [
            "So.",
            "Be able to being able to reference parts of media content was not something that was already something existing at that time.",
            "So here I'm just giving you again a very old example from 2007 where when you used to for the ones who have a Flickr account.",
            "Upload your photos there.",
            "You could already mark up some region and give it some tags to this region.",
            "So in Flickr terms it's called a flicker nodes.",
            "So here you have this picture.",
            "Applauded by my colleague, a macro sunglass who just got his third baby and decided to draw a bounding box around this baby to give it this baby already at the blog when he was born."
        ],
        [
            "This is for special region but temporal region.",
            "Temporal sequence already existed, so this is the official announcement from YouTube that it back from September 2008 where You Tube announced which support tempered seek point.",
            "What does it mean?",
            "It means that at the end of any YouTube video you can just use this syntax.",
            "Ash sequel an A number M for minutes and number, S for seconds and what it will mean.",
            "It will mean that the video will directly sick to this time.",
            "Points and start playing from this time point so you can do this, but with a specific syntax and YouTube was not the only one.",
            "The other video sharing platforms add a similar functionality, but they all use a different syntax, so that makes interprete very odd."
        ],
        [
            "So the whole point of the media fragments working Group which was created afterwards.",
            "Westest analyze all those syntaxes and the use cases we were looking after were bookmark sharing part or fragments of audio and video content.",
            "Being able to annotate part of media content, being able to search for part of media Contents, and if of course developed.",
            "Mashup collage of all those parts of media content and one of the strong use case we had behind is to conserve bandwidth.",
            "So if you're interested in 230 seconds.",
            "Video of out of the two hours video you don't want to download.",
            "Of course the two hours video, but just the sequence you were interested in.",
            "So how we enable?"
        ],
        [
            "This well this is exactly the major fragments you write specification.",
            "So basically in media fragments you right you have 4 dimensions, so ways of splitting up your media content.",
            "If you want you have of course that Emperor and the special sequence will come to this in a minute and you have two over dimension one which is called track here and here you really have to think in terms of media content that expose multiple tracks, for example.",
            "In English, Audio and a French audio and you just want one of the two audio so you can select it.",
            "Of course you need to know that the media file exposing number of tracks, but there is ways of doing this and the 4th and the final dimension is the so called name media fragments.",
            "And here I would just make an analogy with DVD where you have chapters which have a name.",
            "So again if your media files expose those chapters you can just reference those chapters here.",
            "So net media fragments is just a shortcut for the temporal media fragment.",
            "To which you give a name for this temporal sequence."
        ],
        [
            "So this is exactly what I'm saying here.",
            "Again, time for fragments is just a clipping along the time dimension.",
            "So you have a start and end for special.",
            "It's limited to rectangular regions, not arbitrary shape, but just rectangle bounding box and tracking name fragment.",
            "As I just said."
        ],
        [
            "So this is what it will look like if you play a media fragments in the player, you will see that this is the video player with the controls.",
            "Here you have the entire timeline.",
            "You know that this video is 38 seconds long, but in grill you have highlighted the fragmente.",
            "I have selected and Gray.",
            "This is the progress bar within this fragmente.",
            "What it means is that when the Gray will reach the end of the green it will stop play OK and if you loop it will start playing again at the beginning of the fragmente.",
            "Not at the beginning of the video."
        ],
        [
            "So this is for the time for media fragments, for the special one.",
            "Here the specification does not tell you how you should render in the in the in the browser, especially defragment it.",
            "Just give you ideas of how you can do that.",
            "So one of course ideas to crap to do.",
            "Sprite like CSS will do on the frame.",
            "Another way is to just put a similar pake overlay on the rest of the image, except for the bounding box.",
            "Selected that you will highlighted OK so you can play additional right to play with a number of.",
            "This is typical media fragmente player where you can input a video and start playing it with those dimensions selected."
        ],
        [
            "So how do you write those dimensions?",
            "This is the syntax that we use.",
            "The syntax basically is for the temporal dimensions, T equal to integer, separated with, which represents the start time and end time of the fragmente.",
            "So here I'm making distinctions between the query parameter and the fragments parameter of a UI.",
            "So this is just the normal UI syntax which starts with a protocol, for example HTTP which GPS: slash slash.",
            "Then a domain name.",
            "So in this case example.org and then after the video resource you append either?",
            "Or hash sign.",
            "So this may be a segmental right syntax is valid for both the query and the hash, and it can be combined as it is done, but it has a different behavior depending if you use it with a query or Weaver hashtag.",
            "So here I'm going a bit more technical.",
            "It depends how much you're familiar already with the euro.",
            "Instructions."
        ],
        [
            "But when we meet your eye in a web server, there is this notion that we provide the web server actually is serving you, or representation of the resource identified by this your eye.",
            "And there is a notion of primary resource and secondary resource.",
            "So here if you use the ashline, we really talk about secondary resource.",
            "That means that the server is serving you bite range exact portion.",
            "Off the primary resource that was identified before the hash sign in the UI, which means that to be able to do that, you need to extract.",
            "The exact bike ranges from this representation.",
            "The.",
            "The advantage is potentially cacheable.",
            "On the other hand, when you use a question mark what the web server is doing is creating a new resource, a new representation of this resource, and this new representation has nothing to do.",
            "In theory, we have the resources stems from, so if you do that here, you will create a new video completely new video which has four left X 10 seconds, which happens to be the excerpt.",
            "Of a longer video that was specified without the question mark, so of course that is not cashable.",
            "So that is a different use case.",
            "And then we did not talk anymore about this.",
            "But now when we talk about media fragments, I would just talk about the ash version."
        ],
        [
            "Of the media fragment.",
            "So let me just skip this part to not add confusion, but for the UI fragment part.",
            "So normally when you try to do a get action on the UI which contains the hash, the normal behavior is to strip out everything after the hash and just send to the server and the UI you want before the hash sign OK.",
            "So that will slightly evolved with the media fragmentary specification in the sense that now you will have smart user agents which will.",
            "Indeed, strip out the fragment definition, but encoded in terms of a custom HTTP headers using the range header into the request.",
            "Which means that media servers that will understand this range requests will strip out the bits of the video that corresponds to the sequence you have requested and just serve this sequence.",
            "Now, if you don't have a smart user agent but the normal user agent but does not understand the syntax, it would just query and request the entire video so the web will not be broken, and on the other side, if you don't have a smart media server, so no more server that does not understand this range request.",
            "Normally the policy on the web is in your everything you don't understand, so it will just ignore this and serve you the entire video.",
            "So again, we're not breaking the web.",
            "OK, So what I'm saying here is backward compatibility with existing web infrastructure."
        ],
        [
            "So if I put this into an image, you still have two scenarios.",
            "The first scenario where basically it's what we call two wins N Shake.",
            "So in this scenario the client wants to have this particular movie between seconds 12 and 21, so he thought he saw this UI request with ash T equal 12, 21.",
            "It will interpret this as a media fragment to write an issuer and requests with this particular range editor.",
            ": sequence equal 12, 21.",
            "The Smart Media server will say OK, I understand this.",
            "It's a read request express in seconds.",
            "I'm trying to strip out this media file.",
            "Well now you have to position me to the closest I frame of this video.",
            "It turns out that it corresponds to 1185.",
            "Two 21.16 units approximation and I'm serving you back.",
            "This bits of content.",
            "We've not two OK answer, but the two or six.",
            "Status code which means partial content.",
            "I'm sending you a partial content of this complete resource.",
            "The four way in check is almost the same.",
            "The same request is issued from the user agent, but the server says OK.",
            "I can serve you this.",
            "Bits of videos.",
            "Actually the closest interval I found is this and what is expressed in seconds.",
            "I can tell you that corresponds to this byte ranges OK, so here it converts the sequence in terms of byte ranges and here it does not send any data in the body is just.",
            "Ahead answer, so it's transparent for the user the user agent received this response and say OK, Now I can issue another request, but instead of expressing it in terms of seconds, I'm expressing in terms of bytes because now I have the mapping between seconds and bites an.",
            "What's the advantage of doing this?",
            "Is that any web server in the world already answered by train request, so the rent request expressed in seconds.",
            "It's complicated.",
            "Not a lot of web servers can do this.",
            "But a request express in terms of byte ranges.",
            "Every web service can do this.",
            "So if you do this then any anything in the in the middle between the server and the user agent, so proxy or cash, whatever will be able to cash this resource an service later on."
        ],
        [
            "OK, so of course in order to be able to do this you need to have media fragments that you can easily split an extract things from from it.",
            "OK, so you can do that with any video format you can do that only with video format, But our budget is seekable, so think about MPEG four web M org.",
            "Those video formats you can extract pieces and be able to play them.",
            "A video format such as Avi, which is not streamable.",
            "You will not be able to do this.",
            "So the requirement is yeah presence of intra coded frames, random access points for video.",
            "For special track is so complicated, we will just keep this for now."
        ],
        [
            "So this is basically what you can do with the media fragmentary UI specifications.",
            "Now you should wonder what tools can I use today, but understand this stuff well from the web browser perspective, the major web browser already natively supports media fragments.",
            "OK, this is the case for Fire Fox since quite a lot of time for Safari for Chrome at least, since now nearly two years.",
            "In addition to this, you have a number of JavaScript library which we call polyfill that have been developed for both the temporal and spatial dimension and what those JavaScript library do is that if you embed them into your website, then whatever browser but will read them because they include this JavaScript, they can simulate emulates the behavior of media fragment your eyes so.",
            "Even Internet Explorer with JavaScript library you could have the behavior of media fragments.",
            "And finally, you have a lot of custom video players that support media fragment.",
            "I just cite a few of them laying the term sign out so Jesse is developing one for the video lectures mashup that you will see tomorrow at vinnies.",
            "Never one extractor.",
            "You have a growing number of clients that supports this."
        ],
        [
            "So that was for the clients.",
            "For the servers is a bit more poor in terms of offerings.",
            "You still have one very good which is called in sooner.",
            "It's developed by I Mines University of Ghent in Belgium, so it's an academic one, but it works pretty nicely and even develop a proxy on top of this.",
            "And there is another one which is being developed at the moment.",
            "It's a partnership between the University of Southampton and Neurocom.",
            "It would be not just based.",
            "Implementations of Media Fragment Server and just to mention some big video sharing platforms, both YouTube and Dailymotion has partial support or bug reports in their system so that they are being soon up fully fully compliant with the specifications at the moment.",
            "While I'm I'm saying it's partial support is because the syntax is slightly different or they don't have all the functionalities yet."
        ],
        [
            "OK, so that covers the point of media fragments.",
            "You're right addressing.",
            "Do you have any questions at this point?",
            "No, it was clear.",
            "OK, so the second part of this talk I want to talk about annotations.",
            "So like Vasilios shown early on issues, a lot of tools that enables to segment 8 video content and then the results could be media fragment Orion's you've just seen.",
            "And then it shows you a number of.",
            "Algorithms and tools for automatically and understanding what the content is about provide annotations in terms of concepts in terms of instance in terms of objects that you would like to re detect, and so on and so on.",
            "So this is what I wanted to talk here about annotations, and I will emphasize the point of getting semantic annotations, not only strings that describe the multimedia content, but if possible, objects.",
            "Identify with semantic web technologies for which we have more background knowledge about, because this is representing in terms in within knowledge bases such as Wikipedia."
        ],
        [
            "So let me just show you an example of what I meant with what I called semantic annotations.",
            "So this is a picture in this case coming from Wikipedia.",
            "This is the famous big free at the Yalta Conference.",
            "So you can recognize Winston Churchills Roosevelts in standing, sitting down in 45 in order to split the world because they won the war.",
            "So if I would like to annotate this picture, I could I use one of those nice tools that Bacillus has shown which can extract for example faces or people within the image.",
            "So in this case Pro bounding box around this character.",
            "And this is what we call the media fragment creation phase.",
            "So Localiza region in the picture.",
            "And if I would like to annotate it, probably this is an interpretation.",
            "It's my interpretation of this picture.",
            "I would like to use terms such as Winston Churchill, UK Prime Minister's Force, World War Two, and every terms like this but remind me about the context of this picture.",
            "Now if I don't want to just annotate this picture but provides semantic annotations, I will do almost the same thing.",
            "I will say in here I'm using a particulare.",
            "Syntax, by the way, which is familiar with semantic web technologies in the room.",
            "Can you just raise your hands?",
            "Semantic web technologies?",
            "How are you?",
            "Is your gym of the morning?",
            "Yeah yeah 123.",
            "45 not many OK so here I'm using a porch Kra syntax tried to abstract from this which sort of triple representations so there is subject predicate and object and what does it mean?",
            "It means that this region here.",
            "Depicts Winston Churchill.",
            "So looks like exactly what I just said with my normal mentation, except that Winston Churchill now is not anymore a string.",
            "It's UI which comes from the pedia or from Wikipedia.",
            "And because of this, and because Wikipedia is nice to give us much more information, we know from Wikipedia that this page this Wikipedia page indicates in the infobox of this page that.",
            "This Churchill guy ask for name Winston Churchill is a person he used to be a Prime Minister of the UK and so on and so on.",
            "Give many more information.",
            "Of course everything written info box so all those information we get it for free because we use your eye from this knowledge base."
        ],
        [
            "I could have done the same thing with the video.",
            "This is another video coming from Richards, the news agency, and this video is entitled History of Jade Violence.",
            "This is just based on the fact that every time you have a submit, you submit a worldwide submitter J or G20 summit.",
            "Often you have a counter protest.",
            "We've sometimes a lot of people who violently protest and Richard wanted to make a video showing up all the.",
            "Problems that you had in the past summit.",
            "So basically this video consists of a lot of clips that each represents violence that happened in one of those summits.",
            "I could do the same thing, creates media fragments.",
            "So is relate some sequence within this long video.",
            "So for example this one and this one and I could start tagging those summits.",
            "So if I'm watching this video will understand that the first one is about the J submits which used to take place in elegant dam in Germany in 2007, while the 4th sequence is about the EU summit, which took place in front one in Gothenburg in Sweden and so on and so on.",
            "But again, I'm telling you I don't want to annotate.",
            "I want to semantically annotate.",
            "So instead of using those strings.",
            "I would say the sequence depicts the 38 to 43rd G8 summit as identified in Wikipedia, and by doing this I know thanks to Wikipedia that the J Summit took place in those Giorgio coordinates, which happens to be the coordinates of elegant damn the city in Germany, and so on and so on.",
            "So immediately I get much more information for free because I'm using information knowledge we have already from Wikipedia or from other sources."
        ],
        [
            "So just my takeaway summary of this semantic condition aspects is that.",
            "Rather than using strings for annotating using, things is much more powerful.",
            "This is what you see.",
            "We've the Google Knowledge Graph panel.",
            "When you're querying Google and so on and so on, we have now massive amounts of knowledge basis, which is the linked open data cloud represented by this picture, which provides a lot of statements of facts about real world objects about reward entities that we can use, force providing automatic annotation.",
            "We have a growing numbers of well curated vocabularies for doing this annotation.",
            "This is called love, linked open vocabularies and we are sort of advocating to follow those four link data principles or even which has been refined by Linda Nixon as the four linked media principle.",
            "So you expose your media descriptions and you basically make everything possible to foster their reuse."
        ],
        [
            "So now.",
            "I start two of all the pieces of the puzzle I show you media fragments your eye as a way of addressing, providing deep linking into media assets.",
            "I show you the link data principles for providing annotations and knowledge bases that exists where you can take your eye of those entities in order to populate your annotations.",
            "What is missing is the glue between them is what's the annotation model that one could use.",
            "So here you have quite a lot of annotation model that exists.",
            "Out there.",
            "But one of them which is growing in terms of support and in terms of adoptions, is the so called open annotation model.",
            "This model is pretty simple.",
            "It starts from the fact that indentation is between the body and the target and what that mean.",
            "It means simply that this body is related in some way to this target and we will be able to specify those things and we will be able to characterize.",
            "What is this related to relationship between this body and this target?",
            "So.",
            "It has a concrete encoding into RDF.",
            "One of these semantic web technologies and you can characterize this really two.",
            "We have a so called motivation, which is the motivation why you would like to annotate this.",
            "Contact so let me show you a concrete example.",
            "Here I just want to add a date using this model.",
            "The simple picture.",
            "So the simple picture which is the Eiffel Tower.",
            "I would like to annotate it with this FL Tower identifier from DB pedia.",
            "Again, the semantic Web version of Wikipedia."
        ],
        [
            "So this is what it will look like.",
            "Bit complicated, but actually not not that much.",
            "As you can see there is this annotation in the center in the middle where you have a body in the target.",
            "The target is the picture.",
            "OK, so it could be a part of the picture, but in this case it's just different out and the body is what you want to say about this picture.",
            "In this case I want to say this is the Eiffel Tower.",
            "Now all the other things around is to indicate provenance of this annotations.",
            "So you can say OK, this is an attention you can specify why.",
            "Why do you want to annotate this picture with this identifier?",
            "Well, in this case I just want to tag this picture, but it could be another motivation so you have a long list of motivation.",
            "Which pre exists in the model.",
            "You can say when did you made this annotations using which tools was it automatic?",
            "Using one of those tools adversity is shown or manual who has made this annotation?",
            "Can I have more informations about this person extra?",
            "So all those information is just provenance around the annotation.",
            "Where the annotation is here."
        ],
        [
            "So now you have the free puzzles I wanted to talk about media fragment, your eye open annotation as a core model and linked open data cloud as form of your eye that you can use for identifying real world entities useful for in your notations.",
            "And you have a number of project that exists, but make use of all of them.",
            "One of them is Map Hub which has for ambition to annotate old Maps from very old Maps.",
            "Of countries of cities and so on.",
            "So here I mean they really have to do this.",
            "They have big Maps, they want to localize part of the Maps and the use media fragmentary for doing this.",
            "They need to say something about those regions and they use the open innovation model for being able to say something about those parts of those Maps."
        ],
        [
            "Another one is the open video annotation project.",
            "We're here again.",
            "It's really it's infancy yet, and there's not much functionality but the idea again is to provide a simple way of providing semantic annotations of parts of videos, and again, here the technologies used.",
            "Army defragmenter.",
            "I open invitation Ontology and so on."
        ],
        [
            "Annotations of video again that exists since quite a lot of time in YouTube.",
            "You have You Tube annotations.",
            "I'm not sure many of you have already used this, but basically YouTube world annotations are clickable text overlays on YouTube videos and the primary goal for YouTube when they introduced this was for getting advertisements.",
            "So you can put deep linking into YouTube video and plug some advertisements that people can click on them.",
            "Ann, get ready to buy things and things like this.",
            "So annotations were really used to boost engagements, give more informations and adding navigation is too."
        ],
        [
            "So this is the type of annotations that you have.",
            "What we call note, think about the Flicker notes.",
            "This is the same idea speech bubble, so it's just what the person is saying.",
            "But put into a bubble.",
            "Transcripts.",
            "And you have a fourth type of annotation, so this is really, really mentari basic annotations."
        ],
        [
            "What we have shown you.",
            "Since the beginning of today was another way of getting automatic annotation.",
            "Is using tools an algorithm that will analyze the content and try by themselves to identify specific type of objects?",
            "So fast videos just before show examples where we can detect concepts or instances out of a video program?",
            "So in this case here, this antique roadshow program or this?",
            "And you show program coming from the RB broadcaster.",
            "It could be also an object detection algorithm that just vasilios shown where the user would draw a bounding by bounding box.",
            "I don't know why it does, it should be something around this painting and by doing so you will actually detect all occurrence of this paintings later on in the video.",
            "So this is how we could get automatic annotations."
        ],
        [
            "And again here the idea is to slowly move from using strings that.",
            "Represents you tags around those media content to really things, but are identified knowledge base because by doing so you get for free more background knowledge about those things.",
            "So in this case if instead of just writing the name of the paint around this paintings I use a UI coming from Wikipedia of this of this painter then immediately I get a lot of more information about this painter.",
            "I know the style that is used to perform.",
            "For his paintings so Cubism, Expressionism, Fauvism's, and based on those style concepts, I can pin points to link to over related content.",
            "Of the same style.",
            "So if you're reading this, accept a video about this paintings then we can suggest you to look at over paintings which we believe are related to becausw.",
            "They share in this.",
            "In this particular example the same style."
        ],
        [
            "So this is what Link TV and media mixers are promoting.",
            "I think you have heard about those use cases earlier, so just to give you a concrete examples, imagine that we have this video here.",
            "We told you on the first steps is always creating subset of this video.",
            "Fragments of this video.",
            "So here for example a particular chapter or sequence of this video.",
            "Identify you see with the media fragment you write an on which you would like to say something about it.",
            "So in this case I want to say that in this particular sequence of this old movie The two main characters are Bogart and Bergman, and they both exists in Wikipedia, so it's great.",
            "I can use this Wikipedia entry for dictating this sequence."
        ],
        [
            "So.",
            "What I really want to do later on is coming from starting from this particular video sequence start.",
            "Proposing related content that could be associated to this particular video sequence.",
            "And for that we rely on chair annotation, so I know that this particular sequence talk about Bogart and Bergman, so he stopped watching these seconds.",
            "I would like to propose over parts of media contents, but either features those same characters, people, or features over related content to be decided by an algorithm to be proposed to be watched.",
            "So here what we see is that.",
            "Name entity play a key role in this hyperlinking world.",
            "So in this case here because I have detected this person I can of course find images of this person over videos where he has played with and among all the videos.",
            "Yes play with I can start proposing what I believe are related sequence because they share a number of attributes and annotations with the original sequence so perhaps I would like to propose this video sequence coming from this longer movie.",
            "As related content to this particular sequence, based on a number of criteria."
        ],
        [
            "So here I'm saying that name entity, player key role.",
            "So it's important to be able to detect named entities.",
            "In videos, so how you do this?",
            "You can rely on the visual signal.",
            "You can implement a face detection algorithm and then once you have detected the face you could try to recognize.",
            "Which person this face belongs to.",
            "So for example, this shaft rain, a machine learning classifier to recognize Barack Obama in videos, and assuming that you have been successful in doing so, then every videos that will.",
            "I like Barack Obama.",
            "You will be able to recognize this person and that could be a good thing for the particle applications you're developing.",
            "Another way is to not rely rely on the visual signal, but to rely on the transcript of the video.",
            "OK, so in this case here we will analyze the textual transcript of the video and then we'll just try to spot this name entity within the transcript.",
            "And of course those methods are not disjoint, and ideally you would like to combine both.",
            "You would like to combine the visual analysis with what you can do on the text and so on and so on.",
            "So this multimodal approach."
        ],
        [
            "So here, let me focus on textual analyzes for detecting name entities within transcript.",
            "Traditionally you could find some standalone software that you can install on your machine and play with to detect an entity.",
            "So I mentioned some of the famous platforms gates because it's an open source academic platforms that anybody can install and even extend because you can develop your own gazetteer to recognize more name entities.",
            "Stanford core NLP, which is sort of the baseline for this community.",
            "Again in open source.",
            "Target and Tim is just because I wanted to mention one commercial company that does this.",
            "Intimes pretends to be the best for the European languages to do that.",
            "But what we have recently seen, let's say over the last four years, is a very large number of Web API, but does exactly what those standalone software do, but on the way what does it mean?",
            "It means that generally they enable you to register to get a developer key, and then you, as a developer can just interact with those black boxes services and send me the text and get results out of it.",
            "You don't understand how those tools work.",
            "Actually, you don't really have to understand how do they work, they're just giving you results.",
            "And here I mentioned a lot of tools, but there are many more.",
            "Of course those tools are different assumptions.",
            "They don't have the same language to cover the account, detect the same type of name entities they have, more or less limitations in terms of the volume of text that can analyze per day, and so on and so on.",
            "So they are not or equally compatible, but the the basic functionalities of extracting name entities and disambiguating images.",
            "Is present for all those tools.",
            "What we have started to work on since 2 years.",
            "Enough now is.",
            "A tool which is called nerd that you can all use that make the sum of all of them.",
            "So no extends from name.",
            "Entity recognition is immigration this is.",
            "Free things this is an API that anyone can use just by reaching to nerd, but this is also a front end user interface so you don't even need to register.",
            "You can just go to Nerd website, submit a text and analyze it.",
            "So what nerd is doing is taking the results of.",
            "Stanford core NLP training algorithm combine?",
            "We've all those tools together and of course the magic is out to learn how to combine optimally all those tools to have the best results."
        ],
        [
            "So this is what we do in there than we do that for two reasons why one because we would like to compare the strength and weakness of all those API and in order to be able to do so we need to put them into the same framework to ease the comparison and seconds to of course improve the recognitions and immigration.",
            "Because if we take the best or the tools or naive assumption which tends to be verified is that we will be better than one of those tools."
        ],
        [
            "So this is what the Nerd user interface.",
            "You can input any UI of a web document or plain text document.",
            "If you use the API, you can even submit a subtitle.",
            "So what we call a time text and you will get the lists.",
            "So then you will be able to select a particular structure or all of them and you get the list of results."
        ],
        [
            "Now, if I'm combining everything, media fragments open innovation and there this is what we obtain.",
            "A very big graph, which is our semantic annotations of the content of the video president is extracted from the subtitle of this video, so I'm not assuming you will read all those details or let me make you a guided tour of this graph here.",
            "This is just a UI of the media resource.",
            "OK, this is an abstract identifier of this media.",
            "Source that exists somewhere this media resource could be a particular play of Shakespeare.",
            "Then of course you have a concrete media file at the end we will particulare encoding of this video, so this will be the locator.",
            "So this abstract media resource, you can find it in the particular resolutions in the particular video correct formats.",
            "In this case MPEG four at this year, right?",
            "Now we can take and accept.",
            "With the fragments out of this media resource, this is this.",
            "Is this media fragments.",
            "So here you will use the same beginning of the UI, but Ash sequel and in particular time sequence.",
            "This major segments you can annotate it so the target of this annotation will be the media fragments and the body.",
            "Remember Target body will be this entity.",
            "This entity in this case as type, its location and it has a value.",
            "In this case this is the handbag.",
            "Resource in Germany, Berlin and believe which is not identified in Wikipedia.",
            "So what we mean here is that this year I annotate this particular fragments, which is part of a larger video which is, As for locator, the popular media file.",
            "And here you can even point back to the particulare time in the video when so sorry the in the subtitle when this entity has been spot and you have additional provenance informations about how you discover this name entity."
        ],
        [
            "OK, now we put all those things into concrete applications.",
            "We call media fragments and richer that you can have a try here.",
            "This year I what you can do here is enter your eye of a video or still on the video sharing platform.",
            "It's supposed to work with both their emotions on YouTube in this case and when we click on preview you will first get a lot of metadata that those video sharing platform is giving to you.",
            "So in this case.",
            "I put this YouTube video which is in title or simple ideas lead to scientific discovery by Adam Savage.",
            "It's a Ted talk and I will immediately get a number of things that YouTube is telling me, such as descriptions.",
            "The category of this video, it's education.",
            "But it has been applauded.",
            "A number of statistics, such as the number of views, favorite comments, blah blah blah, no tax for this video, its duration.",
            "So this is what you can get.",
            "What we can Additionally get is the subtitle of this video.",
            "So in this application we put you have to Scroll down a big button which is called notify, but this means we'll just launch nerd on the subtitles, and if you do that."
        ],
        [
            "This immediately you will get.",
            "You will go to another page where you have your original video.",
            "We've all the name entities we have extracted from the subtitle and what this means that doesn't limit is also our disambiguating.",
            "So in this particular video they talk about the Challenger disaster.",
            "If you don't remember what the challenges disaster was.",
            "It has been highlighted to just moves over this string and you get the info box of Wikipedia reminding you what the Challenger disaster."
        ],
        [
            "It is.",
            "Later on in this video they talk about fame, angiograms math.",
            "My math was a bit rusty.",
            "I couldn't remember what the fame and diagrams was, but Luckily it has been the name entity recognized by the software.",
            "I can just over it and I will have the Wikipedia infobox reminding me what defend map diagrams here.",
            "And if I'm clicking on it will have the full Wikipedia page and so on.",
            "So you see this is just a simple applications that enrich fragments of video with named entities we can extract from the subtitles."
        ],
        [
            "So that that this idea of linking fragments of videos to over content Wikipedia page, but potentially over multimedia content, is the root of what we call the linked media layer, is the idea of you linked your video sequence to over video sequence?",
            "So.",
            "Here we work with another.",
            "Collections and benchmark which is called medieval and to which Benoit will talk more about in 5 minutes.",
            "So here I just wanted to basically make the years old with what you will see next, But the idea here is that you have a closed collections big closed collections of of video material and you would like to use those techniques.",
            "I've just shown in order to re late one passage of a video to another pasage and do that automatically.",
            "For doing this we.",
            "So in this case and then we go deeper into this, you want to analyze this entire closed collections to analyze content to content."
        ],
        [
            "And another way of doing this is to link part of your video to any over multimedia content that you can find, for example on social networks.",
            "So here we have developed another tool called Media Collector.",
            "Which query alot of social networks that exists.",
            "So Twitter, Flickr, Facebook, Google Plus, YouTube, Instagram extracts, raw?",
            "Again, we unify all their response into a single format.",
            "So we implement.",
            "This is not the server, but you can also use this because it has a public IP."
        ],
        [
            "And this is what you get.",
            "So if you submit a particular query in this case my query was.",
            "Sandy, the storm that it's US year ago I get a number of media.",
            "Which are related to shared on different social networks which are related to this query.",
            "So in this case I got this particular image published on Twitter and I get the permitting of the tweets of course, but the text into the tweet content press the media items, and potentially some social interactions and number of people of time.",
            "This tweet has been retweeted commented.",
            "Like favorite extractor."
        ],
        [
            "So this is this is another example where here I'm submitting a query about Angela Merkel and I get more related content about her."
        ],
        [
            "Uh.",
            "And basically this is the idea that from you video sequence you would be able to link to much more multimedia content from those social networks."
        ],
        [
            "And it would probably almost conclude on this is Richmond's 'cause I have talked about annotations, even talking about enrichments, the idea that you link your video, we've over media content, annotations 2, so we model the enrichments as an annotation, but the only thing I would change is this.",
            "The motivation of the annotations.",
            "Remember, before it was tagging, now the motivation is linking.",
            "As simple as this, and again you you must represent this explicitly as an annotation, but you just change the motivation of annotation."
        ],
        [
            "I encourage you to watch this project still coming from Link TV where this is exactly all those ideas I have just presented into practice here.",
            "You will be able to basically watch a news program and automatically gets proposed content.",
            "Linked to the program, you're being much based on this name, entity detection and examination.",
            "And that's it for me now."
        ],
        [
            "Uh."
        ],
        [
            "Yeah, my takeaway summary.",
            "Before putting their hand to burn up video is a first class citizen on the web.",
            "I hope I convinced you that now this is the case.",
            "We have models, ontologies and API for describing those resource and I pinpoint the open innovation model for that.",
            "We have ways of addressing fine grain parts of multimedia content using media fragment to write the.",
            "Net platform is a great tool that you can all use to extract named entities from the transcripts.",
            "And what we really I think we would like to push you to do, because you're just starting doing research in this field is really embracing the link media vision.",
            "What does it mean?",
            "It means that you all ran aggregates to analyze your media content while not thinking of exposing the results of those analysis to the larger community.",
            "Perhaps you don't see now the need or the reason why you should do this.",
            "This is the network effects someone will take those results.",
            "Would be able to compare this with his own results, or will perhaps do something clever that you couldn't imagine doing with those analysis results.",
            "So just do this as soon as you.",
            "Analyze multimedia content.",
            "Try to use standard technologies to represent the output and to expose it an let the network effect play and someone will reuse it and do something nice with it."
        ],
        [
            "A lot of people to credit for those slides, but I'm heading over to Benoit now.",
            "Will more deeply talk about medieval."
        ],
        [
            "OK, good afternoon.",
            "So I'm going to continue.",
            "Today's winter school going deeper into what we can do with many of the things that we've been talking about since the morning.",
            "And I'm going to put that into a specific context.",
            "The context of a benchmark which took place easier in media eval, which is called search and hyperlinking.",
            "How many of you have heard of Mediaval and maybe the search and hyperlinking task?",
            "OK, quite a few hands.",
            "OK, very good.",
            "So this work is work that we're doing both in media mixer and in the link TV consortium.",
            "Most of the work here, well, a lot of the work, sorry here has been done by one of my PhD student material.",
            "So gay, but a lot of other partners from Link TV I've been working on this not to name them serve has been working on it.",
            "The University of Prag XYZ Economic University of Prague has been working on this as well as front over, so it's a multipartite effort from Link TV and Media Mixer Partners."
        ],
        [
            "So to put it into context with what we want to.",
            "Talk about today about.",
            "What media mixer is about?",
            "We've seen a lot of things about Fragmente creation Fragmente description in the previous talks.",
            "What I'm going to address, right?"
        ],
        [
            "Now is only a small part.",
            "It's a search bot.",
            "How are we going to search for information once it's been fragmented?",
            "Once it's been described, OK."
        ],
        [
            "And really, we thought the medieval search and hyperlinking task or challenge was really a good way to demonstrate the capabilities of what we were working on.",
            "So I'm going to start with presenting this particular task and then describe how we went about basically performing the tasks, showing some example, then some results, and then I'll conclude."
        ],
        [
            "Rafael has already started to introduce a task where what really, what we have to do is to search for video into a big data set.",
            "So the data set has been provided by the BBC and there are in fact two tasks that we are addressing.",
            "One of the task is a search task where you are given a dis textual description.",
            "Of what you are looking for and then you have to search for.",
            "Media fragments that correspond to this description in addition to a textual description.",
            "There is a separate description.",
            "Again, textual which describe the visual content of what we are looking for, so it gives you extra information about what you are supposed to find, what you're searching for.",
            "So this is one part of the challenge.",
            "The other part of the challenge is hyperlinking.",
            "Hyperlinking is a slightly different setting where instead of starting from a sexual query we are actually starting from a media fragment, and from this we are fragmente.",
            "We're trying to find other media fragments, so we basically reverting back into.",
            "What's Raphel was just mentioning all these enrichment.",
            "How can you enrich a particular media with more media that are refering to similar things?",
            "So those are the two tasks set by."
        ],
        [
            "Medieval and this tasks have to be done in a very large data set.",
            "We have 2323 videos.",
            "They basically are broadcast from different draws from the BBC.",
            "All in all, those all those videos are basically average out to 1.700 hours of video.",
            "Media eval is also providing extra meter data or extra information about those video they are providing is of course the video and the audio information they are providing is two types of Asrs actually provided by other people, but they are given as part of the data you can play with.",
            "They are providing us.",
            "Manual subtitles I've been provided by the BBC meter data about the program, which channel was it broadcasted in?",
            "Who are the people in this program?",
            "And so on is provided.",
            "They also provide shout, shout boundaries, so if you want to provide you already some sort of media fragment at a certain granularity and the corresponding key frames for all of these fragments, they also provided very late in the challenge.",
            "Information about faces, detection and similarity information within the videos.",
            "And concept detection also very late in the challenge."
        ],
        [
            "So this is typically the two types of query we're dealing with.",
            "The first one at the top being the search and the one at the bottom being the hyperlinking.",
            "So what we can see is that indeed there are two parts of the query.",
            "You have a query text.",
            "What does a bowl look like when it's the world during squash?",
            "OK, this is a typical query.",
            "This is one of the queries that we had to find videos for.",
            "And it is.",
            "It has an additional information.",
            "The visual cue the video Q says bowl eating a role in slow motion.",
            "OK, so not only you have to find a video which.",
            "Looks which were bowl is seen eating a wall.",
            "You also have to find a video which is doing that in slow motion.",
            "So this is a task.",
            "The search task VIP linking task is starting with less.",
            "I would say semantic information.",
            "At least from the level of the query, because what it gives you is just the start point and the end point within a video for which you have to find more relevant information.",
            "Which means you have to somehow understand what's happening in this video before you can go and search for information."
        ],
        [
            "So all those queries have been user generated, which means that some people have been looking at the data set and then decided, oh, I would like people to search for this particular media fragment.",
            "And here is a query that they will have to that I think would be a good query for answering this so they provide 50 queries from 29 different users for the search task and based on the search tasks actually started with.",
            "Provided.",
            "98 anchors for which we can do the hyperlink.",
            "For evaluation.",
            "The search task is provided already by the user for hyperlinking.",
            "They perform crowdsourcing from the result provided by all the participants to find out which are the good hyperlinks and which are not so good.",
            "I pairings.",
            "And due to time and cost of such crowdsourcing, they could not do it on all the 98 anchors that could only do it on 30."
        ],
        [
            "How do they?",
            "Evaluate whether one.",
            "Persons are one groups.",
            "Algorithm performs better than another one.",
            "Well, there are actually devise a number of measures to identify that.",
            "The simplest one is the mean reciprocal rank.",
            "OK, it basically looks at the rank of the good.",
            "Of the first good anchor or media fragment that is retrieved by system and it averages out this over all the 50 queries.",
            "Of course this does not provide any information about whether you have how far you are from the right segment, OK?"
        ],
        [
            "So they provided some extra measures.",
            "One of them is the mean generalized average precision.",
            "Which takes into account the starting point of the fragment that is going to be played OK.",
            "Done, it is again extended with the mean average segment precision, which measures both the ranking and the segmentation position.",
            "OK, so now we have both the ranking and the starting and end time of each of the fragments, which is taken into account for hyperlinking only precision and mean average Precision's have been used."
        ],
        [
            "So how do we address the problem?",
            "Well, originally really this is a real research problem.",
            "We don't have a real solution, So what we did is performed.",
            "All the algorithm that we had in our hands on the data set and then to see how far we could go with providing results.",
            "So we processed all.",
            "16197 hours of video and here you have the running times for all of the different things, so we use the algorithm mentioned earlier for visual concept detection, unforeseen segmentation and it run for one of them.",
            "Run for 20 days on 100 cores and so on.",
            "OK, so we had also OCR from found over so the scene segmentation and video concept where from surf then we have OCR keyword extraction, name, entity extraction from Eureka Man you EP.",
            "Face detection and tracking from year ago.",
            "So all of this information is now becoming meta data that we can use for.",
            "Indexing and searching into our collection."
        ],
        [
            "So how did we do this search and how did we index?",
            "Well, very simply, we use Lucene solar to do that.",
            "So this is Apache, so it's available for use.",
            "Easily you can just download it and use it.",
            "And what we did is we indexed or data set.",
            "So the whole of the video according to different principles according to different program Ularity because we wanted to identify the right big in an end for each of the media fragmente.",
            "But also according to different types of features.",
            "So we could compare weather.",
            "For example visual Concepts would actually bring a lot of information to the search onto the hyperlinking.",
            "And all this information was stored in a unified, structured way which was very handy and flexible for doing the search and the hyperlinking from one single index.",
            "So how did we actually?",
            "Encode the date."
        ],
        [
            "Well, in solo what you have to do is you have to define a schema which basically gives you the structure of the document that you're going to be using.",
            "So here we can see for example that we have a number of fields and for each of those fields you can define whether it has to be indexed, whether it is as it should not be indexed, whether you want to store it, whether it's multivalued, whether it is required and for all this and you can define your documents.",
            "So for each of the documents that we had in the database.",
            "Basically those documents are media fragment of different length.",
            "Then we had a description which video does it come from?",
            "When does it start?",
            "When does it hand?",
            "The subtitles attached to the transcripts attached to it and also.",
            "The V."
        ],
        [
            "Cool concepts that have been detected and with which.",
            "Accuracy."
        ],
        [
            "Then Solar did its thing.",
            "OK. Basically the good thing we saw is that it provides you different ways to index different different ways to solve the information.",
            "We tried many.",
            "I'm not going to report all of this here, but basically all this is provided for you."
        ],
        [
            "Then you can perform the search.",
            "So what we did is.",
            "Search over two different methodologies.",
            "But the big problem when doing such?",
            "Is when you try to Add all the type of data that the search engine is not used to.",
            "Like the visual data, the visual concepts for text is very.",
            "It's very traditional and there's no real problem there.",
            "So how do you include the visual feature into this?",
            "Well, the starting point is the visual cues from the text.",
            "So actually, even though you're talking about visual feature, you're starting from text information, so you have to somehow do a mapping between the textual description and the visual concepts that are being provided by the algorithms.",
            "So I'm going to."
        ],
        [
            "To show you how we did that, basically what we did is take the visual cues and.",
            "Identify keyword that we could find in in Word net.",
            "And based on those keywords and word net, find a mapping with all the visual concepts detectors that we had.",
            "OK, so we end up with this sort of mapping warehouse.",
            "Could be a church house, could be a school house, could be a building.",
            "And so on for animals and for some of the concepts we have no mapping like exploration.",
            "We do not have any visual concepts actually link to that.",
            "OK, so this is an automatic way to go from the visual cues to the visual concept as the one described earlier by Vasilis.",
            "And you can actually have some confidence information about how those mappings.",
            "Valid or not directly based from the word net."
        ],
        [
            "The other thing that we need to look at is how good or visual detectors are OK and what we did is to basically look at the each for each of the concepts.",
            "Visual visual concept detectors that we have.",
            "Sorry we looked at how well it was doing on the 1st.",
            "Highest ranked images or if you want the first highest rank.",
            "Media fragments and out of those 100, we counted how many actually really depicted the visual concept, and we use that as a score to identify how good this detector is or not OK, so these are the two info."
        ],
        [
            "Nations that we're using all this information is going to be used for indexing the whole of the data set based on two strategies, one based on media fragments which are basically the scene exactly the scenes adversaries was mentioning this morning and the other one is a more traditional approach which is basically merging consecutive shots according to how similar they are.",
            "And instead of using the.",
            "Visual information, which is what the scene algorithm presented by Vasilis is doing for the second one, what we used is similarity in the textual information within the transcript or within the subtitles.",
            "Then we submitted a large number of runs and for each of those runs we gave a specific name like.",
            "The first one seems no see or seen.",
            "C means scene search using textual and or not using visual features.",
            "Then we have scenes using only textual features from transcript, different types of transcript.",
            "And then we have the different clustering.",
            "For grouping the different shots."
        ],
        [
            "Running this over the algorithm, running the algorithm over the whole of the data set, we found out that actually when we run the whole thing at the video level, not even looking lower at the media fragment, then we could already do a lot of prefiltering, meaning that at the high level video that is talking about something in general.",
            "Actually it's more likely to contain the exact information that you're looking for, so you can do some prefiltering, which is pretty good, because that means we're going to be able to scale more easily if we.",
            "If we can do such type of pre filtering so I'm not going into the details, you can find that in papers of how good the prefiltering can be, but still there is an issue is how do you when do you cut the prefiltering?",
            "How many videos do you still have to consider?",
            "So this is still an open research issue."
        ],
        [
            "So doing the query is actually very simple.",
            "You can either do it using the web interface, those of you sitting at the back, I gather that you probably can't see that, but here at the top I'm getting.",
            "A query which basically just says what does the ball look like when it's the rule.",
            "OK, so I just type this in and then I press search and there it goes.",
            "This is quite convenient for testing."
        ],
        [
            "But when you want to do that a number of times, you probably want to use the HTTP request.",
            "Which is just the one that you can see there.",
            "OK, so here it's searching for a chicken out of pre trip exploracion of poetry by schoolchildren from writing.",
            "And it's trying to find the visual concept animal and visual concept building."
        ],
        [
            "So this is for the search for the hyperlinking.",
            "We basically what we did is we use.",
            "The search component, but not instead of retrieving only one document, we were retrieving more than one document OK."
        ],
        [
            "So here is an example of.",
            "Of what actually happened when we played with the system.",
            "So here we have a text query.",
            "So what does what to cook with?",
            "Sorry what to cook with everyday ingredients on a budget then is run out and John Barrowman inspired Seabass as progress ostritch mushroom, sweet potato, mango tomatoes and we have a number of visual cues, so denizen out and should be seen.",
            "OK, so here you can see that we have a video and it's using the media fragment attached to it because this is what was supposed to be retrieved.",
            "So this is what.",
            "Was supposed to be retrieved.",
            "Right, so this is typically the video that should be sound.",
            "OK, so we have a cooking video.",
            "This is what the person was looking for.",
            "This is what.",
            "The person wanted us to find when.",
            "Issuing the query that we have seen.",
            "OK, so going back there.",
            "I'm not going to run the other videos, but just looking at the indices, the time indices we can see that what the user wanted us to find was going from second 67 all the way to 2nd 321 using scenes we were able to find the right video.",
            "Going from second 49 to 323 and using clustering of shots.",
            "We were getting the right video, but the wrong fragment.",
            "OK, so this is just an example.",
            "We can.",
            "We can look at some more."
        ],
        [
            "Ah.",
            "Some other time, but in terms of overall, the results are showing that indeed.",
            "This seems construction is providing more meaningful.",
            "Access to the media according to what users are expected to find.",
            "So things are giving the best performance.",
            "The impact of transcript so you can see the one with South.",
            "The one with I and the one with you.",
            "They show different types of text that has been extracted either through subtitle.",
            "This is SIN, your transcript and you can see that those are providing lower scores.",
            "So indeed.",
            "Transcript which are which are computed by through ASR are not as good as.",
            "Direct subtitles.",
            "Oh sorry, one last thing, the top two are showing using the visual concept and not showing visual concept and we can see that this color slightly better when we're using."
        ],
        [
            "Visual concepts so overall, out of all the participants of mediaval this year, we perform second in terms of end gap.",
            "With a window of 50 of 60 seconds for the search task."
        ],
        [
            "Looking at the hyperlinking we could see two different approaches.",
            "One is using directly the anchor.",
            "Only the anchor the other else is using extra information.",
            "Meta data coming from the video itself, but we can see that we can get very reasonable.",
            "Results of 2.56 in terms of accuracy and precision.",
            "10"
        ],
        [
            "Compared with the others, the our approach are the first 2 bars.",
            "At Precision 10."
        ],
        [
            "So what is showing is this is showing us that when we are searching for information we are searching for information.",
            "Not a whole video.",
            "We are usually searching for very precise part of the video.",
            "Videos I've been usually cut in shots is definitely not what people are expecting when they are searching for subparts of video.",
            "We need to rearrange all this information according to the semantic content of the video.",
            "And this is what can still be improved compared to what we're doing.",
            "But we can see that clearly the shot is not what people are looking for.",
            "Scenes are better, and for that media fragment offer the flexibility.",
            "And this is a very big strength.",
            "Visual feature can provide some gain.",
            "We have to do some more work to identify how.",
            "To improve this quality, how to use those concept detectors better within such frameworks and how to do the mapping between the textual queries and the visual.",
            "So somehow it's a semantic gap the other way around."
        ],
        [
            "So if you want to find more information, there are some related publications, including talk at the end of MSYS next week.",
            "You also have links to the media eval campaign and media Mixer Link TV as well as the solar system.",
            "And with this I close this talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The title of this lecture now is semantic multimedia remixing, and the first part will be about deep linking into media assets at the fragment level.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know the ones who are familiar with content management systems in the room, so it's EMS to manage your website, and in particular so those are like WordPress or Drupal, and for the ones who are using group or within the in the middle of July last year Drupal eight was announcing that they were supporting particulare vocabulary semantic vocabulary for marking up web pages managed by this content management system.",
                    "label": 0
                },
                {
                    "sent": "So this was this announcement from Drupal 8, but actually Drupal is pretty user ready to play.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Semantic technologies.",
                    "label": 0
                },
                {
                    "sent": "And this is a much older blog post.",
                    "label": 0
                },
                {
                    "sent": "So it's dated from 2010 and here it was Drupal six I think, or perhaps seven was announced and they were already trying to support semantic vocabularies for indexing pages.",
                    "label": 0
                },
                {
                    "sent": "And if you read this very serious blog post which is published in our technical Journal, you can read this.",
                    "label": 0
                },
                {
                    "sent": "So I will read for you.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure you can read it from the back, but basically it tells you you know semantic content management, consuming and producing RDF in report is possible.",
                    "label": 0
                },
                {
                    "sent": "And if you're really interesting into this, you should click on this link of this video recording session and then you should specifically go between 9 minutes and annual segments between minutes 44 two minutes, 42.",
                    "label": 0
                },
                {
                    "sent": "So it's very precise instructions you have to click on this link.",
                    "label": 0
                },
                {
                    "sent": "Sick to the minute 44?",
                    "label": 0
                },
                {
                    "sent": "Stop watching at minutes 42 because this is those 9 minutes that are really interesting for you.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in this topic now, you could wonder there must be a better way of sharing.",
                    "label": 0
                },
                {
                    "sent": "Watch across sequence of a video on the web without relying on those lengthy instructions that you as a human have to interpret in order to be able to play this particular fragment.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what about so?",
                    "label": 0
                },
                {
                    "sent": "This is really as leading to watch ever since the beginning of the morning media fragmente.",
                    "label": 0
                },
                {
                    "sent": "So what we really would like to do is be able to publish on Facebook, on Twitter or bookmark or whatever, but direct link which contains this media fragments and when you click on it, you just play the requested sequence and not before and not after.",
                    "label": 0
                },
                {
                    "sent": "So this is now possible with double VC recommendation.",
                    "label": 0
                },
                {
                    "sent": "And there is an example here that if you click on this particular tweet that I have made, you will just play this video between seconds.",
                    "label": 0
                },
                {
                    "sent": "Here 22 to 32.",
                    "label": 0
                },
                {
                    "sent": "So Twitter is skating the link so I have put it in bigger here.",
                    "label": 0
                },
                {
                    "sent": "But you see this is normal link with hash sign T equal and then two value, separated.",
                    "label": 0
                },
                {
                    "sent": "And this is the definitions of the video sequence.",
                    "label": 0
                },
                {
                    "sent": "I want to read.",
                    "label": 0
                },
                {
                    "sent": "So you can see here I'm using the hash sign for the limiting their, imitating the FRAGMENTA specification, and it's used to highlights.",
                    "label": 0
                },
                {
                    "sent": "In this case the temporal sequence, but it could also be used to highlight a special region within a frame.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me just bring a bit more historical perspective on how that happened.",
                    "label": 0
                },
                {
                    "sent": "Back in 2007, we come back in history.",
                    "label": 0
                },
                {
                    "sent": "The words just the fact that video at that time was really not really a first class citizen on the web to play video back in 2007.",
                    "label": 0
                },
                {
                    "sent": "You needed to have a plugin in your browser.",
                    "label": 0
                },
                {
                    "sent": "It could be a flash silver plug-in or Silverlight, QuickTime, whatever.",
                    "label": 0
                },
                {
                    "sent": "But you need a nut source.",
                    "label": 0
                },
                {
                    "sent": "Outside program within your web browser to be able to decode and play a video.",
                    "label": 0
                },
                {
                    "sent": "The browser couldn't play natively a video and it was very hard to find videos on the web.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you add YouTube already, but it was.",
                    "label": 0
                },
                {
                    "sent": "I mean, the indexing was not that good and so on so.",
                    "label": 0
                },
                {
                    "sent": "Based on this fact and the fact that we anticipate video playing a major role on the web, double officials organized two workshop in Brussels and in the San Jose in California in order to bring all interesting partners to build the stone up that would make video more used on the web.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, there's been a lot of papers submitted coming from many organizations.",
                    "label": 0
                },
                {
                    "sent": "You can read this in the web archive.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm just summarizing what our domain topics discussed in these two workshop addressing we wanted to have global identifiers for identifying special and temporal clips for for exactly what I'm just showing deep linking, bookmarking, caching and indexing.",
                    "label": 0
                },
                {
                    "sent": "Smaller subset of video content.",
                    "label": 0
                },
                {
                    "sent": "We wanted to have good metadata standards in order to be able to search and discover new content.",
                    "label": 0
                },
                {
                    "sent": "There was a lot of papers about video codec.",
                    "label": 0
                },
                {
                    "sent": "Codec is quite a nightmare.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of patterns around and there is no.",
                    "label": 0
                },
                {
                    "sent": "Open video correct for the World Wide Web.",
                    "label": 0
                },
                {
                    "sent": "So many people were arguing we need one so this one has been leftover.",
                    "label": 0
                },
                {
                    "sent": "WCC will not summarize any codec, but it was an issue, pointed out.",
                    "label": 0
                },
                {
                    "sent": "And finally there was the problem of content protection.",
                    "label": 0
                },
                {
                    "sent": "Managing digital rights associated with the media is key and we did.",
                    "label": 0
                },
                {
                    "sent": "The roof is issued work into metadata for digital rights.",
                    "label": 0
                },
                {
                    "sent": "It has been slow for doing this, but it's now coming up again more and more and actually just after this lecture, Roberto will exactly talk about this.",
                    "label": 0
                },
                {
                    "sent": "Topic, so in the chair I will talk about addressing in meta data with burner and later on you will have content protection covered with Roberto.",
                    "label": 0
                },
                {
                    "sent": "So Media Mixer is going through exactly the research agenda of this workshop at that time.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this workshop has led to the creation of an activity within GCP called video on the web for which the goal was to make the video of 1st Class Citizen the same way that image was a first class citizen on the web, and you can really see that with HTML5 where you have the video and audio elements, but enables the browser to natively decode audio and video files.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Be able to being able to reference parts of media content was not something that was already something existing at that time.",
                    "label": 0
                },
                {
                    "sent": "So here I'm just giving you again a very old example from 2007 where when you used to for the ones who have a Flickr account.",
                    "label": 0
                },
                {
                    "sent": "Upload your photos there.",
                    "label": 0
                },
                {
                    "sent": "You could already mark up some region and give it some tags to this region.",
                    "label": 0
                },
                {
                    "sent": "So in Flickr terms it's called a flicker nodes.",
                    "label": 0
                },
                {
                    "sent": "So here you have this picture.",
                    "label": 0
                },
                {
                    "sent": "Applauded by my colleague, a macro sunglass who just got his third baby and decided to draw a bounding box around this baby to give it this baby already at the blog when he was born.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is for special region but temporal region.",
                    "label": 0
                },
                {
                    "sent": "Temporal sequence already existed, so this is the official announcement from YouTube that it back from September 2008 where You Tube announced which support tempered seek point.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that at the end of any YouTube video you can just use this syntax.",
                    "label": 0
                },
                {
                    "sent": "Ash sequel an A number M for minutes and number, S for seconds and what it will mean.",
                    "label": 0
                },
                {
                    "sent": "It will mean that the video will directly sick to this time.",
                    "label": 0
                },
                {
                    "sent": "Points and start playing from this time point so you can do this, but with a specific syntax and YouTube was not the only one.",
                    "label": 0
                },
                {
                    "sent": "The other video sharing platforms add a similar functionality, but they all use a different syntax, so that makes interprete very odd.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the whole point of the media fragments working Group which was created afterwards.",
                    "label": 0
                },
                {
                    "sent": "Westest analyze all those syntaxes and the use cases we were looking after were bookmark sharing part or fragments of audio and video content.",
                    "label": 0
                },
                {
                    "sent": "Being able to annotate part of media content, being able to search for part of media Contents, and if of course developed.",
                    "label": 0
                },
                {
                    "sent": "Mashup collage of all those parts of media content and one of the strong use case we had behind is to conserve bandwidth.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in 230 seconds.",
                    "label": 0
                },
                {
                    "sent": "Video of out of the two hours video you don't want to download.",
                    "label": 0
                },
                {
                    "sent": "Of course the two hours video, but just the sequence you were interested in.",
                    "label": 0
                },
                {
                    "sent": "So how we enable?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This well this is exactly the major fragments you write specification.",
                    "label": 0
                },
                {
                    "sent": "So basically in media fragments you right you have 4 dimensions, so ways of splitting up your media content.",
                    "label": 0
                },
                {
                    "sent": "If you want you have of course that Emperor and the special sequence will come to this in a minute and you have two over dimension one which is called track here and here you really have to think in terms of media content that expose multiple tracks, for example.",
                    "label": 0
                },
                {
                    "sent": "In English, Audio and a French audio and you just want one of the two audio so you can select it.",
                    "label": 0
                },
                {
                    "sent": "Of course you need to know that the media file exposing number of tracks, but there is ways of doing this and the 4th and the final dimension is the so called name media fragments.",
                    "label": 0
                },
                {
                    "sent": "And here I would just make an analogy with DVD where you have chapters which have a name.",
                    "label": 0
                },
                {
                    "sent": "So again if your media files expose those chapters you can just reference those chapters here.",
                    "label": 0
                },
                {
                    "sent": "So net media fragments is just a shortcut for the temporal media fragment.",
                    "label": 0
                },
                {
                    "sent": "To which you give a name for this temporal sequence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is exactly what I'm saying here.",
                    "label": 0
                },
                {
                    "sent": "Again, time for fragments is just a clipping along the time dimension.",
                    "label": 0
                },
                {
                    "sent": "So you have a start and end for special.",
                    "label": 0
                },
                {
                    "sent": "It's limited to rectangular regions, not arbitrary shape, but just rectangle bounding box and tracking name fragment.",
                    "label": 0
                },
                {
                    "sent": "As I just said.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what it will look like if you play a media fragments in the player, you will see that this is the video player with the controls.",
                    "label": 0
                },
                {
                    "sent": "Here you have the entire timeline.",
                    "label": 0
                },
                {
                    "sent": "You know that this video is 38 seconds long, but in grill you have highlighted the fragmente.",
                    "label": 0
                },
                {
                    "sent": "I have selected and Gray.",
                    "label": 0
                },
                {
                    "sent": "This is the progress bar within this fragmente.",
                    "label": 0
                },
                {
                    "sent": "What it means is that when the Gray will reach the end of the green it will stop play OK and if you loop it will start playing again at the beginning of the fragmente.",
                    "label": 0
                },
                {
                    "sent": "Not at the beginning of the video.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is for the time for media fragments, for the special one.",
                    "label": 0
                },
                {
                    "sent": "Here the specification does not tell you how you should render in the in the in the browser, especially defragment it.",
                    "label": 0
                },
                {
                    "sent": "Just give you ideas of how you can do that.",
                    "label": 0
                },
                {
                    "sent": "So one of course ideas to crap to do.",
                    "label": 0
                },
                {
                    "sent": "Sprite like CSS will do on the frame.",
                    "label": 0
                },
                {
                    "sent": "Another way is to just put a similar pake overlay on the rest of the image, except for the bounding box.",
                    "label": 0
                },
                {
                    "sent": "Selected that you will highlighted OK so you can play additional right to play with a number of.",
                    "label": 0
                },
                {
                    "sent": "This is typical media fragmente player where you can input a video and start playing it with those dimensions selected.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do you write those dimensions?",
                    "label": 0
                },
                {
                    "sent": "This is the syntax that we use.",
                    "label": 0
                },
                {
                    "sent": "The syntax basically is for the temporal dimensions, T equal to integer, separated with, which represents the start time and end time of the fragmente.",
                    "label": 0
                },
                {
                    "sent": "So here I'm making distinctions between the query parameter and the fragments parameter of a UI.",
                    "label": 0
                },
                {
                    "sent": "So this is just the normal UI syntax which starts with a protocol, for example HTTP which GPS: slash slash.",
                    "label": 0
                },
                {
                    "sent": "Then a domain name.",
                    "label": 0
                },
                {
                    "sent": "So in this case example.org and then after the video resource you append either?",
                    "label": 0
                },
                {
                    "sent": "Or hash sign.",
                    "label": 0
                },
                {
                    "sent": "So this may be a segmental right syntax is valid for both the query and the hash, and it can be combined as it is done, but it has a different behavior depending if you use it with a query or Weaver hashtag.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going a bit more technical.",
                    "label": 0
                },
                {
                    "sent": "It depends how much you're familiar already with the euro.",
                    "label": 0
                },
                {
                    "sent": "Instructions.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when we meet your eye in a web server, there is this notion that we provide the web server actually is serving you, or representation of the resource identified by this your eye.",
                    "label": 0
                },
                {
                    "sent": "And there is a notion of primary resource and secondary resource.",
                    "label": 0
                },
                {
                    "sent": "So here if you use the ashline, we really talk about secondary resource.",
                    "label": 0
                },
                {
                    "sent": "That means that the server is serving you bite range exact portion.",
                    "label": 0
                },
                {
                    "sent": "Off the primary resource that was identified before the hash sign in the UI, which means that to be able to do that, you need to extract.",
                    "label": 0
                },
                {
                    "sent": "The exact bike ranges from this representation.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The advantage is potentially cacheable.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when you use a question mark what the web server is doing is creating a new resource, a new representation of this resource, and this new representation has nothing to do.",
                    "label": 0
                },
                {
                    "sent": "In theory, we have the resources stems from, so if you do that here, you will create a new video completely new video which has four left X 10 seconds, which happens to be the excerpt.",
                    "label": 0
                },
                {
                    "sent": "Of a longer video that was specified without the question mark, so of course that is not cashable.",
                    "label": 0
                },
                {
                    "sent": "So that is a different use case.",
                    "label": 0
                },
                {
                    "sent": "And then we did not talk anymore about this.",
                    "label": 0
                },
                {
                    "sent": "But now when we talk about media fragments, I would just talk about the ash version.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the media fragment.",
                    "label": 0
                },
                {
                    "sent": "So let me just skip this part to not add confusion, but for the UI fragment part.",
                    "label": 0
                },
                {
                    "sent": "So normally when you try to do a get action on the UI which contains the hash, the normal behavior is to strip out everything after the hash and just send to the server and the UI you want before the hash sign OK.",
                    "label": 0
                },
                {
                    "sent": "So that will slightly evolved with the media fragmentary specification in the sense that now you will have smart user agents which will.",
                    "label": 0
                },
                {
                    "sent": "Indeed, strip out the fragment definition, but encoded in terms of a custom HTTP headers using the range header into the request.",
                    "label": 0
                },
                {
                    "sent": "Which means that media servers that will understand this range requests will strip out the bits of the video that corresponds to the sequence you have requested and just serve this sequence.",
                    "label": 0
                },
                {
                    "sent": "Now, if you don't have a smart user agent but the normal user agent but does not understand the syntax, it would just query and request the entire video so the web will not be broken, and on the other side, if you don't have a smart media server, so no more server that does not understand this range request.",
                    "label": 0
                },
                {
                    "sent": "Normally the policy on the web is in your everything you don't understand, so it will just ignore this and serve you the entire video.",
                    "label": 0
                },
                {
                    "sent": "So again, we're not breaking the web.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm saying here is backward compatibility with existing web infrastructure.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I put this into an image, you still have two scenarios.",
                    "label": 0
                },
                {
                    "sent": "The first scenario where basically it's what we call two wins N Shake.",
                    "label": 0
                },
                {
                    "sent": "So in this scenario the client wants to have this particular movie between seconds 12 and 21, so he thought he saw this UI request with ash T equal 12, 21.",
                    "label": 0
                },
                {
                    "sent": "It will interpret this as a media fragment to write an issuer and requests with this particular range editor.",
                    "label": 0
                },
                {
                    "sent": ": sequence equal 12, 21.",
                    "label": 0
                },
                {
                    "sent": "The Smart Media server will say OK, I understand this.",
                    "label": 0
                },
                {
                    "sent": "It's a read request express in seconds.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to strip out this media file.",
                    "label": 0
                },
                {
                    "sent": "Well now you have to position me to the closest I frame of this video.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it corresponds to 1185.",
                    "label": 0
                },
                {
                    "sent": "Two 21.16 units approximation and I'm serving you back.",
                    "label": 0
                },
                {
                    "sent": "This bits of content.",
                    "label": 0
                },
                {
                    "sent": "We've not two OK answer, but the two or six.",
                    "label": 0
                },
                {
                    "sent": "Status code which means partial content.",
                    "label": 0
                },
                {
                    "sent": "I'm sending you a partial content of this complete resource.",
                    "label": 0
                },
                {
                    "sent": "The four way in check is almost the same.",
                    "label": 0
                },
                {
                    "sent": "The same request is issued from the user agent, but the server says OK.",
                    "label": 0
                },
                {
                    "sent": "I can serve you this.",
                    "label": 0
                },
                {
                    "sent": "Bits of videos.",
                    "label": 0
                },
                {
                    "sent": "Actually the closest interval I found is this and what is expressed in seconds.",
                    "label": 0
                },
                {
                    "sent": "I can tell you that corresponds to this byte ranges OK, so here it converts the sequence in terms of byte ranges and here it does not send any data in the body is just.",
                    "label": 0
                },
                {
                    "sent": "Ahead answer, so it's transparent for the user the user agent received this response and say OK, Now I can issue another request, but instead of expressing it in terms of seconds, I'm expressing in terms of bytes because now I have the mapping between seconds and bites an.",
                    "label": 0
                },
                {
                    "sent": "What's the advantage of doing this?",
                    "label": 0
                },
                {
                    "sent": "Is that any web server in the world already answered by train request, so the rent request expressed in seconds.",
                    "label": 0
                },
                {
                    "sent": "It's complicated.",
                    "label": 0
                },
                {
                    "sent": "Not a lot of web servers can do this.",
                    "label": 0
                },
                {
                    "sent": "But a request express in terms of byte ranges.",
                    "label": 0
                },
                {
                    "sent": "Every web service can do this.",
                    "label": 0
                },
                {
                    "sent": "So if you do this then any anything in the in the middle between the server and the user agent, so proxy or cash, whatever will be able to cash this resource an service later on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so of course in order to be able to do this you need to have media fragments that you can easily split an extract things from from it.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do that with any video format you can do that only with video format, But our budget is seekable, so think about MPEG four web M org.",
                    "label": 0
                },
                {
                    "sent": "Those video formats you can extract pieces and be able to play them.",
                    "label": 0
                },
                {
                    "sent": "A video format such as Avi, which is not streamable.",
                    "label": 0
                },
                {
                    "sent": "You will not be able to do this.",
                    "label": 0
                },
                {
                    "sent": "So the requirement is yeah presence of intra coded frames, random access points for video.",
                    "label": 0
                },
                {
                    "sent": "For special track is so complicated, we will just keep this for now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is basically what you can do with the media fragmentary UI specifications.",
                    "label": 0
                },
                {
                    "sent": "Now you should wonder what tools can I use today, but understand this stuff well from the web browser perspective, the major web browser already natively supports media fragments.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the case for Fire Fox since quite a lot of time for Safari for Chrome at least, since now nearly two years.",
                    "label": 0
                },
                {
                    "sent": "In addition to this, you have a number of JavaScript library which we call polyfill that have been developed for both the temporal and spatial dimension and what those JavaScript library do is that if you embed them into your website, then whatever browser but will read them because they include this JavaScript, they can simulate emulates the behavior of media fragment your eyes so.",
                    "label": 0
                },
                {
                    "sent": "Even Internet Explorer with JavaScript library you could have the behavior of media fragments.",
                    "label": 0
                },
                {
                    "sent": "And finally, you have a lot of custom video players that support media fragment.",
                    "label": 0
                },
                {
                    "sent": "I just cite a few of them laying the term sign out so Jesse is developing one for the video lectures mashup that you will see tomorrow at vinnies.",
                    "label": 0
                },
                {
                    "sent": "Never one extractor.",
                    "label": 0
                },
                {
                    "sent": "You have a growing number of clients that supports this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was for the clients.",
                    "label": 0
                },
                {
                    "sent": "For the servers is a bit more poor in terms of offerings.",
                    "label": 0
                },
                {
                    "sent": "You still have one very good which is called in sooner.",
                    "label": 0
                },
                {
                    "sent": "It's developed by I Mines University of Ghent in Belgium, so it's an academic one, but it works pretty nicely and even develop a proxy on top of this.",
                    "label": 0
                },
                {
                    "sent": "And there is another one which is being developed at the moment.",
                    "label": 0
                },
                {
                    "sent": "It's a partnership between the University of Southampton and Neurocom.",
                    "label": 0
                },
                {
                    "sent": "It would be not just based.",
                    "label": 0
                },
                {
                    "sent": "Implementations of Media Fragment Server and just to mention some big video sharing platforms, both YouTube and Dailymotion has partial support or bug reports in their system so that they are being soon up fully fully compliant with the specifications at the moment.",
                    "label": 0
                },
                {
                    "sent": "While I'm I'm saying it's partial support is because the syntax is slightly different or they don't have all the functionalities yet.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that covers the point of media fragments.",
                    "label": 0
                },
                {
                    "sent": "You're right addressing.",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions at this point?",
                    "label": 0
                },
                {
                    "sent": "No, it was clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second part of this talk I want to talk about annotations.",
                    "label": 0
                },
                {
                    "sent": "So like Vasilios shown early on issues, a lot of tools that enables to segment 8 video content and then the results could be media fragment Orion's you've just seen.",
                    "label": 0
                },
                {
                    "sent": "And then it shows you a number of.",
                    "label": 0
                },
                {
                    "sent": "Algorithms and tools for automatically and understanding what the content is about provide annotations in terms of concepts in terms of instance in terms of objects that you would like to re detect, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is what I wanted to talk here about annotations, and I will emphasize the point of getting semantic annotations, not only strings that describe the multimedia content, but if possible, objects.",
                    "label": 0
                },
                {
                    "sent": "Identify with semantic web technologies for which we have more background knowledge about, because this is representing in terms in within knowledge bases such as Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me just show you an example of what I meant with what I called semantic annotations.",
                    "label": 0
                },
                {
                    "sent": "So this is a picture in this case coming from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "This is the famous big free at the Yalta Conference.",
                    "label": 0
                },
                {
                    "sent": "So you can recognize Winston Churchills Roosevelts in standing, sitting down in 45 in order to split the world because they won the war.",
                    "label": 0
                },
                {
                    "sent": "So if I would like to annotate this picture, I could I use one of those nice tools that Bacillus has shown which can extract for example faces or people within the image.",
                    "label": 0
                },
                {
                    "sent": "So in this case Pro bounding box around this character.",
                    "label": 0
                },
                {
                    "sent": "And this is what we call the media fragment creation phase.",
                    "label": 0
                },
                {
                    "sent": "So Localiza region in the picture.",
                    "label": 0
                },
                {
                    "sent": "And if I would like to annotate it, probably this is an interpretation.",
                    "label": 0
                },
                {
                    "sent": "It's my interpretation of this picture.",
                    "label": 0
                },
                {
                    "sent": "I would like to use terms such as Winston Churchill, UK Prime Minister's Force, World War Two, and every terms like this but remind me about the context of this picture.",
                    "label": 0
                },
                {
                    "sent": "Now if I don't want to just annotate this picture but provides semantic annotations, I will do almost the same thing.",
                    "label": 0
                },
                {
                    "sent": "I will say in here I'm using a particulare.",
                    "label": 0
                },
                {
                    "sent": "Syntax, by the way, which is familiar with semantic web technologies in the room.",
                    "label": 0
                },
                {
                    "sent": "Can you just raise your hands?",
                    "label": 0
                },
                {
                    "sent": "Semantic web technologies?",
                    "label": 0
                },
                {
                    "sent": "How are you?",
                    "label": 0
                },
                {
                    "sent": "Is your gym of the morning?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah 123.",
                    "label": 0
                },
                {
                    "sent": "45 not many OK so here I'm using a porch Kra syntax tried to abstract from this which sort of triple representations so there is subject predicate and object and what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that this region here.",
                    "label": 0
                },
                {
                    "sent": "Depicts Winston Churchill.",
                    "label": 0
                },
                {
                    "sent": "So looks like exactly what I just said with my normal mentation, except that Winston Churchill now is not anymore a string.",
                    "label": 0
                },
                {
                    "sent": "It's UI which comes from the pedia or from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And because of this, and because Wikipedia is nice to give us much more information, we know from Wikipedia that this page this Wikipedia page indicates in the infobox of this page that.",
                    "label": 0
                },
                {
                    "sent": "This Churchill guy ask for name Winston Churchill is a person he used to be a Prime Minister of the UK and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "Give many more information.",
                    "label": 0
                },
                {
                    "sent": "Of course everything written info box so all those information we get it for free because we use your eye from this knowledge base.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I could have done the same thing with the video.",
                    "label": 0
                },
                {
                    "sent": "This is another video coming from Richards, the news agency, and this video is entitled History of Jade Violence.",
                    "label": 0
                },
                {
                    "sent": "This is just based on the fact that every time you have a submit, you submit a worldwide submitter J or G20 summit.",
                    "label": 0
                },
                {
                    "sent": "Often you have a counter protest.",
                    "label": 0
                },
                {
                    "sent": "We've sometimes a lot of people who violently protest and Richard wanted to make a video showing up all the.",
                    "label": 0
                },
                {
                    "sent": "Problems that you had in the past summit.",
                    "label": 0
                },
                {
                    "sent": "So basically this video consists of a lot of clips that each represents violence that happened in one of those summits.",
                    "label": 0
                },
                {
                    "sent": "I could do the same thing, creates media fragments.",
                    "label": 0
                },
                {
                    "sent": "So is relate some sequence within this long video.",
                    "label": 0
                },
                {
                    "sent": "So for example this one and this one and I could start tagging those summits.",
                    "label": 0
                },
                {
                    "sent": "So if I'm watching this video will understand that the first one is about the J submits which used to take place in elegant dam in Germany in 2007, while the 4th sequence is about the EU summit, which took place in front one in Gothenburg in Sweden and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "But again, I'm telling you I don't want to annotate.",
                    "label": 0
                },
                {
                    "sent": "I want to semantically annotate.",
                    "label": 0
                },
                {
                    "sent": "So instead of using those strings.",
                    "label": 0
                },
                {
                    "sent": "I would say the sequence depicts the 38 to 43rd G8 summit as identified in Wikipedia, and by doing this I know thanks to Wikipedia that the J Summit took place in those Giorgio coordinates, which happens to be the coordinates of elegant damn the city in Germany, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So immediately I get much more information for free because I'm using information knowledge we have already from Wikipedia or from other sources.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just my takeaway summary of this semantic condition aspects is that.",
                    "label": 0
                },
                {
                    "sent": "Rather than using strings for annotating using, things is much more powerful.",
                    "label": 0
                },
                {
                    "sent": "This is what you see.",
                    "label": 0
                },
                {
                    "sent": "We've the Google Knowledge Graph panel.",
                    "label": 0
                },
                {
                    "sent": "When you're querying Google and so on and so on, we have now massive amounts of knowledge basis, which is the linked open data cloud represented by this picture, which provides a lot of statements of facts about real world objects about reward entities that we can use, force providing automatic annotation.",
                    "label": 0
                },
                {
                    "sent": "We have a growing numbers of well curated vocabularies for doing this annotation.",
                    "label": 0
                },
                {
                    "sent": "This is called love, linked open vocabularies and we are sort of advocating to follow those four link data principles or even which has been refined by Linda Nixon as the four linked media principle.",
                    "label": 0
                },
                {
                    "sent": "So you expose your media descriptions and you basically make everything possible to foster their reuse.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "I start two of all the pieces of the puzzle I show you media fragments your eye as a way of addressing, providing deep linking into media assets.",
                    "label": 0
                },
                {
                    "sent": "I show you the link data principles for providing annotations and knowledge bases that exists where you can take your eye of those entities in order to populate your annotations.",
                    "label": 0
                },
                {
                    "sent": "What is missing is the glue between them is what's the annotation model that one could use.",
                    "label": 0
                },
                {
                    "sent": "So here you have quite a lot of annotation model that exists.",
                    "label": 0
                },
                {
                    "sent": "Out there.",
                    "label": 0
                },
                {
                    "sent": "But one of them which is growing in terms of support and in terms of adoptions, is the so called open annotation model.",
                    "label": 0
                },
                {
                    "sent": "This model is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "It starts from the fact that indentation is between the body and the target and what that mean.",
                    "label": 0
                },
                {
                    "sent": "It means simply that this body is related in some way to this target and we will be able to specify those things and we will be able to characterize.",
                    "label": 0
                },
                {
                    "sent": "What is this related to relationship between this body and this target?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It has a concrete encoding into RDF.",
                    "label": 0
                },
                {
                    "sent": "One of these semantic web technologies and you can characterize this really two.",
                    "label": 0
                },
                {
                    "sent": "We have a so called motivation, which is the motivation why you would like to annotate this.",
                    "label": 0
                },
                {
                    "sent": "Contact so let me show you a concrete example.",
                    "label": 0
                },
                {
                    "sent": "Here I just want to add a date using this model.",
                    "label": 0
                },
                {
                    "sent": "The simple picture.",
                    "label": 0
                },
                {
                    "sent": "So the simple picture which is the Eiffel Tower.",
                    "label": 0
                },
                {
                    "sent": "I would like to annotate it with this FL Tower identifier from DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Again, the semantic Web version of Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what it will look like.",
                    "label": 0
                },
                {
                    "sent": "Bit complicated, but actually not not that much.",
                    "label": 0
                },
                {
                    "sent": "As you can see there is this annotation in the center in the middle where you have a body in the target.",
                    "label": 0
                },
                {
                    "sent": "The target is the picture.",
                    "label": 0
                },
                {
                    "sent": "OK, so it could be a part of the picture, but in this case it's just different out and the body is what you want to say about this picture.",
                    "label": 0
                },
                {
                    "sent": "In this case I want to say this is the Eiffel Tower.",
                    "label": 0
                },
                {
                    "sent": "Now all the other things around is to indicate provenance of this annotations.",
                    "label": 0
                },
                {
                    "sent": "So you can say OK, this is an attention you can specify why.",
                    "label": 0
                },
                {
                    "sent": "Why do you want to annotate this picture with this identifier?",
                    "label": 0
                },
                {
                    "sent": "Well, in this case I just want to tag this picture, but it could be another motivation so you have a long list of motivation.",
                    "label": 0
                },
                {
                    "sent": "Which pre exists in the model.",
                    "label": 0
                },
                {
                    "sent": "You can say when did you made this annotations using which tools was it automatic?",
                    "label": 0
                },
                {
                    "sent": "Using one of those tools adversity is shown or manual who has made this annotation?",
                    "label": 0
                },
                {
                    "sent": "Can I have more informations about this person extra?",
                    "label": 0
                },
                {
                    "sent": "So all those information is just provenance around the annotation.",
                    "label": 0
                },
                {
                    "sent": "Where the annotation is here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now you have the free puzzles I wanted to talk about media fragment, your eye open annotation as a core model and linked open data cloud as form of your eye that you can use for identifying real world entities useful for in your notations.",
                    "label": 0
                },
                {
                    "sent": "And you have a number of project that exists, but make use of all of them.",
                    "label": 0
                },
                {
                    "sent": "One of them is Map Hub which has for ambition to annotate old Maps from very old Maps.",
                    "label": 0
                },
                {
                    "sent": "Of countries of cities and so on.",
                    "label": 0
                },
                {
                    "sent": "So here I mean they really have to do this.",
                    "label": 0
                },
                {
                    "sent": "They have big Maps, they want to localize part of the Maps and the use media fragmentary for doing this.",
                    "label": 0
                },
                {
                    "sent": "They need to say something about those regions and they use the open innovation model for being able to say something about those parts of those Maps.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another one is the open video annotation project.",
                    "label": 0
                },
                {
                    "sent": "We're here again.",
                    "label": 0
                },
                {
                    "sent": "It's really it's infancy yet, and there's not much functionality but the idea again is to provide a simple way of providing semantic annotations of parts of videos, and again, here the technologies used.",
                    "label": 0
                },
                {
                    "sent": "Army defragmenter.",
                    "label": 0
                },
                {
                    "sent": "I open invitation Ontology and so on.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annotations of video again that exists since quite a lot of time in YouTube.",
                    "label": 0
                },
                {
                    "sent": "You have You Tube annotations.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure many of you have already used this, but basically YouTube world annotations are clickable text overlays on YouTube videos and the primary goal for YouTube when they introduced this was for getting advertisements.",
                    "label": 0
                },
                {
                    "sent": "So you can put deep linking into YouTube video and plug some advertisements that people can click on them.",
                    "label": 0
                },
                {
                    "sent": "Ann, get ready to buy things and things like this.",
                    "label": 0
                },
                {
                    "sent": "So annotations were really used to boost engagements, give more informations and adding navigation is too.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the type of annotations that you have.",
                    "label": 0
                },
                {
                    "sent": "What we call note, think about the Flicker notes.",
                    "label": 0
                },
                {
                    "sent": "This is the same idea speech bubble, so it's just what the person is saying.",
                    "label": 0
                },
                {
                    "sent": "But put into a bubble.",
                    "label": 0
                },
                {
                    "sent": "Transcripts.",
                    "label": 0
                },
                {
                    "sent": "And you have a fourth type of annotation, so this is really, really mentari basic annotations.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we have shown you.",
                    "label": 0
                },
                {
                    "sent": "Since the beginning of today was another way of getting automatic annotation.",
                    "label": 0
                },
                {
                    "sent": "Is using tools an algorithm that will analyze the content and try by themselves to identify specific type of objects?",
                    "label": 0
                },
                {
                    "sent": "So fast videos just before show examples where we can detect concepts or instances out of a video program?",
                    "label": 0
                },
                {
                    "sent": "So in this case here, this antique roadshow program or this?",
                    "label": 0
                },
                {
                    "sent": "And you show program coming from the RB broadcaster.",
                    "label": 0
                },
                {
                    "sent": "It could be also an object detection algorithm that just vasilios shown where the user would draw a bounding by bounding box.",
                    "label": 0
                },
                {
                    "sent": "I don't know why it does, it should be something around this painting and by doing so you will actually detect all occurrence of this paintings later on in the video.",
                    "label": 0
                },
                {
                    "sent": "So this is how we could get automatic annotations.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again here the idea is to slowly move from using strings that.",
                    "label": 0
                },
                {
                    "sent": "Represents you tags around those media content to really things, but are identified knowledge base because by doing so you get for free more background knowledge about those things.",
                    "label": 0
                },
                {
                    "sent": "So in this case if instead of just writing the name of the paint around this paintings I use a UI coming from Wikipedia of this of this painter then immediately I get a lot of more information about this painter.",
                    "label": 0
                },
                {
                    "sent": "I know the style that is used to perform.",
                    "label": 0
                },
                {
                    "sent": "For his paintings so Cubism, Expressionism, Fauvism's, and based on those style concepts, I can pin points to link to over related content.",
                    "label": 0
                },
                {
                    "sent": "Of the same style.",
                    "label": 0
                },
                {
                    "sent": "So if you're reading this, accept a video about this paintings then we can suggest you to look at over paintings which we believe are related to becausw.",
                    "label": 0
                },
                {
                    "sent": "They share in this.",
                    "label": 0
                },
                {
                    "sent": "In this particular example the same style.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what Link TV and media mixers are promoting.",
                    "label": 0
                },
                {
                    "sent": "I think you have heard about those use cases earlier, so just to give you a concrete examples, imagine that we have this video here.",
                    "label": 0
                },
                {
                    "sent": "We told you on the first steps is always creating subset of this video.",
                    "label": 0
                },
                {
                    "sent": "Fragments of this video.",
                    "label": 0
                },
                {
                    "sent": "So here for example a particular chapter or sequence of this video.",
                    "label": 0
                },
                {
                    "sent": "Identify you see with the media fragment you write an on which you would like to say something about it.",
                    "label": 0
                },
                {
                    "sent": "So in this case I want to say that in this particular sequence of this old movie The two main characters are Bogart and Bergman, and they both exists in Wikipedia, so it's great.",
                    "label": 0
                },
                {
                    "sent": "I can use this Wikipedia entry for dictating this sequence.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I really want to do later on is coming from starting from this particular video sequence start.",
                    "label": 0
                },
                {
                    "sent": "Proposing related content that could be associated to this particular video sequence.",
                    "label": 0
                },
                {
                    "sent": "And for that we rely on chair annotation, so I know that this particular sequence talk about Bogart and Bergman, so he stopped watching these seconds.",
                    "label": 0
                },
                {
                    "sent": "I would like to propose over parts of media contents, but either features those same characters, people, or features over related content to be decided by an algorithm to be proposed to be watched.",
                    "label": 0
                },
                {
                    "sent": "So here what we see is that.",
                    "label": 0
                },
                {
                    "sent": "Name entity play a key role in this hyperlinking world.",
                    "label": 0
                },
                {
                    "sent": "So in this case here because I have detected this person I can of course find images of this person over videos where he has played with and among all the videos.",
                    "label": 0
                },
                {
                    "sent": "Yes play with I can start proposing what I believe are related sequence because they share a number of attributes and annotations with the original sequence so perhaps I would like to propose this video sequence coming from this longer movie.",
                    "label": 0
                },
                {
                    "sent": "As related content to this particular sequence, based on a number of criteria.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I'm saying that name entity, player key role.",
                    "label": 0
                },
                {
                    "sent": "So it's important to be able to detect named entities.",
                    "label": 0
                },
                {
                    "sent": "In videos, so how you do this?",
                    "label": 0
                },
                {
                    "sent": "You can rely on the visual signal.",
                    "label": 0
                },
                {
                    "sent": "You can implement a face detection algorithm and then once you have detected the face you could try to recognize.",
                    "label": 0
                },
                {
                    "sent": "Which person this face belongs to.",
                    "label": 0
                },
                {
                    "sent": "So for example, this shaft rain, a machine learning classifier to recognize Barack Obama in videos, and assuming that you have been successful in doing so, then every videos that will.",
                    "label": 0
                },
                {
                    "sent": "I like Barack Obama.",
                    "label": 0
                },
                {
                    "sent": "You will be able to recognize this person and that could be a good thing for the particle applications you're developing.",
                    "label": 0
                },
                {
                    "sent": "Another way is to not rely rely on the visual signal, but to rely on the transcript of the video.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case here we will analyze the textual transcript of the video and then we'll just try to spot this name entity within the transcript.",
                    "label": 0
                },
                {
                    "sent": "And of course those methods are not disjoint, and ideally you would like to combine both.",
                    "label": 0
                },
                {
                    "sent": "You would like to combine the visual analysis with what you can do on the text and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So this multimodal approach.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, let me focus on textual analyzes for detecting name entities within transcript.",
                    "label": 0
                },
                {
                    "sent": "Traditionally you could find some standalone software that you can install on your machine and play with to detect an entity.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned some of the famous platforms gates because it's an open source academic platforms that anybody can install and even extend because you can develop your own gazetteer to recognize more name entities.",
                    "label": 0
                },
                {
                    "sent": "Stanford core NLP, which is sort of the baseline for this community.",
                    "label": 0
                },
                {
                    "sent": "Again in open source.",
                    "label": 0
                },
                {
                    "sent": "Target and Tim is just because I wanted to mention one commercial company that does this.",
                    "label": 0
                },
                {
                    "sent": "Intimes pretends to be the best for the European languages to do that.",
                    "label": 0
                },
                {
                    "sent": "But what we have recently seen, let's say over the last four years, is a very large number of Web API, but does exactly what those standalone software do, but on the way what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that generally they enable you to register to get a developer key, and then you, as a developer can just interact with those black boxes services and send me the text and get results out of it.",
                    "label": 0
                },
                {
                    "sent": "You don't understand how those tools work.",
                    "label": 0
                },
                {
                    "sent": "Actually, you don't really have to understand how do they work, they're just giving you results.",
                    "label": 0
                },
                {
                    "sent": "And here I mentioned a lot of tools, but there are many more.",
                    "label": 0
                },
                {
                    "sent": "Of course those tools are different assumptions.",
                    "label": 0
                },
                {
                    "sent": "They don't have the same language to cover the account, detect the same type of name entities they have, more or less limitations in terms of the volume of text that can analyze per day, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So they are not or equally compatible, but the the basic functionalities of extracting name entities and disambiguating images.",
                    "label": 0
                },
                {
                    "sent": "Is present for all those tools.",
                    "label": 0
                },
                {
                    "sent": "What we have started to work on since 2 years.",
                    "label": 0
                },
                {
                    "sent": "Enough now is.",
                    "label": 0
                },
                {
                    "sent": "A tool which is called nerd that you can all use that make the sum of all of them.",
                    "label": 0
                },
                {
                    "sent": "So no extends from name.",
                    "label": 0
                },
                {
                    "sent": "Entity recognition is immigration this is.",
                    "label": 0
                },
                {
                    "sent": "Free things this is an API that anyone can use just by reaching to nerd, but this is also a front end user interface so you don't even need to register.",
                    "label": 0
                },
                {
                    "sent": "You can just go to Nerd website, submit a text and analyze it.",
                    "label": 0
                },
                {
                    "sent": "So what nerd is doing is taking the results of.",
                    "label": 0
                },
                {
                    "sent": "Stanford core NLP training algorithm combine?",
                    "label": 0
                },
                {
                    "sent": "We've all those tools together and of course the magic is out to learn how to combine optimally all those tools to have the best results.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what we do in there than we do that for two reasons why one because we would like to compare the strength and weakness of all those API and in order to be able to do so we need to put them into the same framework to ease the comparison and seconds to of course improve the recognitions and immigration.",
                    "label": 0
                },
                {
                    "sent": "Because if we take the best or the tools or naive assumption which tends to be verified is that we will be better than one of those tools.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what the Nerd user interface.",
                    "label": 0
                },
                {
                    "sent": "You can input any UI of a web document or plain text document.",
                    "label": 0
                },
                {
                    "sent": "If you use the API, you can even submit a subtitle.",
                    "label": 0
                },
                {
                    "sent": "So what we call a time text and you will get the lists.",
                    "label": 0
                },
                {
                    "sent": "So then you will be able to select a particular structure or all of them and you get the list of results.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if I'm combining everything, media fragments open innovation and there this is what we obtain.",
                    "label": 1
                },
                {
                    "sent": "A very big graph, which is our semantic annotations of the content of the video president is extracted from the subtitle of this video, so I'm not assuming you will read all those details or let me make you a guided tour of this graph here.",
                    "label": 0
                },
                {
                    "sent": "This is just a UI of the media resource.",
                    "label": 0
                },
                {
                    "sent": "OK, this is an abstract identifier of this media.",
                    "label": 0
                },
                {
                    "sent": "Source that exists somewhere this media resource could be a particular play of Shakespeare.",
                    "label": 0
                },
                {
                    "sent": "Then of course you have a concrete media file at the end we will particulare encoding of this video, so this will be the locator.",
                    "label": 0
                },
                {
                    "sent": "So this abstract media resource, you can find it in the particular resolutions in the particular video correct formats.",
                    "label": 0
                },
                {
                    "sent": "In this case MPEG four at this year, right?",
                    "label": 0
                },
                {
                    "sent": "Now we can take and accept.",
                    "label": 0
                },
                {
                    "sent": "With the fragments out of this media resource, this is this.",
                    "label": 0
                },
                {
                    "sent": "Is this media fragments.",
                    "label": 0
                },
                {
                    "sent": "So here you will use the same beginning of the UI, but Ash sequel and in particular time sequence.",
                    "label": 1
                },
                {
                    "sent": "This major segments you can annotate it so the target of this annotation will be the media fragments and the body.",
                    "label": 0
                },
                {
                    "sent": "Remember Target body will be this entity.",
                    "label": 0
                },
                {
                    "sent": "This entity in this case as type, its location and it has a value.",
                    "label": 0
                },
                {
                    "sent": "In this case this is the handbag.",
                    "label": 0
                },
                {
                    "sent": "Resource in Germany, Berlin and believe which is not identified in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So what we mean here is that this year I annotate this particular fragments, which is part of a larger video which is, As for locator, the popular media file.",
                    "label": 0
                },
                {
                    "sent": "And here you can even point back to the particulare time in the video when so sorry the in the subtitle when this entity has been spot and you have additional provenance informations about how you discover this name entity.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we put all those things into concrete applications.",
                    "label": 0
                },
                {
                    "sent": "We call media fragments and richer that you can have a try here.",
                    "label": 0
                },
                {
                    "sent": "This year I what you can do here is enter your eye of a video or still on the video sharing platform.",
                    "label": 0
                },
                {
                    "sent": "It's supposed to work with both their emotions on YouTube in this case and when we click on preview you will first get a lot of metadata that those video sharing platform is giving to you.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "I put this YouTube video which is in title or simple ideas lead to scientific discovery by Adam Savage.",
                    "label": 0
                },
                {
                    "sent": "It's a Ted talk and I will immediately get a number of things that YouTube is telling me, such as descriptions.",
                    "label": 0
                },
                {
                    "sent": "The category of this video, it's education.",
                    "label": 0
                },
                {
                    "sent": "But it has been applauded.",
                    "label": 0
                },
                {
                    "sent": "A number of statistics, such as the number of views, favorite comments, blah blah blah, no tax for this video, its duration.",
                    "label": 0
                },
                {
                    "sent": "So this is what you can get.",
                    "label": 0
                },
                {
                    "sent": "What we can Additionally get is the subtitle of this video.",
                    "label": 0
                },
                {
                    "sent": "So in this application we put you have to Scroll down a big button which is called notify, but this means we'll just launch nerd on the subtitles, and if you do that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This immediately you will get.",
                    "label": 0
                },
                {
                    "sent": "You will go to another page where you have your original video.",
                    "label": 0
                },
                {
                    "sent": "We've all the name entities we have extracted from the subtitle and what this means that doesn't limit is also our disambiguating.",
                    "label": 0
                },
                {
                    "sent": "So in this particular video they talk about the Challenger disaster.",
                    "label": 0
                },
                {
                    "sent": "If you don't remember what the challenges disaster was.",
                    "label": 0
                },
                {
                    "sent": "It has been highlighted to just moves over this string and you get the info box of Wikipedia reminding you what the Challenger disaster.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "Later on in this video they talk about fame, angiograms math.",
                    "label": 0
                },
                {
                    "sent": "My math was a bit rusty.",
                    "label": 0
                },
                {
                    "sent": "I couldn't remember what the fame and diagrams was, but Luckily it has been the name entity recognized by the software.",
                    "label": 0
                },
                {
                    "sent": "I can just over it and I will have the Wikipedia infobox reminding me what defend map diagrams here.",
                    "label": 0
                },
                {
                    "sent": "And if I'm clicking on it will have the full Wikipedia page and so on.",
                    "label": 0
                },
                {
                    "sent": "So you see this is just a simple applications that enrich fragments of video with named entities we can extract from the subtitles.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that that this idea of linking fragments of videos to over content Wikipedia page, but potentially over multimedia content, is the root of what we call the linked media layer, is the idea of you linked your video sequence to over video sequence?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we work with another.",
                    "label": 0
                },
                {
                    "sent": "Collections and benchmark which is called medieval and to which Benoit will talk more about in 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "So here I just wanted to basically make the years old with what you will see next, But the idea here is that you have a closed collections big closed collections of of video material and you would like to use those techniques.",
                    "label": 0
                },
                {
                    "sent": "I've just shown in order to re late one passage of a video to another pasage and do that automatically.",
                    "label": 0
                },
                {
                    "sent": "For doing this we.",
                    "label": 0
                },
                {
                    "sent": "So in this case and then we go deeper into this, you want to analyze this entire closed collections to analyze content to content.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another way of doing this is to link part of your video to any over multimedia content that you can find, for example on social networks.",
                    "label": 0
                },
                {
                    "sent": "So here we have developed another tool called Media Collector.",
                    "label": 0
                },
                {
                    "sent": "Which query alot of social networks that exists.",
                    "label": 0
                },
                {
                    "sent": "So Twitter, Flickr, Facebook, Google Plus, YouTube, Instagram extracts, raw?",
                    "label": 0
                },
                {
                    "sent": "Again, we unify all their response into a single format.",
                    "label": 0
                },
                {
                    "sent": "So we implement.",
                    "label": 0
                },
                {
                    "sent": "This is not the server, but you can also use this because it has a public IP.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what you get.",
                    "label": 0
                },
                {
                    "sent": "So if you submit a particular query in this case my query was.",
                    "label": 0
                },
                {
                    "sent": "Sandy, the storm that it's US year ago I get a number of media.",
                    "label": 0
                },
                {
                    "sent": "Which are related to shared on different social networks which are related to this query.",
                    "label": 0
                },
                {
                    "sent": "So in this case I got this particular image published on Twitter and I get the permitting of the tweets of course, but the text into the tweet content press the media items, and potentially some social interactions and number of people of time.",
                    "label": 0
                },
                {
                    "sent": "This tweet has been retweeted commented.",
                    "label": 0
                },
                {
                    "sent": "Like favorite extractor.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is this is another example where here I'm submitting a query about Angela Merkel and I get more related content about her.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "And basically this is the idea that from you video sequence you would be able to link to much more multimedia content from those social networks.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it would probably almost conclude on this is Richmond's 'cause I have talked about annotations, even talking about enrichments, the idea that you link your video, we've over media content, annotations 2, so we model the enrichments as an annotation, but the only thing I would change is this.",
                    "label": 0
                },
                {
                    "sent": "The motivation of the annotations.",
                    "label": 0
                },
                {
                    "sent": "Remember, before it was tagging, now the motivation is linking.",
                    "label": 0
                },
                {
                    "sent": "As simple as this, and again you you must represent this explicitly as an annotation, but you just change the motivation of annotation.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I encourage you to watch this project still coming from Link TV where this is exactly all those ideas I have just presented into practice here.",
                    "label": 0
                },
                {
                    "sent": "You will be able to basically watch a news program and automatically gets proposed content.",
                    "label": 0
                },
                {
                    "sent": "Linked to the program, you're being much based on this name, entity detection and examination.",
                    "label": 0
                },
                {
                    "sent": "And that's it for me now.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, my takeaway summary.",
                    "label": 0
                },
                {
                    "sent": "Before putting their hand to burn up video is a first class citizen on the web.",
                    "label": 0
                },
                {
                    "sent": "I hope I convinced you that now this is the case.",
                    "label": 0
                },
                {
                    "sent": "We have models, ontologies and API for describing those resource and I pinpoint the open innovation model for that.",
                    "label": 0
                },
                {
                    "sent": "We have ways of addressing fine grain parts of multimedia content using media fragment to write the.",
                    "label": 0
                },
                {
                    "sent": "Net platform is a great tool that you can all use to extract named entities from the transcripts.",
                    "label": 0
                },
                {
                    "sent": "And what we really I think we would like to push you to do, because you're just starting doing research in this field is really embracing the link media vision.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that you all ran aggregates to analyze your media content while not thinking of exposing the results of those analysis to the larger community.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you don't see now the need or the reason why you should do this.",
                    "label": 0
                },
                {
                    "sent": "This is the network effects someone will take those results.",
                    "label": 0
                },
                {
                    "sent": "Would be able to compare this with his own results, or will perhaps do something clever that you couldn't imagine doing with those analysis results.",
                    "label": 0
                },
                {
                    "sent": "So just do this as soon as you.",
                    "label": 0
                },
                {
                    "sent": "Analyze multimedia content.",
                    "label": 0
                },
                {
                    "sent": "Try to use standard technologies to represent the output and to expose it an let the network effect play and someone will reuse it and do something nice with it.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of people to credit for those slides, but I'm heading over to Benoit now.",
                    "label": 0
                },
                {
                    "sent": "Will more deeply talk about medieval.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good afternoon.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to continue.",
                    "label": 0
                },
                {
                    "sent": "Today's winter school going deeper into what we can do with many of the things that we've been talking about since the morning.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to put that into a specific context.",
                    "label": 0
                },
                {
                    "sent": "The context of a benchmark which took place easier in media eval, which is called search and hyperlinking.",
                    "label": 0
                },
                {
                    "sent": "How many of you have heard of Mediaval and maybe the search and hyperlinking task?",
                    "label": 0
                },
                {
                    "sent": "OK, quite a few hands.",
                    "label": 0
                },
                {
                    "sent": "OK, very good.",
                    "label": 0
                },
                {
                    "sent": "So this work is work that we're doing both in media mixer and in the link TV consortium.",
                    "label": 0
                },
                {
                    "sent": "Most of the work here, well, a lot of the work, sorry here has been done by one of my PhD student material.",
                    "label": 0
                },
                {
                    "sent": "So gay, but a lot of other partners from Link TV I've been working on this not to name them serve has been working on it.",
                    "label": 0
                },
                {
                    "sent": "The University of Prag XYZ Economic University of Prague has been working on this as well as front over, so it's a multipartite effort from Link TV and Media Mixer Partners.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to put it into context with what we want to.",
                    "label": 0
                },
                {
                    "sent": "Talk about today about.",
                    "label": 0
                },
                {
                    "sent": "What media mixer is about?",
                    "label": 0
                },
                {
                    "sent": "We've seen a lot of things about Fragmente creation Fragmente description in the previous talks.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to address, right?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now is only a small part.",
                    "label": 0
                },
                {
                    "sent": "It's a search bot.",
                    "label": 0
                },
                {
                    "sent": "How are we going to search for information once it's been fragmented?",
                    "label": 0
                },
                {
                    "sent": "Once it's been described, OK.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And really, we thought the medieval search and hyperlinking task or challenge was really a good way to demonstrate the capabilities of what we were working on.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start with presenting this particular task and then describe how we went about basically performing the tasks, showing some example, then some results, and then I'll conclude.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rafael has already started to introduce a task where what really, what we have to do is to search for video into a big data set.",
                    "label": 0
                },
                {
                    "sent": "So the data set has been provided by the BBC and there are in fact two tasks that we are addressing.",
                    "label": 0
                },
                {
                    "sent": "One of the task is a search task where you are given a dis textual description.",
                    "label": 0
                },
                {
                    "sent": "Of what you are looking for and then you have to search for.",
                    "label": 0
                },
                {
                    "sent": "Media fragments that correspond to this description in addition to a textual description.",
                    "label": 0
                },
                {
                    "sent": "There is a separate description.",
                    "label": 0
                },
                {
                    "sent": "Again, textual which describe the visual content of what we are looking for, so it gives you extra information about what you are supposed to find, what you're searching for.",
                    "label": 0
                },
                {
                    "sent": "So this is one part of the challenge.",
                    "label": 0
                },
                {
                    "sent": "The other part of the challenge is hyperlinking.",
                    "label": 0
                },
                {
                    "sent": "Hyperlinking is a slightly different setting where instead of starting from a sexual query we are actually starting from a media fragment, and from this we are fragmente.",
                    "label": 0
                },
                {
                    "sent": "We're trying to find other media fragments, so we basically reverting back into.",
                    "label": 0
                },
                {
                    "sent": "What's Raphel was just mentioning all these enrichment.",
                    "label": 0
                },
                {
                    "sent": "How can you enrich a particular media with more media that are refering to similar things?",
                    "label": 0
                },
                {
                    "sent": "So those are the two tasks set by.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Medieval and this tasks have to be done in a very large data set.",
                    "label": 0
                },
                {
                    "sent": "We have 2323 videos.",
                    "label": 0
                },
                {
                    "sent": "They basically are broadcast from different draws from the BBC.",
                    "label": 0
                },
                {
                    "sent": "All in all, those all those videos are basically average out to 1.700 hours of video.",
                    "label": 0
                },
                {
                    "sent": "Media eval is also providing extra meter data or extra information about those video they are providing is of course the video and the audio information they are providing is two types of Asrs actually provided by other people, but they are given as part of the data you can play with.",
                    "label": 0
                },
                {
                    "sent": "They are providing us.",
                    "label": 0
                },
                {
                    "sent": "Manual subtitles I've been provided by the BBC meter data about the program, which channel was it broadcasted in?",
                    "label": 0
                },
                {
                    "sent": "Who are the people in this program?",
                    "label": 0
                },
                {
                    "sent": "And so on is provided.",
                    "label": 0
                },
                {
                    "sent": "They also provide shout, shout boundaries, so if you want to provide you already some sort of media fragment at a certain granularity and the corresponding key frames for all of these fragments, they also provided very late in the challenge.",
                    "label": 0
                },
                {
                    "sent": "Information about faces, detection and similarity information within the videos.",
                    "label": 0
                },
                {
                    "sent": "And concept detection also very late in the challenge.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is typically the two types of query we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "The first one at the top being the search and the one at the bottom being the hyperlinking.",
                    "label": 0
                },
                {
                    "sent": "So what we can see is that indeed there are two parts of the query.",
                    "label": 0
                },
                {
                    "sent": "You have a query text.",
                    "label": 0
                },
                {
                    "sent": "What does a bowl look like when it's the world during squash?",
                    "label": 0
                },
                {
                    "sent": "OK, this is a typical query.",
                    "label": 0
                },
                {
                    "sent": "This is one of the queries that we had to find videos for.",
                    "label": 0
                },
                {
                    "sent": "And it is.",
                    "label": 0
                },
                {
                    "sent": "It has an additional information.",
                    "label": 0
                },
                {
                    "sent": "The visual cue the video Q says bowl eating a role in slow motion.",
                    "label": 0
                },
                {
                    "sent": "OK, so not only you have to find a video which.",
                    "label": 0
                },
                {
                    "sent": "Looks which were bowl is seen eating a wall.",
                    "label": 0
                },
                {
                    "sent": "You also have to find a video which is doing that in slow motion.",
                    "label": 0
                },
                {
                    "sent": "So this is a task.",
                    "label": 0
                },
                {
                    "sent": "The search task VIP linking task is starting with less.",
                    "label": 0
                },
                {
                    "sent": "I would say semantic information.",
                    "label": 0
                },
                {
                    "sent": "At least from the level of the query, because what it gives you is just the start point and the end point within a video for which you have to find more relevant information.",
                    "label": 0
                },
                {
                    "sent": "Which means you have to somehow understand what's happening in this video before you can go and search for information.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all those queries have been user generated, which means that some people have been looking at the data set and then decided, oh, I would like people to search for this particular media fragment.",
                    "label": 0
                },
                {
                    "sent": "And here is a query that they will have to that I think would be a good query for answering this so they provide 50 queries from 29 different users for the search task and based on the search tasks actually started with.",
                    "label": 0
                },
                {
                    "sent": "Provided.",
                    "label": 0
                },
                {
                    "sent": "98 anchors for which we can do the hyperlink.",
                    "label": 0
                },
                {
                    "sent": "For evaluation.",
                    "label": 0
                },
                {
                    "sent": "The search task is provided already by the user for hyperlinking.",
                    "label": 0
                },
                {
                    "sent": "They perform crowdsourcing from the result provided by all the participants to find out which are the good hyperlinks and which are not so good.",
                    "label": 0
                },
                {
                    "sent": "I pairings.",
                    "label": 0
                },
                {
                    "sent": "And due to time and cost of such crowdsourcing, they could not do it on all the 98 anchors that could only do it on 30.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do they?",
                    "label": 0
                },
                {
                    "sent": "Evaluate whether one.",
                    "label": 0
                },
                {
                    "sent": "Persons are one groups.",
                    "label": 0
                },
                {
                    "sent": "Algorithm performs better than another one.",
                    "label": 0
                },
                {
                    "sent": "Well, there are actually devise a number of measures to identify that.",
                    "label": 0
                },
                {
                    "sent": "The simplest one is the mean reciprocal rank.",
                    "label": 0
                },
                {
                    "sent": "OK, it basically looks at the rank of the good.",
                    "label": 0
                },
                {
                    "sent": "Of the first good anchor or media fragment that is retrieved by system and it averages out this over all the 50 queries.",
                    "label": 0
                },
                {
                    "sent": "Of course this does not provide any information about whether you have how far you are from the right segment, OK?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they provided some extra measures.",
                    "label": 0
                },
                {
                    "sent": "One of them is the mean generalized average precision.",
                    "label": 0
                },
                {
                    "sent": "Which takes into account the starting point of the fragment that is going to be played OK.",
                    "label": 0
                },
                {
                    "sent": "Done, it is again extended with the mean average segment precision, which measures both the ranking and the segmentation position.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have both the ranking and the starting and end time of each of the fragments, which is taken into account for hyperlinking only precision and mean average Precision's have been used.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we address the problem?",
                    "label": 0
                },
                {
                    "sent": "Well, originally really this is a real research problem.",
                    "label": 0
                },
                {
                    "sent": "We don't have a real solution, So what we did is performed.",
                    "label": 0
                },
                {
                    "sent": "All the algorithm that we had in our hands on the data set and then to see how far we could go with providing results.",
                    "label": 0
                },
                {
                    "sent": "So we processed all.",
                    "label": 0
                },
                {
                    "sent": "16197 hours of video and here you have the running times for all of the different things, so we use the algorithm mentioned earlier for visual concept detection, unforeseen segmentation and it run for one of them.",
                    "label": 0
                },
                {
                    "sent": "Run for 20 days on 100 cores and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so we had also OCR from found over so the scene segmentation and video concept where from surf then we have OCR keyword extraction, name, entity extraction from Eureka Man you EP.",
                    "label": 0
                },
                {
                    "sent": "Face detection and tracking from year ago.",
                    "label": 0
                },
                {
                    "sent": "So all of this information is now becoming meta data that we can use for.",
                    "label": 0
                },
                {
                    "sent": "Indexing and searching into our collection.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how did we do this search and how did we index?",
                    "label": 0
                },
                {
                    "sent": "Well, very simply, we use Lucene solar to do that.",
                    "label": 0
                },
                {
                    "sent": "So this is Apache, so it's available for use.",
                    "label": 0
                },
                {
                    "sent": "Easily you can just download it and use it.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we indexed or data set.",
                    "label": 0
                },
                {
                    "sent": "So the whole of the video according to different principles according to different program Ularity because we wanted to identify the right big in an end for each of the media fragmente.",
                    "label": 0
                },
                {
                    "sent": "But also according to different types of features.",
                    "label": 0
                },
                {
                    "sent": "So we could compare weather.",
                    "label": 0
                },
                {
                    "sent": "For example visual Concepts would actually bring a lot of information to the search onto the hyperlinking.",
                    "label": 0
                },
                {
                    "sent": "And all this information was stored in a unified, structured way which was very handy and flexible for doing the search and the hyperlinking from one single index.",
                    "label": 0
                },
                {
                    "sent": "So how did we actually?",
                    "label": 0
                },
                {
                    "sent": "Encode the date.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, in solo what you have to do is you have to define a schema which basically gives you the structure of the document that you're going to be using.",
                    "label": 0
                },
                {
                    "sent": "So here we can see for example that we have a number of fields and for each of those fields you can define whether it has to be indexed, whether it is as it should not be indexed, whether you want to store it, whether it's multivalued, whether it is required and for all this and you can define your documents.",
                    "label": 0
                },
                {
                    "sent": "So for each of the documents that we had in the database.",
                    "label": 0
                },
                {
                    "sent": "Basically those documents are media fragment of different length.",
                    "label": 0
                },
                {
                    "sent": "Then we had a description which video does it come from?",
                    "label": 0
                },
                {
                    "sent": "When does it start?",
                    "label": 0
                },
                {
                    "sent": "When does it hand?",
                    "label": 0
                },
                {
                    "sent": "The subtitles attached to the transcripts attached to it and also.",
                    "label": 0
                },
                {
                    "sent": "The V.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool concepts that have been detected and with which.",
                    "label": 0
                },
                {
                    "sent": "Accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then Solar did its thing.",
                    "label": 0
                },
                {
                    "sent": "OK. Basically the good thing we saw is that it provides you different ways to index different different ways to solve the information.",
                    "label": 0
                },
                {
                    "sent": "We tried many.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to report all of this here, but basically all this is provided for you.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can perform the search.",
                    "label": 0
                },
                {
                    "sent": "So what we did is.",
                    "label": 0
                },
                {
                    "sent": "Search over two different methodologies.",
                    "label": 0
                },
                {
                    "sent": "But the big problem when doing such?",
                    "label": 0
                },
                {
                    "sent": "Is when you try to Add all the type of data that the search engine is not used to.",
                    "label": 0
                },
                {
                    "sent": "Like the visual data, the visual concepts for text is very.",
                    "label": 0
                },
                {
                    "sent": "It's very traditional and there's no real problem there.",
                    "label": 0
                },
                {
                    "sent": "So how do you include the visual feature into this?",
                    "label": 0
                },
                {
                    "sent": "Well, the starting point is the visual cues from the text.",
                    "label": 0
                },
                {
                    "sent": "So actually, even though you're talking about visual feature, you're starting from text information, so you have to somehow do a mapping between the textual description and the visual concepts that are being provided by the algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To show you how we did that, basically what we did is take the visual cues and.",
                    "label": 0
                },
                {
                    "sent": "Identify keyword that we could find in in Word net.",
                    "label": 0
                },
                {
                    "sent": "And based on those keywords and word net, find a mapping with all the visual concepts detectors that we had.",
                    "label": 0
                },
                {
                    "sent": "OK, so we end up with this sort of mapping warehouse.",
                    "label": 0
                },
                {
                    "sent": "Could be a church house, could be a school house, could be a building.",
                    "label": 0
                },
                {
                    "sent": "And so on for animals and for some of the concepts we have no mapping like exploration.",
                    "label": 0
                },
                {
                    "sent": "We do not have any visual concepts actually link to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an automatic way to go from the visual cues to the visual concept as the one described earlier by Vasilis.",
                    "label": 0
                },
                {
                    "sent": "And you can actually have some confidence information about how those mappings.",
                    "label": 0
                },
                {
                    "sent": "Valid or not directly based from the word net.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing that we need to look at is how good or visual detectors are OK and what we did is to basically look at the each for each of the concepts.",
                    "label": 0
                },
                {
                    "sent": "Visual visual concept detectors that we have.",
                    "label": 0
                },
                {
                    "sent": "Sorry we looked at how well it was doing on the 1st.",
                    "label": 0
                },
                {
                    "sent": "Highest ranked images or if you want the first highest rank.",
                    "label": 0
                },
                {
                    "sent": "Media fragments and out of those 100, we counted how many actually really depicted the visual concept, and we use that as a score to identify how good this detector is or not OK, so these are the two info.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations that we're using all this information is going to be used for indexing the whole of the data set based on two strategies, one based on media fragments which are basically the scene exactly the scenes adversaries was mentioning this morning and the other one is a more traditional approach which is basically merging consecutive shots according to how similar they are.",
                    "label": 0
                },
                {
                    "sent": "And instead of using the.",
                    "label": 0
                },
                {
                    "sent": "Visual information, which is what the scene algorithm presented by Vasilis is doing for the second one, what we used is similarity in the textual information within the transcript or within the subtitles.",
                    "label": 0
                },
                {
                    "sent": "Then we submitted a large number of runs and for each of those runs we gave a specific name like.",
                    "label": 0
                },
                {
                    "sent": "The first one seems no see or seen.",
                    "label": 0
                },
                {
                    "sent": "C means scene search using textual and or not using visual features.",
                    "label": 0
                },
                {
                    "sent": "Then we have scenes using only textual features from transcript, different types of transcript.",
                    "label": 0
                },
                {
                    "sent": "And then we have the different clustering.",
                    "label": 0
                },
                {
                    "sent": "For grouping the different shots.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Running this over the algorithm, running the algorithm over the whole of the data set, we found out that actually when we run the whole thing at the video level, not even looking lower at the media fragment, then we could already do a lot of prefiltering, meaning that at the high level video that is talking about something in general.",
                    "label": 0
                },
                {
                    "sent": "Actually it's more likely to contain the exact information that you're looking for, so you can do some prefiltering, which is pretty good, because that means we're going to be able to scale more easily if we.",
                    "label": 0
                },
                {
                    "sent": "If we can do such type of pre filtering so I'm not going into the details, you can find that in papers of how good the prefiltering can be, but still there is an issue is how do you when do you cut the prefiltering?",
                    "label": 0
                },
                {
                    "sent": "How many videos do you still have to consider?",
                    "label": 0
                },
                {
                    "sent": "So this is still an open research issue.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So doing the query is actually very simple.",
                    "label": 0
                },
                {
                    "sent": "You can either do it using the web interface, those of you sitting at the back, I gather that you probably can't see that, but here at the top I'm getting.",
                    "label": 0
                },
                {
                    "sent": "A query which basically just says what does the ball look like when it's the rule.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just type this in and then I press search and there it goes.",
                    "label": 0
                },
                {
                    "sent": "This is quite convenient for testing.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when you want to do that a number of times, you probably want to use the HTTP request.",
                    "label": 0
                },
                {
                    "sent": "Which is just the one that you can see there.",
                    "label": 0
                },
                {
                    "sent": "OK, so here it's searching for a chicken out of pre trip exploracion of poetry by schoolchildren from writing.",
                    "label": 0
                },
                {
                    "sent": "And it's trying to find the visual concept animal and visual concept building.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is for the search for the hyperlinking.",
                    "label": 0
                },
                {
                    "sent": "We basically what we did is we use.",
                    "label": 0
                },
                {
                    "sent": "The search component, but not instead of retrieving only one document, we were retrieving more than one document OK.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an example of.",
                    "label": 0
                },
                {
                    "sent": "Of what actually happened when we played with the system.",
                    "label": 0
                },
                {
                    "sent": "So here we have a text query.",
                    "label": 0
                },
                {
                    "sent": "So what does what to cook with?",
                    "label": 0
                },
                {
                    "sent": "Sorry what to cook with everyday ingredients on a budget then is run out and John Barrowman inspired Seabass as progress ostritch mushroom, sweet potato, mango tomatoes and we have a number of visual cues, so denizen out and should be seen.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you can see that we have a video and it's using the media fragment attached to it because this is what was supposed to be retrieved.",
                    "label": 0
                },
                {
                    "sent": "So this is what.",
                    "label": 0
                },
                {
                    "sent": "Was supposed to be retrieved.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is typically the video that should be sound.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a cooking video.",
                    "label": 0
                },
                {
                    "sent": "This is what the person was looking for.",
                    "label": 0
                },
                {
                    "sent": "This is what.",
                    "label": 0
                },
                {
                    "sent": "The person wanted us to find when.",
                    "label": 0
                },
                {
                    "sent": "Issuing the query that we have seen.",
                    "label": 0
                },
                {
                    "sent": "OK, so going back there.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to run the other videos, but just looking at the indices, the time indices we can see that what the user wanted us to find was going from second 67 all the way to 2nd 321 using scenes we were able to find the right video.",
                    "label": 0
                },
                {
                    "sent": "Going from second 49 to 323 and using clustering of shots.",
                    "label": 0
                },
                {
                    "sent": "We were getting the right video, but the wrong fragment.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just an example.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can look at some more.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Some other time, but in terms of overall, the results are showing that indeed.",
                    "label": 0
                },
                {
                    "sent": "This seems construction is providing more meaningful.",
                    "label": 0
                },
                {
                    "sent": "Access to the media according to what users are expected to find.",
                    "label": 0
                },
                {
                    "sent": "So things are giving the best performance.",
                    "label": 0
                },
                {
                    "sent": "The impact of transcript so you can see the one with South.",
                    "label": 0
                },
                {
                    "sent": "The one with I and the one with you.",
                    "label": 0
                },
                {
                    "sent": "They show different types of text that has been extracted either through subtitle.",
                    "label": 0
                },
                {
                    "sent": "This is SIN, your transcript and you can see that those are providing lower scores.",
                    "label": 0
                },
                {
                    "sent": "So indeed.",
                    "label": 0
                },
                {
                    "sent": "Transcript which are which are computed by through ASR are not as good as.",
                    "label": 0
                },
                {
                    "sent": "Direct subtitles.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, one last thing, the top two are showing using the visual concept and not showing visual concept and we can see that this color slightly better when we're using.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visual concepts so overall, out of all the participants of mediaval this year, we perform second in terms of end gap.",
                    "label": 0
                },
                {
                    "sent": "With a window of 50 of 60 seconds for the search task.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at the hyperlinking we could see two different approaches.",
                    "label": 0
                },
                {
                    "sent": "One is using directly the anchor.",
                    "label": 0
                },
                {
                    "sent": "Only the anchor the other else is using extra information.",
                    "label": 0
                },
                {
                    "sent": "Meta data coming from the video itself, but we can see that we can get very reasonable.",
                    "label": 0
                },
                {
                    "sent": "Results of 2.56 in terms of accuracy and precision.",
                    "label": 0
                },
                {
                    "sent": "10",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared with the others, the our approach are the first 2 bars.",
                    "label": 0
                },
                {
                    "sent": "At Precision 10.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is showing is this is showing us that when we are searching for information we are searching for information.",
                    "label": 0
                },
                {
                    "sent": "Not a whole video.",
                    "label": 0
                },
                {
                    "sent": "We are usually searching for very precise part of the video.",
                    "label": 0
                },
                {
                    "sent": "Videos I've been usually cut in shots is definitely not what people are expecting when they are searching for subparts of video.",
                    "label": 0
                },
                {
                    "sent": "We need to rearrange all this information according to the semantic content of the video.",
                    "label": 0
                },
                {
                    "sent": "And this is what can still be improved compared to what we're doing.",
                    "label": 0
                },
                {
                    "sent": "But we can see that clearly the shot is not what people are looking for.",
                    "label": 0
                },
                {
                    "sent": "Scenes are better, and for that media fragment offer the flexibility.",
                    "label": 0
                },
                {
                    "sent": "And this is a very big strength.",
                    "label": 0
                },
                {
                    "sent": "Visual feature can provide some gain.",
                    "label": 0
                },
                {
                    "sent": "We have to do some more work to identify how.",
                    "label": 0
                },
                {
                    "sent": "To improve this quality, how to use those concept detectors better within such frameworks and how to do the mapping between the textual queries and the visual.",
                    "label": 0
                },
                {
                    "sent": "So somehow it's a semantic gap the other way around.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to find more information, there are some related publications, including talk at the end of MSYS next week.",
                    "label": 0
                },
                {
                    "sent": "You also have links to the media eval campaign and media Mixer Link TV as well as the solar system.",
                    "label": 0
                },
                {
                    "sent": "And with this I close this talk.",
                    "label": 0
                }
            ]
        }
    }
}