{
    "id": "3fnhr3s3neflomvoue45rbp6mhr24qhw",
    "title": "Concentration inequalities in machine learning",
    "info": {
        "author": [
            "Gabor Lugosi, Department of Economics and Business, Pompeu Fabra University"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_lugosi_concentration_inequalities/",
    "segmentation": [
        [
            "I will try to introduce you to this this fascinating field of concentration inequality.",
            "This is probability theory, so if you don't like math, you can leave now.",
            "But I would.",
            "I will try to argue that this is useful stuff also for analyzing.",
            "Learning algorithms in machine learning.",
            "I think someone, especially a little bit theoretically oriented researchers, should be aware of this because it's really gives you a very rich set of tools that makes analysis really much easier.",
            "If you understand this.",
            "So what do I mean by concentrate?"
        ],
        [
            "And what's concentration?",
            "So in one sentence, what we're interested in is if we have something that depends on lots of.",
            "Random variables that they are independent.",
            "Then we would like to control it's random.",
            "The random fluctuations of this something.",
            "OK, so we have a function which is real valued function which depends on lots of lots of lots of independent random variables.",
            "We would like to know how close is this function to its expected value.",
            "So this is what this whole subject of concentration inequality is about.",
            "Of course we know that if we have this is a classical probability, tells us that if we have.",
            "An average of independent random variables.",
            "Then the loaf large numbers tells us that this average will be close to its expected value.",
            "This is just drove large numbers, but in.",
            "In many, many cases, more often than not, we don't have just averages, but we have something complicated, much more complicated function that still depends on lots of lots of independent random variables, so this course is about this.",
            "How can we deal with complicated functions of independent random variables?"
        ],
        [
            "So here's this, is the setup that I will consider we have an independent random variables.",
            "Independence is the keyword?",
            "OK, that's why it's pink because that's the we will assume that we have.",
            "All these variables are independent I. I won't necessarily assume that they are.",
            "They have the same distribution.",
            "Independence is enough, so that's one of the beauties of this theory that all we need is it made this independence.",
            "OK, so we have independent random variables that take values in some space.",
            "It doesn't matter.",
            "This could be a real valued, random variables, could be discreet, could take values in some big function space.",
            "This could be random functions, anything just independent.",
            "And now we have a function that takes these as an input, takes this N random variables.",
            "And produces a real number.",
            "OK, so as some would be a special case right if we have if we have real valued random variables, then the summer and average is a function.",
            "But now you can think about something much more complicated.",
            "And this of course is now random variable.",
            "It's a random variable that depends on all these.",
            "An independent guys.",
            "OK so I would like to understand is how far is this random variable from its expected value.",
            "And concentration inequalities will show us that under very very nice general conditions, we can guarantee that the difference is not big OK.",
            "If we have a big, complicated function of independent random variables, then it will behave.",
            "As expected, OK with high probability, low of large numbers just tells us that this is the case when F is a sum or or or on average.",
            "But what happens when we have something more general?",
            "OK, So what do I mean mean by we're trying to control the random fluctuations?",
            "Well, we want to get upper bounds for this type of probabilities, so what's the probability that the value of this random variable differs by its expected value by?",
            "TI by some some amount.",
            "So for some positive number we would like to say that this probability that probably is greater than the expected value plus T is small and also would like to say that the probability that Z is less than its expected values OK.",
            "Please not if you understand and please raise your hand if you don't.",
            "OK so OK, thank you.",
            "Alright."
        ],
        [
            "So lots of this has been a half topics and and a lot of advances came from different methods and I will try to mention.",
            "Basically I will concentrate on these two.",
            "Well this looks scary, but it won't be scary.",
            "Hopefully when when I tell you what it is so marketing goals were Martindale techniques were used were originally the the classical tools to deal with this type of problems.",
            "I will show you what the basic idea behind this.",
            "But somehow they are.",
            "They get stuck.",
            "These methods are don't lead, don't lead all the way where some other methods do.",
            "There was a great breakthrough in the mid 90s by the work of Michel Telekon he developed from nothing his own way of proving concentration, inequality's and but later people understood.",
            "These developed other techniques based on information theoretic tools.",
            "And these logarithmic Sobolev inequality's and and I will I like this one and then I will show you how this method works and then what kind of results we can get.",
            "OK, so there are a few references here, but this is really a big field now and it's this field is people have worked on this type of problems in real math people in high dimensional geometry for example also in discrete mathematics because concentration inequalities for example.",
            "One of some of these earlier references come from people working in random graphs because in the theory of random graphs you typically have this phenomenon.",
            "You have a big random graph, so that there's a lot of randomness independence, but then you get something which is a complicated function of those things, right?",
            "You have random graph and you want to look at what's the set of what's the size of the largest click, the largest fully connected subset.",
            "That's a typical function that we might be interested in.",
            "It's a complete, very complicated functions, lots of lots of independent random.",
            "Variables, so some of these references the early work came from there and still people in that field are still working on this.",
            "Some people work in information theory that where this came from, so there are lots of lots of fields in in in the theory of empirical processes, uniform convergence.",
            "Learning theorists have also added their fair share in the in the development.",
            "OK, so there's this even if you're not interested in machine learning, but I know you are, but if you weren't, this could.",
            "Also be interesting for."
        ],
        [
            "OK, so.",
            "Every baby knows Markov's inequality.",
            "OK, this is probably 1 one.",
            "If we have a non negative random variable then the probability that it's greater than its expected value plus T is bounded by the expected value divided by T. If you don't know this, try to prove it.",
            "This is an easy exercise.",
            "OK here seen it must be non negative.",
            "OK this is only true if so this is our first concentration inequality.",
            "It tells us that that if the expected value if we have a non negative thing, if the expected values.",
            "The value of the of the random variable cannot be much, much, much bigger than its expected value with the big probability.",
            "This is a very easy.",
            "There's no assumption here apart apart from.",
            "From nonnegativity, OK."
        ],
        [
            "Now still, every baby knows that markups inequality can be used in various ways.",
            "One of the ways is Chebyshev's inequality.",
            "So for example, if we want to bound that random variable differs from its expected value by more than P, then we can rewrite this event.",
            "This is the same that the square of this random variable is greater than the square of this quantity.",
            "OK, why is this good?",
            "Well, because because now if we use Markov's inequality.",
            "For this guy here.",
            "Then we have something which is the variance which we know what it is.",
            "Is a different inequality if we had just use Markov's inequality for this random variable OK then we would have got something like the expected value of this absolute value divided by T. Here we have the variance divided by T ^2.",
            "This."
        ],
        [
            "Is mark off?",
            "OK.",
            "So, so why do we like Chebyshev's inequality?",
            "Why is this nice form?",
            "Well, because because the variance is something that we can handle, especially if we talk about sums of independent random variable."
        ],
        [
            "OK, so this is where it comes from.",
            "This is why Chebyshev's inequality was born, because Chebyshev I think he was the first one who did this.",
            "He he was interested in sums of independent random variables.",
            "So if you have if Z is the sum of independent random variables, then the variance of the sum equals the sum of the variances and this is where independence comes in.",
            "And This is why the variance is a nice thing to work with because because we have this really nice relationship, the variance of the sum equals the sum of the variances.",
            "If we have independence.",
            "OK, so now we can just write down what happens, but.",
            "You can write down Chebyshev's inequality and what we get.",
            "These are two equivalent forms I just rescaled.",
            "It's like this.",
            "This is just the the loaf large numbers.",
            "This is called the so-called weak low flush numbers, which says that if these guys have finite variance then.",
            "Then then, then maybe here then.",
            "If there is something like N times epsilon, then this guy will go to zero.",
            "OK, so this is the week loaf large numbers and I rewrite it in this form because this form shows that this is kind of the right order of magnitude.",
            "But this inequality tells us is that the typical deviations of of the sum from its expected value of the order of square root of North or the typical deviations.",
            "Of the average from its expected value of the order of 1 / sqrt N. And this is the right order, this is."
        ],
        [
            "Abusive when he was young."
        ],
        [
            "The central Limit Theorem tells us that this is.",
            "This is the right kind of order that if you look at the probability that the this.",
            "The sum differs from its expected value by more than three times route, and then this thing converges to something with the central limit theorem is asymptotic, Chebyshev's inequality is not asymptotically OK, but still.",
            "If you look at Chebyshev's inequality."
        ],
        [
            "Then you see that this guy.",
            "Is small if these large.",
            "If these squared is larger than the variance then this will be small.",
            "The same."
        ],
        [
            "I think that the central Limit Theorem suggests if this squared is much larger than the variance, then this will be small.",
            "OK, but here we get something much nicer, much stronger.",
            "This decreases exponentially in the squared divided by the variance, so you can see that something is wrong.",
            "Something is not quite sharp in Chebyshev's inequality."
        ],
        [
            "The speed at which this decreases as T squared divided by the variance goes to 0 is much."
        ],
        [
            "Much lower than what we would expect if we looked at the central Limit Theorem, but this interesting limit theorem.",
            "We don't like it because it's asymptotic, and machine learners don't like synthetics OK?"
        ],
        [
            "So what can we do?",
            "Well, we will go back to Markov's inequality and we will try to use it in another clever way.",
            "Chubby Chef said that right.",
            "Instead of this square it and use Markov's inequality like that.",
            "But we can use any monotone function.",
            "So if Lambda is a positive number then I can rewrite this probability is the probability that Y to the Lambda times this random variable is greater than Y to the Lambda T. And now I get no.",
            "I can use Markov's inequality.",
            "These are non negative.",
            "This is a non negative random variable so I can use Markov's inequality and get something else.",
            "OK, so there's the big trick of what we call turn off bounds.",
            "It's the same trick Establishers bound.",
            "But now instead of the squares, we use the exponential function and now what do we need to do?",
            "We need to study.",
            "The moment generating function this this guy is called the moment generating function of the random variable C. So turn offs, method, method.",
            "Consists in doing this.",
            "We bound, we get an upper bound for the for the moment generating function, and then that will depend on Lambda.",
            "We get an upper bound which depends on Lambda and we choose Lambda to minimize that upper bound.",
            "OK, this is turn off stick technique.",
            "So why do we take the exponential function again?"
        ],
        [
            "The same reason is why we took the the squared, because if we have independence then this behaves nicely.",
            "Because the exponential function converts sums into products.",
            "And if we have independence, then the expected value of a product equals the product of the expected value.",
            "OK, this This is why turn off.",
            "Actually it was Bernstein."
        ],
        [
            "Who came up with this method?",
            "It was him who used this.",
            "I think rush in Russia.",
            "They still called it burst.",
            "Ensure enough method.",
            "OK, so so now.",
            "It suffices to look at the moment generating function of the individual terms.",
            "If we have a sum of independent random variables.",
            "So if we manage to get bounds for this guy, then we can go back to the previous slide."
        ],
        [
            "Plug it in here.",
            "And see what happens, OK?",
            "So far so good.",
            "Yeah.",
            "OK."
        ],
        [
            "So."
        ],
        [
            "Here's one of the basic cases.",
            "So if the axes are bounded and that's all we need, and I may just assume that they are bounded between zero and one.",
            "Otherwise you can just re scale.",
            "So if these guys are bounded then it's easy to bound the moment generating function of just one of them.",
            "It looks like this E to the minus Y to the Lambda squared divided by 8.",
            "This is nice.",
            "This is called a sub Gaussian behavior because.",
            "Let me just.",
            "Is this the best?",
            "OK, so if you have if you have a if X is a standard normal random variable then the then the moment generating function of this is Y to the.",
            "Lambda squared divided by two.",
            "OK, so.",
            "So whenever we have something a moment generating function that looks like this.",
            "It's bounded by the moment generating function of a Gaussian random variable.",
            "Well, I should say that if X is, let's say normal.",
            "Centered, normal and variance Sigma squared.",
            "Then I should put a Sigma squared here.",
            "Divided by two.",
            "OK so so these guys have a moment generating function which is less than the moment generating function of a normal with variance.",
            "1/4 OK, of course.",
            "The variance of all these guys is less than 1/4 cause because they are between zero and one.",
            "So this is somehow the right scale.",
            "OK so this is called Holdings inequality.",
            "It's a small analytical inequality.",
            "You choose this convexity of the of the exponential function.",
            "It's really not a huge deal, but this is."
        ],
        [
            "Extremely useful, this is helping here.",
            "So if you if we plug this back in the previous slide and you optimize, then this we get this wonderful inequality.",
            "So what this says is that if we have an average of independent random variables that are bounded between zero and one.",
            "Then the probability that it differs from its expected value by more than T that goes down like this.",
            "And this is this really now reminds us what we what we saw in the central Limit Theorem.",
            "OK, it has this nice exponential decrease in T ^2.",
            "Tells us that the typical deviations are of the order of one over square root of North, and if it's much larger than then it goes down exponentially.",
            "OK, so this is how things inequality.",
            "This is really, really nice and very very useful because now if for example, if you have Bernoulli random variables, that means that coin flips independent coin flips even if they don't have the same probability, the different outcomes don't have the same probability, But if they are independent then we have this nice inequality.",
            "This is completely distribution free."
        ],
        [
            "OK, have things inequalities, distribution for the."
        ],
        [
            "That means that the bounds we get doesn't depend on the distribution or we need this boundedness.",
            "OK, from a machine learning POV this is great because we don't know anything about.",
            "Our data.",
            "OK, free, unless you're a Bayesian, but but otherwise we don't know it, right?",
            "OK, sorry.",
            "So so all you see is that is that you have you have some independent trials.",
            "We don't.",
            "We want to infer what happens there, but we we can have distribution free inequalities like this."
        ],
        [
            "But sometimes we still want to do so.",
            "OK, so one more thing."
        ],
        [
            "So it's wonderful that this is a distribution free inequality, but also it has its disadvantages.",
            "So for example, if you if.",
            "If the size are, let's say one with probability.",
            "P and zero with probability 1 -- P, so these are.",
            "If they are Bernoulli random variables, then.",
            "Then then of course, the sum is a binomial random variable and then from the central limit theorem we would expect something like this.",
            "The probability that the sum.",
            "The average of XI minus the expected value, which is now P is greater than T. This should be something like E to the minus.",
            "In the squared divided by the variance right?",
            "Remember, in the central Limit theorem, let me go back a little bit."
        ],
        [
            "The variance shows up here.",
            "OK, and the variance is P * 1 -- P. So let's say P if P is small.",
            "Well, whatever P. 1 -- P. And I'm sure there's a 2 here.",
            "OK, so this is what we would want.",
            "The central Limit Theorem tells us, but everything's inequality is."
        ],
        [
            "Great if peak was 1/2 because that's exactly what we would like to see.",
            "But if P is very small then we lose big time.",
            "Right?",
            "If P is small then then this is much much, much, much smaller than either the minus 20 ^2.",
            "Exactly because of the reason, since have things inequalities distribution free, it has to be prepared for the worst case distribution.",
            "The worst case distribution turns out to be a binomial symmetric binomial when when we have a symmetric binomial, something like this with P = 1/2, then this is really what we expect from the central Limit Theorem.",
            "For all other cases.",
            "We lose OK.",
            "So."
        ],
        [
            "First time is inequality in which this was the the birth of Chernov method.",
            "This was basically the first the Bell gives us this variance information here.",
            "OK, it's, this looks so now.",
            "OK, let's look at this inequality.",
            "What it says is that now if we have independent random variables this can be negative.",
            "Here the only thing we assume that they are less than or equal to 1.",
            "And then we have this V thing which looks like variance.",
            "So if these guys had zero mean then this would be the variance.",
            "But this is just the the sum of the squares.",
            "Then we have this guy.",
            "OK, if you think about this case.",
            "Then then V is just N * P. Right?",
            "So then you see that we get.",
            "Something like this.",
            "Each of the minus N T ^2 / 2 P. But here, right?",
            "This would be.",
            "Proportional to P. This is this guy is P here.",
            "OK, but now we pay with this tea.",
            "Factor this is this the this team will only kick in when when P is very large.",
            "So V here is N * P. So so this guy will only become important when 20 is greater than N * P. OK, so this is like the very far tails in the large deviation regime.",
            "And actually you can prove that this is necessary.",
            "This corrective term is necessary.",
            "The central Limit theorem is not correct in that in that regime the central Limit Theorem tells us.",
            "It's asymptotic, but in a very specific."
        ],
        [
            "Way it says that if if I look at deviations of the order of square root of North and now take the limit, then this will converge to this.",
            "But if I want to look at deviations of the order of North, then the story changes.",
            "Then this guy doesn't tell me anything.",
            "It just says that that goes to 0, but it's useless.",
            "So this is one of the reasons."
        ],
        [
            "Why not asymptotically inequalities in my in my view are very very useful, because this is true no matter what, end is no matter what the distribution is OK?",
            "So one has to be very careful with asymptotically approximations, but it may not tell you the whole story.",
            "There are different regimes of asymptotics.",
            "What's more relevant, it's hard to say, but inequality like worsening inequality, is always there for you, OK?",
            "So far so good."
        ],
        [
            "OK.",
            "So let me show you one little application of.",
            "Of everything's inequality.",
            "So suppose now that we have N random variables and they don't have to be independent.",
            "This can be completely arbitrary, so there's no independence here.",
            "But what I need is that they are sub Gaussian.",
            "They have this kind of moment generating function.",
            "So everything's inequality tells us that.",
            "At the binomial, the binomial distribution is like this sub Gaussian.",
            "So suppose that we have N random variables of this type.",
            "And we want to we want to know.",
            "How large is the largest of these?",
            "Of course, in machine learning we are interested in this all the time, right?",
            "Because we have many trials, many things going on and we want to know.",
            "When for how?",
            "Let's say we have 100 classifiers classification algorithms that we want to try and which we choose the best at the end that looks best.",
            "But now there's a bias there because maybe the one that we chose that's best because for that particular data, it looked better if we fit fit, fit the data with that method better.",
            "But maybe in some other data it will be difficult.",
            "So we want to understand how, how far are.",
            "Random variables from their typical behavior.",
            "OK, when we have many, many trials, so I will show you a concrete application of this.",
            "So suppose we have sub Gaussian, so sub Gaussian somehow tell us that the probability that each of these is much larger than the expected value is is not large, right?",
            "That's what's the sub gaussianity gives us.",
            "Then the largest of these guys.",
            "Cannot be too large.",
            "It will be so one of them should be of the order of Sigma, right?",
            "But if we have many of them well.",
            "This is how much we pay.",
            "OK, so let's see the proof is 1."
        ],
        [
            "Find it's really nice.",
            "So how do we prove this inequality?",
            "So instead of just looking at the expected this expected value, we look at Y to the Lambda times the expected value.",
            "OK now Y to the Lambda times the expected value E to the Lambda Lambda is positive here.",
            "So so this is it with Lambda X is a convex function.",
            "So I can bring out the expected value and I only increase this is Jensen's inequality OK if we have?",
            "If I have a convex function then the value of the function at the expected value is less, right?",
            "You know this so if you have if F is a convex function then F. At the expected value of any random variable is less than or equal to the expected value of ethics, right?",
            "This is called Johnson's.",
            "Inequality.",
            "This is basically the definition of convexity if you wish.",
            "So here we use Jensen's inequality and now.",
            "The maximum here we are.",
            "The maximum is.",
            "The maximum comes outside because each of the Lambda that's an increasing function, so I can put the maximum here.",
            "Now I have the maximum of non negative random variables that's for sure less than the sum.",
            "Of these random variables, so I instead of the maximum I upper bounded by a son.",
            "But if I have the sum then then then then the sum because of the linearity of the expectation, the sum comes outside and what's inside is just the moment generating function of this Lambda of this, why eyes and now I can use the assumption?",
            "Now we have an inequality here so I can take the log of both sides divided by Lambda and then.",
            "And then we get what do we get?",
            "We get the expected value of the maximum of the wise.",
            "Is less than or equal to?",
            "So we have the log of N divided by Lambda plus.",
            "Lambda Times Sigma squared, divided by two.",
            "Here we have two terms.",
            "One of them increases with Lambda, the other decreases with Lambda.",
            "So you can optimize.",
            "We can differentiate with respect to Lambda, you choose Lambda.",
            "Optimally this is true for all lambdas, right?",
            "So I can choose Lambda, do a like and if you do then this is what you get OK. Is this a good inequality?",
            "This is very good, because in fact if you take independent Gaussian random variables independent and normal standard normal standard normals means that the expected value of itadi Lambda times y = y to the Lambda squared divided by two.",
            "So if you have standard normal random variables and independent then.",
            "You can prove that this is asymptotically the correct answer.",
            "OK, so this is not in provable.",
            "But if we have the nice thing in this inequality is that here we don't need gaussianity, just sub gaussianity, and we don't need independence."
        ],
        [
            "Alright.",
            "So how do we apply this?",
            "We can apply this as a classical.",
            "Classical question in learning theory, uniform convergence.",
            "So we have.",
            "We have some SpaceX, that's our space of observations and we have any independent random variables that take values in that space in that SpaceX OK. Now, in this SpaceX we have sets capital and different sets and we're measuring.",
            "We are measuring the empirically.",
            "The probability of each of these sets.",
            "It's like when you have N classifiers then.",
            "This could be maybe the error rate of each one of those classifiers and this is the true.",
            "The true error which we don't see.",
            "OK, the large numbers tells us that the end of a will be close to P of a OK.",
            "But we have now many of these, so we want to make sure that that we have uniform convergence.",
            "These two guys are close not only individually for each a, but even the largest of the differences is small.",
            "OK, this is absolutely crucial for evaluating machine learning algorithms.",
            "I want to make sure that the difference between these two.",
            "A small overall, so I want to bound the.",
            "I want to say something about the maximum deviation between empirical probabilities and true probabilities.",
            "I know that individually for if you give me a set for any fixed set.",
            "If N is large enough then.",
            "This difference will be small, there's just a lot of large numbers.",
            "But was the largest difference?",
            "Clearly, if I have lots of lots of lots of sets, then I can always find the set which exactly fits the data and then I get something complete nonsense.",
            "OK, now this little inequality tells us that if I don't take too many of them.",
            "And I can really take a lot, because because the dependence on the number of them is cap is logarithmic, then we're fine.",
            "OK, so how do we prove this well?",
            "We just have to look at the moment generating function of these right?",
            "I will.",
            "I will use the previous inequality."
        ],
        [
            "This inequality tells us that if these guys are sub Gaussian then.",
            "Then I have a bound, so I will use."
        ],
        [
            "This bound here and all we need to prove is that.",
            "These differences are sub Gaussian.",
            "We don't care about independence, these are not independent, very very dependent random variables, But this previous."
        ],
        [
            "Inequality doesn't require independence."
        ],
        [
            "The condition of the previous inequality."
        ],
        [
            "This was just about the moment generating function of the individual guys."
        ],
        [
            "OK, so let's look at the moment generating function of an individual guy E to the Lambda times this difference.",
            "Well, I just write it out.",
            "What this means PN is just the sum so that's what we get now.",
            "These are now independent.",
            "Here this is a sum of independent random variables and as I already said, the exponential function.",
            "Converge the sum into a product and then because of independence the product goes outside OK and now here we have the moment generating function of a random variable that takes values between.",
            "What is this?",
            "Well, this guy between zero and one.",
            "Right, and this is just it's expected value so I can use her things inequality.",
            "This is exactly what everything is, inequality tells us."
        ],
        [
            "If I have a random variable which is between zero and one, then the moment generating function is bounded by Y to the Lambda squared divided by 8.",
            "Now instead of four lamb."
        ],
        [
            "Over here we use it for Lambda divided by N, so that's where this end comes in OK. Good, so now I can plug it in the in our previous I can plug this bound.",
            "You know previous inequality, right?"
        ],
        [
            "In this inequality, which says that if the.",
            "If the moment generating function is less than Y to the Lambda squared times Sigma Square."
        ],
        [
            "Divided by two now is Sigma squared is now one over N 1 / 4 in here.",
            "OK so this is how we get it.",
            "So the largest deviation is bounded by the square root of log number of.",
            "Events that we want to control together.",
            "Divided by the sample size.",
            "So this is nice.",
            "This means that if we have a large sample then we can we can.",
            "We can take capital and to be really large, almost exponentially large in the sample size, which is good OK?"
        ],
        [
            "Alright."
        ],
        [
            "OK, so.",
            "So this is everything that I said so far was about sums of independent random variables.",
            "But as I said in the in the introduction.",
            "We want to we want to handle, not own."
        ],
        [
            "The sums but arbitrary functions of independent random variables.",
            "OK, so now we have.",
            "We're back to the slide number one.",
            "We have independent random variables and we have some arbitrary real valued function of these independent guys, and we want to say something about how close this is to its expected value.",
            "OK, now the big trick is a so called marketing representation.",
            "If you don't know what the martingale is, don't worry.",
            "We will not need it, but this is.",
            "This is the official name of this type of things so.",
            "I will introduce this operator, which is the expected value the conditional expectation given the first I random variables.",
            "So there's some kind of order of the random variables here.",
            "It doesn't matter because because of independence, so we just fix an order and we take the 1st.",
            "I have them fix their value and then calculate the expected value with respect to the distribution of the rest of them.",
            "This is what the conditional expectation means.",
            "We fix these guys and take the expected value.",
            "With respect to the distribution of the XI plus one to XN, OK, so if equals zero, then it means that I'm just taking the expected value with respect to all of them, so there's the ordinary expected value.",
            "And if I fix all of them, all of the random variables.",
            "Then the expected value.",
            "If I fix all the all the random variables, then the condition expectation is just the random variable itself.",
            "OK, so these are the two extremes.",
            "If I take this conditional expectations, then I go from the expected value of the random variable to random variable itself."
        ],
        [
            "Now this then we can naturally.",
            "Introduced these differences?",
            "So what's the difference between this guy and this guy?",
            "Well, I just take here.",
            "I just take one more conditioning variable.",
            "OK, here I condition on the first time I minus one.",
            "I fixed those values and take the expected value with respect to the rest and here I condition on one more and I look at what's the difference between these two expected values.",
            "Now the sum of these guys is a telescoping sum.",
            "And what we get is.",
            "Isabelle of Z -- Y sub 0Z right?",
            "Which is y -- Y of Z. OK, so if this is a nice way of writing down.",
            "The difference between the random variable and its expected value, it's just the sum of.",
            "These are so-called martindale differences.",
            "OK, so this is the sum of marketing your differences that we can write down.",
            "We can do this with any random variable in the world, and this is called the Duke marketing representation."
        ],
        [
            "Named after this guy.",
            "OK.",
            "So this is the dude Martindale representation of an arbitrary random variable which is a function of independent random variables.",
            "OK, alright, So what is this good for?",
            "Well, the nice thing in this representation is that even though now this is not a sum of independent random variables anymore, but instead of independence we at least get orthogonality.",
            "OK, So what do I mean by this?",
            "Turns out and this is very easy to see I will show it in the next slide that even though these random variables are not independent, they are not correlated, they are uncorrelated random variables and this is called a martingale."
        ],
        [
            "So here, here's for example.",
            "One nice consequence of of."
        ],
        [
            "Of writing random variable writing the fluctuation.",
            "The random fluctuation of a random variable difference between the random variable and its expected value as a sum of these martingales difference."
        ],
        [
            "Guys.",
            "OK, so if I want to bound for example the variance, the variance is a is a good quantity.",
            "Remember that Chebyshev's inequality tells us that if the variance variance is small, then then the random variable is concentrated.",
            "So this is good to keep in mind that the probability.",
            "The XZ minus the expected value of Z is greater than T. This is less than the variance of Z / T ^2.",
            "So that's somehow kind of a first order approach.",
            "We saw that this doesn't always give us sharp bounds.",
            "For example, when we dealt with sums of independent random variables that are bounded, this was not necessary.",
            "The optimal, but many times you can't beat it.",
            "OK, so let's just start with this inequality 1st and let's try to understand the variance.",
            "Good, so let's write the variance of an arbitrary random variable.",
            "Because of the martingale representation, I can write it like this.",
            "And now I can expand the square so there are two kinds of terms here.",
            "There are the Delta I squares, expected value of the Delta I squares, and there are the cross terms OK. Yeah, so far so good.",
            "Now the nice thing is that.",
            "This is zero.",
            "This term here is 0 and that follows from the way we define this.",
            "Lamb dies OK. Why is this true?",
            "So let me let me maybe explain it here.",
            "Maybe this one?",
            "OK.",
            "So what's what's the expected value?",
            "There's an index I there, so that means that I will take the expected value when I fix.",
            "The first I random variables and what's Delta I.",
            "So OK, one thing.",
            "Delta I hear.",
            "If."
        ],
        [
            "If we look at the definition here, we fixed the first I random variables and here is the first I -- 1 random variables.",
            "So if I take the."
        ],
        [
            "Expected value with respect by fixing the last I random variables, then Delta is a constant.",
            "OK, if I fix the first I random variables then this guy is a constant."
        ],
        [
            "This doesn't depend on the rest of the random variables, because we integrate it out with respect to their distribution.",
            "OK, so."
        ],
        [
            "It's a constant so so I can take it outside this Delta I comes outside.",
            "I only consider now indices when J is greater than I right?",
            "Because that's how I wrote it.",
            "I put the two here so that I can only consider this so Delta I comes outside.",
            "And now here I have the expected value with respect to.",
            "The expected value of this guy and here inside I have something like Z.",
            "The expected value of Z given X1.",
            "XJ I have this term and minus I have another term in which insight here I have X -- 1.",
            "XJ minus one.",
            "The expected value of this given X1XI.",
            "OK, but James greater than I so that means that J minus one is greater than or equal to Y and that means that if I fix this and take the expected value of this guy, then simply just using the.",
            "What is this called the?",
            "The theorem of total expectation.",
            "Just a.",
            "Just using this formula, this guy up here is.",
            "The expected value of Z given X1.",
            "Thanks, I and this guy is the same.",
            "Excellent.",
            "XI.",
            "So that's all we need to know about conditional expectations to prove this.",
            "OK, that this cross terms go out, that's why that's why I said that these are orthogonal.",
            "These are uncorrelated random variables.",
            "The expected value of the product equals 0, so that means that the variance can be written something like the sum of the variances.",
            "This is exactly what we saw when at the beginning when we talked about Chebyshev's inequality, right?",
            "That was the nice thing about about the variance that."
        ],
        [
            "It's here the variance of a sum equals the sum of the variances.",
            "This was true for independence.",
            "Now here we have at least formally something that looks similar."
        ],
        [
            "OK, all we need to do now is to understand these guys.",
            "OK, understand these these Delta eyes.",
            "Now."
        ],
        [
            "What are these?",
            "Delta is just intuitively?",
            "We look at a random variable.",
            "You look at the random variable.",
            "F which depends on lots of things XN and now I look at.",
            "This the expected value of this given X1XI, so I fixed the first high values and look at the expected value of this.",
            "And now I compare this.",
            "With what I would get if I had only fixed one less variable X 1X N given X1 X I -- 1.",
            "From this formulation you can see that this guy here.",
            "The only difference between between these two is that I fixed here one more variable.",
            "And here I integrate with respect to that variable.",
            "So somehow this intuitively it should be clear that if this function doesn't depend too heavily on any of the variables, then we should be in business.",
            "Right, if this function is stable in the sense that it doesn't.",
            "If I if I change one variable but leave the others fixed, then the value of the function doesn't change then, then this should be small.",
            "This should be controllable.",
            "So, and this is somehow the principle that we will explore that if I have a function of many, many, many random variables such that.",
            "It doesn't depend very strongly on any of the.",
            "Components, any of the variables, then it will be concentrated OK and this is the first instance."
        ],
        [
            "Offer of such a such an inequality OK?",
            "Should we take 5 minutes?",
            "OK, so when we take 5 minutes then after after I will show you a very very useful inequality that follows from this in an easy way."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will try to introduce you to this this fascinating field of concentration inequality.",
                    "label": 0
                },
                {
                    "sent": "This is probability theory, so if you don't like math, you can leave now.",
                    "label": 0
                },
                {
                    "sent": "But I would.",
                    "label": 0
                },
                {
                    "sent": "I will try to argue that this is useful stuff also for analyzing.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithms in machine learning.",
                    "label": 0
                },
                {
                    "sent": "I think someone, especially a little bit theoretically oriented researchers, should be aware of this because it's really gives you a very rich set of tools that makes analysis really much easier.",
                    "label": 0
                },
                {
                    "sent": "If you understand this.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by concentrate?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what's concentration?",
                    "label": 0
                },
                {
                    "sent": "So in one sentence, what we're interested in is if we have something that depends on lots of.",
                    "label": 0
                },
                {
                    "sent": "Random variables that they are independent.",
                    "label": 0
                },
                {
                    "sent": "Then we would like to control it's random.",
                    "label": 0
                },
                {
                    "sent": "The random fluctuations of this something.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have a function which is real valued function which depends on lots of lots of lots of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "We would like to know how close is this function to its expected value.",
                    "label": 0
                },
                {
                    "sent": "So this is what this whole subject of concentration inequality is about.",
                    "label": 0
                },
                {
                    "sent": "Of course we know that if we have this is a classical probability, tells us that if we have.",
                    "label": 0
                },
                {
                    "sent": "An average of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "Then the loaf large numbers tells us that this average will be close to its expected value.",
                    "label": 0
                },
                {
                    "sent": "This is just drove large numbers, but in.",
                    "label": 0
                },
                {
                    "sent": "In many, many cases, more often than not, we don't have just averages, but we have something complicated, much more complicated function that still depends on lots of lots of independent random variables, so this course is about this.",
                    "label": 0
                },
                {
                    "sent": "How can we deal with complicated functions of independent random variables?",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's this, is the setup that I will consider we have an independent random variables.",
                    "label": 0
                },
                {
                    "sent": "Independence is the keyword?",
                    "label": 0
                },
                {
                    "sent": "OK, that's why it's pink because that's the we will assume that we have.",
                    "label": 0
                },
                {
                    "sent": "All these variables are independent I. I won't necessarily assume that they are.",
                    "label": 1
                },
                {
                    "sent": "They have the same distribution.",
                    "label": 0
                },
                {
                    "sent": "Independence is enough, so that's one of the beauties of this theory that all we need is it made this independence.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have independent random variables that take values in some space.",
                    "label": 1
                },
                {
                    "sent": "It doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "This could be a real valued, random variables, could be discreet, could take values in some big function space.",
                    "label": 0
                },
                {
                    "sent": "This could be random functions, anything just independent.",
                    "label": 0
                },
                {
                    "sent": "And now we have a function that takes these as an input, takes this N random variables.",
                    "label": 0
                },
                {
                    "sent": "And produces a real number.",
                    "label": 0
                },
                {
                    "sent": "OK, so as some would be a special case right if we have if we have real valued random variables, then the summer and average is a function.",
                    "label": 0
                },
                {
                    "sent": "But now you can think about something much more complicated.",
                    "label": 0
                },
                {
                    "sent": "And this of course is now random variable.",
                    "label": 0
                },
                {
                    "sent": "It's a random variable that depends on all these.",
                    "label": 0
                },
                {
                    "sent": "An independent guys.",
                    "label": 0
                },
                {
                    "sent": "OK so I would like to understand is how far is this random variable from its expected value.",
                    "label": 0
                },
                {
                    "sent": "And concentration inequalities will show us that under very very nice general conditions, we can guarantee that the difference is not big OK.",
                    "label": 1
                },
                {
                    "sent": "If we have a big, complicated function of independent random variables, then it will behave.",
                    "label": 0
                },
                {
                    "sent": "As expected, OK with high probability, low of large numbers just tells us that this is the case when F is a sum or or or on average.",
                    "label": 1
                },
                {
                    "sent": "But what happens when we have something more general?",
                    "label": 0
                },
                {
                    "sent": "OK, So what do I mean mean by we're trying to control the random fluctuations?",
                    "label": 0
                },
                {
                    "sent": "Well, we want to get upper bounds for this type of probabilities, so what's the probability that the value of this random variable differs by its expected value by?",
                    "label": 0
                },
                {
                    "sent": "TI by some some amount.",
                    "label": 0
                },
                {
                    "sent": "So for some positive number we would like to say that this probability that probably is greater than the expected value plus T is small and also would like to say that the probability that Z is less than its expected values OK.",
                    "label": 0
                },
                {
                    "sent": "Please not if you understand and please raise your hand if you don't.",
                    "label": 0
                },
                {
                    "sent": "OK so OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So lots of this has been a half topics and and a lot of advances came from different methods and I will try to mention.",
                    "label": 0
                },
                {
                    "sent": "Basically I will concentrate on these two.",
                    "label": 0
                },
                {
                    "sent": "Well this looks scary, but it won't be scary.",
                    "label": 0
                },
                {
                    "sent": "Hopefully when when I tell you what it is so marketing goals were Martindale techniques were used were originally the the classical tools to deal with this type of problems.",
                    "label": 0
                },
                {
                    "sent": "I will show you what the basic idea behind this.",
                    "label": 0
                },
                {
                    "sent": "But somehow they are.",
                    "label": 0
                },
                {
                    "sent": "They get stuck.",
                    "label": 0
                },
                {
                    "sent": "These methods are don't lead, don't lead all the way where some other methods do.",
                    "label": 0
                },
                {
                    "sent": "There was a great breakthrough in the mid 90s by the work of Michel Telekon he developed from nothing his own way of proving concentration, inequality's and but later people understood.",
                    "label": 0
                },
                {
                    "sent": "These developed other techniques based on information theoretic tools.",
                    "label": 0
                },
                {
                    "sent": "And these logarithmic Sobolev inequality's and and I will I like this one and then I will show you how this method works and then what kind of results we can get.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are a few references here, but this is really a big field now and it's this field is people have worked on this type of problems in real math people in high dimensional geometry for example also in discrete mathematics because concentration inequalities for example.",
                    "label": 0
                },
                {
                    "sent": "One of some of these earlier references come from people working in random graphs because in the theory of random graphs you typically have this phenomenon.",
                    "label": 0
                },
                {
                    "sent": "You have a big random graph, so that there's a lot of randomness independence, but then you get something which is a complicated function of those things, right?",
                    "label": 0
                },
                {
                    "sent": "You have random graph and you want to look at what's the set of what's the size of the largest click, the largest fully connected subset.",
                    "label": 0
                },
                {
                    "sent": "That's a typical function that we might be interested in.",
                    "label": 0
                },
                {
                    "sent": "It's a complete, very complicated functions, lots of lots of independent random.",
                    "label": 0
                },
                {
                    "sent": "Variables, so some of these references the early work came from there and still people in that field are still working on this.",
                    "label": 0
                },
                {
                    "sent": "Some people work in information theory that where this came from, so there are lots of lots of fields in in in the theory of empirical processes, uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "Learning theorists have also added their fair share in the in the development.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's this even if you're not interested in machine learning, but I know you are, but if you weren't, this could.",
                    "label": 0
                },
                {
                    "sent": "Also be interesting for.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Every baby knows Markov's inequality.",
                    "label": 1
                },
                {
                    "sent": "OK, this is probably 1 one.",
                    "label": 0
                },
                {
                    "sent": "If we have a non negative random variable then the probability that it's greater than its expected value plus T is bounded by the expected value divided by T. If you don't know this, try to prove it.",
                    "label": 0
                },
                {
                    "sent": "This is an easy exercise.",
                    "label": 0
                },
                {
                    "sent": "OK here seen it must be non negative.",
                    "label": 0
                },
                {
                    "sent": "OK this is only true if so this is our first concentration inequality.",
                    "label": 0
                },
                {
                    "sent": "It tells us that that if the expected value if we have a non negative thing, if the expected values.",
                    "label": 0
                },
                {
                    "sent": "The value of the of the random variable cannot be much, much, much bigger than its expected value with the big probability.",
                    "label": 0
                },
                {
                    "sent": "This is a very easy.",
                    "label": 0
                },
                {
                    "sent": "There's no assumption here apart apart from.",
                    "label": 0
                },
                {
                    "sent": "From nonnegativity, OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now still, every baby knows that markups inequality can be used in various ways.",
                    "label": 0
                },
                {
                    "sent": "One of the ways is Chebyshev's inequality.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we want to bound that random variable differs from its expected value by more than P, then we can rewrite this event.",
                    "label": 0
                },
                {
                    "sent": "This is the same that the square of this random variable is greater than the square of this quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, why is this good?",
                    "label": 0
                },
                {
                    "sent": "Well, because because now if we use Markov's inequality.",
                    "label": 0
                },
                {
                    "sent": "For this guy here.",
                    "label": 0
                },
                {
                    "sent": "Then we have something which is the variance which we know what it is.",
                    "label": 0
                },
                {
                    "sent": "Is a different inequality if we had just use Markov's inequality for this random variable OK then we would have got something like the expected value of this absolute value divided by T. Here we have the variance divided by T ^2.",
                    "label": 1
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is mark off?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, so why do we like Chebyshev's inequality?",
                    "label": 0
                },
                {
                    "sent": "Why is this nice form?",
                    "label": 0
                },
                {
                    "sent": "Well, because because the variance is something that we can handle, especially if we talk about sums of independent random variable.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is where it comes from.",
                    "label": 0
                },
                {
                    "sent": "This is why Chebyshev's inequality was born, because Chebyshev I think he was the first one who did this.",
                    "label": 0
                },
                {
                    "sent": "He he was interested in sums of independent random variables.",
                    "label": 1
                },
                {
                    "sent": "So if you have if Z is the sum of independent random variables, then the variance of the sum equals the sum of the variances and this is where independence comes in.",
                    "label": 0
                },
                {
                    "sent": "And This is why the variance is a nice thing to work with because because we have this really nice relationship, the variance of the sum equals the sum of the variances.",
                    "label": 0
                },
                {
                    "sent": "If we have independence.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we can just write down what happens, but.",
                    "label": 0
                },
                {
                    "sent": "You can write down Chebyshev's inequality and what we get.",
                    "label": 0
                },
                {
                    "sent": "These are two equivalent forms I just rescaled.",
                    "label": 0
                },
                {
                    "sent": "It's like this.",
                    "label": 0
                },
                {
                    "sent": "This is just the the loaf large numbers.",
                    "label": 0
                },
                {
                    "sent": "This is called the so-called weak low flush numbers, which says that if these guys have finite variance then.",
                    "label": 0
                },
                {
                    "sent": "Then then, then maybe here then.",
                    "label": 0
                },
                {
                    "sent": "If there is something like N times epsilon, then this guy will go to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the week loaf large numbers and I rewrite it in this form because this form shows that this is kind of the right order of magnitude.",
                    "label": 1
                },
                {
                    "sent": "But this inequality tells us is that the typical deviations of of the sum from its expected value of the order of square root of North or the typical deviations.",
                    "label": 0
                },
                {
                    "sent": "Of the average from its expected value of the order of 1 / sqrt N. And this is the right order, this is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abusive when he was young.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The central Limit Theorem tells us that this is.",
                    "label": 1
                },
                {
                    "sent": "This is the right kind of order that if you look at the probability that the this.",
                    "label": 0
                },
                {
                    "sent": "The sum differs from its expected value by more than three times route, and then this thing converges to something with the central limit theorem is asymptotic, Chebyshev's inequality is not asymptotically OK, but still.",
                    "label": 0
                },
                {
                    "sent": "If you look at Chebyshev's inequality.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you see that this guy.",
                    "label": 0
                },
                {
                    "sent": "Is small if these large.",
                    "label": 0
                },
                {
                    "sent": "If these squared is larger than the variance then this will be small.",
                    "label": 0
                },
                {
                    "sent": "The same.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think that the central Limit Theorem suggests if this squared is much larger than the variance, then this will be small.",
                    "label": 1
                },
                {
                    "sent": "OK, but here we get something much nicer, much stronger.",
                    "label": 0
                },
                {
                    "sent": "This decreases exponentially in the squared divided by the variance, so you can see that something is wrong.",
                    "label": 0
                },
                {
                    "sent": "Something is not quite sharp in Chebyshev's inequality.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The speed at which this decreases as T squared divided by the variance goes to 0 is much.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much lower than what we would expect if we looked at the central Limit Theorem, but this interesting limit theorem.",
                    "label": 0
                },
                {
                    "sent": "We don't like it because it's asymptotic, and machine learners don't like synthetics OK?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we will go back to Markov's inequality and we will try to use it in another clever way.",
                    "label": 0
                },
                {
                    "sent": "Chubby Chef said that right.",
                    "label": 0
                },
                {
                    "sent": "Instead of this square it and use Markov's inequality like that.",
                    "label": 0
                },
                {
                    "sent": "But we can use any monotone function.",
                    "label": 0
                },
                {
                    "sent": "So if Lambda is a positive number then I can rewrite this probability is the probability that Y to the Lambda times this random variable is greater than Y to the Lambda T. And now I get no.",
                    "label": 0
                },
                {
                    "sent": "I can use Markov's inequality.",
                    "label": 1
                },
                {
                    "sent": "These are non negative.",
                    "label": 0
                },
                {
                    "sent": "This is a non negative random variable so I can use Markov's inequality and get something else.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's the big trick of what we call turn off bounds.",
                    "label": 0
                },
                {
                    "sent": "It's the same trick Establishers bound.",
                    "label": 0
                },
                {
                    "sent": "But now instead of the squares, we use the exponential function and now what do we need to do?",
                    "label": 0
                },
                {
                    "sent": "We need to study.",
                    "label": 0
                },
                {
                    "sent": "The moment generating function this this guy is called the moment generating function of the random variable C. So turn offs, method, method.",
                    "label": 0
                },
                {
                    "sent": "Consists in doing this.",
                    "label": 0
                },
                {
                    "sent": "We bound, we get an upper bound for the for the moment generating function, and then that will depend on Lambda.",
                    "label": 1
                },
                {
                    "sent": "We get an upper bound which depends on Lambda and we choose Lambda to minimize that upper bound.",
                    "label": 0
                },
                {
                    "sent": "OK, this is turn off stick technique.",
                    "label": 0
                },
                {
                    "sent": "So why do we take the exponential function again?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same reason is why we took the the squared, because if we have independence then this behaves nicely.",
                    "label": 0
                },
                {
                    "sent": "Because the exponential function converts sums into products.",
                    "label": 0
                },
                {
                    "sent": "And if we have independence, then the expected value of a product equals the product of the expected value.",
                    "label": 0
                },
                {
                    "sent": "OK, this This is why turn off.",
                    "label": 0
                },
                {
                    "sent": "Actually it was Bernstein.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who came up with this method?",
                    "label": 0
                },
                {
                    "sent": "It was him who used this.",
                    "label": 0
                },
                {
                    "sent": "I think rush in Russia.",
                    "label": 0
                },
                {
                    "sent": "They still called it burst.",
                    "label": 0
                },
                {
                    "sent": "Ensure enough method.",
                    "label": 0
                },
                {
                    "sent": "OK, so so now.",
                    "label": 0
                },
                {
                    "sent": "It suffices to look at the moment generating function of the individual terms.",
                    "label": 1
                },
                {
                    "sent": "If we have a sum of independent random variables.",
                    "label": 1
                },
                {
                    "sent": "So if we manage to get bounds for this guy, then we can go back to the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plug it in here.",
                    "label": 0
                },
                {
                    "sent": "And see what happens, OK?",
                    "label": 0
                },
                {
                    "sent": "So far so good.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's one of the basic cases.",
                    "label": 0
                },
                {
                    "sent": "So if the axes are bounded and that's all we need, and I may just assume that they are bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can just re scale.",
                    "label": 0
                },
                {
                    "sent": "So if these guys are bounded then it's easy to bound the moment generating function of just one of them.",
                    "label": 0
                },
                {
                    "sent": "It looks like this E to the minus Y to the Lambda squared divided by 8.",
                    "label": 0
                },
                {
                    "sent": "This is nice.",
                    "label": 0
                },
                {
                    "sent": "This is called a sub Gaussian behavior because.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "Is this the best?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have if you have a if X is a standard normal random variable then the then the moment generating function of this is Y to the.",
                    "label": 0
                },
                {
                    "sent": "Lambda squared divided by two.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So whenever we have something a moment generating function that looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's bounded by the moment generating function of a Gaussian random variable.",
                    "label": 0
                },
                {
                    "sent": "Well, I should say that if X is, let's say normal.",
                    "label": 0
                },
                {
                    "sent": "Centered, normal and variance Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Then I should put a Sigma squared here.",
                    "label": 0
                },
                {
                    "sent": "Divided by two.",
                    "label": 0
                },
                {
                    "sent": "OK so so these guys have a moment generating function which is less than the moment generating function of a normal with variance.",
                    "label": 0
                },
                {
                    "sent": "1/4 OK, of course.",
                    "label": 0
                },
                {
                    "sent": "The variance of all these guys is less than 1/4 cause because they are between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So this is somehow the right scale.",
                    "label": 0
                },
                {
                    "sent": "OK so this is called Holdings inequality.",
                    "label": 0
                },
                {
                    "sent": "It's a small analytical inequality.",
                    "label": 0
                },
                {
                    "sent": "You choose this convexity of the of the exponential function.",
                    "label": 0
                },
                {
                    "sent": "It's really not a huge deal, but this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extremely useful, this is helping here.",
                    "label": 0
                },
                {
                    "sent": "So if you if we plug this back in the previous slide and you optimize, then this we get this wonderful inequality.",
                    "label": 0
                },
                {
                    "sent": "So what this says is that if we have an average of independent random variables that are bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Then the probability that it differs from its expected value by more than T that goes down like this.",
                    "label": 0
                },
                {
                    "sent": "And this is this really now reminds us what we what we saw in the central Limit Theorem.",
                    "label": 0
                },
                {
                    "sent": "OK, it has this nice exponential decrease in T ^2.",
                    "label": 0
                },
                {
                    "sent": "Tells us that the typical deviations are of the order of one over square root of North, and if it's much larger than then it goes down exponentially.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how things inequality.",
                    "label": 0
                },
                {
                    "sent": "This is really, really nice and very very useful because now if for example, if you have Bernoulli random variables, that means that coin flips independent coin flips even if they don't have the same probability, the different outcomes don't have the same probability, But if they are independent then we have this nice inequality.",
                    "label": 0
                },
                {
                    "sent": "This is completely distribution free.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, have things inequalities, distribution for the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That means that the bounds we get doesn't depend on the distribution or we need this boundedness.",
                    "label": 0
                },
                {
                    "sent": "OK, from a machine learning POV this is great because we don't know anything about.",
                    "label": 0
                },
                {
                    "sent": "Our data.",
                    "label": 0
                },
                {
                    "sent": "OK, free, unless you're a Bayesian, but but otherwise we don't know it, right?",
                    "label": 0
                },
                {
                    "sent": "OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "So so all you see is that is that you have you have some independent trials.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We want to infer what happens there, but we we can have distribution free inequalities like this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But sometimes we still want to do so.",
                    "label": 0
                },
                {
                    "sent": "OK, so one more thing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's wonderful that this is a distribution free inequality, but also it has its disadvantages.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you if.",
                    "label": 0
                },
                {
                    "sent": "If the size are, let's say one with probability.",
                    "label": 0
                },
                {
                    "sent": "P and zero with probability 1 -- P, so these are.",
                    "label": 0
                },
                {
                    "sent": "If they are Bernoulli random variables, then.",
                    "label": 0
                },
                {
                    "sent": "Then then of course, the sum is a binomial random variable and then from the central limit theorem we would expect something like this.",
                    "label": 0
                },
                {
                    "sent": "The probability that the sum.",
                    "label": 0
                },
                {
                    "sent": "The average of XI minus the expected value, which is now P is greater than T. This should be something like E to the minus.",
                    "label": 0
                },
                {
                    "sent": "In the squared divided by the variance right?",
                    "label": 0
                },
                {
                    "sent": "Remember, in the central Limit theorem, let me go back a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The variance shows up here.",
                    "label": 0
                },
                {
                    "sent": "OK, and the variance is P * 1 -- P. So let's say P if P is small.",
                    "label": 0
                },
                {
                    "sent": "Well, whatever P. 1 -- P. And I'm sure there's a 2 here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what we would want.",
                    "label": 0
                },
                {
                    "sent": "The central Limit Theorem tells us, but everything's inequality is.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great if peak was 1/2 because that's exactly what we would like to see.",
                    "label": 0
                },
                {
                    "sent": "But if P is very small then we lose big time.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "If P is small then then this is much much, much, much smaller than either the minus 20 ^2.",
                    "label": 0
                },
                {
                    "sent": "Exactly because of the reason, since have things inequalities distribution free, it has to be prepared for the worst case distribution.",
                    "label": 0
                },
                {
                    "sent": "The worst case distribution turns out to be a binomial symmetric binomial when when we have a symmetric binomial, something like this with P = 1/2, then this is really what we expect from the central Limit Theorem.",
                    "label": 0
                },
                {
                    "sent": "For all other cases.",
                    "label": 0
                },
                {
                    "sent": "We lose OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First time is inequality in which this was the the birth of Chernov method.",
                    "label": 0
                },
                {
                    "sent": "This was basically the first the Bell gives us this variance information here.",
                    "label": 0
                },
                {
                    "sent": "OK, it's, this looks so now.",
                    "label": 0
                },
                {
                    "sent": "OK, let's look at this inequality.",
                    "label": 0
                },
                {
                    "sent": "What it says is that now if we have independent random variables this can be negative.",
                    "label": 0
                },
                {
                    "sent": "Here the only thing we assume that they are less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And then we have this V thing which looks like variance.",
                    "label": 0
                },
                {
                    "sent": "So if these guys had zero mean then this would be the variance.",
                    "label": 0
                },
                {
                    "sent": "But this is just the the sum of the squares.",
                    "label": 0
                },
                {
                    "sent": "Then we have this guy.",
                    "label": 0
                },
                {
                    "sent": "OK, if you think about this case.",
                    "label": 0
                },
                {
                    "sent": "Then then V is just N * P. Right?",
                    "label": 0
                },
                {
                    "sent": "So then you see that we get.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "Each of the minus N T ^2 / 2 P. But here, right?",
                    "label": 0
                },
                {
                    "sent": "This would be.",
                    "label": 0
                },
                {
                    "sent": "Proportional to P. This is this guy is P here.",
                    "label": 0
                },
                {
                    "sent": "OK, but now we pay with this tea.",
                    "label": 0
                },
                {
                    "sent": "Factor this is this the this team will only kick in when when P is very large.",
                    "label": 0
                },
                {
                    "sent": "So V here is N * P. So so this guy will only become important when 20 is greater than N * P. OK, so this is like the very far tails in the large deviation regime.",
                    "label": 0
                },
                {
                    "sent": "And actually you can prove that this is necessary.",
                    "label": 0
                },
                {
                    "sent": "This corrective term is necessary.",
                    "label": 0
                },
                {
                    "sent": "The central Limit theorem is not correct in that in that regime the central Limit Theorem tells us.",
                    "label": 0
                },
                {
                    "sent": "It's asymptotic, but in a very specific.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way it says that if if I look at deviations of the order of square root of North and now take the limit, then this will converge to this.",
                    "label": 0
                },
                {
                    "sent": "But if I want to look at deviations of the order of North, then the story changes.",
                    "label": 0
                },
                {
                    "sent": "Then this guy doesn't tell me anything.",
                    "label": 0
                },
                {
                    "sent": "It just says that that goes to 0, but it's useless.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the reasons.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why not asymptotically inequalities in my in my view are very very useful, because this is true no matter what, end is no matter what the distribution is OK?",
                    "label": 0
                },
                {
                    "sent": "So one has to be very careful with asymptotically approximations, but it may not tell you the whole story.",
                    "label": 0
                },
                {
                    "sent": "There are different regimes of asymptotics.",
                    "label": 0
                },
                {
                    "sent": "What's more relevant, it's hard to say, but inequality like worsening inequality, is always there for you, OK?",
                    "label": 0
                },
                {
                    "sent": "So far so good.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me show you one little application of.",
                    "label": 0
                },
                {
                    "sent": "Of everything's inequality.",
                    "label": 0
                },
                {
                    "sent": "So suppose now that we have N random variables and they don't have to be independent.",
                    "label": 0
                },
                {
                    "sent": "This can be completely arbitrary, so there's no independence here.",
                    "label": 0
                },
                {
                    "sent": "But what I need is that they are sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "They have this kind of moment generating function.",
                    "label": 0
                },
                {
                    "sent": "So everything's inequality tells us that.",
                    "label": 0
                },
                {
                    "sent": "At the binomial, the binomial distribution is like this sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we have N random variables of this type.",
                    "label": 0
                },
                {
                    "sent": "And we want to we want to know.",
                    "label": 0
                },
                {
                    "sent": "How large is the largest of these?",
                    "label": 0
                },
                {
                    "sent": "Of course, in machine learning we are interested in this all the time, right?",
                    "label": 0
                },
                {
                    "sent": "Because we have many trials, many things going on and we want to know.",
                    "label": 0
                },
                {
                    "sent": "When for how?",
                    "label": 0
                },
                {
                    "sent": "Let's say we have 100 classifiers classification algorithms that we want to try and which we choose the best at the end that looks best.",
                    "label": 0
                },
                {
                    "sent": "But now there's a bias there because maybe the one that we chose that's best because for that particular data, it looked better if we fit fit, fit the data with that method better.",
                    "label": 0
                },
                {
                    "sent": "But maybe in some other data it will be difficult.",
                    "label": 0
                },
                {
                    "sent": "So we want to understand how, how far are.",
                    "label": 0
                },
                {
                    "sent": "Random variables from their typical behavior.",
                    "label": 0
                },
                {
                    "sent": "OK, when we have many, many trials, so I will show you a concrete application of this.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have sub Gaussian, so sub Gaussian somehow tell us that the probability that each of these is much larger than the expected value is is not large, right?",
                    "label": 0
                },
                {
                    "sent": "That's what's the sub gaussianity gives us.",
                    "label": 0
                },
                {
                    "sent": "Then the largest of these guys.",
                    "label": 0
                },
                {
                    "sent": "Cannot be too large.",
                    "label": 0
                },
                {
                    "sent": "It will be so one of them should be of the order of Sigma, right?",
                    "label": 0
                },
                {
                    "sent": "But if we have many of them well.",
                    "label": 0
                },
                {
                    "sent": "This is how much we pay.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see the proof is 1.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find it's really nice.",
                    "label": 0
                },
                {
                    "sent": "So how do we prove this inequality?",
                    "label": 0
                },
                {
                    "sent": "So instead of just looking at the expected this expected value, we look at Y to the Lambda times the expected value.",
                    "label": 0
                },
                {
                    "sent": "OK now Y to the Lambda times the expected value E to the Lambda Lambda is positive here.",
                    "label": 0
                },
                {
                    "sent": "So so this is it with Lambda X is a convex function.",
                    "label": 0
                },
                {
                    "sent": "So I can bring out the expected value and I only increase this is Jensen's inequality OK if we have?",
                    "label": 0
                },
                {
                    "sent": "If I have a convex function then the value of the function at the expected value is less, right?",
                    "label": 0
                },
                {
                    "sent": "You know this so if you have if F is a convex function then F. At the expected value of any random variable is less than or equal to the expected value of ethics, right?",
                    "label": 0
                },
                {
                    "sent": "This is called Johnson's.",
                    "label": 0
                },
                {
                    "sent": "Inequality.",
                    "label": 0
                },
                {
                    "sent": "This is basically the definition of convexity if you wish.",
                    "label": 0
                },
                {
                    "sent": "So here we use Jensen's inequality and now.",
                    "label": 0
                },
                {
                    "sent": "The maximum here we are.",
                    "label": 0
                },
                {
                    "sent": "The maximum is.",
                    "label": 0
                },
                {
                    "sent": "The maximum comes outside because each of the Lambda that's an increasing function, so I can put the maximum here.",
                    "label": 0
                },
                {
                    "sent": "Now I have the maximum of non negative random variables that's for sure less than the sum.",
                    "label": 0
                },
                {
                    "sent": "Of these random variables, so I instead of the maximum I upper bounded by a son.",
                    "label": 0
                },
                {
                    "sent": "But if I have the sum then then then then the sum because of the linearity of the expectation, the sum comes outside and what's inside is just the moment generating function of this Lambda of this, why eyes and now I can use the assumption?",
                    "label": 0
                },
                {
                    "sent": "Now we have an inequality here so I can take the log of both sides divided by Lambda and then.",
                    "label": 0
                },
                {
                    "sent": "And then we get what do we get?",
                    "label": 0
                },
                {
                    "sent": "We get the expected value of the maximum of the wise.",
                    "label": 0
                },
                {
                    "sent": "Is less than or equal to?",
                    "label": 0
                },
                {
                    "sent": "So we have the log of N divided by Lambda plus.",
                    "label": 0
                },
                {
                    "sent": "Lambda Times Sigma squared, divided by two.",
                    "label": 0
                },
                {
                    "sent": "Here we have two terms.",
                    "label": 0
                },
                {
                    "sent": "One of them increases with Lambda, the other decreases with Lambda.",
                    "label": 0
                },
                {
                    "sent": "So you can optimize.",
                    "label": 0
                },
                {
                    "sent": "We can differentiate with respect to Lambda, you choose Lambda.",
                    "label": 0
                },
                {
                    "sent": "Optimally this is true for all lambdas, right?",
                    "label": 0
                },
                {
                    "sent": "So I can choose Lambda, do a like and if you do then this is what you get OK. Is this a good inequality?",
                    "label": 0
                },
                {
                    "sent": "This is very good, because in fact if you take independent Gaussian random variables independent and normal standard normal standard normals means that the expected value of itadi Lambda times y = y to the Lambda squared divided by two.",
                    "label": 0
                },
                {
                    "sent": "So if you have standard normal random variables and independent then.",
                    "label": 0
                },
                {
                    "sent": "You can prove that this is asymptotically the correct answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not in provable.",
                    "label": 0
                },
                {
                    "sent": "But if we have the nice thing in this inequality is that here we don't need gaussianity, just sub gaussianity, and we don't need independence.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So how do we apply this?",
                    "label": 0
                },
                {
                    "sent": "We can apply this as a classical.",
                    "label": 0
                },
                {
                    "sent": "Classical question in learning theory, uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We have some SpaceX, that's our space of observations and we have any independent random variables that take values in that space in that SpaceX OK. Now, in this SpaceX we have sets capital and different sets and we're measuring.",
                    "label": 0
                },
                {
                    "sent": "We are measuring the empirically.",
                    "label": 0
                },
                {
                    "sent": "The probability of each of these sets.",
                    "label": 0
                },
                {
                    "sent": "It's like when you have N classifiers then.",
                    "label": 0
                },
                {
                    "sent": "This could be maybe the error rate of each one of those classifiers and this is the true.",
                    "label": 0
                },
                {
                    "sent": "The true error which we don't see.",
                    "label": 0
                },
                {
                    "sent": "OK, the large numbers tells us that the end of a will be close to P of a OK.",
                    "label": 0
                },
                {
                    "sent": "But we have now many of these, so we want to make sure that that we have uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "These two guys are close not only individually for each a, but even the largest of the differences is small.",
                    "label": 1
                },
                {
                    "sent": "OK, this is absolutely crucial for evaluating machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "I want to make sure that the difference between these two.",
                    "label": 0
                },
                {
                    "sent": "A small overall, so I want to bound the.",
                    "label": 0
                },
                {
                    "sent": "I want to say something about the maximum deviation between empirical probabilities and true probabilities.",
                    "label": 0
                },
                {
                    "sent": "I know that individually for if you give me a set for any fixed set.",
                    "label": 0
                },
                {
                    "sent": "If N is large enough then.",
                    "label": 0
                },
                {
                    "sent": "This difference will be small, there's just a lot of large numbers.",
                    "label": 0
                },
                {
                    "sent": "But was the largest difference?",
                    "label": 0
                },
                {
                    "sent": "Clearly, if I have lots of lots of lots of sets, then I can always find the set which exactly fits the data and then I get something complete nonsense.",
                    "label": 0
                },
                {
                    "sent": "OK, now this little inequality tells us that if I don't take too many of them.",
                    "label": 0
                },
                {
                    "sent": "And I can really take a lot, because because the dependence on the number of them is cap is logarithmic, then we're fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we prove this well?",
                    "label": 0
                },
                {
                    "sent": "We just have to look at the moment generating function of these right?",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "I will use the previous inequality.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This inequality tells us that if these guys are sub Gaussian then.",
                    "label": 0
                },
                {
                    "sent": "Then I have a bound, so I will use.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This bound here and all we need to prove is that.",
                    "label": 0
                },
                {
                    "sent": "These differences are sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We don't care about independence, these are not independent, very very dependent random variables, But this previous.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inequality doesn't require independence.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The condition of the previous inequality.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was just about the moment generating function of the individual guys.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at the moment generating function of an individual guy E to the Lambda times this difference.",
                    "label": 0
                },
                {
                    "sent": "Well, I just write it out.",
                    "label": 0
                },
                {
                    "sent": "What this means PN is just the sum so that's what we get now.",
                    "label": 0
                },
                {
                    "sent": "These are now independent.",
                    "label": 0
                },
                {
                    "sent": "Here this is a sum of independent random variables and as I already said, the exponential function.",
                    "label": 0
                },
                {
                    "sent": "Converge the sum into a product and then because of independence the product goes outside OK and now here we have the moment generating function of a random variable that takes values between.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "Well, this guy between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is just it's expected value so I can use her things inequality.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what everything is, inequality tells us.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I have a random variable which is between zero and one, then the moment generating function is bounded by Y to the Lambda squared divided by 8.",
                    "label": 0
                },
                {
                    "sent": "Now instead of four lamb.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over here we use it for Lambda divided by N, so that's where this end comes in OK. Good, so now I can plug it in the in our previous I can plug this bound.",
                    "label": 0
                },
                {
                    "sent": "You know previous inequality, right?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this inequality, which says that if the.",
                    "label": 0
                },
                {
                    "sent": "If the moment generating function is less than Y to the Lambda squared times Sigma Square.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Divided by two now is Sigma squared is now one over N 1 / 4 in here.",
                    "label": 0
                },
                {
                    "sent": "OK so this is how we get it.",
                    "label": 0
                },
                {
                    "sent": "So the largest deviation is bounded by the square root of log number of.",
                    "label": 0
                },
                {
                    "sent": "Events that we want to control together.",
                    "label": 0
                },
                {
                    "sent": "Divided by the sample size.",
                    "label": 0
                },
                {
                    "sent": "So this is nice.",
                    "label": 0
                },
                {
                    "sent": "This means that if we have a large sample then we can we can.",
                    "label": 0
                },
                {
                    "sent": "We can take capital and to be really large, almost exponentially large in the sample size, which is good OK?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is everything that I said so far was about sums of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "But as I said in the in the introduction.",
                    "label": 0
                },
                {
                    "sent": "We want to we want to handle, not own.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sums but arbitrary functions of independent random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so now we have.",
                    "label": 0
                },
                {
                    "sent": "We're back to the slide number one.",
                    "label": 0
                },
                {
                    "sent": "We have independent random variables and we have some arbitrary real valued function of these independent guys, and we want to say something about how close this is to its expected value.",
                    "label": 0
                },
                {
                    "sent": "OK, now the big trick is a so called marketing representation.",
                    "label": 0
                },
                {
                    "sent": "If you don't know what the martingale is, don't worry.",
                    "label": 0
                },
                {
                    "sent": "We will not need it, but this is.",
                    "label": 0
                },
                {
                    "sent": "This is the official name of this type of things so.",
                    "label": 0
                },
                {
                    "sent": "I will introduce this operator, which is the expected value the conditional expectation given the first I random variables.",
                    "label": 0
                },
                {
                    "sent": "So there's some kind of order of the random variables here.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter because because of independence, so we just fix an order and we take the 1st.",
                    "label": 0
                },
                {
                    "sent": "I have them fix their value and then calculate the expected value with respect to the distribution of the rest of them.",
                    "label": 0
                },
                {
                    "sent": "This is what the conditional expectation means.",
                    "label": 0
                },
                {
                    "sent": "We fix these guys and take the expected value.",
                    "label": 0
                },
                {
                    "sent": "With respect to the distribution of the XI plus one to XN, OK, so if equals zero, then it means that I'm just taking the expected value with respect to all of them, so there's the ordinary expected value.",
                    "label": 0
                },
                {
                    "sent": "And if I fix all of them, all of the random variables.",
                    "label": 0
                },
                {
                    "sent": "Then the expected value.",
                    "label": 0
                },
                {
                    "sent": "If I fix all the all the random variables, then the condition expectation is just the random variable itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the two extremes.",
                    "label": 0
                },
                {
                    "sent": "If I take this conditional expectations, then I go from the expected value of the random variable to random variable itself.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this then we can naturally.",
                    "label": 0
                },
                {
                    "sent": "Introduced these differences?",
                    "label": 0
                },
                {
                    "sent": "So what's the difference between this guy and this guy?",
                    "label": 0
                },
                {
                    "sent": "Well, I just take here.",
                    "label": 0
                },
                {
                    "sent": "I just take one more conditioning variable.",
                    "label": 0
                },
                {
                    "sent": "OK, here I condition on the first time I minus one.",
                    "label": 0
                },
                {
                    "sent": "I fixed those values and take the expected value with respect to the rest and here I condition on one more and I look at what's the difference between these two expected values.",
                    "label": 0
                },
                {
                    "sent": "Now the sum of these guys is a telescoping sum.",
                    "label": 0
                },
                {
                    "sent": "And what we get is.",
                    "label": 0
                },
                {
                    "sent": "Isabelle of Z -- Y sub 0Z right?",
                    "label": 1
                },
                {
                    "sent": "Which is y -- Y of Z. OK, so if this is a nice way of writing down.",
                    "label": 0
                },
                {
                    "sent": "The difference between the random variable and its expected value, it's just the sum of.",
                    "label": 0
                },
                {
                    "sent": "These are so-called martindale differences.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the sum of marketing your differences that we can write down.",
                    "label": 1
                },
                {
                    "sent": "We can do this with any random variable in the world, and this is called the Duke marketing representation.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Named after this guy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the dude Martindale representation of an arbitrary random variable which is a function of independent random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, alright, So what is this good for?",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing in this representation is that even though now this is not a sum of independent random variables anymore, but instead of independence we at least get orthogonality.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do I mean by this?",
                    "label": 0
                },
                {
                    "sent": "Turns out and this is very easy to see I will show it in the next slide that even though these random variables are not independent, they are not correlated, they are uncorrelated random variables and this is called a martingale.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, here's for example.",
                    "label": 0
                },
                {
                    "sent": "One nice consequence of of.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of writing random variable writing the fluctuation.",
                    "label": 0
                },
                {
                    "sent": "The random fluctuation of a random variable difference between the random variable and its expected value as a sum of these martingales difference.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Guys.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I want to bound for example the variance, the variance is a is a good quantity.",
                    "label": 0
                },
                {
                    "sent": "Remember that Chebyshev's inequality tells us that if the variance variance is small, then then the random variable is concentrated.",
                    "label": 0
                },
                {
                    "sent": "So this is good to keep in mind that the probability.",
                    "label": 0
                },
                {
                    "sent": "The XZ minus the expected value of Z is greater than T. This is less than the variance of Z / T ^2.",
                    "label": 0
                },
                {
                    "sent": "So that's somehow kind of a first order approach.",
                    "label": 0
                },
                {
                    "sent": "We saw that this doesn't always give us sharp bounds.",
                    "label": 0
                },
                {
                    "sent": "For example, when we dealt with sums of independent random variables that are bounded, this was not necessary.",
                    "label": 0
                },
                {
                    "sent": "The optimal, but many times you can't beat it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just start with this inequality 1st and let's try to understand the variance.",
                    "label": 0
                },
                {
                    "sent": "Good, so let's write the variance of an arbitrary random variable.",
                    "label": 1
                },
                {
                    "sent": "Because of the martingale representation, I can write it like this.",
                    "label": 0
                },
                {
                    "sent": "And now I can expand the square so there are two kinds of terms here.",
                    "label": 0
                },
                {
                    "sent": "There are the Delta I squares, expected value of the Delta I squares, and there are the cross terms OK. Yeah, so far so good.",
                    "label": 0
                },
                {
                    "sent": "Now the nice thing is that.",
                    "label": 0
                },
                {
                    "sent": "This is zero.",
                    "label": 0
                },
                {
                    "sent": "This term here is 0 and that follows from the way we define this.",
                    "label": 0
                },
                {
                    "sent": "Lamb dies OK. Why is this true?",
                    "label": 0
                },
                {
                    "sent": "So let me let me maybe explain it here.",
                    "label": 0
                },
                {
                    "sent": "Maybe this one?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what's what's the expected value?",
                    "label": 0
                },
                {
                    "sent": "There's an index I there, so that means that I will take the expected value when I fix.",
                    "label": 0
                },
                {
                    "sent": "The first I random variables and what's Delta I.",
                    "label": 0
                },
                {
                    "sent": "So OK, one thing.",
                    "label": 0
                },
                {
                    "sent": "Delta I hear.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the definition here, we fixed the first I random variables and here is the first I -- 1 random variables.",
                    "label": 0
                },
                {
                    "sent": "So if I take the.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expected value with respect by fixing the last I random variables, then Delta is a constant.",
                    "label": 0
                },
                {
                    "sent": "OK, if I fix the first I random variables then this guy is a constant.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This doesn't depend on the rest of the random variables, because we integrate it out with respect to their distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a constant so so I can take it outside this Delta I comes outside.",
                    "label": 0
                },
                {
                    "sent": "I only consider now indices when J is greater than I right?",
                    "label": 0
                },
                {
                    "sent": "Because that's how I wrote it.",
                    "label": 0
                },
                {
                    "sent": "I put the two here so that I can only consider this so Delta I comes outside.",
                    "label": 0
                },
                {
                    "sent": "And now here I have the expected value with respect to.",
                    "label": 0
                },
                {
                    "sent": "The expected value of this guy and here inside I have something like Z.",
                    "label": 0
                },
                {
                    "sent": "The expected value of Z given X1.",
                    "label": 0
                },
                {
                    "sent": "XJ I have this term and minus I have another term in which insight here I have X -- 1.",
                    "label": 0
                },
                {
                    "sent": "XJ minus one.",
                    "label": 0
                },
                {
                    "sent": "The expected value of this given X1XI.",
                    "label": 0
                },
                {
                    "sent": "OK, but James greater than I so that means that J minus one is greater than or equal to Y and that means that if I fix this and take the expected value of this guy, then simply just using the.",
                    "label": 0
                },
                {
                    "sent": "What is this called the?",
                    "label": 0
                },
                {
                    "sent": "The theorem of total expectation.",
                    "label": 0
                },
                {
                    "sent": "Just a.",
                    "label": 0
                },
                {
                    "sent": "Just using this formula, this guy up here is.",
                    "label": 0
                },
                {
                    "sent": "The expected value of Z given X1.",
                    "label": 0
                },
                {
                    "sent": "Thanks, I and this guy is the same.",
                    "label": 0
                },
                {
                    "sent": "Excellent.",
                    "label": 0
                },
                {
                    "sent": "XI.",
                    "label": 0
                },
                {
                    "sent": "So that's all we need to know about conditional expectations to prove this.",
                    "label": 0
                },
                {
                    "sent": "OK, that this cross terms go out, that's why that's why I said that these are orthogonal.",
                    "label": 0
                },
                {
                    "sent": "These are uncorrelated random variables.",
                    "label": 0
                },
                {
                    "sent": "The expected value of the product equals 0, so that means that the variance can be written something like the sum of the variances.",
                    "label": 1
                },
                {
                    "sent": "This is exactly what we saw when at the beginning when we talked about Chebyshev's inequality, right?",
                    "label": 0
                },
                {
                    "sent": "That was the nice thing about about the variance that.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's here the variance of a sum equals the sum of the variances.",
                    "label": 0
                },
                {
                    "sent": "This was true for independence.",
                    "label": 0
                },
                {
                    "sent": "Now here we have at least formally something that looks similar.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, all we need to do now is to understand these guys.",
                    "label": 0
                },
                {
                    "sent": "OK, understand these these Delta eyes.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are these?",
                    "label": 0
                },
                {
                    "sent": "Delta is just intuitively?",
                    "label": 0
                },
                {
                    "sent": "We look at a random variable.",
                    "label": 0
                },
                {
                    "sent": "You look at the random variable.",
                    "label": 0
                },
                {
                    "sent": "F which depends on lots of things XN and now I look at.",
                    "label": 0
                },
                {
                    "sent": "This the expected value of this given X1XI, so I fixed the first high values and look at the expected value of this.",
                    "label": 0
                },
                {
                    "sent": "And now I compare this.",
                    "label": 0
                },
                {
                    "sent": "With what I would get if I had only fixed one less variable X 1X N given X1 X I -- 1.",
                    "label": 0
                },
                {
                    "sent": "From this formulation you can see that this guy here.",
                    "label": 0
                },
                {
                    "sent": "The only difference between between these two is that I fixed here one more variable.",
                    "label": 0
                },
                {
                    "sent": "And here I integrate with respect to that variable.",
                    "label": 0
                },
                {
                    "sent": "So somehow this intuitively it should be clear that if this function doesn't depend too heavily on any of the variables, then we should be in business.",
                    "label": 0
                },
                {
                    "sent": "Right, if this function is stable in the sense that it doesn't.",
                    "label": 0
                },
                {
                    "sent": "If I if I change one variable but leave the others fixed, then the value of the function doesn't change then, then this should be small.",
                    "label": 0
                },
                {
                    "sent": "This should be controllable.",
                    "label": 0
                },
                {
                    "sent": "So, and this is somehow the principle that we will explore that if I have a function of many, many, many random variables such that.",
                    "label": 1
                },
                {
                    "sent": "It doesn't depend very strongly on any of the.",
                    "label": 0
                },
                {
                    "sent": "Components, any of the variables, then it will be concentrated OK and this is the first instance.",
                    "label": 1
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Offer of such a such an inequality OK?",
                    "label": 0
                },
                {
                    "sent": "Should we take 5 minutes?",
                    "label": 0
                },
                {
                    "sent": "OK, so when we take 5 minutes then after after I will show you a very very useful inequality that follows from this in an easy way.",
                    "label": 0
                }
            ]
        }
    }
}