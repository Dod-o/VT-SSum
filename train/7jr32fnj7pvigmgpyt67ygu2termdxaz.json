{
    "id": "7jr32fnj7pvigmgpyt67ygu2termdxaz",
    "title": "Capturing Semantic and Syntactic Information for Link Prediction in Knowledge Graphs",
    "info": {
        "author": [
            "Changjian Wang, Hefei University of Technology"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_wang_prediction_knowledge_graphs/",
    "segmentation": [
        [
            "Hello everyone, I'm glad to be here to present my work.",
            "My name is Juan Tenjin and I'm a student at Institute of Information Engineering Academy of Sciences.",
            "Capturing semantic and syntactic information for link prediction in knowledge graphs.",
            "Soup"
        ],
        [
            "Knowledge graphs are databases that contain facts about world, recent years, several knowledge graph such as Jago, Freebase and DPD have been constructed.",
            "This knowledge graphs are extremely useful vessels for manual applications such as question answering, information extraction, recommendation and so on.",
            "However, knowledge graphs are usually far from complete that missing links between entertains.",
            "For example in Freebase and DP.",
            "There are more than 16 states, percent of person missing the birthplace.",
            "This hinders their usefulness in the above applications.",
            "Therefore, many works devoted to so solving this kind of problem called link prediction."
        ],
        [
            "In order to describe our research problem, or more accurately, we first give a formal representation of knowledge graphs that is cagey, cagey is a set of triples, each triple HRTD knows there is a relation R between the head into TH and tail entity.",
            "For example, the fact that Beijing is the capital of China can be writing this for this.",
            "Is this kind of.",
            "Triple.",
            "Think of protection users at first.",
            "Input link prediction you refer to the task of predicting another intuitive, forgiving Lucian relation that is predicting the tail entertain, giving head, entity and relation or predict the predicting the head interactive giving relation and tail entity.",
            "No, today is the 2nd task can be right in this form by adding an inverse relation.",
            "So these two tasks can be unified as winters and our goal is to estimate the conditional probability distribution of the target entity."
        ],
        [
            "In recent years the most successful models for link prediction are embedding based in this models, into relations are represented as voters or matrix, but simple operations like addition or multiplication.",
            "The reputation of entity and relation in a triple constitute a scoring function but simple operation.",
            "Publishing so that it scores valid triple, triple, higher than in value triples.",
            "For example, this is a illustration of transition.",
            "It's going, it's going.",
            "Function is construct is constructed by addition and.",
            "It is cold there that ripples harder in variables.",
            "There are mainly three category of these models, including translational models by linear models and new network based models.",
            "They are constructed the scoring function using addition, multiplication and new networks respectively.",
            "However, these models only consider the triples, so the information you can use is limited compared with triple.",
            "The paths are more information, so we use past instead of triples to do link to for link prediction tasks which surpass as pseudo sentences similar to the natural language used.",
            "In a sense, we can capture the semantic information and text information in the past.",
            "To improve the performance of Link prediction, the semantic information in this means the similar entity get similar representations and the syntactic information means.",
            "This means the order of entertainer relations in your past."
        ],
        [
            "This is the overview of our method.",
            "In general, we propose a method called RWM for link prediction.",
            "It consists of two parts.",
            "Random word algorithm for cutie and Umm, a language model based link prediction model.",
            "Most specifically, as shown in this figure.",
            "We first convert triple into a graph.",
            "Then we perform random walks on this graph to generate a setup pass.",
            "This past will be viewed as a pseudoscience to train our OML model.",
            "Finally, we use change model to do link prediction task."
        ],
        [
            "Next I will describe our method in detail.",
            "Firstly, we convert people into a graph that is simple.",
            "We treat each entity as a vertex.",
            "And each relation as a directly from H2 T and we will also add a inverse relation from T2H."
        ],
        [
            "The other part of our method is a random work algorithm which is inspired by the work we use this algorithm to generate a set of paths.",
            "We perform random work starting from each vertex of graph and workout steps along the outgoing direction of the vertex to generate a pass for each step, we randomly choose an outgoing edges, vectors firstly, then we choose the edge between the two vertex.",
            "If there are multiple edges between the two vertices, we will randomly choose one.",
            "After multi multiple alterations will get a set of paths."
        ],
        [
            "Our LMN model is inspired by the standard language model, which has ability to capture the semantic and syntactic information of natural language sentences.",
            "Under it had ability to predict the next World giving previous words.",
            "The object objective of the standard language model is to maximum the probability of a sentence or sequence of tokens or seeking tokens.",
            "It's it's the object is maximum.",
            "This probability I will our model is similar to this.",
            "We treat a path as a pseudo sentence and our objective is to maximize this probability.",
            "We we construct our model using transformer decoder that is a decoder part of transformer.",
            "As a simple but effective effective model, transformer decoder has been widely used for language modeling in recent years.",
            "This is architecture of TD.",
            "They transform decoder.",
            "TD is mainly composed of two parts that including the mask Mount, Head attention layer and feed forward layer.",
            "No, maximal her tension there use must self protection mechanism and the feed forward layer is fully connected position while the feed forward network which consists of two.",
            "Menage transforming transformations in addition TD also use the residual connection and normalization normalization between every two layers."
        ],
        [
            "Our.",
            "Almost vertically, this figure shows the architecture of our model.",
            "Firstly model get the embedding representations of Internet and relations using the Internet embedding matrix and relation embedding matrix.",
            "Then we calculate these two these two embeddings to wine and.",
            "Position embedding which is governed from the position effective position matrix.",
            "Invite the measures there than you.",
            "Embedding will be viewed as the input of TD.",
            "This line showing the Max self thinking mechanism of TD.",
            "It takes the.",
            "Reputation of this position by considering its previous representations and itself.",
            "Automount after multilayer TV and two projection.",
            "There we use a software function to get the distribution probability distribution of the text.",
            "Take this past exam example easier or other products, even an easier as you even our products into that is the input is previous entities and relations and the target is next entity, which our model.",
            "By minimizing this loss function."
        ],
        [
            "Well, we've only to our method on 4 bit modest SW 8 in the subset Walnut Tree.",
            "BFB 15K is a subset of rebates later to.",
            "These sets are subsets of related to another subset of the formal to buy, but removing the inverse relation to for more challenging and realistic distance after all these datasets consist of three parts, this including.",
            "Training data validation.",
            "Set an testing center.",
            "This statistic shows the least people should set of these for it sets.",
            "We can kind of get the probability stripping of target by our model.",
            "We rank the Kennedys, entertains, Avicii, testing and triple by descending order according to the probability values, and our evaluation is performed on these rankings.",
            "There are usually two in valuation settings, including the role and the field.",
            "Roy Roy use the original rankings and filter user.",
            "Filter removes some entertains before linking to avoid some misleading behaviour.",
            "To compare the performance of our method and others with you.",
            "Three measures, including me rank, mean restaurant and his side."
        ],
        [
            "This slide shows the experimental results of our method.",
            "We can show that our method obtain the best of the second best results on most metrics."
        ],
        [
            "We also did more experiments to analyze our methods in several aspects.",
            "We first investigated the effect of the two parameters, including the number of iterations, T and work, less error of RW.",
            "Um?",
            "RW underperformance oh oh, he's at a time.",
            "We know this figure shows the results.",
            "We know that he's at his window.",
            "His attention creates as T increase.",
            "This is intuitive.",
            "More alterations can get more information for link prediction, and we know that L = 5.",
            "I have the improvement coming to equals three, but from average qualifier to equals 10 there is almost no change.",
            "That means we don't need too much too many workers steps to obtain the best performance.",
            "2nd, we investigate.",
            "We investigated the effect of missing the.",
            "Missing the relation information.",
            "We can see that all the performance dramatic declines, and we also perform a ablation studies without all the information.",
            "All the metrics declare and up to 8%.",
            "Eight percent relative decrease on his site, he said one.",
            "This means a auto information and the relation information are two important important information for the performance of link prediction."
        ],
        [
            "We also come here our model with the with the Berliner model dismount and a new network based model, This table shows us without our model.",
            "For for the two models with a signal with simple number of parameters and in general overall.",
            "Our model is.",
            "Our model is 0.94 million parameters performance than company with 1.89 million parameters.",
            "Our model with 0.46 million parameters still performs and dismount with 1.89 million parameters.",
            "Overall, our model is 2 town proper inflation conversion, at least full time profession dismount."
        ],
        [
            "Finally, I make her concluding for our work, we propose a novel method called uh, Umm, for link producing encourages, and we find a way.",
            "We find that more worker styles may not always necessary for to get the best performance.",
            "This may help other works to choose.",
            "Bob has passed last when they want to improve the link prediction performance by the past information.",
            "We also show that our model is highly parameter efficient.",
            "In the future we plan to a protein found to direct directions we want to study the effect of the dimension of entertaining relations on performance more specifically, and we also plan to study the relationship between the optimal optimal path things and the number of entertain.",
            "All relations are more datasets and our code and distance are available to GitHub.",
            "The top thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, I'm glad to be here to present my work.",
                    "label": 0
                },
                {
                    "sent": "My name is Juan Tenjin and I'm a student at Institute of Information Engineering Academy of Sciences.",
                    "label": 1
                },
                {
                    "sent": "Capturing semantic and syntactic information for link prediction in knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "Soup",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowledge graphs are databases that contain facts about world, recent years, several knowledge graph such as Jago, Freebase and DPD have been constructed.",
                    "label": 1
                },
                {
                    "sent": "This knowledge graphs are extremely useful vessels for manual applications such as question answering, information extraction, recommendation and so on.",
                    "label": 0
                },
                {
                    "sent": "However, knowledge graphs are usually far from complete that missing links between entertains.",
                    "label": 1
                },
                {
                    "sent": "For example in Freebase and DP.",
                    "label": 0
                },
                {
                    "sent": "There are more than 16 states, percent of person missing the birthplace.",
                    "label": 0
                },
                {
                    "sent": "This hinders their usefulness in the above applications.",
                    "label": 0
                },
                {
                    "sent": "Therefore, many works devoted to so solving this kind of problem called link prediction.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to describe our research problem, or more accurately, we first give a formal representation of knowledge graphs that is cagey, cagey is a set of triples, each triple HRTD knows there is a relation R between the head into TH and tail entity.",
                    "label": 0
                },
                {
                    "sent": "For example, the fact that Beijing is the capital of China can be writing this for this.",
                    "label": 0
                },
                {
                    "sent": "Is this kind of.",
                    "label": 0
                },
                {
                    "sent": "Triple.",
                    "label": 0
                },
                {
                    "sent": "Think of protection users at first.",
                    "label": 0
                },
                {
                    "sent": "Input link prediction you refer to the task of predicting another intuitive, forgiving Lucian relation that is predicting the tail entertain, giving head, entity and relation or predict the predicting the head interactive giving relation and tail entity.",
                    "label": 0
                },
                {
                    "sent": "No, today is the 2nd task can be right in this form by adding an inverse relation.",
                    "label": 0
                },
                {
                    "sent": "So these two tasks can be unified as winters and our goal is to estimate the conditional probability distribution of the target entity.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In recent years the most successful models for link prediction are embedding based in this models, into relations are represented as voters or matrix, but simple operations like addition or multiplication.",
                    "label": 0
                },
                {
                    "sent": "The reputation of entity and relation in a triple constitute a scoring function but simple operation.",
                    "label": 0
                },
                {
                    "sent": "Publishing so that it scores valid triple, triple, higher than in value triples.",
                    "label": 0
                },
                {
                    "sent": "For example, this is a illustration of transition.",
                    "label": 0
                },
                {
                    "sent": "It's going, it's going.",
                    "label": 0
                },
                {
                    "sent": "Function is construct is constructed by addition and.",
                    "label": 0
                },
                {
                    "sent": "It is cold there that ripples harder in variables.",
                    "label": 0
                },
                {
                    "sent": "There are mainly three category of these models, including translational models by linear models and new network based models.",
                    "label": 0
                },
                {
                    "sent": "They are constructed the scoring function using addition, multiplication and new networks respectively.",
                    "label": 0
                },
                {
                    "sent": "However, these models only consider the triples, so the information you can use is limited compared with triple.",
                    "label": 1
                },
                {
                    "sent": "The paths are more information, so we use past instead of triples to do link to for link prediction tasks which surpass as pseudo sentences similar to the natural language used.",
                    "label": 1
                },
                {
                    "sent": "In a sense, we can capture the semantic information and text information in the past.",
                    "label": 0
                },
                {
                    "sent": "To improve the performance of Link prediction, the semantic information in this means the similar entity get similar representations and the syntactic information means.",
                    "label": 1
                },
                {
                    "sent": "This means the order of entertainer relations in your past.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the overview of our method.",
                    "label": 1
                },
                {
                    "sent": "In general, we propose a method called RWM for link prediction.",
                    "label": 0
                },
                {
                    "sent": "It consists of two parts.",
                    "label": 0
                },
                {
                    "sent": "Random word algorithm for cutie and Umm, a language model based link prediction model.",
                    "label": 1
                },
                {
                    "sent": "Most specifically, as shown in this figure.",
                    "label": 0
                },
                {
                    "sent": "We first convert triple into a graph.",
                    "label": 0
                },
                {
                    "sent": "Then we perform random walks on this graph to generate a setup pass.",
                    "label": 0
                },
                {
                    "sent": "This past will be viewed as a pseudoscience to train our OML model.",
                    "label": 0
                },
                {
                    "sent": "Finally, we use change model to do link prediction task.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next I will describe our method in detail.",
                    "label": 0
                },
                {
                    "sent": "Firstly, we convert people into a graph that is simple.",
                    "label": 1
                },
                {
                    "sent": "We treat each entity as a vertex.",
                    "label": 1
                },
                {
                    "sent": "And each relation as a directly from H2 T and we will also add a inverse relation from T2H.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other part of our method is a random work algorithm which is inspired by the work we use this algorithm to generate a set of paths.",
                    "label": 0
                },
                {
                    "sent": "We perform random work starting from each vertex of graph and workout steps along the outgoing direction of the vertex to generate a pass for each step, we randomly choose an outgoing edges, vectors firstly, then we choose the edge between the two vertex.",
                    "label": 0
                },
                {
                    "sent": "If there are multiple edges between the two vertices, we will randomly choose one.",
                    "label": 0
                },
                {
                    "sent": "After multi multiple alterations will get a set of paths.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our LMN model is inspired by the standard language model, which has ability to capture the semantic and syntactic information of natural language sentences.",
                    "label": 0
                },
                {
                    "sent": "Under it had ability to predict the next World giving previous words.",
                    "label": 0
                },
                {
                    "sent": "The object objective of the standard language model is to maximum the probability of a sentence or sequence of tokens or seeking tokens.",
                    "label": 1
                },
                {
                    "sent": "It's it's the object is maximum.",
                    "label": 0
                },
                {
                    "sent": "This probability I will our model is similar to this.",
                    "label": 0
                },
                {
                    "sent": "We treat a path as a pseudo sentence and our objective is to maximize this probability.",
                    "label": 0
                },
                {
                    "sent": "We we construct our model using transformer decoder that is a decoder part of transformer.",
                    "label": 0
                },
                {
                    "sent": "As a simple but effective effective model, transformer decoder has been widely used for language modeling in recent years.",
                    "label": 0
                },
                {
                    "sent": "This is architecture of TD.",
                    "label": 0
                },
                {
                    "sent": "They transform decoder.",
                    "label": 0
                },
                {
                    "sent": "TD is mainly composed of two parts that including the mask Mount, Head attention layer and feed forward layer.",
                    "label": 0
                },
                {
                    "sent": "No, maximal her tension there use must self protection mechanism and the feed forward layer is fully connected position while the feed forward network which consists of two.",
                    "label": 0
                },
                {
                    "sent": "Menage transforming transformations in addition TD also use the residual connection and normalization normalization between every two layers.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our.",
                    "label": 0
                },
                {
                    "sent": "Almost vertically, this figure shows the architecture of our model.",
                    "label": 0
                },
                {
                    "sent": "Firstly model get the embedding representations of Internet and relations using the Internet embedding matrix and relation embedding matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we calculate these two these two embeddings to wine and.",
                    "label": 0
                },
                {
                    "sent": "Position embedding which is governed from the position effective position matrix.",
                    "label": 0
                },
                {
                    "sent": "Invite the measures there than you.",
                    "label": 0
                },
                {
                    "sent": "Embedding will be viewed as the input of TD.",
                    "label": 0
                },
                {
                    "sent": "This line showing the Max self thinking mechanism of TD.",
                    "label": 0
                },
                {
                    "sent": "It takes the.",
                    "label": 0
                },
                {
                    "sent": "Reputation of this position by considering its previous representations and itself.",
                    "label": 0
                },
                {
                    "sent": "Automount after multilayer TV and two projection.",
                    "label": 0
                },
                {
                    "sent": "There we use a software function to get the distribution probability distribution of the text.",
                    "label": 1
                },
                {
                    "sent": "Take this past exam example easier or other products, even an easier as you even our products into that is the input is previous entities and relations and the target is next entity, which our model.",
                    "label": 1
                },
                {
                    "sent": "By minimizing this loss function.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we've only to our method on 4 bit modest SW 8 in the subset Walnut Tree.",
                    "label": 0
                },
                {
                    "sent": "BFB 15K is a subset of rebates later to.",
                    "label": 0
                },
                {
                    "sent": "These sets are subsets of related to another subset of the formal to buy, but removing the inverse relation to for more challenging and realistic distance after all these datasets consist of three parts, this including.",
                    "label": 0
                },
                {
                    "sent": "Training data validation.",
                    "label": 0
                },
                {
                    "sent": "Set an testing center.",
                    "label": 0
                },
                {
                    "sent": "This statistic shows the least people should set of these for it sets.",
                    "label": 0
                },
                {
                    "sent": "We can kind of get the probability stripping of target by our model.",
                    "label": 0
                },
                {
                    "sent": "We rank the Kennedys, entertains, Avicii, testing and triple by descending order according to the probability values, and our evaluation is performed on these rankings.",
                    "label": 0
                },
                {
                    "sent": "There are usually two in valuation settings, including the role and the field.",
                    "label": 0
                },
                {
                    "sent": "Roy Roy use the original rankings and filter user.",
                    "label": 0
                },
                {
                    "sent": "Filter removes some entertains before linking to avoid some misleading behaviour.",
                    "label": 0
                },
                {
                    "sent": "To compare the performance of our method and others with you.",
                    "label": 0
                },
                {
                    "sent": "Three measures, including me rank, mean restaurant and his side.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This slide shows the experimental results of our method.",
                    "label": 0
                },
                {
                    "sent": "We can show that our method obtain the best of the second best results on most metrics.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also did more experiments to analyze our methods in several aspects.",
                    "label": 0
                },
                {
                    "sent": "We first investigated the effect of the two parameters, including the number of iterations, T and work, less error of RW.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "RW underperformance oh oh, he's at a time.",
                    "label": 0
                },
                {
                    "sent": "We know this figure shows the results.",
                    "label": 0
                },
                {
                    "sent": "We know that he's at his window.",
                    "label": 0
                },
                {
                    "sent": "His attention creates as T increase.",
                    "label": 0
                },
                {
                    "sent": "This is intuitive.",
                    "label": 0
                },
                {
                    "sent": "More alterations can get more information for link prediction, and we know that L = 5.",
                    "label": 0
                },
                {
                    "sent": "I have the improvement coming to equals three, but from average qualifier to equals 10 there is almost no change.",
                    "label": 0
                },
                {
                    "sent": "That means we don't need too much too many workers steps to obtain the best performance.",
                    "label": 0
                },
                {
                    "sent": "2nd, we investigate.",
                    "label": 0
                },
                {
                    "sent": "We investigated the effect of missing the.",
                    "label": 0
                },
                {
                    "sent": "Missing the relation information.",
                    "label": 0
                },
                {
                    "sent": "We can see that all the performance dramatic declines, and we also perform a ablation studies without all the information.",
                    "label": 0
                },
                {
                    "sent": "All the metrics declare and up to 8%.",
                    "label": 0
                },
                {
                    "sent": "Eight percent relative decrease on his site, he said one.",
                    "label": 0
                },
                {
                    "sent": "This means a auto information and the relation information are two important important information for the performance of link prediction.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also come here our model with the with the Berliner model dismount and a new network based model, This table shows us without our model.",
                    "label": 0
                },
                {
                    "sent": "For for the two models with a signal with simple number of parameters and in general overall.",
                    "label": 0
                },
                {
                    "sent": "Our model is.",
                    "label": 0
                },
                {
                    "sent": "Our model is 0.94 million parameters performance than company with 1.89 million parameters.",
                    "label": 0
                },
                {
                    "sent": "Our model with 0.46 million parameters still performs and dismount with 1.89 million parameters.",
                    "label": 0
                },
                {
                    "sent": "Overall, our model is 2 town proper inflation conversion, at least full time profession dismount.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, I make her concluding for our work, we propose a novel method called uh, Umm, for link producing encourages, and we find a way.",
                    "label": 0
                },
                {
                    "sent": "We find that more worker styles may not always necessary for to get the best performance.",
                    "label": 1
                },
                {
                    "sent": "This may help other works to choose.",
                    "label": 0
                },
                {
                    "sent": "Bob has passed last when they want to improve the link prediction performance by the past information.",
                    "label": 1
                },
                {
                    "sent": "We also show that our model is highly parameter efficient.",
                    "label": 0
                },
                {
                    "sent": "In the future we plan to a protein found to direct directions we want to study the effect of the dimension of entertaining relations on performance more specifically, and we also plan to study the relationship between the optimal optimal path things and the number of entertain.",
                    "label": 1
                },
                {
                    "sent": "All relations are more datasets and our code and distance are available to GitHub.",
                    "label": 0
                },
                {
                    "sent": "The top thank you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}