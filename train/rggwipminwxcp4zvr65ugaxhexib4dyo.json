{
    "id": "rggwipminwxcp4zvr65ugaxhexib4dyo",
    "title": "RDFox: A Highly-Scalable RDF Store",
    "info": {
        "author": [
            "Yavor Nenov, Department of Computer Science, University of Oxford"
        ],
        "published": "Nov. 10, 2015",
        "recorded": "October 2015",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2015_nenov_rdf_store/",
    "segmentation": [
        [
            "Efficiently."
        ],
        [
            "And more importantly, it also supports highly scalable reasoning.",
            "Materialization based reasoning including incremental reasoning and handling.",
            "Native handling of are same as.",
            "We also implement some theoretical results from conjunctive query answering literature compositions.",
            "In particular for those who know what that means.",
            "Which which provides which allow us to provide an efficient query on string performance.",
            "When we try to to make early folks as versatile as possible, to the best of our abilities.",
            "So we currently we can.",
            "You can use RDF folksy as a C, Java, Python library, or you can use it as a sparkle endpoint.",
            "He's being developed with University of Oxford and she's available on the academic license."
        ],
        [
            "At the core of the system, we have a very highly flexible and and scalable storage scheme.",
            "The key feature that storage scheme is that you can import data in parallel efficiently in a lock free fashion.",
            "Triples is stored in a triple table, which is common for RDF stores.",
            "And.",
            "It's the store supports configurable indexing scheme.",
            "The default, the default indexing scheme is shown on screen.",
            "Depends, but it is configurable so your developers can can can easily plug in their own indexing scheme of choice.",
            "As I said, the crucial property of this car storage screen is that it supports mostly lock free.",
            "Updates so different threads can import triples into the store simultaneously almost without any locking.",
            "This is crucial for if you want to import for scalability in cases where you want to import data from different sources or during parallel reasoning where triples derived where threads derive triples which have to be inserted into the store."
        ],
        [
            "The most appealing feature of data of Arctic Fox is its data look reasoning capabilities it supports.",
            "I eat incorporates a materialization based data lock engine which implements state of the art reasoning algorithms.",
            "For those that don't know what data locate state along his room based language which can capture outer Elance were rules, you should be more familiar with that I hope.",
            "In particular, will Sir just a syntactic variant of Datalog, cruise and for outdoor L Intelligence you can turn any outdoor Ellen Teologi into a set of rules and reason with those data La Cruz.",
            "Or you can use the fixed.",
            "They told program specified in the outer specification to reason with any ontology specified as triples."
        ],
        [
            "Are you folks, yeah.",
            "Takes the materialization based approach so it performs reasoning via materialization.",
            "Which essentially means that you precompute or explicate all the consequences of your data and knowledge and the background knowledge that is specified as an ontology.",
            "And then you answer via queries against the explicated.",
            "Consequences this allows for efficient query answering without the need of reasoning during query evaluation.",
            "The algorithm that are the Fox employees is A is a novel algorithm.",
            "It is a shared memory parallel algorithm which represented the triple AI last year.",
            "It is a AAA time variant of the same evaluation algorithm for people that know what that is and to the crucial properties that you don't repeat work in effect, every rule instantiation is considered at most once.",
            "The interesting part from parallelization POV regarding reasoning is that the reasoning task is split evenly.",
            "In a very, very small subtasks, in particular, 1 task sub task per triple.",
            "This allows for this fine grained partitioning of the reasoning that allows for high scalability.",
            "Without the need of explicit load balancing."
        ],
        [
            "Data on the web changes a lot.",
            "We all know that we have.",
            "We had workshops and tutorials on that topic, stringstream reasoning, and so on.",
            "The artifact solution to this.",
            "Ever changing data is an incremental reasoning algorithm.",
            "Which it implements.",
            "The idea behind incremental reasoning algorithms is not to recompute everything from scratch, but once you have your materialization when the data changes, then you update the materialization's accordingly in an efficient way.",
            "We used an algorithm we just got FBF which stands for forward, backward and forward.",
            "Which involves interesting interaction between far forward, backward and forward chaining reasoning.",
            "The important property of this algorithm is that it doesn't require any additional information gathered by the initial materialization, but so we don't store any counts or dependencies between facts or proofs for that matter.",
            "The algorithm is an improvement of the well known direct algorithm from database community and the improvements come from the fact comes from the fact that we don't over delete but rather delete, perform exact deletions.",
            "Unfortunately, I can't get into more detail at this point."
        ],
        [
            "Additional feature of area folks is that you can actually handle equality reason reasoning natively.",
            "In the semantic Web applications is a very common that you have data which different sources talking about the same entities under different names.",
            "So it is common that you require some way of saying that two resources are actually stand, stand for the same entity.",
            "This is used using the.",
            "This is done using our same as.",
            "The problem is that this equality is may not be just stated in the data, but you also may.",
            "Your background knowledge may somehow infer that individuals are the same.",
            "So equalities can be asserted by reasoning.",
            "In quality reasoning, however, is very expensive.",
            "How much time do I have?",
            "Equality reasoning is very expensive, and so there are common techniques that allow us to optimize the process.",
            "In particular, the rewriting technique is a well known one.",
            "What we did in our triple Air paper in 2015 is to provide an algorithm rewriting algorithm which which performs reasoning in parallel, which is another thing, and we also handle cases where rewritten where you have constants in the rules which are rewritten.",
            "Then we handle these cases in correctly.",
            "They did behind rewriting.",
            "I should have said sorry about that is that you.",
            "If you have a number of individuals which are known to be equal, you assign account representative and then you re write your data with respect to those representatives and then you reason only only in terms of representatives.",
            "Additionally, so now we have incremental algorithm for updating materialization.",
            "We have the rewriting technique for computing materialization sufficiently in the presence of equality.",
            "But so far there was no algorithm which can combine these two, so there was an algorithm which can update the materialization computed via rewriting.",
            "Incrementally, so this is we fill that gap in our indices.",
            "This each case version of.",
            "In a paper the decidueye this years each guy.",
            "So we propose the first algorithm which can deal with incremental updates of writings using equality.",
            "And we think that's implemented in early folks, so it's available to be used."
        ],
        [
            "In our previous evaluation, we've we've established that early folks performs reasonably well on middle and small and midrange servers.",
            "It can store up to 1.5 gigabytes.",
            "1.5 billion triples on 50 gigabytes of RAM.",
            "It can achieve up to 14 times speedup on 16 physical cores and performs efficient incremental reasoning, both in the case with equality without equality.",
            "In this paper, what we wanted to see is how are the folks actually can scale on a larger on the high end server.",
            "So what we did was to evaluate it at Oracle's Sparc T5 machine, which has 40 terabytes of RAM, 8 processors, and.",
            "In total, 200 and 9828 physical threats and 1024 virtual threads were hyper threading."
        ],
        [
            "The results that are in the paper.",
            "Show that at the time when we were testing cardio folks.",
            "And it couldn't run out of memory in cases on LBM in cases when we use more than 128 threads.",
            "We fixed the problem and repeated evaluation, so this is the numbers that I'm going to represent."
        ],
        [
            "Now.",
            "So we use the LBM, the well known benchmark.",
            "We also used Claris, which is an ontology that describes museum artifacts, and we all know what the PDAs.",
            "So we updated the pedia with and Clarice with axioms which.",
            "Somehow derive which are difficult to compute to evaluate.",
            "They derive a lot of facts.",
            "So because we want it really to test idea folks.",
            "To the limit.",
            "So we run our.",
            "They focus on one to 1000 threads, 24 threads and you can see that we achieved speedups of up to 213 times at Max maximum rate of derivation for 60 million triples per second.",
            "So this is reasoning speed.",
            "I'll just let that sink lower."
        ],
        [
            "OK, so we also wanted to test how our.",
            "How are you folks calls on scopes and scales with large data?",
            "So we tested earlier folks on 1000 to 2024 threads on various sizes.",
            "Full album.",
            "Scales more or less linearly.",
            "As expected, so there is no problem there.",
            "We could load materialized up to 22 billion triples."
        ],
        [
            "In conclusion are there folks is is is a versatile system.",
            "It has rich functionality and great performance and scalability.",
            "It has a flexible storage scheme and so which is highly efficient for parallel updates.",
            "Its employees state state of the art data lock reasoning algorithms and provides versatile modes of access.",
            "It is used to be suitable for data intensive applications which require a lot of which require a non non.",
            "Trivial reasoning and.",
            "We show that we demonstrated that we can store up to plenty to.",
            "In fact, billion triples, and that's the reason at speeds 60 million triples per second and achieve speedups of 230 times."
        ],
        [
            "What can you expect from Mario folks in the future?",
            "We are currently working on the query optimization algorithms in our system.",
            "Also we are extending our folks to full Sparkle 1.1.",
            "And we are thinking of how to extend it to support name graphs and reasoning with aggregation and non monotonic negation.",
            "And as I pointed out earlier, we are also distributing the system along on many many servers."
        ],
        [
            "Thank you.",
            "Thank you for the talk.",
            "Do we have questions?",
            "Hey, so one of your first lights.",
            "You mentioned that small server has hundreds of gigabytes of RAM in our University.",
            "Even the biggest server doesn't have 100 gigabytes of RAM, so look at you but.",
            "The principle that you have there is there any possibility that would work on machines which let's say 2040 gigabytes RAM and then swept from SSD for instance, or missed his beer in memory?",
            "Our current well, sorry to hear that you have servers don't have gigabytes of RAM.",
            "I mean personal computers have 16 gigabytes of RAM, so it's reasonable to expect that machine, which is called the server, should have at least a few times more than that.",
            "But that's not really the point that Firefox is really on main memory memory store, but we're not doing magic about this thing, so you can envisage situations in which we have.",
            "Persistent database which feeds into the main memory reasoner and and you can transfer data back and forth so you can visit different different scenarios.",
            "Which so we're not saying that everything should be in memory or everything should be in one in one on one machine.",
            "But this is if you want to.",
            "If you want to do it in memory on one machine, you can do it with earlier folks.",
            "Of course you can extend it and back to how much you can do with your question, how much you can actually do in a smaller computer.",
            "This morning I run some tests on my laptop with 16 gigabytes of RAM and I could load from 1000 for example and reason with materialize it in 30 seconds.",
            "So it is you can do nontrivial amount of work on small machines.",
            "Do we also did you compare it all to other triplestores, especially in terms of like query performance?",
            "So query performance.",
            "No, we haven't done it in a scientist rigorously.",
            "Scientific rigor, there are different.",
            "Triple stores our current focus with some materialization and reasoning, and as I pointed out in the previous slides, we are currently working on coming up with a very better query planning and then how do you compare on reasoning against any other store?",
            "We are better.",
            "So we compared with our Lim.",
            "When when it was used to be called like that, we also compared with the database.",
            "Based Triplestores and there is a comparison in our paper.",
            "AAA 2014 So we compared to not only with different stores, but with different approaches in terms of memory consumption and reasoning time.",
            "So if you want to have a comparison, look there.",
            "You said that Aria Box is available under an academic license.",
            "What does that mean for companies which would like to evaluate the system?",
            "Do they have to buy a license?",
            "Or you can evaluate the system as long as you don't make money out of it at this point, and then we have to contact our legal team and negotiate with them.",
            "What what do you support for Sparkle right now?",
            "Is it just maybe?",
            "GPS are used to optionals and filters with the optional some filters, yes.",
            "What don't you?",
            "I can't remember which features we aggregates we don't have, but from sparkle one point.",
            "So you cover everything.",
            "From Sparkle 1.0 you cover everything.",
            "I can't answer the question, sorry.",
            "I have to work it up.",
            "I'll tell you offline alright thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And more importantly, it also supports highly scalable reasoning.",
                    "label": 0
                },
                {
                    "sent": "Materialization based reasoning including incremental reasoning and handling.",
                    "label": 0
                },
                {
                    "sent": "Native handling of are same as.",
                    "label": 0
                },
                {
                    "sent": "We also implement some theoretical results from conjunctive query answering literature compositions.",
                    "label": 0
                },
                {
                    "sent": "In particular for those who know what that means.",
                    "label": 0
                },
                {
                    "sent": "Which which provides which allow us to provide an efficient query on string performance.",
                    "label": 0
                },
                {
                    "sent": "When we try to to make early folks as versatile as possible, to the best of our abilities.",
                    "label": 0
                },
                {
                    "sent": "So we currently we can.",
                    "label": 0
                },
                {
                    "sent": "You can use RDF folksy as a C, Java, Python library, or you can use it as a sparkle endpoint.",
                    "label": 0
                },
                {
                    "sent": "He's being developed with University of Oxford and she's available on the academic license.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the core of the system, we have a very highly flexible and and scalable storage scheme.",
                    "label": 0
                },
                {
                    "sent": "The key feature that storage scheme is that you can import data in parallel efficiently in a lock free fashion.",
                    "label": 0
                },
                {
                    "sent": "Triples is stored in a triple table, which is common for RDF stores.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's the store supports configurable indexing scheme.",
                    "label": 0
                },
                {
                    "sent": "The default, the default indexing scheme is shown on screen.",
                    "label": 0
                },
                {
                    "sent": "Depends, but it is configurable so your developers can can can easily plug in their own indexing scheme of choice.",
                    "label": 0
                },
                {
                    "sent": "As I said, the crucial property of this car storage screen is that it supports mostly lock free.",
                    "label": 0
                },
                {
                    "sent": "Updates so different threads can import triples into the store simultaneously almost without any locking.",
                    "label": 0
                },
                {
                    "sent": "This is crucial for if you want to import for scalability in cases where you want to import data from different sources or during parallel reasoning where triples derived where threads derive triples which have to be inserted into the store.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The most appealing feature of data of Arctic Fox is its data look reasoning capabilities it supports.",
                    "label": 0
                },
                {
                    "sent": "I eat incorporates a materialization based data lock engine which implements state of the art reasoning algorithms.",
                    "label": 0
                },
                {
                    "sent": "For those that don't know what data locate state along his room based language which can capture outer Elance were rules, you should be more familiar with that I hope.",
                    "label": 0
                },
                {
                    "sent": "In particular, will Sir just a syntactic variant of Datalog, cruise and for outdoor L Intelligence you can turn any outdoor Ellen Teologi into a set of rules and reason with those data La Cruz.",
                    "label": 0
                },
                {
                    "sent": "Or you can use the fixed.",
                    "label": 0
                },
                {
                    "sent": "They told program specified in the outer specification to reason with any ontology specified as triples.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are you folks, yeah.",
                    "label": 0
                },
                {
                    "sent": "Takes the materialization based approach so it performs reasoning via materialization.",
                    "label": 0
                },
                {
                    "sent": "Which essentially means that you precompute or explicate all the consequences of your data and knowledge and the background knowledge that is specified as an ontology.",
                    "label": 0
                },
                {
                    "sent": "And then you answer via queries against the explicated.",
                    "label": 0
                },
                {
                    "sent": "Consequences this allows for efficient query answering without the need of reasoning during query evaluation.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that are the Fox employees is A is a novel algorithm.",
                    "label": 0
                },
                {
                    "sent": "It is a shared memory parallel algorithm which represented the triple AI last year.",
                    "label": 0
                },
                {
                    "sent": "It is a AAA time variant of the same evaluation algorithm for people that know what that is and to the crucial properties that you don't repeat work in effect, every rule instantiation is considered at most once.",
                    "label": 1
                },
                {
                    "sent": "The interesting part from parallelization POV regarding reasoning is that the reasoning task is split evenly.",
                    "label": 0
                },
                {
                    "sent": "In a very, very small subtasks, in particular, 1 task sub task per triple.",
                    "label": 0
                },
                {
                    "sent": "This allows for this fine grained partitioning of the reasoning that allows for high scalability.",
                    "label": 0
                },
                {
                    "sent": "Without the need of explicit load balancing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data on the web changes a lot.",
                    "label": 0
                },
                {
                    "sent": "We all know that we have.",
                    "label": 0
                },
                {
                    "sent": "We had workshops and tutorials on that topic, stringstream reasoning, and so on.",
                    "label": 0
                },
                {
                    "sent": "The artifact solution to this.",
                    "label": 0
                },
                {
                    "sent": "Ever changing data is an incremental reasoning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which it implements.",
                    "label": 0
                },
                {
                    "sent": "The idea behind incremental reasoning algorithms is not to recompute everything from scratch, but once you have your materialization when the data changes, then you update the materialization's accordingly in an efficient way.",
                    "label": 0
                },
                {
                    "sent": "We used an algorithm we just got FBF which stands for forward, backward and forward.",
                    "label": 0
                },
                {
                    "sent": "Which involves interesting interaction between far forward, backward and forward chaining reasoning.",
                    "label": 0
                },
                {
                    "sent": "The important property of this algorithm is that it doesn't require any additional information gathered by the initial materialization, but so we don't store any counts or dependencies between facts or proofs for that matter.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is an improvement of the well known direct algorithm from database community and the improvements come from the fact comes from the fact that we don't over delete but rather delete, perform exact deletions.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I can't get into more detail at this point.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Additional feature of area folks is that you can actually handle equality reason reasoning natively.",
                    "label": 0
                },
                {
                    "sent": "In the semantic Web applications is a very common that you have data which different sources talking about the same entities under different names.",
                    "label": 1
                },
                {
                    "sent": "So it is common that you require some way of saying that two resources are actually stand, stand for the same entity.",
                    "label": 0
                },
                {
                    "sent": "This is used using the.",
                    "label": 0
                },
                {
                    "sent": "This is done using our same as.",
                    "label": 0
                },
                {
                    "sent": "The problem is that this equality is may not be just stated in the data, but you also may.",
                    "label": 0
                },
                {
                    "sent": "Your background knowledge may somehow infer that individuals are the same.",
                    "label": 0
                },
                {
                    "sent": "So equalities can be asserted by reasoning.",
                    "label": 0
                },
                {
                    "sent": "In quality reasoning, however, is very expensive.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Equality reasoning is very expensive, and so there are common techniques that allow us to optimize the process.",
                    "label": 0
                },
                {
                    "sent": "In particular, the rewriting technique is a well known one.",
                    "label": 0
                },
                {
                    "sent": "What we did in our triple Air paper in 2015 is to provide an algorithm rewriting algorithm which which performs reasoning in parallel, which is another thing, and we also handle cases where rewritten where you have constants in the rules which are rewritten.",
                    "label": 0
                },
                {
                    "sent": "Then we handle these cases in correctly.",
                    "label": 0
                },
                {
                    "sent": "They did behind rewriting.",
                    "label": 0
                },
                {
                    "sent": "I should have said sorry about that is that you.",
                    "label": 0
                },
                {
                    "sent": "If you have a number of individuals which are known to be equal, you assign account representative and then you re write your data with respect to those representatives and then you reason only only in terms of representatives.",
                    "label": 0
                },
                {
                    "sent": "Additionally, so now we have incremental algorithm for updating materialization.",
                    "label": 0
                },
                {
                    "sent": "We have the rewriting technique for computing materialization sufficiently in the presence of equality.",
                    "label": 0
                },
                {
                    "sent": "But so far there was no algorithm which can combine these two, so there was an algorithm which can update the materialization computed via rewriting.",
                    "label": 0
                },
                {
                    "sent": "Incrementally, so this is we fill that gap in our indices.",
                    "label": 0
                },
                {
                    "sent": "This each case version of.",
                    "label": 0
                },
                {
                    "sent": "In a paper the decidueye this years each guy.",
                    "label": 0
                },
                {
                    "sent": "So we propose the first algorithm which can deal with incremental updates of writings using equality.",
                    "label": 1
                },
                {
                    "sent": "And we think that's implemented in early folks, so it's available to be used.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our previous evaluation, we've we've established that early folks performs reasonably well on middle and small and midrange servers.",
                    "label": 0
                },
                {
                    "sent": "It can store up to 1.5 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "1.5 billion triples on 50 gigabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "It can achieve up to 14 times speedup on 16 physical cores and performs efficient incremental reasoning, both in the case with equality without equality.",
                    "label": 0
                },
                {
                    "sent": "In this paper, what we wanted to see is how are the folks actually can scale on a larger on the high end server.",
                    "label": 0
                },
                {
                    "sent": "So what we did was to evaluate it at Oracle's Sparc T5 machine, which has 40 terabytes of RAM, 8 processors, and.",
                    "label": 0
                },
                {
                    "sent": "In total, 200 and 9828 physical threats and 1024 virtual threads were hyper threading.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results that are in the paper.",
                    "label": 0
                },
                {
                    "sent": "Show that at the time when we were testing cardio folks.",
                    "label": 0
                },
                {
                    "sent": "And it couldn't run out of memory in cases on LBM in cases when we use more than 128 threads.",
                    "label": 0
                },
                {
                    "sent": "We fixed the problem and repeated evaluation, so this is the numbers that I'm going to represent.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So we use the LBM, the well known benchmark.",
                    "label": 0
                },
                {
                    "sent": "We also used Claris, which is an ontology that describes museum artifacts, and we all know what the PDAs.",
                    "label": 0
                },
                {
                    "sent": "So we updated the pedia with and Clarice with axioms which.",
                    "label": 0
                },
                {
                    "sent": "Somehow derive which are difficult to compute to evaluate.",
                    "label": 0
                },
                {
                    "sent": "They derive a lot of facts.",
                    "label": 0
                },
                {
                    "sent": "So because we want it really to test idea folks.",
                    "label": 0
                },
                {
                    "sent": "To the limit.",
                    "label": 0
                },
                {
                    "sent": "So we run our.",
                    "label": 0
                },
                {
                    "sent": "They focus on one to 1000 threads, 24 threads and you can see that we achieved speedups of up to 213 times at Max maximum rate of derivation for 60 million triples per second.",
                    "label": 0
                },
                {
                    "sent": "So this is reasoning speed.",
                    "label": 0
                },
                {
                    "sent": "I'll just let that sink lower.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we also wanted to test how our.",
                    "label": 0
                },
                {
                    "sent": "How are you folks calls on scopes and scales with large data?",
                    "label": 0
                },
                {
                    "sent": "So we tested earlier folks on 1000 to 2024 threads on various sizes.",
                    "label": 0
                },
                {
                    "sent": "Full album.",
                    "label": 0
                },
                {
                    "sent": "Scales more or less linearly.",
                    "label": 0
                },
                {
                    "sent": "As expected, so there is no problem there.",
                    "label": 0
                },
                {
                    "sent": "We could load materialized up to 22 billion triples.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In conclusion are there folks is is is a versatile system.",
                    "label": 0
                },
                {
                    "sent": "It has rich functionality and great performance and scalability.",
                    "label": 0
                },
                {
                    "sent": "It has a flexible storage scheme and so which is highly efficient for parallel updates.",
                    "label": 0
                },
                {
                    "sent": "Its employees state state of the art data lock reasoning algorithms and provides versatile modes of access.",
                    "label": 0
                },
                {
                    "sent": "It is used to be suitable for data intensive applications which require a lot of which require a non non.",
                    "label": 0
                },
                {
                    "sent": "Trivial reasoning and.",
                    "label": 0
                },
                {
                    "sent": "We show that we demonstrated that we can store up to plenty to.",
                    "label": 0
                },
                {
                    "sent": "In fact, billion triples, and that's the reason at speeds 60 million triples per second and achieve speedups of 230 times.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What can you expect from Mario folks in the future?",
                    "label": 0
                },
                {
                    "sent": "We are currently working on the query optimization algorithms in our system.",
                    "label": 0
                },
                {
                    "sent": "Also we are extending our folks to full Sparkle 1.1.",
                    "label": 0
                },
                {
                    "sent": "And we are thinking of how to extend it to support name graphs and reasoning with aggregation and non monotonic negation.",
                    "label": 0
                },
                {
                    "sent": "And as I pointed out earlier, we are also distributing the system along on many many servers.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the talk.",
                    "label": 0
                },
                {
                    "sent": "Do we have questions?",
                    "label": 0
                },
                {
                    "sent": "Hey, so one of your first lights.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that small server has hundreds of gigabytes of RAM in our University.",
                    "label": 0
                },
                {
                    "sent": "Even the biggest server doesn't have 100 gigabytes of RAM, so look at you but.",
                    "label": 0
                },
                {
                    "sent": "The principle that you have there is there any possibility that would work on machines which let's say 2040 gigabytes RAM and then swept from SSD for instance, or missed his beer in memory?",
                    "label": 0
                },
                {
                    "sent": "Our current well, sorry to hear that you have servers don't have gigabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "I mean personal computers have 16 gigabytes of RAM, so it's reasonable to expect that machine, which is called the server, should have at least a few times more than that.",
                    "label": 0
                },
                {
                    "sent": "But that's not really the point that Firefox is really on main memory memory store, but we're not doing magic about this thing, so you can envisage situations in which we have.",
                    "label": 0
                },
                {
                    "sent": "Persistent database which feeds into the main memory reasoner and and you can transfer data back and forth so you can visit different different scenarios.",
                    "label": 0
                },
                {
                    "sent": "Which so we're not saying that everything should be in memory or everything should be in one in one on one machine.",
                    "label": 0
                },
                {
                    "sent": "But this is if you want to.",
                    "label": 0
                },
                {
                    "sent": "If you want to do it in memory on one machine, you can do it with earlier folks.",
                    "label": 0
                },
                {
                    "sent": "Of course you can extend it and back to how much you can do with your question, how much you can actually do in a smaller computer.",
                    "label": 0
                },
                {
                    "sent": "This morning I run some tests on my laptop with 16 gigabytes of RAM and I could load from 1000 for example and reason with materialize it in 30 seconds.",
                    "label": 0
                },
                {
                    "sent": "So it is you can do nontrivial amount of work on small machines.",
                    "label": 0
                },
                {
                    "sent": "Do we also did you compare it all to other triplestores, especially in terms of like query performance?",
                    "label": 0
                },
                {
                    "sent": "So query performance.",
                    "label": 0
                },
                {
                    "sent": "No, we haven't done it in a scientist rigorously.",
                    "label": 0
                },
                {
                    "sent": "Scientific rigor, there are different.",
                    "label": 0
                },
                {
                    "sent": "Triple stores our current focus with some materialization and reasoning, and as I pointed out in the previous slides, we are currently working on coming up with a very better query planning and then how do you compare on reasoning against any other store?",
                    "label": 0
                },
                {
                    "sent": "We are better.",
                    "label": 0
                },
                {
                    "sent": "So we compared with our Lim.",
                    "label": 0
                },
                {
                    "sent": "When when it was used to be called like that, we also compared with the database.",
                    "label": 0
                },
                {
                    "sent": "Based Triplestores and there is a comparison in our paper.",
                    "label": 0
                },
                {
                    "sent": "AAA 2014 So we compared to not only with different stores, but with different approaches in terms of memory consumption and reasoning time.",
                    "label": 1
                },
                {
                    "sent": "So if you want to have a comparison, look there.",
                    "label": 0
                },
                {
                    "sent": "You said that Aria Box is available under an academic license.",
                    "label": 0
                },
                {
                    "sent": "What does that mean for companies which would like to evaluate the system?",
                    "label": 0
                },
                {
                    "sent": "Do they have to buy a license?",
                    "label": 0
                },
                {
                    "sent": "Or you can evaluate the system as long as you don't make money out of it at this point, and then we have to contact our legal team and negotiate with them.",
                    "label": 0
                },
                {
                    "sent": "What what do you support for Sparkle right now?",
                    "label": 1
                },
                {
                    "sent": "Is it just maybe?",
                    "label": 0
                },
                {
                    "sent": "GPS are used to optionals and filters with the optional some filters, yes.",
                    "label": 0
                },
                {
                    "sent": "What don't you?",
                    "label": 0
                },
                {
                    "sent": "I can't remember which features we aggregates we don't have, but from sparkle one point.",
                    "label": 0
                },
                {
                    "sent": "So you cover everything.",
                    "label": 0
                },
                {
                    "sent": "From Sparkle 1.0 you cover everything.",
                    "label": 0
                },
                {
                    "sent": "I can't answer the question, sorry.",
                    "label": 0
                },
                {
                    "sent": "I have to work it up.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you offline alright thanks.",
                    "label": 0
                }
            ]
        }
    }
}