{
    "id": "ly4dj6k5v2xo5xfthzlxbcywlgcmuodc",
    "title": "Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints",
    "info": {
        "author": [
            "Karthik Sridharan, Department of Computer Science, Cornell University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_sridharan_online_prediction/",
    "segmentation": [
        [
            "This is joint work with Sasha Rakhlin.",
            "So I'll start with any."
        ],
        [
            "Example of a Canonical problem that we're going to look at, and I'll tell you about the general set up in the following slides, but this is the problem that you want to have in mind.",
            "So think about node classification problem.",
            "It's an online node classification problem where you have a bunch of users, so at time, so basically we have played this for three rounds on the next."
        ],
        [
            "Down there is a new person who comes in with some side information XD and we want to basically and once this person comes in this."
        ],
        [
            "And make some connections with the existing people in this network.",
            "So here Brown means it's full and green means it's friend Ann.",
            "Now what we want to do is."
        ],
        [
            "Based on this information, we want to predictive."
        ],
        [
            "This person is going to be a blue or red belong to the blue or red class."
        ],
        [
            "And what our goal is is to minimize regret, except that there's a slight twist here in terms of what we compare our goal against.",
            "You want to make sure that our loss over the if you play 4 V rounds is not much more than the loss of the best competitor.",
            "But notice here that the competitor F is actually dependent on the data.",
            "So for instance, I could basically say that the number of times friends or disagree, or the number of times enemies agree has to be less than or equal to K. And this all this.",
            "This labelings I can only find out after I've gotten all the data after I've gotten all the links and the classes of the other people right, and the main thing to notice here is oftentimes this guy here what we compare with is going to be NP hard, so it's going to be computationally hard, OK and."
        ],
        [
            "The more general setup that I promised this as follows, so the more general setup.",
            "Instead of thinking of just edges, you can essentially think of, for instance, hypergraph or more so a time T. We're not just given the side information we get given a set of constraints, capital C, sub B and we could basically have a multi label problem.",
            "So we can essentially have Kappa number of labels and we are interested in the same thing to put some ground under your feet thus."
        ],
        [
            "Civic form of constraints that we're going to consider are as follows.",
            "So for those of you who are familiar with the CSP's constraint satisfaction problem, it's basically the same flavor.",
            "So a constraint depends on a subset SC of this of the items that we want to predict on, and some cost function on the labelings of this guy.",
            "So for instance, in the graph example given some labeling, we basically SC would be the set of edges, so we are given the edges that we connect connect in the graph and this one basically says that if two friends disagree, then we're going to pay.",
            "A cost of one for it.",
            "OK, and we're interested in competing with this benchmark.",
            "There is all the labelings such that the constraint satisfaction is less than or equal to K or the constraint violation is less than or equal to K and we're going to make the assumption that we know the stochastic model from which the graph is generated.",
            "For instance, if we might know that the graph is generated by.",
            "By geometry creandum, graph model or any such show.",
            "So basically we know how this stochastic model generating the constraints and."
        ],
        [
            "Remind you of the goal we are interested in.",
            "Essentially, the notion of regret here.",
            "And as I mentioned, for most of the problems that we are interested in, this.",
            "Even if I gave you all the data.",
            "So in the end of the veyrons I give you all the data and I basically tell you find me the best up.",
            "Sorry find me the best model from this model class and it turns out that most of these problems the finding such a solution is actually NP hard.",
            "But we are in.",
            "We are still interested in basically having a regret bound with respect to one times the optimal to go to zero and the question is can we do this?",
            "And it turns out that we can actually do this efficiently, and they shouldn't come as a surprise, because essentially we're allowed to do improper learning.",
            "That is, we don't necessarily need to produce our labels.",
            "Also, from this particular model, we're just interested in doing as well as this particular model, and there's a bunch of examples, both in Statistica learning and online learning that do this, and basically what our results are going to show is that if you have an SDP relaxation that solves this efficiently with some approximation guarantee, then you can actually.",
            "Push this regret down to zero, or push the average regret down to 0.",
            "OK so I."
        ],
        [
            "Directly jump to the algorithm for the specific example that I introduced before.",
            "So you are asked to predict what this person is.",
            "And remember, we made the assumption that I can.",
            "Essentially, I know the stochastic model for this, generating this graph, so I'm going to basically hallucinate the rest of the graph.",
            "So I'm going to draw the rest of the graph from this model, and I'm going to basically."
        ],
        [
            "Color these guys randomly, so I'm going to."
        ],
        [
            "Coins and either put them red or blue according to this distribution.",
            "And now I make 2 copies of this graph, one in which this."
        ],
        [
            "Right here is blue one in the other in which this guy here is red.",
            "OK, next I take the class."
        ],
        [
            "That I'm interested in and I'm going to go to an SDP.",
            "Relaxation of this class specifically in the paper we use Lesia hierarchy, which gives a hair arkie of relaxation.",
            "So the higher up the hierarchy you go, the more time you spend.",
            "But you get better approximation guarantees and what we're going to do is we're going to fit the."
        ],
        [
            "This model for this graph here and the best model for this graph here, and we're just going to be interested in the SDP values.",
            "So we look at the ZP value for this graph.",
            "Out of this model and there's the P value for this graph out of this model and this once we normalize this difference, that's going to be the probability of whether we're going to predict red, blue and for the multi label case.",
            "For more complex class is essentially the same thing works, except that for each class or for each of the.",
            "Kappa labels we're going to essentially calculate 1 SDP and then we have an efficient way of calculating the final distribution over labels.",
            "OK, so here's the."
        ],
        [
            "Main result an let me kind of walk you through this.",
            "Basically what it says is that the expected regret.",
            "OK so the expected regret is upper bounded by the gap of this SDP times the optimal regret bounds.",
            "So this is the Rademacher complexity, and it turns out that this is the optimal regret bound.",
            "And so that's the main result which says that instead of one time job, it's instead of gap time stop you can compete with one time stopped and pay basically the gap in terms of the optimal regret form, and I guess."
        ],
        [
            "Out of time.",
            "So I'm going to leave the summary of them.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Sasha Rakhlin.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with any.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example of a Canonical problem that we're going to look at, and I'll tell you about the general set up in the following slides, but this is the problem that you want to have in mind.",
                    "label": 0
                },
                {
                    "sent": "So think about node classification problem.",
                    "label": 0
                },
                {
                    "sent": "It's an online node classification problem where you have a bunch of users, so at time, so basically we have played this for three rounds on the next.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down there is a new person who comes in with some side information XD and we want to basically and once this person comes in this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And make some connections with the existing people in this network.",
                    "label": 0
                },
                {
                    "sent": "So here Brown means it's full and green means it's friend Ann.",
                    "label": 0
                },
                {
                    "sent": "Now what we want to do is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on this information, we want to predictive.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This person is going to be a blue or red belong to the blue or red class.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what our goal is is to minimize regret, except that there's a slight twist here in terms of what we compare our goal against.",
                    "label": 0
                },
                {
                    "sent": "You want to make sure that our loss over the if you play 4 V rounds is not much more than the loss of the best competitor.",
                    "label": 0
                },
                {
                    "sent": "But notice here that the competitor F is actually dependent on the data.",
                    "label": 0
                },
                {
                    "sent": "So for instance, I could basically say that the number of times friends or disagree, or the number of times enemies agree has to be less than or equal to K. And this all this.",
                    "label": 0
                },
                {
                    "sent": "This labelings I can only find out after I've gotten all the data after I've gotten all the links and the classes of the other people right, and the main thing to notice here is oftentimes this guy here what we compare with is going to be NP hard, so it's going to be computationally hard, OK and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The more general setup that I promised this as follows, so the more general setup.",
                    "label": 0
                },
                {
                    "sent": "Instead of thinking of just edges, you can essentially think of, for instance, hypergraph or more so a time T. We're not just given the side information we get given a set of constraints, capital C, sub B and we could basically have a multi label problem.",
                    "label": 0
                },
                {
                    "sent": "So we can essentially have Kappa number of labels and we are interested in the same thing to put some ground under your feet thus.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Civic form of constraints that we're going to consider are as follows.",
                    "label": 0
                },
                {
                    "sent": "So for those of you who are familiar with the CSP's constraint satisfaction problem, it's basically the same flavor.",
                    "label": 0
                },
                {
                    "sent": "So a constraint depends on a subset SC of this of the items that we want to predict on, and some cost function on the labelings of this guy.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the graph example given some labeling, we basically SC would be the set of edges, so we are given the edges that we connect connect in the graph and this one basically says that if two friends disagree, then we're going to pay.",
                    "label": 0
                },
                {
                    "sent": "A cost of one for it.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're interested in competing with this benchmark.",
                    "label": 0
                },
                {
                    "sent": "There is all the labelings such that the constraint satisfaction is less than or equal to K or the constraint violation is less than or equal to K and we're going to make the assumption that we know the stochastic model from which the graph is generated.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we might know that the graph is generated by.",
                    "label": 0
                },
                {
                    "sent": "By geometry creandum, graph model or any such show.",
                    "label": 0
                },
                {
                    "sent": "So basically we know how this stochastic model generating the constraints and.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Remind you of the goal we are interested in.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the notion of regret here.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, for most of the problems that we are interested in, this.",
                    "label": 0
                },
                {
                    "sent": "Even if I gave you all the data.",
                    "label": 1
                },
                {
                    "sent": "So in the end of the veyrons I give you all the data and I basically tell you find me the best up.",
                    "label": 1
                },
                {
                    "sent": "Sorry find me the best model from this model class and it turns out that most of these problems the finding such a solution is actually NP hard.",
                    "label": 0
                },
                {
                    "sent": "But we are in.",
                    "label": 0
                },
                {
                    "sent": "We are still interested in basically having a regret bound with respect to one times the optimal to go to zero and the question is can we do this?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that we can actually do this efficiently, and they shouldn't come as a surprise, because essentially we're allowed to do improper learning.",
                    "label": 0
                },
                {
                    "sent": "That is, we don't necessarily need to produce our labels.",
                    "label": 0
                },
                {
                    "sent": "Also, from this particular model, we're just interested in doing as well as this particular model, and there's a bunch of examples, both in Statistica learning and online learning that do this, and basically what our results are going to show is that if you have an SDP relaxation that solves this efficiently with some approximation guarantee, then you can actually.",
                    "label": 1
                },
                {
                    "sent": "Push this regret down to zero, or push the average regret down to 0.",
                    "label": 0
                },
                {
                    "sent": "OK so I.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directly jump to the algorithm for the specific example that I introduced before.",
                    "label": 0
                },
                {
                    "sent": "So you are asked to predict what this person is.",
                    "label": 0
                },
                {
                    "sent": "And remember, we made the assumption that I can.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I know the stochastic model for this, generating this graph, so I'm going to basically hallucinate the rest of the graph.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to draw the rest of the graph from this model, and I'm going to basically.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Color these guys randomly, so I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coins and either put them red or blue according to this distribution.",
                    "label": 0
                },
                {
                    "sent": "And now I make 2 copies of this graph, one in which this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right here is blue one in the other in which this guy here is red.",
                    "label": 0
                },
                {
                    "sent": "OK, next I take the class.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I'm interested in and I'm going to go to an SDP.",
                    "label": 0
                },
                {
                    "sent": "Relaxation of this class specifically in the paper we use Lesia hierarchy, which gives a hair arkie of relaxation.",
                    "label": 0
                },
                {
                    "sent": "So the higher up the hierarchy you go, the more time you spend.",
                    "label": 0
                },
                {
                    "sent": "But you get better approximation guarantees and what we're going to do is we're going to fit the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model for this graph here and the best model for this graph here, and we're just going to be interested in the SDP values.",
                    "label": 0
                },
                {
                    "sent": "So we look at the ZP value for this graph.",
                    "label": 0
                },
                {
                    "sent": "Out of this model and there's the P value for this graph out of this model and this once we normalize this difference, that's going to be the probability of whether we're going to predict red, blue and for the multi label case.",
                    "label": 0
                },
                {
                    "sent": "For more complex class is essentially the same thing works, except that for each class or for each of the.",
                    "label": 0
                },
                {
                    "sent": "Kappa labels we're going to essentially calculate 1 SDP and then we have an efficient way of calculating the final distribution over labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Main result an let me kind of walk you through this.",
                    "label": 0
                },
                {
                    "sent": "Basically what it says is that the expected regret.",
                    "label": 0
                },
                {
                    "sent": "OK so the expected regret is upper bounded by the gap of this SDP times the optimal regret bounds.",
                    "label": 0
                },
                {
                    "sent": "So this is the Rademacher complexity, and it turns out that this is the optimal regret bound.",
                    "label": 1
                },
                {
                    "sent": "And so that's the main result which says that instead of one time job, it's instead of gap time stop you can compete with one time stopped and pay basically the gap in terms of the optimal regret form, and I guess.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of time.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to leave the summary of them.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}