{
    "id": "t2rv2rxllhuw2jdynzkoolwohohy3iw2",
    "title": "Tailoring Density Estimation via Reproducing Kernel Moment Matching",
    "info": {
        "author": [
            "Xinhua Zhang, NICTA, Australia's ICT Research Centre of Excellence"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_zhang_tde/",
    "segmentation": [
        [
            "OK, so welcome to the kernel session and the 1st paper is tailoring density estimation by reproducing kernel moment matching and it will be presented by Jean Holzem.",
            "So.",
            "So thanks thanks Professor Williams.",
            "So I guess I can stop now this this talk is about density estimation and the objective file paper is to tailor the dance information such that we can better estimate the expectation of family of functions which are known a priori and our approach is based on reproducing kernel moment matching.",
            "This work is bioli, so me and Alex smaller from Victor of Australia and also gratton Benasher call from.",
            "MPI Tubingen so."
        ],
        [
            "First, this talk will be divided into 3 sections.",
            "Our first to motivate our algorithm and give the estimation bounds.",
            "Then I will show how our algorithm can be formulated into a simple quadratic programming.",
            "And finally I will give the experimental."
        ],
        [
            "Doubts before conclusion.",
            "So first motivation.",
            "Observe that in many applications that involve density estimation, the ultimate goal is normally not density estimation.",
            "For example, we may be in we.",
            "We are often interested in the expectation of a random variable or the function of a random random variable.",
            "For example, in parameter estimation for graphical models, especially undirected graphical models, in the using exponential family, if we use gradient descent, then the gradients are essentially the expectation of the sufficient statistics.",
            "So it is not clear whether doing maximum likelihood is the ideal criterion for density estimation.",
            "So now we are faced with a choice.",
            "On one side we can do full density estimation by maximum likelihood estimation and prepared for calculating the expectation for arbitrary subsequent functions.",
            "Or we can focus just on the approximating the expectation of a set of functions known a priori.",
            "So putting this intuition in a formal way, our paper is our paper deals with the following problem.",
            "So given a distribution P Anna function set curly F, please find us distribution petel dot such that the difference, the difference which.",
            "Ahnapee is small for any function in any function.",
            "Small F in the function set curly F."
        ],
        [
            "So we put it other way.",
            "We want the soup of the mean discrepancy to be small."
        ],
        [
            "So now let's take a very short digress to see that the idea of using such touchstone function classes have been widely used in research.",
            "The same spirit has been used to define the weak convergence of probability measures.",
            "So the probability measure mu N is called convert to converges weakly to mu if the expectation of functions converges for all the functions which are real valued continuous in a bounded.",
            "Read the Euclidean space.",
            "So the next example is the independence test criteria by running into 1959 he said that for sufficiently rich function classes, the function correlation or cross covariance serves as an independence test and the work that is most is most close to our work.",
            "Our paper is by Joshua Taylor and Alex to layer in there a SS 2007 paper.",
            "They dealt with density estimation as well.",
            "They measure the loss on a set of randomly drawn touchstone functions, so their optimization requires drawing samples from a set of touchstone functions, and they require characterization of the complexity of this function set.",
            "So in our approach we avoid these two advantage as we will show later."
        ],
        [
            "So let's come back to make the algorithm concrete, we need to specify what function class we need.",
            "We are interesting, so in this paper we pick the functions in the reproducing kernel higher office space with whose norm is less than or equal to 1.",
            "So we have a kernel.",
            "We have our key actress, so this is the mean discrepancy and very Interestingly."
        ],
        [
            "This soup can be represented is equal to the occassion distance between 2 mean elements, namely mu P annum upto Elder.",
            "Them UP is defined as the expectation of the evaluation element KX dot with respect to the distribution P. So the key idea of our approach is to embed distribution P and our estimated distribution peaked Elder into.",
            "The arcade chess and one naturally expects that the P&P told are closed if and only if the architectures distance between their corresponding mean mapping.",
            "New P~ random UP is closed are close, so we can also equally well motivate algorithm.",
            "I'll algorithm by kernel mean mapping which is hinted in the hour.",
            "Paper last year."
        ],
        [
            "So this last slide gives the most important theoretical the theorem in our paper and justifies our approach approach.",
            "So given a real density function density P, we can have a corresponding mean element mupi.",
            "But in practice we never have the full knowledge of P. What we have is just a sample Jaune, Jaune independently and identically from frumpy.",
            "And we use this saragat mean element name, also called the empirical empirical mean element mu X defined as the average of KXI dot.",
            "And we have we are looking for the estimated density to decrease, which confounds to mean element mu, P~ So to do the as density estimation.",
            "What we ultimately want to minimize is the distance between UP and mupi tailed.",
            "However, we have no knowledge of new piece, so we result to the triangle inequality that is the sum of the red distance plus the black distance.",
            "The black distances beyond our beyond our control, so we characterize it by using the right module average and the red distance is within our control.",
            "The new X is computable anamu P~ is can be optimized by our.",
            "Optimize us, so in essence we minimize the render distance in practice as a proxy distance between the blue distance between UP~ MUP.",
            "So now let's state our main theorem, so with a with a high probability, the distance between UP and mutate all the mean discrepancy is upper bounded by the mean discrepancy of new PX and mupi teleda.",
            "Plus a random Archer average plus the tolerance.",
            "That's the key reason why we minimize this mean discrepancy in practice."
        ],
        [
            "And so in the next slide I will show that this minimalization can be carried out using an efficient optimization.",
            "So let's suppose that the Pete Elder can be represented as a mixture model with P prototype spy fixed an Alpha.",
            "I being in the M dimensional probability simplex and we can share with these that this minimalization problem over the Pete Elder can be posed as.",
            "A quadratic programming where this elements of Q&A L can be computed by the expectations and control closed from family exist for Qi and Qi, Gemelli and various combinations of prototypes and kernels.",
            "The details can be found in our."
        ],
        [
            "Paper.",
            "OK, next the last section is about experimental results we used with three sets of experiments.",
            "the UI data set on which we show that our algorithm really better estimates the expectation of functions than other algorithms.",
            "Then we apply the estimator to two applications, namely message passing, compression an image retrieve."
        ],
        [
            "People.",
            "The first UCI data set with the objective of this experiment is to share that our density estimate outperforms in terms of estimating the expectation of functions which are in the architectures, however, does not necessarily outperforming outperform other algorithms in terms of log likelihood.",
            "So the algorithms under comparison are kernel moment matching which is ours and Gaussian mixture model passing window and reduced sets density estimation.",
            "The comparison is made in the following way.",
            "We first randomly generate a function F from the IP address.",
            "Then we use half of the data set, top the data to give a desk estimation of Pete Elder, and use that Pete Elda to compute the expectation of F. Then we use the rest half data to compute the empirical average of F. Finally, we report the relative discrepancy.",
            "The whole process is repeated for 100 times, so so that we can do a pad."
        ],
        [
            "Sign test so this table gives a result of function expectation estimation.",
            "We can see that if we are interested in mixture of RBF functions, then by using RBF kernel our algorithm can give consistently lower error than all other algorithms.",
            "The blue number means that the corresponding algorithm gives lower lower error then other the other algorithms.",
            "With significance level significantly less than .01.",
            "And the number reported is just the median of the 100 repetitions, and similarly for polynomial functions of degree 3.",
            "All our algorithm using polynomial kernel with degree three gives lower error more frequently than other algae."
        ],
        [
            "Prisms.",
            "And similarly, for polynomial functions of degree two and four linear functions.",
            "Because puzzle window and Gaussian mixture, they also match the 1st order moment.",
            "So our algorithm our density estimator using linear kernel gives always give the same result.",
            "As the people we know in the."
        ],
        [
            "Mixture.",
            "As expected, so next we see the first application, which is the message passing compression.",
            "The task is just doing filtering.",
            "Suppose we have temporal system.",
            "The current state St is a function of the previous state S, T -- 1 + A noise, and the current observation YT is a function of the current state current observations.",
            "The function of current state plus a noise, and here we assume that the transition noise sizes.",
            "Mixture of five Gaussians.",
            "The task is to do filtering to compute the posterior probability of the current state given all the observations up to current time step.",
            "The key idea is to recursively estimate to the filtering density and then followed by various rule.",
            "So the key step here and of our interest is that the P of S T + 1 given Y capital YT is can be expressed as the integral, and especially in the expectation this function.",
            "This PST plus one given St is exactly a function in the archaeologist thanks to the definition of our.",
            "Transition model and the noise here.",
            "This PST, given YT, is exactly the density to be estimated, so this density can be very complicated in practice and people approximate it by using particles and this big cause, this expectation falls perfectly into our framework.",
            "We are allowed to compress the particles by using our kernel moment matching."
        ],
        [
            "So this is the result of root mean square error and standard deviation of the far filtering result before and after particle compression.",
            "We can see that our kernel moment matching gives consistently lower or similar error than all the other competitor Al Gore."
        ],
        [
            "Items.",
            "The second application we did is image retrieval.",
            "The task is supposed to be given an image data set D and as a major P. Please retrieve from the data set DA set of images that are similar to P. The idea is on each image we first perform density estimation over the feature distribution and then we retrieve by ranking dissimilarity P the given image P and all the images Q in the data in the database.",
            "The dissimilarity measure we use is the earth movers distance by Reuben ATL and it can be applied on the mixture of Gaussians."
        ],
        [
            "So this is the experimental result.",
            "The horizontal on the scene of the class of each data set.",
            "We have.",
            "The horizontal access is number of retrieved images.",
            "Enter the vertical access gives the signed log P value of parasite test.",
            "So just figuratively I can tell you that when the number is less than minus two then KMM the.",
            "Method retrieves morken correct and images with significance less than .01 so we can see most time is below this green line."
        ],
        [
            "Point minus 2.",
            "So to wrap up.",
            "Being up this work, we propose a density estimation algorithm tailored for a particular function class, and it can be solved by simple QP.",
            "We proved uniform convergence guarantees for approximating function expectations, and we get good experimental results and show that it better approximates the expectation functions when apply out when applying our algorithm to graphical model inference, it is very closely connected to the expectation problem propagation.",
            "As a future directions, we're going to apply the KMM to non power magic.",
            "Do PvP in graphical model inference and to compress online data stream by solving online QP.",
            "That's awful talk.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so welcome to the kernel session and the 1st paper is tailoring density estimation by reproducing kernel moment matching and it will be presented by Jean Holzem.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So thanks thanks Professor Williams.",
                    "label": 0
                },
                {
                    "sent": "So I guess I can stop now this this talk is about density estimation and the objective file paper is to tailor the dance information such that we can better estimate the expectation of family of functions which are known a priori and our approach is based on reproducing kernel moment matching.",
                    "label": 0
                },
                {
                    "sent": "This work is bioli, so me and Alex smaller from Victor of Australia and also gratton Benasher call from.",
                    "label": 0
                },
                {
                    "sent": "MPI Tubingen so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, this talk will be divided into 3 sections.",
                    "label": 0
                },
                {
                    "sent": "Our first to motivate our algorithm and give the estimation bounds.",
                    "label": 1
                },
                {
                    "sent": "Then I will show how our algorithm can be formulated into a simple quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "And finally I will give the experimental.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doubts before conclusion.",
                    "label": 0
                },
                {
                    "sent": "So first motivation.",
                    "label": 0
                },
                {
                    "sent": "Observe that in many applications that involve density estimation, the ultimate goal is normally not density estimation.",
                    "label": 1
                },
                {
                    "sent": "For example, we may be in we.",
                    "label": 1
                },
                {
                    "sent": "We are often interested in the expectation of a random variable or the function of a random random variable.",
                    "label": 0
                },
                {
                    "sent": "For example, in parameter estimation for graphical models, especially undirected graphical models, in the using exponential family, if we use gradient descent, then the gradients are essentially the expectation of the sufficient statistics.",
                    "label": 1
                },
                {
                    "sent": "So it is not clear whether doing maximum likelihood is the ideal criterion for density estimation.",
                    "label": 1
                },
                {
                    "sent": "So now we are faced with a choice.",
                    "label": 0
                },
                {
                    "sent": "On one side we can do full density estimation by maximum likelihood estimation and prepared for calculating the expectation for arbitrary subsequent functions.",
                    "label": 0
                },
                {
                    "sent": "Or we can focus just on the approximating the expectation of a set of functions known a priori.",
                    "label": 1
                },
                {
                    "sent": "So putting this intuition in a formal way, our paper is our paper deals with the following problem.",
                    "label": 0
                },
                {
                    "sent": "So given a distribution P Anna function set curly F, please find us distribution petel dot such that the difference, the difference which.",
                    "label": 0
                },
                {
                    "sent": "Ahnapee is small for any function in any function.",
                    "label": 0
                },
                {
                    "sent": "Small F in the function set curly F.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we put it other way.",
                    "label": 0
                },
                {
                    "sent": "We want the soup of the mean discrepancy to be small.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's take a very short digress to see that the idea of using such touchstone function classes have been widely used in research.",
                    "label": 0
                },
                {
                    "sent": "The same spirit has been used to define the weak convergence of probability measures.",
                    "label": 1
                },
                {
                    "sent": "So the probability measure mu N is called convert to converges weakly to mu if the expectation of functions converges for all the functions which are real valued continuous in a bounded.",
                    "label": 0
                },
                {
                    "sent": "Read the Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So the next example is the independence test criteria by running into 1959 he said that for sufficiently rich function classes, the function correlation or cross covariance serves as an independence test and the work that is most is most close to our work.",
                    "label": 1
                },
                {
                    "sent": "Our paper is by Joshua Taylor and Alex to layer in there a SS 2007 paper.",
                    "label": 0
                },
                {
                    "sent": "They dealt with density estimation as well.",
                    "label": 1
                },
                {
                    "sent": "They measure the loss on a set of randomly drawn touchstone functions, so their optimization requires drawing samples from a set of touchstone functions, and they require characterization of the complexity of this function set.",
                    "label": 0
                },
                {
                    "sent": "So in our approach we avoid these two advantage as we will show later.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's come back to make the algorithm concrete, we need to specify what function class we need.",
                    "label": 0
                },
                {
                    "sent": "We are interesting, so in this paper we pick the functions in the reproducing kernel higher office space with whose norm is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So we have a kernel.",
                    "label": 0
                },
                {
                    "sent": "We have our key actress, so this is the mean discrepancy and very Interestingly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This soup can be represented is equal to the occassion distance between 2 mean elements, namely mu P annum upto Elder.",
                    "label": 0
                },
                {
                    "sent": "Them UP is defined as the expectation of the evaluation element KX dot with respect to the distribution P. So the key idea of our approach is to embed distribution P and our estimated distribution peaked Elder into.",
                    "label": 0
                },
                {
                    "sent": "The arcade chess and one naturally expects that the P&P told are closed if and only if the architectures distance between their corresponding mean mapping.",
                    "label": 1
                },
                {
                    "sent": "New P~ random UP is closed are close, so we can also equally well motivate algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'll algorithm by kernel mean mapping which is hinted in the hour.",
                    "label": 1
                },
                {
                    "sent": "Paper last year.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this last slide gives the most important theoretical the theorem in our paper and justifies our approach approach.",
                    "label": 0
                },
                {
                    "sent": "So given a real density function density P, we can have a corresponding mean element mupi.",
                    "label": 1
                },
                {
                    "sent": "But in practice we never have the full knowledge of P. What we have is just a sample Jaune, Jaune independently and identically from frumpy.",
                    "label": 0
                },
                {
                    "sent": "And we use this saragat mean element name, also called the empirical empirical mean element mu X defined as the average of KXI dot.",
                    "label": 0
                },
                {
                    "sent": "And we have we are looking for the estimated density to decrease, which confounds to mean element mu, P~ So to do the as density estimation.",
                    "label": 1
                },
                {
                    "sent": "What we ultimately want to minimize is the distance between UP and mupi tailed.",
                    "label": 0
                },
                {
                    "sent": "However, we have no knowledge of new piece, so we result to the triangle inequality that is the sum of the red distance plus the black distance.",
                    "label": 0
                },
                {
                    "sent": "The black distances beyond our beyond our control, so we characterize it by using the right module average and the red distance is within our control.",
                    "label": 0
                },
                {
                    "sent": "The new X is computable anamu P~ is can be optimized by our.",
                    "label": 0
                },
                {
                    "sent": "Optimize us, so in essence we minimize the render distance in practice as a proxy distance between the blue distance between UP~ MUP.",
                    "label": 0
                },
                {
                    "sent": "So now let's state our main theorem, so with a with a high probability, the distance between UP and mutate all the mean discrepancy is upper bounded by the mean discrepancy of new PX and mupi teleda.",
                    "label": 0
                },
                {
                    "sent": "Plus a random Archer average plus the tolerance.",
                    "label": 0
                },
                {
                    "sent": "That's the key reason why we minimize this mean discrepancy in practice.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in the next slide I will show that this minimalization can be carried out using an efficient optimization.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that the Pete Elder can be represented as a mixture model with P prototype spy fixed an Alpha.",
                    "label": 0
                },
                {
                    "sent": "I being in the M dimensional probability simplex and we can share with these that this minimalization problem over the Pete Elder can be posed as.",
                    "label": 0
                },
                {
                    "sent": "A quadratic programming where this elements of Q&A L can be computed by the expectations and control closed from family exist for Qi and Qi, Gemelli and various combinations of prototypes and kernels.",
                    "label": 0
                },
                {
                    "sent": "The details can be found in our.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "OK, next the last section is about experimental results we used with three sets of experiments.",
                    "label": 1
                },
                {
                    "sent": "the UI data set on which we show that our algorithm really better estimates the expectation of functions than other algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then we apply the estimator to two applications, namely message passing, compression an image retrieve.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "The first UCI data set with the objective of this experiment is to share that our density estimate outperforms in terms of estimating the expectation of functions which are in the architectures, however, does not necessarily outperforming outperform other algorithms in terms of log likelihood.",
                    "label": 1
                },
                {
                    "sent": "So the algorithms under comparison are kernel moment matching which is ours and Gaussian mixture model passing window and reduced sets density estimation.",
                    "label": 1
                },
                {
                    "sent": "The comparison is made in the following way.",
                    "label": 0
                },
                {
                    "sent": "We first randomly generate a function F from the IP address.",
                    "label": 0
                },
                {
                    "sent": "Then we use half of the data set, top the data to give a desk estimation of Pete Elder, and use that Pete Elda to compute the expectation of F. Then we use the rest half data to compute the empirical average of F. Finally, we report the relative discrepancy.",
                    "label": 0
                },
                {
                    "sent": "The whole process is repeated for 100 times, so so that we can do a pad.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sign test so this table gives a result of function expectation estimation.",
                    "label": 0
                },
                {
                    "sent": "We can see that if we are interested in mixture of RBF functions, then by using RBF kernel our algorithm can give consistently lower error than all other algorithms.",
                    "label": 0
                },
                {
                    "sent": "The blue number means that the corresponding algorithm gives lower lower error then other the other algorithms.",
                    "label": 0
                },
                {
                    "sent": "With significance level significantly less than .01.",
                    "label": 0
                },
                {
                    "sent": "And the number reported is just the median of the 100 repetitions, and similarly for polynomial functions of degree 3.",
                    "label": 0
                },
                {
                    "sent": "All our algorithm using polynomial kernel with degree three gives lower error more frequently than other algae.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prisms.",
                    "label": 0
                },
                {
                    "sent": "And similarly, for polynomial functions of degree two and four linear functions.",
                    "label": 0
                },
                {
                    "sent": "Because puzzle window and Gaussian mixture, they also match the 1st order moment.",
                    "label": 0
                },
                {
                    "sent": "So our algorithm our density estimator using linear kernel gives always give the same result.",
                    "label": 0
                },
                {
                    "sent": "As the people we know in the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mixture.",
                    "label": 0
                },
                {
                    "sent": "As expected, so next we see the first application, which is the message passing compression.",
                    "label": 1
                },
                {
                    "sent": "The task is just doing filtering.",
                    "label": 1
                },
                {
                    "sent": "Suppose we have temporal system.",
                    "label": 0
                },
                {
                    "sent": "The current state St is a function of the previous state S, T -- 1 + A noise, and the current observation YT is a function of the current state current observations.",
                    "label": 0
                },
                {
                    "sent": "The function of current state plus a noise, and here we assume that the transition noise sizes.",
                    "label": 0
                },
                {
                    "sent": "Mixture of five Gaussians.",
                    "label": 0
                },
                {
                    "sent": "The task is to do filtering to compute the posterior probability of the current state given all the observations up to current time step.",
                    "label": 0
                },
                {
                    "sent": "The key idea is to recursively estimate to the filtering density and then followed by various rule.",
                    "label": 1
                },
                {
                    "sent": "So the key step here and of our interest is that the P of S T + 1 given Y capital YT is can be expressed as the integral, and especially in the expectation this function.",
                    "label": 0
                },
                {
                    "sent": "This PST plus one given St is exactly a function in the archaeologist thanks to the definition of our.",
                    "label": 0
                },
                {
                    "sent": "Transition model and the noise here.",
                    "label": 0
                },
                {
                    "sent": "This PST, given YT, is exactly the density to be estimated, so this density can be very complicated in practice and people approximate it by using particles and this big cause, this expectation falls perfectly into our framework.",
                    "label": 0
                },
                {
                    "sent": "We are allowed to compress the particles by using our kernel moment matching.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the result of root mean square error and standard deviation of the far filtering result before and after particle compression.",
                    "label": 0
                },
                {
                    "sent": "We can see that our kernel moment matching gives consistently lower or similar error than all the other competitor Al Gore.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Items.",
                    "label": 0
                },
                {
                    "sent": "The second application we did is image retrieval.",
                    "label": 0
                },
                {
                    "sent": "The task is supposed to be given an image data set D and as a major P. Please retrieve from the data set DA set of images that are similar to P. The idea is on each image we first perform density estimation over the feature distribution and then we retrieve by ranking dissimilarity P the given image P and all the images Q in the data in the database.",
                    "label": 1
                },
                {
                    "sent": "The dissimilarity measure we use is the earth movers distance by Reuben ATL and it can be applied on the mixture of Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the experimental result.",
                    "label": 0
                },
                {
                    "sent": "The horizontal on the scene of the class of each data set.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "The horizontal access is number of retrieved images.",
                    "label": 1
                },
                {
                    "sent": "Enter the vertical access gives the signed log P value of parasite test.",
                    "label": 0
                },
                {
                    "sent": "So just figuratively I can tell you that when the number is less than minus two then KMM the.",
                    "label": 1
                },
                {
                    "sent": "Method retrieves morken correct and images with significance less than .01 so we can see most time is below this green line.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point minus 2.",
                    "label": 0
                },
                {
                    "sent": "So to wrap up.",
                    "label": 0
                },
                {
                    "sent": "Being up this work, we propose a density estimation algorithm tailored for a particular function class, and it can be solved by simple QP.",
                    "label": 1
                },
                {
                    "sent": "We proved uniform convergence guarantees for approximating function expectations, and we get good experimental results and show that it better approximates the expectation functions when apply out when applying our algorithm to graphical model inference, it is very closely connected to the expectation problem propagation.",
                    "label": 1
                },
                {
                    "sent": "As a future directions, we're going to apply the KMM to non power magic.",
                    "label": 0
                },
                {
                    "sent": "Do PvP in graphical model inference and to compress online data stream by solving online QP.",
                    "label": 0
                },
                {
                    "sent": "That's awful talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}