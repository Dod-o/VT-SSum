{
    "id": "qabza2qzu3mkhurlx5iqap2tppol5bpf",
    "title": "Spectral Dimensionality Reduction via Maximum Entropy, incl. discussion by Laurens van der Maaten",
    "info": {
        "author": [
            "Laurens van der Maaten, Delft University of Technology (TU Delft)",
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/aistats2011_lawrence_spectral/",
    "segmentation": [
        [
            "So the idea behind this paper is there's a lot of spectral dimensionality reduction methods around, and they all somehow require a sort of slightly different explanation, and I thought, well, if I want to teach them, I don't want to have to teach them all, because that's the entire course.",
            "So is there some way of trying to come up with the unifying perspective on them?",
            "And this is an attempt to do that.",
            "Unfortunately, to do it I had to invent a new spectral dimensionality reduction method.",
            "Oh well, OK so.",
            "What time?"
        ],
        [
            "Really want to start from is the idea that."
        ],
        [
            "At most."
        ],
        [
            "Well, I think of spectral dimensionality reduction is classical monumental scaling, which has been around for a long time in statistics.",
            "Now statistics if you talk about scaling methods, people tend to think of data being represented in squared distances, and indeed that what will focus on mainly and also transformation between that and similarities.",
            "But the key point to remember is that the square distances is simply related to the data.",
            "In this way.",
            "If it's a Euclidean distance."
        ],
        [
            "Now in classical multidimensional scaling, which has been around for a long time.",
            "Basic idea is to find a linear embedding which somehow approximates the matrix of distances.",
            "So if this is your data point, why?",
            "This is the I TH data point?",
            "You've gotta end by N, where N is the number of data size, distance matrix squared distance matrix to represent your data set in classical onto an internal scaling.",
            "You're trying to find a linear transformation between some latent space, which I tend to think of his ex and this data space Y."
        ],
        [
            "So that's linear, and I'd say that to summarize the main contribution from machine learning is that what typically happens in the methods that we've developed is that you get a nonlinear relationship between the data and the distances, so this is straight Euclidean distances, but what?"
        ],
        [
            "You can do."
        ],
        [
            "Is not compute D directly in the space of Y.",
            "You can for example use the kernel trick to compute D, so this isn't how kernel PCA is presented, but you can definitely see it this way if you decide to compute distances, your squared distances in a feature space.",
            "And then you apply multidimensional scaling to this feature spear this squared distance."
        ],
        [
            "Tricks here what you get is that you should do the eigenvalue problem on the centered kernel matrix that you've computed.",
            "So here H is a centering matrix.",
            "The reason it's sent that you get the relationship between the square distances and the kernel matrices is because this B here is also equal to the center distance matrix, so this gives the equivalence between these two representations."
        ],
        [
            "Now this exactly matches the kernel PCA algorithm, but it's also what would happen if you decided to compute your kernel.",
            "Classical multidimensional scaling distances in feature space instead of normal space.",
            "But there's a problem with this, which is really highlighted."
        ],
        [
            "By Kilian weinberger.",
            "Face shower and Loren Loren Saul, which is for the most commonly used covariance well, Colonel, I should say, sorry, my Gaussian processor stuff coming out there for the most common use kernel matrix that capable PC actually expands the feature space.",
            "And this is because it's K is a full rank matrix.",
            "So when you look at the rank of the resulting, so you do an eigenvalue problem when you visualize according to the eigenvectors.",
            "So when you look at those eigenvectors, since K is full rank.",
            "We've actually gone from, let's say P dimensional.",
            "If P is the dimension of data to end dimensional, where N is your number of data points and if N is larger than P, you've actually expanded your feature space.",
            "So it's not a dimensionality reduction."
        ],
        [
            "Technique now what those guys did in response to that, it said well, can we learn a kernel matrix which will allow for dimensionality reduction and relates a bit to?"
        ],
        [
            "Martin talked about."
        ],
        [
            "Because they used."
        ],
        [
            "Secular norms to try and force that to happen.",
            "So the idea in maximum variance on."
        ],
        [
            "Holding, which is what they developed.",
            "Is to specify distance constraints between data points, so this is the first data point here, and this is the second data point.",
            "If you decide that these are neighbors, you build a neighborhood graph by nearest neighbors in the data space, and then if they are neighbors, you decide that you want to build a covariance matrix.",
            "Or sorry, a kernel matrix under which distance constraints hold.",
            "So we've got this relationship between the squared distance and the kernel matrix.",
            "Here in this form.",
            "And if we decide to enforce constraints for these neighbors.",
            "That's a constraint on the sort of kernel matrices we can look at."
        ],
        [
            "So what you do is you try and maximize the trace of that kernel matrix, which I tend to think of as the sort of total variance.",
            "I think of K as a covariance matrix, so I tend to think of that as the total variance of the data.",
            "So in effect, what you're trying to do is you're trying to blow."
        ],
        [
            "Pull up your data, make it as wide as possible.",
            "While neighbors are constrained to be tide together by little rods which provide distance constraints, so that's a great idea and a great method."
        ],
        [
            "It works really well."
        ],
        [
            "So what we're going to suggest here is to maximize entropy instead of variance, so that seems like an interesting idea because both entropy invariants are measuring uncertainty in some way.",
            "If we maximize the entropy, we're also going to blow up the data, But the nice thing about this is maximum entropy is probabilistic model.",
            "So once you've got a probabilistic model, you can deal with missing data.",
            "You can do mixtures.",
            "You can do any one of the Bayesian things whatever you like.",
            "It also turns out that the model will also provide links to sort of several other different spectral."
        ],
        [
            "Which is.",
            "So if you're doing maximal entropy unfolding, what you're doing is trying to maximize the entropy subject to constraints, But the constraints are on moments.",
            "So now we see the interpretation of this K. We will come out as a covariance matrix because I'm using these angle brackets here to express expectations, not in a product, as some people do.",
            "So under whatever distribution you derived, maximum entropy is about driving a distribution, so these expectations are under that distribution.",
            "What we will find is that we want the constraint these distance constraints to hold for these expectations.",
            "Where this is the expected so.",
            "Where distance between any two points sampled from that model, and this is the."
        ],
        [
            "Love squared distance, so it's a slightly different thing to an exact well, but it looks like it might be slightly different things.",
            "This is an expectation, but of course when you look at what the expectation is, if you represent that expectation, if it's a zero mean solution and you represent that expectation of the convert in terms of the covariance, and you define the covariance to be K. So any valid kernel is a valid covariance matrix, it's exactly the same as the constraints we had before, so it's a very similar setup, but it leads to without showing you the details.",
            "It leads to a Gaussian random field over your data."
        ],
        [
            "Arrival in.",
            "Vacation.",
            "It's so this the distribution.",
            "What's the random variable that which is Y?",
            "So actually it's why there's some details in the paper, but actually all you observe is interpoint distances between Y and that leads to a little thing that will pop up in the next slide.",
            "So you're trying to learn it.",
            "You're trying to derive a distribution over Y.",
            "So this is the distribution you get, it's over Y and it's a Gaussian with the covariance."
        ],
        [
            "K. Because of these distance constraints, well this this gamma.",
            "I won't explain where that comes from.",
            "You have to do it with respect to a base distribution, so that's just the base distribution I use.",
            "This is the L that's coming out of the actual maximum entropy formulation, and this is."
        ],
        [
            "Laplacian matrix So what you get is graph where the precision matrix is a Laplacian plus diagonal term and is if you take that gamma to be vanishingly small then you can think of this is just a graph Laplacian.",
            "You need the gamma to actually invert it to get a proper covariance."
        ],
        [
            "The off diagonal elements of this Laplacian turned out to be the LaGrange multipliers that come from these moment constraints."
        ],
        [
            "Anne Anne on diagonal elements are given by the negative sum of the off diagonal so that you end up with this effect.",
            "The Laplacian Times these got null space in zero.",
            "The overall matrix also doesn't have that null space because of that small vanishingly small term there.",
            "So that's the model."
        ],
        [
            "That you get.",
            "And the interesting thing about it is it's a Gaussian random field, so.",
            "More interesting, I think it's a Gaussian random field that's expressing independence across features, not data points, so most people they play with Gaussian distributions, you'd expect independence across data points, but here the independence across data features so people have looked at that due John Lafferty and Ghahramani looked at in the context of semi supervised learning.",
            "I looked at it in the context of Model GP LVM and Charles Kemp.",
            "Josh Tenenbaum have also looked at models in this type and Josh is still working on these type of models.",
            "So maximum likelihood in this model, though, As for all maximum entropy models, is equivalent to maximum entropy under the distance constraints, so you don't have to worry about the maximum formulation anymore.",
            "You have to do is fit this model by maximum likelihood in your satisfying the maximum entropy set up."
        ],
        [
            "So this is a side note, there's a weird thing going on here.",
            "'cause maximum likelihood is normal."
        ],
        [
            "Consistent."
        ],
        [
            "As we increase the number of data points, parameters become."
        ],
        [
            "Better determined not."
        ],
        [
            "In this model, as you increase the number of data points, you actually increase the number of parameters.",
            "It's nonparametric in this set."
        ],
        [
            "But what does happen is you increase the number of data features.",
            "The parameters become better determined, so you can do a consistency results in data features instead of data points.",
            "So that's a rather weird thing, because it turns the large piece small end problem on its head for this model.",
            "Large N small P is a problem.",
            "Large P small N is not a problem at all.",
            "It's very well."
        ],
        [
            "Determined in that region.",
            "So this is sort of blessing of dimensionality in this model.",
            "Now I think that reflects something which people sometimes miss when they talk about large P. Small and it's all about model definition.",
            "If this model is correct, then you don't have a problem with large P small N. If it's incorrect then you know none of these results hold anyway, so it shows that sort of a model like this is going to be very good if you got very high dimensional data and load."
        ],
        [
            "Update points"
        ],
        [
            "So relations to others."
        ],
        [
            "Methods so quick."
        ],
        [
            "Laplacians."
        ],
        [
            "Google Maps is."
        ],
        [
            "Very similar because in a plastic Tiger match you specify Laplacian graph Laplacian directly and you parameterise it without well by either local distances without looking at the global data set.",
            "Now then you use the smallest eigenvalues of this Laplacian are used for visualizing the data, discarding the constant eigenvector.",
            "But that is exactly the same operation is taking the largest eigenvalue vectors of K, because this is the inverse of that and this H ax to discard the constant eigenvector.",
            "So you have to.",
            "You always get this constant eigenvector, which is the smallest one, and you have to throw.",
            "That way it's got an eigenvalue of 0.",
            "But basically this operation of centering has the same effect.",
            "In this case here so.",
            "It's sort of equivalent, but Laplacian Eigen Maps do not preserve distances between neighbors in the graph that you specify, so they're also based on neighbors, but they don't do anything to try and preserve the distances between those neighbors.",
            "So locally linear."
        ],
        [
            "Embedding so the Laplacian has to be constrained positive DEF."
        ],
        [
            "And one way of imposing that constraint is by factorizing the Laplacian like this.",
            "So L is equals MM transpose."
        ],
        [
            "Now to ensure it's all a plassey and you then would have to need to further constrain M such that M transpose one.",
            "Well, this is one way of doing it is equal to zero.",
            "That forces this thing to be a Laplacian."
        ],
        [
            "Um?"
        ],
        [
            "So that gives you the values for the diagonals of M that the diagonal of EM has to be there.",
            "Some of the negative off diagonals, and in this case you would set the off diagonals to be 0.",
            "If two points are not neighbors.",
            "Actually that doesn't give you the same neighborhood structure in L that you had in M. You get slightly different neighborhood structure, but this would be one way of parameterising the model.",
            "You just wouldn't get quite the same neighborhoods."
        ],
        [
            "So locally linear embedding's and then the specific case of me U where you make a further constraint."
        ],
        [
            "That these diagonal sums are further constrained to unity.",
            "So these guys here you also constrain them to be unit."
        ],
        [
            "And then the model parameters are found by maximizing the pseudo likelihood of the data.",
            "So instead of maximizing the likelihood directly, you maximize the pseudo likelihood, which is typically."
        ],
        [
            "Quicker.",
            "I'm so.",
            "That was, at least to some ideas about me, you, one of which is the fact that it's approximation to maximum likelihood."
        ],
        [
            "And the plastic has this particular factorized form that."
        ],
        [
            "You can use so you get this user likelihood for relatively quick Param."
        ],
        [
            "Attract optimization and ignoring that partition function removes the need to invert to recover the convergence matrix, which speeds things up."
        ],
        [
            "So this means that Eli can be applied to larger datasets than me.",
            "You or envy you."
        ],
        [
            "Typically, but the sparsity pattern for the Laplacian fairly will not typically match that from the standard algorithms due to this."
        ],
        [
            "Factorize representation OK button."
        ],
        [
            "The point about it?"
        ],
        [
            "This is a little bit odd, so early."
        ],
        [
            "Motivated by considering locally linear embeddings of the data, but as you increase the neighborhood size of the data to make the whole thing in neighborhood, so caysen minus one.",
            "So everything's a neighbor.",
            "You don't recover PCA, but PCA is the optimal linear embedding.",
            "So there's something funny there, and it's because Elyse optimizing this pseudo likelihood.",
            "So in contrast, this maximum entropy unfolding algorithm an maximum variance unfolding, which LL is approximating, does recover PCA when K is equal to N -- 1.",
            "It just returns to sort of standard principle coordinate analysis."
        ],
        [
            "So I some apps also interest.",
            "It's not quite as closely related, but you create a sparse graph."
        ],
        [
            "Some distances and you fill in the graph."
        ],
        [
            "For non neighbors with the shortest path algorithm."
        ],
        [
            "So what's going in envy?",
            "You and me you you can start effectively with the sparse graph of squared distances, and then you fill in the other distances by either maximizing the total variance or the entropy.",
            "So this is typically a faster operation, filling in with the shortest path algorithm and certainly for maximum entropy unfolding the distances you're getting between points have an interpretation as a sort of diffusion walk between two data points rather than the shortest path walk, so they are sort of quite closely related, but Isomap slightly quicker.",
            "OK, so I won't talk about the GPL V."
        ],
        [
            "M 'cause I'm short on time."
        ],
        [
            "I'm so I just want."
        ],
        [
            "So a couple of datasets."
        ],
        [
            "The first of which is."
        ],
        [
            "A motion capture data set so it's 3 dimensional is a per."
        ],
        [
            "And doing a run for three strides and we're using a point cloud of the positions of all their limbs in 3D.",
            "So XY zed positions of each point on their body and at least 102 dimensional data containing 55 frames of motion.",
            "So you should see in the visualization sort of starting position followed by three."
        ],
        [
            "Strides have run.",
            "So we're looking at the dominant to eigen values, the eigenvectors associated with him.",
            "What you see is the result from Alaskan Eigen Maps and the results from locally linear."
        ],
        [
            "Heading uses Isomap and this is Max."
        ],
        [
            "Invariants unfolding, and this is the new algorithm me you now."
        ],
        [
            "Say with this much choose between them.",
            "This is a quality measure that was suggested by Stefan Harmeling, which involves using a Gaussian process to map from the latent space to the data space.",
            "The details are in the paper, but if you don't believe this quality measure, then what you see is the methods that are preserving the Inter neighbor distances on all these.",
            "Sorry this one that I haven't talked about, but look at these three.",
            "These three methods that are preserving into neighbor distances do typically much better than the ones that don't."
        ],
        [
            "This is another data set.",
            "It's a similar story, but it's a noisy data set, so it's a robot moving around in a circle and the robot arm is reading Wi-Fi Bout 20 Wi-Fi access points.",
            "It's reading the strength signal strength, but it should be 2 dimensional.",
            "It's movement, so we looked for the inherent dimensionality of this data to be 2.",
            "So in this case Laplacian Eigen Maps fails and locally linear."
        ],
        [
            "Bedding sort of also fails."
        ],
        [
            "The other methods, Isomap this, is reasonably good job.",
            "It comes in from one corridor and then it does a circle this way."
        ],
        [
            "About here envy you.",
            "So I think this is characteristic of this.",
            "I don't have any proofs about this, but I think that this very noisy response is characteristic of the trace norm.",
            "Let's apply because it has.",
            "It tries to push all the information into the low dimensional space, and that's where you get these nice eigenvalue Spectra.",
            "And here's the result from me.",
            "You so they're sort of similar as well."
        ],
        [
            "Then you get the similar story that under the quality measure, which I haven't really talked about in any detail, you see, these three methods are much better than the methods that failed to preserve distances."
        ],
        [
            "So I think the main thing takeaway message from the paper it's not about."
        ],
        [
            "So while there's a fantastic new dimensionality reduction algorithm, 'cause I don't, it's all much of a muchness I think, which is doing better, and that's certainly not the."
        ],
        [
            "Attention, but to show that there are three separate stages using existing spectral dimensionality reduction.",
            "So the first stage I think this is actually the most important in neighborhood between the data points is selected.",
            "If you knew the neighborhood, I think your problem is done, and it doesn't actually matter what method you follow."
        ],
        [
            "But you typically don't, but then the things that we've been focusing in the Internet to interpoint distances between neighbors are fed to algorithms which provide a similarity matrix.",
            "The way the entries in this similarity matrix or covariance matrix or kernel matrix are computed is the main difference between the different algorithms, so they're all doing different things to compute those element."
        ],
        [
            "And the relationship between the points in the similarity matrices then visualize using eigenvectors of the similarity matrix.",
            "So it's what I'm arguing is that you're describing a Gaussian random field with that similarity matrix, is inverse is sparse, and that the final step is that you're visualizing a Gaussian random field with the spectral visualization.",
            "So it's three step one.",
            "You're defining a neighborhood.",
            "OK, we're doing nearest neighbors.",
            "We haven't talked about that.",
            "Then the next stage is you've got a Gaussian random field, and you're fitting the parameters of that Gaussian random field.",
            "By different ways and then the final step is you're visualizing that graph.",
            "Of course, there's many, many different ways of graph visualization that are non spectral and sometimes spectral visualization is not a good idea.",
            "So I think it's worth remembering that these steps is sort of separate and that you could do different things.",
            "So that's why I said there."
        ],
        [
            "I think."
        ],
        [
            "But sometime."
        ],
        [
            "Inflating these?"
        ],
        [
            "The steps allows for much faster algorithms."
        ],
        [
            "So mixing the second and third step allow speed."
        ],
        [
            "By never computing the similarity matrix directly and."
        ],
        [
            "That's what's happening in Eli and Eli, so I've shown results which are not as good for them, but of course they can do massive datasets very very, very fast, and indeed you could even argue that."
        ],
        [
            "Page rank is some variant of these type of algorithms, and that's doing the entire web and it's doing that 'cause it never explicitly computes that matrix K, it only compute.",
            "It only looks at the inverse.",
            "So, but I think we can understand these algorithms from the unifying perspective while exploiting computational advantages from the shortcut.",
            "And I've been thinking about this for a long time and so."
        ],
        [
            "The conversations with all these people have sort of influenced the thinking in the work and that's it.",
            "So I think one of the ideas."
        ],
        [
            "The discussion is to sort of put work in a broader research context.",
            "Annmulle made it kind of hard for me because he related his work through pretty much all Seminole papers in a manifold learning over the last decade.",
            "So I'll try to do."
        ],
        [
            "Say something anyway, so just to give you a timeline.",
            "I sort of think that the first manifold learner sort of in the about that came up about 10 year, 10 years ago.",
            "There were instantiations of kernel PCA that basically use some sort of handcrafted kernels and putting kernels between quotes here because the feature space basically changes if you add extra data.",
            "So they're not really Mercer kernels in that sense, but there were.",
            "They were handcrafted kernels were for incident.",
            "I zone map.",
            "The Colonel was given by some center geodesic distance matrix in Laplacian.",
            "Eigen Maps will be the inverse graph, Laplacian matrix and Ellie well.",
            "Handcrafted thing is not really true anymore because there's a little bit of learning in how you determine the reconstruction weights.",
            "But what you see?",
            "More recently, I think in manifold Learning is an interest shift towards trying to learn a good kernel instead of instead of handcrafting it.",
            "And, uh, so the initial example Neil mentioned already.",
            "Is this maximum variance in folding technique?",
            "But more recently it was also a nice paper in HTML by Blake and Sean Gold, structure preserving embedding and basically what these techniques do is they they put some rods between your between your data points so they that.",
            "Basically I'm for some local distance constraints and then they maximize or minimize some objective that tries to minimize the rank of the solution.",
            "An maximum entropy and falling is basically another.",
            "Another technique that follows the same the same path."
        ],
        [
            "So I want to talk about two things.",
            "So the first one is basically manifold learning versus generative modeling.",
            "The one connection that Neil Luckily didn't discuss was between maximum entropy Enfolding and his own work on Gaussian process process, latent variable models.",
            "Which are you can think of, basically as some sort of nonlinear version of probabilistic PCA.",
            "So both these models basically model P of Y as a Gaussian random field, where each data point.",
            "Is a note in the Gaussian random fields the key difference, however, is in how the covariance of this Gaussian random field is obtained.",
            "So in the in the GP LVM model it's basically just just an inner product of of the latent space coordinates, whereas in maximum entropy unfolding it's this kernel where basically the LR.",
            "This LaGrange multipliers that you're that you're trying to learn.",
            "An so So what seems to be going on in maximum entropy unfolding is that it sort of tries to unify two seemingly very different approaches.",
            "So the one is 1 is manifold learning and the other one is generative modeling.",
            "And here's how I think why."
        ],
        [
            "We're sort of what it seems to be sort of sort of difference.",
            "So in manifold learning you're basically trying to learn a smooth mapping from your data space to your latent space, which means that if you have two similar data points, then these two similar data points cannot be very far apart in your latent space.",
            "In your final embedding, because otherwise the mapping wouldn't be smooth and generative.",
            "Modeling you basically have just moved mapping the other way around, right?",
            "You have a smooth mapping from the latent space too.",
            "The data space.",
            "So what that means is basically if you have two dissimilar data points then they cannot.",
            "They cannot be close together, so so manifold learning is seeing a sense.",
            "Trying to preserve local structure, getting similar stuff close together in your embedding, whereas generic models like probabilistic PCA or GP LVM or just trying to make sure that this similar points don't it up close to close together, which is which is a bit different."
        ],
        [
            "So maximum entropy and falling.",
            "I think this is the first technique that sort of tries to combine the best of two worlds.",
            "So one trying to preserve local data structure in your embedding an in the other one trying to have a probabilistic framework which allows us to do to deal with missing data.",
            "Or do hierarchical models or stuff like that, but the current formulation in maximum entropy unfolding as an oddity, which is that the embedding.",
            "Basically, doesn't appear as a latent variable in the model, so if you think about dimension reduction, if you think that there are some low dimensional latent latent data representation that that was basically underlying your data, that would be nice.",
            "If you had a generative model where you basically infer would infer those latent variables.",
            "But that's not what's happening in EM you if you, if you look at these at the formulation for PP of Y in MU.",
            "There is no, um, this latent space just never shows up.",
            "You just learn a kernel and then in you apply some spectral decomposition on that.",
            "But you could basically do any NDS over in there, so that's an oddity that that is one of the things I guess that could be that people could work on trying to improve."
        ],
        [
            "So the other thing I wanted to mention is about the rank minimization.",
            "So basically the way I think of this of this rank minimization is basically controlling what we do with this similar data in our embedding.",
            "So in medical learning we try to we try to retain local structure, right?",
            "So we know what to do with sort of similar data points.",
            "But we need to figure out what we do with additional once and in the case of maximum variance unfolding, you're maximizing the sum of the kernel eigenvalues, which is basically maximizing variance, and I think will maximum entropy unfolding is doing is maximizing the sum of the log eigenvalues of the kernel, which is which is incense different, because if you have zero eigenvalue, basically that.",
            "That term, you'd get a very negative log eigenvalue.",
            "So, so it's doing a different type of.",
            "Of Rang minimization, and I'm not sure which one is 1 is better."
        ],
        [
            "What I do know is that it's very important how you deal with this similar data in manifold learning wasn't nice paper by Miguel Carreira.",
            "Prepping an ICM L this year where he basically compared stochastic neighbor embedding an impression eigen Maps and he showed that there are the same except for that there is a difference in the covariance constraint.",
            "Basically.",
            "So in how you deal with this similar data and this leads to two very very different results.",
            "So I guess what you could say is that.",
            "In medical learning and maybe more important, how you actually deal with this similar data, then how you deal with with similar data?",
            "'cause that's fairly straightforward.",
            "So recent successes they basically try to push this similar data as far away as possible, and I mean you seem to do something that's a little bit different, so it may lead to new insights.",
            "Here.",
            "That's all I wanted to say, I guess."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea behind this paper is there's a lot of spectral dimensionality reduction methods around, and they all somehow require a sort of slightly different explanation, and I thought, well, if I want to teach them, I don't want to have to teach them all, because that's the entire course.",
                    "label": 0
                },
                {
                    "sent": "So is there some way of trying to come up with the unifying perspective on them?",
                    "label": 0
                },
                {
                    "sent": "And this is an attempt to do that.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, to do it I had to invent a new spectral dimensionality reduction method.",
                    "label": 1
                },
                {
                    "sent": "Oh well, OK so.",
                    "label": 0
                },
                {
                    "sent": "What time?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really want to start from is the idea that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At most.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I think of spectral dimensionality reduction is classical monumental scaling, which has been around for a long time in statistics.",
                    "label": 0
                },
                {
                    "sent": "Now statistics if you talk about scaling methods, people tend to think of data being represented in squared distances, and indeed that what will focus on mainly and also transformation between that and similarities.",
                    "label": 0
                },
                {
                    "sent": "But the key point to remember is that the square distances is simply related to the data.",
                    "label": 0
                },
                {
                    "sent": "In this way.",
                    "label": 0
                },
                {
                    "sent": "If it's a Euclidean distance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in classical multidimensional scaling, which has been around for a long time.",
                    "label": 0
                },
                {
                    "sent": "Basic idea is to find a linear embedding which somehow approximates the matrix of distances.",
                    "label": 1
                },
                {
                    "sent": "So if this is your data point, why?",
                    "label": 1
                },
                {
                    "sent": "This is the I TH data point?",
                    "label": 0
                },
                {
                    "sent": "You've gotta end by N, where N is the number of data size, distance matrix squared distance matrix to represent your data set in classical onto an internal scaling.",
                    "label": 0
                },
                {
                    "sent": "You're trying to find a linear transformation between some latent space, which I tend to think of his ex and this data space Y.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's linear, and I'd say that to summarize the main contribution from machine learning is that what typically happens in the methods that we've developed is that you get a nonlinear relationship between the data and the distances, so this is straight Euclidean distances, but what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is not compute D directly in the space of Y.",
                    "label": 1
                },
                {
                    "sent": "You can for example use the kernel trick to compute D, so this isn't how kernel PCA is presented, but you can definitely see it this way if you decide to compute distances, your squared distances in a feature space.",
                    "label": 0
                },
                {
                    "sent": "And then you apply multidimensional scaling to this feature spear this squared distance.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tricks here what you get is that you should do the eigenvalue problem on the centered kernel matrix that you've computed.",
                    "label": 1
                },
                {
                    "sent": "So here H is a centering matrix.",
                    "label": 0
                },
                {
                    "sent": "The reason it's sent that you get the relationship between the square distances and the kernel matrices is because this B here is also equal to the center distance matrix, so this gives the equivalence between these two representations.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this exactly matches the kernel PCA algorithm, but it's also what would happen if you decided to compute your kernel.",
                    "label": 0
                },
                {
                    "sent": "Classical multidimensional scaling distances in feature space instead of normal space.",
                    "label": 0
                },
                {
                    "sent": "But there's a problem with this, which is really highlighted.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By Kilian weinberger.",
                    "label": 0
                },
                {
                    "sent": "Face shower and Loren Loren Saul, which is for the most commonly used covariance well, Colonel, I should say, sorry, my Gaussian processor stuff coming out there for the most common use kernel matrix that capable PC actually expands the feature space.",
                    "label": 1
                },
                {
                    "sent": "And this is because it's K is a full rank matrix.",
                    "label": 0
                },
                {
                    "sent": "So when you look at the rank of the resulting, so you do an eigenvalue problem when you visualize according to the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So when you look at those eigenvectors, since K is full rank.",
                    "label": 0
                },
                {
                    "sent": "We've actually gone from, let's say P dimensional.",
                    "label": 0
                },
                {
                    "sent": "If P is the dimension of data to end dimensional, where N is your number of data points and if N is larger than P, you've actually expanded your feature space.",
                    "label": 0
                },
                {
                    "sent": "So it's not a dimensionality reduction.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Technique now what those guys did in response to that, it said well, can we learn a kernel matrix which will allow for dimensionality reduction and relates a bit to?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Martin talked about.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because they used.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Secular norms to try and force that to happen.",
                    "label": 0
                },
                {
                    "sent": "So the idea in maximum variance on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Holding, which is what they developed.",
                    "label": 0
                },
                {
                    "sent": "Is to specify distance constraints between data points, so this is the first data point here, and this is the second data point.",
                    "label": 1
                },
                {
                    "sent": "If you decide that these are neighbors, you build a neighborhood graph by nearest neighbors in the data space, and then if they are neighbors, you decide that you want to build a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Or sorry, a kernel matrix under which distance constraints hold.",
                    "label": 0
                },
                {
                    "sent": "So we've got this relationship between the squared distance and the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Here in this form.",
                    "label": 0
                },
                {
                    "sent": "And if we decide to enforce constraints for these neighbors.",
                    "label": 0
                },
                {
                    "sent": "That's a constraint on the sort of kernel matrices we can look at.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you do is you try and maximize the trace of that kernel matrix, which I tend to think of as the sort of total variance.",
                    "label": 0
                },
                {
                    "sent": "I think of K as a covariance matrix, so I tend to think of that as the total variance of the data.",
                    "label": 0
                },
                {
                    "sent": "So in effect, what you're trying to do is you're trying to blow.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pull up your data, make it as wide as possible.",
                    "label": 0
                },
                {
                    "sent": "While neighbors are constrained to be tide together by little rods which provide distance constraints, so that's a great idea and a great method.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works really well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're going to suggest here is to maximize entropy instead of variance, so that seems like an interesting idea because both entropy invariants are measuring uncertainty in some way.",
                    "label": 1
                },
                {
                    "sent": "If we maximize the entropy, we're also going to blow up the data, But the nice thing about this is maximum entropy is probabilistic model.",
                    "label": 1
                },
                {
                    "sent": "So once you've got a probabilistic model, you can deal with missing data.",
                    "label": 0
                },
                {
                    "sent": "You can do mixtures.",
                    "label": 1
                },
                {
                    "sent": "You can do any one of the Bayesian things whatever you like.",
                    "label": 0
                },
                {
                    "sent": "It also turns out that the model will also provide links to sort of several other different spectral.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "So if you're doing maximal entropy unfolding, what you're doing is trying to maximize the entropy subject to constraints, But the constraints are on moments.",
                    "label": 1
                },
                {
                    "sent": "So now we see the interpretation of this K. We will come out as a covariance matrix because I'm using these angle brackets here to express expectations, not in a product, as some people do.",
                    "label": 0
                },
                {
                    "sent": "So under whatever distribution you derived, maximum entropy is about driving a distribution, so these expectations are under that distribution.",
                    "label": 0
                },
                {
                    "sent": "What we will find is that we want the constraint these distance constraints to hold for these expectations.",
                    "label": 0
                },
                {
                    "sent": "Where this is the expected so.",
                    "label": 0
                },
                {
                    "sent": "Where distance between any two points sampled from that model, and this is the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Love squared distance, so it's a slightly different thing to an exact well, but it looks like it might be slightly different things.",
                    "label": 0
                },
                {
                    "sent": "This is an expectation, but of course when you look at what the expectation is, if you represent that expectation, if it's a zero mean solution and you represent that expectation of the convert in terms of the covariance, and you define the covariance to be K. So any valid kernel is a valid covariance matrix, it's exactly the same as the constraints we had before, so it's a very similar setup, but it leads to without showing you the details.",
                    "label": 1
                },
                {
                    "sent": "It leads to a Gaussian random field over your data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arrival in.",
                    "label": 0
                },
                {
                    "sent": "Vacation.",
                    "label": 0
                },
                {
                    "sent": "It's so this the distribution.",
                    "label": 0
                },
                {
                    "sent": "What's the random variable that which is Y?",
                    "label": 0
                },
                {
                    "sent": "So actually it's why there's some details in the paper, but actually all you observe is interpoint distances between Y and that leads to a little thing that will pop up in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to learn it.",
                    "label": 0
                },
                {
                    "sent": "You're trying to derive a distribution over Y.",
                    "label": 0
                },
                {
                    "sent": "So this is the distribution you get, it's over Y and it's a Gaussian with the covariance.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K. Because of these distance constraints, well this this gamma.",
                    "label": 1
                },
                {
                    "sent": "I won't explain where that comes from.",
                    "label": 0
                },
                {
                    "sent": "You have to do it with respect to a base distribution, so that's just the base distribution I use.",
                    "label": 0
                },
                {
                    "sent": "This is the L that's coming out of the actual maximum entropy formulation, and this is.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Laplacian matrix So what you get is graph where the precision matrix is a Laplacian plus diagonal term and is if you take that gamma to be vanishingly small then you can think of this is just a graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "You need the gamma to actually invert it to get a proper covariance.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The off diagonal elements of this Laplacian turned out to be the LaGrange multipliers that come from these moment constraints.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne Anne on diagonal elements are given by the negative sum of the off diagonal so that you end up with this effect.",
                    "label": 1
                },
                {
                    "sent": "The Laplacian Times these got null space in zero.",
                    "label": 0
                },
                {
                    "sent": "The overall matrix also doesn't have that null space because of that small vanishingly small term there.",
                    "label": 0
                },
                {
                    "sent": "So that's the model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you get.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing about it is it's a Gaussian random field, so.",
                    "label": 0
                },
                {
                    "sent": "More interesting, I think it's a Gaussian random field that's expressing independence across features, not data points, so most people they play with Gaussian distributions, you'd expect independence across data points, but here the independence across data features so people have looked at that due John Lafferty and Ghahramani looked at in the context of semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I looked at it in the context of Model GP LVM and Charles Kemp.",
                    "label": 0
                },
                {
                    "sent": "Josh Tenenbaum have also looked at models in this type and Josh is still working on these type of models.",
                    "label": 0
                },
                {
                    "sent": "So maximum likelihood in this model, though, As for all maximum entropy models, is equivalent to maximum entropy under the distance constraints, so you don't have to worry about the maximum formulation anymore.",
                    "label": 1
                },
                {
                    "sent": "You have to do is fit this model by maximum likelihood in your satisfying the maximum entropy set up.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a side note, there's a weird thing going on here.",
                    "label": 0
                },
                {
                    "sent": "'cause maximum likelihood is normal.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consistent.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we increase the number of data points, parameters become.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better determined not.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this model, as you increase the number of data points, you actually increase the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "It's nonparametric in this set.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what does happen is you increase the number of data features.",
                    "label": 0
                },
                {
                    "sent": "The parameters become better determined, so you can do a consistency results in data features instead of data points.",
                    "label": 1
                },
                {
                    "sent": "So that's a rather weird thing, because it turns the large piece small end problem on its head for this model.",
                    "label": 1
                },
                {
                    "sent": "Large N small P is a problem.",
                    "label": 0
                },
                {
                    "sent": "Large P small N is not a problem at all.",
                    "label": 0
                },
                {
                    "sent": "It's very well.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Determined in that region.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of blessing of dimensionality in this model.",
                    "label": 1
                },
                {
                    "sent": "Now I think that reflects something which people sometimes miss when they talk about large P. Small and it's all about model definition.",
                    "label": 1
                },
                {
                    "sent": "If this model is correct, then you don't have a problem with large P small N. If it's incorrect then you know none of these results hold anyway, so it shows that sort of a model like this is going to be very good if you got very high dimensional data and load.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update points",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So relations to others.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Methods so quick.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Laplacians.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Google Maps is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very similar because in a plastic Tiger match you specify Laplacian graph Laplacian directly and you parameterise it without well by either local distances without looking at the global data set.",
                    "label": 0
                },
                {
                    "sent": "Now then you use the smallest eigenvalues of this Laplacian are used for visualizing the data, discarding the constant eigenvector.",
                    "label": 1
                },
                {
                    "sent": "But that is exactly the same operation is taking the largest eigenvalue vectors of K, because this is the inverse of that and this H ax to discard the constant eigenvector.",
                    "label": 0
                },
                {
                    "sent": "So you have to.",
                    "label": 0
                },
                {
                    "sent": "You always get this constant eigenvector, which is the smallest one, and you have to throw.",
                    "label": 0
                },
                {
                    "sent": "That way it's got an eigenvalue of 0.",
                    "label": 0
                },
                {
                    "sent": "But basically this operation of centering has the same effect.",
                    "label": 0
                },
                {
                    "sent": "In this case here so.",
                    "label": 0
                },
                {
                    "sent": "It's sort of equivalent, but Laplacian Eigen Maps do not preserve distances between neighbors in the graph that you specify, so they're also based on neighbors, but they don't do anything to try and preserve the distances between those neighbors.",
                    "label": 0
                },
                {
                    "sent": "So locally linear.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Embedding so the Laplacian has to be constrained positive DEF.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one way of imposing that constraint is by factorizing the Laplacian like this.",
                    "label": 0
                },
                {
                    "sent": "So L is equals MM transpose.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to ensure it's all a plassey and you then would have to need to further constrain M such that M transpose one.",
                    "label": 0
                },
                {
                    "sent": "Well, this is one way of doing it is equal to zero.",
                    "label": 0
                },
                {
                    "sent": "That forces this thing to be a Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that gives you the values for the diagonals of M that the diagonal of EM has to be there.",
                    "label": 0
                },
                {
                    "sent": "Some of the negative off diagonals, and in this case you would set the off diagonals to be 0.",
                    "label": 0
                },
                {
                    "sent": "If two points are not neighbors.",
                    "label": 0
                },
                {
                    "sent": "Actually that doesn't give you the same neighborhood structure in L that you had in M. You get slightly different neighborhood structure, but this would be one way of parameterising the model.",
                    "label": 0
                },
                {
                    "sent": "You just wouldn't get quite the same neighborhoods.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So locally linear embedding's and then the specific case of me U where you make a further constraint.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That these diagonal sums are further constrained to unity.",
                    "label": 0
                },
                {
                    "sent": "So these guys here you also constrain them to be unit.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the model parameters are found by maximizing the pseudo likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "So instead of maximizing the likelihood directly, you maximize the pseudo likelihood, which is typically.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quicker.",
                    "label": 0
                },
                {
                    "sent": "I'm so.",
                    "label": 0
                },
                {
                    "sent": "That was, at least to some ideas about me, you, one of which is the fact that it's approximation to maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the plastic has this particular factorized form that.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can use so you get this user likelihood for relatively quick Param.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attract optimization and ignoring that partition function removes the need to invert to recover the convergence matrix, which speeds things up.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this means that Eli can be applied to larger datasets than me.",
                    "label": 0
                },
                {
                    "sent": "You or envy you.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typically, but the sparsity pattern for the Laplacian fairly will not typically match that from the standard algorithms due to this.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factorize representation OK button.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point about it?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a little bit odd, so early.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motivated by considering locally linear embeddings of the data, but as you increase the neighborhood size of the data to make the whole thing in neighborhood, so caysen minus one.",
                    "label": 1
                },
                {
                    "sent": "So everything's a neighbor.",
                    "label": 0
                },
                {
                    "sent": "You don't recover PCA, but PCA is the optimal linear embedding.",
                    "label": 0
                },
                {
                    "sent": "So there's something funny there, and it's because Elyse optimizing this pseudo likelihood.",
                    "label": 1
                },
                {
                    "sent": "So in contrast, this maximum entropy unfolding algorithm an maximum variance unfolding, which LL is approximating, does recover PCA when K is equal to N -- 1.",
                    "label": 0
                },
                {
                    "sent": "It just returns to sort of standard principle coordinate analysis.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I some apps also interest.",
                    "label": 0
                },
                {
                    "sent": "It's not quite as closely related, but you create a sparse graph.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some distances and you fill in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For non neighbors with the shortest path algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's going in envy?",
                    "label": 0
                },
                {
                    "sent": "You and me you you can start effectively with the sparse graph of squared distances, and then you fill in the other distances by either maximizing the total variance or the entropy.",
                    "label": 1
                },
                {
                    "sent": "So this is typically a faster operation, filling in with the shortest path algorithm and certainly for maximum entropy unfolding the distances you're getting between points have an interpretation as a sort of diffusion walk between two data points rather than the shortest path walk, so they are sort of quite closely related, but Isomap slightly quicker.",
                    "label": 0
                },
                {
                    "sent": "OK, so I won't talk about the GPL V.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "M 'cause I'm short on time.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm so I just want.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a couple of datasets.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first of which is.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A motion capture data set so it's 3 dimensional is a per.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And doing a run for three strides and we're using a point cloud of the positions of all their limbs in 3D.",
                    "label": 0
                },
                {
                    "sent": "So XY zed positions of each point on their body and at least 102 dimensional data containing 55 frames of motion.",
                    "label": 0
                },
                {
                    "sent": "So you should see in the visualization sort of starting position followed by three.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strides have run.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at the dominant to eigen values, the eigenvectors associated with him.",
                    "label": 0
                },
                {
                    "sent": "What you see is the result from Alaskan Eigen Maps and the results from locally linear.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Heading uses Isomap and this is Max.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Invariants unfolding, and this is the new algorithm me you now.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say with this much choose between them.",
                    "label": 0
                },
                {
                    "sent": "This is a quality measure that was suggested by Stefan Harmeling, which involves using a Gaussian process to map from the latent space to the data space.",
                    "label": 0
                },
                {
                    "sent": "The details are in the paper, but if you don't believe this quality measure, then what you see is the methods that are preserving the Inter neighbor distances on all these.",
                    "label": 0
                },
                {
                    "sent": "Sorry this one that I haven't talked about, but look at these three.",
                    "label": 0
                },
                {
                    "sent": "These three methods that are preserving into neighbor distances do typically much better than the ones that don't.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another data set.",
                    "label": 0
                },
                {
                    "sent": "It's a similar story, but it's a noisy data set, so it's a robot moving around in a circle and the robot arm is reading Wi-Fi Bout 20 Wi-Fi access points.",
                    "label": 0
                },
                {
                    "sent": "It's reading the strength signal strength, but it should be 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "It's movement, so we looked for the inherent dimensionality of this data to be 2.",
                    "label": 0
                },
                {
                    "sent": "So in this case Laplacian Eigen Maps fails and locally linear.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bedding sort of also fails.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other methods, Isomap this, is reasonably good job.",
                    "label": 0
                },
                {
                    "sent": "It comes in from one corridor and then it does a circle this way.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About here envy you.",
                    "label": 0
                },
                {
                    "sent": "So I think this is characteristic of this.",
                    "label": 0
                },
                {
                    "sent": "I don't have any proofs about this, but I think that this very noisy response is characteristic of the trace norm.",
                    "label": 0
                },
                {
                    "sent": "Let's apply because it has.",
                    "label": 0
                },
                {
                    "sent": "It tries to push all the information into the low dimensional space, and that's where you get these nice eigenvalue Spectra.",
                    "label": 0
                },
                {
                    "sent": "And here's the result from me.",
                    "label": 0
                },
                {
                    "sent": "You so they're sort of similar as well.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you get the similar story that under the quality measure, which I haven't really talked about in any detail, you see, these three methods are much better than the methods that failed to preserve distances.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think the main thing takeaway message from the paper it's not about.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So while there's a fantastic new dimensionality reduction algorithm, 'cause I don't, it's all much of a muchness I think, which is doing better, and that's certainly not the.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention, but to show that there are three separate stages using existing spectral dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So the first stage I think this is actually the most important in neighborhood between the data points is selected.",
                    "label": 0
                },
                {
                    "sent": "If you knew the neighborhood, I think your problem is done, and it doesn't actually matter what method you follow.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you typically don't, but then the things that we've been focusing in the Internet to interpoint distances between neighbors are fed to algorithms which provide a similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "The way the entries in this similarity matrix or covariance matrix or kernel matrix are computed is the main difference between the different algorithms, so they're all doing different things to compute those element.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the relationship between the points in the similarity matrices then visualize using eigenvectors of the similarity matrix.",
                    "label": 1
                },
                {
                    "sent": "So it's what I'm arguing is that you're describing a Gaussian random field with that similarity matrix, is inverse is sparse, and that the final step is that you're visualizing a Gaussian random field with the spectral visualization.",
                    "label": 0
                },
                {
                    "sent": "So it's three step one.",
                    "label": 0
                },
                {
                    "sent": "You're defining a neighborhood.",
                    "label": 0
                },
                {
                    "sent": "OK, we're doing nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "We haven't talked about that.",
                    "label": 0
                },
                {
                    "sent": "Then the next stage is you've got a Gaussian random field, and you're fitting the parameters of that Gaussian random field.",
                    "label": 0
                },
                {
                    "sent": "By different ways and then the final step is you're visualizing that graph.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's many, many different ways of graph visualization that are non spectral and sometimes spectral visualization is not a good idea.",
                    "label": 0
                },
                {
                    "sent": "So I think it's worth remembering that these steps is sort of separate and that you could do different things.",
                    "label": 0
                },
                {
                    "sent": "So that's why I said there.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But sometime.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inflating these?",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The steps allows for much faster algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So mixing the second and third step allow speed.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By never computing the similarity matrix directly and.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what's happening in Eli and Eli, so I've shown results which are not as good for them, but of course they can do massive datasets very very, very fast, and indeed you could even argue that.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Page rank is some variant of these type of algorithms, and that's doing the entire web and it's doing that 'cause it never explicitly computes that matrix K, it only compute.",
                    "label": 0
                },
                {
                    "sent": "It only looks at the inverse.",
                    "label": 0
                },
                {
                    "sent": "So, but I think we can understand these algorithms from the unifying perspective while exploiting computational advantages from the shortcut.",
                    "label": 0
                },
                {
                    "sent": "And I've been thinking about this for a long time and so.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The conversations with all these people have sort of influenced the thinking in the work and that's it.",
                    "label": 0
                },
                {
                    "sent": "So I think one of the ideas.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The discussion is to sort of put work in a broader research context.",
                    "label": 0
                },
                {
                    "sent": "Annmulle made it kind of hard for me because he related his work through pretty much all Seminole papers in a manifold learning over the last decade.",
                    "label": 0
                },
                {
                    "sent": "So I'll try to do.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say something anyway, so just to give you a timeline.",
                    "label": 0
                },
                {
                    "sent": "I sort of think that the first manifold learner sort of in the about that came up about 10 year, 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "There were instantiations of kernel PCA that basically use some sort of handcrafted kernels and putting kernels between quotes here because the feature space basically changes if you add extra data.",
                    "label": 0
                },
                {
                    "sent": "So they're not really Mercer kernels in that sense, but there were.",
                    "label": 0
                },
                {
                    "sent": "They were handcrafted kernels were for incident.",
                    "label": 0
                },
                {
                    "sent": "I zone map.",
                    "label": 0
                },
                {
                    "sent": "The Colonel was given by some center geodesic distance matrix in Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Eigen Maps will be the inverse graph, Laplacian matrix and Ellie well.",
                    "label": 0
                },
                {
                    "sent": "Handcrafted thing is not really true anymore because there's a little bit of learning in how you determine the reconstruction weights.",
                    "label": 0
                },
                {
                    "sent": "But what you see?",
                    "label": 0
                },
                {
                    "sent": "More recently, I think in manifold Learning is an interest shift towards trying to learn a good kernel instead of instead of handcrafting it.",
                    "label": 0
                },
                {
                    "sent": "And, uh, so the initial example Neil mentioned already.",
                    "label": 0
                },
                {
                    "sent": "Is this maximum variance in folding technique?",
                    "label": 0
                },
                {
                    "sent": "But more recently it was also a nice paper in HTML by Blake and Sean Gold, structure preserving embedding and basically what these techniques do is they they put some rods between your between your data points so they that.",
                    "label": 0
                },
                {
                    "sent": "Basically I'm for some local distance constraints and then they maximize or minimize some objective that tries to minimize the rank of the solution.",
                    "label": 0
                },
                {
                    "sent": "An maximum entropy and falling is basically another.",
                    "label": 0
                },
                {
                    "sent": "Another technique that follows the same the same path.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to talk about two things.",
                    "label": 0
                },
                {
                    "sent": "So the first one is basically manifold learning versus generative modeling.",
                    "label": 0
                },
                {
                    "sent": "The one connection that Neil Luckily didn't discuss was between maximum entropy Enfolding and his own work on Gaussian process process, latent variable models.",
                    "label": 0
                },
                {
                    "sent": "Which are you can think of, basically as some sort of nonlinear version of probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "So both these models basically model P of Y as a Gaussian random field, where each data point.",
                    "label": 0
                },
                {
                    "sent": "Is a note in the Gaussian random fields the key difference, however, is in how the covariance of this Gaussian random field is obtained.",
                    "label": 0
                },
                {
                    "sent": "So in the in the GP LVM model it's basically just just an inner product of of the latent space coordinates, whereas in maximum entropy unfolding it's this kernel where basically the LR.",
                    "label": 0
                },
                {
                    "sent": "This LaGrange multipliers that you're that you're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "An so So what seems to be going on in maximum entropy unfolding is that it sort of tries to unify two seemingly very different approaches.",
                    "label": 0
                },
                {
                    "sent": "So the one is 1 is manifold learning and the other one is generative modeling.",
                    "label": 0
                },
                {
                    "sent": "And here's how I think why.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're sort of what it seems to be sort of sort of difference.",
                    "label": 0
                },
                {
                    "sent": "So in manifold learning you're basically trying to learn a smooth mapping from your data space to your latent space, which means that if you have two similar data points, then these two similar data points cannot be very far apart in your latent space.",
                    "label": 0
                },
                {
                    "sent": "In your final embedding, because otherwise the mapping wouldn't be smooth and generative.",
                    "label": 0
                },
                {
                    "sent": "Modeling you basically have just moved mapping the other way around, right?",
                    "label": 0
                },
                {
                    "sent": "You have a smooth mapping from the latent space too.",
                    "label": 0
                },
                {
                    "sent": "The data space.",
                    "label": 0
                },
                {
                    "sent": "So what that means is basically if you have two dissimilar data points then they cannot.",
                    "label": 0
                },
                {
                    "sent": "They cannot be close together, so so manifold learning is seeing a sense.",
                    "label": 0
                },
                {
                    "sent": "Trying to preserve local structure, getting similar stuff close together in your embedding, whereas generic models like probabilistic PCA or GP LVM or just trying to make sure that this similar points don't it up close to close together, which is which is a bit different.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maximum entropy and falling.",
                    "label": 0
                },
                {
                    "sent": "I think this is the first technique that sort of tries to combine the best of two worlds.",
                    "label": 0
                },
                {
                    "sent": "So one trying to preserve local data structure in your embedding an in the other one trying to have a probabilistic framework which allows us to do to deal with missing data.",
                    "label": 1
                },
                {
                    "sent": "Or do hierarchical models or stuff like that, but the current formulation in maximum entropy unfolding as an oddity, which is that the embedding.",
                    "label": 1
                },
                {
                    "sent": "Basically, doesn't appear as a latent variable in the model, so if you think about dimension reduction, if you think that there are some low dimensional latent latent data representation that that was basically underlying your data, that would be nice.",
                    "label": 0
                },
                {
                    "sent": "If you had a generative model where you basically infer would infer those latent variables.",
                    "label": 0
                },
                {
                    "sent": "But that's not what's happening in EM you if you, if you look at these at the formulation for PP of Y in MU.",
                    "label": 0
                },
                {
                    "sent": "There is no, um, this latent space just never shows up.",
                    "label": 0
                },
                {
                    "sent": "You just learn a kernel and then in you apply some spectral decomposition on that.",
                    "label": 0
                },
                {
                    "sent": "But you could basically do any NDS over in there, so that's an oddity that that is one of the things I guess that could be that people could work on trying to improve.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other thing I wanted to mention is about the rank minimization.",
                    "label": 0
                },
                {
                    "sent": "So basically the way I think of this of this rank minimization is basically controlling what we do with this similar data in our embedding.",
                    "label": 0
                },
                {
                    "sent": "So in medical learning we try to we try to retain local structure, right?",
                    "label": 0
                },
                {
                    "sent": "So we know what to do with sort of similar data points.",
                    "label": 0
                },
                {
                    "sent": "But we need to figure out what we do with additional once and in the case of maximum variance unfolding, you're maximizing the sum of the kernel eigenvalues, which is basically maximizing variance, and I think will maximum entropy unfolding is doing is maximizing the sum of the log eigenvalues of the kernel, which is which is incense different, because if you have zero eigenvalue, basically that.",
                    "label": 0
                },
                {
                    "sent": "That term, you'd get a very negative log eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So, so it's doing a different type of.",
                    "label": 0
                },
                {
                    "sent": "Of Rang minimization, and I'm not sure which one is 1 is better.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I do know is that it's very important how you deal with this similar data in manifold learning wasn't nice paper by Miguel Carreira.",
                    "label": 1
                },
                {
                    "sent": "Prepping an ICM L this year where he basically compared stochastic neighbor embedding an impression eigen Maps and he showed that there are the same except for that there is a difference in the covariance constraint.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "So in how you deal with this similar data and this leads to two very very different results.",
                    "label": 0
                },
                {
                    "sent": "So I guess what you could say is that.",
                    "label": 0
                },
                {
                    "sent": "In medical learning and maybe more important, how you actually deal with this similar data, then how you deal with with similar data?",
                    "label": 0
                },
                {
                    "sent": "'cause that's fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "So recent successes they basically try to push this similar data as far away as possible, and I mean you seem to do something that's a little bit different, so it may lead to new insights.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "That's all I wanted to say, I guess.",
                    "label": 0
                }
            ]
        }
    }
}