{
    "id": "4nxv32d5mou5lajinei4afe2h6zxuipo",
    "title": "Shadow Dirichlet for Restricted Probability Modeling",
    "info": {
        "author": [
            "Maya Gupta, Department of Electrical Engineering, University of Washington"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_gupta_sdr/",
    "segmentation": [
        [
            "Good afternoon, this is joint work with Bella Frigga, can James Chen.",
            "And the problem we're looking at is how do we model the distribution of probability mass functions.",
            "So, for example, say we chose 15 of you and we asked you on a given day what's the probability that you prefer Thai or Indian or pizza for lunch.",
            "So we would collect these these 15 PM FS from each of you and we might plot them on the probability simplex as shown in the black.",
            "X is here.",
            "And then we might want to model the distribution of those black X is so that we can make inferences about the general distributions of these PMS.",
            "So a common model of machine learning to do this is the Deer Slayer model or the display distribution."
        ],
        [
            "The Jewish Way distribution only has one parameter Alpha.",
            "It's a vector of the number of components of your PMF, and it lets you select the mean of that distribution, the mean PMF and sort of how well concentrated the distribution is around the mean.",
            "And the jewishly distribution gives probability to every PMF in the probability simplex.",
            "In our paper, we propose a variant of the Jewishly distribution that we called the Shadowed Irish Lake.",
            "And we add a new parameter to the dish way, which is a matrix parameter M. It's a left stochastic matrix and what it does is it controls the support.",
            "So it's going to change what PMS are even possible.",
            "And we say that a random PMF data is distributed with a shadow deuschle distribution with parameters Alpha and M. If Theta is equal to M times Theta till day where Theta till they is distributed with the richly distribution.",
            "So very succinctly, you can say that we have this underlying jewishly distribution and we use the matrix M to project it into a subset of the simplex.",
            "So why is this a good way to generalize the Jewish law and make it more flex?"
        ],
        [
            "What allows us to enforce a number of constraints that turn up in machine learning applications?",
            "So in natural language processing, we might be interested in probability mass functions over words in a dictionary.",
            "And one of the constraints we might want to enforce is ordered probabilities.",
            "For example, the word the may be much more probable.",
            "We may want to force it to be much more probable than any other word, and this will let us do that.",
            "Another possibility is we might want to make sure that certain probabilities are close to each other.",
            "We might want to enforce that the probability of the word caffeine and the word coffee are not too far apart.",
            "Another type of constraint or regularised PMF?",
            "So we might have a generic language model Q, not an.",
            "We might expect that every realization if we're modeling sort of any particular person in their language model might be a regularization of any PMF and this generic language model Q not.",
            "And again, this restricts the support of possible PMF to be sort of shrunk around, Q not, and we can do that with the shadow jewishly distribution.",
            "So what we show?"
        ],
        [
            "In the paper in the poster are some maximum entry constructions of how to choose M to match certain constraints, and we give an EM algorithm and some experimental results.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, this is joint work with Bella Frigga, can James Chen.",
                    "label": 1
                },
                {
                    "sent": "And the problem we're looking at is how do we model the distribution of probability mass functions.",
                    "label": 1
                },
                {
                    "sent": "So, for example, say we chose 15 of you and we asked you on a given day what's the probability that you prefer Thai or Indian or pizza for lunch.",
                    "label": 1
                },
                {
                    "sent": "So we would collect these these 15 PM FS from each of you and we might plot them on the probability simplex as shown in the black.",
                    "label": 0
                },
                {
                    "sent": "X is here.",
                    "label": 1
                },
                {
                    "sent": "And then we might want to model the distribution of those black X is so that we can make inferences about the general distributions of these PMS.",
                    "label": 0
                },
                {
                    "sent": "So a common model of machine learning to do this is the Deer Slayer model or the display distribution.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Jewish Way distribution only has one parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "It's a vector of the number of components of your PMF, and it lets you select the mean of that distribution, the mean PMF and sort of how well concentrated the distribution is around the mean.",
                    "label": 0
                },
                {
                    "sent": "And the jewishly distribution gives probability to every PMF in the probability simplex.",
                    "label": 0
                },
                {
                    "sent": "In our paper, we propose a variant of the Jewishly distribution that we called the Shadowed Irish Lake.",
                    "label": 0
                },
                {
                    "sent": "And we add a new parameter to the dish way, which is a matrix parameter M. It's a left stochastic matrix and what it does is it controls the support.",
                    "label": 0
                },
                {
                    "sent": "So it's going to change what PMS are even possible.",
                    "label": 0
                },
                {
                    "sent": "And we say that a random PMF data is distributed with a shadow deuschle distribution with parameters Alpha and M. If Theta is equal to M times Theta till day where Theta till they is distributed with the richly distribution.",
                    "label": 0
                },
                {
                    "sent": "So very succinctly, you can say that we have this underlying jewishly distribution and we use the matrix M to project it into a subset of the simplex.",
                    "label": 0
                },
                {
                    "sent": "So why is this a good way to generalize the Jewish law and make it more flex?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What allows us to enforce a number of constraints that turn up in machine learning applications?",
                    "label": 0
                },
                {
                    "sent": "So in natural language processing, we might be interested in probability mass functions over words in a dictionary.",
                    "label": 0
                },
                {
                    "sent": "And one of the constraints we might want to enforce is ordered probabilities.",
                    "label": 0
                },
                {
                    "sent": "For example, the word the may be much more probable.",
                    "label": 0
                },
                {
                    "sent": "We may want to force it to be much more probable than any other word, and this will let us do that.",
                    "label": 0
                },
                {
                    "sent": "Another possibility is we might want to make sure that certain probabilities are close to each other.",
                    "label": 0
                },
                {
                    "sent": "We might want to enforce that the probability of the word caffeine and the word coffee are not too far apart.",
                    "label": 0
                },
                {
                    "sent": "Another type of constraint or regularised PMF?",
                    "label": 0
                },
                {
                    "sent": "So we might have a generic language model Q, not an.",
                    "label": 0
                },
                {
                    "sent": "We might expect that every realization if we're modeling sort of any particular person in their language model might be a regularization of any PMF and this generic language model Q not.",
                    "label": 0
                },
                {
                    "sent": "And again, this restricts the support of possible PMF to be sort of shrunk around, Q not, and we can do that with the shadow jewishly distribution.",
                    "label": 1
                },
                {
                    "sent": "So what we show?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the paper in the poster are some maximum entry constructions of how to choose M to match certain constraints, and we give an EM algorithm and some experimental results.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}