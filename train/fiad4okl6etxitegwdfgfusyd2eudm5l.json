{
    "id": "fiad4okl6etxitegwdfgfusyd2eudm5l",
    "title": "Human Motion Modelling through Dimensional Reduction with Gaussian Processes",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "Feb. 7, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/psm08_lawrence_hmm/",
    "segmentation": [
        [
            "OK so thanks Bella.",
            "So this was a pump priming project with the Oxford Brookes.",
            "So and the collaborator there was Phil Tour and Carl was the RA on the project and he's also a sort of student of fills and now jointly a student of mine."
        ],
        [
            "So quick overview will start with just hopefully very brief review of probabilistic dimensionality reduction.",
            "Just to give context for people who aren't aware of the model, and then really I want to spend more time on the model extensions which were undertaken during the pump priming project and then just conclusions and follow on work coming out of the pump priming."
        ],
        [
            "Correct, so first of all, probabilistic dimensionality reduction or very."
        ],
        [
            "Rapidly some notation Q will be my latent dimension.",
            "D will be my data dimension, N will be the number of data points.",
            "And I'm going to be using centered data, and it's in the form of design matrix, so it has N rows and columns, and then the latent variables are X.",
            "Then they have N rose in Q columns, and then I'll use briefly a mapping matrix which will map from the latent space to the data space, and that's going to be denoted by W and it's in the bike.",
            "You OK the key point about X&Y being design matrices is that the covariance matrix is given by Y, transpose, Y, the inner product matrix is given by YY transpose, so.",
            "Lots of kernel people get very excited when they see in a product matrices, so keep an eye out for those and covariance matrices.",
            "Robs interesting as well."
        ],
        [
            "So linear latent variable models are probabilistic model based on linear relationship says that a data point Y is given by a latent point X multiplied by this mapping matrix plus some noise.",
            "And we typically take this noise to be Gaussian with some sort of spherical covariance.",
            "Now."
        ],
        [
            "What's interesting is tipping and Bishop proved in about 99 and a Royal SoC paper that if I have this structure here, so this might latent spaces my data space.",
            "These are my parameters with the black I can marginalized out X to get this probability distribution independent over data points.",
            "And the nice thing is that I can maximize with respect to W, and if I do this maximization, it turns out that W is equal to UQLV transpose, where you Q are the first Q principal eigenvectors.",
            "If the covariance and the corresponding eigenvalues are Lambda, so what's important about that is it shows that W Prince spans the principle subspace of the data, so it's a probabilistic model for which the maximum likelihood solution is PCA.",
            "OK, unfortunately it's very difficult to non linearize this model in any obvious way, because as soon as you start not linearizing this relationship between X&Y, which was a linear relationship in the previous slide, then this marginalization to obtain this guy becomes intractable.",
            "So Gaussian process latent variable models do something different from a Bayesian perspective, not marginalizing W is as bad as not mines marginalizing X so from a Bayesian perspective, it's equally wrong."
        ],
        [
            "To do this, maximize with respect to X, the latent positions and marginalized out the mappings.",
            "Now the really cool thing about that is if I flip quickly between these two.",
            "You see, it doesn't take long to workout the proof of one from the other.",
            "Once you've seen the duality between them, you just get X is where the WSR and so and so forth.",
            "Because it's a linear model and that X is in WS appear in this dual form.",
            "This is like the probabilistic jewel, so the nice thing is when you do this you can work it out on the night of submission of your NIPS paper, because you've got typings proof in front of you.",
            "Tipping is the dealers.",
            "Tipping is the jewel of Lawrence I'm the Colonel tipping.",
            "So in the in this case though, you get an eigenvalue problem in the inner product matrix.",
            "So you Q and now the principal eigenvectors of the inner product matrix, and you're solving for X the embeddings rather than the mapping.",
            "And that's quite nice.",
            "Again, this V is an arbitrary rotation and L is dependent on the eigen values.",
            "In the same way.",
            "So this is also an interpretation for PCA, the differences."
        ],
        [
            "In one case you're solving this eigenvalue problem on the covariance.",
            "In the other cases, solving this on the inner product matrix.",
            "Now you might be saying I can kernel eyes here, and you can do that and then you will have kernel PCA, so that's not what we're going to do because it was published before there's an equivalent.",
            "So between these two relationships, and this is the starting point to derive kernel PCA."
        ],
        [
            "OK, so Gaussian processes in a slide Gaussian process are a bit like Gaussian distributions, only they have kernel function covariance function, so we're doing 0 mean Gaussian distribution, where X is some input.",
            "And why is some target and the kernel function is a function of X like little covariance function, like a sort of Mercer kernel, and the class is valid.",
            "Kernels are the same as the classes Mercer kernels, so that's one slide on Gaussian process."
        ],
        [
            "This is the linear kernel, so if you want a Gaussian process which is a regression model prior over linear models, this is the kernel you use.",
            "Now, what's interesting?",
            "If you look through this marginal likelihood, so I've actually written the conditional and the prior out here, but the thing to look at here is the marginal.",
            "What you'll see is this looks like a Gaussian process.",
            "Well, a product of Gaussian processes with linear kernels.",
            "So the question is, if I replace that with the kernel."
        ],
        [
            "I can then replace that kernel with a nonlinear kernel for nonlinear model, so those of you who are aware of PCA notice one thing.",
            "Kernel PCA is Colonel Ising on the data space.",
            "We're kernel Ising on the latent space.",
            "Now the ugly thing about that is that it's no longer an eigenvalue problem to solve."
        ],
        [
            "You can plug in an RBF kernel, but basically you have to solve the whole system by large evil optimization, which is a shame, but other than that you can also you can optimize jointly all the parameters of the kernel and it's a nice model for dimensionality reduction.",
            "So that was."
        ],
        [
            "As a Gaussian process, latent variable models in six minutes map solutions for dynamical models have been suggested.",
            "So you can do dynamical models within this framework.",
            "There's a nips paper by Jack Wang from 2005, NIPS published 2007.",
            "You can force the model to respect local distances, which it doesn't do naturally.",
            "That's work by Joaquin Kenyon, Eric, and Ellen myself, and those are sort of important in the following work.",
            "But now we're going to focus on the development variants made under the pump priming grant.",
            "So first of all, we've got sparse approximations for large data set.",
            "This optimization is an cubic and cube per gradient optimization.",
            "That's obviously not good, so one of the things we've worked on is producing a sparse approximation for large datasets.",
            "Hierarchical models for subject decomposition will show the example of that later, and three dimensional pose reconstruction from images and silhouettes.",
            "Those are the three sort of things we've looked at."
        ],
        [
            "OK, so the model extensions, so the first one I'm going to show you and I'm not going to talk much about the details of it.",
            "Because of the time factors, but is stacking Gaussian processes?",
            "So you've got a latent variable model where.",
            "You have a sort of a target and output, so in latent space and an output data space.",
            "Now you could build another latent variable model on the top of that, which was a prior on the on the previous latent space.",
            "So you just sort of do deep versions of these models by stacking them on top of each other.",
            "You can't then do the marginalization's, but you can seek map solutions.",
            "So why might you want to do that well?"
        ],
        [
            "As an example, so this is human motioning modeling of two guys walking through each other and high fiving each other, so two different subjects.",
            "Now they're correlated.",
            "You could model everything that's going on.",
            "You can see these things as angles of the humans position, so it's motion capture data converted to the angles of the limbs.",
            "You could model these guys jointly and projected into one latent space, but it seems far more sensible to model each subject with its own latent space, and then have a latent space that combines the two subjects together controlling what they do.",
            "And indeed, that's sort of what's going on here.",
            "Perhaps a more interest."
        ],
        [
            "Sing decomposition is this one, so the idea is in this, in this work is you're trying to add some of the ideas of conditional independent structures into latent variable modeling.",
            "So here this is a graphical model where you've got a root node which controls the abdomen of the subject and the upper body and the lower body.",
            "Then the limbs themselves are each controlled by a separate Gaussian process latent variable model.",
            "So the way to look at this graph is it's a.",
            "It's a dag.",
            "Well, it's a tree where each of the latent variables is.",
            "Well, reach of the relationships is represented by a Gaussian process.",
            "So we can look at a demo of that in practice, which hopefully since I'm skimming on the explanations will give you the the ideas.",
            "OK, so here's his data of a man running in a man walking.",
            "So what we've got at the top level and it's a bit slow because of Matlab computing all sorts of things.",
            "But at the top level, I've gotta walk and what you should see is I move the walk around is the what determines at the top level where the legs and the upper body are, and then on the right you get a man sort of walking along.",
            "But the nice thing here is I can just focus on one arm and move that up and down.",
            "Or I can move the upper body.",
            "I can move the upper body altogether and just focus on the upper body.",
            "This sort of model is important because you're getting a decomposition of the way human moves.",
            "If you guys see a human walking along.",
            "And he goes like that while he's walking.",
            "You don't think, oh, he can only wave while he's walking.",
            "You think he can wave so you need that decomposition so that you're modeling.",
            "I mean, the idea that you would have in which we haven't done yet is to have lots of data for modeling the arms and model a separate model of the arms which is just plugged in to these models.",
            "So when you want to do the run, you just plug in the right top mode, but it has a generic model of our movement below.",
            "One of the things you could do all these two things.",
            "Two ideas for what you could do with this one is in animation.",
            "This allows you to sort of work forming natural positions with a portion of the body.",
            "The Adam isn't a great example, but with just perhaps one leg or get a whole body global position and then perhaps on move on one arm only to change the position of the arm.",
            "Another idea is in tracking where this model has been used.",
            "You could do things like back off, so here if you're assuming the guys running and they're not actually running in the tracking, obviously you won't get a good model, but you could back down the hierarchy so you can drop down to this level and say, oh, I'll just have a generic model of what legs do, which is apparently they sort of do this sort of thing.",
            "Certainly for running and walking and a separate generic model of what upper bodies do.",
            "And so then I might be able to.",
            "So you can certainly get Irish dancing out of that, which is like legs running and.",
            "Biocide or something.",
            "Say, if you've got enough data, enough models going in, you could certainly do that.",
            "This is a sort of test case to show that's the case, and we want to extend this with more data, so on so forth.",
            "OK.",
            "So that's the first sort of extension hierarchical models."
        ],
        [
            "The next extension was complexity issues and this will be done in two slides, so I might give you exactly what you want and finishing early so Gaussian process is inherently have order N cubed complexity and order N squared storage.",
            "So that's not a good thing if you're dealing with 5000 data points.",
            "Sparse Gaussian process is if you do them right, you hopefully get order K ^2 N complexity and order KN storage.",
            "Now the next sort of extension is to apply what's called the fizzie approximation.",
            "So this is work by it's Nelson and Zubin Ghahramani, who called it sort of sparse sparse Gaussian processes.",
            "But can you narrow Incandela had a nice paper at Jameela?",
            "Which use different terminology built on their work and called it fits the approximation.",
            "Despite that, 2006 2005.",
            "That's before that.",
            "And the nice thing.",
            "One of the nice things about this, a lot of discussion and ideas behind these two papers were presented and developed to Pascal Workshop.",
            "The Gaussian Process Roundtable in Sheffield a couple of years back.",
            "So."
        ],
        [
            "Now, Graham Taylor had some results in human motion capture data at NIPS a couple of years ago.",
            "The data in this case is walking and running motions from a subject from the CMU MOCAP database.",
            "We combined the Gaussian process model I'm talking about with dynamical requirements, so it's got dynamics in the latent space and applied the model to their data.",
            "Now there's two missing data problems.",
            "I'll show you the reconstruction errors from one.",
            "The right leg was removed from the test sequence.",
            "And then the other the upper body was removed from the test sequence.",
            "So then all I'm going to show you I'm not going to show you."
        ],
        [
            "Nice videos, I'm just going to show you the angle errors for doing that when you try and reconstruct using their GP LVM.",
            "Now the point about this data is there's 2613 frames, so that's kind of large for this type of model, so we use this fizzie approximation with 100 so-called inducing points.",
            "That's the K and the models.",
            "We also use back constraints to do this."
        ],
        [
            "So, but the end messages that yes, we can do it and we beat quite good results from nearest neighbor.",
            "So this is a nearest neighbor reconstruction by looking in the training data, finding the nearest neighbor and this is by scaling that all the training data variance one before you look for the nearest neighbor.",
            "And this is by not scaling the training data so the scaled data works better for the leg but worse for the better for the body but worse for the leg and the unscaled works better for the leg and worse for the body.",
            "The GP LVM.",
            "We use three different dimensional latent Space, 3 dimensions, 4 dimensions of five dimensions.",
            "The five dimension was worse on both examples, but the four dimensional was better at reconstructing the leg and the three dimensional was completely reconstructing the body.",
            "These are average root mean squared angle errors, so it's 2 degrees of angle error, which is reasonably small.",
            "Bit away.",
            "I mean you might think oh body, that's a lot more angles, but they are.",
            "But there are a lot smaller, so you tend to make more error on the leg.",
            "'cause even though there's fewer angles, they're doing much more so you expect to make more error on the leg as indeed you do when you're reconstructing the body.",
            "A lot of what you're reconstructing is the spine and the angle movements in the spine.",
            "When you're walking a pretty small.",
            "So for those they don't have a significant contribution to the errors, more the arms."
        ],
        [
            "OK.",
            "So the final sort of extensions.",
            "Shared Gaussian process latent variable models.",
            "So what we were really interested in doing one of the targets of this project was to do joint models of human motion and images.",
            "So you've got some image features and you've got human motion angles that are captured jointly.",
            "In this case, I can't remember which way around they are, but let's say why is the why is, I think the images and zed is the angles from the human motion.",
            "What you do is you say these guys live in a shared latent space and from this latent space you go, you take a position in this latent space and you can map out to either the images or the image features or the human motion angles, and the idea is that given the image, I can then reverse this model and move back up.",
            "Here from the image to X and then predict what the motion the angle motion should be.",
            "This structure of model were sort of 1st proposed by Aaron Sean at Nips again 2005 NIPS and you basically do an objective way.",
            "You have two Gaussian process latent variable models with a shared latent space and you optimize jointly over these X is together.",
            "In some sense, it's a bit of a funny model there."
        ],
        [
            "We have talked about it as being a CCA like model.",
            "It isn't really because if you think of the linear case, it just collapses back to PCA again.",
            "It's like PCA just over 2 subsets of the data.",
            "We also added to that dynamical priors on X to improve the quality of the results and these back constraints.",
            "Now I haven't talked much about these back constraints, but what they do is they force a bijective mapping between X&Z so they force bijective mapping between the latent space and the pose.",
            "What's the reason for that?",
            "Well, the theory is.",
            "But if I've got pose, I know everything I need to generate the image.",
            "If I have the image, I don't have all the information I need to generate the pose.",
            "So if you believe the poses everything you need to generate the image, you should be able to construct a mapping from pose to image to feature, but you should never be able to propose to latent space to image, but you should never be able to generate a mapping from image to latent space to pose, because there's ambiguities in the image about what's going on.",
            "This you could have legs obscuring each other, and in fact in our early results we use a data set from alcohol and tricks which is based on silhouettes.",
            "There's lots of ambiguities in silhouette.",
            "The dynamics are important for resolving these ambiguities.",
            "When you see that these ambiguities are present, you can resolve them by working out.",
            "I mean, if you watch the silhouette move, you can actually know what it's doing.",
            "You may not know if it's one frame, but if you watch it move, you can typically resolve the ambiguities, and that's being done through your dynamical models, and that's why we have to incorporate dynamics as well.",
            "OK, so there's a slight issue with this which is in this case.",
            "This is an example with two D latent space just to show you so I can visualize the latency."
        ],
        [
            "Here's a pose.",
            "And then here's probability contours in the latent space.",
            "Now we would expect this post to be multimodal, yes, but I mean each of these white areas.",
            "There's a mode in it somewhere, or perhaps even multiple modes.",
            "You're getting lots and lots of modes from one silhouette because there's lots of local minima when you learn these sort of mappings."
        ],
        [
            "So it's a highly multimodal latent space.",
            "Given the silhouette, the idea is you you present the silhouette, you find a mode and then you project out to the pose.",
            "Now a lot of these modes are reasonable, but it's really nasty optimization space.",
            "However, Despite that, with the dynamics, we were able to."
        ],
        [
            "To get results like this.",
            "OK here is.",
            "This is an example from the test data.",
            "The training data is the silhouette is artificially generated using poser.",
            "It's data from Agilent rigs.",
            "What you see going on here is the actual silhouette, where putting his input to the model.",
            "What you see here is the known ground truth from that silhouette.",
            "What you see here is the reconstruction.",
            "Using this model we just talked about.",
            "Now you'll notice it's well, OK. First of all, there's an optical illusion which makes it look like he's turning the wrong way at times.",
            "I'll show you different video that shows a different angle to resolve that in a second, but you'll notice is that he does fairly well, although at some points he'll stutter on the turn.",
            "The reason is because he doesn't have data in the training data about that turn, So what tends to happen is he sort of pauses like there.",
            "And he waits for the test data to get to a point where he's got data and then carries on.",
            "But other than that, it's fairly natural motion.",
            "And that's just an issue of getting more training data in the other results we're showing here are doing regression from Silhouette features to angle features, which is 1 sort of proposed technique.",
            "So it's a 3D position features.",
            "These aren't angle, so in this case these aren't angle features we're using.",
            "These are 3D point clouds were using, so it's not using any constraints on limb lengths.",
            "So This is why you see these people shrinking, because these are just 3D point clouds.",
            "It's predicting if you do regression in that, that sort of affect you get.",
            "OK, so to try and resolve that I don't know how many people.",
            "How many people notice the optical illusion of him turning in another direction?",
            "No one there, how good noticed it?",
            "Maybe you with a review of it.",
            "OK so here is with different angles.",
            "You can see that he's actually rotating around.",
            "You can also see the starter step there.",
            "You can also.",
            "The optical illusion is also present in the ground truth.",
            "So this is an above angle to show that he's always turning in the same direction, which is what the silhouettes actually doing.",
            "OK, but we did see that the walls."
        ],
        [
            "A sort of problem with that this multimodal nasty evil latent space, which is real pig to optimize over.",
            "We tend to use things like we discretize the latent space and we use retreat is hidden Markov model, which you can do to get a first pass global discrete Optima and then move out from there to get such good results.",
            "But this is a better thing to do."
        ],
        [
            "So one reason we think that that's going on is we're not informing the model about the structure of the problem, so this is a development.",
            "The modified model does this.",
            "It has opposed specific latent space, A shared information, latent space, and an image specific latent space.",
            "So what you're basically doing, don't worry bout the ARP arrows.",
            "I'm not going to really talk about them.",
            "Worry about the split of the latent space.",
            "This latent space here can have influence on both zed an Y.",
            "This latent space only has influence on zed.",
            "And this latent space only has influence on why.",
            "So what's going on?",
            "There were trying to force this latent space to contain all the ambiguities in the problem.",
            "So if there are issues of silhouettes facing in the wrong way or leg position ambiguities by making sure that this space controls both the information that shared and this space controlled separate information, we can do that.",
            "But when we did this?"
        ],
        [
            "We know with the GP LVM that initialization is crucial.",
            "Getting the right initialization is crucial, so we spend a bit of time thinking about how to initialize this model and the way we did it was to initialize this shared latent space by CCA.",
            "And then look for the non shared latent spaces for orthogonal directions to CCA to inhabit the.",
            "So we look for large variance directions which were orthogonal to the CCA subspace.",
            "But we didn't just you see."
        ],
        [
            "Yeah, we used kernel CCA and we tried various kernels and we scored the quality of those kernels through GP LVM likelihoods.",
            "This is suggestion by Stefan Harmeling while he was up at Edinburgh to basically do lots of different kernels.",
            "Look at your embeddings, do GP, LVM, likelihood and then score the quality of the embedding using that.",
            "So we did exactly that.",
            "Now what was interesting then is that's our initialization, but it."
        ],
        [
            "Look so nice that we never trained the model so the initialization is all done spectrally and in fact the model is all spectral in this case, so we're just using the GP LVM as a probabilistic framework.",
            "On top of that.",
            "So in this case here this is can't remember the name of the human either.",
            "Data set from Michael Black.",
            "I like the features aren't silhouettes.",
            "I can't remember the name of them, but their sift like features emailed Carl but he didn't get back to me in time.",
            "So that's it like features are different types of features.",
            "Now the ambiguity is present in this.",
            "If this was a silhouette, are kind of what's reflected here there's leg ambiguities as to how far forward the legs are, so these modes you see which are in the post specific latent space with visualizing that that's only two dimensional.",
            "This is I'm facing towards the camera, and these are my position.",
            "My legs along here, it's actually all the way up there because it thinks it's probably got the legs and this is I'm facing away from the camera and the leg ambiguity as you're facing away from the camera.",
            "So instead of the sort of nasty multimodality we had before, you're getting this much sort of smoother.",
            "In this case, the multimodality is much more distinct.",
            "It's which leg is forward, so this is like left leg forward.",
            "And this is like right leg forward.",
            "It's getting additional cues 'cause it's not just silhouette based, so that's why the one of the modes instead of dominating and then I can sort of as a final example, show you.",
            "My Bibtex compilation of the slides or.",
            "Just a video of this is 4 frames per second, so it's slowed up because to compute each of these whole space the energy in the whole space takes a bit of time.",
            "So basically you're seeing as the lady moves around, you're seeing the ambiguities and how they change so much simpler ambiguities are much easier to resolve, and there's actually no dynamical model in this work at the moment.",
            "That's something we're going to add, but it really seems to have tidied up one of the problems in the earlier model, despite the fact that model.",
            "Worked very well in practice, OK?"
        ],
        [
            "So."
        ],
        [
            "Illusions so the Gaussian process latent variable model is a probabilistic, non linearized generalization of PCA and in the pump priming project.",
            "We've looked at three extensions which I've tried to briefly summarize, hopefully give you the idea.",
            "First of all, hierarchical representations of the model.",
            "Secondly, how to work with larger datasets, and then Thirdly these shared latent space models.",
            "So follow up come out of this is Carl's visiting Trevor Darryl's Group in Berkeley, 'cause they're very interested in these sort of techniques later in the year.",
            "Travis moving from my Martita Berkeley so he's waiting till they've moved.",
            "There's an imminent EPS RC application that's building on this work, and we have other lots of other ongoing work on Gaussian processes, differential equations, and human motions.",
            "So we've got lots of nice followups coming out of it, and."
        ],
        [
            "There's some references."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so thanks Bella.",
                    "label": 0
                },
                {
                    "sent": "So this was a pump priming project with the Oxford Brookes.",
                    "label": 0
                },
                {
                    "sent": "So and the collaborator there was Phil Tour and Carl was the RA on the project and he's also a sort of student of fills and now jointly a student of mine.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So quick overview will start with just hopefully very brief review of probabilistic dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Just to give context for people who aren't aware of the model, and then really I want to spend more time on the model extensions which were undertaken during the pump priming project and then just conclusions and follow on work coming out of the pump priming.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correct, so first of all, probabilistic dimensionality reduction or very.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rapidly some notation Q will be my latent dimension.",
                    "label": 1
                },
                {
                    "sent": "D will be my data dimension, N will be the number of data points.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to be using centered data, and it's in the form of design matrix, so it has N rows and columns, and then the latent variables are X.",
                    "label": 1
                },
                {
                    "sent": "Then they have N rose in Q columns, and then I'll use briefly a mapping matrix which will map from the latent space to the data space, and that's going to be denoted by W and it's in the bike.",
                    "label": 0
                },
                {
                    "sent": "You OK the key point about X&Y being design matrices is that the covariance matrix is given by Y, transpose, Y, the inner product matrix is given by YY transpose, so.",
                    "label": 1
                },
                {
                    "sent": "Lots of kernel people get very excited when they see in a product matrices, so keep an eye out for those and covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "Robs interesting as well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So linear latent variable models are probabilistic model based on linear relationship says that a data point Y is given by a latent point X multiplied by this mapping matrix plus some noise.",
                    "label": 1
                },
                {
                    "sent": "And we typically take this noise to be Gaussian with some sort of spherical covariance.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's interesting is tipping and Bishop proved in about 99 and a Royal SoC paper that if I have this structure here, so this might latent spaces my data space.",
                    "label": 0
                },
                {
                    "sent": "These are my parameters with the black I can marginalized out X to get this probability distribution independent over data points.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that I can maximize with respect to W, and if I do this maximization, it turns out that W is equal to UQLV transpose, where you Q are the first Q principal eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "If the covariance and the corresponding eigenvalues are Lambda, so what's important about that is it shows that W Prince spans the principle subspace of the data, so it's a probabilistic model for which the maximum likelihood solution is PCA.",
                    "label": 0
                },
                {
                    "sent": "OK, unfortunately it's very difficult to non linearize this model in any obvious way, because as soon as you start not linearizing this relationship between X&Y, which was a linear relationship in the previous slide, then this marginalization to obtain this guy becomes intractable.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian process latent variable models do something different from a Bayesian perspective, not marginalizing W is as bad as not mines marginalizing X so from a Bayesian perspective, it's equally wrong.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do this, maximize with respect to X, the latent positions and marginalized out the mappings.",
                    "label": 0
                },
                {
                    "sent": "Now the really cool thing about that is if I flip quickly between these two.",
                    "label": 0
                },
                {
                    "sent": "You see, it doesn't take long to workout the proof of one from the other.",
                    "label": 0
                },
                {
                    "sent": "Once you've seen the duality between them, you just get X is where the WSR and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "Because it's a linear model and that X is in WS appear in this dual form.",
                    "label": 0
                },
                {
                    "sent": "This is like the probabilistic jewel, so the nice thing is when you do this you can work it out on the night of submission of your NIPS paper, because you've got typings proof in front of you.",
                    "label": 0
                },
                {
                    "sent": "Tipping is the dealers.",
                    "label": 0
                },
                {
                    "sent": "Tipping is the jewel of Lawrence I'm the Colonel tipping.",
                    "label": 0
                },
                {
                    "sent": "So in the in this case though, you get an eigenvalue problem in the inner product matrix.",
                    "label": 0
                },
                {
                    "sent": "So you Q and now the principal eigenvectors of the inner product matrix, and you're solving for X the embeddings rather than the mapping.",
                    "label": 0
                },
                {
                    "sent": "And that's quite nice.",
                    "label": 0
                },
                {
                    "sent": "Again, this V is an arbitrary rotation and L is dependent on the eigen values.",
                    "label": 1
                },
                {
                    "sent": "In the same way.",
                    "label": 0
                },
                {
                    "sent": "So this is also an interpretation for PCA, the differences.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In one case you're solving this eigenvalue problem on the covariance.",
                    "label": 0
                },
                {
                    "sent": "In the other cases, solving this on the inner product matrix.",
                    "label": 0
                },
                {
                    "sent": "Now you might be saying I can kernel eyes here, and you can do that and then you will have kernel PCA, so that's not what we're going to do because it was published before there's an equivalent.",
                    "label": 0
                },
                {
                    "sent": "So between these two relationships, and this is the starting point to derive kernel PCA.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so Gaussian processes in a slide Gaussian process are a bit like Gaussian distributions, only they have kernel function covariance function, so we're doing 0 mean Gaussian distribution, where X is some input.",
                    "label": 1
                },
                {
                    "sent": "And why is some target and the kernel function is a function of X like little covariance function, like a sort of Mercer kernel, and the class is valid.",
                    "label": 0
                },
                {
                    "sent": "Kernels are the same as the classes Mercer kernels, so that's one slide on Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the linear kernel, so if you want a Gaussian process which is a regression model prior over linear models, this is the kernel you use.",
                    "label": 0
                },
                {
                    "sent": "Now, what's interesting?",
                    "label": 0
                },
                {
                    "sent": "If you look through this marginal likelihood, so I've actually written the conditional and the prior out here, but the thing to look at here is the marginal.",
                    "label": 0
                },
                {
                    "sent": "What you'll see is this looks like a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Well, a product of Gaussian processes with linear kernels.",
                    "label": 0
                },
                {
                    "sent": "So the question is, if I replace that with the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I can then replace that kernel with a nonlinear kernel for nonlinear model, so those of you who are aware of PCA notice one thing.",
                    "label": 1
                },
                {
                    "sent": "Kernel PCA is Colonel Ising on the data space.",
                    "label": 0
                },
                {
                    "sent": "We're kernel Ising on the latent space.",
                    "label": 0
                },
                {
                    "sent": "Now the ugly thing about that is that it's no longer an eigenvalue problem to solve.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can plug in an RBF kernel, but basically you have to solve the whole system by large evil optimization, which is a shame, but other than that you can also you can optimize jointly all the parameters of the kernel and it's a nice model for dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So that was.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a Gaussian process, latent variable models in six minutes map solutions for dynamical models have been suggested.",
                    "label": 0
                },
                {
                    "sent": "So you can do dynamical models within this framework.",
                    "label": 0
                },
                {
                    "sent": "There's a nips paper by Jack Wang from 2005, NIPS published 2007.",
                    "label": 0
                },
                {
                    "sent": "You can force the model to respect local distances, which it doesn't do naturally.",
                    "label": 1
                },
                {
                    "sent": "That's work by Joaquin Kenyon, Eric, and Ellen myself, and those are sort of important in the following work.",
                    "label": 1
                },
                {
                    "sent": "But now we're going to focus on the development variants made under the pump priming grant.",
                    "label": 1
                },
                {
                    "sent": "So first of all, we've got sparse approximations for large data set.",
                    "label": 0
                },
                {
                    "sent": "This optimization is an cubic and cube per gradient optimization.",
                    "label": 0
                },
                {
                    "sent": "That's obviously not good, so one of the things we've worked on is producing a sparse approximation for large datasets.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical models for subject decomposition will show the example of that later, and three dimensional pose reconstruction from images and silhouettes.",
                    "label": 1
                },
                {
                    "sent": "Those are the three sort of things we've looked at.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the model extensions, so the first one I'm going to show you and I'm not going to talk much about the details of it.",
                    "label": 0
                },
                {
                    "sent": "Because of the time factors, but is stacking Gaussian processes?",
                    "label": 1
                },
                {
                    "sent": "So you've got a latent variable model where.",
                    "label": 0
                },
                {
                    "sent": "You have a sort of a target and output, so in latent space and an output data space.",
                    "label": 0
                },
                {
                    "sent": "Now you could build another latent variable model on the top of that, which was a prior on the on the previous latent space.",
                    "label": 0
                },
                {
                    "sent": "So you just sort of do deep versions of these models by stacking them on top of each other.",
                    "label": 0
                },
                {
                    "sent": "You can't then do the marginalization's, but you can seek map solutions.",
                    "label": 0
                },
                {
                    "sent": "So why might you want to do that well?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As an example, so this is human motioning modeling of two guys walking through each other and high fiving each other, so two different subjects.",
                    "label": 0
                },
                {
                    "sent": "Now they're correlated.",
                    "label": 0
                },
                {
                    "sent": "You could model everything that's going on.",
                    "label": 0
                },
                {
                    "sent": "You can see these things as angles of the humans position, so it's motion capture data converted to the angles of the limbs.",
                    "label": 0
                },
                {
                    "sent": "You could model these guys jointly and projected into one latent space, but it seems far more sensible to model each subject with its own latent space, and then have a latent space that combines the two subjects together controlling what they do.",
                    "label": 0
                },
                {
                    "sent": "And indeed, that's sort of what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a more interest.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sing decomposition is this one, so the idea is in this, in this work is you're trying to add some of the ideas of conditional independent structures into latent variable modeling.",
                    "label": 0
                },
                {
                    "sent": "So here this is a graphical model where you've got a root node which controls the abdomen of the subject and the upper body and the lower body.",
                    "label": 0
                },
                {
                    "sent": "Then the limbs themselves are each controlled by a separate Gaussian process latent variable model.",
                    "label": 0
                },
                {
                    "sent": "So the way to look at this graph is it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a dag.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a tree where each of the latent variables is.",
                    "label": 0
                },
                {
                    "sent": "Well, reach of the relationships is represented by a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So we can look at a demo of that in practice, which hopefully since I'm skimming on the explanations will give you the the ideas.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's his data of a man running in a man walking.",
                    "label": 1
                },
                {
                    "sent": "So what we've got at the top level and it's a bit slow because of Matlab computing all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "But at the top level, I've gotta walk and what you should see is I move the walk around is the what determines at the top level where the legs and the upper body are, and then on the right you get a man sort of walking along.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing here is I can just focus on one arm and move that up and down.",
                    "label": 0
                },
                {
                    "sent": "Or I can move the upper body.",
                    "label": 0
                },
                {
                    "sent": "I can move the upper body altogether and just focus on the upper body.",
                    "label": 0
                },
                {
                    "sent": "This sort of model is important because you're getting a decomposition of the way human moves.",
                    "label": 0
                },
                {
                    "sent": "If you guys see a human walking along.",
                    "label": 0
                },
                {
                    "sent": "And he goes like that while he's walking.",
                    "label": 0
                },
                {
                    "sent": "You don't think, oh, he can only wave while he's walking.",
                    "label": 0
                },
                {
                    "sent": "You think he can wave so you need that decomposition so that you're modeling.",
                    "label": 0
                },
                {
                    "sent": "I mean, the idea that you would have in which we haven't done yet is to have lots of data for modeling the arms and model a separate model of the arms which is just plugged in to these models.",
                    "label": 0
                },
                {
                    "sent": "So when you want to do the run, you just plug in the right top mode, but it has a generic model of our movement below.",
                    "label": 0
                },
                {
                    "sent": "One of the things you could do all these two things.",
                    "label": 0
                },
                {
                    "sent": "Two ideas for what you could do with this one is in animation.",
                    "label": 0
                },
                {
                    "sent": "This allows you to sort of work forming natural positions with a portion of the body.",
                    "label": 0
                },
                {
                    "sent": "The Adam isn't a great example, but with just perhaps one leg or get a whole body global position and then perhaps on move on one arm only to change the position of the arm.",
                    "label": 0
                },
                {
                    "sent": "Another idea is in tracking where this model has been used.",
                    "label": 0
                },
                {
                    "sent": "You could do things like back off, so here if you're assuming the guys running and they're not actually running in the tracking, obviously you won't get a good model, but you could back down the hierarchy so you can drop down to this level and say, oh, I'll just have a generic model of what legs do, which is apparently they sort of do this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "Certainly for running and walking and a separate generic model of what upper bodies do.",
                    "label": 1
                },
                {
                    "sent": "And so then I might be able to.",
                    "label": 0
                },
                {
                    "sent": "So you can certainly get Irish dancing out of that, which is like legs running and.",
                    "label": 0
                },
                {
                    "sent": "Biocide or something.",
                    "label": 0
                },
                {
                    "sent": "Say, if you've got enough data, enough models going in, you could certainly do that.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of test case to show that's the case, and we want to extend this with more data, so on so forth.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the first sort of extension hierarchical models.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next extension was complexity issues and this will be done in two slides, so I might give you exactly what you want and finishing early so Gaussian process is inherently have order N cubed complexity and order N squared storage.",
                    "label": 0
                },
                {
                    "sent": "So that's not a good thing if you're dealing with 5000 data points.",
                    "label": 0
                },
                {
                    "sent": "Sparse Gaussian process is if you do them right, you hopefully get order K ^2 N complexity and order KN storage.",
                    "label": 0
                },
                {
                    "sent": "Now the next sort of extension is to apply what's called the fizzie approximation.",
                    "label": 0
                },
                {
                    "sent": "So this is work by it's Nelson and Zubin Ghahramani, who called it sort of sparse sparse Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "But can you narrow Incandela had a nice paper at Jameela?",
                    "label": 0
                },
                {
                    "sent": "Which use different terminology built on their work and called it fits the approximation.",
                    "label": 0
                },
                {
                    "sent": "Despite that, 2006 2005.",
                    "label": 0
                },
                {
                    "sent": "That's before that.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing.",
                    "label": 0
                },
                {
                    "sent": "One of the nice things about this, a lot of discussion and ideas behind these two papers were presented and developed to Pascal Workshop.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian Process Roundtable in Sheffield a couple of years back.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, Graham Taylor had some results in human motion capture data at NIPS a couple of years ago.",
                    "label": 1
                },
                {
                    "sent": "The data in this case is walking and running motions from a subject from the CMU MOCAP database.",
                    "label": 1
                },
                {
                    "sent": "We combined the Gaussian process model I'm talking about with dynamical requirements, so it's got dynamics in the latent space and applied the model to their data.",
                    "label": 1
                },
                {
                    "sent": "Now there's two missing data problems.",
                    "label": 1
                },
                {
                    "sent": "I'll show you the reconstruction errors from one.",
                    "label": 1
                },
                {
                    "sent": "The right leg was removed from the test sequence.",
                    "label": 0
                },
                {
                    "sent": "And then the other the upper body was removed from the test sequence.",
                    "label": 0
                },
                {
                    "sent": "So then all I'm going to show you I'm not going to show you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice videos, I'm just going to show you the angle errors for doing that when you try and reconstruct using their GP LVM.",
                    "label": 0
                },
                {
                    "sent": "Now the point about this data is there's 2613 frames, so that's kind of large for this type of model, so we use this fizzie approximation with 100 so-called inducing points.",
                    "label": 1
                },
                {
                    "sent": "That's the K and the models.",
                    "label": 0
                },
                {
                    "sent": "We also use back constraints to do this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but the end messages that yes, we can do it and we beat quite good results from nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So this is a nearest neighbor reconstruction by looking in the training data, finding the nearest neighbor and this is by scaling that all the training data variance one before you look for the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "And this is by not scaling the training data so the scaled data works better for the leg but worse for the better for the body but worse for the leg and the unscaled works better for the leg and worse for the body.",
                    "label": 0
                },
                {
                    "sent": "The GP LVM.",
                    "label": 0
                },
                {
                    "sent": "We use three different dimensional latent Space, 3 dimensions, 4 dimensions of five dimensions.",
                    "label": 0
                },
                {
                    "sent": "The five dimension was worse on both examples, but the four dimensional was better at reconstructing the leg and the three dimensional was completely reconstructing the body.",
                    "label": 0
                },
                {
                    "sent": "These are average root mean squared angle errors, so it's 2 degrees of angle error, which is reasonably small.",
                    "label": 1
                },
                {
                    "sent": "Bit away.",
                    "label": 0
                },
                {
                    "sent": "I mean you might think oh body, that's a lot more angles, but they are.",
                    "label": 0
                },
                {
                    "sent": "But there are a lot smaller, so you tend to make more error on the leg.",
                    "label": 0
                },
                {
                    "sent": "'cause even though there's fewer angles, they're doing much more so you expect to make more error on the leg as indeed you do when you're reconstructing the body.",
                    "label": 0
                },
                {
                    "sent": "A lot of what you're reconstructing is the spine and the angle movements in the spine.",
                    "label": 0
                },
                {
                    "sent": "When you're walking a pretty small.",
                    "label": 0
                },
                {
                    "sent": "So for those they don't have a significant contribution to the errors, more the arms.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the final sort of extensions.",
                    "label": 0
                },
                {
                    "sent": "Shared Gaussian process latent variable models.",
                    "label": 0
                },
                {
                    "sent": "So what we were really interested in doing one of the targets of this project was to do joint models of human motion and images.",
                    "label": 0
                },
                {
                    "sent": "So you've got some image features and you've got human motion angles that are captured jointly.",
                    "label": 0
                },
                {
                    "sent": "In this case, I can't remember which way around they are, but let's say why is the why is, I think the images and zed is the angles from the human motion.",
                    "label": 0
                },
                {
                    "sent": "What you do is you say these guys live in a shared latent space and from this latent space you go, you take a position in this latent space and you can map out to either the images or the image features or the human motion angles, and the idea is that given the image, I can then reverse this model and move back up.",
                    "label": 0
                },
                {
                    "sent": "Here from the image to X and then predict what the motion the angle motion should be.",
                    "label": 0
                },
                {
                    "sent": "This structure of model were sort of 1st proposed by Aaron Sean at Nips again 2005 NIPS and you basically do an objective way.",
                    "label": 0
                },
                {
                    "sent": "You have two Gaussian process latent variable models with a shared latent space and you optimize jointly over these X is together.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it's a bit of a funny model there.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have talked about it as being a CCA like model.",
                    "label": 0
                },
                {
                    "sent": "It isn't really because if you think of the linear case, it just collapses back to PCA again.",
                    "label": 0
                },
                {
                    "sent": "It's like PCA just over 2 subsets of the data.",
                    "label": 0
                },
                {
                    "sent": "We also added to that dynamical priors on X to improve the quality of the results and these back constraints.",
                    "label": 0
                },
                {
                    "sent": "Now I haven't talked much about these back constraints, but what they do is they force a bijective mapping between X&Z so they force bijective mapping between the latent space and the pose.",
                    "label": 1
                },
                {
                    "sent": "What's the reason for that?",
                    "label": 0
                },
                {
                    "sent": "Well, the theory is.",
                    "label": 0
                },
                {
                    "sent": "But if I've got pose, I know everything I need to generate the image.",
                    "label": 0
                },
                {
                    "sent": "If I have the image, I don't have all the information I need to generate the pose.",
                    "label": 0
                },
                {
                    "sent": "So if you believe the poses everything you need to generate the image, you should be able to construct a mapping from pose to image to feature, but you should never be able to propose to latent space to image, but you should never be able to generate a mapping from image to latent space to pose, because there's ambiguities in the image about what's going on.",
                    "label": 0
                },
                {
                    "sent": "This you could have legs obscuring each other, and in fact in our early results we use a data set from alcohol and tricks which is based on silhouettes.",
                    "label": 0
                },
                {
                    "sent": "There's lots of ambiguities in silhouette.",
                    "label": 0
                },
                {
                    "sent": "The dynamics are important for resolving these ambiguities.",
                    "label": 0
                },
                {
                    "sent": "When you see that these ambiguities are present, you can resolve them by working out.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you watch the silhouette move, you can actually know what it's doing.",
                    "label": 0
                },
                {
                    "sent": "You may not know if it's one frame, but if you watch it move, you can typically resolve the ambiguities, and that's being done through your dynamical models, and that's why we have to incorporate dynamics as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a slight issue with this which is in this case.",
                    "label": 0
                },
                {
                    "sent": "This is an example with two D latent space just to show you so I can visualize the latency.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a pose.",
                    "label": 0
                },
                {
                    "sent": "And then here's probability contours in the latent space.",
                    "label": 1
                },
                {
                    "sent": "Now we would expect this post to be multimodal, yes, but I mean each of these white areas.",
                    "label": 0
                },
                {
                    "sent": "There's a mode in it somewhere, or perhaps even multiple modes.",
                    "label": 0
                },
                {
                    "sent": "You're getting lots and lots of modes from one silhouette because there's lots of local minima when you learn these sort of mappings.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's a highly multimodal latent space.",
                    "label": 1
                },
                {
                    "sent": "Given the silhouette, the idea is you you present the silhouette, you find a mode and then you project out to the pose.",
                    "label": 0
                },
                {
                    "sent": "Now a lot of these modes are reasonable, but it's really nasty optimization space.",
                    "label": 0
                },
                {
                    "sent": "However, Despite that, with the dynamics, we were able to.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get results like this.",
                    "label": 0
                },
                {
                    "sent": "OK here is.",
                    "label": 0
                },
                {
                    "sent": "This is an example from the test data.",
                    "label": 0
                },
                {
                    "sent": "The training data is the silhouette is artificially generated using poser.",
                    "label": 0
                },
                {
                    "sent": "It's data from Agilent rigs.",
                    "label": 0
                },
                {
                    "sent": "What you see going on here is the actual silhouette, where putting his input to the model.",
                    "label": 0
                },
                {
                    "sent": "What you see here is the known ground truth from that silhouette.",
                    "label": 0
                },
                {
                    "sent": "What you see here is the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Using this model we just talked about.",
                    "label": 0
                },
                {
                    "sent": "Now you'll notice it's well, OK. First of all, there's an optical illusion which makes it look like he's turning the wrong way at times.",
                    "label": 0
                },
                {
                    "sent": "I'll show you different video that shows a different angle to resolve that in a second, but you'll notice is that he does fairly well, although at some points he'll stutter on the turn.",
                    "label": 0
                },
                {
                    "sent": "The reason is because he doesn't have data in the training data about that turn, So what tends to happen is he sort of pauses like there.",
                    "label": 0
                },
                {
                    "sent": "And he waits for the test data to get to a point where he's got data and then carries on.",
                    "label": 0
                },
                {
                    "sent": "But other than that, it's fairly natural motion.",
                    "label": 0
                },
                {
                    "sent": "And that's just an issue of getting more training data in the other results we're showing here are doing regression from Silhouette features to angle features, which is 1 sort of proposed technique.",
                    "label": 0
                },
                {
                    "sent": "So it's a 3D position features.",
                    "label": 0
                },
                {
                    "sent": "These aren't angle, so in this case these aren't angle features we're using.",
                    "label": 0
                },
                {
                    "sent": "These are 3D point clouds were using, so it's not using any constraints on limb lengths.",
                    "label": 0
                },
                {
                    "sent": "So This is why you see these people shrinking, because these are just 3D point clouds.",
                    "label": 0
                },
                {
                    "sent": "It's predicting if you do regression in that, that sort of affect you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so to try and resolve that I don't know how many people.",
                    "label": 0
                },
                {
                    "sent": "How many people notice the optical illusion of him turning in another direction?",
                    "label": 0
                },
                {
                    "sent": "No one there, how good noticed it?",
                    "label": 0
                },
                {
                    "sent": "Maybe you with a review of it.",
                    "label": 0
                },
                {
                    "sent": "OK so here is with different angles.",
                    "label": 0
                },
                {
                    "sent": "You can see that he's actually rotating around.",
                    "label": 0
                },
                {
                    "sent": "You can also see the starter step there.",
                    "label": 0
                },
                {
                    "sent": "You can also.",
                    "label": 0
                },
                {
                    "sent": "The optical illusion is also present in the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So this is an above angle to show that he's always turning in the same direction, which is what the silhouettes actually doing.",
                    "label": 0
                },
                {
                    "sent": "OK, but we did see that the walls.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A sort of problem with that this multimodal nasty evil latent space, which is real pig to optimize over.",
                    "label": 1
                },
                {
                    "sent": "We tend to use things like we discretize the latent space and we use retreat is hidden Markov model, which you can do to get a first pass global discrete Optima and then move out from there to get such good results.",
                    "label": 0
                },
                {
                    "sent": "But this is a better thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one reason we think that that's going on is we're not informing the model about the structure of the problem, so this is a development.",
                    "label": 0
                },
                {
                    "sent": "The modified model does this.",
                    "label": 1
                },
                {
                    "sent": "It has opposed specific latent space, A shared information, latent space, and an image specific latent space.",
                    "label": 0
                },
                {
                    "sent": "So what you're basically doing, don't worry bout the ARP arrows.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to really talk about them.",
                    "label": 0
                },
                {
                    "sent": "Worry about the split of the latent space.",
                    "label": 0
                },
                {
                    "sent": "This latent space here can have influence on both zed an Y.",
                    "label": 0
                },
                {
                    "sent": "This latent space only has influence on zed.",
                    "label": 0
                },
                {
                    "sent": "And this latent space only has influence on why.",
                    "label": 0
                },
                {
                    "sent": "So what's going on?",
                    "label": 0
                },
                {
                    "sent": "There were trying to force this latent space to contain all the ambiguities in the problem.",
                    "label": 0
                },
                {
                    "sent": "So if there are issues of silhouettes facing in the wrong way or leg position ambiguities by making sure that this space controls both the information that shared and this space controlled separate information, we can do that.",
                    "label": 0
                },
                {
                    "sent": "But when we did this?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We know with the GP LVM that initialization is crucial.",
                    "label": 0
                },
                {
                    "sent": "Getting the right initialization is crucial, so we spend a bit of time thinking about how to initialize this model and the way we did it was to initialize this shared latent space by CCA.",
                    "label": 1
                },
                {
                    "sent": "And then look for the non shared latent spaces for orthogonal directions to CCA to inhabit the.",
                    "label": 1
                },
                {
                    "sent": "So we look for large variance directions which were orthogonal to the CCA subspace.",
                    "label": 0
                },
                {
                    "sent": "But we didn't just you see.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, we used kernel CCA and we tried various kernels and we scored the quality of those kernels through GP LVM likelihoods.",
                    "label": 0
                },
                {
                    "sent": "This is suggestion by Stefan Harmeling while he was up at Edinburgh to basically do lots of different kernels.",
                    "label": 0
                },
                {
                    "sent": "Look at your embeddings, do GP, LVM, likelihood and then score the quality of the embedding using that.",
                    "label": 0
                },
                {
                    "sent": "So we did exactly that.",
                    "label": 0
                },
                {
                    "sent": "Now what was interesting then is that's our initialization, but it.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look so nice that we never trained the model so the initialization is all done spectrally and in fact the model is all spectral in this case, so we're just using the GP LVM as a probabilistic framework.",
                    "label": 0
                },
                {
                    "sent": "On top of that.",
                    "label": 0
                },
                {
                    "sent": "So in this case here this is can't remember the name of the human either.",
                    "label": 0
                },
                {
                    "sent": "Data set from Michael Black.",
                    "label": 0
                },
                {
                    "sent": "I like the features aren't silhouettes.",
                    "label": 0
                },
                {
                    "sent": "I can't remember the name of them, but their sift like features emailed Carl but he didn't get back to me in time.",
                    "label": 0
                },
                {
                    "sent": "So that's it like features are different types of features.",
                    "label": 0
                },
                {
                    "sent": "Now the ambiguity is present in this.",
                    "label": 0
                },
                {
                    "sent": "If this was a silhouette, are kind of what's reflected here there's leg ambiguities as to how far forward the legs are, so these modes you see which are in the post specific latent space with visualizing that that's only two dimensional.",
                    "label": 0
                },
                {
                    "sent": "This is I'm facing towards the camera, and these are my position.",
                    "label": 0
                },
                {
                    "sent": "My legs along here, it's actually all the way up there because it thinks it's probably got the legs and this is I'm facing away from the camera and the leg ambiguity as you're facing away from the camera.",
                    "label": 0
                },
                {
                    "sent": "So instead of the sort of nasty multimodality we had before, you're getting this much sort of smoother.",
                    "label": 0
                },
                {
                    "sent": "In this case, the multimodality is much more distinct.",
                    "label": 0
                },
                {
                    "sent": "It's which leg is forward, so this is like left leg forward.",
                    "label": 0
                },
                {
                    "sent": "And this is like right leg forward.",
                    "label": 0
                },
                {
                    "sent": "It's getting additional cues 'cause it's not just silhouette based, so that's why the one of the modes instead of dominating and then I can sort of as a final example, show you.",
                    "label": 0
                },
                {
                    "sent": "My Bibtex compilation of the slides or.",
                    "label": 0
                },
                {
                    "sent": "Just a video of this is 4 frames per second, so it's slowed up because to compute each of these whole space the energy in the whole space takes a bit of time.",
                    "label": 0
                },
                {
                    "sent": "So basically you're seeing as the lady moves around, you're seeing the ambiguities and how they change so much simpler ambiguities are much easier to resolve, and there's actually no dynamical model in this work at the moment.",
                    "label": 0
                },
                {
                    "sent": "That's something we're going to add, but it really seems to have tidied up one of the problems in the earlier model, despite the fact that model.",
                    "label": 0
                },
                {
                    "sent": "Worked very well in practice, OK?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Illusions so the Gaussian process latent variable model is a probabilistic, non linearized generalization of PCA and in the pump priming project.",
                    "label": 1
                },
                {
                    "sent": "We've looked at three extensions which I've tried to briefly summarize, hopefully give you the idea.",
                    "label": 1
                },
                {
                    "sent": "First of all, hierarchical representations of the model.",
                    "label": 1
                },
                {
                    "sent": "Secondly, how to work with larger datasets, and then Thirdly these shared latent space models.",
                    "label": 0
                },
                {
                    "sent": "So follow up come out of this is Carl's visiting Trevor Darryl's Group in Berkeley, 'cause they're very interested in these sort of techniques later in the year.",
                    "label": 0
                },
                {
                    "sent": "Travis moving from my Martita Berkeley so he's waiting till they've moved.",
                    "label": 0
                },
                {
                    "sent": "There's an imminent EPS RC application that's building on this work, and we have other lots of other ongoing work on Gaussian processes, differential equations, and human motions.",
                    "label": 1
                },
                {
                    "sent": "So we've got lots of nice followups coming out of it, and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some references.",
                    "label": 0
                }
            ]
        }
    }
}