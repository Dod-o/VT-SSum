{
    "id": "y6nrgrhw4mt6bq73ifrj7cyokuzfmtv4",
    "title": "Memory, Reading, and Comprehension",
    "info": {
        "author": [
            "Phil Blunsom, Department of Computer Science, University of Oxford"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_blunsom_memory_reading/",
    "segmentation": [
        [
            "OK, so.",
            "Part 3 for NLP today.",
            "So this for this second lecture I'm going to talk more about research rather than sort of background material, so these are.",
            "Going to be sort of two things that we're doing at DeepMind with with NLP.",
            "So yes, I spend maybe 80% of my time at DeepMind and 20% at University of Oxford, and these people here are sort of a subset of the group at at deep mind that I work with.",
            "Thomas is up the back there somewhere who did most of the implementation for the comprehension part of this this talk so you can ask him about that.",
            "OK, so lots of people have been asking me about deep mind.",
            "So just for the fun of it, nothing to do with what I'm talking about.",
            "I throw in a couple of deep mind videos for context so those of you that know about deep mind probably seen all the Atari stuff before, but it's still fun for those who haven't seen it, so I'll play this so deep mind is well, startup was acquired by Google.",
            "It's basically a Research Center interested in general problems in AI were quite big now.",
            "Lots of good machine learning researchers there doing all sorts of interesting things, but one of the sort of most well known public things was the Atari reinforcement learning and the key thing about this was that it was this model with a big covenant with a reinforcement objective, and it was learning to play these games just from the pixels.",
            "The pixels, not from the actual memory state of the emulator or anything like that.",
            "So this is it playing breakout.",
            "You can see it gets pretty good, never really misses.",
            "And.",
            "Eventually he learns to go over the top at some point.",
            "Yeah, about there.",
            "So it's cool.",
            "It's only say this is Roadrunner and you can see that's Roadrunner.",
            "It's it.",
            "It gets pretty much perfect killing the coyote every time.",
            "And then left learns various tricks.",
            "Where is it?",
            "Yeah, like that.",
            "No.",
            "And this one so.",
            "But the white is the the.",
            "The neural network.",
            "You get away in a moment, I think.",
            "Yeah, then back to that.",
            "Yes.",
            "Yeah, the tennis is sort of boring.",
            "It just really learn to Clock that one every time.",
            "I also had I don't know, has anyone talked about draw yet.",
            "So deep generative models?",
            "OK.",
            "So you're probably more about deep generative models.",
            "I think I saw someone talk about this.",
            "We have a recurrent network with an attention variable.",
            "The interesting thing about this model called Draw was it didn't just do attention and classification.",
            "It also does generation.",
            "So it's a recurrent generation model as well with attention so.",
            "So that's the model reading digits so that green boxes the attention this is actually generating digits.",
            "You see the attention as it generates.",
            "It looks quite cool, but like someone writing and we can, we can generate multiple digits.",
            "And this is Street View numbers from Google, so this is purely generative, and they're pretty damn good.",
            "Street View number renderings.",
            "I'll see if I can.",
            "Who?",
            "Now can I?",
            "Now, well?",
            "So some of the cool things.",
            "Going out don't mind what what I do.",
            "There is language so we have a language group, NLP, deep learning, the Inter interested in all sorts of problems, particularly these sorts of things that I'll talk about."
        ],
        [
            "Now.",
            "So.",
            "And this is sort of recapping on where we've been this morning.",
            "So in NLP at least, there's lots of work now in deep learning.",
            "Initially, a lot of it focused on classification problems.",
            "Redoing a lot of the stuff we've done previously with other machine learning models, kernel machines, and such document classification tagging.",
            "And there we've seen lots of good results and mostly incremental over previous results.",
            "And what is really the sort of interesting thing now for the future is doing the harder tasks that we've not been able to do so much or so well in the past.",
            "So those sorts of things we're interested in going towards natural language, understanding tasks.",
            "So a classic example of this is question answering.",
            "This is just an example of a very simple question answering paradigm where you have a question.",
            "And you have some candidate utterances and you're trying to workout whether that the sentence answers the question or not.",
            "So this is a sort of thing that's used in some sort of information retrieval style question answering where like on Google when you type in a question and it looks at the results and decides that the IT can find the answer to your question from one of the documents in the results and it will highlight at the top.",
            "So that's the question answer selection problem anyway, so we can do that very well using these embedding models that we've been talking about.",
            "We can just embed the question, we can embed the answers and we can learn a function of the embeddings.",
            "That predicts whether this is just binary, it's whether it's answers a question or not.",
            "It's not a huge amount, it's 10 or 20,000.",
            "It's a TREC data set.",
            "Yeah."
        ],
        [
            "There we have a lot more so, and you can also train on the existing.",
            "System if you want so that's called web answers that does it when you type questions into Google, there's lots of different systems to get fine.",
            "That's just one of them that that might try and answer a question."
        ],
        [
            "Yeah, so I mean, we've been hearing about Ellis, James and Recurrent models and so they they prove quite good ways to do this.",
            "We can just have a answer embedding recurrent network and a question embedding recurrent network and then some function G at the end that computes that."
        ],
        [
            "We've seen earlier that you can do translation this way.",
            "There was that slide from the encoding.",
            "Decoding ways of doing translation.",
            "So for me translation is interesting from a higher level because it's basically the most.",
            "Difficult or the closest to a natural language understanding task in NLP, for which we have lots of data.",
            "So most of the natural language understanding tasks you would like to work on, like question, answering, dialogue, processing all these sorts of things.",
            "We just don't have any data to train models on, so that's largely part of the problem.",
            "Trying to find data translation, we have loads of data that's quite a lot of semantics going on.",
            "So really in some sense it's interesting because it's sort of the most difficult task we can we can tackle.",
            "It's not just classification, it's more interesting.",
            "But we have lots of data, so it's currently a cool point in the space of tasks.",
            "Also the."
        ],
        [
            "Do we have for translations actually useful for lots of other things, so parallel data isn't just good for learning to translate, it's good for learning semantics of language as well, because if you think about a parallel Chinese English corpus, Chinese and English look completely different.",
            "They expressed meaning in a completely different way, but if you have a Chinese sentence in its translation, that main pretty much the only thing they share in common is their meaning, so that's a classic sort of latent variable problem that we have this data where we have these.",
            "Different ways of expressing the same meaning, so we can exploit that to learn some sense of language understanding.",
            "OK, so at the moment it deep mine, what we're mostly interested in is this.",
            "Going past these tasks too.",
            "More.",
            "Sort of deeper, natural language.",
            "Understanding tasks that we hope to be able to get data for, so one that I'll talk about in the second half of this lecture is reading comprehension, so that's a question answering task, but we're rather than trying to answer questions from a database or something like that.",
            "We have a document.",
            "In this case, will be news stories and we have questions about those news stories.",
            "It's an interesting problem because you need to learn something about the structure of language.",
            "You need to be quite high precision because you only get the one document question answering on the web.",
            "You can get away with being low precision because you can rely on the same thing being stated many times on the web, so you can have sort of low precision sort of relation extraction algorithms when you've just got one document, and you're being asked a question, then you need to be able to find the one sort of statement or set of statements that support it.",
            "The other thing we are interested in is.",
            "Connecting language and action in real environment.",
            "So deep mines very interested in acting environments.",
            "That's where all the Atari things came from and lots of other interesting things.",
            "And so we work on how do we map from language to actions and environment, not just one action like question answering where you might your action is to query a database or something like that.",
            "But can we do things like planning?",
            "Here's an instruction.",
            "Have an agent go off and do some sort of planning following that instruction and we.",
            "We can do some interesting things again with recurrent networks."
        ],
        [
            "Such OK, so specifically this lecture today.",
            "I'm going to talk about first some some sort of algorithm quick we're doing on.",
            "Augmenting neural networks with memories.",
            "And in the second part I'm going to talk about this idea of reading comprehension machine rating."
        ],
        [
            "So recently there's been lots of interest in this idea of taking your recurrent network, something like that and augmenting it with a memory, some sort of persistent data structure with the network and store information, and read from it.",
            "And there's been sort of increasing number of papers on on different architectures for this, so there's the sort of thing that Facebook had been doing.",
            "Jason.",
            "With memory networks, it turns out the memory networks and attention models are pretty much two sides of the same coin in the sense that the memory network sort of you write in all the information you want, and then you compute softmax is over, which bits of it you should read.",
            "As you're out putting.",
            "And that at DeepMind we have the neural Turing machine that Alex and Greg did.",
            "It sort of adds an extra extra writing ability so it can both read and write from the memory and generalizes those option operations so they can be both.",
            "Content based an.",
            "Address based in at least lots of things.",
            "So these are really cool and there's lots is increasing number of sort of demos, mostly all synthetic.",
            "That shows that you can learn some really interesting or complex functions like Alex sorting numbers with the neural Turing machine.",
            "But from a NLP POV, it's we looked at this and thought.",
            "Well, maybe these are cool, but maybe they're overkill for the task.",
            "Do we really need random access memory to solve lots of the problems we're interested in?",
            "That often sort of transduction like problems like machine translation.",
            "And the problem with random access Memories is the scaling is not great, so you tend to get some sort of linear in the size of your memory factor for each prediction, because you normally have to do a big softmax or something over your over your memory and you'd like your memory to scale quite big, and so that's that's rather limiting.",
            "So in this talk I'll talk about our sort of initial exploration in different memory architectures that might be more efficient and more suited to NLP tasks.",
            "Basically, by restricting the operations to a set that we think will."
        ],
        [
            "Adequate and will give us the performance that we.",
            "We want.",
            "So this this part is is all going to be about synthetic transduction problems, as we've seen in the earlier lectures.",
            "Lots of NLP problems can be thought of as transduction.",
            "Transduction is just transforming one sequence into another.",
            "Obviously, machine translation is like this.",
            "People have been trying to do syntactic parsing like this.",
            "Take a sentence and transform it into a string that includes its past tree.",
            "You can also think of computation like this, so there's work from the brain group at Google trying to take Python source code and transduced that into the result that that source code is computing.",
            "But"
        ],
        [
            "Through this sort of lens of transduction you can.",
            "You can view lots of problems.",
            "So just sort of reviewing what we saw earlier, what a lot of people are doing so that we have our input S our target T. If we have an RNN, we can model these conditional probabilities.",
            "And we want to read in the source and generate the target and that can either be just purely greedy or some sort of beam search where we keep around."
        ],
        [
            "And invest.",
            "And as we saw earlier, you can do this just with recurrent network just by concatenating your strings and generating and this sort of forms out.",
            "Baseline in effect, but as I said earlier, there's there's this bottleneck.",
            "If we do this just with a raw RNN."
        ],
        [
            "OK, so this is an example of the Python, so this one example.",
            "This is the Python thing that Wojtek Anelia were doing, so reading in Python scripts like that and outputing as a string.",
            "The the value that is calculated so you can see that we can treat this like a transduction problem.",
            "It's a lot more complex in some way than translation.",
            "This is not a one to one mapping.",
            "There's inference that has to go on here and it sort of worked for some simple examples."
        ],
        [
            "And they were just using a deep deep LSM.",
            "So what I'm going to talk about here is sort of as I said, other memory architectures besides random access.",
            "So if you pick up your sort of CS101 textbook, we see stacks and queues and deques.",
            "Ann, we thought?",
            "Let's try those.",
            "So we're going to implement neural stacks and New York use in your decks, just like the.",
            "NTM the Rotary machine we're going to do this.",
            "With soft variables, we're going to do this where everything is differentiable.",
            "Of course, you could do this by just modeling a stackers with stochastic latent variables.",
            "We tried the the continuous version, so like everything this has been done in the 90s so soon had a series of papers from the early 90s to later on with pushdown automata and stacks.",
            "Some interesting papers, guys at Facebook are doing the same thing with similar.",
            "With a similar to slightly different stack architecture and at the bottom is it Ed's work that is what I'm actually talking about here.",
            "So the advantages here is that we want, unlike the MTM, we want the memory size to grow dynamically.",
            "That's a great thing about a sticker or Q.",
            "We don't have to pre define the amount of memory that we're going to store, we can just keep pushing things onto a stack and popping them off as we need.",
            "So that's why we call it unbounded memory.",
            "So everything is going to be solved by continuous pushing pump.",
            "The sort of we can get unbounded long range dependencies with sort of a qualifier there, and flawless propagation of gradient again with a bit of a qualifier.",
            "So this is kind of allow.",
            "Although we've claimed that the LS DMS are sort of giving you these long range dependencies, this is going to allow the model to more explicitly."
        ],
        [
            "T. Record long range dependencies, so this is what the stack looks like.",
            "I'll just show the stack, the Q and the Decker just.",
            "Obvious extensions of this, so we're going to have this continuous stack, so V there is a vector.",
            "That's the thing we're pushing on.",
            "So, so at each time step from our current network, we compute through just an MLP vector.",
            "That's going to be pushed onto the stack.",
            "We have another network that computes what its weight should be, so that's this thing here.",
            "So T is the time step up here.",
            "Forget about you for the moment and D is the weight of the thing that we're pushing on.",
            "So we have this sense of a soft push.",
            "We don't push on everything with the unit weight.",
            "This V is being pushed on with .8 weight.",
            "And we also have a read operation here that just waits each vector by its weight and some some together, and that's a read.",
            "And the thing about the readers.",
            "It takes the first unit size, amount of weight.",
            "Here the vectors less than one, so it takes the whole vector."
        ],
        [
            "Later on it will make more sense.",
            "So the second time step, so this you, this is actually the pop variable, so it just says so.",
            "Again we have a network that's actually predicting how much we should pop, so that's given the current hidden state.",
            "Output pop variable and that says we pop off .1.",
            "So now the ones weights gone down to .7.",
            "So we popped off .1 V one.",
            "We then push on .5 of the next vector.",
            "The vector at time Step 2 so now we have two vectors.",
            "First the top has .5 weight, the bottom .7 and as I said, we read off a unit amount of weight.",
            "So we read off .5 * V Two and .5 * V One and anything below that unit we throw away."
        ],
        [
            "And so on for the next time step.",
            "So at the next time step we.",
            "We pop off .9 so now V2 was only .5, so its weight actually goes down to zero, so it's essentially been completely popped off the stack and V1 is reduced accordingly.",
            "With the leftover wait.",
            "And then we're also pushing on the next time step with .9.",
            "And again, we get this.",
            "That's how we read off.",
            "As a weighted sum.",
            "So this is our.",
            "Sort of continuous neural stack.",
            "So the thing here is that because we're doing this soft pushing and popping with these weights, everything is differentiable, so we can differentiate this end to end.",
            "These are not stochastic latent variables as such.",
            "It's that the forward pass is still completely deterministic, so that gives us that makes inference easier.",
            "We don't have to do any sort of dynamic programming or sampling over over stochastic variables, we can just run this and back propagate.",
            "And learn these operations and what we'd like to see is Ken.",
            "Even though we've defined this in a sort of continuous software, cannot learn to behave like a.",
            "A traditional symbolic stack."
        ],
        [
            "So this is a schematic diagram of what our stack looks like, and basically this is the way you implemented.",
            "We just implemented like a torch module and neural stack.",
            "It has these inputs.",
            "The previous state in the current input.",
            "The previous state is is.",
            "The values are the vectors on the stack and their strengths, so that's our previous stack and our input is how much we're pushing.",
            "How much were popping and what the next value is to go onto the stack and output, of course is the next state, and the output redvector that gets passed on to whatever the next bit of the network is to predict.",
            "So this is just becomes a nice modular unit that we can treat like any MLP or an LST, MSIL or anything like that.",
            "We can implement the torch module and connect it to."
        ],
        [
            "Two recurrent networks.",
            "So that's what it looks like if we connected up.",
            "So we have an R and controller, so that's the controller that's taking in.",
            "If this was language modeling, it would be taking in the words.",
            "It would be, it would be computing the different push pop and value operations to feed into the stack.",
            "It's just taking again the previous date in the input output.",
            "In the next state in the output where next state is composed of our in state neural stack state and all these things.",
            "So just like Richard was saying these things, you can just sort of put together like Lego blocks.",
            "You can also put any other controller in here.",
            "You could put just an order, new MLP or something like that.",
            "Not much reason why you would do that.",
            "Nice LSD M there.",
            "Is going to do the job well."
        ],
        [
            "OK, so.",
            "At this point we're just we're just applying these models to synthetic transaction problems where we basically designing problems that we think have some relationship to the sorts of tasks we're interested in, and then we try investigate the properties to see whether the models can learn what we want them to learn.",
            "Simplest things that are sort of very common to do copying.",
            "So can we take a sequence and just output the same sequence again?",
            "Now something like the Deep L STM can't do this beyond a certain length.",
            "At some point the hidden layer saturates and you can't generate very long sequences.",
            "Reversal is similar but require some sort of generalization.",
            "Now, if you think about the data structures I talked about copying.",
            "She is going to suit Q quite well and reversal is going to suit stack quite well 'cause you can just push things onto a stack.",
            "We can think about more interesting problems like what if we try and flip every neighboring input, so that's what we call bigram flipping.",
            "Now that sort of thing is obviously what you see locally in some translation problems like French and English.",
            "You need to be able to do local ordering reversing, so this is kind of model locally reorder things.",
            "These seem quite simple, but many of the models that people are using for transduction and translation and such things can't do these synthetic problems."
        ],
        [
            "OK, if we get a bit more fancy, this a bit cryptic, but.",
            "In translation we see higher level reordering things, so a classic one is subject verb, object, subject, object, verb type reordering.",
            "Where you get a systematic reordering of constituents in sentences between different languages.",
            "So we can simulate that again by having we have a simple so these are all done from we generate from an ITG an inversion transduction grammar that we hand write and we generate examples like this and the various symbols like S here means subject.",
            "I is the input and one is just the index of the like word index.",
            "So we have lots of different words.",
            "V is a verb, always an object.",
            "At some point we throw in some relative pronouns and some clauses just to make things interesting.",
            "And so we could sort of infinitely embed these.",
            "So there's quite a lot of hierarchical structure in some of these strings, and they get quite long.",
            "These are very short ones, but we can generate.",
            "50 or 100 length strings.",
            "And yeah, the output looks the same except always for output there.",
            "So there's lots of local reorderings here, so we have subject, verb, object and subject object.",
            "With the verb gone, never goes right to the end.",
            "We also have a sort of.",
            "This is supposed to be a German gender like example where we have a language which doesn't have any gender, so that sort of being our English and we have particular hidden rules about which of these symbols have which gender, and the output has to use the right.",
            "Term is in various things to make the gender agree.",
            "So there's a little simulations of the sorts of things you see in real translation problems just trying to tease apart when we isolate things this much.",
            "Can these models learn this sort of thing?",
            "I."
        ],
        [
            "A lot more going on in real translation, so you think if they can't learn this then it's going to be difficult.",
            "OK, so we just have.",
            "We have two different ways of it, so we're going to generate lots of these examples and feed them into the network and then look at the accuracy.",
            "That they produce, and we can either look at whether it gets the whole string right, or we also have a fine grained accuracy, which is sort of how much of the prefix could get right."
        ],
        [
            "So these are the.",
            "Various models that I've been talking about, so we gotta stack your deck and this is your good old fashioned DLS TM.",
            "All of these we tuned quite extensively on thousands of machines, so the DL STM we explored sort of all of the depths and hyperparameters and everything else and optimize those.",
            "So here we have a different task, copy reversal bigram.",
            "Flip so solved here.",
            "The green basically means that the model algorithmically learns the problem, and by that I mean that we train on up to a certain length of string, and then if we give it strings that are longer up to an arbitrary length, it can still reproduce.",
            "So copying we might train on less than 20 length strings, but will it still reproduce the same algorithm if we give it 100 link string?",
            "What you find with the DLS team so they can learn the training distribution quite well.",
            "But they don't generalize once, once you increase the length, they haven't actually learned the algorithm, they've just learned to operate within a particular range.",
            "The nice thing about the more structured models that they do actually learn the algorithms so they learn what it means to copy, and they can just produce that on an infinite length string.",
            "Wasn't there?",
            "Um?",
            "What does converted mean?",
            "Actually, that's a good question.",
            "I think.",
            "These were added definitions.",
            "What do you mean by converge so?",
            "It.",
            "I think it basically just means that it tops out of a certain value that.",
            "So, so we ran lots of these, right?",
            "They're quite sensitive to initialization and lots of them just never get beyond random, so convergence mean it converges to something that's not random, so it's discovered some structure in the problem, and it's able to model it, but it's not close to optimal in anyway.",
            "So I think it's just anything that I don't know what exactly what its criteria for not random wars, but.",
            "Yeah, most of those things take.",
            "Often quite awhile to get out of the random regime as well.",
            "So, and the patterns are sort of predictable, like stacks about it copying, but they're good.",
            "At reversal queues are good at copying and better traversal that makes sense.",
            "Having a deck you can do both, so that's nice.",
            "So.",
            "And the the motivation here again to go back to the start was that with all of these, are the stack.",
            "Back in Q we can get essentially constant time updates.",
            "We don't have to do a big softmax over the whole of the memory 'cause we're just operating on the top or in terms of the reading the top unit."
        ],
        [
            "I'm not sure.",
            "So it's differentiable.",
            "It's not terribly.",
            "So it's locally differentials, like a value or something like that.",
            "It has a, uh.",
            "Phase change at the boundary.",
            "But yeah, it's still.",
            "It's still differentiable in that sense.",
            "OK, so these are the convergent graphs and these so this is.",
            "Maybe gives a better indication of what it means to converge.",
            "These are basically the models are the best models that did actually do something other than random.",
            "The colors are all probably harder to see.",
            "Basically, most of these are your various LS teams of different depths.",
            "The things that go up like this.",
            "So this was bigram flipping, and this is the Qi think.",
            "Yeah, that's the Q, and that's the deck, and you can see this sort of algorithmic behavior.",
            "They once they cotton on to exactly what the right strategies for pushing and popping, they suddenly just learn the task.",
            "Which is what we want, and then you can just use them again on arbitrary link sequences, so they've actually learned what the algorithm is rather than representing it in a in a soft sort of RNN way.",
            "And this was a gender conjugation one as well.",
            "OK."
        ],
        [
            "How am I going?",
            "So that was.",
            "That was a sort of brief detour into maybe where we might be going with these transduction problems and machine translation and such things.",
            "There's lots of relationships between this sort of stuff and attention models and other such models.",
            "The advantage of something like the stacks and queues is that you're basically doing this linear pass.",
            "Over the input and then you can start producing output.",
            "You don't have to go back and do linear passes through the input again.",
            "OK.",
            "So now for Part 2."
        ],
        [
            "So.",
            "As I said, this part is going to be about reading comprehension, so the idea is like our Android there that the Guardian newspaper kindly drew for us.",
            "We want to be able to read an article, news article or Wikipedia article or something like that and answer questions about it.",
            "True.",
            "It's not crucial.",
            "Depends what makes.",
            "Both are going to work just like with the other attention models.",
            "This idea of hard and soft attention or stochastic variables or continuous ones.",
            "They both work.",
            "My philosophy is that if you care about the variables and make them stochastic.",
            "So if you actually want to read off the attention.",
            "Then you should have stochastic variables.",
            "If you don't care if it's just some bit of the process to get you the end result, then I think it's better to make them soft.",
            "It just makes everything easier because.",
            "I mean we spent or.",
            "I've spent years trying to do inference in graphical models and it's hard when there when there's no sort of polynomial algorithm and sampling and all these sorts of things are hard to get to work, especially in these sorts of networks.",
            "So just by passing that if you don't actually care what the the latent variable is, if it's not part of your output.",
            "So that's my general perception.",
            "I think there's some the flip side of that is there's some optimization value in being hard, like when you make hard decisions, things tend to optimize quicker, and these so the various models.",
            "Definitely as I said, some of them never converge, they just.",
            "Have random behavior.",
            "Some of the stacks and queues just behave like iron inside.",
            "Never actually used the stack or queue, and so there's this sort of initialization problem that so if you think about the way for the stack, if you think about the way pushing was defined, if the initialization is such that the model doesn't push to start with, it can never learn to push.",
            "So if you just get an initialization, which is if you randomly initialize and there's no wait on pushing, then the model will never use the memory in, it will just behave like an RNN.",
            "Of course, in when we we know that we can counter it just with initialization.",
            "If you have stochastic variables, you get a bit more randomness in that exploration that you don't get in the terministic models.",
            "OK, so reading comprehension, so we want to read articles.",
            "We're going to be able to answer questions and the key thing is that we need to be high precision because we can't rely on seeing the facts were interested in stated many times.",
            "So as I said earlier, for lots of these more interesting problems, the biggest barrier to working in the area is the lack of data.",
            "So if we want to work on something like this, the first thing we have to do is think about where the data going to come from.",
            "So we're going to have to be creative about how we managed to go about doing that.",
            "And."
        ],
        [
            "And, at least in my mind, I think that a lot of where the future progress is going to come from, not necessarily.",
            "Newer and fancier models, but people being more creative and finding better sources of data for these problems.",
            "So in reading comprehension is a few existing data sources, so a classic one is from Microsoft that's called MC test and these are little stories that are most to be at a sort of primary school level children's primary school level, and the idea is that you have a little discourse like this little story.",
            "Then you have a Multichoice question and you want to build a model to answer these stories, and so they've intentionally made the language simple to make it easier.",
            "And divorced from the real world.",
            "This is not connected to facts like sort of who is the President of United States or any of these world facts.",
            "You don't need that sort of world knowledge, you just need common sense knowledge.",
            "So this is a nice test set.",
            "The problem is there's no real training data.",
            "There's a few hundred of these documents, but there's no way you can train a model to do this from such a small amount of data.",
            "So this means that if you're going to use these sorts of things, you're going to be in some sort of unsupervised or rule based regime.",
            "We want to do something with sort of supervised machine learning, so we want something where we can get a significant amount of data."
        ],
        [
            "So another solution is to simulate data to generate it synthetically.",
            "So Jason Weston at Facebook has been doing this recently, so it's a different sort of angle.",
            "Hit the idea is that you write down a grammar that generates sentences and you can generate simple simple discourse like this and you can answer a question.",
            "And so in theory, if you've got the generating grammar, you can generate an infinite number of these things, train on them.",
            "And test your model and this is nice because you can control the phenomena that you're modeling.",
            "You can you can target specific sorts of things that you're trying to to capture.",
            "The downside is that we're just no good at generating interesting natural language.",
            "If we were, we have.",
            "We have made a lot more progress on natural language processing, so these things tend to be very simple.",
            "The grammars are very simple in a lot of these cases are actually finite, so they can't generate infinite amounts of data.",
            "And it's just a very.",
            "Biased slice of simple slice of language so.",
            "From a point of view of building reading comprehension models, you can use this sort of approach to test them, but it's not much good for training them because you don't want to train on this data because you're not really going to learn anything about natural language.",
            "You're just going to learn something about the generating grammar.",
            "So again, that doesn't really solve our problem.",
            "We're still mostly just looking at a testing regime."
        ],
        [
            "So when we thought about this problem, we came up with the idea or the.",
            "Or the thought that.",
            "What we need is paraphrase data, so if we have news stories.",
            "We want we want articles and we want questions.",
            "If we have an article from Wikipedia or somewhere like that, we can take a sentence and it's easy to turn that sentence into a question.",
            "So that's an easy way to get documents and questions.",
            "The problem is there's not much point in doing that because the questions will be very easy to answer because you'll just get very strong lexical overlap.",
            "You can just look up in the in the article.",
            "We use all those words and you'll find what the answer is and the questions won't require any sort of reasoning or inference across sentences or reference resolution or any of these things that are actually the hard parts of of doing reading comprehension.",
            "So that's too simple, but if we can get something similar then we might be in town.",
            "So what we need paraphrases of information from articles.",
            "And people have looked at this quite a bit in in other areas, in particularly summarization so we knew about this corpus that various people had used to build summarization models where from the CNN website.",
            "The nice thing about CNN is they have on their website these articles.",
            "This ones are nice article about Chuck Norris's birthday.",
            "The whole thing is it's blurry there, but the whole thing is a Chuck Norris joke.",
            "If you know Chuck Norris jokes there.",
            "So the whole article structure as these.",
            "The good thing about CNN is, here's the article here and here.",
            "What are called highlights?",
            "So their short sentence is which a little summary sentence is about the contents of the article and the nice thing is that they're written by humans are not automatically extracted, so they paraphrase the information in the article.",
            "So what we're going to do is take these highlights and turn them into questions.",
            "These give us good questions because the information that in that little highlight is often spread out in the document.",
            "It's not just matching one sentence, so that's exactly what we're looking for.",
            "At some point we realized that the Daily Mail did the same thing, so we also have these Daily Mail articles and the Daily Mail is quite fun to talk.",
            "I don't know how many of you know the Daily Mail is a English tabloid, but it makes for quite good reading.",
            "They're also quite fascinated with us, so we get to our models, get to read about themselves in the Daily Mail.",
            "Or in this case, this article here is about Google interns running Wild in in Mountain View, which you can't see there.",
            "But if you Scroll down, there's a photo of one of the authors of this of our paper.",
            "So yeah, the dining miles.",
            "Lots of fun.",
            "So that's what we're going to do.",
            "We're going to collect lots of these articles.",
            "The nice thing is that they've been doing this for years, so when we saw this, a summarization data sets are quite small.",
            "They only have a few thousand articles.",
            "But we basically went to the Google index and we can look, and I think the CNN's been doing this since about 2007 and so we can just take all of those.",
            "All of those are."
        ],
        [
            "Tickles so we end up with things like this.",
            "So here's a short snippet of a document about Top Gear Presenter top Gears of British TV show about cars, it's presenter, Jeremy Clarkson, is known for being unstable.",
            "In this case he was punching his producer.",
            "This is one of the highlights and what we do is we just create questions out of this by looking for entities in the query and removing them.",
            "So here we've removed the name of the producer.",
            "So this hasn't.",
            "It's actually called a close query that's often used in second language learning.",
            "But we can create these sort of queries and so the model is going to take the document and the query and it has to output the answer.",
            "So we can get lots of these things from CNN and the Daily Mail."
        ],
        [
            "But the problem and would be familiar to anyone that read the Daily Mail.",
            "That these things are quite predictable.",
            "So to answer a lot of these questions, all you need is local context.",
            "So people have done this just as a language modeling task where you just take a sentence, remove one word and get your language model to predict the word.",
            "You don't necessarily need any world knowledge to do this, so a lot of the queries you can, you can guess the answer to without ever reading the article.",
            "The Classic One in the Daily Mail is that the Daily Mail is obsessed with things that cause or cure cancer.",
            "So if you see something like the high tech brother helps you beat breast, you're going to know that the answer is cancer.",
            "You don't need to read the article and so on for all the rest.",
            "So.",
            "So we need to get around this somehow because that would be a pretty boring.",
            "Task if we could just yes it with an N gram language model.",
            "So Ngram language model is just going to do very well on these sorts of things."
        ],
        [
            "So basically solution is to take ideas from other areas in particular.",
            "Ever popular MNIST data set so?",
            "One thing that sort of very quickly observed with these sort of simple data sets is that if you have an image of a zero, then you can transform that image and you know within reason that it's still a 0.",
            "So you can translate it or rotate it within reason, and so that's what people do, right?",
            "They permute their data set, they create lots of extra training data, and that helps their model learn these things in order sorts of different poses and such and.",
            "What what this is, is the idea that we have some knowledge about the task.",
            "We have some rule in our head about invariants in the task, and we're exploiting that to generate more data.",
            "So we want to do the same thing for our question answering problem."
        ],
        [
            "So what we're going to do is here again is our article about Jeremy Clarkson on the left.",
            "As it was, we're going to run an entity recognizer over the article.",
            "We're going to find all of the entities in the article and the query.",
            "Are we going to anonymize them so we're going to throw away Jeremy Clarkson and replace him by entity 212?",
            "So what we're doing here is trying to remove that sort of world knowledge aspect, and we're trying to make the task harder for the model in that the way the only way it has to answer the questions is to look at the document.",
            "So we're making this so that there's no way that can get the right answer without looking at the document.",
            "And so to do that we also randomize these entities.",
            "So every time we produce a training instance and answer pair for the model to train on, we re randomize the entities.",
            "So we know that it doesn't matter what these labels are, as long as the label for the the answer matches the one in the in the in the answer.",
            "So we can.",
            "We can re permute these each time and essentially almost gotten you training example.",
            "So now the model has just from the query in the just from the query alone, the model has no way of of guessing what the entity is even if it saw this query previously in training 'cause it could be a different a different label.",
            "It has to look at the document.",
            "It has to look at the context of the entities.",
            "And do some sort of process to solve the right answer.",
            "So that's the basic idea that we're trying to force the model to read the document and make the task that bit harder so that we can actually get this problem of reading rather than world knowledge.",
            "Obviously, if you wanted to deploy something like, this is a question answering system.",
            "You don't care about how your model is answering the question, you just wanted to get it right.",
            "So you would use any source of information you had, including world knowledge.",
            "But our point of view is just to try and focus on this and see how see how we can go.",
            "And this produces a reasonably hard."
        ],
        [
            "And answering tasks so.",
            "That said, even.",
            "Once we've anonymized these entities, these articles are pretty hard to read for a person, but you can still almost always answer the question.",
            "So it's it's a bit harder to read, but a human will still be able to workout without knowing what the entity refers to just by looking at the discourse context we understand the meaning of the language, and we can workout what their identity is.",
            "So."
        ],
        [
            "Still a well defined problem.",
            "So.",
            "So we collected this data.",
            "As I said, we went to the Google index and just took as much as we could back.",
            "However, long Daily Mail and CNN have been doing this for CNN.",
            "Our training data went back about 95 months.",
            "Daily Mail 56.",
            "The distributions have different properties like the Daily Mail tends to have more highlights per document.",
            "Longer articles so you can see the total number of queries we've got about 400,000 CNN, about 800,000 Daily Mail so.",
            "This is not huge.",
            "I would like more, but now we've got a significant number of training examples for reading comprehension time, so this is a lot better than having a few 100 now.",
            "We can actually think about building supervised models and training them.",
            "Yes.",
            "Is raw data.",
            "It's not easy to do so, so for obvious reasons we don't own the Copyright on the Daily Mail or the CNN, so we can't.",
            "We can't distribute the article text.",
            "Our solution is to distribute a script that will download articles and annotate them with the to reproduce what we're doing, we're going to need the articles and the entity annotations.",
            "The entity annotations were produced by a internal Google system, so you can't get that either.",
            "But what we can do is produce a script that will download them and annotate the entities.",
            "That's what we thought we'd do.",
            "It turned out to be more difficult, becausw.",
            "Firstly, I think Daily Mail in particular blocked our IP address pretty quickly when we started scraping.",
            "Those articles and that would obviously happen that to anyone that tried to reproduce it.",
            "Secondly, they keep changing their article templates and the annotations are.",
            "Obviously if you change a template and the structure of the article changes at all the entity annotations going to be off and our tokenization and so it's all going to mess it up.",
            "Anyway, we've solved that now and basically what our current solution is to.",
            "We have a script that will download them from the Internet Archive from a snapshot.",
            "With just trying to finalize that, so not all of the articles we got are in there.",
            "There's actually some articles we didn't get, so the data sets are a bit off and we just re running all our models.",
            "On our new data set, but hopefully that should all be available.",
            "Soon.",
            "OK, so let's talk about the data set.",
            "So we've got.",
            "We've got more than a million training points in total, and that's before we start permuting them, which essentially produces an exponential number.",
            "These are not, so all the examples I give a quite short just so I can fit the model slide, but the articles aren't short in general, so the average number of tokens we've got about 780 or 1000 words in CNN and the Daily Mail, so these are not 30 word articles.",
            "Some of the articles are the maximum number of entities, some of them have sort of hundreds of entities.",
            "The average number is about 30 or 40.",
            "And that's roughly the set that you have to choose from to answer the question so it's a non trivial.",
            "Problem.",
            "Yeah, and the vocabs are relatively large as well.",
            "OK.",
            "So I mean the other nice thing about this is it's evolving so we can keep grabbing more months.",
            "So this was basically there so you can see that up until the end of April, which was about when we were doing the testing.",
            "But each month, particularly Daily Mail is.",
            "The number of articles and highlights are producing is not linear across time.",
            "the Daily Mail seems to be increasing so that data sets growing.",
            "Re"
        ],
        [
            "Lative Lee quickly.",
            "OK, so one thing we did on this first question obviously is.",
            "Is this a well defined set of question answering task and how hard is it so we did a sort of informal survey of our data looking at trying to categorize a different different questions on what sort of processing you would need to do to answer this question and whether you could answer it at all as a human.",
            "So we came up with these different categories, so a simple question is one that you can answer.",
            "Just basically the question is stated word for word in the article.",
            "So all you have to do is match like a template and you get the right answer and so roughly 12% were that simple.",
            "And here we have the sentences so and so a lexical is 1 where the question is essentially stated in the article, but there's been some lexical lexical change so.",
            "The question might say something about someone being murdered and the article might say something about someone being shot.",
            "So to get the right answer you just need to know that lexical relationship between shot and murdered, but otherwise the rough structure of the query and the way it's the way it's stated at the same, so roughly 40% of those you can get and we know that sort of things like word embeddings and these sort of things are pretty good at getting upset lexical generalization.",
            "OK, so coreference so that code references where you've got some entities which are referring to the same underlying entity, possibly in different ways, and that includes anaphora and things like this so.",
            "John went to the park.",
            "There he played baseball so that he is a reference to Jaune.",
            "So some questions you're going to need to be able to resolve an effort like that.",
            "The answer to the question may be stated in terms of a pronoun, which is the actual correct entity, but you have to know which entity that pronouns referring to to get the right entity.",
            "So roughly 10% of our answers needed coref resolution, but simple coref here means that the.",
            "The statements basically word for word for the question, but there's some reference resolution you need coreference and lexical means that you need to do both reference resolution, an lexical generalization to get the right answer.",
            "Yeah, so this was just a sample.",
            "So yes, I mean in theory you could have curve, but we just have from this sample.",
            "We didn't have those.",
            "But you're mostly the car ever going to be across multiple?",
            "Send this is two or three sentences, so we're up to psycho.",
            "Reference in lexical is just a combination of those where you've got to both resolve references and do some sort of lexical generalization.",
            "And then we have this category which we're calling complex, which basically means you need to do inference.",
            "So as a human you need to read this, you need to think about it in some way and put together the connections and then you can answer the question.",
            "But the answer is not not sort of stated for you.",
            "So that's where?",
            "Yeah, that's that's the level that you're doing inference, and then roughly 10% where what we said is unanswerable.",
            "That is, a human would not be able to answer this question.",
            "And that's because the anonymization can lose information that you need.",
            "So the answer may be dependent on exactly.",
            "Who the entity is and your knowledge of that, and without knowing who the entity is, you won't be able to answer the question, but that's a relatively small amount.",
            "Only 10% of these questions actually were unanswerable after this, so still 90% of the questions are answerable just from the anonymized data.",
            "The interesting thing here that's interesting for later on is if you take out the complex.",
            "Basically 60% of the questions you should be able to answer.",
            "If you can do Co reference in lexical generalization and if you want to get anymore than that, you probably going to be doing some sort of inference or getting lucky."
        ],
        [
            "OK, so.",
            "So now we have.",
            "We have some data.",
            "The next thing we need is some baselines and benchmarks to know.",
            "How hard this problem is, and whether there are simple solutions.",
            "So we tried a few different things, so the obvious baseline to start with is just a frequency baseline.",
            "So what if you just take the most frequent entity mentioned in the article and always answer that as your answer, so that will give you sort of CNN.",
            "We've got sort of 2822 for the Daily Mail percent accuracy, and that's pretty natural.",
            "You imagine these highlights are going to talk about frequent entities, so there's going to be a bit of advice there, but still, that's a long way short of solving the problem.",
            "We can refine that a little bit by saying that if an entity is mentioned in the query, then exclude it from the frequency, 'cause it's unlikely that the the answer the question will be mentioned in the query other than where it's actually answered.",
            "So that gives you a bit better again.",
            "So then we're about 30%.",
            "So 30% is basically our baseline.",
            "If we can't do better than 30%, then we're not doing anything."
        ],
        [
            "Next, we we sort of go to the traditional NLP bag of tricks.",
            "And at Google we have we have a big NLP pipeline and traditional pipeline of parsing, tagging and semantic annotation.",
            "And so we can go to this.",
            "We can feed in any document and it will spit out all of these annotations for us.",
            "So we looked at these and tried to.",
            "We've tried various combinations of these to try and come up with sort of a heuristic rule based baseline to see if if it was easy to get these answers just from basically relations that we could extract quite quite comfortably.",
            "So we ran as a semantic role labeling system.",
            "And then one of the engineers spent loads of time trying different sequences of heuristics, and this is when he came up with the best.",
            "So we had this so that the semantic role labeling is giving these triples of relations and we basically back off from one of these to the other.",
            "So if we see an exact match like this then we can answer a question.",
            "We got exact matches to be if the frame is correct.",
            "But the arguments don't necessarily match.",
            "We can still guess that as an answer, and so on.",
            "The frame might be permitted, but if it's the same by that point, we might as well have a go at guessing that.",
            "And then if it's just the same entity again, even if the frame relation is different, it's worthwhile guessing that is an answer, so we're taking this long sort of engineered statistical pipeline through our roles and semantic roles, and then trying to use those in a heuristic way.",
            "This is far from optimal.",
            "You could do a lot more with this, and obviously you probably apply some machine learning to learn this better, but it gives us a starting point that just would this be with the information.",
            "Just be easily accessible given a traditional NLP pipeline and the answer is no."
        ],
        [
            "So this doesn't really do much better than the baseline, and as I said, an engineer spent a lot of time trying to all different combinations of this.",
            "It could never really do much better than this.",
            "And the problem is that in the traditional pipeline, you just don't really have access to the right information to answer these questions.",
            "So that sort of semantic role labeling systems at just not capturing the relations you want to answer the questions.",
            "So, so you have the poor coverage of relations.",
            "Also, some of these require multiple relations too.",
            "Answer Again we could.",
            "We tried to get that with you risztics but couldn't really do so."
        ],
        [
            "So here's an example.",
            "That the sort of semantic annotation system just fails on, so Tom Hanks is friends with ex's Manager Scooter Braun.",
            "So that's the query we want to know what X is.",
            "This is what the document says.",
            "So the the.",
            "The semantic web just completely fails to pick up those relationships.",
            "It's not a nicely cleanly structured sentence, and again, this comes down to that sort of precision issue.",
            "The annotation models are they just not that high precision for capturing relations.",
            "So if you're running them on huge amounts of data then you can wear the relations are stated in multiple different ways.",
            "You'll generally get get it eventually, but if you have one shot at picking out the right information, it's unlikely.",
            "OK, but if you look at this you can see that there's a lot of over lexical overlap between the question and the answer so."
        ],
        [
            "Maybe something far simpler is going to work, and that's what we call the word distance benchmark.",
            "And what we did is we just basically took.",
            "For the entity in the query, we took a window of words around that and then we looked at every entity in the document and we looked at the words around that we matched up, try to match up these two windows and align the words mentioned and the entity that has basically shared the most words in common with the query.",
            "We returned as the answer.",
            "Modulo a few more heuristics to make this a bit better."
        ],
        [
            "And that works pretty well.",
            "So now we start to get a lot better than our baseline.",
            "So now we're up in the mid 40s or even the mid 50s for the Daily Mail.",
            "So that word distance benchmarks actually pretty robust, so it's good at capturing lots of those.",
            "Lots of the sort of exact match ones.",
            "It's not bad at the lexical generalization where it just ignores it, and if there's at least one word in common, it sort of can get lucky."
        ],
        [
            "OK, so that was some.",
            "Sort of non machine learning type approaches.",
            "Obviously we collected lots of data because we wanted to apply machine learning supervised machine learning to this problem.",
            "So that's what we're going to do now.",
            "So our framework is basically this we have.",
            "We have a document D and a query Q and we want to produce and answer A.",
            "We're going to do that by jointly embedding the document in the query and then doing a softmax with the answers.",
            "The answers are going to be anything that occurs in the document, so in these models were not going to.",
            "We're not going to tell the model at the answer is an entity.",
            "It could be any token in the input for all of our baselines and benchmarks.",
            "We just structure that 'cause we knew exactly what the constraints were on the answers, but we want to make this more general, so we're just saying here all the words in the document pick which one is the answer.",
            "So we're going to look at different ways of doing G, basically so that how do we."
        ],
        [
            "The documents in the queries.",
            "OK, so here's our DLS, DM.",
            "Again, if you were missing it from earlier on.",
            "So this is basically our first idea is.",
            "Let's go so lots of people are using DLS teams to any sort of encoder decoder framework to do transduction problems.",
            "Let's just treat this as a big encoder decoder problem where we're going to feed the document and the query all as one big string into a deep SDM, and then read out the answer.",
            "It sounds sort of hopeful.",
            "As I said, our documents are up to about 2000 time steps, so this is getting a bit past the sort of ordinary sentence links dependencies.",
            "But that's our starting point, so here's our big deep LS TM.",
            "We're going to run this.",
            "Our G function is simply going to be the final state from the STM, so this just says we run we.",
            "Create one big long string from our document.",
            "Now query.",
            "We run the Ellis team up until the last date we take that last date and that sell our embedding for our document query and we try and pick which entity.",
            "So remember that.",
            "The entities are being permuted so the entities that model is going to learn embeddings as such for the entities, but they're going to be meeting entities themselves have no meaning, so this is the substitution problem.",
            "The model needs to be able to detect a symbol in the input and substitute it to the output so it's a bit more difficult than than the other transaction."
        ],
        [
            "Blooms.",
            "So this is sort of the diagrammatic representation where we're saying we got this deep recurrent network.",
            "We're just feeding everything in, so this is our document.",
            "It's a bit of a trivial document, but this is our query at the end we get.",
            "Gee, there's two versions of this.",
            "Obviously we could go document."
        ],
        [
            "First, then query or we could go query 1st and then document.",
            "If you think about if we if we go document 1st and then query.",
            "The model doesn't know what the query is going to be, so it sort of has to put all of the information about the possible answer into the hidden layer at the end of the document.",
            "If we go query first, then you can sort of think of this like a filtering problem where the model first gets to know what it's looking for and then it goes through the document trying to find that.",
            "So this should be a bit easier and it turned out that this would work a bit better the other way.",
            "It did learn something, but it didn't learn as much as as this.",
            "But this is still an extremely hard problem for this sort of model, 'cause I said we're talking about on average for Daily Mail 1000 time steps, so we're not telling the model anything about the structure.",
            "We're not telling it that there's a query in a document it doesn't know that there's just symbols were not telling it that there's a symbol that it has to substitute for.",
            "It just has to learn that.",
            "So there's a really long range dependency from some X variable up here to the answer way at the end.",
            "1000 time steps later, so it's a pretty hard problem."
        ],
        [
            "For a model like that to learn so it does it a bit better than the word distance model.",
            "So it's not much better, but as I said, this is structured in the way that we structured I'm I was quite impressed at this model and managed to learn anything over such a long.",
            "With that long dependencies.",
            "So these models are surprisingly.",
            "Adaptable."
        ],
        [
            "But we know we can do better than that.",
            "So.",
            "Following on from the themes of earlier, the next step is to introduce an attention.",
            "Mechanism 'cause that's going to get around this bottleneck of the final vector.",
            "So now what we're going to do is we're going to embed the document.",
            "We're going to embed the query and we're going to let the model attend over the tokens in the document and summarize that and then produce an answer so.",
            "The mass of this is basically that we run and we're going to use bidirectional Ellis James to do all of our embedding, so we run bidirectional STM.",
            "They're going to produce two embeddings for the query, and we're going to concatenate those together into.",
            "You were going to run bidirectional STM's over the document, and for every time step T we're going to create an embedding for that time step.",
            "This is very much like the translation model I showed earlier.",
            "Then when it comes to answer the question, we're going to create, we're going to calculate these attention variables M and they're just a function of a given time step, and the query embedding we're going to apply a softmax, and then we're going to mix all of those together into one representation for the whole document are.",
            "So as I said, roughly the same as what's going on in the empty model, and then our overall embedding is just going to be an LP of those quantities that we've calculated, and so that's what we call the attentive reader."
        ],
        [
            "So there's my diagrammatic representation of that, so here we have our document.",
            "We have bidirectional STM's we concatenating timesteps together.",
            "We these are our attention variables.",
            "We mix all that together into.",
            "We do our bidirectional embedding of the query.",
            "We get you.",
            "We put these together into G. So now what we want is the query to pick out the bits of the document that are going to help it answer the question.",
            "So this gets around our bottleneck problem.",
            "It's still a hard problem because it still has to do this substitution.",
            "It still has to somehow, so it's going to look at the entities that might answer the question in the document.",
            "But the entities themselves have no information.",
            "It has to look at the context around them to decide whether it should extract them and substitute them with their questionnaire.",
            "The attention doesn't know about the question, sorry it.",
            "Sorry, so this diagram doesn't doesn't include sort of how these are calculated, so these are calculated dependent on these.",
            "This is just showing what happens after we have the attention variables."
        ],
        [
            "So.",
            "So back here, this is Mr.",
            "The unnormalized attention variables.",
            "So there are functions are just an MLP function of you, which is our query embedding and the particular time step.",
            "So we compute one of these for every token in the document."
        ],
        [
            "So that starts to start to work quite well.",
            "So now we're up into the sort of 60% region for this task.",
            "And we also have a benchmark here, which we call the uniform attention because just to compare and say, well, does attention actually do anything or are we just mixing together everything like a bag of words and that doesn't work at all, so the attention mechanism is definitely.",
            "Crucial to that that performance so uniform attention just means set all those attention variables to zero and so every time step gets mixed in the same amount.",
            "So.",
            "Roughly, according to our expectation, that starts to work quite well.",
            "So the nice."
        ],
        [
            "Think about the well before I get there.",
            "OK so if we.",
            "So some more implementation level.",
            "Details.",
            "So these are quite big models, so we're now looking at training LS teams where thousands of time steps in mini batches.",
            "We're doing this on GPU's, I think on average we used about 25 GPS.",
            "Thomas can correct me if I'm wrong on any of these details as he did at all.",
            "This is quite big models.",
            "We're running for this is the attentive reader.",
            "This hours on the bottom we're doing this with.",
            "Asynchronous.",
            "Gradient descent, the interesting thing about the behavior of these models for anyone that tries to reproduce this, is that for at least the 1st five hours, they do nothing, so they're basically random.",
            "So if you just sort of.",
            "Start your model running.",
            "Go and have lunch and come back and say oh it's running random.",
            "I must have.",
            "I must have a bug.",
            "You might have to wait a bit longer so it takes quite a long time for the Model 2.",
            "To get any sort of gradient on the document so it can work out what the direction is to go in.",
            "And that's because the problem is is quite hard, because the first thing the model tries to do is learn embeddings for the the entity labels that will help in Atlanta the question, but the entities are random, so there's no information in them, so it puts all this grading on those and it just oscillates around random for a long time, and eventually enough gradient goes onto the context around the entities for it to start understanding that actually it's not the entities that matter, it's the context around them.",
            "It's how how they're realizing the text.",
            "So we start.",
            "We see this sort of convergence behavior so.",
            "And this sort of thing is true for a lot of the models people are playing with now, so these memory networks and neural Turing machines and such.",
            "They don't behave like your old fashioned neural models of just a nice increase.",
            "They have lots of discontinuity's in the objective function.",
            "Well.",
            "So there's two sides to that.",
            "Yeah, we could come up with initialization, but at the same time we're interested in not just sort of cracking the task, but seeing what the model can learn, so we could sort of initialize to to emphasize the context over the entities or things like that, but in some sense we want to see if the model can learn that.",
            "Well, I think the they behave differently enough to the other models like the NTM is worse, so you see this behavior where it does nothing for awhile.",
            "There's a big jump does nothing for awhile.",
            "Another big jump, so it it's much harder to monitor and see how this is going and it's hard to workout why those things happen.",
            "The very simple task like the stack tasks you can tie it to just whether it's like in something like reversal with a stack, it just has to learn the right push and pop weights.",
            "That's quite 1 dimensional, but this is a lot more.",
            "Complex, but yeah, I think.",
            "It's it's an interesting initially.",
            "I mean, the problem here is just the classic problem that the gradient on the thing you don't care about is big, and the grading on the thing in the context is really tiny and it takes a long time for the model to.",
            "For that grade, integrada featherweights that grow up enough that they start to impact the learning.",
            "No, I mean, you could easily come up with all sorts of curriculum approaches and things like that.",
            "And at the moment we are very much in the phase of.",
            "Creating the data set and trying out different things on this task to see firstly whether it's an interesting problem, how these different models perform, whether whether this is easy to do with traditional models or whether the newer models can't do this at all.",
            "As I said, we've got very long range dependencies, so it wasn't clear whether the recurrent networks would do anything with this, so we're not at the point where we're trying to build a specific model to crack this task.",
            "We are more interested in.",
            "Is a reasonably generic sort of attention architecture able to to get anywhere with this problem?",
            "Yeah you don't.",
            "I mean, that's it.",
            "That's the problem with the optimization, and that's always a problem with optimization.",
            "But it gets worse with these models.",
            "If you're doing a synthetic task, then you you know when it solves the problem.",
            "But if it's a real task, and yes, that's a real problem.",
            "And sometimes the models won't learn anything you'll.",
            "Not these ones always seem to learn eventually this, but that five hours can be can vary quite a lot.",
            "So yeah, it's there a bit more difficult to to work with."
        ],
        [
            "OK, so the nice thing about attention models you can visualize what they're doing, so here's a short article.",
            "I don't know if it's readable.",
            "Which one was this?",
            "OK, so this is a queries about a deceased sailor.",
            "Who was identified by someone?",
            "And we want to know who they were.",
            "So some entity in here is that the name of the sailor.",
            "So we've got to see that.",
            "So here we are saying that someone was killed in a parachute accident.",
            "Here we have a pronouncer.",
            "He was identified as into the 49.",
            "That's the right answer.",
            "So.",
            "For.",
            "And you can see that the models have mostly getting the attention around there in a bit.",
            "Spread out around here.",
            "You can see that to sort of every to get a reason to answer this question, you need to resolve this anaphora.",
            "But you can also see that you can probably guess the answer abit from local context, but the connect the person that was killed is this person you need to go through that.",
            "So so the the annoying thing about these visualizations is always visualizing the attention variable underneath the attention.",
            "Variables are bidirectional, STM that's spreading information around and our hope is that that is what gives us.",
            "Some managers connect up into these a little bit.",
            "We know that that LS teams are pretty good at doing coreference resolution, so we're hopeful that they might do something like that.",
            "But it's hard to tell what we need to do is visualize what's going on down there, so.",
            "This is just telling you, sort of where the attention is entering the sequence, but it's not telling us where the information is spreading out over the sequence.",
            "So if we can visualize the LS teams, hopefully we might see that there was some some weight going here and some weight back here on killed and things like that.",
            "If we were really lucky, or it might just be some random words around here that helped it.",
            "Guess the answer.",
            "But yes, that we're working on better."
        ],
        [
            "Realizations.",
            "How are going OK so actually this one is pretty boring."
        ],
        [
            "It's better to look at one that got wrong, so this is an example where it got wrong the answer.",
            "So the question is about a passenger train derails 35 kilometers East of Entity 11 in North and X and the correct answer is 37 up here and the model goes for 85.",
            "The modeler tends to basically the two locations, so both of these are locations, so in some sense you can sort of see how it's gone wrong.",
            "It's it's unsure which of the two locations mentioned are the right answer and it's picked the wrong one.",
            "Now it's looking at every token, but the model does learn that the answer is going to be one of these entity sets, so we're not telling it that, but it learns it.",
            "So all the model sees as integers because these are all every every unique token has an integer as just one set, so there's nothing a priority that marks out the entities, but the model smart enough to know that it keeps seeing into these answers so.",
            "They're probably good things to look at."
        ],
        [
            "So that's another one.",
            "So this is a sort of question where it's actually you can't necessarily answer it, so the question is asking about a car detonated.",
            "Near a police vehicle in X and the locations mentioned at multiple granularities of the city level and the regional level and basically the models unsure between.",
            "So this is Southern somewhere and happening somewhere.",
            "So it makes intra."
        ],
        [
            "Eating sort of."
        ],
        [
            "Magical.",
            "Errors.",
            "OK, so to continue.",
            "So that was a model that basically got to see the query in the document and sort of one chance to attend.",
            "An obvious extension is to give the model the ability to attend multiple times to a document to actually look at multiple bits of the document over a few time steps.",
            "So the first thing we tried along this line is something we called the impatient reader and basically as we read the query, we let the model computer tension variable.",
            "So now we run our current network over the query and at each time step as we were eating the query with also attending to the document, summarizing it down to a vector an reading it.",
            "So this is the math.",
            "I mean, it's pretty similar.",
            "The only difference now is that we've when we include the attention variables we've got the the attention.",
            "Document summary vector from the previous time step and we have a second dimension here 'cause we're doing this attending at every every time step."
        ],
        [
            "So this is vaguely what this looks like.",
            "So again, we've got our document building query embedding.",
            "We calculate sort of 1 attention variable, then we do it again.",
            "Then we do it again as we go through the query and then we produce our final.",
            "G."
        ],
        [
            "So this works a bit better, not a huge amount better.",
            "And it's not clear that tying the attention to the query is the right way to go.",
            "We're now trying other things where the the number of sort of attendant steps is untied from the query, but it does work a little bit better.",
            "This also makes for extremely big models, so some of our queries might have sort of 40 time steps in it.",
            "So now we're replicating our the size of our previous model by about 40, so we had a lot of problems getting all of this to run on on GPU's.",
            "Lots of work went into the optimization."
        ],
        [
            "Not hugely interesting, but this is the precision recall curve, right?",
            "This is the uniform attention baseline, and the two yellow and red models.",
            "There are the different attention models and you can see that they have quite a nice linear backoff in recall.",
            "Versus precision so.",
            "OK, so the nice thing about the impatient readers, we can visualize the repeated attention step.",
            "So we made some nice.",
            "Nice videos of this.",
            "So actually I will start this again.",
            "It's going a bit fast.",
            "So the blue highlight is the query and as it reads each word you can see the attention where it's attending.",
            "So the interesting thing maybe is that as it goes past the X, so basically up until the X, it's got no idea, but once it goes past the X you can see that it starts to put its attention on the HOV Lane up here, which is the right answer.",
            "So this is an article about a guy that.",
            "Was driving the HRV Lane, which I think is something you have in North America for cars that have two occupants in them, you get to drive in a special Lane so this guy put a Cardboard cut out of a guy from a beer commercial in the passenger seat and drove in that Lane and was caught.",
            "So that the correct answer is HOV Lane there.",
            "And the interesting thing is that up until about the Texan model has no idea.",
            "Once it sees that basically it has most of the information it needs, and you can see the attention goes on to the right answer.",
            "So that's nice.",
            "Another one.",
            "So.",
            "To explain first, so this is.",
            "Lamborghini that was found.",
            "And so the answer is Lamborghini.",
            "Interesting thing is, like the yellow here is never mentioned in the article, so you would think that that would be a good clue, but it's never actually mentioned in the article.",
            "So again.",
            "We can.",
            "If you follow the model as it goes, you can see it's got no real idea when it.",
            "So at this point you've got some attention on Lamborghini, but it's also got some attention on the Dallas North Tollway for for some reason which doesn't make so much sense.",
            "You can see that that that is mentioned here.",
            "So as soon as it sees that it realizes that's not the right answer.",
            "And it had focuses on the right thing.",
            "OK, so that's a couple of those examples."
        ],
        [
            "And.",
            "That's about it for me so.",
            "This is very much initial work.",
            "We weren't necessarily set trying to produce the sort of final deep learning model or machine learning models for this task.",
            "What we mostly set out to do is see if one we could come up with a plausibly big enough training set for supervised reading comprehension that we could actually start exploring these models and then to explore a few different simple models.",
            "Based roughly on things from other people have been doing for tasks and see how these perform, and it's clear that one.",
            "The.",
            "The deep learning models work quite well.",
            "They seem complex, but they're actually very simple models.",
            "When you think about it and we're just giving them the starter, and they roughly learn the right problem given lots of time and lots of GPU's.",
            "So it seems like this.",
            "There's definitely some way to go in this problem of using these models to read the text.",
            "The attention aspect seems very.",
            "Useful, it seems like a good way of structuring the models of these problems.",
            "Obviously, if you.",
            "If we wanted to do inference, you notice that those those all those scores, the accuracies topped out about 60%, which earlier and I said was about the limit before you actually not start to need to do some inference, so.",
            "We're probably not doing any sort of high level inference at this point, but it's interesting to think about how we might be able to do that with these attention models.",
            "So we're going to have to put multiple bits of information in our document together, do some processing on it, do some sort of reasoning, and come up with an answer.",
            "But so the other thing is that this the basic idea for creating this data is is general, so anywhere where we can find documents and some sort of paraphrase or repeated information from these, we can turn this into queries and train these sorts of models.",
            "So this works also when you have abstracts and introductions on documents and then document itself where that information is expanded upon.",
            "You can create queries from the abstracts and things like this.",
            "So there's more to be explored and to really do this to really learn.",
            "The sort of structure of language you're going to need to answer or do inference."
        ],
        [
            "Alex inferences, we're going to need a lot more than a million documents, millions a good start, but we're going to need a billion or something like that.",
            "OK.",
            "Thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Part 3 for NLP today.",
                    "label": 0
                },
                {
                    "sent": "So this for this second lecture I'm going to talk more about research rather than sort of background material, so these are.",
                    "label": 0
                },
                {
                    "sent": "Going to be sort of two things that we're doing at DeepMind with with NLP.",
                    "label": 0
                },
                {
                    "sent": "So yes, I spend maybe 80% of my time at DeepMind and 20% at University of Oxford, and these people here are sort of a subset of the group at at deep mind that I work with.",
                    "label": 0
                },
                {
                    "sent": "Thomas is up the back there somewhere who did most of the implementation for the comprehension part of this this talk so you can ask him about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so lots of people have been asking me about deep mind.",
                    "label": 0
                },
                {
                    "sent": "So just for the fun of it, nothing to do with what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "I throw in a couple of deep mind videos for context so those of you that know about deep mind probably seen all the Atari stuff before, but it's still fun for those who haven't seen it, so I'll play this so deep mind is well, startup was acquired by Google.",
                    "label": 0
                },
                {
                    "sent": "It's basically a Research Center interested in general problems in AI were quite big now.",
                    "label": 0
                },
                {
                    "sent": "Lots of good machine learning researchers there doing all sorts of interesting things, but one of the sort of most well known public things was the Atari reinforcement learning and the key thing about this was that it was this model with a big covenant with a reinforcement objective, and it was learning to play these games just from the pixels.",
                    "label": 0
                },
                {
                    "sent": "The pixels, not from the actual memory state of the emulator or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So this is it playing breakout.",
                    "label": 0
                },
                {
                    "sent": "You can see it gets pretty good, never really misses.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Eventually he learns to go over the top at some point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, about there.",
                    "label": 0
                },
                {
                    "sent": "So it's cool.",
                    "label": 0
                },
                {
                    "sent": "It's only say this is Roadrunner and you can see that's Roadrunner.",
                    "label": 0
                },
                {
                    "sent": "It's it.",
                    "label": 0
                },
                {
                    "sent": "It gets pretty much perfect killing the coyote every time.",
                    "label": 0
                },
                {
                    "sent": "And then left learns various tricks.",
                    "label": 0
                },
                {
                    "sent": "Where is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, like that.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "And this one so.",
                    "label": 0
                },
                {
                    "sent": "But the white is the the.",
                    "label": 0
                },
                {
                    "sent": "The neural network.",
                    "label": 0
                },
                {
                    "sent": "You get away in a moment, I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then back to that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the tennis is sort of boring.",
                    "label": 0
                },
                {
                    "sent": "It just really learn to Clock that one every time.",
                    "label": 0
                },
                {
                    "sent": "I also had I don't know, has anyone talked about draw yet.",
                    "label": 0
                },
                {
                    "sent": "So deep generative models?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you're probably more about deep generative models.",
                    "label": 0
                },
                {
                    "sent": "I think I saw someone talk about this.",
                    "label": 0
                },
                {
                    "sent": "We have a recurrent network with an attention variable.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing about this model called Draw was it didn't just do attention and classification.",
                    "label": 0
                },
                {
                    "sent": "It also does generation.",
                    "label": 0
                },
                {
                    "sent": "So it's a recurrent generation model as well with attention so.",
                    "label": 0
                },
                {
                    "sent": "So that's the model reading digits so that green boxes the attention this is actually generating digits.",
                    "label": 0
                },
                {
                    "sent": "You see the attention as it generates.",
                    "label": 0
                },
                {
                    "sent": "It looks quite cool, but like someone writing and we can, we can generate multiple digits.",
                    "label": 0
                },
                {
                    "sent": "And this is Street View numbers from Google, so this is purely generative, and they're pretty damn good.",
                    "label": 0
                },
                {
                    "sent": "Street View number renderings.",
                    "label": 0
                },
                {
                    "sent": "I'll see if I can.",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Now can I?",
                    "label": 0
                },
                {
                    "sent": "Now, well?",
                    "label": 0
                },
                {
                    "sent": "So some of the cool things.",
                    "label": 0
                },
                {
                    "sent": "Going out don't mind what what I do.",
                    "label": 0
                },
                {
                    "sent": "There is language so we have a language group, NLP, deep learning, the Inter interested in all sorts of problems, particularly these sorts of things that I'll talk about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And this is sort of recapping on where we've been this morning.",
                    "label": 1
                },
                {
                    "sent": "So in NLP at least, there's lots of work now in deep learning.",
                    "label": 1
                },
                {
                    "sent": "Initially, a lot of it focused on classification problems.",
                    "label": 0
                },
                {
                    "sent": "Redoing a lot of the stuff we've done previously with other machine learning models, kernel machines, and such document classification tagging.",
                    "label": 0
                },
                {
                    "sent": "And there we've seen lots of good results and mostly incremental over previous results.",
                    "label": 0
                },
                {
                    "sent": "And what is really the sort of interesting thing now for the future is doing the harder tasks that we've not been able to do so much or so well in the past.",
                    "label": 0
                },
                {
                    "sent": "So those sorts of things we're interested in going towards natural language, understanding tasks.",
                    "label": 0
                },
                {
                    "sent": "So a classic example of this is question answering.",
                    "label": 0
                },
                {
                    "sent": "This is just an example of a very simple question answering paradigm where you have a question.",
                    "label": 0
                },
                {
                    "sent": "And you have some candidate utterances and you're trying to workout whether that the sentence answers the question or not.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of thing that's used in some sort of information retrieval style question answering where like on Google when you type in a question and it looks at the results and decides that the IT can find the answer to your question from one of the documents in the results and it will highlight at the top.",
                    "label": 0
                },
                {
                    "sent": "So that's the question answer selection problem anyway, so we can do that very well using these embedding models that we've been talking about.",
                    "label": 1
                },
                {
                    "sent": "We can just embed the question, we can embed the answers and we can learn a function of the embeddings.",
                    "label": 0
                },
                {
                    "sent": "That predicts whether this is just binary, it's whether it's answers a question or not.",
                    "label": 0
                },
                {
                    "sent": "It's not a huge amount, it's 10 or 20,000.",
                    "label": 0
                },
                {
                    "sent": "It's a TREC data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There we have a lot more so, and you can also train on the existing.",
                    "label": 0
                },
                {
                    "sent": "System if you want so that's called web answers that does it when you type questions into Google, there's lots of different systems to get fine.",
                    "label": 0
                },
                {
                    "sent": "That's just one of them that that might try and answer a question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so I mean, we've been hearing about Ellis, James and Recurrent models and so they they prove quite good ways to do this.",
                    "label": 0
                },
                {
                    "sent": "We can just have a answer embedding recurrent network and a question embedding recurrent network and then some function G at the end that computes that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've seen earlier that you can do translation this way.",
                    "label": 0
                },
                {
                    "sent": "There was that slide from the encoding.",
                    "label": 0
                },
                {
                    "sent": "Decoding ways of doing translation.",
                    "label": 0
                },
                {
                    "sent": "So for me translation is interesting from a higher level because it's basically the most.",
                    "label": 0
                },
                {
                    "sent": "Difficult or the closest to a natural language understanding task in NLP, for which we have lots of data.",
                    "label": 0
                },
                {
                    "sent": "So most of the natural language understanding tasks you would like to work on, like question, answering, dialogue, processing all these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "We just don't have any data to train models on, so that's largely part of the problem.",
                    "label": 0
                },
                {
                    "sent": "Trying to find data translation, we have loads of data that's quite a lot of semantics going on.",
                    "label": 0
                },
                {
                    "sent": "So really in some sense it's interesting because it's sort of the most difficult task we can we can tackle.",
                    "label": 0
                },
                {
                    "sent": "It's not just classification, it's more interesting.",
                    "label": 0
                },
                {
                    "sent": "But we have lots of data, so it's currently a cool point in the space of tasks.",
                    "label": 0
                },
                {
                    "sent": "Also the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do we have for translations actually useful for lots of other things, so parallel data isn't just good for learning to translate, it's good for learning semantics of language as well, because if you think about a parallel Chinese English corpus, Chinese and English look completely different.",
                    "label": 0
                },
                {
                    "sent": "They expressed meaning in a completely different way, but if you have a Chinese sentence in its translation, that main pretty much the only thing they share in common is their meaning, so that's a classic sort of latent variable problem that we have this data where we have these.",
                    "label": 0
                },
                {
                    "sent": "Different ways of expressing the same meaning, so we can exploit that to learn some sense of language understanding.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the moment it deep mine, what we're mostly interested in is this.",
                    "label": 0
                },
                {
                    "sent": "Going past these tasks too.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "Sort of deeper, natural language.",
                    "label": 1
                },
                {
                    "sent": "Understanding tasks that we hope to be able to get data for, so one that I'll talk about in the second half of this lecture is reading comprehension, so that's a question answering task, but we're rather than trying to answer questions from a database or something like that.",
                    "label": 0
                },
                {
                    "sent": "We have a document.",
                    "label": 0
                },
                {
                    "sent": "In this case, will be news stories and we have questions about those news stories.",
                    "label": 0
                },
                {
                    "sent": "It's an interesting problem because you need to learn something about the structure of language.",
                    "label": 0
                },
                {
                    "sent": "You need to be quite high precision because you only get the one document question answering on the web.",
                    "label": 0
                },
                {
                    "sent": "You can get away with being low precision because you can rely on the same thing being stated many times on the web, so you can have sort of low precision sort of relation extraction algorithms when you've just got one document, and you're being asked a question, then you need to be able to find the one sort of statement or set of statements that support it.",
                    "label": 0
                },
                {
                    "sent": "The other thing we are interested in is.",
                    "label": 1
                },
                {
                    "sent": "Connecting language and action in real environment.",
                    "label": 0
                },
                {
                    "sent": "So deep mines very interested in acting environments.",
                    "label": 0
                },
                {
                    "sent": "That's where all the Atari things came from and lots of other interesting things.",
                    "label": 0
                },
                {
                    "sent": "And so we work on how do we map from language to actions and environment, not just one action like question answering where you might your action is to query a database or something like that.",
                    "label": 0
                },
                {
                    "sent": "But can we do things like planning?",
                    "label": 0
                },
                {
                    "sent": "Here's an instruction.",
                    "label": 0
                },
                {
                    "sent": "Have an agent go off and do some sort of planning following that instruction and we.",
                    "label": 0
                },
                {
                    "sent": "We can do some interesting things again with recurrent networks.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such OK, so specifically this lecture today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about first some some sort of algorithm quick we're doing on.",
                    "label": 0
                },
                {
                    "sent": "Augmenting neural networks with memories.",
                    "label": 0
                },
                {
                    "sent": "And in the second part I'm going to talk about this idea of reading comprehension machine rating.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So recently there's been lots of interest in this idea of taking your recurrent network, something like that and augmenting it with a memory, some sort of persistent data structure with the network and store information, and read from it.",
                    "label": 0
                },
                {
                    "sent": "And there's been sort of increasing number of papers on on different architectures for this, so there's the sort of thing that Facebook had been doing.",
                    "label": 0
                },
                {
                    "sent": "Jason.",
                    "label": 0
                },
                {
                    "sent": "With memory networks, it turns out the memory networks and attention models are pretty much two sides of the same coin in the sense that the memory network sort of you write in all the information you want, and then you compute softmax is over, which bits of it you should read.",
                    "label": 0
                },
                {
                    "sent": "As you're out putting.",
                    "label": 0
                },
                {
                    "sent": "And that at DeepMind we have the neural Turing machine that Alex and Greg did.",
                    "label": 0
                },
                {
                    "sent": "It sort of adds an extra extra writing ability so it can both read and write from the memory and generalizes those option operations so they can be both.",
                    "label": 0
                },
                {
                    "sent": "Content based an.",
                    "label": 0
                },
                {
                    "sent": "Address based in at least lots of things.",
                    "label": 0
                },
                {
                    "sent": "So these are really cool and there's lots is increasing number of sort of demos, mostly all synthetic.",
                    "label": 0
                },
                {
                    "sent": "That shows that you can learn some really interesting or complex functions like Alex sorting numbers with the neural Turing machine.",
                    "label": 0
                },
                {
                    "sent": "But from a NLP POV, it's we looked at this and thought.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe these are cool, but maybe they're overkill for the task.",
                    "label": 0
                },
                {
                    "sent": "Do we really need random access memory to solve lots of the problems we're interested in?",
                    "label": 0
                },
                {
                    "sent": "That often sort of transduction like problems like machine translation.",
                    "label": 0
                },
                {
                    "sent": "And the problem with random access Memories is the scaling is not great, so you tend to get some sort of linear in the size of your memory factor for each prediction, because you normally have to do a big softmax or something over your over your memory and you'd like your memory to scale quite big, and so that's that's rather limiting.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll talk about our sort of initial exploration in different memory architectures that might be more efficient and more suited to NLP tasks.",
                    "label": 1
                },
                {
                    "sent": "Basically, by restricting the operations to a set that we think will.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adequate and will give us the performance that we.",
                    "label": 0
                },
                {
                    "sent": "We want.",
                    "label": 0
                },
                {
                    "sent": "So this this part is is all going to be about synthetic transduction problems, as we've seen in the earlier lectures.",
                    "label": 0
                },
                {
                    "sent": "Lots of NLP problems can be thought of as transduction.",
                    "label": 0
                },
                {
                    "sent": "Transduction is just transforming one sequence into another.",
                    "label": 0
                },
                {
                    "sent": "Obviously, machine translation is like this.",
                    "label": 0
                },
                {
                    "sent": "People have been trying to do syntactic parsing like this.",
                    "label": 0
                },
                {
                    "sent": "Take a sentence and transform it into a string that includes its past tree.",
                    "label": 0
                },
                {
                    "sent": "You can also think of computation like this, so there's work from the brain group at Google trying to take Python source code and transduced that into the result that that source code is computing.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through this sort of lens of transduction you can.",
                    "label": 0
                },
                {
                    "sent": "You can view lots of problems.",
                    "label": 0
                },
                {
                    "sent": "So just sort of reviewing what we saw earlier, what a lot of people are doing so that we have our input S our target T. If we have an RNN, we can model these conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "And we want to read in the source and generate the target and that can either be just purely greedy or some sort of beam search where we keep around.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And invest.",
                    "label": 0
                },
                {
                    "sent": "And as we saw earlier, you can do this just with recurrent network just by concatenating your strings and generating and this sort of forms out.",
                    "label": 0
                },
                {
                    "sent": "Baseline in effect, but as I said earlier, there's there's this bottleneck.",
                    "label": 0
                },
                {
                    "sent": "If we do this just with a raw RNN.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is an example of the Python, so this one example.",
                    "label": 0
                },
                {
                    "sent": "This is the Python thing that Wojtek Anelia were doing, so reading in Python scripts like that and outputing as a string.",
                    "label": 0
                },
                {
                    "sent": "The the value that is calculated so you can see that we can treat this like a transduction problem.",
                    "label": 0
                },
                {
                    "sent": "It's a lot more complex in some way than translation.",
                    "label": 0
                },
                {
                    "sent": "This is not a one to one mapping.",
                    "label": 0
                },
                {
                    "sent": "There's inference that has to go on here and it sort of worked for some simple examples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they were just using a deep deep LSM.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to talk about here is sort of as I said, other memory architectures besides random access.",
                    "label": 0
                },
                {
                    "sent": "So if you pick up your sort of CS101 textbook, we see stacks and queues and deques.",
                    "label": 0
                },
                {
                    "sent": "Ann, we thought?",
                    "label": 0
                },
                {
                    "sent": "Let's try those.",
                    "label": 0
                },
                {
                    "sent": "So we're going to implement neural stacks and New York use in your decks, just like the.",
                    "label": 0
                },
                {
                    "sent": "NTM the Rotary machine we're going to do this.",
                    "label": 0
                },
                {
                    "sent": "With soft variables, we're going to do this where everything is differentiable.",
                    "label": 0
                },
                {
                    "sent": "Of course, you could do this by just modeling a stackers with stochastic latent variables.",
                    "label": 0
                },
                {
                    "sent": "We tried the the continuous version, so like everything this has been done in the 90s so soon had a series of papers from the early 90s to later on with pushdown automata and stacks.",
                    "label": 0
                },
                {
                    "sent": "Some interesting papers, guys at Facebook are doing the same thing with similar.",
                    "label": 0
                },
                {
                    "sent": "With a similar to slightly different stack architecture and at the bottom is it Ed's work that is what I'm actually talking about here.",
                    "label": 0
                },
                {
                    "sent": "So the advantages here is that we want, unlike the MTM, we want the memory size to grow dynamically.",
                    "label": 0
                },
                {
                    "sent": "That's a great thing about a sticker or Q.",
                    "label": 0
                },
                {
                    "sent": "We don't have to pre define the amount of memory that we're going to store, we can just keep pushing things onto a stack and popping them off as we need.",
                    "label": 0
                },
                {
                    "sent": "So that's why we call it unbounded memory.",
                    "label": 0
                },
                {
                    "sent": "So everything is going to be solved by continuous pushing pump.",
                    "label": 0
                },
                {
                    "sent": "The sort of we can get unbounded long range dependencies with sort of a qualifier there, and flawless propagation of gradient again with a bit of a qualifier.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of allow.",
                    "label": 0
                },
                {
                    "sent": "Although we've claimed that the LS DMS are sort of giving you these long range dependencies, this is going to allow the model to more explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "T. Record long range dependencies, so this is what the stack looks like.",
                    "label": 0
                },
                {
                    "sent": "I'll just show the stack, the Q and the Decker just.",
                    "label": 0
                },
                {
                    "sent": "Obvious extensions of this, so we're going to have this continuous stack, so V there is a vector.",
                    "label": 0
                },
                {
                    "sent": "That's the thing we're pushing on.",
                    "label": 0
                },
                {
                    "sent": "So, so at each time step from our current network, we compute through just an MLP vector.",
                    "label": 0
                },
                {
                    "sent": "That's going to be pushed onto the stack.",
                    "label": 0
                },
                {
                    "sent": "We have another network that computes what its weight should be, so that's this thing here.",
                    "label": 0
                },
                {
                    "sent": "So T is the time step up here.",
                    "label": 0
                },
                {
                    "sent": "Forget about you for the moment and D is the weight of the thing that we're pushing on.",
                    "label": 0
                },
                {
                    "sent": "So we have this sense of a soft push.",
                    "label": 0
                },
                {
                    "sent": "We don't push on everything with the unit weight.",
                    "label": 0
                },
                {
                    "sent": "This V is being pushed on with .8 weight.",
                    "label": 0
                },
                {
                    "sent": "And we also have a read operation here that just waits each vector by its weight and some some together, and that's a read.",
                    "label": 0
                },
                {
                    "sent": "And the thing about the readers.",
                    "label": 0
                },
                {
                    "sent": "It takes the first unit size, amount of weight.",
                    "label": 0
                },
                {
                    "sent": "Here the vectors less than one, so it takes the whole vector.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later on it will make more sense.",
                    "label": 0
                },
                {
                    "sent": "So the second time step, so this you, this is actually the pop variable, so it just says so.",
                    "label": 0
                },
                {
                    "sent": "Again we have a network that's actually predicting how much we should pop, so that's given the current hidden state.",
                    "label": 0
                },
                {
                    "sent": "Output pop variable and that says we pop off .1.",
                    "label": 0
                },
                {
                    "sent": "So now the ones weights gone down to .7.",
                    "label": 0
                },
                {
                    "sent": "So we popped off .1 V one.",
                    "label": 0
                },
                {
                    "sent": "We then push on .5 of the next vector.",
                    "label": 0
                },
                {
                    "sent": "The vector at time Step 2 so now we have two vectors.",
                    "label": 0
                },
                {
                    "sent": "First the top has .5 weight, the bottom .7 and as I said, we read off a unit amount of weight.",
                    "label": 0
                },
                {
                    "sent": "So we read off .5 * V Two and .5 * V One and anything below that unit we throw away.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on for the next time step.",
                    "label": 0
                },
                {
                    "sent": "So at the next time step we.",
                    "label": 0
                },
                {
                    "sent": "We pop off .9 so now V2 was only .5, so its weight actually goes down to zero, so it's essentially been completely popped off the stack and V1 is reduced accordingly.",
                    "label": 0
                },
                {
                    "sent": "With the leftover wait.",
                    "label": 0
                },
                {
                    "sent": "And then we're also pushing on the next time step with .9.",
                    "label": 0
                },
                {
                    "sent": "And again, we get this.",
                    "label": 0
                },
                {
                    "sent": "That's how we read off.",
                    "label": 0
                },
                {
                    "sent": "As a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "So this is our.",
                    "label": 0
                },
                {
                    "sent": "Sort of continuous neural stack.",
                    "label": 0
                },
                {
                    "sent": "So the thing here is that because we're doing this soft pushing and popping with these weights, everything is differentiable, so we can differentiate this end to end.",
                    "label": 0
                },
                {
                    "sent": "These are not stochastic latent variables as such.",
                    "label": 0
                },
                {
                    "sent": "It's that the forward pass is still completely deterministic, so that gives us that makes inference easier.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do any sort of dynamic programming or sampling over over stochastic variables, we can just run this and back propagate.",
                    "label": 0
                },
                {
                    "sent": "And learn these operations and what we'd like to see is Ken.",
                    "label": 0
                },
                {
                    "sent": "Even though we've defined this in a sort of continuous software, cannot learn to behave like a.",
                    "label": 0
                },
                {
                    "sent": "A traditional symbolic stack.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a schematic diagram of what our stack looks like, and basically this is the way you implemented.",
                    "label": 0
                },
                {
                    "sent": "We just implemented like a torch module and neural stack.",
                    "label": 0
                },
                {
                    "sent": "It has these inputs.",
                    "label": 0
                },
                {
                    "sent": "The previous state in the current input.",
                    "label": 0
                },
                {
                    "sent": "The previous state is is.",
                    "label": 0
                },
                {
                    "sent": "The values are the vectors on the stack and their strengths, so that's our previous stack and our input is how much we're pushing.",
                    "label": 0
                },
                {
                    "sent": "How much were popping and what the next value is to go onto the stack and output, of course is the next state, and the output redvector that gets passed on to whatever the next bit of the network is to predict.",
                    "label": 0
                },
                {
                    "sent": "So this is just becomes a nice modular unit that we can treat like any MLP or an LST, MSIL or anything like that.",
                    "label": 0
                },
                {
                    "sent": "We can implement the torch module and connect it to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two recurrent networks.",
                    "label": 0
                },
                {
                    "sent": "So that's what it looks like if we connected up.",
                    "label": 0
                },
                {
                    "sent": "So we have an R and controller, so that's the controller that's taking in.",
                    "label": 0
                },
                {
                    "sent": "If this was language modeling, it would be taking in the words.",
                    "label": 0
                },
                {
                    "sent": "It would be, it would be computing the different push pop and value operations to feed into the stack.",
                    "label": 0
                },
                {
                    "sent": "It's just taking again the previous date in the input output.",
                    "label": 0
                },
                {
                    "sent": "In the next state in the output where next state is composed of our in state neural stack state and all these things.",
                    "label": 1
                },
                {
                    "sent": "So just like Richard was saying these things, you can just sort of put together like Lego blocks.",
                    "label": 0
                },
                {
                    "sent": "You can also put any other controller in here.",
                    "label": 0
                },
                {
                    "sent": "You could put just an order, new MLP or something like that.",
                    "label": 0
                },
                {
                    "sent": "Not much reason why you would do that.",
                    "label": 0
                },
                {
                    "sent": "Nice LSD M there.",
                    "label": 0
                },
                {
                    "sent": "Is going to do the job well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "At this point we're just we're just applying these models to synthetic transaction problems where we basically designing problems that we think have some relationship to the sorts of tasks we're interested in, and then we try investigate the properties to see whether the models can learn what we want them to learn.",
                    "label": 0
                },
                {
                    "sent": "Simplest things that are sort of very common to do copying.",
                    "label": 0
                },
                {
                    "sent": "So can we take a sequence and just output the same sequence again?",
                    "label": 0
                },
                {
                    "sent": "Now something like the Deep L STM can't do this beyond a certain length.",
                    "label": 0
                },
                {
                    "sent": "At some point the hidden layer saturates and you can't generate very long sequences.",
                    "label": 0
                },
                {
                    "sent": "Reversal is similar but require some sort of generalization.",
                    "label": 0
                },
                {
                    "sent": "Now, if you think about the data structures I talked about copying.",
                    "label": 0
                },
                {
                    "sent": "She is going to suit Q quite well and reversal is going to suit stack quite well 'cause you can just push things onto a stack.",
                    "label": 0
                },
                {
                    "sent": "We can think about more interesting problems like what if we try and flip every neighboring input, so that's what we call bigram flipping.",
                    "label": 0
                },
                {
                    "sent": "Now that sort of thing is obviously what you see locally in some translation problems like French and English.",
                    "label": 0
                },
                {
                    "sent": "You need to be able to do local ordering reversing, so this is kind of model locally reorder things.",
                    "label": 0
                },
                {
                    "sent": "These seem quite simple, but many of the models that people are using for transduction and translation and such things can't do these synthetic problems.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, if we get a bit more fancy, this a bit cryptic, but.",
                    "label": 0
                },
                {
                    "sent": "In translation we see higher level reordering things, so a classic one is subject verb, object, subject, object, verb type reordering.",
                    "label": 0
                },
                {
                    "sent": "Where you get a systematic reordering of constituents in sentences between different languages.",
                    "label": 0
                },
                {
                    "sent": "So we can simulate that again by having we have a simple so these are all done from we generate from an ITG an inversion transduction grammar that we hand write and we generate examples like this and the various symbols like S here means subject.",
                    "label": 0
                },
                {
                    "sent": "I is the input and one is just the index of the like word index.",
                    "label": 0
                },
                {
                    "sent": "So we have lots of different words.",
                    "label": 0
                },
                {
                    "sent": "V is a verb, always an object.",
                    "label": 0
                },
                {
                    "sent": "At some point we throw in some relative pronouns and some clauses just to make things interesting.",
                    "label": 0
                },
                {
                    "sent": "And so we could sort of infinitely embed these.",
                    "label": 0
                },
                {
                    "sent": "So there's quite a lot of hierarchical structure in some of these strings, and they get quite long.",
                    "label": 0
                },
                {
                    "sent": "These are very short ones, but we can generate.",
                    "label": 0
                },
                {
                    "sent": "50 or 100 length strings.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the output looks the same except always for output there.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of local reorderings here, so we have subject, verb, object and subject object.",
                    "label": 0
                },
                {
                    "sent": "With the verb gone, never goes right to the end.",
                    "label": 0
                },
                {
                    "sent": "We also have a sort of.",
                    "label": 0
                },
                {
                    "sent": "This is supposed to be a German gender like example where we have a language which doesn't have any gender, so that sort of being our English and we have particular hidden rules about which of these symbols have which gender, and the output has to use the right.",
                    "label": 0
                },
                {
                    "sent": "Term is in various things to make the gender agree.",
                    "label": 0
                },
                {
                    "sent": "So there's a little simulations of the sorts of things you see in real translation problems just trying to tease apart when we isolate things this much.",
                    "label": 0
                },
                {
                    "sent": "Can these models learn this sort of thing?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot more going on in real translation, so you think if they can't learn this then it's going to be difficult.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just have.",
                    "label": 0
                },
                {
                    "sent": "We have two different ways of it, so we're going to generate lots of these examples and feed them into the network and then look at the accuracy.",
                    "label": 0
                },
                {
                    "sent": "That they produce, and we can either look at whether it gets the whole string right, or we also have a fine grained accuracy, which is sort of how much of the prefix could get right.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the.",
                    "label": 0
                },
                {
                    "sent": "Various models that I've been talking about, so we gotta stack your deck and this is your good old fashioned DLS TM.",
                    "label": 0
                },
                {
                    "sent": "All of these we tuned quite extensively on thousands of machines, so the DL STM we explored sort of all of the depths and hyperparameters and everything else and optimize those.",
                    "label": 0
                },
                {
                    "sent": "So here we have a different task, copy reversal bigram.",
                    "label": 0
                },
                {
                    "sent": "Flip so solved here.",
                    "label": 0
                },
                {
                    "sent": "The green basically means that the model algorithmically learns the problem, and by that I mean that we train on up to a certain length of string, and then if we give it strings that are longer up to an arbitrary length, it can still reproduce.",
                    "label": 0
                },
                {
                    "sent": "So copying we might train on less than 20 length strings, but will it still reproduce the same algorithm if we give it 100 link string?",
                    "label": 0
                },
                {
                    "sent": "What you find with the DLS team so they can learn the training distribution quite well.",
                    "label": 0
                },
                {
                    "sent": "But they don't generalize once, once you increase the length, they haven't actually learned the algorithm, they've just learned to operate within a particular range.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about the more structured models that they do actually learn the algorithms so they learn what it means to copy, and they can just produce that on an infinite length string.",
                    "label": 0
                },
                {
                    "sent": "Wasn't there?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What does converted mean?",
                    "label": 0
                },
                {
                    "sent": "Actually, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "These were added definitions.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by converge so?",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "I think it basically just means that it tops out of a certain value that.",
                    "label": 0
                },
                {
                    "sent": "So, so we ran lots of these, right?",
                    "label": 0
                },
                {
                    "sent": "They're quite sensitive to initialization and lots of them just never get beyond random, so convergence mean it converges to something that's not random, so it's discovered some structure in the problem, and it's able to model it, but it's not close to optimal in anyway.",
                    "label": 0
                },
                {
                    "sent": "So I think it's just anything that I don't know what exactly what its criteria for not random wars, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, most of those things take.",
                    "label": 0
                },
                {
                    "sent": "Often quite awhile to get out of the random regime as well.",
                    "label": 0
                },
                {
                    "sent": "So, and the patterns are sort of predictable, like stacks about it copying, but they're good.",
                    "label": 0
                },
                {
                    "sent": "At reversal queues are good at copying and better traversal that makes sense.",
                    "label": 0
                },
                {
                    "sent": "Having a deck you can do both, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the the motivation here again to go back to the start was that with all of these, are the stack.",
                    "label": 0
                },
                {
                    "sent": "Back in Q we can get essentially constant time updates.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do a big softmax over the whole of the memory 'cause we're just operating on the top or in terms of the reading the top unit.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "So it's differentiable.",
                    "label": 0
                },
                {
                    "sent": "It's not terribly.",
                    "label": 0
                },
                {
                    "sent": "So it's locally differentials, like a value or something like that.",
                    "label": 0
                },
                {
                    "sent": "It has a, uh.",
                    "label": 0
                },
                {
                    "sent": "Phase change at the boundary.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's still.",
                    "label": 0
                },
                {
                    "sent": "It's still differentiable in that sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the convergent graphs and these so this is.",
                    "label": 0
                },
                {
                    "sent": "Maybe gives a better indication of what it means to converge.",
                    "label": 0
                },
                {
                    "sent": "These are basically the models are the best models that did actually do something other than random.",
                    "label": 0
                },
                {
                    "sent": "The colors are all probably harder to see.",
                    "label": 0
                },
                {
                    "sent": "Basically, most of these are your various LS teams of different depths.",
                    "label": 0
                },
                {
                    "sent": "The things that go up like this.",
                    "label": 0
                },
                {
                    "sent": "So this was bigram flipping, and this is the Qi think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the Q, and that's the deck, and you can see this sort of algorithmic behavior.",
                    "label": 0
                },
                {
                    "sent": "They once they cotton on to exactly what the right strategies for pushing and popping, they suddenly just learn the task.",
                    "label": 0
                },
                {
                    "sent": "Which is what we want, and then you can just use them again on arbitrary link sequences, so they've actually learned what the algorithm is rather than representing it in a in a soft sort of RNN way.",
                    "label": 0
                },
                {
                    "sent": "And this was a gender conjugation one as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How am I going?",
                    "label": 0
                },
                {
                    "sent": "So that was.",
                    "label": 0
                },
                {
                    "sent": "That was a sort of brief detour into maybe where we might be going with these transduction problems and machine translation and such things.",
                    "label": 0
                },
                {
                    "sent": "There's lots of relationships between this sort of stuff and attention models and other such models.",
                    "label": 0
                },
                {
                    "sent": "The advantage of something like the stacks and queues is that you're basically doing this linear pass.",
                    "label": 0
                },
                {
                    "sent": "Over the input and then you can start producing output.",
                    "label": 0
                },
                {
                    "sent": "You don't have to go back and do linear passes through the input again.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now for Part 2.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As I said, this part is going to be about reading comprehension, so the idea is like our Android there that the Guardian newspaper kindly drew for us.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to read an article, news article or Wikipedia article or something like that and answer questions about it.",
                    "label": 0
                },
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "It's not crucial.",
                    "label": 0
                },
                {
                    "sent": "Depends what makes.",
                    "label": 0
                },
                {
                    "sent": "Both are going to work just like with the other attention models.",
                    "label": 0
                },
                {
                    "sent": "This idea of hard and soft attention or stochastic variables or continuous ones.",
                    "label": 0
                },
                {
                    "sent": "They both work.",
                    "label": 0
                },
                {
                    "sent": "My philosophy is that if you care about the variables and make them stochastic.",
                    "label": 0
                },
                {
                    "sent": "So if you actually want to read off the attention.",
                    "label": 0
                },
                {
                    "sent": "Then you should have stochastic variables.",
                    "label": 0
                },
                {
                    "sent": "If you don't care if it's just some bit of the process to get you the end result, then I think it's better to make them soft.",
                    "label": 0
                },
                {
                    "sent": "It just makes everything easier because.",
                    "label": 0
                },
                {
                    "sent": "I mean we spent or.",
                    "label": 0
                },
                {
                    "sent": "I've spent years trying to do inference in graphical models and it's hard when there when there's no sort of polynomial algorithm and sampling and all these sorts of things are hard to get to work, especially in these sorts of networks.",
                    "label": 0
                },
                {
                    "sent": "So just by passing that if you don't actually care what the the latent variable is, if it's not part of your output.",
                    "label": 0
                },
                {
                    "sent": "So that's my general perception.",
                    "label": 0
                },
                {
                    "sent": "I think there's some the flip side of that is there's some optimization value in being hard, like when you make hard decisions, things tend to optimize quicker, and these so the various models.",
                    "label": 0
                },
                {
                    "sent": "Definitely as I said, some of them never converge, they just.",
                    "label": 0
                },
                {
                    "sent": "Have random behavior.",
                    "label": 0
                },
                {
                    "sent": "Some of the stacks and queues just behave like iron inside.",
                    "label": 0
                },
                {
                    "sent": "Never actually used the stack or queue, and so there's this sort of initialization problem that so if you think about the way for the stack, if you think about the way pushing was defined, if the initialization is such that the model doesn't push to start with, it can never learn to push.",
                    "label": 0
                },
                {
                    "sent": "So if you just get an initialization, which is if you randomly initialize and there's no wait on pushing, then the model will never use the memory in, it will just behave like an RNN.",
                    "label": 0
                },
                {
                    "sent": "Of course, in when we we know that we can counter it just with initialization.",
                    "label": 0
                },
                {
                    "sent": "If you have stochastic variables, you get a bit more randomness in that exploration that you don't get in the terministic models.",
                    "label": 0
                },
                {
                    "sent": "OK, so reading comprehension, so we want to read articles.",
                    "label": 0
                },
                {
                    "sent": "We're going to be able to answer questions and the key thing is that we need to be high precision because we can't rely on seeing the facts were interested in stated many times.",
                    "label": 0
                },
                {
                    "sent": "So as I said earlier, for lots of these more interesting problems, the biggest barrier to working in the area is the lack of data.",
                    "label": 0
                },
                {
                    "sent": "So if we want to work on something like this, the first thing we have to do is think about where the data going to come from.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to be creative about how we managed to go about doing that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And, at least in my mind, I think that a lot of where the future progress is going to come from, not necessarily.",
                    "label": 0
                },
                {
                    "sent": "Newer and fancier models, but people being more creative and finding better sources of data for these problems.",
                    "label": 0
                },
                {
                    "sent": "So in reading comprehension is a few existing data sources, so a classic one is from Microsoft that's called MC test and these are little stories that are most to be at a sort of primary school level children's primary school level, and the idea is that you have a little discourse like this little story.",
                    "label": 0
                },
                {
                    "sent": "Then you have a Multichoice question and you want to build a model to answer these stories, and so they've intentionally made the language simple to make it easier.",
                    "label": 0
                },
                {
                    "sent": "And divorced from the real world.",
                    "label": 0
                },
                {
                    "sent": "This is not connected to facts like sort of who is the President of United States or any of these world facts.",
                    "label": 0
                },
                {
                    "sent": "You don't need that sort of world knowledge, you just need common sense knowledge.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice test set.",
                    "label": 0
                },
                {
                    "sent": "The problem is there's no real training data.",
                    "label": 0
                },
                {
                    "sent": "There's a few hundred of these documents, but there's no way you can train a model to do this from such a small amount of data.",
                    "label": 0
                },
                {
                    "sent": "So this means that if you're going to use these sorts of things, you're going to be in some sort of unsupervised or rule based regime.",
                    "label": 0
                },
                {
                    "sent": "We want to do something with sort of supervised machine learning, so we want something where we can get a significant amount of data.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another solution is to simulate data to generate it synthetically.",
                    "label": 0
                },
                {
                    "sent": "So Jason Weston at Facebook has been doing this recently, so it's a different sort of angle.",
                    "label": 0
                },
                {
                    "sent": "Hit the idea is that you write down a grammar that generates sentences and you can generate simple simple discourse like this and you can answer a question.",
                    "label": 0
                },
                {
                    "sent": "And so in theory, if you've got the generating grammar, you can generate an infinite number of these things, train on them.",
                    "label": 0
                },
                {
                    "sent": "And test your model and this is nice because you can control the phenomena that you're modeling.",
                    "label": 0
                },
                {
                    "sent": "You can you can target specific sorts of things that you're trying to to capture.",
                    "label": 0
                },
                {
                    "sent": "The downside is that we're just no good at generating interesting natural language.",
                    "label": 0
                },
                {
                    "sent": "If we were, we have.",
                    "label": 0
                },
                {
                    "sent": "We have made a lot more progress on natural language processing, so these things tend to be very simple.",
                    "label": 0
                },
                {
                    "sent": "The grammars are very simple in a lot of these cases are actually finite, so they can't generate infinite amounts of data.",
                    "label": 0
                },
                {
                    "sent": "And it's just a very.",
                    "label": 0
                },
                {
                    "sent": "Biased slice of simple slice of language so.",
                    "label": 0
                },
                {
                    "sent": "From a point of view of building reading comprehension models, you can use this sort of approach to test them, but it's not much good for training them because you don't want to train on this data because you're not really going to learn anything about natural language.",
                    "label": 0
                },
                {
                    "sent": "You're just going to learn something about the generating grammar.",
                    "label": 0
                },
                {
                    "sent": "So again, that doesn't really solve our problem.",
                    "label": 0
                },
                {
                    "sent": "We're still mostly just looking at a testing regime.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we thought about this problem, we came up with the idea or the.",
                    "label": 0
                },
                {
                    "sent": "Or the thought that.",
                    "label": 0
                },
                {
                    "sent": "What we need is paraphrase data, so if we have news stories.",
                    "label": 0
                },
                {
                    "sent": "We want we want articles and we want questions.",
                    "label": 0
                },
                {
                    "sent": "If we have an article from Wikipedia or somewhere like that, we can take a sentence and it's easy to turn that sentence into a question.",
                    "label": 0
                },
                {
                    "sent": "So that's an easy way to get documents and questions.",
                    "label": 0
                },
                {
                    "sent": "The problem is there's not much point in doing that because the questions will be very easy to answer because you'll just get very strong lexical overlap.",
                    "label": 0
                },
                {
                    "sent": "You can just look up in the in the article.",
                    "label": 0
                },
                {
                    "sent": "We use all those words and you'll find what the answer is and the questions won't require any sort of reasoning or inference across sentences or reference resolution or any of these things that are actually the hard parts of of doing reading comprehension.",
                    "label": 0
                },
                {
                    "sent": "So that's too simple, but if we can get something similar then we might be in town.",
                    "label": 0
                },
                {
                    "sent": "So what we need paraphrases of information from articles.",
                    "label": 0
                },
                {
                    "sent": "And people have looked at this quite a bit in in other areas, in particularly summarization so we knew about this corpus that various people had used to build summarization models where from the CNN website.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about CNN is they have on their website these articles.",
                    "label": 0
                },
                {
                    "sent": "This ones are nice article about Chuck Norris's birthday.",
                    "label": 0
                },
                {
                    "sent": "The whole thing is it's blurry there, but the whole thing is a Chuck Norris joke.",
                    "label": 0
                },
                {
                    "sent": "If you know Chuck Norris jokes there.",
                    "label": 0
                },
                {
                    "sent": "So the whole article structure as these.",
                    "label": 0
                },
                {
                    "sent": "The good thing about CNN is, here's the article here and here.",
                    "label": 0
                },
                {
                    "sent": "What are called highlights?",
                    "label": 0
                },
                {
                    "sent": "So their short sentence is which a little summary sentence is about the contents of the article and the nice thing is that they're written by humans are not automatically extracted, so they paraphrase the information in the article.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is take these highlights and turn them into questions.",
                    "label": 0
                },
                {
                    "sent": "These give us good questions because the information that in that little highlight is often spread out in the document.",
                    "label": 0
                },
                {
                    "sent": "It's not just matching one sentence, so that's exactly what we're looking for.",
                    "label": 0
                },
                {
                    "sent": "At some point we realized that the Daily Mail did the same thing, so we also have these Daily Mail articles and the Daily Mail is quite fun to talk.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many of you know the Daily Mail is a English tabloid, but it makes for quite good reading.",
                    "label": 0
                },
                {
                    "sent": "They're also quite fascinated with us, so we get to our models, get to read about themselves in the Daily Mail.",
                    "label": 0
                },
                {
                    "sent": "Or in this case, this article here is about Google interns running Wild in in Mountain View, which you can't see there.",
                    "label": 0
                },
                {
                    "sent": "But if you Scroll down, there's a photo of one of the authors of this of our paper.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the dining miles.",
                    "label": 0
                },
                {
                    "sent": "Lots of fun.",
                    "label": 0
                },
                {
                    "sent": "So that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to collect lots of these articles.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is that they've been doing this for years, so when we saw this, a summarization data sets are quite small.",
                    "label": 0
                },
                {
                    "sent": "They only have a few thousand articles.",
                    "label": 0
                },
                {
                    "sent": "But we basically went to the Google index and we can look, and I think the CNN's been doing this since about 2007 and so we can just take all of those.",
                    "label": 0
                },
                {
                    "sent": "All of those are.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tickles so we end up with things like this.",
                    "label": 0
                },
                {
                    "sent": "So here's a short snippet of a document about Top Gear Presenter top Gears of British TV show about cars, it's presenter, Jeremy Clarkson, is known for being unstable.",
                    "label": 0
                },
                {
                    "sent": "In this case he was punching his producer.",
                    "label": 0
                },
                {
                    "sent": "This is one of the highlights and what we do is we just create questions out of this by looking for entities in the query and removing them.",
                    "label": 0
                },
                {
                    "sent": "So here we've removed the name of the producer.",
                    "label": 0
                },
                {
                    "sent": "So this hasn't.",
                    "label": 0
                },
                {
                    "sent": "It's actually called a close query that's often used in second language learning.",
                    "label": 0
                },
                {
                    "sent": "But we can create these sort of queries and so the model is going to take the document and the query and it has to output the answer.",
                    "label": 0
                },
                {
                    "sent": "So we can get lots of these things from CNN and the Daily Mail.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the problem and would be familiar to anyone that read the Daily Mail.",
                    "label": 0
                },
                {
                    "sent": "That these things are quite predictable.",
                    "label": 0
                },
                {
                    "sent": "So to answer a lot of these questions, all you need is local context.",
                    "label": 0
                },
                {
                    "sent": "So people have done this just as a language modeling task where you just take a sentence, remove one word and get your language model to predict the word.",
                    "label": 0
                },
                {
                    "sent": "You don't necessarily need any world knowledge to do this, so a lot of the queries you can, you can guess the answer to without ever reading the article.",
                    "label": 0
                },
                {
                    "sent": "The Classic One in the Daily Mail is that the Daily Mail is obsessed with things that cause or cure cancer.",
                    "label": 0
                },
                {
                    "sent": "So if you see something like the high tech brother helps you beat breast, you're going to know that the answer is cancer.",
                    "label": 0
                },
                {
                    "sent": "You don't need to read the article and so on for all the rest.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we need to get around this somehow because that would be a pretty boring.",
                    "label": 0
                },
                {
                    "sent": "Task if we could just yes it with an N gram language model.",
                    "label": 0
                },
                {
                    "sent": "So Ngram language model is just going to do very well on these sorts of things.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically solution is to take ideas from other areas in particular.",
                    "label": 0
                },
                {
                    "sent": "Ever popular MNIST data set so?",
                    "label": 0
                },
                {
                    "sent": "One thing that sort of very quickly observed with these sort of simple data sets is that if you have an image of a zero, then you can transform that image and you know within reason that it's still a 0.",
                    "label": 0
                },
                {
                    "sent": "So you can translate it or rotate it within reason, and so that's what people do, right?",
                    "label": 0
                },
                {
                    "sent": "They permute their data set, they create lots of extra training data, and that helps their model learn these things in order sorts of different poses and such and.",
                    "label": 1
                },
                {
                    "sent": "What what this is, is the idea that we have some knowledge about the task.",
                    "label": 1
                },
                {
                    "sent": "We have some rule in our head about invariants in the task, and we're exploiting that to generate more data.",
                    "label": 0
                },
                {
                    "sent": "So we want to do the same thing for our question answering problem.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to do is here again is our article about Jeremy Clarkson on the left.",
                    "label": 0
                },
                {
                    "sent": "As it was, we're going to run an entity recognizer over the article.",
                    "label": 0
                },
                {
                    "sent": "We're going to find all of the entities in the article and the query.",
                    "label": 0
                },
                {
                    "sent": "Are we going to anonymize them so we're going to throw away Jeremy Clarkson and replace him by entity 212?",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is trying to remove that sort of world knowledge aspect, and we're trying to make the task harder for the model in that the way the only way it has to answer the questions is to look at the document.",
                    "label": 0
                },
                {
                    "sent": "So we're making this so that there's no way that can get the right answer without looking at the document.",
                    "label": 0
                },
                {
                    "sent": "And so to do that we also randomize these entities.",
                    "label": 0
                },
                {
                    "sent": "So every time we produce a training instance and answer pair for the model to train on, we re randomize the entities.",
                    "label": 0
                },
                {
                    "sent": "So we know that it doesn't matter what these labels are, as long as the label for the the answer matches the one in the in the in the answer.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can re permute these each time and essentially almost gotten you training example.",
                    "label": 0
                },
                {
                    "sent": "So now the model has just from the query in the just from the query alone, the model has no way of of guessing what the entity is even if it saw this query previously in training 'cause it could be a different a different label.",
                    "label": 0
                },
                {
                    "sent": "It has to look at the document.",
                    "label": 0
                },
                {
                    "sent": "It has to look at the context of the entities.",
                    "label": 0
                },
                {
                    "sent": "And do some sort of process to solve the right answer.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea that we're trying to force the model to read the document and make the task that bit harder so that we can actually get this problem of reading rather than world knowledge.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if you wanted to deploy something like, this is a question answering system.",
                    "label": 0
                },
                {
                    "sent": "You don't care about how your model is answering the question, you just wanted to get it right.",
                    "label": 0
                },
                {
                    "sent": "So you would use any source of information you had, including world knowledge.",
                    "label": 0
                },
                {
                    "sent": "But our point of view is just to try and focus on this and see how see how we can go.",
                    "label": 0
                },
                {
                    "sent": "And this produces a reasonably hard.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And answering tasks so.",
                    "label": 0
                },
                {
                    "sent": "That said, even.",
                    "label": 0
                },
                {
                    "sent": "Once we've anonymized these entities, these articles are pretty hard to read for a person, but you can still almost always answer the question.",
                    "label": 0
                },
                {
                    "sent": "So it's it's a bit harder to read, but a human will still be able to workout without knowing what the entity refers to just by looking at the discourse context we understand the meaning of the language, and we can workout what their identity is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Still a well defined problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we collected this data.",
                    "label": 0
                },
                {
                    "sent": "As I said, we went to the Google index and just took as much as we could back.",
                    "label": 0
                },
                {
                    "sent": "However, long Daily Mail and CNN have been doing this for CNN.",
                    "label": 1
                },
                {
                    "sent": "Our training data went back about 95 months.",
                    "label": 0
                },
                {
                    "sent": "Daily Mail 56.",
                    "label": 0
                },
                {
                    "sent": "The distributions have different properties like the Daily Mail tends to have more highlights per document.",
                    "label": 0
                },
                {
                    "sent": "Longer articles so you can see the total number of queries we've got about 400,000 CNN, about 800,000 Daily Mail so.",
                    "label": 0
                },
                {
                    "sent": "This is not huge.",
                    "label": 0
                },
                {
                    "sent": "I would like more, but now we've got a significant number of training examples for reading comprehension time, so this is a lot better than having a few 100 now.",
                    "label": 0
                },
                {
                    "sent": "We can actually think about building supervised models and training them.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is raw data.",
                    "label": 0
                },
                {
                    "sent": "It's not easy to do so, so for obvious reasons we don't own the Copyright on the Daily Mail or the CNN, so we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't distribute the article text.",
                    "label": 0
                },
                {
                    "sent": "Our solution is to distribute a script that will download articles and annotate them with the to reproduce what we're doing, we're going to need the articles and the entity annotations.",
                    "label": 0
                },
                {
                    "sent": "The entity annotations were produced by a internal Google system, so you can't get that either.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is produce a script that will download them and annotate the entities.",
                    "label": 0
                },
                {
                    "sent": "That's what we thought we'd do.",
                    "label": 0
                },
                {
                    "sent": "It turned out to be more difficult, becausw.",
                    "label": 0
                },
                {
                    "sent": "Firstly, I think Daily Mail in particular blocked our IP address pretty quickly when we started scraping.",
                    "label": 0
                },
                {
                    "sent": "Those articles and that would obviously happen that to anyone that tried to reproduce it.",
                    "label": 0
                },
                {
                    "sent": "Secondly, they keep changing their article templates and the annotations are.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you change a template and the structure of the article changes at all the entity annotations going to be off and our tokenization and so it's all going to mess it up.",
                    "label": 0
                },
                {
                    "sent": "Anyway, we've solved that now and basically what our current solution is to.",
                    "label": 0
                },
                {
                    "sent": "We have a script that will download them from the Internet Archive from a snapshot.",
                    "label": 0
                },
                {
                    "sent": "With just trying to finalize that, so not all of the articles we got are in there.",
                    "label": 0
                },
                {
                    "sent": "There's actually some articles we didn't get, so the data sets are a bit off and we just re running all our models.",
                    "label": 0
                },
                {
                    "sent": "On our new data set, but hopefully that should all be available.",
                    "label": 0
                },
                {
                    "sent": "Soon.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about the data set.",
                    "label": 0
                },
                {
                    "sent": "So we've got.",
                    "label": 0
                },
                {
                    "sent": "We've got more than a million training points in total, and that's before we start permuting them, which essentially produces an exponential number.",
                    "label": 0
                },
                {
                    "sent": "These are not, so all the examples I give a quite short just so I can fit the model slide, but the articles aren't short in general, so the average number of tokens we've got about 780 or 1000 words in CNN and the Daily Mail, so these are not 30 word articles.",
                    "label": 0
                },
                {
                    "sent": "Some of the articles are the maximum number of entities, some of them have sort of hundreds of entities.",
                    "label": 0
                },
                {
                    "sent": "The average number is about 30 or 40.",
                    "label": 0
                },
                {
                    "sent": "And that's roughly the set that you have to choose from to answer the question so it's a non trivial.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the vocabs are relatively large as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I mean the other nice thing about this is it's evolving so we can keep grabbing more months.",
                    "label": 0
                },
                {
                    "sent": "So this was basically there so you can see that up until the end of April, which was about when we were doing the testing.",
                    "label": 1
                },
                {
                    "sent": "But each month, particularly Daily Mail is.",
                    "label": 0
                },
                {
                    "sent": "The number of articles and highlights are producing is not linear across time.",
                    "label": 0
                },
                {
                    "sent": "the Daily Mail seems to be increasing so that data sets growing.",
                    "label": 0
                },
                {
                    "sent": "Re",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lative Lee quickly.",
                    "label": 0
                },
                {
                    "sent": "OK, so one thing we did on this first question obviously is.",
                    "label": 0
                },
                {
                    "sent": "Is this a well defined set of question answering task and how hard is it so we did a sort of informal survey of our data looking at trying to categorize a different different questions on what sort of processing you would need to do to answer this question and whether you could answer it at all as a human.",
                    "label": 0
                },
                {
                    "sent": "So we came up with these different categories, so a simple question is one that you can answer.",
                    "label": 0
                },
                {
                    "sent": "Just basically the question is stated word for word in the article.",
                    "label": 0
                },
                {
                    "sent": "So all you have to do is match like a template and you get the right answer and so roughly 12% were that simple.",
                    "label": 0
                },
                {
                    "sent": "And here we have the sentences so and so a lexical is 1 where the question is essentially stated in the article, but there's been some lexical lexical change so.",
                    "label": 0
                },
                {
                    "sent": "The question might say something about someone being murdered and the article might say something about someone being shot.",
                    "label": 0
                },
                {
                    "sent": "So to get the right answer you just need to know that lexical relationship between shot and murdered, but otherwise the rough structure of the query and the way it's the way it's stated at the same, so roughly 40% of those you can get and we know that sort of things like word embeddings and these sort of things are pretty good at getting upset lexical generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, so coreference so that code references where you've got some entities which are referring to the same underlying entity, possibly in different ways, and that includes anaphora and things like this so.",
                    "label": 0
                },
                {
                    "sent": "John went to the park.",
                    "label": 0
                },
                {
                    "sent": "There he played baseball so that he is a reference to Jaune.",
                    "label": 0
                },
                {
                    "sent": "So some questions you're going to need to be able to resolve an effort like that.",
                    "label": 0
                },
                {
                    "sent": "The answer to the question may be stated in terms of a pronoun, which is the actual correct entity, but you have to know which entity that pronouns referring to to get the right entity.",
                    "label": 0
                },
                {
                    "sent": "So roughly 10% of our answers needed coref resolution, but simple coref here means that the.",
                    "label": 0
                },
                {
                    "sent": "The statements basically word for word for the question, but there's some reference resolution you need coreference and lexical means that you need to do both reference resolution, an lexical generalization to get the right answer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this was just a sample.",
                    "label": 0
                },
                {
                    "sent": "So yes, I mean in theory you could have curve, but we just have from this sample.",
                    "label": 0
                },
                {
                    "sent": "We didn't have those.",
                    "label": 0
                },
                {
                    "sent": "But you're mostly the car ever going to be across multiple?",
                    "label": 0
                },
                {
                    "sent": "Send this is two or three sentences, so we're up to psycho.",
                    "label": 0
                },
                {
                    "sent": "Reference in lexical is just a combination of those where you've got to both resolve references and do some sort of lexical generalization.",
                    "label": 0
                },
                {
                    "sent": "And then we have this category which we're calling complex, which basically means you need to do inference.",
                    "label": 0
                },
                {
                    "sent": "So as a human you need to read this, you need to think about it in some way and put together the connections and then you can answer the question.",
                    "label": 0
                },
                {
                    "sent": "But the answer is not not sort of stated for you.",
                    "label": 0
                },
                {
                    "sent": "So that's where?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's the level that you're doing inference, and then roughly 10% where what we said is unanswerable.",
                    "label": 0
                },
                {
                    "sent": "That is, a human would not be able to answer this question.",
                    "label": 0
                },
                {
                    "sent": "And that's because the anonymization can lose information that you need.",
                    "label": 0
                },
                {
                    "sent": "So the answer may be dependent on exactly.",
                    "label": 0
                },
                {
                    "sent": "Who the entity is and your knowledge of that, and without knowing who the entity is, you won't be able to answer the question, but that's a relatively small amount.",
                    "label": 0
                },
                {
                    "sent": "Only 10% of these questions actually were unanswerable after this, so still 90% of the questions are answerable just from the anonymized data.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing here that's interesting for later on is if you take out the complex.",
                    "label": 0
                },
                {
                    "sent": "Basically 60% of the questions you should be able to answer.",
                    "label": 0
                },
                {
                    "sent": "If you can do Co reference in lexical generalization and if you want to get anymore than that, you probably going to be doing some sort of inference or getting lucky.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So now we have.",
                    "label": 0
                },
                {
                    "sent": "We have some data.",
                    "label": 0
                },
                {
                    "sent": "The next thing we need is some baselines and benchmarks to know.",
                    "label": 0
                },
                {
                    "sent": "How hard this problem is, and whether there are simple solutions.",
                    "label": 0
                },
                {
                    "sent": "So we tried a few different things, so the obvious baseline to start with is just a frequency baseline.",
                    "label": 0
                },
                {
                    "sent": "So what if you just take the most frequent entity mentioned in the article and always answer that as your answer, so that will give you sort of CNN.",
                    "label": 0
                },
                {
                    "sent": "We've got sort of 2822 for the Daily Mail percent accuracy, and that's pretty natural.",
                    "label": 0
                },
                {
                    "sent": "You imagine these highlights are going to talk about frequent entities, so there's going to be a bit of advice there, but still, that's a long way short of solving the problem.",
                    "label": 0
                },
                {
                    "sent": "We can refine that a little bit by saying that if an entity is mentioned in the query, then exclude it from the frequency, 'cause it's unlikely that the the answer the question will be mentioned in the query other than where it's actually answered.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a bit better again.",
                    "label": 0
                },
                {
                    "sent": "So then we're about 30%.",
                    "label": 0
                },
                {
                    "sent": "So 30% is basically our baseline.",
                    "label": 0
                },
                {
                    "sent": "If we can't do better than 30%, then we're not doing anything.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, we we sort of go to the traditional NLP bag of tricks.",
                    "label": 0
                },
                {
                    "sent": "And at Google we have we have a big NLP pipeline and traditional pipeline of parsing, tagging and semantic annotation.",
                    "label": 0
                },
                {
                    "sent": "And so we can go to this.",
                    "label": 0
                },
                {
                    "sent": "We can feed in any document and it will spit out all of these annotations for us.",
                    "label": 0
                },
                {
                    "sent": "So we looked at these and tried to.",
                    "label": 0
                },
                {
                    "sent": "We've tried various combinations of these to try and come up with sort of a heuristic rule based baseline to see if if it was easy to get these answers just from basically relations that we could extract quite quite comfortably.",
                    "label": 0
                },
                {
                    "sent": "So we ran as a semantic role labeling system.",
                    "label": 0
                },
                {
                    "sent": "And then one of the engineers spent loads of time trying different sequences of heuristics, and this is when he came up with the best.",
                    "label": 0
                },
                {
                    "sent": "So we had this so that the semantic role labeling is giving these triples of relations and we basically back off from one of these to the other.",
                    "label": 0
                },
                {
                    "sent": "So if we see an exact match like this then we can answer a question.",
                    "label": 0
                },
                {
                    "sent": "We got exact matches to be if the frame is correct.",
                    "label": 0
                },
                {
                    "sent": "But the arguments don't necessarily match.",
                    "label": 0
                },
                {
                    "sent": "We can still guess that as an answer, and so on.",
                    "label": 0
                },
                {
                    "sent": "The frame might be permitted, but if it's the same by that point, we might as well have a go at guessing that.",
                    "label": 0
                },
                {
                    "sent": "And then if it's just the same entity again, even if the frame relation is different, it's worthwhile guessing that is an answer, so we're taking this long sort of engineered statistical pipeline through our roles and semantic roles, and then trying to use those in a heuristic way.",
                    "label": 0
                },
                {
                    "sent": "This is far from optimal.",
                    "label": 0
                },
                {
                    "sent": "You could do a lot more with this, and obviously you probably apply some machine learning to learn this better, but it gives us a starting point that just would this be with the information.",
                    "label": 0
                },
                {
                    "sent": "Just be easily accessible given a traditional NLP pipeline and the answer is no.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this doesn't really do much better than the baseline, and as I said, an engineer spent a lot of time trying to all different combinations of this.",
                    "label": 0
                },
                {
                    "sent": "It could never really do much better than this.",
                    "label": 0
                },
                {
                    "sent": "And the problem is that in the traditional pipeline, you just don't really have access to the right information to answer these questions.",
                    "label": 0
                },
                {
                    "sent": "So that sort of semantic role labeling systems at just not capturing the relations you want to answer the questions.",
                    "label": 0
                },
                {
                    "sent": "So, so you have the poor coverage of relations.",
                    "label": 0
                },
                {
                    "sent": "Also, some of these require multiple relations too.",
                    "label": 0
                },
                {
                    "sent": "Answer Again we could.",
                    "label": 0
                },
                {
                    "sent": "We tried to get that with you risztics but couldn't really do so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "That the sort of semantic annotation system just fails on, so Tom Hanks is friends with ex's Manager Scooter Braun.",
                    "label": 0
                },
                {
                    "sent": "So that's the query we want to know what X is.",
                    "label": 0
                },
                {
                    "sent": "This is what the document says.",
                    "label": 0
                },
                {
                    "sent": "So the the.",
                    "label": 0
                },
                {
                    "sent": "The semantic web just completely fails to pick up those relationships.",
                    "label": 0
                },
                {
                    "sent": "It's not a nicely cleanly structured sentence, and again, this comes down to that sort of precision issue.",
                    "label": 0
                },
                {
                    "sent": "The annotation models are they just not that high precision for capturing relations.",
                    "label": 0
                },
                {
                    "sent": "So if you're running them on huge amounts of data then you can wear the relations are stated in multiple different ways.",
                    "label": 0
                },
                {
                    "sent": "You'll generally get get it eventually, but if you have one shot at picking out the right information, it's unlikely.",
                    "label": 0
                },
                {
                    "sent": "OK, but if you look at this you can see that there's a lot of over lexical overlap between the question and the answer so.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe something far simpler is going to work, and that's what we call the word distance benchmark.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we just basically took.",
                    "label": 0
                },
                {
                    "sent": "For the entity in the query, we took a window of words around that and then we looked at every entity in the document and we looked at the words around that we matched up, try to match up these two windows and align the words mentioned and the entity that has basically shared the most words in common with the query.",
                    "label": 0
                },
                {
                    "sent": "We returned as the answer.",
                    "label": 0
                },
                {
                    "sent": "Modulo a few more heuristics to make this a bit better.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that works pretty well.",
                    "label": 0
                },
                {
                    "sent": "So now we start to get a lot better than our baseline.",
                    "label": 0
                },
                {
                    "sent": "So now we're up in the mid 40s or even the mid 50s for the Daily Mail.",
                    "label": 0
                },
                {
                    "sent": "So that word distance benchmarks actually pretty robust, so it's good at capturing lots of those.",
                    "label": 0
                },
                {
                    "sent": "Lots of the sort of exact match ones.",
                    "label": 0
                },
                {
                    "sent": "It's not bad at the lexical generalization where it just ignores it, and if there's at least one word in common, it sort of can get lucky.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was some.",
                    "label": 0
                },
                {
                    "sent": "Sort of non machine learning type approaches.",
                    "label": 0
                },
                {
                    "sent": "Obviously we collected lots of data because we wanted to apply machine learning supervised machine learning to this problem.",
                    "label": 0
                },
                {
                    "sent": "So that's what we're going to do now.",
                    "label": 0
                },
                {
                    "sent": "So our framework is basically this we have.",
                    "label": 0
                },
                {
                    "sent": "We have a document D and a query Q and we want to produce and answer A.",
                    "label": 0
                },
                {
                    "sent": "We're going to do that by jointly embedding the document in the query and then doing a softmax with the answers.",
                    "label": 0
                },
                {
                    "sent": "The answers are going to be anything that occurs in the document, so in these models were not going to.",
                    "label": 0
                },
                {
                    "sent": "We're not going to tell the model at the answer is an entity.",
                    "label": 0
                },
                {
                    "sent": "It could be any token in the input for all of our baselines and benchmarks.",
                    "label": 0
                },
                {
                    "sent": "We just structure that 'cause we knew exactly what the constraints were on the answers, but we want to make this more general, so we're just saying here all the words in the document pick which one is the answer.",
                    "label": 0
                },
                {
                    "sent": "So we're going to look at different ways of doing G, basically so that how do we.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The documents in the queries.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's our DLS, DM.",
                    "label": 0
                },
                {
                    "sent": "Again, if you were missing it from earlier on.",
                    "label": 0
                },
                {
                    "sent": "So this is basically our first idea is.",
                    "label": 0
                },
                {
                    "sent": "Let's go so lots of people are using DLS teams to any sort of encoder decoder framework to do transduction problems.",
                    "label": 0
                },
                {
                    "sent": "Let's just treat this as a big encoder decoder problem where we're going to feed the document and the query all as one big string into a deep SDM, and then read out the answer.",
                    "label": 0
                },
                {
                    "sent": "It sounds sort of hopeful.",
                    "label": 0
                },
                {
                    "sent": "As I said, our documents are up to about 2000 time steps, so this is getting a bit past the sort of ordinary sentence links dependencies.",
                    "label": 0
                },
                {
                    "sent": "But that's our starting point, so here's our big deep LS TM.",
                    "label": 0
                },
                {
                    "sent": "We're going to run this.",
                    "label": 0
                },
                {
                    "sent": "Our G function is simply going to be the final state from the STM, so this just says we run we.",
                    "label": 0
                },
                {
                    "sent": "Create one big long string from our document.",
                    "label": 0
                },
                {
                    "sent": "Now query.",
                    "label": 0
                },
                {
                    "sent": "We run the Ellis team up until the last date we take that last date and that sell our embedding for our document query and we try and pick which entity.",
                    "label": 0
                },
                {
                    "sent": "So remember that.",
                    "label": 0
                },
                {
                    "sent": "The entities are being permuted so the entities that model is going to learn embeddings as such for the entities, but they're going to be meeting entities themselves have no meaning, so this is the substitution problem.",
                    "label": 0
                },
                {
                    "sent": "The model needs to be able to detect a symbol in the input and substitute it to the output so it's a bit more difficult than than the other transaction.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Blooms.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the diagrammatic representation where we're saying we got this deep recurrent network.",
                    "label": 0
                },
                {
                    "sent": "We're just feeding everything in, so this is our document.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a trivial document, but this is our query at the end we get.",
                    "label": 0
                },
                {
                    "sent": "Gee, there's two versions of this.",
                    "label": 0
                },
                {
                    "sent": "Obviously we could go document.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, then query or we could go query 1st and then document.",
                    "label": 0
                },
                {
                    "sent": "If you think about if we if we go document 1st and then query.",
                    "label": 0
                },
                {
                    "sent": "The model doesn't know what the query is going to be, so it sort of has to put all of the information about the possible answer into the hidden layer at the end of the document.",
                    "label": 0
                },
                {
                    "sent": "If we go query first, then you can sort of think of this like a filtering problem where the model first gets to know what it's looking for and then it goes through the document trying to find that.",
                    "label": 0
                },
                {
                    "sent": "So this should be a bit easier and it turned out that this would work a bit better the other way.",
                    "label": 0
                },
                {
                    "sent": "It did learn something, but it didn't learn as much as as this.",
                    "label": 0
                },
                {
                    "sent": "But this is still an extremely hard problem for this sort of model, 'cause I said we're talking about on average for Daily Mail 1000 time steps, so we're not telling the model anything about the structure.",
                    "label": 0
                },
                {
                    "sent": "We're not telling it that there's a query in a document it doesn't know that there's just symbols were not telling it that there's a symbol that it has to substitute for.",
                    "label": 0
                },
                {
                    "sent": "It just has to learn that.",
                    "label": 0
                },
                {
                    "sent": "So there's a really long range dependency from some X variable up here to the answer way at the end.",
                    "label": 0
                },
                {
                    "sent": "1000 time steps later, so it's a pretty hard problem.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a model like that to learn so it does it a bit better than the word distance model.",
                    "label": 0
                },
                {
                    "sent": "So it's not much better, but as I said, this is structured in the way that we structured I'm I was quite impressed at this model and managed to learn anything over such a long.",
                    "label": 0
                },
                {
                    "sent": "With that long dependencies.",
                    "label": 0
                },
                {
                    "sent": "So these models are surprisingly.",
                    "label": 0
                },
                {
                    "sent": "Adaptable.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we know we can do better than that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Following on from the themes of earlier, the next step is to introduce an attention.",
                    "label": 0
                },
                {
                    "sent": "Mechanism 'cause that's going to get around this bottleneck of the final vector.",
                    "label": 0
                },
                {
                    "sent": "So now what we're going to do is we're going to embed the document.",
                    "label": 0
                },
                {
                    "sent": "We're going to embed the query and we're going to let the model attend over the tokens in the document and summarize that and then produce an answer so.",
                    "label": 0
                },
                {
                    "sent": "The mass of this is basically that we run and we're going to use bidirectional Ellis James to do all of our embedding, so we run bidirectional STM.",
                    "label": 0
                },
                {
                    "sent": "They're going to produce two embeddings for the query, and we're going to concatenate those together into.",
                    "label": 0
                },
                {
                    "sent": "You were going to run bidirectional STM's over the document, and for every time step T we're going to create an embedding for that time step.",
                    "label": 0
                },
                {
                    "sent": "This is very much like the translation model I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "Then when it comes to answer the question, we're going to create, we're going to calculate these attention variables M and they're just a function of a given time step, and the query embedding we're going to apply a softmax, and then we're going to mix all of those together into one representation for the whole document are.",
                    "label": 0
                },
                {
                    "sent": "So as I said, roughly the same as what's going on in the empty model, and then our overall embedding is just going to be an LP of those quantities that we've calculated, and so that's what we call the attentive reader.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's my diagrammatic representation of that, so here we have our document.",
                    "label": 0
                },
                {
                    "sent": "We have bidirectional STM's we concatenating timesteps together.",
                    "label": 0
                },
                {
                    "sent": "We these are our attention variables.",
                    "label": 0
                },
                {
                    "sent": "We mix all that together into.",
                    "label": 0
                },
                {
                    "sent": "We do our bidirectional embedding of the query.",
                    "label": 0
                },
                {
                    "sent": "We get you.",
                    "label": 0
                },
                {
                    "sent": "We put these together into G. So now what we want is the query to pick out the bits of the document that are going to help it answer the question.",
                    "label": 0
                },
                {
                    "sent": "So this gets around our bottleneck problem.",
                    "label": 0
                },
                {
                    "sent": "It's still a hard problem because it still has to do this substitution.",
                    "label": 0
                },
                {
                    "sent": "It still has to somehow, so it's going to look at the entities that might answer the question in the document.",
                    "label": 0
                },
                {
                    "sent": "But the entities themselves have no information.",
                    "label": 0
                },
                {
                    "sent": "It has to look at the context around them to decide whether it should extract them and substitute them with their questionnaire.",
                    "label": 0
                },
                {
                    "sent": "The attention doesn't know about the question, sorry it.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so this diagram doesn't doesn't include sort of how these are calculated, so these are calculated dependent on these.",
                    "label": 0
                },
                {
                    "sent": "This is just showing what happens after we have the attention variables.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So back here, this is Mr.",
                    "label": 0
                },
                {
                    "sent": "The unnormalized attention variables.",
                    "label": 0
                },
                {
                    "sent": "So there are functions are just an MLP function of you, which is our query embedding and the particular time step.",
                    "label": 1
                },
                {
                    "sent": "So we compute one of these for every token in the document.",
                    "label": 1
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that starts to start to work quite well.",
                    "label": 0
                },
                {
                    "sent": "So now we're up into the sort of 60% region for this task.",
                    "label": 0
                },
                {
                    "sent": "And we also have a benchmark here, which we call the uniform attention because just to compare and say, well, does attention actually do anything or are we just mixing together everything like a bag of words and that doesn't work at all, so the attention mechanism is definitely.",
                    "label": 0
                },
                {
                    "sent": "Crucial to that that performance so uniform attention just means set all those attention variables to zero and so every time step gets mixed in the same amount.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Roughly, according to our expectation, that starts to work quite well.",
                    "label": 0
                },
                {
                    "sent": "So the nice.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about the well before I get there.",
                    "label": 0
                },
                {
                    "sent": "OK so if we.",
                    "label": 0
                },
                {
                    "sent": "So some more implementation level.",
                    "label": 0
                },
                {
                    "sent": "Details.",
                    "label": 0
                },
                {
                    "sent": "So these are quite big models, so we're now looking at training LS teams where thousands of time steps in mini batches.",
                    "label": 0
                },
                {
                    "sent": "We're doing this on GPU's, I think on average we used about 25 GPS.",
                    "label": 0
                },
                {
                    "sent": "Thomas can correct me if I'm wrong on any of these details as he did at all.",
                    "label": 0
                },
                {
                    "sent": "This is quite big models.",
                    "label": 0
                },
                {
                    "sent": "We're running for this is the attentive reader.",
                    "label": 1
                },
                {
                    "sent": "This hours on the bottom we're doing this with.",
                    "label": 0
                },
                {
                    "sent": "Asynchronous.",
                    "label": 0
                },
                {
                    "sent": "Gradient descent, the interesting thing about the behavior of these models for anyone that tries to reproduce this, is that for at least the 1st five hours, they do nothing, so they're basically random.",
                    "label": 0
                },
                {
                    "sent": "So if you just sort of.",
                    "label": 0
                },
                {
                    "sent": "Start your model running.",
                    "label": 0
                },
                {
                    "sent": "Go and have lunch and come back and say oh it's running random.",
                    "label": 0
                },
                {
                    "sent": "I must have.",
                    "label": 0
                },
                {
                    "sent": "I must have a bug.",
                    "label": 0
                },
                {
                    "sent": "You might have to wait a bit longer so it takes quite a long time for the Model 2.",
                    "label": 0
                },
                {
                    "sent": "To get any sort of gradient on the document so it can work out what the direction is to go in.",
                    "label": 0
                },
                {
                    "sent": "And that's because the problem is is quite hard, because the first thing the model tries to do is learn embeddings for the the entity labels that will help in Atlanta the question, but the entities are random, so there's no information in them, so it puts all this grading on those and it just oscillates around random for a long time, and eventually enough gradient goes onto the context around the entities for it to start understanding that actually it's not the entities that matter, it's the context around them.",
                    "label": 0
                },
                {
                    "sent": "It's how how they're realizing the text.",
                    "label": 0
                },
                {
                    "sent": "So we start.",
                    "label": 0
                },
                {
                    "sent": "We see this sort of convergence behavior so.",
                    "label": 0
                },
                {
                    "sent": "And this sort of thing is true for a lot of the models people are playing with now, so these memory networks and neural Turing machines and such.",
                    "label": 0
                },
                {
                    "sent": "They don't behave like your old fashioned neural models of just a nice increase.",
                    "label": 0
                },
                {
                    "sent": "They have lots of discontinuity's in the objective function.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So there's two sides to that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we could come up with initialization, but at the same time we're interested in not just sort of cracking the task, but seeing what the model can learn, so we could sort of initialize to to emphasize the context over the entities or things like that, but in some sense we want to see if the model can learn that.",
                    "label": 0
                },
                {
                    "sent": "Well, I think the they behave differently enough to the other models like the NTM is worse, so you see this behavior where it does nothing for awhile.",
                    "label": 0
                },
                {
                    "sent": "There's a big jump does nothing for awhile.",
                    "label": 0
                },
                {
                    "sent": "Another big jump, so it it's much harder to monitor and see how this is going and it's hard to workout why those things happen.",
                    "label": 0
                },
                {
                    "sent": "The very simple task like the stack tasks you can tie it to just whether it's like in something like reversal with a stack, it just has to learn the right push and pop weights.",
                    "label": 0
                },
                {
                    "sent": "That's quite 1 dimensional, but this is a lot more.",
                    "label": 0
                },
                {
                    "sent": "Complex, but yeah, I think.",
                    "label": 0
                },
                {
                    "sent": "It's it's an interesting initially.",
                    "label": 0
                },
                {
                    "sent": "I mean, the problem here is just the classic problem that the gradient on the thing you don't care about is big, and the grading on the thing in the context is really tiny and it takes a long time for the model to.",
                    "label": 0
                },
                {
                    "sent": "For that grade, integrada featherweights that grow up enough that they start to impact the learning.",
                    "label": 0
                },
                {
                    "sent": "No, I mean, you could easily come up with all sorts of curriculum approaches and things like that.",
                    "label": 0
                },
                {
                    "sent": "And at the moment we are very much in the phase of.",
                    "label": 0
                },
                {
                    "sent": "Creating the data set and trying out different things on this task to see firstly whether it's an interesting problem, how these different models perform, whether whether this is easy to do with traditional models or whether the newer models can't do this at all.",
                    "label": 0
                },
                {
                    "sent": "As I said, we've got very long range dependencies, so it wasn't clear whether the recurrent networks would do anything with this, so we're not at the point where we're trying to build a specific model to crack this task.",
                    "label": 0
                },
                {
                    "sent": "We are more interested in.",
                    "label": 0
                },
                {
                    "sent": "Is a reasonably generic sort of attention architecture able to to get anywhere with this problem?",
                    "label": 0
                },
                {
                    "sent": "Yeah you don't.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's it.",
                    "label": 0
                },
                {
                    "sent": "That's the problem with the optimization, and that's always a problem with optimization.",
                    "label": 0
                },
                {
                    "sent": "But it gets worse with these models.",
                    "label": 0
                },
                {
                    "sent": "If you're doing a synthetic task, then you you know when it solves the problem.",
                    "label": 0
                },
                {
                    "sent": "But if it's a real task, and yes, that's a real problem.",
                    "label": 0
                },
                {
                    "sent": "And sometimes the models won't learn anything you'll.",
                    "label": 0
                },
                {
                    "sent": "Not these ones always seem to learn eventually this, but that five hours can be can vary quite a lot.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's there a bit more difficult to to work with.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the nice thing about attention models you can visualize what they're doing, so here's a short article.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's readable.",
                    "label": 0
                },
                {
                    "sent": "Which one was this?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a queries about a deceased sailor.",
                    "label": 0
                },
                {
                    "sent": "Who was identified by someone?",
                    "label": 0
                },
                {
                    "sent": "And we want to know who they were.",
                    "label": 0
                },
                {
                    "sent": "So some entity in here is that the name of the sailor.",
                    "label": 0
                },
                {
                    "sent": "So we've got to see that.",
                    "label": 0
                },
                {
                    "sent": "So here we are saying that someone was killed in a parachute accident.",
                    "label": 0
                },
                {
                    "sent": "Here we have a pronouncer.",
                    "label": 0
                },
                {
                    "sent": "He was identified as into the 49.",
                    "label": 0
                },
                {
                    "sent": "That's the right answer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the models have mostly getting the attention around there in a bit.",
                    "label": 0
                },
                {
                    "sent": "Spread out around here.",
                    "label": 0
                },
                {
                    "sent": "You can see that to sort of every to get a reason to answer this question, you need to resolve this anaphora.",
                    "label": 0
                },
                {
                    "sent": "But you can also see that you can probably guess the answer abit from local context, but the connect the person that was killed is this person you need to go through that.",
                    "label": 0
                },
                {
                    "sent": "So so the the annoying thing about these visualizations is always visualizing the attention variable underneath the attention.",
                    "label": 0
                },
                {
                    "sent": "Variables are bidirectional, STM that's spreading information around and our hope is that that is what gives us.",
                    "label": 0
                },
                {
                    "sent": "Some managers connect up into these a little bit.",
                    "label": 0
                },
                {
                    "sent": "We know that that LS teams are pretty good at doing coreference resolution, so we're hopeful that they might do something like that.",
                    "label": 0
                },
                {
                    "sent": "But it's hard to tell what we need to do is visualize what's going on down there, so.",
                    "label": 0
                },
                {
                    "sent": "This is just telling you, sort of where the attention is entering the sequence, but it's not telling us where the information is spreading out over the sequence.",
                    "label": 0
                },
                {
                    "sent": "So if we can visualize the LS teams, hopefully we might see that there was some some weight going here and some weight back here on killed and things like that.",
                    "label": 0
                },
                {
                    "sent": "If we were really lucky, or it might just be some random words around here that helped it.",
                    "label": 0
                },
                {
                    "sent": "Guess the answer.",
                    "label": 0
                },
                {
                    "sent": "But yes, that we're working on better.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Realizations.",
                    "label": 0
                },
                {
                    "sent": "How are going OK so actually this one is pretty boring.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's better to look at one that got wrong, so this is an example where it got wrong the answer.",
                    "label": 0
                },
                {
                    "sent": "So the question is about a passenger train derails 35 kilometers East of Entity 11 in North and X and the correct answer is 37 up here and the model goes for 85.",
                    "label": 0
                },
                {
                    "sent": "The modeler tends to basically the two locations, so both of these are locations, so in some sense you can sort of see how it's gone wrong.",
                    "label": 0
                },
                {
                    "sent": "It's it's unsure which of the two locations mentioned are the right answer and it's picked the wrong one.",
                    "label": 0
                },
                {
                    "sent": "Now it's looking at every token, but the model does learn that the answer is going to be one of these entity sets, so we're not telling it that, but it learns it.",
                    "label": 0
                },
                {
                    "sent": "So all the model sees as integers because these are all every every unique token has an integer as just one set, so there's nothing a priority that marks out the entities, but the model smart enough to know that it keeps seeing into these answers so.",
                    "label": 0
                },
                {
                    "sent": "They're probably good things to look at.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's another one.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of question where it's actually you can't necessarily answer it, so the question is asking about a car detonated.",
                    "label": 0
                },
                {
                    "sent": "Near a police vehicle in X and the locations mentioned at multiple granularities of the city level and the regional level and basically the models unsure between.",
                    "label": 0
                },
                {
                    "sent": "So this is Southern somewhere and happening somewhere.",
                    "label": 0
                },
                {
                    "sent": "So it makes intra.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eating sort of.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Magical.",
                    "label": 0
                },
                {
                    "sent": "Errors.",
                    "label": 0
                },
                {
                    "sent": "OK, so to continue.",
                    "label": 0
                },
                {
                    "sent": "So that was a model that basically got to see the query in the document and sort of one chance to attend.",
                    "label": 0
                },
                {
                    "sent": "An obvious extension is to give the model the ability to attend multiple times to a document to actually look at multiple bits of the document over a few time steps.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we tried along this line is something we called the impatient reader and basically as we read the query, we let the model computer tension variable.",
                    "label": 0
                },
                {
                    "sent": "So now we run our current network over the query and at each time step as we were eating the query with also attending to the document, summarizing it down to a vector an reading it.",
                    "label": 0
                },
                {
                    "sent": "So this is the math.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's pretty similar.",
                    "label": 0
                },
                {
                    "sent": "The only difference now is that we've when we include the attention variables we've got the the attention.",
                    "label": 0
                },
                {
                    "sent": "Document summary vector from the previous time step and we have a second dimension here 'cause we're doing this attending at every every time step.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is vaguely what this looks like.",
                    "label": 0
                },
                {
                    "sent": "So again, we've got our document building query embedding.",
                    "label": 0
                },
                {
                    "sent": "We calculate sort of 1 attention variable, then we do it again.",
                    "label": 0
                },
                {
                    "sent": "Then we do it again as we go through the query and then we produce our final.",
                    "label": 0
                },
                {
                    "sent": "G.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this works a bit better, not a huge amount better.",
                    "label": 0
                },
                {
                    "sent": "And it's not clear that tying the attention to the query is the right way to go.",
                    "label": 0
                },
                {
                    "sent": "We're now trying other things where the the number of sort of attendant steps is untied from the query, but it does work a little bit better.",
                    "label": 0
                },
                {
                    "sent": "This also makes for extremely big models, so some of our queries might have sort of 40 time steps in it.",
                    "label": 0
                },
                {
                    "sent": "So now we're replicating our the size of our previous model by about 40, so we had a lot of problems getting all of this to run on on GPU's.",
                    "label": 0
                },
                {
                    "sent": "Lots of work went into the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not hugely interesting, but this is the precision recall curve, right?",
                    "label": 0
                },
                {
                    "sent": "This is the uniform attention baseline, and the two yellow and red models.",
                    "label": 0
                },
                {
                    "sent": "There are the different attention models and you can see that they have quite a nice linear backoff in recall.",
                    "label": 0
                },
                {
                    "sent": "Versus precision so.",
                    "label": 0
                },
                {
                    "sent": "OK, so the nice thing about the impatient readers, we can visualize the repeated attention step.",
                    "label": 0
                },
                {
                    "sent": "So we made some nice.",
                    "label": 0
                },
                {
                    "sent": "Nice videos of this.",
                    "label": 0
                },
                {
                    "sent": "So actually I will start this again.",
                    "label": 0
                },
                {
                    "sent": "It's going a bit fast.",
                    "label": 0
                },
                {
                    "sent": "So the blue highlight is the query and as it reads each word you can see the attention where it's attending.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing maybe is that as it goes past the X, so basically up until the X, it's got no idea, but once it goes past the X you can see that it starts to put its attention on the HOV Lane up here, which is the right answer.",
                    "label": 0
                },
                {
                    "sent": "So this is an article about a guy that.",
                    "label": 0
                },
                {
                    "sent": "Was driving the HRV Lane, which I think is something you have in North America for cars that have two occupants in them, you get to drive in a special Lane so this guy put a Cardboard cut out of a guy from a beer commercial in the passenger seat and drove in that Lane and was caught.",
                    "label": 0
                },
                {
                    "sent": "So that the correct answer is HOV Lane there.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that up until about the Texan model has no idea.",
                    "label": 0
                },
                {
                    "sent": "Once it sees that basically it has most of the information it needs, and you can see the attention goes on to the right answer.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "Another one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To explain first, so this is.",
                    "label": 0
                },
                {
                    "sent": "Lamborghini that was found.",
                    "label": 0
                },
                {
                    "sent": "And so the answer is Lamborghini.",
                    "label": 0
                },
                {
                    "sent": "Interesting thing is, like the yellow here is never mentioned in the article, so you would think that that would be a good clue, but it's never actually mentioned in the article.",
                    "label": 0
                },
                {
                    "sent": "So again.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "If you follow the model as it goes, you can see it's got no real idea when it.",
                    "label": 0
                },
                {
                    "sent": "So at this point you've got some attention on Lamborghini, but it's also got some attention on the Dallas North Tollway for for some reason which doesn't make so much sense.",
                    "label": 0
                },
                {
                    "sent": "You can see that that that is mentioned here.",
                    "label": 0
                },
                {
                    "sent": "So as soon as it sees that it realizes that's not the right answer.",
                    "label": 0
                },
                {
                    "sent": "And it had focuses on the right thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a couple of those examples.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's about it for me so.",
                    "label": 0
                },
                {
                    "sent": "This is very much initial work.",
                    "label": 0
                },
                {
                    "sent": "We weren't necessarily set trying to produce the sort of final deep learning model or machine learning models for this task.",
                    "label": 0
                },
                {
                    "sent": "What we mostly set out to do is see if one we could come up with a plausibly big enough training set for supervised reading comprehension that we could actually start exploring these models and then to explore a few different simple models.",
                    "label": 0
                },
                {
                    "sent": "Based roughly on things from other people have been doing for tasks and see how these perform, and it's clear that one.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The deep learning models work quite well.",
                    "label": 0
                },
                {
                    "sent": "They seem complex, but they're actually very simple models.",
                    "label": 0
                },
                {
                    "sent": "When you think about it and we're just giving them the starter, and they roughly learn the right problem given lots of time and lots of GPU's.",
                    "label": 0
                },
                {
                    "sent": "So it seems like this.",
                    "label": 0
                },
                {
                    "sent": "There's definitely some way to go in this problem of using these models to read the text.",
                    "label": 0
                },
                {
                    "sent": "The attention aspect seems very.",
                    "label": 0
                },
                {
                    "sent": "Useful, it seems like a good way of structuring the models of these problems.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if you.",
                    "label": 0
                },
                {
                    "sent": "If we wanted to do inference, you notice that those those all those scores, the accuracies topped out about 60%, which earlier and I said was about the limit before you actually not start to need to do some inference, so.",
                    "label": 0
                },
                {
                    "sent": "We're probably not doing any sort of high level inference at this point, but it's interesting to think about how we might be able to do that with these attention models.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to put multiple bits of information in our document together, do some processing on it, do some sort of reasoning, and come up with an answer.",
                    "label": 0
                },
                {
                    "sent": "But so the other thing is that this the basic idea for creating this data is is general, so anywhere where we can find documents and some sort of paraphrase or repeated information from these, we can turn this into queries and train these sorts of models.",
                    "label": 0
                },
                {
                    "sent": "So this works also when you have abstracts and introductions on documents and then document itself where that information is expanded upon.",
                    "label": 0
                },
                {
                    "sent": "You can create queries from the abstracts and things like this.",
                    "label": 0
                },
                {
                    "sent": "So there's more to be explored and to really do this to really learn.",
                    "label": 0
                },
                {
                    "sent": "The sort of structure of language you're going to need to answer or do inference.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alex inferences, we're going to need a lot more than a million documents, millions a good start, but we're going to need a billion or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you again.",
                    "label": 0
                }
            ]
        }
    }
}