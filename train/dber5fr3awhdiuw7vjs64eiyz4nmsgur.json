{
    "id": "dber5fr3awhdiuw7vjs64eiyz4nmsgur",
    "title": "BErMin: A Model Selection Algorithm for Reinforcement Learning Problems",
    "info": {
        "author": [
            "Amir-massoud Farahmand, Vector Institute"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_farahmand_bermin/",
    "segmentation": [
        [
            "And thanks for coming and so this is joint work with my supervisor, PhD, supervisor choppers obituary and this is actually a paper that has already been publishing machine learning Journal so I'm going to give a review of the results of the paper.",
            "So basically the problem that we want to solve is the problem of model selection in the back setting when we have an MVP and different models and then we try to find a model that has the minimum bailment.",
            "Arrow so I will describe it in more."
        ],
        [
            "Detail.",
            "So the general problem that we want to address is this.",
            "Suppose we have some interaction data that comes from interacting with the sequential decision making problems with some MVP that has large state space.",
            "So for example, status based continues and now the question that we want to answer that was the best thing.",
            "What's the best action that we can do given these data?",
            "So this is very general question of course.",
            "And The thing is that we don't want to assume much about the problem, so we want to only to use data for."
        ],
        [
            "All the decision-making.",
            "The approach that I'm going to talk about is will be based on the value based approach that is usually like these peers use the data to estimate the sum, action value function and then we follow the greedy policy with respect to that.",
            "But and then we have a larger state space that we usually use some kind of function approximation.",
            "But the problem is that how to choose that architecture of the function approximator?"
        ],
        [
            "This is difficult problem because the right choice of the function approximator depends on many factors that are unknown.",
            "For example, the problem that the value function may have regularity's like smoothness or sparsity or action gap, or the data coming from allow dimensional manifold that we don't really know a priori.",
            "The other thing is that, well, the right choice of function approximator should depend on the number of samples.",
            "The usual practice in reinforcement learning is not real like this we fixed.",
            "Architecture and save user kind of linear function approximated with fixed number of bases and then we use the same architecture with.",
            "If you have 100 samples or a 10,000,000 samples.",
            "So this is the question how to choose this architecture based on the data?"
        ],
        [
            "In an adaptive way.",
            "And a solution or kind of approach to address this question is by going through adaptive algorithms.",
            "So adaptive algorithms have these.",
            "We can say that it has these two elements.",
            "One is the flexible statistical inference algorithm that can get the parameters of some function approximator architecture.",
            "And given that parameter, try to estimate, say the value function.",
            "An example are different nonparametric function.",
            "Approximate are basically estimators that have been introduced in reinforcement learning.",
            "Regularizer SPI fitted Q, iteration's tree based neural network based fitted, Q iteration Gaussian Process TD and many other algorithms and of course in supervised learning we have.",
            "More examples of these algorithms, the other parties and model selection algorithm that tunes the parameter of this algorithm such that the overall performance would be the best or be maximized."
        ],
        [
            "So to be more concrete, I define the setup of the problem that I'm going."
        ],
        [
            "To talk about, so I'm considering discounted MPs.",
            "I consider the X as a general state space continues whatever actions is a finite action state space and the problem is discounted.",
            "We have the usual definitions for value, function and action action value function.",
            "For example, Q Pius State X and A is the expected at discounted reward when the starting state is X and the first action that we take is A and then we follow policy \u03c0.",
            "We have the optimal action value function, which is the up to action value function that maximizes.",
            "Paul action value function for policies.",
            "We have the bill mano optimal to operator, which is essentially T star of Q is the reward at that state plus discounted average value of next state.",
            "And then we have this interesting property of fixed point property of the Bellman optimality operator, which is essentially saying that the optimal if we find a solution to this, the fixed point of this operator.",
            "That solution would be the optimal value function.",
            "So it's kind of a good idea if it can try to find a solution that approximately minimizes the difference between Q&T star of Q, then the solution would be close to the optimal solution.",
            "And then I have the diffusional definitions of the norm L2 norm according to measure new and also empirical norm that I denote by Q with sub index.",
            "So which is defined as the summation of our Q squared over there some data point that should be specified.",
            "So the."
        ],
        [
            "Is this kind of setup?",
            "Now the problem is that we are given a list of action value functions Q1Q22QP and so these are.",
            "You can think of these as something that is generated by some algorithm.",
            "Your favorite reinforcement learning algorithm with different parts.",
            "And sorry.",
            "And then we are given a data set of interaction of in the form of XI action I reward I and then next state from XI and we have an off this topples exercise coming from some fixed distribution of our state space.",
            "Actions are chosen by some behavior policy rewards are generated by the reward.",
            "Colonel Annex is coming from the basically Markov process.",
            "The goal is to devise a processor that selects an action value function.",
            "That has the minimum Bellman error, so we want to choose QFQFK hat that K hat is the one that minimizes this Bellman error.",
            "If you remember from previous slide, I said that it's trying to have a model or action value function that has a small bellman error.",
            "Is meaningful thing to do because the one that has zero Bellman error is essentially the optimal solution.",
            "And there are some results that say that relates the sizeof.",
            "Bellman error to the quality of your value function comparing to the optimal one.",
            "So we want to select this one.",
            "Of course we can't do it because we don't know this.",
            "This is a norm but we just have some data from interacting with the problem and also we don't have the bellman operator or the build mental pressure is something that is unknown.",
            "We just have the data."
        ],
        [
            "So.",
            "One idea is basically OK. Let's use the data to estimate this Bellman error so it would be essentially like this.",
            "So we use empirical sample and say that we take this summation overall Q -- R I plus gamma over Max.",
            "So this is would be a sample of the Bellman optimality to operator.",
            "And then, well, this essentially should be a fine thing of fine estimate of the true Bellman error.",
            "The problem is that this is not the case because the bellman error would be a empirical Burma error would be biased estimate of the true Bellman error.",
            "So it can be decomposed.",
            "The expectation of this empirical Berman error to the true Bellman error and some extra term which depends on.",
            "Sorry.",
            "Depends on the choice of Q.",
            "So in supervised learning, if we have, say, supposed classic regression problem, we can do the same decomposition and we see that we have the kind of the error of the model plus some term that depends on the variance of the noise.",
            "Which is fine because it's the same for all the models.",
            "But here that variance term depends on the model itself, so we can't actually try to minimize this term because the size of the variance depends on.",
            "The model itself.",
            "So we can't directly use empirical Bellman error.",
            "What can?"
        ],
        [
            "I do.",
            "So the idea is that OK, let's try to estimate the bellman operator itself.",
            "So if you have, suppose we have a very good estimate of the bellman operator, then we can use that estimate of the bellman operator instead of true Bellman one."
        ],
        [
            "Operator one, we don't actually exactly do that, but we do something very similar.",
            "What we do is like this.",
            "Suppose this blue curve is Q of one of the models.",
            "And TT star is the effect of the bellman operator on QK so we get these green curve.",
            "We don't have that but we have some samples.",
            "This red dots from the belmin effect of tier star Q.",
            "So that would be basically our process, maximal or QK.",
            "Now we try to estimate to fit some regression function Q~ to this points and.",
            "Because this is essentially a usual regression problem, we can hope that if you have a good estimate of the bellman operator, the actual effect of TSR Q and we call it Q~ then we can approximately say that.",
            "Well, the difference between the norm of the true norm of Q, K -- Q~ would be approximately the same as the bellman error.",
            "Right and then we can approve the samples Q K -- Q OF.",
            "This form Q, K -- Q~ and its empirical average to estimate these Sky.",
            "So because of law of large number, so basically use empirical samples to estimate this guy and then because if the estimate Q~ is very close to T star Q, these two things should be the same, so we can use kind of sorrow gates measure and then we try to minimize that.",
            "Instead of the true Bellman error, the problem here is that."
        ],
        [
            "This would happen if the.",
            "Model The Cutie Elders are not very accurate.",
            "The thing that would happen is that we may basically."
        ],
        [
            "Guitar billman error.",
            "So this situation would be something like this.",
            "Suppose you have one QK, this is 1 function and this would be the start of QK, right?",
            "And then in one scenario, consider that your Q~ their product estimate of T star Q would be like this.",
            "So it's kind of close to the truth.",
            "But in another estimates in another scenario, your estimate would be Q~ F2, which is bad estimate.",
            "But The thing is that if you use the size of this size or that size.",
            "And want to choose the model that has the smallest kind of approximate or surrogate of the bellman error you choose Q~ 2, which is not a good approximation of the true value of the Bellman error.",
            "So in order to avoid this kind of underestimation, what we can do is try to estimate the error between Q~ and to start Q.",
            "So if we have some estimates B bar of this value and B bar 2.",
            "For this value and add these two things together, we should be fine.",
            "Dispensing so basically there are Bellman error would be upper bounded half of it would be upper bounded by Q, K -- Q~ which can be estimated by empirical samples.",
            "And T star Q K -- Q~ which can be estimated.",
            "That we hope to be estimate or upper bounded by B bar K. So if there are regression algorithm, BASIC essentially provides an upper bound on its quality on the access error of estimating T star, Q by Q~ we should be fine.",
            "If we minimize this quantity, everything would be fine because that would be an upper bound of the true measure that we want to minimize.",
            "Sam five more minutes, right?"
        ],
        [
            "So this is the algorithm.",
            "They look a bit complicated, but actually it's simple, so the way that."
        ],
        [
            "It works is like this.",
            "Suppose OK, the input of the algorithm is the set of the action value functions.",
            "Some data points, datasets, regression function and some parameters.",
            "Which Delta is the confidence parameter B is the bound on all the function involved.",
            "So you can.",
            "Just set it.",
            "Let's look maximum, Q function and Tao is the forgetting time of the Markov chain.",
            "So we first split the data into disjoint sets and choose CK's in some special way that is given example that these conditions will be satisfied and then also we decompose our Delta confidence value to some Delta I Delta primes.",
            "Such that's the summation of Delta primes.",
            "Overall models would be half of the true confidence.",
            "So what we do is first call a regression function so that we estimate Q~ That regression algorithm should return us some estimates on its access error.",
            "So be bar is given by the regression algorithm.",
            "Now we use empirical error of Q, K -- Q~ K for all the models cuccos given by the regression algorithm.",
            "Now we inflates the.",
            "Our empirical estimate and then add the bar.",
            "So after we do it for all the models, we select a model that minimizes this kind of inflated risk plus CK when CCR sequence that satisfies this.",
            "Inequality.",
            "The basic idea is like this.",
            "Suppose we have these different models Q.",
            "This blue bar is the Q -- Q bar, Q~ the green is the empirical version of that.",
            "The Orange One is the inflated version plus B~ and then we add the red one, which is the red bar which is CK.",
            "So it's kind of small thing and then we choose the one that has the minimum of these values.",
            "After adding all these things to each other and then the result for example in this diagram would be Q2, the one that minimizes that."
        ],
        [
            "So well, we have a lot of assumptions.",
            "I'm not going to talk about that.",
            "Dog basically talked about all the assumptions implicitly.",
            "We need to have some Markov chair.",
            "We need to know that for getting time off that we need to have a regression procedure that gives us an up high confidence upper bound on its access error, and I guess the others are."
        ],
        [
            "Kind of mild assumptions that error would be like this if we run this algorithm with the para meters A and Delta and CK satisfy this property.",
            "Then the index that is selected by the vermin algorithm would satisfy this Oracle like property.",
            "So it's would be the QF development error for the KKK hat model would be upper bounded by this term, so I will describe it in the next slide and talk."
        ],
        [
            "Different elements of its.",
            "So remember that what we wanted."
        ],
        [
            "Actually two is to find the minimum Bellman error, but this was too much to expect."
        ],
        [
            "So we instead get something like this our selected model.",
            "Would choose a model that has the minimum Bellman error plus some multiplicative constants, right?",
            "So if you ignore all the other terms and you just consider this there, it will choose the model that is optimal to creative constants.",
            "The minimum has a minimum Bellman error, but then we have other terms.",
            "One is beep beep bar and the other is CK.",
            "Sobibor is given by the regression algorithm and its quality depends on the quality of regression algorithm to regression algorithm can use.",
            "Kind of a model selection itself and provide.",
            "If it can provide a tight estimate for Q~ that's would be fine.",
            "CK term has the behavior of order 1 / N, so it's a faster.",
            "The extra term would be order of 1 / N so."
        ],
        [
            "That would be fine.",
            "So what has been achieved to conclude we have introduced the complexity regularization based algorithm for choosing model with the minimum Bellman error.",
            "Now we have Oracle large performance guarantee for the quality of the selected model, but there are many things to study and one is that how to generate the list of all the possible models Q1QP2P I haven't talked about that.",
            "The other is that how to efficiently estimate the access error.",
            "We have some suggestions but.",
            "Probably that's that way of doing that.",
            "Is suboptimal and also the other problem is that it's the result would be not finite sample, but it would be a synthetic kind of guarantee.",
            "So this model selection result is finite sample.",
            "But in order to assume in order to estimate the bar, we just have a result that is a synthetic in nature and the other thing is that the relation of the bellman error and the quality of the resulting policy.",
            "Thank you."
        ],
        [
            "So the question was that how the number of models P affect quality of result.",
            "So the P appears basically in the choice of CK an if you have larger PCK would be become a bit larger.",
            "But the effect would be kind of minimal.",
            "We can essentially handle infinite number of models.",
            "The difference would be that."
        ],
        [
            "CK should be chosen such that distinct be satisfied.",
            "So if you have infinite number of samples."
        ],
        [
            "I'd only five OK, so one example.",
            "FCK would be like this.",
            "So the effect of the number of model or is in logarithmic form.",
            "So if you have 1000 model ellenoff thousand would appear in your bonds.",
            "OK, thank you thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And thanks for coming and so this is joint work with my supervisor, PhD, supervisor choppers obituary and this is actually a paper that has already been publishing machine learning Journal so I'm going to give a review of the results of the paper.",
                    "label": 1
                },
                {
                    "sent": "So basically the problem that we want to solve is the problem of model selection in the back setting when we have an MVP and different models and then we try to find a model that has the minimum bailment.",
                    "label": 1
                },
                {
                    "sent": "Arrow so I will describe it in more.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detail.",
                    "label": 0
                },
                {
                    "sent": "So the general problem that we want to address is this.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have some interaction data that comes from interacting with the sequential decision making problems with some MVP that has large state space.",
                    "label": 1
                },
                {
                    "sent": "So for example, status based continues and now the question that we want to answer that was the best thing.",
                    "label": 0
                },
                {
                    "sent": "What's the best action that we can do given these data?",
                    "label": 0
                },
                {
                    "sent": "So this is very general question of course.",
                    "label": 0
                },
                {
                    "sent": "And The thing is that we don't want to assume much about the problem, so we want to only to use data for.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All the decision-making.",
                    "label": 0
                },
                {
                    "sent": "The approach that I'm going to talk about is will be based on the value based approach that is usually like these peers use the data to estimate the sum, action value function and then we follow the greedy policy with respect to that.",
                    "label": 0
                },
                {
                    "sent": "But and then we have a larger state space that we usually use some kind of function approximation.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that how to choose that architecture of the function approximator?",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is difficult problem because the right choice of the function approximator depends on many factors that are unknown.",
                    "label": 1
                },
                {
                    "sent": "For example, the problem that the value function may have regularity's like smoothness or sparsity or action gap, or the data coming from allow dimensional manifold that we don't really know a priori.",
                    "label": 1
                },
                {
                    "sent": "The other thing is that, well, the right choice of function approximator should depend on the number of samples.",
                    "label": 0
                },
                {
                    "sent": "The usual practice in reinforcement learning is not real like this we fixed.",
                    "label": 0
                },
                {
                    "sent": "Architecture and save user kind of linear function approximated with fixed number of bases and then we use the same architecture with.",
                    "label": 0
                },
                {
                    "sent": "If you have 100 samples or a 10,000,000 samples.",
                    "label": 1
                },
                {
                    "sent": "So this is the question how to choose this architecture based on the data?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In an adaptive way.",
                    "label": 0
                },
                {
                    "sent": "And a solution or kind of approach to address this question is by going through adaptive algorithms.",
                    "label": 0
                },
                {
                    "sent": "So adaptive algorithms have these.",
                    "label": 1
                },
                {
                    "sent": "We can say that it has these two elements.",
                    "label": 0
                },
                {
                    "sent": "One is the flexible statistical inference algorithm that can get the parameters of some function approximator architecture.",
                    "label": 1
                },
                {
                    "sent": "And given that parameter, try to estimate, say the value function.",
                    "label": 0
                },
                {
                    "sent": "An example are different nonparametric function.",
                    "label": 0
                },
                {
                    "sent": "Approximate are basically estimators that have been introduced in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Regularizer SPI fitted Q, iteration's tree based neural network based fitted, Q iteration Gaussian Process TD and many other algorithms and of course in supervised learning we have.",
                    "label": 1
                },
                {
                    "sent": "More examples of these algorithms, the other parties and model selection algorithm that tunes the parameter of this algorithm such that the overall performance would be the best or be maximized.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to be more concrete, I define the setup of the problem that I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To talk about, so I'm considering discounted MPs.",
                    "label": 0
                },
                {
                    "sent": "I consider the X as a general state space continues whatever actions is a finite action state space and the problem is discounted.",
                    "label": 0
                },
                {
                    "sent": "We have the usual definitions for value, function and action action value function.",
                    "label": 0
                },
                {
                    "sent": "For example, Q Pius State X and A is the expected at discounted reward when the starting state is X and the first action that we take is A and then we follow policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "We have the optimal action value function, which is the up to action value function that maximizes.",
                    "label": 0
                },
                {
                    "sent": "Paul action value function for policies.",
                    "label": 0
                },
                {
                    "sent": "We have the bill mano optimal to operator, which is essentially T star of Q is the reward at that state plus discounted average value of next state.",
                    "label": 0
                },
                {
                    "sent": "And then we have this interesting property of fixed point property of the Bellman optimality operator, which is essentially saying that the optimal if we find a solution to this, the fixed point of this operator.",
                    "label": 0
                },
                {
                    "sent": "That solution would be the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a good idea if it can try to find a solution that approximately minimizes the difference between Q&T star of Q, then the solution would be close to the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And then I have the diffusional definitions of the norm L2 norm according to measure new and also empirical norm that I denote by Q with sub index.",
                    "label": 0
                },
                {
                    "sent": "So which is defined as the summation of our Q squared over there some data point that should be specified.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this kind of setup?",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that we are given a list of action value functions Q1Q22QP and so these are.",
                    "label": 0
                },
                {
                    "sent": "You can think of these as something that is generated by some algorithm.",
                    "label": 0
                },
                {
                    "sent": "Your favorite reinforcement learning algorithm with different parts.",
                    "label": 0
                },
                {
                    "sent": "And sorry.",
                    "label": 0
                },
                {
                    "sent": "And then we are given a data set of interaction of in the form of XI action I reward I and then next state from XI and we have an off this topples exercise coming from some fixed distribution of our state space.",
                    "label": 0
                },
                {
                    "sent": "Actions are chosen by some behavior policy rewards are generated by the reward.",
                    "label": 0
                },
                {
                    "sent": "Colonel Annex is coming from the basically Markov process.",
                    "label": 0
                },
                {
                    "sent": "The goal is to devise a processor that selects an action value function.",
                    "label": 0
                },
                {
                    "sent": "That has the minimum Bellman error, so we want to choose QFQFK hat that K hat is the one that minimizes this Bellman error.",
                    "label": 0
                },
                {
                    "sent": "If you remember from previous slide, I said that it's trying to have a model or action value function that has a small bellman error.",
                    "label": 0
                },
                {
                    "sent": "Is meaningful thing to do because the one that has zero Bellman error is essentially the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And there are some results that say that relates the sizeof.",
                    "label": 0
                },
                {
                    "sent": "Bellman error to the quality of your value function comparing to the optimal one.",
                    "label": 0
                },
                {
                    "sent": "So we want to select this one.",
                    "label": 0
                },
                {
                    "sent": "Of course we can't do it because we don't know this.",
                    "label": 0
                },
                {
                    "sent": "This is a norm but we just have some data from interacting with the problem and also we don't have the bellman operator or the build mental pressure is something that is unknown.",
                    "label": 0
                },
                {
                    "sent": "We just have the data.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One idea is basically OK. Let's use the data to estimate this Bellman error so it would be essentially like this.",
                    "label": 0
                },
                {
                    "sent": "So we use empirical sample and say that we take this summation overall Q -- R I plus gamma over Max.",
                    "label": 0
                },
                {
                    "sent": "So this is would be a sample of the Bellman optimality to operator.",
                    "label": 0
                },
                {
                    "sent": "And then, well, this essentially should be a fine thing of fine estimate of the true Bellman error.",
                    "label": 1
                },
                {
                    "sent": "The problem is that this is not the case because the bellman error would be a empirical Burma error would be biased estimate of the true Bellman error.",
                    "label": 0
                },
                {
                    "sent": "So it can be decomposed.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this empirical Berman error to the true Bellman error and some extra term which depends on.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Depends on the choice of Q.",
                    "label": 0
                },
                {
                    "sent": "So in supervised learning, if we have, say, supposed classic regression problem, we can do the same decomposition and we see that we have the kind of the error of the model plus some term that depends on the variance of the noise.",
                    "label": 0
                },
                {
                    "sent": "Which is fine because it's the same for all the models.",
                    "label": 0
                },
                {
                    "sent": "But here that variance term depends on the model itself, so we can't actually try to minimize this term because the size of the variance depends on.",
                    "label": 0
                },
                {
                    "sent": "The model itself.",
                    "label": 0
                },
                {
                    "sent": "So we can't directly use empirical Bellman error.",
                    "label": 1
                },
                {
                    "sent": "What can?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I do.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that OK, let's try to estimate the bellman operator itself.",
                    "label": 1
                },
                {
                    "sent": "So if you have, suppose we have a very good estimate of the bellman operator, then we can use that estimate of the bellman operator instead of true Bellman one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operator one, we don't actually exactly do that, but we do something very similar.",
                    "label": 0
                },
                {
                    "sent": "What we do is like this.",
                    "label": 0
                },
                {
                    "sent": "Suppose this blue curve is Q of one of the models.",
                    "label": 0
                },
                {
                    "sent": "And TT star is the effect of the bellman operator on QK so we get these green curve.",
                    "label": 0
                },
                {
                    "sent": "We don't have that but we have some samples.",
                    "label": 1
                },
                {
                    "sent": "This red dots from the belmin effect of tier star Q.",
                    "label": 0
                },
                {
                    "sent": "So that would be basically our process, maximal or QK.",
                    "label": 0
                },
                {
                    "sent": "Now we try to estimate to fit some regression function Q~ to this points and.",
                    "label": 0
                },
                {
                    "sent": "Because this is essentially a usual regression problem, we can hope that if you have a good estimate of the bellman operator, the actual effect of TSR Q and we call it Q~ then we can approximately say that.",
                    "label": 1
                },
                {
                    "sent": "Well, the difference between the norm of the true norm of Q, K -- Q~ would be approximately the same as the bellman error.",
                    "label": 0
                },
                {
                    "sent": "Right and then we can approve the samples Q K -- Q OF.",
                    "label": 0
                },
                {
                    "sent": "This form Q, K -- Q~ and its empirical average to estimate these Sky.",
                    "label": 0
                },
                {
                    "sent": "So because of law of large number, so basically use empirical samples to estimate this guy and then because if the estimate Q~ is very close to T star Q, these two things should be the same, so we can use kind of sorrow gates measure and then we try to minimize that.",
                    "label": 0
                },
                {
                    "sent": "Instead of the true Bellman error, the problem here is that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would happen if the.",
                    "label": 0
                },
                {
                    "sent": "Model The Cutie Elders are not very accurate.",
                    "label": 0
                },
                {
                    "sent": "The thing that would happen is that we may basically.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guitar billman error.",
                    "label": 0
                },
                {
                    "sent": "So this situation would be something like this.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have one QK, this is 1 function and this would be the start of QK, right?",
                    "label": 0
                },
                {
                    "sent": "And then in one scenario, consider that your Q~ their product estimate of T star Q would be like this.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of close to the truth.",
                    "label": 0
                },
                {
                    "sent": "But in another estimates in another scenario, your estimate would be Q~ F2, which is bad estimate.",
                    "label": 0
                },
                {
                    "sent": "But The thing is that if you use the size of this size or that size.",
                    "label": 0
                },
                {
                    "sent": "And want to choose the model that has the smallest kind of approximate or surrogate of the bellman error you choose Q~ 2, which is not a good approximation of the true value of the Bellman error.",
                    "label": 0
                },
                {
                    "sent": "So in order to avoid this kind of underestimation, what we can do is try to estimate the error between Q~ and to start Q.",
                    "label": 0
                },
                {
                    "sent": "So if we have some estimates B bar of this value and B bar 2.",
                    "label": 0
                },
                {
                    "sent": "For this value and add these two things together, we should be fine.",
                    "label": 0
                },
                {
                    "sent": "Dispensing so basically there are Bellman error would be upper bounded half of it would be upper bounded by Q, K -- Q~ which can be estimated by empirical samples.",
                    "label": 0
                },
                {
                    "sent": "And T star Q K -- Q~ which can be estimated.",
                    "label": 0
                },
                {
                    "sent": "That we hope to be estimate or upper bounded by B bar K. So if there are regression algorithm, BASIC essentially provides an upper bound on its quality on the access error of estimating T star, Q by Q~ we should be fine.",
                    "label": 0
                },
                {
                    "sent": "If we minimize this quantity, everything would be fine because that would be an upper bound of the true measure that we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "Sam five more minutes, right?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "They look a bit complicated, but actually it's simple, so the way that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works is like this.",
                    "label": 0
                },
                {
                    "sent": "Suppose OK, the input of the algorithm is the set of the action value functions.",
                    "label": 1
                },
                {
                    "sent": "Some data points, datasets, regression function and some parameters.",
                    "label": 1
                },
                {
                    "sent": "Which Delta is the confidence parameter B is the bound on all the function involved.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "Just set it.",
                    "label": 0
                },
                {
                    "sent": "Let's look maximum, Q function and Tao is the forgetting time of the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So we first split the data into disjoint sets and choose CK's in some special way that is given example that these conditions will be satisfied and then also we decompose our Delta confidence value to some Delta I Delta primes.",
                    "label": 0
                },
                {
                    "sent": "Such that's the summation of Delta primes.",
                    "label": 0
                },
                {
                    "sent": "Overall models would be half of the true confidence.",
                    "label": 1
                },
                {
                    "sent": "So what we do is first call a regression function so that we estimate Q~ That regression algorithm should return us some estimates on its access error.",
                    "label": 0
                },
                {
                    "sent": "So be bar is given by the regression algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we use empirical error of Q, K -- Q~ K for all the models cuccos given by the regression algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we inflates the.",
                    "label": 0
                },
                {
                    "sent": "Our empirical estimate and then add the bar.",
                    "label": 0
                },
                {
                    "sent": "So after we do it for all the models, we select a model that minimizes this kind of inflated risk plus CK when CCR sequence that satisfies this.",
                    "label": 0
                },
                {
                    "sent": "Inequality.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is like this.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have these different models Q.",
                    "label": 0
                },
                {
                    "sent": "This blue bar is the Q -- Q bar, Q~ the green is the empirical version of that.",
                    "label": 0
                },
                {
                    "sent": "The Orange One is the inflated version plus B~ and then we add the red one, which is the red bar which is CK.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of small thing and then we choose the one that has the minimum of these values.",
                    "label": 0
                },
                {
                    "sent": "After adding all these things to each other and then the result for example in this diagram would be Q2, the one that minimizes that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So well, we have a lot of assumptions.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "Dog basically talked about all the assumptions implicitly.",
                    "label": 0
                },
                {
                    "sent": "We need to have some Markov chair.",
                    "label": 0
                },
                {
                    "sent": "We need to know that for getting time off that we need to have a regression procedure that gives us an up high confidence upper bound on its access error, and I guess the others are.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of mild assumptions that error would be like this if we run this algorithm with the para meters A and Delta and CK satisfy this property.",
                    "label": 0
                },
                {
                    "sent": "Then the index that is selected by the vermin algorithm would satisfy this Oracle like property.",
                    "label": 0
                },
                {
                    "sent": "So it's would be the QF development error for the KKK hat model would be upper bounded by this term, so I will describe it in the next slide and talk.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different elements of its.",
                    "label": 0
                },
                {
                    "sent": "So remember that what we wanted.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually two is to find the minimum Bellman error, but this was too much to expect.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we instead get something like this our selected model.",
                    "label": 0
                },
                {
                    "sent": "Would choose a model that has the minimum Bellman error plus some multiplicative constants, right?",
                    "label": 0
                },
                {
                    "sent": "So if you ignore all the other terms and you just consider this there, it will choose the model that is optimal to creative constants.",
                    "label": 0
                },
                {
                    "sent": "The minimum has a minimum Bellman error, but then we have other terms.",
                    "label": 0
                },
                {
                    "sent": "One is beep beep bar and the other is CK.",
                    "label": 0
                },
                {
                    "sent": "Sobibor is given by the regression algorithm and its quality depends on the quality of regression algorithm to regression algorithm can use.",
                    "label": 0
                },
                {
                    "sent": "Kind of a model selection itself and provide.",
                    "label": 0
                },
                {
                    "sent": "If it can provide a tight estimate for Q~ that's would be fine.",
                    "label": 0
                },
                {
                    "sent": "CK term has the behavior of order 1 / N, so it's a faster.",
                    "label": 0
                },
                {
                    "sent": "The extra term would be order of 1 / N so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That would be fine.",
                    "label": 0
                },
                {
                    "sent": "So what has been achieved to conclude we have introduced the complexity regularization based algorithm for choosing model with the minimum Bellman error.",
                    "label": 0
                },
                {
                    "sent": "Now we have Oracle large performance guarantee for the quality of the selected model, but there are many things to study and one is that how to generate the list of all the possible models Q1QP2P I haven't talked about that.",
                    "label": 0
                },
                {
                    "sent": "The other is that how to efficiently estimate the access error.",
                    "label": 0
                },
                {
                    "sent": "We have some suggestions but.",
                    "label": 0
                },
                {
                    "sent": "Probably that's that way of doing that.",
                    "label": 0
                },
                {
                    "sent": "Is suboptimal and also the other problem is that it's the result would be not finite sample, but it would be a synthetic kind of guarantee.",
                    "label": 0
                },
                {
                    "sent": "So this model selection result is finite sample.",
                    "label": 0
                },
                {
                    "sent": "But in order to assume in order to estimate the bar, we just have a result that is a synthetic in nature and the other thing is that the relation of the bellman error and the quality of the resulting policy.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question was that how the number of models P affect quality of result.",
                    "label": 0
                },
                {
                    "sent": "So the P appears basically in the choice of CK an if you have larger PCK would be become a bit larger.",
                    "label": 0
                },
                {
                    "sent": "But the effect would be kind of minimal.",
                    "label": 0
                },
                {
                    "sent": "We can essentially handle infinite number of models.",
                    "label": 0
                },
                {
                    "sent": "The difference would be that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "CK should be chosen such that distinct be satisfied.",
                    "label": 0
                },
                {
                    "sent": "So if you have infinite number of samples.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd only five OK, so one example.",
                    "label": 0
                },
                {
                    "sent": "FCK would be like this.",
                    "label": 0
                },
                {
                    "sent": "So the effect of the number of model or is in logarithmic form.",
                    "label": 0
                },
                {
                    "sent": "So if you have 1000 model ellenoff thousand would appear in your bonds.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you thanks.",
                    "label": 0
                }
            ]
        }
    }
}