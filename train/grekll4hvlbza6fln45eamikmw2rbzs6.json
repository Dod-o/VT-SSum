{
    "id": "grekll4hvlbza6fln45eamikmw2rbzs6",
    "title": "Spooky Stuff in Metric Space",
    "info": {
        "author": [
            "Rich Caruana, Microsoft Research"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2007",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/solomon_caruana_ssms/",
    "segmentation": [
        [
            "And for those of you who are sitting through now, the third talks.",
            "That's right, so there's another informal talk this afternoon, and then one more talk tomorrow.",
            "I think.",
            "So, OK, hopefully you don't get tired of me.",
            "So this is a paper we presented at KDD conference two years ago 2004, but I also gave this talk a year ago on Halloween All Saints Day and.",
            "So that's why I have these are pumpkins that my wife and I carved last year.",
            "So this is a picture from our house, but.",
            "So the talk really is data mining in metric space, but spooky stuff.",
            "I don't know if that word means something to you, but this is joint work with one of my grad students, Alex Niculescu, who's also been joint on all of the other things I've talked about, so he'll be graduating year really good student."
        ],
        [
            "OK, so let me start off with some motivation and in fact the motivational takes sort of five minutes to set it up.",
            "It's a pretty simple talk, so nothing too fancy is going to happen.",
            "So.",
            "I don't know how many of you know."
        ],
        [
            "But I'm the guy who started doing this work on multi task learning training models on a dozen or 100 tasks at the same time, with the hopes that you get some inductive transfer.",
            "That is, some of what you learn for this task helps you learn this other task better.",
            "So what you learn from that tells helps you learn that other tasks better and basically as long as you have some shared representation and the tests are related to each other, it's often better to learn these things all at the same time as opposed to learning them as separate tasks.",
            "So that's the same attribute set.",
            "That's right, you have the same input attributes class.",
            "Classes right so one example of this would be if any of you are familiar with Tom Mitchell's Calendar Apprentice system work from 10 years ago.",
            "There for a meeting, so he'd be trying to automatically schedule a meeting, say, with Donya, and he'd have to figure out where should the meeting occur should it be in Toms office, Daniel's office, or in some conference room or something like that.",
            "What time of day should the meeting occur at?",
            "How long should the meeting be?",
            "Half an hour, hour, whatever?",
            "And what day of week?",
            "Should the meeting occur at and the rules really change depending on who Tom's meeting with he's meeting with the Dean of the University the meeting is going to be in the Deans Office, it's going to be anytime of the week, day or night, and it'll probably be an hour.",
            "But if he's meeting with me, a graduate student, or Donya visiting student, it's probably going to be half an hour.",
            "Most of the meetings were half an hour.",
            "It'll be in Toms office.",
            "It won't be in the mornings usually because that's time Tom reserved to write papers, so it would probably be in the afternoon.",
            "And for me it was always on Thursdays.",
            "So, but I don't know about you so anyway.",
            "So those are four different prediction tasks, but they're all predictions for the same event.",
            "And they're not constrained in any interesting.",
            "It's not a complex structure, but you still have to predict these four independent things to schedule a meeting.",
            "Well, it turns out, if you were to learn a model that tries to predict all four things at the same time, some of what it learns about how long the meeting should be helps it decide where the meeting should be.",
            "'cause there are correlations between these things, so, so anyway, that's this whole area now of what's known as inductive transfer.",
            "This sub area that I work in is called multi task learning and the easiest way to do it is.",
            "Now I know most of you don't know neural Nets, but so you have all these inputs to the neural Nets.",
            "You have this big shared hidden layer.",
            "All these connections to the inputs and then you typically have this task on the output of the network that gets to see all those hidden units well in multi task learning you've got multiple outputs and you're training all of these outputs in parallel and they all share this hidden layer.",
            "So what this task causes to be learned in this hidden layer is now visible by all the other tasks.",
            "And they may benefit from it.",
            "So if anybody is interested in that, I can give a talk on that.",
            "You know we can do A6 talk, but it's easiest to see that in neural Nets and if you guys aren't into neural Nets that much, it might not be as exciting for you.",
            "Although you can do this in everything.",
            "You can do this in SVM and decision trees.",
            "All that sort of stuff, but it's really easy to see in neural net and I did most of the work with neural Nets were very popular, so so I also did a lot of the work in neural Nets.",
            "So here's a neural net.",
            "So back to our motivation for the data mining in metric space.",
            "Here's a neural net.",
            "These are the input variables.",
            "That's the big shared hidden layer, and these are the multiple outputs.",
            "This is the output that really counts were trying to predict your risk of dying from pneumonia.",
            "OK, so everybody in the room hasn ammonia, and we're just trying to figure out low risk, low risk, low risk, high risk, sorry, low risk, high risk, low risk, lower stroke, and the people who are low risk.",
            "We send you home, we give you antibiotics.",
            "Chicken noodle soup.",
            "Call us in three days if you're not feeling better.",
            "The people who are high risk you and you.",
            "You go straight to the hospital.",
            "OK?",
            "'cause pneumonia is a dangerous thing if you're seriously ill and you need to be in the hospital.",
            "So we're trying to figure out here what patients are.",
            "High risk or low risk from all these measurements we have.",
            "We know your age or gender, whether you have asthma, whether you have chest pain, you know what your cough sounds like, whether you're diabetic, we know all those things and we're trying to figure out whether you're high risk or low risk.",
            "And then the thing we did with multi task learning here.",
            "I won't spend too much time on it.",
            "But we have all these other measurements about these patients because they were part of a special study.",
            "We only have these measurements for patients who have already been in the hospital.",
            "Turns out everybody in this study was in the hospital because of the study, so it's a very unusual situation 'cause it's expensive to put people in the hospital and you don't want to do it unless they should be there.",
            "So we have all these measurements.",
            "These are things like your white blood cell count and your partial pressure of oxygen in your blood and all those sorts of things.",
            "So these are the kind of tests we only measure if you're in the hospital.",
            "And what we're doing is we can't use these as inputs for this model, because we're going to use the model in practice for patients who have not yet been in the hospital to determine who's high risk or low risk.",
            "So rather than throw this information away, it turns out if you put it on the output side of the network and use it as extra tasks, some of what you learn trying to predict the white blood cell count in that hidden unit, and you end up predicting mortality better.",
            "The risk of mortality better.",
            "So that's the inductive transfer multitest learning thing.",
            "So now why is this motivation for our work in data mining in metric space?",
            "Well, here's the annoying thing.",
            "Like most back prop Nets, I was training this with squared are to minimize squared are on this data set, and here's what I found.",
            "Suppose I didn't have these outputs.",
            "These extra outputs on the network and I just trained in network with one output.",
            "That's the usual way of doing things, and I trained it to minimize squared R. I'd get some squared error, let's say .1, so that's pretty good.",
            "Now I train the network again and I put these extra outputs on it, do back prop through all of these things at the same time, and something really good would happen.",
            "The squared error here would drop from .1 to, say, .08.",
            "So it's great.",
            "I get a 20% reduction in squared error on the output.",
            "That really counts.",
            "By putting these extra things in the network, that's great.",
            "I have a thesis, I get to graduate.",
            "All those good things.",
            "There's a problem the way they really evaluate these networks in the medical domain is not by looking at the squared error of the model.",
            "They look at the Roc of the model.",
            "So the Roc is a measure of how well it orders patients by risk.",
            "That is, remember you're going to the hospital and you're going to hospital use.",
            "We want to sort all these patients so that you 2 come up near the top as the highest risk patients and everybody else.",
            "It's OK if you sort lower, that's fine, we don't really care what the squared error is.",
            "As long as we get the sort right now, they're correlated measures OK.",
            "If if we get this squared error, perfect, say near zero, well then we have to get the sort right.",
            "If we sort on that prediction, but that's often not the case, especially when you train with small sample.",
            "So here's the really annoying thing I consistently got.",
            "Let's say a 10 to 20% reduction in squared error by putting extra outputs on this network.",
            "That's great news.",
            "That's a big improvement.",
            "Don't know how to do it any other way.",
            "Yet when we measured performance.",
            "With borrow.",
            "See how well we could order things.",
            "Sometimes it helps.",
            "Sometimes it hurts, sometimes it made no real difference.",
            "That was really annoying on the metrics I was optimizing too.",
            "I always got a significant improvement.",
            "And yet, when I looked at a different metric, which is what the doctors were using.",
            "Sometimes I did, sometimes I didn't.",
            "That means these two metrics were not telling me the same story and my method was working for the metric.",
            "It was optimizing too, but not necessarily for some other metrics so correlated, but but but not enough, right?",
            "And if I could get the squared error down very, very low then they would be even more correlated.",
            "It turns out once you reach perfection, any reasonable metric is perfectly correlated with any other reasonable metric.",
            "But when you're not near perfection.",
            "You can be different from perfection in a variety of ways, so in different metrics measure different properties of how you differ from perfection.",
            "This small detail which you mentioned, so if if you have this extra outputs then also the primary output gets gets better.",
            "So yes this is work which you did right exactly 6 exactly.",
            "That's this multi task learning work.",
            "And what's fun about this is.",
            "I sort of stopped doing a lot of this around 98 because I couldn't talk many other people into thinking it was very important and now it's all changed.",
            "Now there's suddenly a huge community DARPA is throwing in millions of dollars to fund it, and so it's really.",
            "It's really good.",
            "So all of a sudden this has gotten popular again, which is great.",
            "After about five years of nobody, everybody thinking, oh that's cool, but I'm doing something else.",
            "Yeah yeah, so normally have this 50 years like the Russian changes and then you guys don't know.",
            "Yeah yeah.",
            "So this is nice.",
            "Something I'm getting invited to go places to talk about this old work because yeah, and we're doing some new work in this so Alex Niculescu's thesis is in fact going to be multi task learning in Bayes Nets.",
            "So OK, so what's the moral of this story?",
            "Well, the moral of the story is I had a better technique.",
            "It consistently worked on what I asked it to do, which was optimized squared error.",
            "It made this output, which is very important.",
            "Consistently say 20% better.",
            "And yet, on the evaluation criterion that other people were using to evaluate the model, sometimes it helps.",
            "Sometimes it hurt, often just no big difference, which is very, very annoying.",
            "So what I did was I thought I've got to solve this problem.",
            "I have to be optimizing to the right metric.",
            "So I spent a little bit of time figuring out how to train a neural net to not optimize squared error, but to optimize in ordering.",
            "A rank so that I would basically optimized orosi in RSC area.",
            "So that's why this is now called a rank prop output.",
            "That's the algorithm I invented for training networks to optimize to RSC.",
            "Turns out all these other things were still doing squared error on those, but we're now optimizing this tomorrow.",
            "See now.",
            "It all works perfectly.",
            "So it turns out well.",
            "First of all, one thing is very interesting.",
            "Get rid of these extra outputs.",
            "Just train the thing to optimize the ordering.",
            "All of a sudden the ordering is 40% better then we could achieve just optimizing the squared are so.",
            "Optimizing to the right thing really did make a big difference, and then the good news is if I add these extra tasks to the up with the network, we get another 10% improvement in the ordering and now we get a 50% total improvement.",
            "So so that's good.",
            "But this is the first time I recognized how important it was to optimize to the right thing, and that different metrics really meant different things.",
            "So, and that's what we're going to talk about.",
            "So the question.",
            "Typical values for mortality rank on the training.",
            "The training data.",
            "All we know is lives or dies for the patient.",
            "We just have a Boolean.",
            "Turns out you died and you die times.",
            "I'm sorry everyone else lived so all we know is this very noisy signal that a patient was so high risk that ultimately they died.",
            "We'd much rather have some intermediate value, like your probability of dying is .8 or two, but of course we don't know that and the doctors don't give us that information.",
            "Knowing whether you died is a very noisy thing, by the way, because there most high risk patients, even with with pneumonia will live because medicine does everything they can to save them, and there are a few lower risk patients who do die because they fall through the cracks in the healthcare system.",
            "They just don't don't get the care that they should get, so it's a very noisy signal.",
            "It doesn't necessarily mean if you died, you probably were high risk, but not necessarily the opposite, though, is not really true.",
            "If you lived, it doesn't mean you were low risk.",
            "It just means they were able to save you, so yeah, so it's a very noisy thing.",
            "And it turns out, that's one of the reasons that's not what the talk is about.",
            "I could talk about learning to order things in a separate talk, but one of the reasons why minimizing squared error yields such different results than trying to optimize the ordering is be cause that signal is so noisy.",
            "Yeah, the noisier that signal, the more those two measures disagree or can disagree.",
            "But let's say if you want to optimize this ranking with with such training data, did you have?",
            "Any kind of sense?",
            "What kind of what was the right ordering?",
            "So no, I mean, it's weird, right you have.",
            "85 to 90% of the patients all all live and then depending on the data set, the sample you've got 5 to 15% die and you have no ordering within the 85% who live.",
            "And you have no ordering within the five to 15% who died.",
            "You don't know how to order those.",
            "The only thing you know is for the most part.",
            "The ones the people who die should be ordered above the ones who live.",
            "So this is basically, but even that's not true, right?",
            "'cause you know that probably there exist.",
            "The patients who died who had the least risk probably or less risk than the patients who lived who had the most risk.",
            "So you know that in fact the ordering should mix a little, but you don't have that information, it's part of what makes it a hard learning problem is that your signal is so noisy compared to what you need to learn.",
            "Yeah.",
            "So it's one of the reasons these other tests help too, is it turns out, if it turns out another task we put on the network sometimes is how long you were in the hospital.",
            "And that seems kind of crazy, but it turns out that if I say oh he had pneumonia, he was in the hospital for a day and then they.",
            "Dismiss them beyond.",
            "Sorry you weren't very sick.",
            "Suppose I say we had an ammonia in the hospital for a month.",
            "The odds are you were extremely ill, so it turns out that having an extra measure here of how long your hospital stay is gives you some interesting intermediate information about how seriously ill the patient probably was, and then that helps you predict the risk.",
            "So it's a noisy signal though, because a patient might be in the hospital only for one day.",
            "If they die after the first day, so so it's a difficult.",
            "It's a difficult signal to interpret, but OK, so this is just motivation.",
            "The big picture is.",
            "Chase different metrics mean different things, and if you're not optimizing to the right metric, it may not do what you expect.",
            "OK, now I have a problem.",
            "I understood the problem.",
            "I was very motivated to solve this problem.",
            "So I came up with this rank prop method.",
            "I had to invent a new learning algorithm for neural Nets to."
        ],
        [
            "To solve this problem well, as you saw in my previous talk, you know we've got all these different learning methods around every time we have this incompatibility between two metrics to somebody who's working with this particular learning method have to now go and sort of find a way to get around that particular incompatibility for that learning metric, right?",
            "There's a dozen metrics were interested in.",
            "There is easily 20 or 30 learning algorithms.",
            "That means we have to do sort of 100 problems like this, and nobody wants to do that, right?",
            "It's the wrong way to approach it.",
            "It's not general enough."
        ],
        [
            "OK, so let's let's go to some other motivation.",
            "So I worked one of the problems that was in the other talks I did was this particle physics problem and here we've got this particle and its anti particle.",
            "They call him B&B Bar an we have about 75 parameters from the particles that describe the particles track through a cloud chamber.",
            "So we have about 75 parameters and our job is to distinguish the bee from the bee bar particles and the parameters that you can measure in the track.",
            "Or not really sufficient to do this perfectly.",
            "A human expert can't necessarily do it.",
            "It's a probabilistic thing.",
            "You can say, oh, I'm 90% certain that sub particle as opposed to be bar particle.",
            "So that's what we're doing and the people at the Stanford Linear accelerator.",
            "They were already using machine learning to do this, so they don't have any other approach to doing it.",
            "So they were using machine learning to classify these tracks as being be bar.",
            "And they had this measure that they created.",
            "Called the SLQ score, the Stanford Linear Accelerator, Q score and Accuscore in some sense it measures how large of a statistical sample, what they're actually trying to do, is estimate some fundamental property of the universe.",
            "You know some fundamental constant, and it turns out that the better this Q score is, the closer it gets to one, the larger their effective sample size from doing their experiments.",
            "So suppose the Q score was very low, well then it means that they have to do billions of trials.",
            "To get a good estimate of this parameter, suppose the Q scores very high.",
            "They might only need thousands of trials to get a good estimate of this fundamental parameter, so this is a score that's a measure they specially created.",
            "You wouldn't want to use it, probably for anything else, but it does.",
            "They know exactly what problem they're working on.",
            "They know exactly how these things go into that prediction of the fundamental part of the fundamental constant, so they could come up with a very good score to measure how well the experiment was running.",
            "And this is a score of how well learning is doing classifying these things you can never.",
            "And she perfection, 'cause we don't give up with this or with some deep understanding.",
            "Physicists are very smart.",
            "So it's a complex formula and they gave it to us at some point and we coded it up.",
            "That wasn't so hard and we started using it.",
            "It didn't threaten to send it, but we did.",
            "Eventually we did eventually, and I'll tell you one hint about we don't fully understand it.",
            "In fact, at some point we argued with him that maybe it wasn't the right thing, but.",
            "Now they understand it yeah, so I mean they had very good.",
            "They can drive it.",
            "They can say starting from the following assumptions.",
            "Given that this is what we want to do, here's why this is our effective sample size.",
            "So you can think of it as a multiplier.",
            "If this is .5 and you.",
            "Capture a million tracks.",
            "You only really get a sample size of 500,000.",
            "If this is .7, then you capture a million tracks.",
            "Well now you get a sample size of 700,000 and it turns out it takes months to run the experiment to collect thousands of tracks.",
            "So if you can do just a 5% increase in this score, you save them over $1,000,000 in running the accelerator.",
            "Basically they can do one of two things.",
            "They can either run the accelerator less for the same quality or they can get higher quality for the same amount of runs.",
            "Either way they win OK so they tried a variety of machine learning methods and they got pretty good.",
            "Performance and then they got in touch with us and we tried some standard methods and we got results very similar to what they were doing so we didn't do any better than they were doing.",
            "This was like they were using.",
            "Cheryl Nets they were using decision trees and they tried SVM's so just sort of the popular methods that were lying around.",
            "So we tried those things to see if they were training them well and sure enough they were doing fine.",
            "So then what we do is we study this metric.",
            "We had multiple meetings, Johannes, Gehrke, E. Myself and a student.",
            "We had lots of meetings where we studied this thing and tried to understand it.",
            "Emails going back and forth calls to California.",
            "And I finally said, you know, this metric is very much like that.",
            "Did everybody see the talk where I talked about reliability diagrams?",
            "OK, I'll show you one of those in a second.",
            "It's very much like what goes into calculating a reliability diagram.",
            "If your if your predictions are well calibrated.",
            "You'll do pretty well on this score.",
            "So and I had done some work.",
            "This is older stuff I had before.",
            "All this probability calibration stuff with Platt scaling isotonic regression.",
            "Before I knew any of that stuff we had this problem and I had done some work before the show that's bagged.",
            "Trees had really excellent calibration, so we tried bagged trees in this because they had excellent calibration and we thought this metric was like a measure of calibration.",
            "Sure enough, we got much better performance on their SLQ score than we had ever been able to achieve before.",
            "Now that took some some work right we had.",
            "Understand this measure, decide it was similar to something else.",
            "Then figure out what seemed to work well at optimizing to that other thing.",
            "Try it and then sure enough, it works, so you."
        ],
        [
            "I know what bagging is, let me just show you so this is calibration.",
            "This is a graph you haven't seen, so here we're bagging trees.",
            "This is 1 tree 2345 up to 100 trees in the bag and this is calibration.",
            "Using that score that I used in the other talks.",
            "So down is we're not doing these normalized scores.",
            "Down is good on this score and if you look at it bagging improves the calibration by about an order of magnitude.",
            "It reduces the calibration error, so it improves calibration about an order of magnitude.",
            "That's a tremendous improvement on this measure.",
            "It doesn't have an order of magnitude increase in accuracy, but achieved sort of an order of magnitude decrease or improvement in calibration."
        ],
        [
            "And here's the cool thing.",
            "If you now look at the SLQ score, this is the kind of performance we were getting with a single decision tree.",
            "This is again as we bag up to 100 trees.",
            "Neural net, by the way, was somewhere around here.",
            "An SVM's were a little worse than this because they're not well calibrated and we didn't know how to calibrate them at the time.",
            "So those are the kind of performances were getting before a single tree was giving us poor performance.",
            "But if we bagged a lot of trees we got this really great.",
            "So Q performance much much better than we were ever able to achieve before.",
            "OK, so.",
            "The.",
            "The summary of this is by really understanding the metric they were interested in and then finding that it was similar to some other metric we knew about, and knowing that one learning methods seem really good at that metric, we could then guess that that learning method would work very well in this metric, and sure enough, that seemed to be true.",
            "So that's another case where knowing your metric turned out to be critical to doing the right thing."
        ],
        [
            "OK, so could we automate that sort of process of knowing that this metric is like this other metric and that's part of what we're going."
        ],
        [
            "Try to do."
        ],
        [
            "So third piece of motivation when you've already seen these sorts of tables right, we have all these different learning methods and they have different performance and all these different measures, measures, and some of them are better at different measures.",
            "This is all before calibration is being done.",
            "So things like neural Nets are really good at these numbers, but bagged trees and boosted trees are good over here."
        ],
        [
            "Here.",
            "So we already know from the other stuff we've talked about that.",
            "One, if you could predict the right probability.",
            "Almost any metric you'd be getting near optimal results, but it's actually very hard to predict the right probability.",
            "And just like in that medical domain where the patients are just living or dying as opposed to you having some estimate of their risk, it turns out it's actually very hard to predict the right probabilities if you don't need the right probabilities.",
            "This is maybe too hard of a thing to try to do from a small sample.",
            "Maybe you should try to do something simpler.",
            "It turns out that learning just to order some cases is easier than learning to predict the correct probabilities.",
            "'cause having the correct probabilities would entail the correct ordering, but having the correct ordering doesn't necessarily entail the correct probabilities.",
            "So it turns out that this is easier, even easier.",
            "Suppose all you need is accuracy.",
            "Well now you just need things to be on the right side of a threshold.",
            "You don't even need the right orderings of the things that are on the left side or the right ordering of the things on the right side.",
            "You just need that at the threshold you've got the right separation.",
            "That's even easier than predicting and ordering so often, you just don't need the ideal model that could predict the right probabilities.",
            "And in practice, what we do is we use whatever metric is appropriate to the problem we're working on.",
            "Sometimes rarely it's a domain specific measure.",
            "Well, it's funny.",
            "You know machine learning we're always using accuracy, precision, recall squared error when you get out into the real world.",
            "Those aren't the measures, right?",
            "The real measures are things like dollars of profit.",
            "After you take out the loss of doing the transaction, you know when you're betting on the stock market, right?",
            "And that's a very different number measure than any of these measures, so.",
            "So so when you get to the real world, it turns out you often end up with very specific measures that you need to optimize too.",
            "So we're trying to find some way to sort of come to grips automatically with different measures.",
            "These measures really do represent different tradeoffs, so so it's important to use the right measure unless you have so much data that you can do essentially perfectly on the problem, then it doesn't matter at all so."
        ],
        [
            "OK, so let me jump into metrics.",
            "Now we're already we're already halfway through the talk, right?",
            "It's all motivation so far, but we're halfway there.",
            "So what have I got here?",
            "This is a model that just has two parameters in it, 2 weights.",
            "So think of this as being.",
            "Well, a neural net with just two weights in it or.",
            "I guess.",
            "Linear regression with one input variable and constant?",
            "I mean you know, so our models aren't really ever this simple, but that's the only way I can draw a picture and what we've got is we've got a simple problem where we have some real data and we have this very simple two parameter model and I'm showing you the performance of that model in squared error on this data set.",
            "And it turns out that's the minimum point.",
            "Let's see how good I am.",
            "That's the that's the minimum point right there.",
            "That's where the squared error is minimized, so that's the peak performance would be a weight of about .32 and .09 or something like that in this model.",
            "And then these are the level curves of equal performance falling off from that.",
            "OK, so that just sort of tells you what this space would look like.",
            "Of course we hope their model would quickly zoom in, do gradient descent or something like that, and end up right there.",
            "Then we have the best quarter.",
            "But here's."
        ],
        [
            "Different measures alright, so here's the RMS model that we just saw.",
            "Here's log loss, cross entropy, and it's pretty similar to squared over there.",
            "Very similar things in Spirit, but they're not exactly the same.",
            "They're optimized at almost the same place.",
            "That optimal point is almost the same as that optimal point.",
            "It's a little bit over to the right and a little bit down lower about there, but they're very similar.",
            "The shape of the level curves isn't quite the same, but they're pretty similar and they should be 'cause log loss and squared are are essentially the same sort of thing.",
            "But look at accuracy accuracy in this simple model class is in fact this entire hyperplane.",
            "OK, you have optimal accuracy anywhere along that hyperplane and then your performance falls off to either side.",
            "Look at our OC area area under the RC curve.",
            "You also have a hyperplane is optimal performance, but it's not that hyperplane OK so so the model that's going to give you the optimal accuracy is going to be up there somewhere in the model that gives you optimal orosi is going to be down here somewhere.",
            "They're not the same models, so really you do need different models to optimize different metrics here.",
            "And note that that point and that point it doesn't fall on either of those straight lines.",
            "OK, so the model that optimizes log loss and squared error isn't going to optimize accuracy in RFC.",
            "Here's a really bizarre graph.",
            "It has local local Maxima, local minima actually in the way.",
            "This is represented.",
            "This is this calibration measure which we talked about, and that's a really funky measure, and it turns out that that's the optimal point, which again isn't the same as any of those, although it's pretty close to being on the maximum accuracy line, but you can do pretty well right there.",
            "That's actually just a tiny bit off of being optimal.",
            "If you look at the level curves, and you can do almost as well there, and calibration is strange.",
            "Alright, so here's something I didn't tell you before I told you.",
            "Calibration was a weird measure, but let me tell you how weird it is.",
            "Suppose your data set is 70% zeros 30% positives.",
            "If you just predict .3 for everybody.",
            "You're perfectly calibrated.",
            "Right, and if you think about our histogram method, everybody would fall in one bin to bin.",
            "That's sort of centered .3, and 30% of them would turn out to be positives, and you would have predicted .3 on average for all of them.",
            "You nailed the calibration.",
            "Now your reliability diagram would be just one point.",
            "At that bin you'd have perfect calibration though, so the weatherman, who just predicts what did I say that?",
            "I thought the book was telling me that it rained fifth half of the days, 50% of the days in June in Slovenia, so the weatherman, who just says every day in June, 50% chance of rain tomorrow, is apparently perfectly calibrated.",
            "It's not a very useful prediction.",
            "They're not accurate there.",
            "Squared error isn't necessarily good, but they've got perfect calibration when they say .5 there, right?",
            "OK.",
            "So this is a weird measure and that's why you get this sort of weird behavior.",
            "I mean that looks scary and very interesting.",
            "So how do you get that that you have a model with two right right?",
            "When you say optimizing this measure, that's right.",
            "Use the model right she's optimizing for that, yes.",
            "So what we actually did was this model is so simple that we could exhaustively enumerate all possible values of the model, right?",
            "So we picked every possible combination of AW1 and W2, figured out what the performance of the model was on the quote train set, and then we could plot this graph based on that.",
            "Right now the way you would normally do it.",
            "Yes, yes, and the way you would normally do it of course, as you would start at some point in the space and then you would do gradient descent to get stuck in local minima.",
            "And for most of these measures in this two parameter very simple to perimeter model, you would with gradient descent get to an optimal point for the training status, although this one you wouldn't, you could get stuck there, there or there.",
            "Yeah, of course, but would you be able to say, let's say if we would go to more than two parameters and this intuition would be just exactly same?",
            "So the intuition is similar, but the things get much more complicated, right?",
            "Yes.",
            "Yes, yeah.",
            "I mean that's the whole.",
            "No, no, they're they're really different.",
            "Yeah, yeah, I mean different measures really do measure different things and.",
            "Yeah, so I'll show you more of that.",
            "I mean, that's what the whole talks about.",
            "So yeah, it's amazing.",
            "I mean, it's really critical unless you're doing incredibly well.",
            "It's really critical to optimize to the right thing.",
            "Because the odds are you'll only be optimal on the thing you optimize to and not necessarily on other measures.",
            "So when you see people who are like training to minimize squared error, if you use decision trees, you're often doing information gain or gain ratio in the splits or things like that, and then they report performance on, say, F score, precision, recall, break, even point or something like that.",
            "It could be variance, right?",
            "It could be.",
            "It could be that you've got two methods A&B.",
            "Each train to their own respective measure, like a neural net trained isquared R decision tree trained to information gain or gain ratio.",
            "So you have two different measures.",
            "You train these models to their respective measures and then you look at their F scores and you say oh this one is better than that one.",
            "It may just be that the measure that you were naturally training this one two was better than the measure of the other or could just be variant.",
            "It could be that they both differ from F score in some sort of noisy systematic way, and it's just variance that makes one look better than the other.",
            "It's not that this algorithm is necessarily consistently better than the other on this class of problems, it may be just just an artifact of the measures that you used.",
            "So.",
            "You optimally one measure, then you're also invalid.",
            "How could you see that from OK?",
            "So if you really could get to perfect performance?",
            "On an infinite training set.",
            "Then these things would all have to be the same, right?",
            "It's only because these are.",
            "That's a great question.",
            "I should have made that clear.",
            "It's only because these are finite training sets that these things happen, yeah.",
            "Point we just point on the four of them will be in the same way.",
            "This sub WC would also like right right?",
            "Except for calibration, which is this oddball where you could be perfectly calibrated in at least two different ways, one of which is if your data set is P percent positives, just predict point P for everything, which is a sort of useless model you know.",
            "Or you could actually get everything right and get the squared over to 0.",
            "And then you'd also be well calibrated.",
            "Those will both have the same calibration.",
            "It turns out now, which means this is a bad measure.",
            "Calibration is not a not a good measure.",
            "You would notice if you look at the reliability diagram 'cause you'd see in one case you had just one point on the diagram and in the other case you'd have a full distribution on the diagram and you prefer that.",
            "So these are very very good questions.",
            "I mean, that's exactly exactly right.",
            "So.",
            "OK, sure.",
            "I'm truly.",
            "Then you can't.",
            "That's a yes yes yes.",
            "So imagine that your model can't explore all these possible values in this space, right?",
            "Some subset of these points is iaccessible to your model.",
            "Well, then the best year model can do.",
            "In the ideal case is get to the point that's nearest, but has the highest performance on a level curve that gets gets nearest to the optimal right.",
            "For this reason or stumps or something, right?",
            "Right, like stumps can't do it, yeah.",
            "Yeah.",
            "But it's really interesting to see this intuition which we are talking about all the times.",
            "It's a nice diagram, so yeah."
        ],
        [
            "OK, so let's get to.",
            "I mean that was all sort of set up, but it turns out that most of the interesting stuff 'cause the rest is pretty straightforward, so we're going to use 9 performance measures.",
            "I'll actually use a tent you've seen a lot of these performance measures before, and our goal is to discover relationships between the performance measures as automatically as possible.",
            "You know we want to do things like if you were to optimize to X, like train your neural net to squared error.",
            "How well would you do on some other measure wire?",
            "If Y is a lot like X?",
            "Then it's OK to optimize to actual, probably do well and why.",
            "If Y is really different from Max and you optimize decks, then all bets are off for why so that sort of thing and we'll talk a little bit about which of these metrics seems safer than the others, and if we could, we'd like to design new, better metrics, but we failed at doing that, will show you why."
        ],
        [
            "I've seen these.",
            "You've seen the threshold metrics, the ordering metrics, probability metrics, and we've got one new metric in here.",
            "Sorry, well, it's a bad name in English.",
            "I don't know if it's bad here.",
            "Sorry, is just a combination of accuracy area under the RC curve and squared error.",
            "Those are my three favorite measures, so it's just a combination of those things and it's really, really simple.",
            "It's basically just the average of them and squared error goes the other way.",
            "It's just one minus and so it's just very very simple and this is almost thrown in.",
            "There is a quick test to see if this would be any better than those things and you'll see it showing up in the diagram so it sort of forms an anchor point.",
            "Party I should probably end by 1, right?",
            "Start coming in.",
            "Yeah you guys know what accuracy is.",
            "I already briefly explain what lift."
        ],
        [
            "Was this is a lift curve?",
            "If you've never seen one, this is how many people you send the mailing too.",
            "If you send the mailing only to the top 10% that you think are most likely to respond, you'll have a lift of about 4.5.",
            "If you send it to the top 20%.",
            "With this model you have 3.5, you'll do 3 1/2 times better than random prediction.",
            "By using this model and eventually if you send your mailing to everybody, your lift by definition has to be one 'cause you can't do better than random.",
            "If you send it to everybody matter, how smart your model is so.",
            "So that's a lift curve.",
            "Precision, Recall F score, break even point."
        ],
        [
            "I think everybody knows those things.",
            "There's your typical graph.",
            "There are relationships between these things that not everybody knows about.",
            "It turns out, if you set your threshold.",
            "Suppose there are 30% positives in your data set and you set your threshold so that you return 30% of the cases doesn't mean it's the right 30%, but you always return 30% as being positive.",
            "Then it turns out that the F score will be equal to the break even point.",
            "So it's simple math to work that out, but so there are relationships between all these different measures, and these things go by different names in different communities, right?",
            "True positives, false negatives, true negatives.",
            "True positive rate hits and misses correct rejections.",
            "False alarms.",
            "Depends on what community you're in, whether you're doing information retrieval, medicine, electrical engineering, you know what you call these things."
        ],
        [
            "What hits is sometimes used in information retrieval, right?",
            "You talk about a hit or a miss, so that's often used in information retrieval.",
            "It's also sometimes like false alarm is often used in electrical engineering.",
            "Correct rejection is also often used in wireless communications, so it depends on what community you're in.",
            "What names you put on these things?"
        ],
        [
            "But let's face it, they're all essentially the same thing.",
            "Orosi plots are related.",
            "I tend to like RC.",
            "It's got a sort of firmer statistical basis than the other ordering measures like precision and recall.",
            "This goes back to World War Two when people were looking at noisy radar signals in trying to reliably detect whether a plane was coming in or whether it was a bird or weather or whatever.",
            "So that's how they developed RC's.",
            "But RC is related to other things, like RC is a plot of two true positive rate versus false positive rate.",
            "As you vary the threshold, that's how you sweep out an RC curve and I'll show you one in a second.",
            "Well, it turns out that sensitivity is the true positive rate.",
            "So if you're familiar with sensitivity.",
            "In precision recall graphs, that's that's the true positive rate.",
            "So that's one of these axes.",
            "Well, sensitivity is equal to recall, and that's also equal to the numerator of the lift thing.",
            "So all these measures are sort of trying to measure the same thing from just slightly different perspectives, so dip."
        ],
        [
            "And what's important to you?",
            "And there's a classic RC plot.",
            "Diagonal line is random prediction.",
            "This is a pretty good RSC, an area of .9 that's nice.",
            "The ideal RSC curve would go straight up and then straight across.",
            "That means you had perfect ordering and this is the kind of thing I was trying to optimize with that rank prop thing.",
            "I had to invent for neural Nets.",
            "Calibration already told you about that everything is exactly.",
            "Whoops, I was afraid of that."
        ],
        [
            "There's a calibration plot that's actually a really good calibration plot, so this is the kind of thing you get from bad decision trees without any extra calibration step, and that's why we got such good SLQ performance.",
            "It turns out if you do this well on calibration, you degrade on that particle physics problem."
        ],
        [
            "OK, so let's go to the experiments.",
            "Some of the learning methods that we've used before."
        ],
        [
            "You already know all this stuff.",
            "This is a subset of the."
        ],
        [
            "The problems we were looking at in the experiments they talked about two days ago.",
            "Alot of the setup is very very similar.",
            "We're basically running the same set of experiments, but looking at the results in a completely different way.",
            "And it's, you know, fairly big, so it's not actually as big as the result of the experiments I talked about on Tuesday, but it's you know, it's still reasonably expensive to do this sort of thing.",
            "We've got to train, you know."
        ],
        [
            "100,000 models or something like that so.",
            "So here's the 1st result, so I'm just going to.",
            "There's lots of these.",
            "I could plot.",
            "This is accuracy on this axis, and this is this funny calibration measure in this other axis, and each of these things is a different tree learning type, so this is a neural Nets all these red things.",
            "These blue things that are called seed.",
            "That's just vanilla decision trees.",
            "These are phmsa.",
            "These purple boxes I'm glad we're not using the cable 'cause color would be critical for this.",
            "These little things are K nearest neighbor, which you'll see really span the space.",
            "These things over here are boosted trees, boosted decision trees, and the yellow things are boosted stumps and the black dots are bag trees.",
            "We have far more of these now, but let me just walk you through this diagram.",
            "So good accuracy is this way and good calibration is that way deviation from the diagonal line calibration plot.",
            "Well boy, it really stands out that boosted trees have excellent accuracy, right?",
            "Some of the best boosted trees, not all boosted trees are great, by the way.",
            "But some of the very best ones have extremely high accuracy on this particular problem, which is great, but their calibration is just terrible.",
            "This is before applying something like Platt's method or isotonic regression.",
            "By the way, if we were to apply Plasma, I haven't done it, but if we apply Platt's method or isotonic regression and redrew this graph.",
            "All of those points would suddenly squish way down into a sort of noisy band.",
            "That would be about that height.",
            "Well, they'd get almost.",
            "They get that low actually, so they'd all be in this little noisy band down here, because Platt's method and isotonic regression would improve their calibration that much.",
            "But this is before we've done anything like that, so you really can see the different differences in models and neural Nets when you see a thread like this like there's an arc there in a thread there, that's often the same neural net just being trained for a different number of epochs, so it says you keep training it and take different snapshots of it.",
            "But so the big picture here is, it really does matter what measure you would want.",
            "I mean, there's no wonder can here.",
            "Perhaps a really good one would be this K nearest neighbor model.",
            "It has pretty good calibration and it has pretty good accuracy.",
            "But if you want the very best accuracy, you'd have to pick one of those points and you'd lose a lot in calibration.",
            "If you wanted the very best calibration you'd have to pick this boosted stump, then you'd lose a lot in terms of accuracy, so there really are tradeoffs to make, and the most interesting thing about this graph is the trade off is not just defined by the problem, it's defined by the learning method you're going to use on that problem.",
            "What that tradeoff looks like.",
            "Because if you're using a learning method then you can only restrict yourself to one color of these points, so your trade off is just going to completely different depending on what learning method you're using.",
            "So and as I said, if we did this calibration, it would suddenly change the tradeoff tremendously, 'cause that access would then collapse.",
            "But if that access was area under the RC curve where we don't know a way to suddenly fix that, so so you have to live."
        ],
        [
            "With that distinction, OK, I can show you a bunch of these.",
            "They are fun, but.",
            "Let's jump in to the fun stuff.",
            "I think most of you know what multidimensional scaling is, right?",
            "So we're just trying to find a low dimensional representation of the data that preserves the distances between points, so it's multidimensional scaling.",
            "So what have we got?",
            "We've got 10 performance measures here, so 10 rows, and then we've trained 14,000 models, so we have 14,000 columns here, an for every model we measure its performance on each of these measures.",
            "OK, so we've got 14,000 miles 'cause it's 2000 models for each of seven problems, so that's where we get 14,000.",
            "So imagine filling in that table with numbers.",
            "And of course we do that all the time."
        ],
        [
            "Time when we do these experiments, but I'll save you that one table, full numbers, and then we've got this problem I mentioned before, which is, you know, some metrics up is good, some down is good, some in the middle is good, so there are different ways of doing things.",
            "One is these normalized scores where zero is baseline and one is the best.",
            "That's the kind of thing we used in the talk on Tuesday.",
            "That's a weird kind of scaling.",
            "Another way to scale things is just by the standard deviation that you actually observe on that performance measure, and a third is.",
            "Let's not really scale.",
            "Let's just rank the things first best next best next best next best.",
            "According to that measure, and then we'll just take their rank and do do nonparametric statistics like rank coral."
        ],
        [
            "So those are different ways of looking at it.",
            "If we do multidimensional scaling and look at the stress, it turns out by time you get to 2 dimensions right?",
            "The original problem is 10 dimensions.",
            "By the time you get to 2 dimensions, you've already illuminated more than 85% of the variance by time you get to 3 dimensions.",
            "You have maybe 7 or 8% of the variance left.",
            "Definitely by the time you get to 5.",
            "Dimensions you sort of nailed it, and this is sort of just noise out here so so, even though we have 10 metrics to begin with, there really only measuring something like 2, three, or four.",
            "Different dimensions OK, so so that's an interesting fact right away.",
            "You might have thought maybe that all 10 metrics each measured something genuinely different, and that's not the case, and you shouldn't be surprised, right?",
            "Right, right?",
            "So certainly orosi area, an average precision and precision recall break even point.",
            "Boy, those had better be pretty similar to each other, 'cause we almost use them interchangeably and it will turn out there.",
            "The most similar of all three.",
            "So that's good news."
        ],
        [
            "So let's look at some of the two D. MD's plots.",
            "I'd look at the 3D plots, but they're just harder to see and I didn't bring 3D glasses, so will look at the 2D plots and will look at the 2D plots where MD S has been done.",
            "Multiple ways either scaling by normalized scores by standard deviation or by ranks.",
            "Let's just look at them and I'll do some MD's plots that are just across all of the 7 problems and I'll show some MD's plots for each of the problems individually will turn out there."
        ],
        [
            "Pretty much the same.",
            "This is probably the main result, right?",
            "I mean, half of the talk was motivation.",
            "And then this is the other half of the talk.",
            "Is that slide so, so what's this?",
            "It's a 2D MDS plot.",
            "All of our measures are here.",
            "Calibration log loss squared error.",
            "That's that measure that combines squared error accuracy.",
            "An area under the RC curve.",
            "Here's lift area under the RC curve, average precision, precision, recall, break, even point accuracy in the F score.",
            "So now what's the idea?",
            "Is that things that are similar to each other should be located near each other in this plot, and things that are far away or more likely to be different from each other.",
            "Well, first of all, there's an oddball.",
            "He's not playing with anybody else.",
            "And of course, we already knew that, and sure enough, it's just sitting there distal in the curve, so so that's kind of good.",
            "If it's close to anything, it's close to squared R and log loss, which is good because most people would say that this was their preferred measure of measuring calibration anyway, would be say squared R or maybe log loss, so so it's nice that this measure calibration.",
            "If it's near anything, is near those things 'cause all three of these remember, these are the three things that I called probability measures, and I made that classification before I knew this diagram, so.",
            "So it's nice that the diagram in some sense is respecting my category.",
            "XR, which combines squared error area into the RC curve in accuracy, is somewhat, appropriately enough, kind of in the middle of those things will turn out soars.",
            "A disappointment.",
            "So don't worry about it too much, but it's kind of in the place where you'd expect.",
            "So that's further evidence that the MD's plot is kind of done the right thing.",
            "Even point average precision and area under the RC curve, these are clustering tightly together and in fact average precision and area under the RC curve are the closest to of any of these metrics.",
            "There essentially measuring the same thing.",
            "You can use them almost interchangeably.",
            "These other three that depend on the threshold F score, accuracy and lift.",
            "They don't cluster quite so well.",
            "It turns out if we do a 3D projection there closer than you think it's only when we squashed down to two D that they're not close to each other, but they still don't cluster so tightly.",
            "And the truth is, when we were coming up with these categories for the metrics we right away had a category here.",
            "Probability metrics we had a category here for ordering metrics and then we were kind of stuck.",
            "What to call the other three and we sort of decided they all depend on a threshold one way or another.",
            "Let's call them threshold metrics and.",
            "And so so it's not so surprising that they don't cluster so well because we had problems classifying them in the 1st place.",
            "OK, so this is kind of interesting.",
            "Another thing to notice is log loss is consistently going to be sort of off the edge a little bit while squared error is going to be beautifully placed in the center of most of these graphs, and one of the conclusions is that if you were only going to use one measure, that's the safe one.",
            "Not only is it low variance, it turns out this measure is the most correlated with all the other.",
            "Measures except for SAR, but this isn't much better, so it turns out that one's been around for a couple 100 years.",
            "We use it.",
            "That's just going to turn out to be a great measure.",
            "And then also these things are surprisingly interchangeable.",
            "Although the break even point is higher variance than average precision and AUC.",
            "So in fact, if you've got a variant situation, I'd use either one of those instead of that.",
            "Then it's interesting that accuracy, which is probably the favored of the three threshold metrics.",
            "That's also sort of closer to the center, and it's kind of funny, right?",
            "The measures that we probably trusted the longest.",
            "Squared error, accuracy, and AUC.",
            "They they sort of span in the middle of the space, not in this one."
        ],
        [
            "Quite so much.",
            "So is it the same?",
            "Disrespective planning for two minutes to two minutes I hope.",
            "OK, I'll definitely be done in 10 or less 5.",
            "No problem.",
            "So question yeah so.",
            "Did you try to test some kind of interpretation on?",
            "But now I mean we tried but we didn't succeed.",
            "No, but but there might be one.",
            "If somebody has an intuition, we'd love to get help there, yeah?",
            "Yeah, we were hoping that would fall out when we were doing the experiments, but it's not so obvious.",
            "So so this just shows that in one case we do rank correlations.",
            "The other case, this normalized scores ranking and scoring are very different things, and yet we get vaguely similar graphs.",
            "The graphs look different.",
            "The only reason they look different if you'll notice Cal is here, but it's not over here, and that's because if we're going to normalize scores where baseline is zero.",
            "Well, baseline on Cal is really weird.",
            "Remember, because just predicting equals .7.",
            "Would be optimal for everything, so it's a really weird baseline prediction for Cal is perfect calibration, so it's hard to normalize it scale.",
            "So when we working with calibration we often do this sort of rank measure instead.",
            "That's what makes the distortion, but if you look at it, if you squint and remove cow and then pull things down from here, you'll find there are actually pretty similar right RMS and cross entropy will then move in down here.",
            "These things will get pulled down a little bit."
        ],
        [
            "Pretty similar grass, and here's if we just look over six of the problems.",
            "It's fairly similar graphs from problem to problem.",
            "I never expected those other graphs were average across all 7 problems.",
            "I never expected we'd see this kind of consistency from problem to problem, but it's pretty good consistency.",
            "So the pictures kind of stable.",
            "Which I had hadn't thought.",
            "It's more variable.",
            "If you go to 3D.",
            "But in 2D, it's remarkably stay."
        ],
        [
            "We've also done an analysis just by doing correlation.",
            "That is, don't do MD S. Just look at the correlation of performance on one metric.",
            "How that correlates with performance?"
        ],
        [
            "Different measure turns out the results are very similar, so.",
            "Correlation is one across the axis, 'cause that's accuracy with accuracy, escorts F score.",
            "You only really need half of these numbers.",
            "You know the upper diagonal or I mean the upper half of the bottom half.",
            "Just look at a few of these, so accuracy have pretty good correlation with area under the RC curve, average precision break even point with these things.",
            "Not very good.",
            "Accuracy doesn't correlate very well with cross entropy.",
            "Here's the correlation between these three ordering measures.",
            "AUC, average precision break, even point is actually pretty high.",
            "Point 95.9 two point 9 two point 92.9 five.",
            "They correlate pretty tightly.",
            "It turns out the real oddball is calibration, which doesn't correlate very well with with most of these other things.",
            "And that's 'cause it really is a strange measure, so that's not.",
            "Not a surprise.",
            "In fact, we were going to stop using this single number to summarize calibration, 'cause it's too misleading if you're going to use this number, always use it in combination with some other performance measure, so use it as like a tie splitter.",
            "The average correlation with all the other measures is nice.",
            "Likes are really is pretty good.",
            "It has a .9 calibration.",
            "I'm sorry, a .9 correlation with respect to all those measures is a nice general purpose measure, but the truth is that RMS, which is right there, .872, that's almost the same, and that's the highest correlation of a single measure to all the other measures.",
            "So just like it was that point central in the graph, sure enough it has the highest correlation to all other measures.",
            "But basically the story we're getting from the correlation analysis is very consistent with the story we get from the MD's plots.",
            "So I'm just showing this to sort of further confirm that the MD's plots.",
            "Showed something reasonable.",
            "I don't expect you to jump into these numbers, and suddenly it's very good news that these are a mess because it's so old and so exactly.",
            "It's amazing, right, right, right, right?",
            "You know, maybe there were three other measures you know invented 200 years ago and they didn't survive the test of time.",
            "I have no idea, but it is interesting that RMS has proved to be so robust.",
            "And I mean, I always kind of liked it now.",
            "I really like it.",
            "So so there are funny things too.",
            "By the way, there's pretty good correlation between the ordering measures.",
            "In RMS in the following sense, especially if you get the ordering right, then Platt's method and isotonic regression can almost always recover good probabilistic predictions, so getting an ordering correct is almost a sufficient condition for getting good probabilities, which is very, very interesting.",
            "Yeah, yeah, so that's kind of neat.",
            "I mean that this is kind of constrained, which really push pushes the whole result in the right direction, right?",
            "You still have to do some extra work then to go from that ordering too good probabilities and it doesn't.",
            "If you could really learn good probabilities in the 1st place, there's a certain coarsening that happens when you do isotonic regression or Platt scaling to convert the ordering, two probabilities, there's coarsening effect.",
            "The coarsening is worse if your data set is small.",
            "If your data set is large, it's not much loss.",
            "It turns out you won't do quite as well as if you could predict perfect squared are in the 1st place.",
            "If the data set is small.",
            "Because of this coarsening effect, but you'll do close so learning a good ordering is almost sufficient for getting good RMS.",
            "In fact, if the sample sizes are large, it is sufficient so.",
            "OK, so so."
        ],
        [
            "That's it, the metrics really only spend a few dimensions.",
            "As you might expect, we get pretty consistent results for more consistent than we ever expected across different problems.",
            "Different ways of scaling, doing MD S versus correlation analysis.",
            "They all seem to say roughly the same thing, and roughly what they say is, well.",
            "These ordering metrics are almost interchangeable, especially average precision in area under the curve .95 correlation there.",
            "They're very, very similar.",
            "Cal really is a bizarre guy, probably don't want to use it, certainly not alone.",
            "If it is near to anybody, it's near to the other things that look like probabilities, so that's good.",
            "RMS and cross entropy are close to each other as you'd expect, and you know I have people tell me Oh no I should be optimizing the cross entropy.",
            "It's the preferred thing.",
            "My own empirical experience has always been that they either yielded the same results or squared error was slightly epsilon better, so I've always favored RMS and now maybe I have some evidence that RMS actually is a more stable measure.",
            "Threshold metrics it's a weird family and they don't really cluster that.",
            "I mean they cluster roughly, but not that tightly.",
            "And this or metric is good.",
            "This thing that combines the three measures.",
            "But it turns out that RMS is so good to begin with that this doesn't really provide much that's useful.",
            "And I think that's it.",
            "So we have code that."
        ],
        [
            "Calculates all these measures by the way.",
            "So if you're interested that codes available on a website and we've got all these different models in their predictions.",
            "If anybody wants to borrow that, there's actually far more than this now.",
            "That's pretty much."
        ],
        [
            "So.",
            "Packers back to the first picture book ends.",
            "But no, that's pretty much it."
        ],
        [
            "So should get OK.",
            "Thanks for.",
            "Thanks for suffering through yet another talk.",
            "Actually one last question, should you try to come up with some?",
            "Special way of producing operational measure, which are which would really fit right nation is you coming out.",
            "There were no no something no the only thing we have from this work is if you give us a measure like SLQ we can throw it into the mix but we have all these models already trained.",
            "You give us a new measure.",
            "We can just quickly evaluate all those models on that new measure, so there's no new training to do, just a quick evaluation of 14,140 thousand models.",
            "It's easy, but that's not taken out, so that's it.",
            "Take you longer to write the code then to run it.",
            "Now you have a new.",
            "A new row in that table.",
            "Then you could redo MD S and you could see where that measure fell in the space, and I'm assuming, although I haven't done it, I'm assuming SLQ would fall down between squared error and calibration and therefore anything that's good at that end of the space, such as bag trees should be good on ESO que.",
            "So that's the one thing you could do is you could take a new custom measure and you could instantly put it in in a couple hours.",
            "Put into this space and suddenly no, oh, that's kind of like this other thing.",
            "These learning methods are good at that thing.",
            "Therefore they should be good on this new measure.",
            "That's the only this will be.",
            "But about discarding measures by some, let's say if it's like this robustness, and I think you counted some some more.",
            "We are trying.",
            "So we think that one of the things that makes attributes differ a lot his variance.",
            "So there can be attributes that are measuring a very similar thing, but one is a much higher variance version than the other, and it's one of the reasons why learning and ordering.",
            "May not be the best way to learn squared error.",
            "It turns out that it's easier to overfit in ordering then it is to overfit squared error squared error is beautifully low variance.",
            "I mean it's got so many nice properties.",
            "Low variance is one of the man.",
            "That's great.",
            "It means that it's harder to overfit to it ordering.",
            "You know if you change the value of two predicted cases just a tiny bit in the 5th decimal place, they may swap places.",
            "And suddenly the ordering is different.",
            "In one looks, one ordering looks better than the other in terms of squared error you've made just.",
            "An insignificant change, but in terms of ordering, suddenly something has popped past the other, and if you make a bunch of those little tiny changes, you can think you've improved ordering quite a bit, when in fact you've just made these little epsilon changes to predictions that don't amount to much and probably aren't.",
            "They have low margin, so one of the things we're trying to do is come up with a way of its work we're doing right now is we're trying to come up with a generalized notion of margin that will work with any performance measure.",
            "OK, because then if you could do ordering with margin for example.",
            "Now it would be a great great measure it be low variance then.",
            "And Torsten has also done some work on this, yes.",
            "I guess we'll have to.",
            "Sure, sure.",
            "A couple of more questions, actually."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for those of you who are sitting through now, the third talks.",
                    "label": 0
                },
                {
                    "sent": "That's right, so there's another informal talk this afternoon, and then one more talk tomorrow.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "So, OK, hopefully you don't get tired of me.",
                    "label": 0
                },
                {
                    "sent": "So this is a paper we presented at KDD conference two years ago 2004, but I also gave this talk a year ago on Halloween All Saints Day and.",
                    "label": 0
                },
                {
                    "sent": "So that's why I have these are pumpkins that my wife and I carved last year.",
                    "label": 0
                },
                {
                    "sent": "So this is a picture from our house, but.",
                    "label": 0
                },
                {
                    "sent": "So the talk really is data mining in metric space, but spooky stuff.",
                    "label": 1
                },
                {
                    "sent": "I don't know if that word means something to you, but this is joint work with one of my grad students, Alex Niculescu, who's also been joint on all of the other things I've talked about, so he'll be graduating year really good student.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me start off with some motivation and in fact the motivational takes sort of five minutes to set it up.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty simple talk, so nothing too fancy is going to happen.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many of you know.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm the guy who started doing this work on multi task learning training models on a dozen or 100 tasks at the same time, with the hopes that you get some inductive transfer.",
                    "label": 0
                },
                {
                    "sent": "That is, some of what you learn for this task helps you learn this other task better.",
                    "label": 0
                },
                {
                    "sent": "So what you learn from that tells helps you learn that other tasks better and basically as long as you have some shared representation and the tests are related to each other, it's often better to learn these things all at the same time as opposed to learning them as separate tasks.",
                    "label": 0
                },
                {
                    "sent": "So that's the same attribute set.",
                    "label": 0
                },
                {
                    "sent": "That's right, you have the same input attributes class.",
                    "label": 0
                },
                {
                    "sent": "Classes right so one example of this would be if any of you are familiar with Tom Mitchell's Calendar Apprentice system work from 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "There for a meeting, so he'd be trying to automatically schedule a meeting, say, with Donya, and he'd have to figure out where should the meeting occur should it be in Toms office, Daniel's office, or in some conference room or something like that.",
                    "label": 0
                },
                {
                    "sent": "What time of day should the meeting occur at?",
                    "label": 0
                },
                {
                    "sent": "How long should the meeting be?",
                    "label": 0
                },
                {
                    "sent": "Half an hour, hour, whatever?",
                    "label": 0
                },
                {
                    "sent": "And what day of week?",
                    "label": 0
                },
                {
                    "sent": "Should the meeting occur at and the rules really change depending on who Tom's meeting with he's meeting with the Dean of the University the meeting is going to be in the Deans Office, it's going to be anytime of the week, day or night, and it'll probably be an hour.",
                    "label": 0
                },
                {
                    "sent": "But if he's meeting with me, a graduate student, or Donya visiting student, it's probably going to be half an hour.",
                    "label": 0
                },
                {
                    "sent": "Most of the meetings were half an hour.",
                    "label": 0
                },
                {
                    "sent": "It'll be in Toms office.",
                    "label": 0
                },
                {
                    "sent": "It won't be in the mornings usually because that's time Tom reserved to write papers, so it would probably be in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "And for me it was always on Thursdays.",
                    "label": 0
                },
                {
                    "sent": "So, but I don't know about you so anyway.",
                    "label": 0
                },
                {
                    "sent": "So those are four different prediction tasks, but they're all predictions for the same event.",
                    "label": 0
                },
                {
                    "sent": "And they're not constrained in any interesting.",
                    "label": 0
                },
                {
                    "sent": "It's not a complex structure, but you still have to predict these four independent things to schedule a meeting.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out, if you were to learn a model that tries to predict all four things at the same time, some of what it learns about how long the meeting should be helps it decide where the meeting should be.",
                    "label": 0
                },
                {
                    "sent": "'cause there are correlations between these things, so, so anyway, that's this whole area now of what's known as inductive transfer.",
                    "label": 0
                },
                {
                    "sent": "This sub area that I work in is called multi task learning and the easiest way to do it is.",
                    "label": 0
                },
                {
                    "sent": "Now I know most of you don't know neural Nets, but so you have all these inputs to the neural Nets.",
                    "label": 0
                },
                {
                    "sent": "You have this big shared hidden layer.",
                    "label": 0
                },
                {
                    "sent": "All these connections to the inputs and then you typically have this task on the output of the network that gets to see all those hidden units well in multi task learning you've got multiple outputs and you're training all of these outputs in parallel and they all share this hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So what this task causes to be learned in this hidden layer is now visible by all the other tasks.",
                    "label": 0
                },
                {
                    "sent": "And they may benefit from it.",
                    "label": 0
                },
                {
                    "sent": "So if anybody is interested in that, I can give a talk on that.",
                    "label": 0
                },
                {
                    "sent": "You know we can do A6 talk, but it's easiest to see that in neural Nets and if you guys aren't into neural Nets that much, it might not be as exciting for you.",
                    "label": 0
                },
                {
                    "sent": "Although you can do this in everything.",
                    "label": 0
                },
                {
                    "sent": "You can do this in SVM and decision trees.",
                    "label": 0
                },
                {
                    "sent": "All that sort of stuff, but it's really easy to see in neural net and I did most of the work with neural Nets were very popular, so so I also did a lot of the work in neural Nets.",
                    "label": 0
                },
                {
                    "sent": "So here's a neural net.",
                    "label": 0
                },
                {
                    "sent": "So back to our motivation for the data mining in metric space.",
                    "label": 0
                },
                {
                    "sent": "Here's a neural net.",
                    "label": 0
                },
                {
                    "sent": "These are the input variables.",
                    "label": 0
                },
                {
                    "sent": "That's the big shared hidden layer, and these are the multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "This is the output that really counts were trying to predict your risk of dying from pneumonia.",
                    "label": 0
                },
                {
                    "sent": "OK, so everybody in the room hasn ammonia, and we're just trying to figure out low risk, low risk, low risk, high risk, sorry, low risk, high risk, low risk, lower stroke, and the people who are low risk.",
                    "label": 0
                },
                {
                    "sent": "We send you home, we give you antibiotics.",
                    "label": 0
                },
                {
                    "sent": "Chicken noodle soup.",
                    "label": 0
                },
                {
                    "sent": "Call us in three days if you're not feeling better.",
                    "label": 0
                },
                {
                    "sent": "The people who are high risk you and you.",
                    "label": 0
                },
                {
                    "sent": "You go straight to the hospital.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "'cause pneumonia is a dangerous thing if you're seriously ill and you need to be in the hospital.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to figure out here what patients are.",
                    "label": 0
                },
                {
                    "sent": "High risk or low risk from all these measurements we have.",
                    "label": 0
                },
                {
                    "sent": "We know your age or gender, whether you have asthma, whether you have chest pain, you know what your cough sounds like, whether you're diabetic, we know all those things and we're trying to figure out whether you're high risk or low risk.",
                    "label": 0
                },
                {
                    "sent": "And then the thing we did with multi task learning here.",
                    "label": 0
                },
                {
                    "sent": "I won't spend too much time on it.",
                    "label": 0
                },
                {
                    "sent": "But we have all these other measurements about these patients because they were part of a special study.",
                    "label": 0
                },
                {
                    "sent": "We only have these measurements for patients who have already been in the hospital.",
                    "label": 0
                },
                {
                    "sent": "Turns out everybody in this study was in the hospital because of the study, so it's a very unusual situation 'cause it's expensive to put people in the hospital and you don't want to do it unless they should be there.",
                    "label": 0
                },
                {
                    "sent": "So we have all these measurements.",
                    "label": 0
                },
                {
                    "sent": "These are things like your white blood cell count and your partial pressure of oxygen in your blood and all those sorts of things.",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of tests we only measure if you're in the hospital.",
                    "label": 0
                },
                {
                    "sent": "And what we're doing is we can't use these as inputs for this model, because we're going to use the model in practice for patients who have not yet been in the hospital to determine who's high risk or low risk.",
                    "label": 0
                },
                {
                    "sent": "So rather than throw this information away, it turns out if you put it on the output side of the network and use it as extra tasks, some of what you learn trying to predict the white blood cell count in that hidden unit, and you end up predicting mortality better.",
                    "label": 0
                },
                {
                    "sent": "The risk of mortality better.",
                    "label": 0
                },
                {
                    "sent": "So that's the inductive transfer multitest learning thing.",
                    "label": 0
                },
                {
                    "sent": "So now why is this motivation for our work in data mining in metric space?",
                    "label": 0
                },
                {
                    "sent": "Well, here's the annoying thing.",
                    "label": 0
                },
                {
                    "sent": "Like most back prop Nets, I was training this with squared are to minimize squared are on this data set, and here's what I found.",
                    "label": 0
                },
                {
                    "sent": "Suppose I didn't have these outputs.",
                    "label": 0
                },
                {
                    "sent": "These extra outputs on the network and I just trained in network with one output.",
                    "label": 0
                },
                {
                    "sent": "That's the usual way of doing things, and I trained it to minimize squared R. I'd get some squared error, let's say .1, so that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "Now I train the network again and I put these extra outputs on it, do back prop through all of these things at the same time, and something really good would happen.",
                    "label": 0
                },
                {
                    "sent": "The squared error here would drop from .1 to, say, .08.",
                    "label": 0
                },
                {
                    "sent": "So it's great.",
                    "label": 0
                },
                {
                    "sent": "I get a 20% reduction in squared error on the output.",
                    "label": 0
                },
                {
                    "sent": "That really counts.",
                    "label": 0
                },
                {
                    "sent": "By putting these extra things in the network, that's great.",
                    "label": 0
                },
                {
                    "sent": "I have a thesis, I get to graduate.",
                    "label": 0
                },
                {
                    "sent": "All those good things.",
                    "label": 0
                },
                {
                    "sent": "There's a problem the way they really evaluate these networks in the medical domain is not by looking at the squared error of the model.",
                    "label": 0
                },
                {
                    "sent": "They look at the Roc of the model.",
                    "label": 0
                },
                {
                    "sent": "So the Roc is a measure of how well it orders patients by risk.",
                    "label": 0
                },
                {
                    "sent": "That is, remember you're going to the hospital and you're going to hospital use.",
                    "label": 0
                },
                {
                    "sent": "We want to sort all these patients so that you 2 come up near the top as the highest risk patients and everybody else.",
                    "label": 0
                },
                {
                    "sent": "It's OK if you sort lower, that's fine, we don't really care what the squared error is.",
                    "label": 0
                },
                {
                    "sent": "As long as we get the sort right now, they're correlated measures OK.",
                    "label": 0
                },
                {
                    "sent": "If if we get this squared error, perfect, say near zero, well then we have to get the sort right.",
                    "label": 0
                },
                {
                    "sent": "If we sort on that prediction, but that's often not the case, especially when you train with small sample.",
                    "label": 0
                },
                {
                    "sent": "So here's the really annoying thing I consistently got.",
                    "label": 0
                },
                {
                    "sent": "Let's say a 10 to 20% reduction in squared error by putting extra outputs on this network.",
                    "label": 0
                },
                {
                    "sent": "That's great news.",
                    "label": 0
                },
                {
                    "sent": "That's a big improvement.",
                    "label": 0
                },
                {
                    "sent": "Don't know how to do it any other way.",
                    "label": 0
                },
                {
                    "sent": "Yet when we measured performance.",
                    "label": 0
                },
                {
                    "sent": "With borrow.",
                    "label": 0
                },
                {
                    "sent": "See how well we could order things.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it helps.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it hurts, sometimes it made no real difference.",
                    "label": 0
                },
                {
                    "sent": "That was really annoying on the metrics I was optimizing too.",
                    "label": 0
                },
                {
                    "sent": "I always got a significant improvement.",
                    "label": 0
                },
                {
                    "sent": "And yet, when I looked at a different metric, which is what the doctors were using.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I did, sometimes I didn't.",
                    "label": 0
                },
                {
                    "sent": "That means these two metrics were not telling me the same story and my method was working for the metric.",
                    "label": 0
                },
                {
                    "sent": "It was optimizing too, but not necessarily for some other metrics so correlated, but but but not enough, right?",
                    "label": 0
                },
                {
                    "sent": "And if I could get the squared error down very, very low then they would be even more correlated.",
                    "label": 0
                },
                {
                    "sent": "It turns out once you reach perfection, any reasonable metric is perfectly correlated with any other reasonable metric.",
                    "label": 0
                },
                {
                    "sent": "But when you're not near perfection.",
                    "label": 0
                },
                {
                    "sent": "You can be different from perfection in a variety of ways, so in different metrics measure different properties of how you differ from perfection.",
                    "label": 0
                },
                {
                    "sent": "This small detail which you mentioned, so if if you have this extra outputs then also the primary output gets gets better.",
                    "label": 0
                },
                {
                    "sent": "So yes this is work which you did right exactly 6 exactly.",
                    "label": 0
                },
                {
                    "sent": "That's this multi task learning work.",
                    "label": 0
                },
                {
                    "sent": "And what's fun about this is.",
                    "label": 0
                },
                {
                    "sent": "I sort of stopped doing a lot of this around 98 because I couldn't talk many other people into thinking it was very important and now it's all changed.",
                    "label": 0
                },
                {
                    "sent": "Now there's suddenly a huge community DARPA is throwing in millions of dollars to fund it, and so it's really.",
                    "label": 0
                },
                {
                    "sent": "It's really good.",
                    "label": 0
                },
                {
                    "sent": "So all of a sudden this has gotten popular again, which is great.",
                    "label": 0
                },
                {
                    "sent": "After about five years of nobody, everybody thinking, oh that's cool, but I'm doing something else.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so normally have this 50 years like the Russian changes and then you guys don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is nice.",
                    "label": 0
                },
                {
                    "sent": "Something I'm getting invited to go places to talk about this old work because yeah, and we're doing some new work in this so Alex Niculescu's thesis is in fact going to be multi task learning in Bayes Nets.",
                    "label": 0
                },
                {
                    "sent": "So OK, so what's the moral of this story?",
                    "label": 0
                },
                {
                    "sent": "Well, the moral of the story is I had a better technique.",
                    "label": 0
                },
                {
                    "sent": "It consistently worked on what I asked it to do, which was optimized squared error.",
                    "label": 0
                },
                {
                    "sent": "It made this output, which is very important.",
                    "label": 0
                },
                {
                    "sent": "Consistently say 20% better.",
                    "label": 0
                },
                {
                    "sent": "And yet, on the evaluation criterion that other people were using to evaluate the model, sometimes it helps.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it hurt, often just no big difference, which is very, very annoying.",
                    "label": 0
                },
                {
                    "sent": "So what I did was I thought I've got to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "I have to be optimizing to the right metric.",
                    "label": 0
                },
                {
                    "sent": "So I spent a little bit of time figuring out how to train a neural net to not optimize squared error, but to optimize in ordering.",
                    "label": 0
                },
                {
                    "sent": "A rank so that I would basically optimized orosi in RSC area.",
                    "label": 0
                },
                {
                    "sent": "So that's why this is now called a rank prop output.",
                    "label": 0
                },
                {
                    "sent": "That's the algorithm I invented for training networks to optimize to RSC.",
                    "label": 0
                },
                {
                    "sent": "Turns out all these other things were still doing squared error on those, but we're now optimizing this tomorrow.",
                    "label": 0
                },
                {
                    "sent": "See now.",
                    "label": 0
                },
                {
                    "sent": "It all works perfectly.",
                    "label": 0
                },
                {
                    "sent": "So it turns out well.",
                    "label": 0
                },
                {
                    "sent": "First of all, one thing is very interesting.",
                    "label": 0
                },
                {
                    "sent": "Get rid of these extra outputs.",
                    "label": 0
                },
                {
                    "sent": "Just train the thing to optimize the ordering.",
                    "label": 0
                },
                {
                    "sent": "All of a sudden the ordering is 40% better then we could achieve just optimizing the squared are so.",
                    "label": 0
                },
                {
                    "sent": "Optimizing to the right thing really did make a big difference, and then the good news is if I add these extra tasks to the up with the network, we get another 10% improvement in the ordering and now we get a 50% total improvement.",
                    "label": 0
                },
                {
                    "sent": "So so that's good.",
                    "label": 0
                },
                {
                    "sent": "But this is the first time I recognized how important it was to optimize to the right thing, and that different metrics really meant different things.",
                    "label": 0
                },
                {
                    "sent": "So, and that's what we're going to talk about.",
                    "label": 0
                },
                {
                    "sent": "So the question.",
                    "label": 0
                },
                {
                    "sent": "Typical values for mortality rank on the training.",
                    "label": 0
                },
                {
                    "sent": "The training data.",
                    "label": 0
                },
                {
                    "sent": "All we know is lives or dies for the patient.",
                    "label": 0
                },
                {
                    "sent": "We just have a Boolean.",
                    "label": 0
                },
                {
                    "sent": "Turns out you died and you die times.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry everyone else lived so all we know is this very noisy signal that a patient was so high risk that ultimately they died.",
                    "label": 0
                },
                {
                    "sent": "We'd much rather have some intermediate value, like your probability of dying is .8 or two, but of course we don't know that and the doctors don't give us that information.",
                    "label": 0
                },
                {
                    "sent": "Knowing whether you died is a very noisy thing, by the way, because there most high risk patients, even with with pneumonia will live because medicine does everything they can to save them, and there are a few lower risk patients who do die because they fall through the cracks in the healthcare system.",
                    "label": 0
                },
                {
                    "sent": "They just don't don't get the care that they should get, so it's a very noisy signal.",
                    "label": 0
                },
                {
                    "sent": "It doesn't necessarily mean if you died, you probably were high risk, but not necessarily the opposite, though, is not really true.",
                    "label": 0
                },
                {
                    "sent": "If you lived, it doesn't mean you were low risk.",
                    "label": 0
                },
                {
                    "sent": "It just means they were able to save you, so yeah, so it's a very noisy thing.",
                    "label": 0
                },
                {
                    "sent": "And it turns out, that's one of the reasons that's not what the talk is about.",
                    "label": 0
                },
                {
                    "sent": "I could talk about learning to order things in a separate talk, but one of the reasons why minimizing squared error yields such different results than trying to optimize the ordering is be cause that signal is so noisy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the noisier that signal, the more those two measures disagree or can disagree.",
                    "label": 0
                },
                {
                    "sent": "But let's say if you want to optimize this ranking with with such training data, did you have?",
                    "label": 0
                },
                {
                    "sent": "Any kind of sense?",
                    "label": 0
                },
                {
                    "sent": "What kind of what was the right ordering?",
                    "label": 0
                },
                {
                    "sent": "So no, I mean, it's weird, right you have.",
                    "label": 0
                },
                {
                    "sent": "85 to 90% of the patients all all live and then depending on the data set, the sample you've got 5 to 15% die and you have no ordering within the 85% who live.",
                    "label": 0
                },
                {
                    "sent": "And you have no ordering within the five to 15% who died.",
                    "label": 0
                },
                {
                    "sent": "You don't know how to order those.",
                    "label": 0
                },
                {
                    "sent": "The only thing you know is for the most part.",
                    "label": 0
                },
                {
                    "sent": "The ones the people who die should be ordered above the ones who live.",
                    "label": 0
                },
                {
                    "sent": "So this is basically, but even that's not true, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you know that probably there exist.",
                    "label": 0
                },
                {
                    "sent": "The patients who died who had the least risk probably or less risk than the patients who lived who had the most risk.",
                    "label": 0
                },
                {
                    "sent": "So you know that in fact the ordering should mix a little, but you don't have that information, it's part of what makes it a hard learning problem is that your signal is so noisy compared to what you need to learn.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's one of the reasons these other tests help too, is it turns out, if it turns out another task we put on the network sometimes is how long you were in the hospital.",
                    "label": 0
                },
                {
                    "sent": "And that seems kind of crazy, but it turns out that if I say oh he had pneumonia, he was in the hospital for a day and then they.",
                    "label": 0
                },
                {
                    "sent": "Dismiss them beyond.",
                    "label": 0
                },
                {
                    "sent": "Sorry you weren't very sick.",
                    "label": 0
                },
                {
                    "sent": "Suppose I say we had an ammonia in the hospital for a month.",
                    "label": 0
                },
                {
                    "sent": "The odds are you were extremely ill, so it turns out that having an extra measure here of how long your hospital stay is gives you some interesting intermediate information about how seriously ill the patient probably was, and then that helps you predict the risk.",
                    "label": 0
                },
                {
                    "sent": "So it's a noisy signal though, because a patient might be in the hospital only for one day.",
                    "label": 0
                },
                {
                    "sent": "If they die after the first day, so so it's a difficult.",
                    "label": 0
                },
                {
                    "sent": "It's a difficult signal to interpret, but OK, so this is just motivation.",
                    "label": 0
                },
                {
                    "sent": "The big picture is.",
                    "label": 0
                },
                {
                    "sent": "Chase different metrics mean different things, and if you're not optimizing to the right metric, it may not do what you expect.",
                    "label": 0
                },
                {
                    "sent": "OK, now I have a problem.",
                    "label": 0
                },
                {
                    "sent": "I understood the problem.",
                    "label": 0
                },
                {
                    "sent": "I was very motivated to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So I came up with this rank prop method.",
                    "label": 0
                },
                {
                    "sent": "I had to invent a new learning algorithm for neural Nets to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To solve this problem well, as you saw in my previous talk, you know we've got all these different learning methods around every time we have this incompatibility between two metrics to somebody who's working with this particular learning method have to now go and sort of find a way to get around that particular incompatibility for that learning metric, right?",
                    "label": 0
                },
                {
                    "sent": "There's a dozen metrics were interested in.",
                    "label": 0
                },
                {
                    "sent": "There is easily 20 or 30 learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "That means we have to do sort of 100 problems like this, and nobody wants to do that, right?",
                    "label": 0
                },
                {
                    "sent": "It's the wrong way to approach it.",
                    "label": 0
                },
                {
                    "sent": "It's not general enough.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's let's go to some other motivation.",
                    "label": 0
                },
                {
                    "sent": "So I worked one of the problems that was in the other talks I did was this particle physics problem and here we've got this particle and its anti particle.",
                    "label": 0
                },
                {
                    "sent": "They call him B&B Bar an we have about 75 parameters from the particles that describe the particles track through a cloud chamber.",
                    "label": 0
                },
                {
                    "sent": "So we have about 75 parameters and our job is to distinguish the bee from the bee bar particles and the parameters that you can measure in the track.",
                    "label": 0
                },
                {
                    "sent": "Or not really sufficient to do this perfectly.",
                    "label": 0
                },
                {
                    "sent": "A human expert can't necessarily do it.",
                    "label": 0
                },
                {
                    "sent": "It's a probabilistic thing.",
                    "label": 0
                },
                {
                    "sent": "You can say, oh, I'm 90% certain that sub particle as opposed to be bar particle.",
                    "label": 0
                },
                {
                    "sent": "So that's what we're doing and the people at the Stanford Linear accelerator.",
                    "label": 0
                },
                {
                    "sent": "They were already using machine learning to do this, so they don't have any other approach to doing it.",
                    "label": 0
                },
                {
                    "sent": "So they were using machine learning to classify these tracks as being be bar.",
                    "label": 1
                },
                {
                    "sent": "And they had this measure that they created.",
                    "label": 0
                },
                {
                    "sent": "Called the SLQ score, the Stanford Linear Accelerator, Q score and Accuscore in some sense it measures how large of a statistical sample, what they're actually trying to do, is estimate some fundamental property of the universe.",
                    "label": 0
                },
                {
                    "sent": "You know some fundamental constant, and it turns out that the better this Q score is, the closer it gets to one, the larger their effective sample size from doing their experiments.",
                    "label": 0
                },
                {
                    "sent": "So suppose the Q score was very low, well then it means that they have to do billions of trials.",
                    "label": 0
                },
                {
                    "sent": "To get a good estimate of this parameter, suppose the Q scores very high.",
                    "label": 0
                },
                {
                    "sent": "They might only need thousands of trials to get a good estimate of this fundamental parameter, so this is a score that's a measure they specially created.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't want to use it, probably for anything else, but it does.",
                    "label": 0
                },
                {
                    "sent": "They know exactly what problem they're working on.",
                    "label": 0
                },
                {
                    "sent": "They know exactly how these things go into that prediction of the fundamental part of the fundamental constant, so they could come up with a very good score to measure how well the experiment was running.",
                    "label": 0
                },
                {
                    "sent": "And this is a score of how well learning is doing classifying these things you can never.",
                    "label": 0
                },
                {
                    "sent": "And she perfection, 'cause we don't give up with this or with some deep understanding.",
                    "label": 0
                },
                {
                    "sent": "Physicists are very smart.",
                    "label": 0
                },
                {
                    "sent": "So it's a complex formula and they gave it to us at some point and we coded it up.",
                    "label": 0
                },
                {
                    "sent": "That wasn't so hard and we started using it.",
                    "label": 0
                },
                {
                    "sent": "It didn't threaten to send it, but we did.",
                    "label": 0
                },
                {
                    "sent": "Eventually we did eventually, and I'll tell you one hint about we don't fully understand it.",
                    "label": 0
                },
                {
                    "sent": "In fact, at some point we argued with him that maybe it wasn't the right thing, but.",
                    "label": 0
                },
                {
                    "sent": "Now they understand it yeah, so I mean they had very good.",
                    "label": 0
                },
                {
                    "sent": "They can drive it.",
                    "label": 0
                },
                {
                    "sent": "They can say starting from the following assumptions.",
                    "label": 0
                },
                {
                    "sent": "Given that this is what we want to do, here's why this is our effective sample size.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as a multiplier.",
                    "label": 0
                },
                {
                    "sent": "If this is .5 and you.",
                    "label": 0
                },
                {
                    "sent": "Capture a million tracks.",
                    "label": 0
                },
                {
                    "sent": "You only really get a sample size of 500,000.",
                    "label": 0
                },
                {
                    "sent": "If this is .7, then you capture a million tracks.",
                    "label": 0
                },
                {
                    "sent": "Well now you get a sample size of 700,000 and it turns out it takes months to run the experiment to collect thousands of tracks.",
                    "label": 1
                },
                {
                    "sent": "So if you can do just a 5% increase in this score, you save them over $1,000,000 in running the accelerator.",
                    "label": 0
                },
                {
                    "sent": "Basically they can do one of two things.",
                    "label": 0
                },
                {
                    "sent": "They can either run the accelerator less for the same quality or they can get higher quality for the same amount of runs.",
                    "label": 0
                },
                {
                    "sent": "Either way they win OK so they tried a variety of machine learning methods and they got pretty good.",
                    "label": 1
                },
                {
                    "sent": "Performance and then they got in touch with us and we tried some standard methods and we got results very similar to what they were doing so we didn't do any better than they were doing.",
                    "label": 0
                },
                {
                    "sent": "This was like they were using.",
                    "label": 0
                },
                {
                    "sent": "Cheryl Nets they were using decision trees and they tried SVM's so just sort of the popular methods that were lying around.",
                    "label": 0
                },
                {
                    "sent": "So we tried those things to see if they were training them well and sure enough they were doing fine.",
                    "label": 0
                },
                {
                    "sent": "So then what we do is we study this metric.",
                    "label": 0
                },
                {
                    "sent": "We had multiple meetings, Johannes, Gehrke, E. Myself and a student.",
                    "label": 0
                },
                {
                    "sent": "We had lots of meetings where we studied this thing and tried to understand it.",
                    "label": 0
                },
                {
                    "sent": "Emails going back and forth calls to California.",
                    "label": 0
                },
                {
                    "sent": "And I finally said, you know, this metric is very much like that.",
                    "label": 0
                },
                {
                    "sent": "Did everybody see the talk where I talked about reliability diagrams?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll show you one of those in a second.",
                    "label": 0
                },
                {
                    "sent": "It's very much like what goes into calculating a reliability diagram.",
                    "label": 0
                },
                {
                    "sent": "If your if your predictions are well calibrated.",
                    "label": 0
                },
                {
                    "sent": "You'll do pretty well on this score.",
                    "label": 1
                },
                {
                    "sent": "So and I had done some work.",
                    "label": 0
                },
                {
                    "sent": "This is older stuff I had before.",
                    "label": 0
                },
                {
                    "sent": "All this probability calibration stuff with Platt scaling isotonic regression.",
                    "label": 0
                },
                {
                    "sent": "Before I knew any of that stuff we had this problem and I had done some work before the show that's bagged.",
                    "label": 0
                },
                {
                    "sent": "Trees had really excellent calibration, so we tried bagged trees in this because they had excellent calibration and we thought this metric was like a measure of calibration.",
                    "label": 0
                },
                {
                    "sent": "Sure enough, we got much better performance on their SLQ score than we had ever been able to achieve before.",
                    "label": 0
                },
                {
                    "sent": "Now that took some some work right we had.",
                    "label": 0
                },
                {
                    "sent": "Understand this measure, decide it was similar to something else.",
                    "label": 0
                },
                {
                    "sent": "Then figure out what seemed to work well at optimizing to that other thing.",
                    "label": 0
                },
                {
                    "sent": "Try it and then sure enough, it works, so you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know what bagging is, let me just show you so this is calibration.",
                    "label": 0
                },
                {
                    "sent": "This is a graph you haven't seen, so here we're bagging trees.",
                    "label": 0
                },
                {
                    "sent": "This is 1 tree 2345 up to 100 trees in the bag and this is calibration.",
                    "label": 0
                },
                {
                    "sent": "Using that score that I used in the other talks.",
                    "label": 0
                },
                {
                    "sent": "So down is we're not doing these normalized scores.",
                    "label": 0
                },
                {
                    "sent": "Down is good on this score and if you look at it bagging improves the calibration by about an order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "It reduces the calibration error, so it improves calibration about an order of magnitude.",
                    "label": 1
                },
                {
                    "sent": "That's a tremendous improvement on this measure.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have an order of magnitude increase in accuracy, but achieved sort of an order of magnitude decrease or improvement in calibration.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's the cool thing.",
                    "label": 0
                },
                {
                    "sent": "If you now look at the SLQ score, this is the kind of performance we were getting with a single decision tree.",
                    "label": 0
                },
                {
                    "sent": "This is again as we bag up to 100 trees.",
                    "label": 0
                },
                {
                    "sent": "Neural net, by the way, was somewhere around here.",
                    "label": 0
                },
                {
                    "sent": "An SVM's were a little worse than this because they're not well calibrated and we didn't know how to calibrate them at the time.",
                    "label": 0
                },
                {
                    "sent": "So those are the kind of performances were getting before a single tree was giving us poor performance.",
                    "label": 1
                },
                {
                    "sent": "But if we bagged a lot of trees we got this really great.",
                    "label": 0
                },
                {
                    "sent": "So Q performance much much better than we were ever able to achieve before.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The summary of this is by really understanding the metric they were interested in and then finding that it was similar to some other metric we knew about, and knowing that one learning methods seem really good at that metric, we could then guess that that learning method would work very well in this metric, and sure enough, that seemed to be true.",
                    "label": 0
                },
                {
                    "sent": "So that's another case where knowing your metric turned out to be critical to doing the right thing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so could we automate that sort of process of knowing that this metric is like this other metric and that's part of what we're going.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to do.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So third piece of motivation when you've already seen these sorts of tables right, we have all these different learning methods and they have different performance and all these different measures, measures, and some of them are better at different measures.",
                    "label": 0
                },
                {
                    "sent": "This is all before calibration is being done.",
                    "label": 0
                },
                {
                    "sent": "So things like neural Nets are really good at these numbers, but bagged trees and boosted trees are good over here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So we already know from the other stuff we've talked about that.",
                    "label": 0
                },
                {
                    "sent": "One, if you could predict the right probability.",
                    "label": 0
                },
                {
                    "sent": "Almost any metric you'd be getting near optimal results, but it's actually very hard to predict the right probability.",
                    "label": 0
                },
                {
                    "sent": "And just like in that medical domain where the patients are just living or dying as opposed to you having some estimate of their risk, it turns out it's actually very hard to predict the right probabilities if you don't need the right probabilities.",
                    "label": 0
                },
                {
                    "sent": "This is maybe too hard of a thing to try to do from a small sample.",
                    "label": 0
                },
                {
                    "sent": "Maybe you should try to do something simpler.",
                    "label": 0
                },
                {
                    "sent": "It turns out that learning just to order some cases is easier than learning to predict the correct probabilities.",
                    "label": 0
                },
                {
                    "sent": "'cause having the correct probabilities would entail the correct ordering, but having the correct ordering doesn't necessarily entail the correct probabilities.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this is easier, even easier.",
                    "label": 0
                },
                {
                    "sent": "Suppose all you need is accuracy.",
                    "label": 0
                },
                {
                    "sent": "Well now you just need things to be on the right side of a threshold.",
                    "label": 0
                },
                {
                    "sent": "You don't even need the right orderings of the things that are on the left side or the right ordering of the things on the right side.",
                    "label": 0
                },
                {
                    "sent": "You just need that at the threshold you've got the right separation.",
                    "label": 0
                },
                {
                    "sent": "That's even easier than predicting and ordering so often, you just don't need the ideal model that could predict the right probabilities.",
                    "label": 0
                },
                {
                    "sent": "And in practice, what we do is we use whatever metric is appropriate to the problem we're working on.",
                    "label": 0
                },
                {
                    "sent": "Sometimes rarely it's a domain specific measure.",
                    "label": 0
                },
                {
                    "sent": "Well, it's funny.",
                    "label": 0
                },
                {
                    "sent": "You know machine learning we're always using accuracy, precision, recall squared error when you get out into the real world.",
                    "label": 0
                },
                {
                    "sent": "Those aren't the measures, right?",
                    "label": 0
                },
                {
                    "sent": "The real measures are things like dollars of profit.",
                    "label": 0
                },
                {
                    "sent": "After you take out the loss of doing the transaction, you know when you're betting on the stock market, right?",
                    "label": 0
                },
                {
                    "sent": "And that's a very different number measure than any of these measures, so.",
                    "label": 0
                },
                {
                    "sent": "So so when you get to the real world, it turns out you often end up with very specific measures that you need to optimize too.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to find some way to sort of come to grips automatically with different measures.",
                    "label": 0
                },
                {
                    "sent": "These measures really do represent different tradeoffs, so so it's important to use the right measure unless you have so much data that you can do essentially perfectly on the problem, then it doesn't matter at all so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me jump into metrics.",
                    "label": 0
                },
                {
                    "sent": "Now we're already we're already halfway through the talk, right?",
                    "label": 0
                },
                {
                    "sent": "It's all motivation so far, but we're halfway there.",
                    "label": 0
                },
                {
                    "sent": "So what have I got here?",
                    "label": 0
                },
                {
                    "sent": "This is a model that just has two parameters in it, 2 weights.",
                    "label": 0
                },
                {
                    "sent": "So think of this as being.",
                    "label": 0
                },
                {
                    "sent": "Well, a neural net with just two weights in it or.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Linear regression with one input variable and constant?",
                    "label": 0
                },
                {
                    "sent": "I mean you know, so our models aren't really ever this simple, but that's the only way I can draw a picture and what we've got is we've got a simple problem where we have some real data and we have this very simple two parameter model and I'm showing you the performance of that model in squared error on this data set.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that's the minimum point.",
                    "label": 0
                },
                {
                    "sent": "Let's see how good I am.",
                    "label": 0
                },
                {
                    "sent": "That's the that's the minimum point right there.",
                    "label": 0
                },
                {
                    "sent": "That's where the squared error is minimized, so that's the peak performance would be a weight of about .32 and .09 or something like that in this model.",
                    "label": 0
                },
                {
                    "sent": "And then these are the level curves of equal performance falling off from that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that just sort of tells you what this space would look like.",
                    "label": 0
                },
                {
                    "sent": "Of course we hope their model would quickly zoom in, do gradient descent or something like that, and end up right there.",
                    "label": 0
                },
                {
                    "sent": "Then we have the best quarter.",
                    "label": 0
                },
                {
                    "sent": "But here's.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different measures alright, so here's the RMS model that we just saw.",
                    "label": 0
                },
                {
                    "sent": "Here's log loss, cross entropy, and it's pretty similar to squared over there.",
                    "label": 0
                },
                {
                    "sent": "Very similar things in Spirit, but they're not exactly the same.",
                    "label": 0
                },
                {
                    "sent": "They're optimized at almost the same place.",
                    "label": 0
                },
                {
                    "sent": "That optimal point is almost the same as that optimal point.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit over to the right and a little bit down lower about there, but they're very similar.",
                    "label": 0
                },
                {
                    "sent": "The shape of the level curves isn't quite the same, but they're pretty similar and they should be 'cause log loss and squared are are essentially the same sort of thing.",
                    "label": 0
                },
                {
                    "sent": "But look at accuracy accuracy in this simple model class is in fact this entire hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, you have optimal accuracy anywhere along that hyperplane and then your performance falls off to either side.",
                    "label": 0
                },
                {
                    "sent": "Look at our OC area area under the RC curve.",
                    "label": 0
                },
                {
                    "sent": "You also have a hyperplane is optimal performance, but it's not that hyperplane OK so so the model that's going to give you the optimal accuracy is going to be up there somewhere in the model that gives you optimal orosi is going to be down here somewhere.",
                    "label": 0
                },
                {
                    "sent": "They're not the same models, so really you do need different models to optimize different metrics here.",
                    "label": 0
                },
                {
                    "sent": "And note that that point and that point it doesn't fall on either of those straight lines.",
                    "label": 0
                },
                {
                    "sent": "OK, so the model that optimizes log loss and squared error isn't going to optimize accuracy in RFC.",
                    "label": 0
                },
                {
                    "sent": "Here's a really bizarre graph.",
                    "label": 0
                },
                {
                    "sent": "It has local local Maxima, local minima actually in the way.",
                    "label": 0
                },
                {
                    "sent": "This is represented.",
                    "label": 0
                },
                {
                    "sent": "This is this calibration measure which we talked about, and that's a really funky measure, and it turns out that that's the optimal point, which again isn't the same as any of those, although it's pretty close to being on the maximum accuracy line, but you can do pretty well right there.",
                    "label": 0
                },
                {
                    "sent": "That's actually just a tiny bit off of being optimal.",
                    "label": 0
                },
                {
                    "sent": "If you look at the level curves, and you can do almost as well there, and calibration is strange.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's something I didn't tell you before I told you.",
                    "label": 0
                },
                {
                    "sent": "Calibration was a weird measure, but let me tell you how weird it is.",
                    "label": 0
                },
                {
                    "sent": "Suppose your data set is 70% zeros 30% positives.",
                    "label": 0
                },
                {
                    "sent": "If you just predict .3 for everybody.",
                    "label": 0
                },
                {
                    "sent": "You're perfectly calibrated.",
                    "label": 0
                },
                {
                    "sent": "Right, and if you think about our histogram method, everybody would fall in one bin to bin.",
                    "label": 0
                },
                {
                    "sent": "That's sort of centered .3, and 30% of them would turn out to be positives, and you would have predicted .3 on average for all of them.",
                    "label": 0
                },
                {
                    "sent": "You nailed the calibration.",
                    "label": 0
                },
                {
                    "sent": "Now your reliability diagram would be just one point.",
                    "label": 0
                },
                {
                    "sent": "At that bin you'd have perfect calibration though, so the weatherman, who just predicts what did I say that?",
                    "label": 0
                },
                {
                    "sent": "I thought the book was telling me that it rained fifth half of the days, 50% of the days in June in Slovenia, so the weatherman, who just says every day in June, 50% chance of rain tomorrow, is apparently perfectly calibrated.",
                    "label": 0
                },
                {
                    "sent": "It's not a very useful prediction.",
                    "label": 0
                },
                {
                    "sent": "They're not accurate there.",
                    "label": 0
                },
                {
                    "sent": "Squared error isn't necessarily good, but they've got perfect calibration when they say .5 there, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a weird measure and that's why you get this sort of weird behavior.",
                    "label": 0
                },
                {
                    "sent": "I mean that looks scary and very interesting.",
                    "label": 0
                },
                {
                    "sent": "So how do you get that that you have a model with two right right?",
                    "label": 0
                },
                {
                    "sent": "When you say optimizing this measure, that's right.",
                    "label": 0
                },
                {
                    "sent": "Use the model right she's optimizing for that, yes.",
                    "label": 0
                },
                {
                    "sent": "So what we actually did was this model is so simple that we could exhaustively enumerate all possible values of the model, right?",
                    "label": 0
                },
                {
                    "sent": "So we picked every possible combination of AW1 and W2, figured out what the performance of the model was on the quote train set, and then we could plot this graph based on that.",
                    "label": 0
                },
                {
                    "sent": "Right now the way you would normally do it.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, and the way you would normally do it of course, as you would start at some point in the space and then you would do gradient descent to get stuck in local minima.",
                    "label": 0
                },
                {
                    "sent": "And for most of these measures in this two parameter very simple to perimeter model, you would with gradient descent get to an optimal point for the training status, although this one you wouldn't, you could get stuck there, there or there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course, but would you be able to say, let's say if we would go to more than two parameters and this intuition would be just exactly same?",
                    "label": 0
                },
                {
                    "sent": "So the intuition is similar, but the things get much more complicated, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean that's the whole.",
                    "label": 0
                },
                {
                    "sent": "No, no, they're they're really different.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I mean different measures really do measure different things and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'll show you more of that.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's what the whole talks about.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's amazing.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's really critical unless you're doing incredibly well.",
                    "label": 0
                },
                {
                    "sent": "It's really critical to optimize to the right thing.",
                    "label": 0
                },
                {
                    "sent": "Because the odds are you'll only be optimal on the thing you optimize to and not necessarily on other measures.",
                    "label": 0
                },
                {
                    "sent": "So when you see people who are like training to minimize squared error, if you use decision trees, you're often doing information gain or gain ratio in the splits or things like that, and then they report performance on, say, F score, precision, recall, break, even point or something like that.",
                    "label": 0
                },
                {
                    "sent": "It could be variance, right?",
                    "label": 0
                },
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "It could be that you've got two methods A&B.",
                    "label": 0
                },
                {
                    "sent": "Each train to their own respective measure, like a neural net trained isquared R decision tree trained to information gain or gain ratio.",
                    "label": 0
                },
                {
                    "sent": "So you have two different measures.",
                    "label": 0
                },
                {
                    "sent": "You train these models to their respective measures and then you look at their F scores and you say oh this one is better than that one.",
                    "label": 0
                },
                {
                    "sent": "It may just be that the measure that you were naturally training this one two was better than the measure of the other or could just be variant.",
                    "label": 0
                },
                {
                    "sent": "It could be that they both differ from F score in some sort of noisy systematic way, and it's just variance that makes one look better than the other.",
                    "label": 0
                },
                {
                    "sent": "It's not that this algorithm is necessarily consistently better than the other on this class of problems, it may be just just an artifact of the measures that you used.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You optimally one measure, then you're also invalid.",
                    "label": 0
                },
                {
                    "sent": "How could you see that from OK?",
                    "label": 0
                },
                {
                    "sent": "So if you really could get to perfect performance?",
                    "label": 0
                },
                {
                    "sent": "On an infinite training set.",
                    "label": 0
                },
                {
                    "sent": "Then these things would all have to be the same, right?",
                    "label": 0
                },
                {
                    "sent": "It's only because these are.",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                },
                {
                    "sent": "I should have made that clear.",
                    "label": 0
                },
                {
                    "sent": "It's only because these are finite training sets that these things happen, yeah.",
                    "label": 0
                },
                {
                    "sent": "Point we just point on the four of them will be in the same way.",
                    "label": 0
                },
                {
                    "sent": "This sub WC would also like right right?",
                    "label": 0
                },
                {
                    "sent": "Except for calibration, which is this oddball where you could be perfectly calibrated in at least two different ways, one of which is if your data set is P percent positives, just predict point P for everything, which is a sort of useless model you know.",
                    "label": 0
                },
                {
                    "sent": "Or you could actually get everything right and get the squared over to 0.",
                    "label": 0
                },
                {
                    "sent": "And then you'd also be well calibrated.",
                    "label": 0
                },
                {
                    "sent": "Those will both have the same calibration.",
                    "label": 0
                },
                {
                    "sent": "It turns out now, which means this is a bad measure.",
                    "label": 0
                },
                {
                    "sent": "Calibration is not a not a good measure.",
                    "label": 0
                },
                {
                    "sent": "You would notice if you look at the reliability diagram 'cause you'd see in one case you had just one point on the diagram and in the other case you'd have a full distribution on the diagram and you prefer that.",
                    "label": 0
                },
                {
                    "sent": "So these are very very good questions.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's exactly exactly right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, sure.",
                    "label": 0
                },
                {
                    "sent": "I'm truly.",
                    "label": 0
                },
                {
                    "sent": "Then you can't.",
                    "label": 0
                },
                {
                    "sent": "That's a yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "So imagine that your model can't explore all these possible values in this space, right?",
                    "label": 0
                },
                {
                    "sent": "Some subset of these points is iaccessible to your model.",
                    "label": 0
                },
                {
                    "sent": "Well, then the best year model can do.",
                    "label": 0
                },
                {
                    "sent": "In the ideal case is get to the point that's nearest, but has the highest performance on a level curve that gets gets nearest to the optimal right.",
                    "label": 0
                },
                {
                    "sent": "For this reason or stumps or something, right?",
                    "label": 0
                },
                {
                    "sent": "Right, like stumps can't do it, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But it's really interesting to see this intuition which we are talking about all the times.",
                    "label": 0
                },
                {
                    "sent": "It's a nice diagram, so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's get to.",
                    "label": 0
                },
                {
                    "sent": "I mean that was all sort of set up, but it turns out that most of the interesting stuff 'cause the rest is pretty straightforward, so we're going to use 9 performance measures.",
                    "label": 0
                },
                {
                    "sent": "I'll actually use a tent you've seen a lot of these performance measures before, and our goal is to discover relationships between the performance measures as automatically as possible.",
                    "label": 1
                },
                {
                    "sent": "You know we want to do things like if you were to optimize to X, like train your neural net to squared error.",
                    "label": 1
                },
                {
                    "sent": "How well would you do on some other measure wire?",
                    "label": 1
                },
                {
                    "sent": "If Y is a lot like X?",
                    "label": 0
                },
                {
                    "sent": "Then it's OK to optimize to actual, probably do well and why.",
                    "label": 1
                },
                {
                    "sent": "If Y is really different from Max and you optimize decks, then all bets are off for why so that sort of thing and we'll talk a little bit about which of these metrics seems safer than the others, and if we could, we'd like to design new, better metrics, but we failed at doing that, will show you why.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've seen these.",
                    "label": 0
                },
                {
                    "sent": "You've seen the threshold metrics, the ordering metrics, probability metrics, and we've got one new metric in here.",
                    "label": 1
                },
                {
                    "sent": "Sorry, well, it's a bad name in English.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's bad here.",
                    "label": 0
                },
                {
                    "sent": "Sorry, is just a combination of accuracy area under the RC curve and squared error.",
                    "label": 0
                },
                {
                    "sent": "Those are my three favorite measures, so it's just a combination of those things and it's really, really simple.",
                    "label": 1
                },
                {
                    "sent": "It's basically just the average of them and squared error goes the other way.",
                    "label": 0
                },
                {
                    "sent": "It's just one minus and so it's just very very simple and this is almost thrown in.",
                    "label": 0
                },
                {
                    "sent": "There is a quick test to see if this would be any better than those things and you'll see it showing up in the diagram so it sort of forms an anchor point.",
                    "label": 0
                },
                {
                    "sent": "Party I should probably end by 1, right?",
                    "label": 0
                },
                {
                    "sent": "Start coming in.",
                    "label": 0
                },
                {
                    "sent": "Yeah you guys know what accuracy is.",
                    "label": 0
                },
                {
                    "sent": "I already briefly explain what lift.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was this is a lift curve?",
                    "label": 0
                },
                {
                    "sent": "If you've never seen one, this is how many people you send the mailing too.",
                    "label": 0
                },
                {
                    "sent": "If you send the mailing only to the top 10% that you think are most likely to respond, you'll have a lift of about 4.5.",
                    "label": 0
                },
                {
                    "sent": "If you send it to the top 20%.",
                    "label": 0
                },
                {
                    "sent": "With this model you have 3.5, you'll do 3 1/2 times better than random prediction.",
                    "label": 0
                },
                {
                    "sent": "By using this model and eventually if you send your mailing to everybody, your lift by definition has to be one 'cause you can't do better than random.",
                    "label": 0
                },
                {
                    "sent": "If you send it to everybody matter, how smart your model is so.",
                    "label": 0
                },
                {
                    "sent": "So that's a lift curve.",
                    "label": 0
                },
                {
                    "sent": "Precision, Recall F score, break even point.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think everybody knows those things.",
                    "label": 0
                },
                {
                    "sent": "There's your typical graph.",
                    "label": 0
                },
                {
                    "sent": "There are relationships between these things that not everybody knows about.",
                    "label": 0
                },
                {
                    "sent": "It turns out, if you set your threshold.",
                    "label": 0
                },
                {
                    "sent": "Suppose there are 30% positives in your data set and you set your threshold so that you return 30% of the cases doesn't mean it's the right 30%, but you always return 30% as being positive.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that the F score will be equal to the break even point.",
                    "label": 0
                },
                {
                    "sent": "So it's simple math to work that out, but so there are relationships between all these different measures, and these things go by different names in different communities, right?",
                    "label": 0
                },
                {
                    "sent": "True positives, false negatives, true negatives.",
                    "label": 0
                },
                {
                    "sent": "True positive rate hits and misses correct rejections.",
                    "label": 0
                },
                {
                    "sent": "False alarms.",
                    "label": 0
                },
                {
                    "sent": "Depends on what community you're in, whether you're doing information retrieval, medicine, electrical engineering, you know what you call these things.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What hits is sometimes used in information retrieval, right?",
                    "label": 0
                },
                {
                    "sent": "You talk about a hit or a miss, so that's often used in information retrieval.",
                    "label": 0
                },
                {
                    "sent": "It's also sometimes like false alarm is often used in electrical engineering.",
                    "label": 0
                },
                {
                    "sent": "Correct rejection is also often used in wireless communications, so it depends on what community you're in.",
                    "label": 0
                },
                {
                    "sent": "What names you put on these things?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's face it, they're all essentially the same thing.",
                    "label": 0
                },
                {
                    "sent": "Orosi plots are related.",
                    "label": 0
                },
                {
                    "sent": "I tend to like RC.",
                    "label": 0
                },
                {
                    "sent": "It's got a sort of firmer statistical basis than the other ordering measures like precision and recall.",
                    "label": 0
                },
                {
                    "sent": "This goes back to World War Two when people were looking at noisy radar signals in trying to reliably detect whether a plane was coming in or whether it was a bird or weather or whatever.",
                    "label": 0
                },
                {
                    "sent": "So that's how they developed RC's.",
                    "label": 0
                },
                {
                    "sent": "But RC is related to other things, like RC is a plot of two true positive rate versus false positive rate.",
                    "label": 0
                },
                {
                    "sent": "As you vary the threshold, that's how you sweep out an RC curve and I'll show you one in a second.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that sensitivity is the true positive rate.",
                    "label": 0
                },
                {
                    "sent": "So if you're familiar with sensitivity.",
                    "label": 0
                },
                {
                    "sent": "In precision recall graphs, that's that's the true positive rate.",
                    "label": 0
                },
                {
                    "sent": "So that's one of these axes.",
                    "label": 0
                },
                {
                    "sent": "Well, sensitivity is equal to recall, and that's also equal to the numerator of the lift thing.",
                    "label": 0
                },
                {
                    "sent": "So all these measures are sort of trying to measure the same thing from just slightly different perspectives, so dip.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what's important to you?",
                    "label": 0
                },
                {
                    "sent": "And there's a classic RC plot.",
                    "label": 0
                },
                {
                    "sent": "Diagonal line is random prediction.",
                    "label": 1
                },
                {
                    "sent": "This is a pretty good RSC, an area of .9 that's nice.",
                    "label": 0
                },
                {
                    "sent": "The ideal RSC curve would go straight up and then straight across.",
                    "label": 0
                },
                {
                    "sent": "That means you had perfect ordering and this is the kind of thing I was trying to optimize with that rank prop thing.",
                    "label": 0
                },
                {
                    "sent": "I had to invent for neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Calibration already told you about that everything is exactly.",
                    "label": 0
                },
                {
                    "sent": "Whoops, I was afraid of that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a calibration plot that's actually a really good calibration plot, so this is the kind of thing you get from bad decision trees without any extra calibration step, and that's why we got such good SLQ performance.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you do this well on calibration, you degrade on that particle physics problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go to the experiments.",
                    "label": 0
                },
                {
                    "sent": "Some of the learning methods that we've used before.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You already know all this stuff.",
                    "label": 0
                },
                {
                    "sent": "This is a subset of the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problems we were looking at in the experiments they talked about two days ago.",
                    "label": 0
                },
                {
                    "sent": "Alot of the setup is very very similar.",
                    "label": 0
                },
                {
                    "sent": "We're basically running the same set of experiments, but looking at the results in a completely different way.",
                    "label": 0
                },
                {
                    "sent": "And it's, you know, fairly big, so it's not actually as big as the result of the experiments I talked about on Tuesday, but it's you know, it's still reasonably expensive to do this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "We've got to train, you know.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "100,000 models or something like that so.",
                    "label": 0
                },
                {
                    "sent": "So here's the 1st result, so I'm just going to.",
                    "label": 0
                },
                {
                    "sent": "There's lots of these.",
                    "label": 0
                },
                {
                    "sent": "I could plot.",
                    "label": 0
                },
                {
                    "sent": "This is accuracy on this axis, and this is this funny calibration measure in this other axis, and each of these things is a different tree learning type, so this is a neural Nets all these red things.",
                    "label": 0
                },
                {
                    "sent": "These blue things that are called seed.",
                    "label": 0
                },
                {
                    "sent": "That's just vanilla decision trees.",
                    "label": 0
                },
                {
                    "sent": "These are phmsa.",
                    "label": 0
                },
                {
                    "sent": "These purple boxes I'm glad we're not using the cable 'cause color would be critical for this.",
                    "label": 0
                },
                {
                    "sent": "These little things are K nearest neighbor, which you'll see really span the space.",
                    "label": 0
                },
                {
                    "sent": "These things over here are boosted trees, boosted decision trees, and the yellow things are boosted stumps and the black dots are bag trees.",
                    "label": 0
                },
                {
                    "sent": "We have far more of these now, but let me just walk you through this diagram.",
                    "label": 0
                },
                {
                    "sent": "So good accuracy is this way and good calibration is that way deviation from the diagonal line calibration plot.",
                    "label": 0
                },
                {
                    "sent": "Well boy, it really stands out that boosted trees have excellent accuracy, right?",
                    "label": 0
                },
                {
                    "sent": "Some of the best boosted trees, not all boosted trees are great, by the way.",
                    "label": 0
                },
                {
                    "sent": "But some of the very best ones have extremely high accuracy on this particular problem, which is great, but their calibration is just terrible.",
                    "label": 0
                },
                {
                    "sent": "This is before applying something like Platt's method or isotonic regression.",
                    "label": 0
                },
                {
                    "sent": "By the way, if we were to apply Plasma, I haven't done it, but if we apply Platt's method or isotonic regression and redrew this graph.",
                    "label": 0
                },
                {
                    "sent": "All of those points would suddenly squish way down into a sort of noisy band.",
                    "label": 0
                },
                {
                    "sent": "That would be about that height.",
                    "label": 0
                },
                {
                    "sent": "Well, they'd get almost.",
                    "label": 0
                },
                {
                    "sent": "They get that low actually, so they'd all be in this little noisy band down here, because Platt's method and isotonic regression would improve their calibration that much.",
                    "label": 0
                },
                {
                    "sent": "But this is before we've done anything like that, so you really can see the different differences in models and neural Nets when you see a thread like this like there's an arc there in a thread there, that's often the same neural net just being trained for a different number of epochs, so it says you keep training it and take different snapshots of it.",
                    "label": 0
                },
                {
                    "sent": "But so the big picture here is, it really does matter what measure you would want.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no wonder can here.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a really good one would be this K nearest neighbor model.",
                    "label": 0
                },
                {
                    "sent": "It has pretty good calibration and it has pretty good accuracy.",
                    "label": 0
                },
                {
                    "sent": "But if you want the very best accuracy, you'd have to pick one of those points and you'd lose a lot in calibration.",
                    "label": 0
                },
                {
                    "sent": "If you wanted the very best calibration you'd have to pick this boosted stump, then you'd lose a lot in terms of accuracy, so there really are tradeoffs to make, and the most interesting thing about this graph is the trade off is not just defined by the problem, it's defined by the learning method you're going to use on that problem.",
                    "label": 0
                },
                {
                    "sent": "What that tradeoff looks like.",
                    "label": 0
                },
                {
                    "sent": "Because if you're using a learning method then you can only restrict yourself to one color of these points, so your trade off is just going to completely different depending on what learning method you're using.",
                    "label": 0
                },
                {
                    "sent": "So and as I said, if we did this calibration, it would suddenly change the tradeoff tremendously, 'cause that access would then collapse.",
                    "label": 0
                },
                {
                    "sent": "But if that access was area under the RC curve where we don't know a way to suddenly fix that, so so you have to live.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With that distinction, OK, I can show you a bunch of these.",
                    "label": 0
                },
                {
                    "sent": "They are fun, but.",
                    "label": 0
                },
                {
                    "sent": "Let's jump in to the fun stuff.",
                    "label": 0
                },
                {
                    "sent": "I think most of you know what multidimensional scaling is, right?",
                    "label": 0
                },
                {
                    "sent": "So we're just trying to find a low dimensional representation of the data that preserves the distances between points, so it's multidimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "So what have we got?",
                    "label": 0
                },
                {
                    "sent": "We've got 10 performance measures here, so 10 rows, and then we've trained 14,000 models, so we have 14,000 columns here, an for every model we measure its performance on each of these measures.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got 14,000 miles 'cause it's 2000 models for each of seven problems, so that's where we get 14,000.",
                    "label": 0
                },
                {
                    "sent": "So imagine filling in that table with numbers.",
                    "label": 0
                },
                {
                    "sent": "And of course we do that all the time.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time when we do these experiments, but I'll save you that one table, full numbers, and then we've got this problem I mentioned before, which is, you know, some metrics up is good, some down is good, some in the middle is good, so there are different ways of doing things.",
                    "label": 0
                },
                {
                    "sent": "One is these normalized scores where zero is baseline and one is the best.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of thing we used in the talk on Tuesday.",
                    "label": 0
                },
                {
                    "sent": "That's a weird kind of scaling.",
                    "label": 0
                },
                {
                    "sent": "Another way to scale things is just by the standard deviation that you actually observe on that performance measure, and a third is.",
                    "label": 0
                },
                {
                    "sent": "Let's not really scale.",
                    "label": 0
                },
                {
                    "sent": "Let's just rank the things first best next best next best next best.",
                    "label": 0
                },
                {
                    "sent": "According to that measure, and then we'll just take their rank and do do nonparametric statistics like rank coral.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So those are different ways of looking at it.",
                    "label": 0
                },
                {
                    "sent": "If we do multidimensional scaling and look at the stress, it turns out by time you get to 2 dimensions right?",
                    "label": 0
                },
                {
                    "sent": "The original problem is 10 dimensions.",
                    "label": 0
                },
                {
                    "sent": "By the time you get to 2 dimensions, you've already illuminated more than 85% of the variance by time you get to 3 dimensions.",
                    "label": 0
                },
                {
                    "sent": "You have maybe 7 or 8% of the variance left.",
                    "label": 0
                },
                {
                    "sent": "Definitely by the time you get to 5.",
                    "label": 0
                },
                {
                    "sent": "Dimensions you sort of nailed it, and this is sort of just noise out here so so, even though we have 10 metrics to begin with, there really only measuring something like 2, three, or four.",
                    "label": 0
                },
                {
                    "sent": "Different dimensions OK, so so that's an interesting fact right away.",
                    "label": 0
                },
                {
                    "sent": "You might have thought maybe that all 10 metrics each measured something genuinely different, and that's not the case, and you shouldn't be surprised, right?",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So certainly orosi area, an average precision and precision recall break even point.",
                    "label": 0
                },
                {
                    "sent": "Boy, those had better be pretty similar to each other, 'cause we almost use them interchangeably and it will turn out there.",
                    "label": 0
                },
                {
                    "sent": "The most similar of all three.",
                    "label": 0
                },
                {
                    "sent": "So that's good news.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at some of the two D. MD's plots.",
                    "label": 1
                },
                {
                    "sent": "I'd look at the 3D plots, but they're just harder to see and I didn't bring 3D glasses, so will look at the 2D plots and will look at the 2D plots where MD S has been done.",
                    "label": 0
                },
                {
                    "sent": "Multiple ways either scaling by normalized scores by standard deviation or by ranks.",
                    "label": 1
                },
                {
                    "sent": "Let's just look at them and I'll do some MD's plots that are just across all of the 7 problems and I'll show some MD's plots for each of the problems individually will turn out there.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "This is probably the main result, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, half of the talk was motivation.",
                    "label": 0
                },
                {
                    "sent": "And then this is the other half of the talk.",
                    "label": 0
                },
                {
                    "sent": "Is that slide so, so what's this?",
                    "label": 0
                },
                {
                    "sent": "It's a 2D MDS plot.",
                    "label": 0
                },
                {
                    "sent": "All of our measures are here.",
                    "label": 0
                },
                {
                    "sent": "Calibration log loss squared error.",
                    "label": 0
                },
                {
                    "sent": "That's that measure that combines squared error accuracy.",
                    "label": 0
                },
                {
                    "sent": "An area under the RC curve.",
                    "label": 0
                },
                {
                    "sent": "Here's lift area under the RC curve, average precision, precision, recall, break, even point accuracy in the F score.",
                    "label": 0
                },
                {
                    "sent": "So now what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Is that things that are similar to each other should be located near each other in this plot, and things that are far away or more likely to be different from each other.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, there's an oddball.",
                    "label": 0
                },
                {
                    "sent": "He's not playing with anybody else.",
                    "label": 0
                },
                {
                    "sent": "And of course, we already knew that, and sure enough, it's just sitting there distal in the curve, so so that's kind of good.",
                    "label": 0
                },
                {
                    "sent": "If it's close to anything, it's close to squared R and log loss, which is good because most people would say that this was their preferred measure of measuring calibration anyway, would be say squared R or maybe log loss, so so it's nice that this measure calibration.",
                    "label": 0
                },
                {
                    "sent": "If it's near anything, is near those things 'cause all three of these remember, these are the three things that I called probability measures, and I made that classification before I knew this diagram, so.",
                    "label": 0
                },
                {
                    "sent": "So it's nice that the diagram in some sense is respecting my category.",
                    "label": 0
                },
                {
                    "sent": "XR, which combines squared error area into the RC curve in accuracy, is somewhat, appropriately enough, kind of in the middle of those things will turn out soars.",
                    "label": 0
                },
                {
                    "sent": "A disappointment.",
                    "label": 0
                },
                {
                    "sent": "So don't worry about it too much, but it's kind of in the place where you'd expect.",
                    "label": 0
                },
                {
                    "sent": "So that's further evidence that the MD's plot is kind of done the right thing.",
                    "label": 0
                },
                {
                    "sent": "Even point average precision and area under the RC curve, these are clustering tightly together and in fact average precision and area under the RC curve are the closest to of any of these metrics.",
                    "label": 0
                },
                {
                    "sent": "There essentially measuring the same thing.",
                    "label": 0
                },
                {
                    "sent": "You can use them almost interchangeably.",
                    "label": 0
                },
                {
                    "sent": "These other three that depend on the threshold F score, accuracy and lift.",
                    "label": 0
                },
                {
                    "sent": "They don't cluster quite so well.",
                    "label": 0
                },
                {
                    "sent": "It turns out if we do a 3D projection there closer than you think it's only when we squashed down to two D that they're not close to each other, but they still don't cluster so tightly.",
                    "label": 0
                },
                {
                    "sent": "And the truth is, when we were coming up with these categories for the metrics we right away had a category here.",
                    "label": 0
                },
                {
                    "sent": "Probability metrics we had a category here for ordering metrics and then we were kind of stuck.",
                    "label": 0
                },
                {
                    "sent": "What to call the other three and we sort of decided they all depend on a threshold one way or another.",
                    "label": 0
                },
                {
                    "sent": "Let's call them threshold metrics and.",
                    "label": 0
                },
                {
                    "sent": "And so so it's not so surprising that they don't cluster so well because we had problems classifying them in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "Another thing to notice is log loss is consistently going to be sort of off the edge a little bit while squared error is going to be beautifully placed in the center of most of these graphs, and one of the conclusions is that if you were only going to use one measure, that's the safe one.",
                    "label": 0
                },
                {
                    "sent": "Not only is it low variance, it turns out this measure is the most correlated with all the other.",
                    "label": 0
                },
                {
                    "sent": "Measures except for SAR, but this isn't much better, so it turns out that one's been around for a couple 100 years.",
                    "label": 0
                },
                {
                    "sent": "We use it.",
                    "label": 0
                },
                {
                    "sent": "That's just going to turn out to be a great measure.",
                    "label": 0
                },
                {
                    "sent": "And then also these things are surprisingly interchangeable.",
                    "label": 0
                },
                {
                    "sent": "Although the break even point is higher variance than average precision and AUC.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you've got a variant situation, I'd use either one of those instead of that.",
                    "label": 0
                },
                {
                    "sent": "Then it's interesting that accuracy, which is probably the favored of the three threshold metrics.",
                    "label": 0
                },
                {
                    "sent": "That's also sort of closer to the center, and it's kind of funny, right?",
                    "label": 0
                },
                {
                    "sent": "The measures that we probably trusted the longest.",
                    "label": 0
                },
                {
                    "sent": "Squared error, accuracy, and AUC.",
                    "label": 0
                },
                {
                    "sent": "They they sort of span in the middle of the space, not in this one.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite so much.",
                    "label": 0
                },
                {
                    "sent": "So is it the same?",
                    "label": 0
                },
                {
                    "sent": "Disrespective planning for two minutes to two minutes I hope.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll definitely be done in 10 or less 5.",
                    "label": 0
                },
                {
                    "sent": "No problem.",
                    "label": 0
                },
                {
                    "sent": "So question yeah so.",
                    "label": 0
                },
                {
                    "sent": "Did you try to test some kind of interpretation on?",
                    "label": 0
                },
                {
                    "sent": "But now I mean we tried but we didn't succeed.",
                    "label": 0
                },
                {
                    "sent": "No, but but there might be one.",
                    "label": 0
                },
                {
                    "sent": "If somebody has an intuition, we'd love to get help there, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we were hoping that would fall out when we were doing the experiments, but it's not so obvious.",
                    "label": 0
                },
                {
                    "sent": "So so this just shows that in one case we do rank correlations.",
                    "label": 0
                },
                {
                    "sent": "The other case, this normalized scores ranking and scoring are very different things, and yet we get vaguely similar graphs.",
                    "label": 1
                },
                {
                    "sent": "The graphs look different.",
                    "label": 0
                },
                {
                    "sent": "The only reason they look different if you'll notice Cal is here, but it's not over here, and that's because if we're going to normalize scores where baseline is zero.",
                    "label": 0
                },
                {
                    "sent": "Well, baseline on Cal is really weird.",
                    "label": 0
                },
                {
                    "sent": "Remember, because just predicting equals .7.",
                    "label": 0
                },
                {
                    "sent": "Would be optimal for everything, so it's a really weird baseline prediction for Cal is perfect calibration, so it's hard to normalize it scale.",
                    "label": 0
                },
                {
                    "sent": "So when we working with calibration we often do this sort of rank measure instead.",
                    "label": 0
                },
                {
                    "sent": "That's what makes the distortion, but if you look at it, if you squint and remove cow and then pull things down from here, you'll find there are actually pretty similar right RMS and cross entropy will then move in down here.",
                    "label": 0
                },
                {
                    "sent": "These things will get pulled down a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty similar grass, and here's if we just look over six of the problems.",
                    "label": 0
                },
                {
                    "sent": "It's fairly similar graphs from problem to problem.",
                    "label": 0
                },
                {
                    "sent": "I never expected those other graphs were average across all 7 problems.",
                    "label": 0
                },
                {
                    "sent": "I never expected we'd see this kind of consistency from problem to problem, but it's pretty good consistency.",
                    "label": 0
                },
                {
                    "sent": "So the pictures kind of stable.",
                    "label": 0
                },
                {
                    "sent": "Which I had hadn't thought.",
                    "label": 0
                },
                {
                    "sent": "It's more variable.",
                    "label": 0
                },
                {
                    "sent": "If you go to 3D.",
                    "label": 0
                },
                {
                    "sent": "But in 2D, it's remarkably stay.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've also done an analysis just by doing correlation.",
                    "label": 0
                },
                {
                    "sent": "That is, don't do MD S. Just look at the correlation of performance on one metric.",
                    "label": 0
                },
                {
                    "sent": "How that correlates with performance?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different measure turns out the results are very similar, so.",
                    "label": 0
                },
                {
                    "sent": "Correlation is one across the axis, 'cause that's accuracy with accuracy, escorts F score.",
                    "label": 0
                },
                {
                    "sent": "You only really need half of these numbers.",
                    "label": 0
                },
                {
                    "sent": "You know the upper diagonal or I mean the upper half of the bottom half.",
                    "label": 0
                },
                {
                    "sent": "Just look at a few of these, so accuracy have pretty good correlation with area under the RC curve, average precision break even point with these things.",
                    "label": 0
                },
                {
                    "sent": "Not very good.",
                    "label": 0
                },
                {
                    "sent": "Accuracy doesn't correlate very well with cross entropy.",
                    "label": 0
                },
                {
                    "sent": "Here's the correlation between these three ordering measures.",
                    "label": 0
                },
                {
                    "sent": "AUC, average precision break, even point is actually pretty high.",
                    "label": 0
                },
                {
                    "sent": "Point 95.9 two point 9 two point 92.9 five.",
                    "label": 0
                },
                {
                    "sent": "They correlate pretty tightly.",
                    "label": 0
                },
                {
                    "sent": "It turns out the real oddball is calibration, which doesn't correlate very well with with most of these other things.",
                    "label": 0
                },
                {
                    "sent": "And that's 'cause it really is a strange measure, so that's not.",
                    "label": 0
                },
                {
                    "sent": "Not a surprise.",
                    "label": 0
                },
                {
                    "sent": "In fact, we were going to stop using this single number to summarize calibration, 'cause it's too misleading if you're going to use this number, always use it in combination with some other performance measure, so use it as like a tie splitter.",
                    "label": 0
                },
                {
                    "sent": "The average correlation with all the other measures is nice.",
                    "label": 0
                },
                {
                    "sent": "Likes are really is pretty good.",
                    "label": 0
                },
                {
                    "sent": "It has a .9 calibration.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, a .9 correlation with respect to all those measures is a nice general purpose measure, but the truth is that RMS, which is right there, .872, that's almost the same, and that's the highest correlation of a single measure to all the other measures.",
                    "label": 0
                },
                {
                    "sent": "So just like it was that point central in the graph, sure enough it has the highest correlation to all other measures.",
                    "label": 0
                },
                {
                    "sent": "But basically the story we're getting from the correlation analysis is very consistent with the story we get from the MD's plots.",
                    "label": 0
                },
                {
                    "sent": "So I'm just showing this to sort of further confirm that the MD's plots.",
                    "label": 0
                },
                {
                    "sent": "Showed something reasonable.",
                    "label": 0
                },
                {
                    "sent": "I don't expect you to jump into these numbers, and suddenly it's very good news that these are a mess because it's so old and so exactly.",
                    "label": 0
                },
                {
                    "sent": "It's amazing, right, right, right, right?",
                    "label": 0
                },
                {
                    "sent": "You know, maybe there were three other measures you know invented 200 years ago and they didn't survive the test of time.",
                    "label": 0
                },
                {
                    "sent": "I have no idea, but it is interesting that RMS has proved to be so robust.",
                    "label": 0
                },
                {
                    "sent": "And I mean, I always kind of liked it now.",
                    "label": 0
                },
                {
                    "sent": "I really like it.",
                    "label": 0
                },
                {
                    "sent": "So so there are funny things too.",
                    "label": 0
                },
                {
                    "sent": "By the way, there's pretty good correlation between the ordering measures.",
                    "label": 0
                },
                {
                    "sent": "In RMS in the following sense, especially if you get the ordering right, then Platt's method and isotonic regression can almost always recover good probabilistic predictions, so getting an ordering correct is almost a sufficient condition for getting good probabilities, which is very, very interesting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so that's kind of neat.",
                    "label": 0
                },
                {
                    "sent": "I mean that this is kind of constrained, which really push pushes the whole result in the right direction, right?",
                    "label": 0
                },
                {
                    "sent": "You still have to do some extra work then to go from that ordering too good probabilities and it doesn't.",
                    "label": 0
                },
                {
                    "sent": "If you could really learn good probabilities in the 1st place, there's a certain coarsening that happens when you do isotonic regression or Platt scaling to convert the ordering, two probabilities, there's coarsening effect.",
                    "label": 0
                },
                {
                    "sent": "The coarsening is worse if your data set is small.",
                    "label": 0
                },
                {
                    "sent": "If your data set is large, it's not much loss.",
                    "label": 0
                },
                {
                    "sent": "It turns out you won't do quite as well as if you could predict perfect squared are in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "If the data set is small.",
                    "label": 0
                },
                {
                    "sent": "Because of this coarsening effect, but you'll do close so learning a good ordering is almost sufficient for getting good RMS.",
                    "label": 0
                },
                {
                    "sent": "In fact, if the sample sizes are large, it is sufficient so.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's it, the metrics really only spend a few dimensions.",
                    "label": 0
                },
                {
                    "sent": "As you might expect, we get pretty consistent results for more consistent than we ever expected across different problems.",
                    "label": 1
                },
                {
                    "sent": "Different ways of scaling, doing MD S versus correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "They all seem to say roughly the same thing, and roughly what they say is, well.",
                    "label": 0
                },
                {
                    "sent": "These ordering metrics are almost interchangeable, especially average precision in area under the curve .95 correlation there.",
                    "label": 0
                },
                {
                    "sent": "They're very, very similar.",
                    "label": 0
                },
                {
                    "sent": "Cal really is a bizarre guy, probably don't want to use it, certainly not alone.",
                    "label": 0
                },
                {
                    "sent": "If it is near to anybody, it's near to the other things that look like probabilities, so that's good.",
                    "label": 0
                },
                {
                    "sent": "RMS and cross entropy are close to each other as you'd expect, and you know I have people tell me Oh no I should be optimizing the cross entropy.",
                    "label": 0
                },
                {
                    "sent": "It's the preferred thing.",
                    "label": 0
                },
                {
                    "sent": "My own empirical experience has always been that they either yielded the same results or squared error was slightly epsilon better, so I've always favored RMS and now maybe I have some evidence that RMS actually is a more stable measure.",
                    "label": 0
                },
                {
                    "sent": "Threshold metrics it's a weird family and they don't really cluster that.",
                    "label": 0
                },
                {
                    "sent": "I mean they cluster roughly, but not that tightly.",
                    "label": 1
                },
                {
                    "sent": "And this or metric is good.",
                    "label": 1
                },
                {
                    "sent": "This thing that combines the three measures.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that RMS is so good to begin with that this doesn't really provide much that's useful.",
                    "label": 0
                },
                {
                    "sent": "And I think that's it.",
                    "label": 0
                },
                {
                    "sent": "So we have code that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculates all these measures by the way.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested that codes available on a website and we've got all these different models in their predictions.",
                    "label": 0
                },
                {
                    "sent": "If anybody wants to borrow that, there's actually far more than this now.",
                    "label": 0
                },
                {
                    "sent": "That's pretty much.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Packers back to the first picture book ends.",
                    "label": 0
                },
                {
                    "sent": "But no, that's pretty much it.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So should get OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks for.",
                    "label": 0
                },
                {
                    "sent": "Thanks for suffering through yet another talk.",
                    "label": 0
                },
                {
                    "sent": "Actually one last question, should you try to come up with some?",
                    "label": 0
                },
                {
                    "sent": "Special way of producing operational measure, which are which would really fit right nation is you coming out.",
                    "label": 0
                },
                {
                    "sent": "There were no no something no the only thing we have from this work is if you give us a measure like SLQ we can throw it into the mix but we have all these models already trained.",
                    "label": 0
                },
                {
                    "sent": "You give us a new measure.",
                    "label": 0
                },
                {
                    "sent": "We can just quickly evaluate all those models on that new measure, so there's no new training to do, just a quick evaluation of 14,140 thousand models.",
                    "label": 0
                },
                {
                    "sent": "It's easy, but that's not taken out, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Take you longer to write the code then to run it.",
                    "label": 0
                },
                {
                    "sent": "Now you have a new.",
                    "label": 0
                },
                {
                    "sent": "A new row in that table.",
                    "label": 0
                },
                {
                    "sent": "Then you could redo MD S and you could see where that measure fell in the space, and I'm assuming, although I haven't done it, I'm assuming SLQ would fall down between squared error and calibration and therefore anything that's good at that end of the space, such as bag trees should be good on ESO que.",
                    "label": 0
                },
                {
                    "sent": "So that's the one thing you could do is you could take a new custom measure and you could instantly put it in in a couple hours.",
                    "label": 0
                },
                {
                    "sent": "Put into this space and suddenly no, oh, that's kind of like this other thing.",
                    "label": 0
                },
                {
                    "sent": "These learning methods are good at that thing.",
                    "label": 0
                },
                {
                    "sent": "Therefore they should be good on this new measure.",
                    "label": 0
                },
                {
                    "sent": "That's the only this will be.",
                    "label": 0
                },
                {
                    "sent": "But about discarding measures by some, let's say if it's like this robustness, and I think you counted some some more.",
                    "label": 0
                },
                {
                    "sent": "We are trying.",
                    "label": 0
                },
                {
                    "sent": "So we think that one of the things that makes attributes differ a lot his variance.",
                    "label": 0
                },
                {
                    "sent": "So there can be attributes that are measuring a very similar thing, but one is a much higher variance version than the other, and it's one of the reasons why learning and ordering.",
                    "label": 0
                },
                {
                    "sent": "May not be the best way to learn squared error.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's easier to overfit in ordering then it is to overfit squared error squared error is beautifully low variance.",
                    "label": 0
                },
                {
                    "sent": "I mean it's got so many nice properties.",
                    "label": 0
                },
                {
                    "sent": "Low variance is one of the man.",
                    "label": 0
                },
                {
                    "sent": "That's great.",
                    "label": 0
                },
                {
                    "sent": "It means that it's harder to overfit to it ordering.",
                    "label": 0
                },
                {
                    "sent": "You know if you change the value of two predicted cases just a tiny bit in the 5th decimal place, they may swap places.",
                    "label": 0
                },
                {
                    "sent": "And suddenly the ordering is different.",
                    "label": 0
                },
                {
                    "sent": "In one looks, one ordering looks better than the other in terms of squared error you've made just.",
                    "label": 0
                },
                {
                    "sent": "An insignificant change, but in terms of ordering, suddenly something has popped past the other, and if you make a bunch of those little tiny changes, you can think you've improved ordering quite a bit, when in fact you've just made these little epsilon changes to predictions that don't amount to much and probably aren't.",
                    "label": 0
                },
                {
                    "sent": "They have low margin, so one of the things we're trying to do is come up with a way of its work we're doing right now is we're trying to come up with a generalized notion of margin that will work with any performance measure.",
                    "label": 0
                },
                {
                    "sent": "OK, because then if you could do ordering with margin for example.",
                    "label": 0
                },
                {
                    "sent": "Now it would be a great great measure it be low variance then.",
                    "label": 0
                },
                {
                    "sent": "And Torsten has also done some work on this, yes.",
                    "label": 0
                },
                {
                    "sent": "I guess we'll have to.",
                    "label": 0
                },
                {
                    "sent": "Sure, sure.",
                    "label": 0
                },
                {
                    "sent": "A couple of more questions, actually.",
                    "label": 0
                }
            ]
        }
    }
}