{
    "id": "hsidt3v54nqqo2exiqmuwgo6mjtodjo7",
    "title": "Utilizing Unlabeled Data for Classification-Prediction Learning",
    "info": {
        "author": [
            "Shai Ben-David, David R. Cheriton School of Computer Science, University of Waterloo"
        ],
        "published": "Nov. 11, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/wapa2011_ben_david_unlabeled/",
    "segmentation": [
        [
            "So I think this is going to be a theoretical talk, but it's.",
            "Motivated by practical implications.",
            "So let me just start by, so I hesitated between talking about clustering because I realized clustering here is very popular and an also semi supervised learning, but his more recent stuff so.",
            "And to me it's more exciting, so I chosen a very egocentric way.",
            "This is joint work, mainly with my PhD student, Ruth owner that it's a series of paper that we've been doing through the past year and a half or so.",
            "Some of these papers that we should have shot Sanoja Jamir, an one paper with my son, Solomon David.",
            "So I'm very lucky that I have a son that is now.",
            "We agreed to work with me OK."
        ],
        [
            "So.",
            "So we want to talk about utilizing label data for classification, prediction, learning and the many applications in which unlabeled data is cheap and abundant, and on the other hand, classified data is how to get by.",
            "So one example, typical example is email for spam.",
            "It's very easy to get lots of emails, but it's very difficult to get the user to really label every email, whether it's permanent.",
            "Another one is part of speech tagging.",
            "It's very easy to get big corpora of text.",
            "But if you want to do each word to say, each part of which part of speech is it, then it is a very laborious work.",
            "It costs money very difficult to get labeled examples.",
            "Another we saw yesterday you want to classify tweets for the sentiment and it's very easy to get the tweets but to get the correct sentiments.",
            "It's difficult, so there's a big drive to see.",
            "How can we utilize unlabeled data?",
            "Downloaded data is cheap.",
            "Can it really help us and?"
        ],
        [
            "So what I'm going to do here is talk about several scenarios in which unable data can really utilize to help in classification learning, and then I'll explain some algorithmic paradigms.",
            "So there's not.",
            "There's no one way in which you can utilize unlabeled another several ways.",
            "I'll describe three different ways in which unable that can be utilized, and I'll talk a little bit about the theoretical analysis we want to really be able to prove that by using valuable data you really save.",
            "Improve the accuracy or save the need in labeled examples."
        ],
        [
            "So.",
            "I'll describe three paradigms for three setups in which you can probably utilize unable samples, so one of them is proper, semi supervised learning, semi supervised learning is the this whole area where you have labeled data and unlabeled data and you want to take advantage of the animal data proper.",
            "Semi supervised learning is when you have a prescribed class that you want the your predict account from this class.",
            "So there is some desirable way of a predictor, say want to predict with half space because of.",
            "Different reasons that I'll describe.",
            "Can you come up with a good headspace and for that purpose we can show how unlabeled data can help you.",
            "The other domain area is domain adaptation to meditation in situation where you train on one type of data, but you application comes from a slightly different type of data.",
            "In that case, unlabeled data can help.",
            "You will see how and the third one is a very new kind of a direction that we're playing with, and this is learning from week teachers.",
            "So recently there's a lot of buzz around, like using information like the Amazons Mechanical Turk crowdsourcing, but we know that once you will rely on labeling from Mechanical Turk that lots of errors, those errors are not just random errors.",
            "I mean we do have work about random noise, but that's not random noise.",
            "They're consistent reasons why they make certain error, not another.",
            "We want to see how can you utilize.",
            "Labels that are coming from things like Mechanical Turk from week teachers to improve prediction.",
            "So this is the three areas Delta."
        ],
        [
            "About so let me start with a semi supervised learning, so the similar learning the first approach was the.",
            "Hey, you want to use it to improve accuracy, so when can unlabeled data help you to use to improve accuracy?",
            "The intuition says that if your data is clustered into cleanly separated clusters and every cluster is homogeneous in terms of its label, then if you have the unlabeled data, you can detect those clusters and then labeling becomes very easy.",
            "You just need one example from every cluster and you know the labeling of everything.",
            "So that's a very appealing idea.",
            "The two big problems with it.",
            "One problem is that it is not clear how to formulate the assumption of classical data.",
            "I mean, sure, we can draw pictures and have clusters which are separated, but you want some formalization that on one hand would be mathematically solid.",
            "On the other hand, will be relevant to applications and implications.",
            "Your data is never appearing in nice bowls which are away from each other.",
            "So there is a real issue of how do you formalize this notion of cluster ability?",
            "Will talk about it.",
            "The other problem is that in order to apply this approach, you want to assume that every cluster is homogeneously labeled.",
            "So this chasm correlation between the labels and the arrangement of the unlabeled data and.",
            "This isn't something which is very difficult to verify actually verifying it is just as hard as finding out what are the labelings and we could show in previous work that without this assumption.",
            "The basic paradigm of semi silver element may badly fail.",
            "So rather than trying to do semi supervised learning in this straightforward way, we're talking about, we're going to talk about some subproblem of semi supervised learning and that's the."
        ],
        [
            "Them of properly Nick, so in proper learning.",
            "In some applications you have requirements about the type of classifiers that you want to output, so the exact the motivating example for us was there is this company which is called mobile.",
            "I don't know if you heard about it.",
            "They build the devices for cars that help the driver.",
            "So for example they watched the driver and sound an alarm if they figure that the driver is falling asleep or if they figure that you're getting too close to another car.",
            "Or something like that.",
            "Now for them you want to find out.",
            "So if you check features about the driver, how is moving his head, how his eyes are and so on.",
            "You check those features and you want to predict is the driver falling asleep or not.",
            "So you have a predictable but the problem is that when you already figured out a predictor, you learn to predict.",
            "Oh now you see the features.",
            "From the driver you want to predict very fast.",
            "It will be kind of useless if you allow the driver that he fell asleep.",
            "A minute after he did fall asleep, it may be a bit too late for the driver, so speed is very important.",
            "You want classifiers you are willing to sacrifice some accuracy for gaining speed.",
            "I mean, maybe your driver will get some false positives and they will tell him he's falling asleep and he'll say I'm not asleep.",
            "I'm a very alive but it's not so bad as being late in alarming, so we're willing to sacrifice some accuracy for having a predictor of some certain type certain type that can you see a point you can predict very fast.",
            "Another type of desirability requirements from predictors is.",
            "They are going to be readily interpretable if I want to find a predictor for medical purposes.",
            "I want to explain to doctors what causes Alzheimer's disease.",
            "It is kind of.",
            "Frustrating for them if you tell them OK, I have this huge neural network.",
            "You feed your features.",
            "I'll tell you what the outcome is, but you want to tell the patient you know, eat less butter, drink more red wine, whatever you want, something that can be easily be interpreted so you want you are willing to sacrifice some accuracy here for the sake of interpretability.",
            "So we're talking here about situations in which we have a certain class of desirable classifiers, and you want the prediction to come from that class.",
            "So a good example of such a classifier, half spaces of course halfspaces are very nice, we can compute them very fast if you have a high space, you have a point, you just do the dot product, you know if it's above or below the half space and also very interpretable because just give you weights for every feature so you can see which are the important features.",
            "So say our desirable classes have spaces and now we want to find prediction which is going to predict which is going to be a linear have space for that purpose.",
            "We can show that unlabeled data probably."
        ],
        [
            "Apps so.",
            "This is what we're going to talk about.",
            "I call it the usability utility feasibility tradeoff."
        ],
        [
            "You want a very usable, usable.",
            "Predictor and the first demonstration of the potential use of unlabeled sample.",
            "We're going to show that such samples can be utilized to overcome this utility feasibility tradeoff.",
            "So we have a class of desirable classifiers and we show her unable surplus can give.",
            "You can help?"
        ],
        [
            "You find a good classifier from this class, so here is my formal set up.",
            "The formal setup is we have some domain set, say it's lives in Rd.",
            "And you have a labeling set.",
            "For simplicity, assume that you have only binary labels, either 01 and there is some target distribution which is not known to the learner that generates the data.",
            "And now you're given a class of disabled predictors H and you get a sample and ID sample from your distribution and you want to.",
            "On top of it to utilized a large, unlabeled sample so we have a small, you have a small label sample S Ann.",
            "You have a large unlabeled sample T and you want to use them in order to find the best predictor in the class.",
            "H, say the class H is linear, half spaces.",
            "That will minimize the error."
        ],
        [
            "So here is my first algorithmic approach.",
            "So what you're going to do is you are going to use the labeled sample to same to learn some predictor from a surrogate class.",
            "So you're going to use the label data to learn some predictor, which is not nice, which is not a linear predictor.",
            "It could be a complex neural network, it could be some will see examples of how it is used.",
            "In practice it could be something based on the kernel side, like champion coefficients, which are very difficult to compute.",
            "You get some.",
            "Ugly but precise predictor, but you wanted a linear classifier.",
            "So in the second stage you apply your already existing classifier to label the unlabeled sample.",
            "So we have the unlabeled sample.",
            "You now take the ugly classifier and you use it to label the unlabeled sample.",
            "Now you get this big sample of the previously unlabeled points is now labeled not completely correctly, because you're using a classifier that you learned, but it is almost correctly, and you use that to feed it into a learner for the class that you really want.",
            "So what happens here is that.",
            "From the point of view of the class that you really want to learn rather than having this small labeled sample, you now got a big sample which was generated from the unlabeled data and that will allow you to find linear halfspace faster.",
            "So here is the."
        ],
        [
            "Some examples where this approach has been used in the past, so we first came up with this idea and did the theory.",
            "Then we found out that some people use it in practice, there was no previous analytical analysis of it, but the idea is simple enough.",
            "It has been used so these are two papers that used similar idea in practice, so the first one wanted to learn fast independent district regression classifiers and they used conditional random fields which are more expressive but very.",
            "Very slow to compute.",
            "They are very computationally intensive.",
            "The other one use a similar idea for NLP."
        ],
        [
            "An so.",
            "We analyze how in this kind of framework, the unlabeled data probably.",
            "Help you so we analyze it in kind of two.",
            "Anne.",
            "Set up one of them is that our our classifier.",
            "Our correct labeling is realizable by some.",
            "Bigger class of ugly classifiers.",
            "And the other situation is in which our data satisfies some.",
            "Cluster assumptions, some niceness assumptions, and then we can use as the first classifier.",
            "We can use nearest neighbor, so we discuss these two types of."
        ],
        [
            "Assumptions and it is our algorithm as I described, so you first learn so H prime is your ugly class.",
            "H is the desirable class.",
            "So the first step is you first learn a classifier in the ugly class.",
            "Using the the label sample, we assume that the ugly class is easy to learn, so you can learn it with a small label sample.",
            "Then what you do is you use this H prime classifier to label the big unable data T and then you feed this set T, which is now labeled to an agnostic learner for the desirable class H and the point is that agnostic learners are kind of robust to noise, so if you have some errors on your labeling on T, it will not effect the final result by much."
        ],
        [
            "And so the kind of results that we can get is that.",
            "We can learn using a sample if you have a so.",
            "This formula is the most important thing is the dependent on epsilon.",
            "Epsilon is the accuracy that you get at the end so.",
            "Usual learning in diagnostic setting requires dependence of one over epsilon square.",
            "Here we can show that you can settle for this label.",
            "Sample is just of order one over epsilon.",
            "You need one over Epsilon square, unlabeled points but unlabeled points are cheap.",
            "So with lots of unlabeled points but few labeled points, you get accuracy which is close to the optimum class plus epsilon.",
            "So we have gained the gap between one over Epsilon Square one over epsilon in terms of the number of labeled example that we require.",
            "Once we have this unlabeled data."
        ],
        [
            "Anne.",
            "As such, games can come in different ways that can come due to having a lower approximation error.",
            "If I don't know how much we want to get into this theory.",
            "But if you have a class with a with a lower approximation error, then the convergence rates of the errors are faster.",
            "So if I have the ugly class with lower approximation error, I get faster convergence rates.",
            "Other reasons can be different in this dimension.",
            "Other reasons could be computational considerations, so we have a class that we can learn.",
            "Fast.",
            "But undesirable classifiers we use it to label the animal data and then then the."
        ],
        [
            "Disable class here is a demonstration of how it works, so assume that your true labeling looks like this.",
            "Then you see the minuses into pluses, so the Gray area are the minuses of the true labeling and I want to come up with a linear predictor.",
            "So the problem is that no linear predictor will give me a.",
            "Perfect prediction here, but they can get perfect prediction with a predictor, which is just the union of two half spaces.",
            "So what I do is I first learned this union half spaces since I have a Union halfspaces that predicts the data precisely.",
            "I can do it with sample size that grows only as one over the error.",
            "On the other hand, once I have the correct predictable, I used the unlabeled data to decide which linear classifier is better.",
            "If there's more weight, he'll then I prefer this linear classifier.",
            "If the wait here is bigger, I prefer this linear classifier, but that decision can be made just by viewing the unlabeled data.",
            "I don't need more label examples.",
            "So in such a situation, we can really show."
        ],
        [
            "So lower bounds that if I did not have the unlabeled data, I have a lower bound that behaves like one over epsilon Square.",
            "And with the use of the unlabeled data, I could use, I could get accuracy epsilon from just one over epsilon label examples, so there's approvable.",
            "Advantage to the use you probable utility, from the unlabeled examples.",
            "Anne.",
            "Well, we.",
            "The second."
        ],
        [
            "Another situation which we can get such an advantage is.",
            "If you have some nice cluster assumption, so how is the cluster assumption we are trying to formalize the notion that the data nicely partitioned into clusters.",
            "So one way of doing it was saying the labeling function satisfies the Lipschitz condition was election conditions.",
            "If two labels.",
            "If two points have different labels and must be far apart.",
            "So in order to satisfy the Lipschitz, a usual Lipschitz condition of the labeling function, the data must split into completely disjoint clusters.",
            "This is not a very realistic scenario if the data underlying distribution, the major distribution is kind of continuous.",
            "Then the only way it will satisfy Lipschitz condition is if the labeling function is constant because you otherwise you will always have two points that have different labels and their arbitrary close to each other.",
            "So we relaxed these slips in this condition."
        ],
        [
            "To a notion of probabilistically blip sciousness we just look at the probability that there will be a point with a close neighbor that has a different label.",
            "And this probabilistic lips S can be viewed as a way of measuring how clusterable the data is.",
            "So this set of bad points the wise that have a neighbor which is closer than Lambda to them but has a different label.",
            "This set is the kind of the set of points that live around the boundary of the decision.",
            "Student boundary, so we want to say that the probability of seeing a points around the decision boundary is small, so that's.",
            "A formal way of describing that my data is clusterable, and it's more realistic than saying that it splits into disjoint clusters.",
            "It is just a way of formulating the area around the boundary.",
            "The decision boundaries or the weight of the points around decision boundaries is small, so we call it probabilistically Christmas and we."
        ],
        [
            "This assumption so for example data that looks like this.",
            "Say this is labeled.",
            "Plus this is label minus.",
            "This data does not satisfy Lipschitz condition because they have arbitrarily close points with opposing labels, but it does satisfy a very nice probabilistic lipnice because the probability of sync two points which are close and have different labels is very small.",
            "So with such a."
        ],
        [
            "Assumption.",
            "We can prove that the nearest neighbour algorithm is a very good predictor.",
            "The nearest neighbor algorithm is not, of course, undesirable predictor, but we can use it as the first stage of our."
        ],
        [
            "Process so we get similar."
        ],
        [
            "Bounce here I will not get."
        ],
        [
            "Into the formulas is a similar situation is we've seen before that the unlabeled data gives you gives you advantage over labeled if you over situation where you use only label data.",
            "So I want to just show graphs of two experiments that we did, so we did experiments with the basic amnesty digit recognition database.",
            "So we were trying to do binary classification between the digits zero to four and which is 5 to 9.",
            "With 60,000 training examples and 10,000 test examples, and.",
            "Here again, our ugly classifiers were kernel based on this kernel function, which is very difficult to compute, so this is a kernel that you can use for the training, but if you need to be fast at prediction time, you cannot use this kernel because it is too computationally extensive.",
            "And we want to come up with a good linear class."
        ],
        [
            "Sapphire, so here are the results.",
            "So let me explain to you what you see, what you see here.",
            "This is the number of training example labeled training examples that we use if we.",
            "If we train this complex kernel, this is the way the rate the error rate goes down.",
            "So it's a very good kernel.",
            "The problem is you can you not use it in prediction time because it's too slow to predict.",
            "If I give you a point and you want to predict you have to compute its kernel function to all the points.",
            "Told the training points and that's very slow.",
            "Instead, we use it to label a big unable data set and come up with a linear classifier.",
            "So what you see above is the error rate of linear classifiers.",
            "The blue line is there way to fill in a classifier without unlabeled data, so it goes down much more slowly.",
            "The red curve is the linear classifier.",
            "When I do use unlabeled data and I use the kernel to label the unlabeled data.",
            "So I can get it about 2000 points.",
            "I get 2000 label points.",
            "For the linear classifiers I get the same accuracy as without the unlabeled data.",
            "I would need in the order of 20,000 label points.",
            "And we have a similar."
        ],
        [
            "Results."
        ],
        [
            "Where is the nearest neighbor classifier with the same story, except that here the fast the fast converging rate is from nearest neighbor.",
            "Again nearest neighbor is not very practical, specially when you have 10s of thousands of examples, but we just use it offline to label the unlabeled points and then we can get the same kind of advantage from the use of unlabeled points to output a linear classifier.",
            "OK, so that was my first use that was using unlabeled data.",
            "In the situation of semi supervised learning.",
            "When you have a requirement to output a predictor from a given class."
        ],
        [
            "And one thing you can notice here it's you can ask how many unlabeled examples do I need, and that's that's a question that can be formulated on its own, and that's the question that I have investigated in this.",
            "Also 2011 two weeks ago with my son and the question is it's kind of paradoxical question you ask yourself.",
            "How difficult it is to learn when you know in advance what is the labeling?",
            "So what is there to learn?",
            "I mean, you know the leveling in advance, but what you want to learn is what is the best predictor from your class.",
            "So we want to find like we saw the picture before.",
            "I want to find the best linear predictor when I know that no linear predictor is perfect.",
            "So in order to find the best linear predictor I have to get some intuition about where is the weight of the unlabeled data and the question is how many unlabeled examples you need to make such decisions.",
            "So we call this model the known label classifier, learning KCL.",
            "So you're given.",
            "Some labeling function and your input is an unlabeled sample and you want to find a predictor from a given class.",
            "This class does not have a F inside.",
            "You want to find the predictor that minimizes the error with spectral.",
            "This unknown distribution channel distribution is disclosed to you only through the unlabeled data, and we got there."
        ],
        [
            "A clean results.",
            "We got combinatorial optimization of the complexity of this problem.",
            "So what I'm talking about now is how many unlabeled examples I need.",
            "What I showed you before is that unlabeled examples save you labeled examples.",
            "Now it's kind of side issue how many of these unlabeled examples we need, and we gotta classification trichotomy into three types of classes that we can easily distinguish.",
            "In one of them, you don't need any labeled examples.",
            "In the other, you need order of one over epsilon.",
            "Unlabeled example in the third class, which is most common, you need one over epsilon square.",
            "Labeled unlabeled examples.",
            "Just to learn how to get the advantage from unlabeled data.",
            "So the question is how many unlabeled examples do you need?",
            "If they are very cheap, you don't really care about this question.",
            "OK, so that was the same supervised learning my second set up is the set."
        ],
        [
            "Set up of domain adaptation.",
            "So what is a meditation?",
            "Most of the classical machine learning assumes that the training data in the test data come from the same distribution, but.",
            "It's not always a realistic assumption.",
            "So in many machine learning applications it may happen that you train on one type of data, but you it's test time you have slightly different data, so here."
        ],
        [
            "Some examples, so say you want to learn in some way to distinguish from images of faces, men from women, and you train on images of.",
            "Asian people and you want to predict on images of Caucasian people?",
            "I mean it's a different distribution.",
            "Is the same task you train on one you want to test another.",
            "A very common one, the one I was concerned with when we wrote those papers is my daughter was learning how to drive.",
            "So of course you learn to drive on one car, then at the testing you may have to drive a different car.",
            "How do you adapt?",
            "You do the manifestation intuitively right.",
            "Another very practical application is spam filters for spam filters.",
            "I mean if Google tries to come up with the spam filter, there is a big issue that they don't know what is distribution of emails that you will get.",
            "They want to sell you the spam filter, but they cannot train it on the email that you will get the training on.",
            "Different corpora of emails.",
            "So this issue of domain adaptation.",
            "Another area which is very I don't know why you would get this natural language processing and natural language processing.",
            "You have corporate off.",
            "Data that you say you want to do.",
            "So speech tagging, so you have data, say coming from one kind of documents that you did have training, but you want to apply your part of speech tagging to a different type of document, say one of them was legal documents.",
            "You want to apply it to.",
            "Medical documents it's a different distribution of words.",
            "How can you adapt your classifier from one training set to a different application set so we call them source and target?"
        ],
        [
            "And there's lots of practical work on this domain adaptation.",
            "Alot of it in the natural language processing community.",
            "Also in the imaging community.",
            "But there's very little to vertical understanding, So what we're trying to do here is to come up with theoretical analysis."
        ],
        [
            "Of domain adaptation, and there are three aspects of the problem that determine what is the task at hand.",
            "One of them is what assumptions you make about the relationship between the training data and test data.",
            "Of course, if there is no relationship between them, there's no way you can learn on one and apply to the other.",
            "So one thing you have to determine is how what is the relationship between the domain and target tasks.",
            "The other thing you have to determine is what kind of data do you get.",
            "Do you get only labeled examples from the training or you also have some training examples from the target?",
            "What kind of input do you have and the last component is you have to determine what is the kind of prior knowledge that the learner has.",
            "We know from the no free lunch theorem, no learning is possible without any prior knowledge.",
            "We want to formalize to formalize the prior knowledge here to analyze under what kind of prior knowledge can you do the meditation.",
            "So these are the three components we need for doing."
        ],
        [
            "Meditation and let me go through one by one the the way we model that we discuss here.",
            "So in terms of the inputs available to the learner, we assume here that the learner has labeled data from the source domain and from the target domain.",
            "It only has unlabeled data.",
            "So if you think of the spam filters, I have some training domain for which someone marked marked what is spam and what is not spam.",
            "So this is supervised, it's labeled, but I want to apply it to your email so maybe I can.",
            "If I'm Google I can get a lot of.",
            "Your emails, I don't know what you call spam or not, but so I have unlabeled data from the target domain.",
            "I have labels only from the training domain, which is different, so that's the scenario that we are talking about here and we want to find a predictor that has low error with respect to the target.",
            "So PT denotes the target domain distribution.",
            "PS denotes the source distribution.",
            "So I want to find the predictable that has low error with respect to the target distribution."
        ],
        [
            "That's the input in terms of what assumptions do we make?",
            "So in this work that many assumptions that people use to analyze the meditation.",
            "Here we make two simple assumptions.",
            "One of them is the covariate shift assumption to fancy feast option is the assumption that the labeling is the same in the source in the target they differ only in terms of the marginal unable distribution.",
            "For example, in natural language processing, part of speech tagging it should be the same tagging regardless of whether you're talking about bioinformatics.",
            "A documents or you're talking about legal documents, but the distribution of words will be different, so that's the covariate.",
            "Shift assumption and the other assumption is that there is a bounded weight ratio, the.",
            "Every point that is likely to appear in the target has also some probability of appearing in the source.",
            "You need some kind of an assumption like this, otherwise there will be points that only appear in the target, but you never saw them on the training and you have no way of predicting how you're going to behave there.",
            "So we need some kind of such an assumption to.",
            "The first assumption is about the labeling.",
            "The other assumption is about relatedness between the unlabeled distributions, the marginal distributions.",
            "So these are pretty standard."
        ],
        [
            "And.",
            "Under these assumptions, people usually come up with two type of strategies strategies for the medication can be divided into two types.",
            "One of them is conservative, so the conservative that patient says I will learn a good predictor for the source and then analyze when having a good predictor for the source is also good enough for the target.",
            "So I'm not doing any adaptation, I'm just using what I learned for the source and hope that it will work for the target.",
            "So there's a lot of analysis of I called them conservative predictors.",
            "Adaptive predictors, on the other hand, are predictors for which you really use the unlabeled data, and you adapt your predictor to your knowledge about the target."
        ],
        [
            "And there's more knowledge about the conservative because they are easier to analyze.",
            "For example, these are two works that I was involved in one NIPS 2006 when we showed first.",
            "To ethical results about the success of conservative, under which conditions.",
            "If you learn for the source, it will be reasonably good for the target.",
            "We have more recent results here in ICML 2011 that shows that under certain conditions nearest neighbor you can use nearest neighbor for the target, so you will take the labeled examples.",
            "From the source and when you get the target point, you look at the nearest neighbor and copy its label.",
            "So on the set of assumptions this can be used, but this is all conservative.",
            "I'm not really adapting to the target, so here I want to."
        ],
        [
            "Use to discuss adaptation.",
            "How can we utilize the unlabeled data that we have from the targets to improve our prediction?",
            "So here is an proposal from this paper by it's very natural proposal.",
            "Use the target unlabeled data to reweigh the training sample so you get the training sample.",
            "It was generated by the underlying source distribution you want to give it.",
            "New weights, weights that will reflect how the target distribution behaves.",
            "And then train on the new weighted newly weighted sum.",
            "So that sounds very appealing and they are discussing in a series of paper that discussing algorithmically how to do the best we waiting.",
            "Alas, we could prove that it may really badly fail, I mean this.",
            "We waiting.",
            "In some bad scenarios, can completely mislead you so.",
            "Hey can we use the antibody anyway?",
            "That will probably 6."
        ],
        [
            "Did.",
            "And I get now to the third component, so the components of the domain adaptation where what is the input?",
            "What are the assumptions about relationship between the source and target distributions?",
            "And now I get to the third component and 3rd component is what is the prior knowledge that the user has?",
            "So we discuss here two types of prior knowledge of 1st is that the user knows some class of predictors that one of them has zero air respect to the source and the other one is that the user knows some task class of predictors HD, such as one of them has 0 error with respect to the target.",
            "So this is like traditional PAC learning traditional packing.",
            "In Valiant model you assume that there is some class of predictors that is guaranteed that it has the real error, but here because it's mentioned we can.",
            "Really make it into two different assumptions.",
            "Do we have zero inspectors?",
            "Also, do you have the error respective target and are there any of these assumptions we analyze how?",
            "Useful is the only."
        ],
        [
            "The data, so we show that under the first assumption.",
            "If you know a class which is a 0L0 approximation error with Spectra Source, the unlabeled data is useless.",
            "Anything you can do with the unlabeled data you could also do without an emulator.",
            "On the other hand.",
            "Anne.",
            "Who is the 2nd assumption in the scenario in which you know of a class that has zero target error?",
            "We can prove that the unlabeled data."
        ],
        [
            "Really helps you.",
            "We have upper and lower bounds.",
            "So again, what is my algorithmic paradigm here now with MC paradigm is that I'm giving a source generated sample S which is labeled and I have an unlabeled target generated sample T. So.",
            "What I do is I take my sample S and I clean from the sample S every point which is not hit by the unlabeled sample T. So we assume you have a very big unlabeled sample T you have the label sample S. Any point in S which is not hit by the sample T you throw it out, then you're left with a smaller sample, but the smaller sample becomes realizable by the target distribution, and then you can use it to learn.",
            "So with this strategy.",
            "So we're doing some kind of a very crude reweighting.",
            "You take the sample and you just decide whether you keep a point or throw it away, and you keep a point only if it's also visited by that big, unlabeled sample and we analyzed."
        ],
        [
            "What is happening with this scenario and we can show that here in this case, if you have a sufficiently large target generated.",
            "Samples.",
            "Then with this number of labeled examples, which is pretty small, it's behaves only like one over epsilon, so small number of labeled example suffices to learn when adaptation.",
            "Once you have a very big target generating sample.",
            "On the other hand, if you don't even have unable data, then we can show a lower bound.",
            "In this case for domain adaptation you cannot learn without square root of the size of the domain points.",
            "So it's a huge gap between what you can do with unlabeled data and without labeled it unable data with unlabeled data, you have a very tight, very small bound on the number of samples of labeled examples that you need.",
            "Without unlabeled data, we can show a lower bound, which is huge.",
            "The size of the domain.",
            "So in this setting there probably the unlabeled data makes a big difference."
        ],
        [
            "And.",
            "OK, so this was my second application.",
            "The domain adaptation and I'm going into my third application, so we're just trying to look at different areas in which we can probably utilize unlabeled data to save labeled examples.",
            "So this third application came from a visitor head in Universal.",
            "Burton wasn't better have a Center for machine learning and trying to have lots of health applications.",
            "So I was talking to Russ Greiner and he explained to me this problem that are struggling with.",
            "They want to design an automatic way to classify brain images.",
            "So you have brain images.",
            "You'll want to detect whether there is a tumor there or not, where the two tumor is benign or malignant.",
            "And you want an automatic way of doing it.",
            "Becausw veggie ologists are very expensive and few of them it will be very nice if you could have automatic system that does it for you.",
            "The problem is that in order to train such a system you need a lot of training data, so the training data you have to generate lots of lots of images of brain images and then go to this expert of geology and tell him please sit down and classify those.",
            "3000 images for me because I needed to develop some machine learning application.",
            "That guy is not likely to do it for you.",
            "I mean, to sit down and he's very busy.",
            "I mean, maybe you have experience with them or not yet they're not going to sit for you and do things just for the sake of your nice research.",
            "So the question is, what other options do we have?",
            "How do we get training data and what rights grinder suggested it?",
            "Maybe you can get training data from medical students, medical students.",
            "They're poor.",
            "You give them $10 per marking such an image and they will be happy to do it.",
            "That's more available.",
            "They have lots of money, they'll be OK.",
            "But of course they are prone to making errors, so this brought up this idea of.",
            "How can we learn from?",
            "Cheap what we call weak teachers so.",
            "Classifying labeling those images.",
            "By medical students we call them weak teachers and we want to see how can we learn using weak teacher labels.",
            "Now the main thing to notice.",
            "I mean there has been working machine learning about aronis labelings, but usually it's about either the labeling is is random noise, random noise is a very different characteristic than this kind of thing.",
            "I mean if you take medical students there are some images that will always classify correctly.",
            "And we know the type, the probability of error depends on the image, how the images are likely to a queue, errors, easy ones will not take you out there.",
            "Another difference is that in random labeling noise, if you repeat the same.",
            "Query over and over again.",
            "You can average out the noise, but here when I'm using weak teachers, if I ask this question again and again, there will persist in they give you the same wrong answer, so it is very different than random noises.",
            "Also very different in malicious nodes.",
            "These are the two models have been discussed before and the question is how do we model this type of week features and another example of the teachers that I mentioned in the introduction is labeling you get from Mechanical Turk labeling you get from crowdsourcing now also.",
            "They contain some information, but they may have errors and those errors depend on the type of point that you give them and persistent.",
            "So how do we model?"
        ],
        [
            "Those weak teachers so.",
            "My.",
            "We want to learn a good labor predictor for model examples that come from two sources, strong teachers, that labels correctly weak teachers that make mistakes, but they can handle clearcut cases.",
            "And the question is, how do we model the week teachers labeling and how can we utilize labeling coming from weak teachers?"
        ],
        [
            "And we consider kind of.",
            "Only requirements like sufficient requirements from week teachers that will make them useful and sufficient requirements are one of them.",
            "That week teachers can label clearcut cases correctly with high probability.",
            "The other requirements is that week teacher.",
            "When you're getting close to the decision boundaries they make.",
            "They can give you both replies so they get confused around the decision boundaries and the search requirements, and it's there are not too many points in the bottle and areas which is also natural to assume."
        ],
        [
            "Under these assumptions, we model the big teachers rather than defining them.",
            "We just model this requirements.",
            "Any labeling rules that will satisfy this requirement can serve as a weak teacher for us and we can utilize tables we get from it.",
            "So this is a more precise formalization of the requirements we saw in the previous."
        ],
        [
            "Slide and other door under those requirements.",
            "So every rule of labeling that obey these two assumptions, we can utilize it to save queries to the strong teacher.",
            "So we prove that whenever the underlying data generating distribution satisfies some elements, conditions and the labels generated by a rule that satisfies those requirements, then we can use the utilized the week teachers to get a low error prediction while saving queries to the strong teacher.",
            "We can actually use these medical students to save queries to the expert radiologist and."
        ],
        [
            "What is the algorithmic paradigm?",
            "How do we do it?",
            "So what we do is we take 2 random samples, we call them S&T is before we use the week teachers to label one of the samples and then we estimate the confidence of the week teacher on one of the points of each of the points of T. So a point is considered confident if all its neighbors have the same label and the content is considered.",
            "I'm confident if in this neighborhood I see both labels.",
            "So I estimate the confidence of every point by the homogeneity of its neighborhood, and then I pass onto the strongly teacher to the query to the strong teacher only four points that have heterogeneous neighborhoods only for points on which I'm unconfident so I can show that I can.",
            "Get a good estimates of the confidence of labeling of a point and then if I have a good estimate, I can pass to the strong teacher only the points on which I don't trust the week teacher.",
            "So have a good estimate of when should I trust him and why not ipass on only the points on which I don't trust and we can show that this paradigm can really save examples."
        ],
        [
            "So here are the results.",
            "So we want to get this is a start.",
            "What happens with standard learner?",
            "What is happened with what happens in with our algorithm?",
            "How many strong queries like to the strong teacher will queries we need and what is the number of we queries?",
            "Of course the standard learner doesn't make any queries and to achieve our epsilon we let EM epsilon be the number of.",
            "Hey, queries to the strong teacher to the expert radiologist that irregular learner would do without the week teacher without an obvious advice.",
            "And with an obvious advice, we see that our aero slightly deteriorates, but the number of examples improves.",
            "So those parameters Phi and PSI are parameters of their niceness of distribution.",
            "They measure the weight around the boundary areas and they measure how likely it is that you have a point with one label such at all of its neighborhood is truly labeled differently, like an island.",
            "Of a different label than on its neighborhood, so we stood with these two parameters.",
            "We show that using weak teachers for a small decrease of the accuracy we can get a significant saving in the number of queries to the strong teacher that that we need."
        ],
        [
            "OK, so this is these are the three applications that I wanted to discuss, so we wanted to learn.",
            "To see how much unlabeled weakly labeled.",
            "Examples can help you in classification learning, and we tried to give provable bounds for such savings, and we managed to do it in those three cases.",
            "The proper semi supervised learning when you needed to.",
            "Bring to come up with the prediction from a given class and the domain adaptation where you had labeled examples from the source, but only unlabeled examples from the target and in the learning from weak teachers.",
            "In all of these three areas, we could show provable benefits of the use of unlabeled or weakly labeled examples, and."
        ],
        [
            "So what are the remaining open questions?",
            "The main warning I should give you is of course don't be fooled.",
            "This is only a theoretical research.",
            "I mean we haven't run it, we haven't tried it.",
            "In theory it works.",
            "I mean, we can prove that it works, but we need some real applications too.",
            "And all my students now are theoretical students.",
            "They don't like to run any applications, so I pass it on to you.",
            "The real remaining challenges to find ways to make this paradigms really work in practice, and I hope that I will hear back from you one day with.",
            "Telling me if and how these ideas can really be implemented.",
            "So thank you very much.",
            "So the public, the police ticklishness talks about right?",
            "It's a police exception is some kind of measure of how much weight you have around the decision boundaries, and this is similar to the one of the parameters that we need here, right?",
            "So you you in both cases in all of those applications you want to assume that the weight around the boundary areas between the two classes is not very high.",
            "If there's lots of points that are living close to points of opposing labels, it's very difficult to do the prediction right, so it's the same parameter, right?",
            "Prediction of having aliens is not subsumed by these.",
            "OK, so that yeah, intuitively you can think it is subsumed we can prove we can.",
            "Create artificial situations in which.",
            "Those two things are separated, right?",
            "Right?",
            "Yeah, but it's a very good question.",
            "It's a very natural question to ask, and this is the place where it's fun to do.",
            "The mathematics to come up with contrived examples in which you can show that they are independent.",
            "So yeah, it's a very good question, but there are independent of parameters, right?",
            "Question I didn't really understand the lower bound in the second application domain adaptation, right?",
            "We wanted to have a lower bound on this.",
            "There are square root of the of the size.",
            "Right size was set right.",
            "And there's some conditions?",
            "Or is it OK so the lower bound?",
            "Any algorithm, but it's for the lower bound in the sense that if the only two assumptions that you make is the covariate shift, the labeling is the same and that every point, say its probability of seeing this point in the sample, is at least half of its probability of seeing this point in the target.",
            "So if these are only the only assumptions you make, although they sound pretty strong, the labeling function is the same and every point is at least half likely to be seen in their source.",
            "Another target, if these are the only two assumptions that you make.",
            "Then any algorithm that doesn't have unlabeled data will need square root of the size of X to get, and that's a very strong lower bound, right?",
            "But understanding right, but it's a lower bound, and again, I mean for me it's fun too.",
            "When I get a chance to do some clean combinatorics so clean probability and that's the lower bands are one of those things I don't know how useful they are in practice.",
            "How, how relevant outta practice man?",
            "Yes, I can send you says the for the case will be teachers there been research done before where you could shoot machine random noise on the teachers.",
            "Then use require that the medical medical students get the results wrong half the time, no, no.",
            "Well, they require is that on on points, so I did not get into the details.",
            "The crucial issue isn't defining the similarity neighborhood of a point.",
            "So we define the notion of the similarity neighborhood point.",
            "We assume that the learner has some intuitive notion of which points are similar.",
            "Right, and you want to in situations in which the similarity neighborhood of the point is homogeneous.",
            "Every similar point has the same label.",
            "Then you want the medical students to predict the label rather precisely.",
            "If ever you can think of it as if the novice teacher sees the attributes on a very rough scale.",
            "So when you so show it a point it doesn't measure, say the blood pressure in it measures it in in a quantized way.",
            "So it can confuse the point with this close neighbors.",
            "So if all the neighborhood is the same labeling and you wanted to predict precisely if their similarity neighborhood is at Regina's you want to have some probability for having one label in some probability for having the other label, not half and half.",
            "I mean, just say bounded away from being deterministic, you want to say that on boundary areas they hesitate.",
            "You don't tell them, give me half, of course, that will be too much.",
            "But you guys just say, don't be.",
            "I don't want learners that are completely confident around the boundary.",
            "I wanted to be a little bit unconfident about the wonder if that's what I need for my algorithm.",
            "Service certainly in the medical application.",
            "It's often very important that false negatives tend to be a lot worse than those positives, so we might accept more false positives just as long as we avoid voice impulse negatives.",
            "Would that be easy to bring into the framework?",
            "So that's a very good question.",
            "We haven't considered it, but that's a very good suggestion for the next step on this right?",
            "I mean, I think this is the kind of input that I was hoping to get here, like people telling me, I mean.",
            "In practice, there is a consideration here that you have.",
            "Overlooked and definitely we haven't looked at this and it's an interesting point.",
            "Another question about the semi supervised so.",
            "Is kind of the other paradigm of 70 supervised learning is where you try and take your labeled and unlabeled some together and do the learning.",
            "And in one stage somehow.",
            "So maybe you are using something about manifold.",
            "Learn some kind of graph structure or something, right?",
            "Do you think that the the clustering assumptions and the assumptions that you are making in your theory there would apply to that type of?",
            "That's OK.",
            "So let me just repeat the question because it's a very good question.",
            "So when semi supervised learning is really being done in practice and the problem is that we don't have theoretical support, we cannot give you guarantee that will really work.",
            "And sometimes people notice that it fails.",
            "Use the unlabeled data and things can deteriorate rather than improve.",
            "And the question is.",
            "If I may, I have now a formalization of an ocean of cluster assumption with this probabilistic sleeplessness.",
            "Under these assumptions one so now we did make a step forward.",
            "We know how to formulate a notion of a cluster assumption, and the data is clustered in a way which makes sense, and it's not completely artificial.",
            "Can we show that the usual methods succeed under this assumption?",
            "That's the main problem that we're working on right now, so.",
            "Very good question.",
            "And then another one at this rate, which was that you were using the.",
            "This is what you call the ugly classifier to then learn very simple linear classifier on the full and they will say.",
            "But it seems like maybe you know an ensemble of simple classifiers would also be cheap to compute.",
            "Run sign as well and there may be that your compact complex classifier could.",
            "She could easily give you a distribution over.",
            "Over simple, you know, a combination of simple linear classifiers and that might give you a boost in performance.",
            "There you know, OK, your data wasn't really linearly separable, but maybe a combination of few, right?",
            "So I'm not, it's not.",
            "This result is restricted to linear classifiers.",
            "It's it's very general to any situation in which the class of desirable classifiers is not the class that has.",
            "Non zero approximation error.",
            "So we have some notion of desirable classifiers and you don't have a perfectly good classifier in this class.",
            "Under that scenario, we can show that you get the benefit if not necessarily half spaces, and this scenario, in which the desirable classifiers are not likely to give you the error.",
            "I think it's pretty common that nice classifiers do have some inevitable error just because being nice is not reflecting the real world very well under such a scenario, we can show the benefits of the unlabeled data.",
            "Is there something that for the medical students example, but their knowledge is somehow sort of linear in that they know roughly like they know?",
            "This is roughly what achievement looks like and then the next week later, abit tighter rather than.",
            "They done, you know exactly what one time machine looks like.",
            "And yeah, right, yeah, so I think I think a good way of modeling this assumption is saying that the week teacher, when it looks at the attributes it quant quantized them at to Inna course way.",
            "And then the better the teacher, the finer the can see the input.",
            "Yeah, missing is that true for students?",
            "So that's a good question.",
            "That's that's a good question, no.",
            "And then I think it's a very because I also think that there is not going to be a unique model for week teacher that will apply overall applications.",
            "We can think of.",
            "I mean, it's very likely that we were trying to look at a mechanical Turk.",
            "Just saying, you know, giving them images and asking you know where is Val?",
            "Do you know that this kind of and sometimes it's easier to find him in front?",
            "Sometimes how to find him?",
            "And.",
            "The kind of mistakes that you get consistently are different than what you get for medical.",
            "Students are trying to label Benjam, as I don't think that there's going to be a unique way of labeling of modeling week teachers.",
            "I think this research will have to have a variety of options and you will have to based on your prior knowledge, decide which model is most suitable for your application.",
            "A new product.",
            "Maybe you have circled it already, which would be to assess to what extent the week teachers satisfy this hesitant condition.",
            "Oral to type with addictive risk that everybody is very, very sure value.",
            "But being very Commission of the wrong thing.",
            "Which would give no no.",
            "Yeah no, that's not.",
            "It would kill me only if they're very convinced on the wrong thing near a boundary.",
            "I.",
            "Anyway, I mean, but yeah, we did try.",
            "I mean it's very difficult to get this results for the brain imaging we don't have real results for the brain imaging.",
            "For kind of things like Mechanical Turk, on the very simple task, we know that it behaves slightly different than our model of like teachers.",
            "And as I answered before, I think that there should be different models of like teachers, but I think that what's interesting here is just to bring forward this question.",
            "I mean, because it is something that may become practical."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think this is going to be a theoretical talk, but it's.",
                    "label": 0
                },
                {
                    "sent": "Motivated by practical implications.",
                    "label": 0
                },
                {
                    "sent": "So let me just start by, so I hesitated between talking about clustering because I realized clustering here is very popular and an also semi supervised learning, but his more recent stuff so.",
                    "label": 0
                },
                {
                    "sent": "And to me it's more exciting, so I chosen a very egocentric way.",
                    "label": 0
                },
                {
                    "sent": "This is joint work, mainly with my PhD student, Ruth owner that it's a series of paper that we've been doing through the past year and a half or so.",
                    "label": 0
                },
                {
                    "sent": "Some of these papers that we should have shot Sanoja Jamir, an one paper with my son, Solomon David.",
                    "label": 0
                },
                {
                    "sent": "So I'm very lucky that I have a son that is now.",
                    "label": 0
                },
                {
                    "sent": "We agreed to work with me OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we want to talk about utilizing label data for classification, prediction, learning and the many applications in which unlabeled data is cheap and abundant, and on the other hand, classified data is how to get by.",
                    "label": 1
                },
                {
                    "sent": "So one example, typical example is email for spam.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to get lots of emails, but it's very difficult to get the user to really label every email, whether it's permanent.",
                    "label": 1
                },
                {
                    "sent": "Another one is part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to get big corpora of text.",
                    "label": 0
                },
                {
                    "sent": "But if you want to do each word to say, each part of which part of speech is it, then it is a very laborious work.",
                    "label": 0
                },
                {
                    "sent": "It costs money very difficult to get labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Another we saw yesterday you want to classify tweets for the sentiment and it's very easy to get the tweets but to get the correct sentiments.",
                    "label": 0
                },
                {
                    "sent": "It's difficult, so there's a big drive to see.",
                    "label": 0
                },
                {
                    "sent": "How can we utilize unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "Downloaded data is cheap.",
                    "label": 0
                },
                {
                    "sent": "Can it really help us and?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to do here is talk about several scenarios in which unable data can really utilize to help in classification learning, and then I'll explain some algorithmic paradigms.",
                    "label": 1
                },
                {
                    "sent": "So there's not.",
                    "label": 0
                },
                {
                    "sent": "There's no one way in which you can utilize unlabeled another several ways.",
                    "label": 1
                },
                {
                    "sent": "I'll describe three different ways in which unable that can be utilized, and I'll talk a little bit about the theoretical analysis we want to really be able to prove that by using valuable data you really save.",
                    "label": 0
                },
                {
                    "sent": "Improve the accuracy or save the need in labeled examples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'll describe three paradigms for three setups in which you can probably utilize unable samples, so one of them is proper, semi supervised learning, semi supervised learning is the this whole area where you have labeled data and unlabeled data and you want to take advantage of the animal data proper.",
                    "label": 1
                },
                {
                    "sent": "Semi supervised learning is when you have a prescribed class that you want the your predict account from this class.",
                    "label": 0
                },
                {
                    "sent": "So there is some desirable way of a predictor, say want to predict with half space because of.",
                    "label": 0
                },
                {
                    "sent": "Different reasons that I'll describe.",
                    "label": 0
                },
                {
                    "sent": "Can you come up with a good headspace and for that purpose we can show how unlabeled data can help you.",
                    "label": 0
                },
                {
                    "sent": "The other domain area is domain adaptation to meditation in situation where you train on one type of data, but you application comes from a slightly different type of data.",
                    "label": 0
                },
                {
                    "sent": "In that case, unlabeled data can help.",
                    "label": 0
                },
                {
                    "sent": "You will see how and the third one is a very new kind of a direction that we're playing with, and this is learning from week teachers.",
                    "label": 0
                },
                {
                    "sent": "So recently there's a lot of buzz around, like using information like the Amazons Mechanical Turk crowdsourcing, but we know that once you will rely on labeling from Mechanical Turk that lots of errors, those errors are not just random errors.",
                    "label": 0
                },
                {
                    "sent": "I mean we do have work about random noise, but that's not random noise.",
                    "label": 0
                },
                {
                    "sent": "They're consistent reasons why they make certain error, not another.",
                    "label": 0
                },
                {
                    "sent": "We want to see how can you utilize.",
                    "label": 0
                },
                {
                    "sent": "Labels that are coming from things like Mechanical Turk from week teachers to improve prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is the three areas Delta.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About so let me start with a semi supervised learning, so the similar learning the first approach was the.",
                    "label": 0
                },
                {
                    "sent": "Hey, you want to use it to improve accuracy, so when can unlabeled data help you to use to improve accuracy?",
                    "label": 1
                },
                {
                    "sent": "The intuition says that if your data is clustered into cleanly separated clusters and every cluster is homogeneous in terms of its label, then if you have the unlabeled data, you can detect those clusters and then labeling becomes very easy.",
                    "label": 0
                },
                {
                    "sent": "You just need one example from every cluster and you know the labeling of everything.",
                    "label": 0
                },
                {
                    "sent": "So that's a very appealing idea.",
                    "label": 0
                },
                {
                    "sent": "The two big problems with it.",
                    "label": 0
                },
                {
                    "sent": "One problem is that it is not clear how to formulate the assumption of classical data.",
                    "label": 1
                },
                {
                    "sent": "I mean, sure, we can draw pictures and have clusters which are separated, but you want some formalization that on one hand would be mathematically solid.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, will be relevant to applications and implications.",
                    "label": 0
                },
                {
                    "sent": "Your data is never appearing in nice bowls which are away from each other.",
                    "label": 0
                },
                {
                    "sent": "So there is a real issue of how do you formalize this notion of cluster ability?",
                    "label": 0
                },
                {
                    "sent": "Will talk about it.",
                    "label": 0
                },
                {
                    "sent": "The other problem is that in order to apply this approach, you want to assume that every cluster is homogeneously labeled.",
                    "label": 1
                },
                {
                    "sent": "So this chasm correlation between the labels and the arrangement of the unlabeled data and.",
                    "label": 0
                },
                {
                    "sent": "This isn't something which is very difficult to verify actually verifying it is just as hard as finding out what are the labelings and we could show in previous work that without this assumption.",
                    "label": 0
                },
                {
                    "sent": "The basic paradigm of semi silver element may badly fail.",
                    "label": 0
                },
                {
                    "sent": "So rather than trying to do semi supervised learning in this straightforward way, we're talking about, we're going to talk about some subproblem of semi supervised learning and that's the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Them of properly Nick, so in proper learning.",
                    "label": 1
                },
                {
                    "sent": "In some applications you have requirements about the type of classifiers that you want to output, so the exact the motivating example for us was there is this company which is called mobile.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you heard about it.",
                    "label": 0
                },
                {
                    "sent": "They build the devices for cars that help the driver.",
                    "label": 0
                },
                {
                    "sent": "So for example they watched the driver and sound an alarm if they figure that the driver is falling asleep or if they figure that you're getting too close to another car.",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "Now for them you want to find out.",
                    "label": 0
                },
                {
                    "sent": "So if you check features about the driver, how is moving his head, how his eyes are and so on.",
                    "label": 0
                },
                {
                    "sent": "You check those features and you want to predict is the driver falling asleep or not.",
                    "label": 0
                },
                {
                    "sent": "So you have a predictable but the problem is that when you already figured out a predictor, you learn to predict.",
                    "label": 0
                },
                {
                    "sent": "Oh now you see the features.",
                    "label": 0
                },
                {
                    "sent": "From the driver you want to predict very fast.",
                    "label": 0
                },
                {
                    "sent": "It will be kind of useless if you allow the driver that he fell asleep.",
                    "label": 0
                },
                {
                    "sent": "A minute after he did fall asleep, it may be a bit too late for the driver, so speed is very important.",
                    "label": 0
                },
                {
                    "sent": "You want classifiers you are willing to sacrifice some accuracy for gaining speed.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe your driver will get some false positives and they will tell him he's falling asleep and he'll say I'm not asleep.",
                    "label": 0
                },
                {
                    "sent": "I'm a very alive but it's not so bad as being late in alarming, so we're willing to sacrifice some accuracy for having a predictor of some certain type certain type that can you see a point you can predict very fast.",
                    "label": 0
                },
                {
                    "sent": "Another type of desirability requirements from predictors is.",
                    "label": 1
                },
                {
                    "sent": "They are going to be readily interpretable if I want to find a predictor for medical purposes.",
                    "label": 0
                },
                {
                    "sent": "I want to explain to doctors what causes Alzheimer's disease.",
                    "label": 0
                },
                {
                    "sent": "It is kind of.",
                    "label": 0
                },
                {
                    "sent": "Frustrating for them if you tell them OK, I have this huge neural network.",
                    "label": 0
                },
                {
                    "sent": "You feed your features.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you what the outcome is, but you want to tell the patient you know, eat less butter, drink more red wine, whatever you want, something that can be easily be interpreted so you want you are willing to sacrifice some accuracy here for the sake of interpretability.",
                    "label": 0
                },
                {
                    "sent": "So we're talking here about situations in which we have a certain class of desirable classifiers, and you want the prediction to come from that class.",
                    "label": 0
                },
                {
                    "sent": "So a good example of such a classifier, half spaces of course halfspaces are very nice, we can compute them very fast if you have a high space, you have a point, you just do the dot product, you know if it's above or below the half space and also very interpretable because just give you weights for every feature so you can see which are the important features.",
                    "label": 0
                },
                {
                    "sent": "So say our desirable classes have spaces and now we want to find prediction which is going to predict which is going to be a linear have space for that purpose.",
                    "label": 0
                },
                {
                    "sent": "We can show that unlabeled data probably.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apps so.",
                    "label": 0
                },
                {
                    "sent": "This is what we're going to talk about.",
                    "label": 0
                },
                {
                    "sent": "I call it the usability utility feasibility tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want a very usable, usable.",
                    "label": 0
                },
                {
                    "sent": "Predictor and the first demonstration of the potential use of unlabeled sample.",
                    "label": 1
                },
                {
                    "sent": "We're going to show that such samples can be utilized to overcome this utility feasibility tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So we have a class of desirable classifiers and we show her unable surplus can give.",
                    "label": 0
                },
                {
                    "sent": "You can help?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You find a good classifier from this class, so here is my formal set up.",
                    "label": 0
                },
                {
                    "sent": "The formal setup is we have some domain set, say it's lives in Rd.",
                    "label": 0
                },
                {
                    "sent": "And you have a labeling set.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, assume that you have only binary labels, either 01 and there is some target distribution which is not known to the learner that generates the data.",
                    "label": 0
                },
                {
                    "sent": "And now you're given a class of disabled predictors H and you get a sample and ID sample from your distribution and you want to.",
                    "label": 1
                },
                {
                    "sent": "On top of it to utilized a large, unlabeled sample so we have a small, you have a small label sample S Ann.",
                    "label": 1
                },
                {
                    "sent": "You have a large unlabeled sample T and you want to use them in order to find the best predictor in the class.",
                    "label": 1
                },
                {
                    "sent": "H, say the class H is linear, half spaces.",
                    "label": 0
                },
                {
                    "sent": "That will minimize the error.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is my first algorithmic approach.",
                    "label": 0
                },
                {
                    "sent": "So what you're going to do is you are going to use the labeled sample to same to learn some predictor from a surrogate class.",
                    "label": 1
                },
                {
                    "sent": "So you're going to use the label data to learn some predictor, which is not nice, which is not a linear predictor.",
                    "label": 0
                },
                {
                    "sent": "It could be a complex neural network, it could be some will see examples of how it is used.",
                    "label": 0
                },
                {
                    "sent": "In practice it could be something based on the kernel side, like champion coefficients, which are very difficult to compute.",
                    "label": 0
                },
                {
                    "sent": "You get some.",
                    "label": 0
                },
                {
                    "sent": "Ugly but precise predictor, but you wanted a linear classifier.",
                    "label": 1
                },
                {
                    "sent": "So in the second stage you apply your already existing classifier to label the unlabeled sample.",
                    "label": 0
                },
                {
                    "sent": "So we have the unlabeled sample.",
                    "label": 0
                },
                {
                    "sent": "You now take the ugly classifier and you use it to label the unlabeled sample.",
                    "label": 0
                },
                {
                    "sent": "Now you get this big sample of the previously unlabeled points is now labeled not completely correctly, because you're using a classifier that you learned, but it is almost correctly, and you use that to feed it into a learner for the class that you really want.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that.",
                    "label": 0
                },
                {
                    "sent": "From the point of view of the class that you really want to learn rather than having this small labeled sample, you now got a big sample which was generated from the unlabeled data and that will allow you to find linear halfspace faster.",
                    "label": 0
                },
                {
                    "sent": "So here is the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some examples where this approach has been used in the past, so we first came up with this idea and did the theory.",
                    "label": 0
                },
                {
                    "sent": "Then we found out that some people use it in practice, there was no previous analytical analysis of it, but the idea is simple enough.",
                    "label": 0
                },
                {
                    "sent": "It has been used so these are two papers that used similar idea in practice, so the first one wanted to learn fast independent district regression classifiers and they used conditional random fields which are more expressive but very.",
                    "label": 1
                },
                {
                    "sent": "Very slow to compute.",
                    "label": 0
                },
                {
                    "sent": "They are very computationally intensive.",
                    "label": 1
                },
                {
                    "sent": "The other one use a similar idea for NLP.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An so.",
                    "label": 0
                },
                {
                    "sent": "We analyze how in this kind of framework, the unlabeled data probably.",
                    "label": 0
                },
                {
                    "sent": "Help you so we analyze it in kind of two.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Set up one of them is that our our classifier.",
                    "label": 0
                },
                {
                    "sent": "Our correct labeling is realizable by some.",
                    "label": 0
                },
                {
                    "sent": "Bigger class of ugly classifiers.",
                    "label": 0
                },
                {
                    "sent": "And the other situation is in which our data satisfies some.",
                    "label": 0
                },
                {
                    "sent": "Cluster assumptions, some niceness assumptions, and then we can use as the first classifier.",
                    "label": 0
                },
                {
                    "sent": "We can use nearest neighbor, so we discuss these two types of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assumptions and it is our algorithm as I described, so you first learn so H prime is your ugly class.",
                    "label": 0
                },
                {
                    "sent": "H is the desirable class.",
                    "label": 0
                },
                {
                    "sent": "So the first step is you first learn a classifier in the ugly class.",
                    "label": 0
                },
                {
                    "sent": "Using the the label sample, we assume that the ugly class is easy to learn, so you can learn it with a small label sample.",
                    "label": 0
                },
                {
                    "sent": "Then what you do is you use this H prime classifier to label the big unable data T and then you feed this set T, which is now labeled to an agnostic learner for the desirable class H and the point is that agnostic learners are kind of robust to noise, so if you have some errors on your labeling on T, it will not effect the final result by much.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the kind of results that we can get is that.",
                    "label": 0
                },
                {
                    "sent": "We can learn using a sample if you have a so.",
                    "label": 0
                },
                {
                    "sent": "This formula is the most important thing is the dependent on epsilon.",
                    "label": 0
                },
                {
                    "sent": "Epsilon is the accuracy that you get at the end so.",
                    "label": 0
                },
                {
                    "sent": "Usual learning in diagnostic setting requires dependence of one over epsilon square.",
                    "label": 0
                },
                {
                    "sent": "Here we can show that you can settle for this label.",
                    "label": 0
                },
                {
                    "sent": "Sample is just of order one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "You need one over Epsilon square, unlabeled points but unlabeled points are cheap.",
                    "label": 0
                },
                {
                    "sent": "So with lots of unlabeled points but few labeled points, you get accuracy which is close to the optimum class plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we have gained the gap between one over Epsilon Square one over epsilon in terms of the number of labeled example that we require.",
                    "label": 0
                },
                {
                    "sent": "Once we have this unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "As such, games can come in different ways that can come due to having a lower approximation error.",
                    "label": 1
                },
                {
                    "sent": "If I don't know how much we want to get into this theory.",
                    "label": 0
                },
                {
                    "sent": "But if you have a class with a with a lower approximation error, then the convergence rates of the errors are faster.",
                    "label": 0
                },
                {
                    "sent": "So if I have the ugly class with lower approximation error, I get faster convergence rates.",
                    "label": 1
                },
                {
                    "sent": "Other reasons can be different in this dimension.",
                    "label": 0
                },
                {
                    "sent": "Other reasons could be computational considerations, so we have a class that we can learn.",
                    "label": 0
                },
                {
                    "sent": "Fast.",
                    "label": 0
                },
                {
                    "sent": "But undesirable classifiers we use it to label the animal data and then then the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Disable class here is a demonstration of how it works, so assume that your true labeling looks like this.",
                    "label": 0
                },
                {
                    "sent": "Then you see the minuses into pluses, so the Gray area are the minuses of the true labeling and I want to come up with a linear predictor.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that no linear predictor will give me a.",
                    "label": 0
                },
                {
                    "sent": "Perfect prediction here, but they can get perfect prediction with a predictor, which is just the union of two half spaces.",
                    "label": 0
                },
                {
                    "sent": "So what I do is I first learned this union half spaces since I have a Union halfspaces that predicts the data precisely.",
                    "label": 0
                },
                {
                    "sent": "I can do it with sample size that grows only as one over the error.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, once I have the correct predictable, I used the unlabeled data to decide which linear classifier is better.",
                    "label": 1
                },
                {
                    "sent": "If there's more weight, he'll then I prefer this linear classifier.",
                    "label": 0
                },
                {
                    "sent": "If the wait here is bigger, I prefer this linear classifier, but that decision can be made just by viewing the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "I don't need more label examples.",
                    "label": 0
                },
                {
                    "sent": "So in such a situation, we can really show.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So lower bounds that if I did not have the unlabeled data, I have a lower bound that behaves like one over epsilon Square.",
                    "label": 1
                },
                {
                    "sent": "And with the use of the unlabeled data, I could use, I could get accuracy epsilon from just one over epsilon label examples, so there's approvable.",
                    "label": 0
                },
                {
                    "sent": "Advantage to the use you probable utility, from the unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, we.",
                    "label": 0
                },
                {
                    "sent": "The second.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another situation which we can get such an advantage is.",
                    "label": 0
                },
                {
                    "sent": "If you have some nice cluster assumption, so how is the cluster assumption we are trying to formalize the notion that the data nicely partitioned into clusters.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing it was saying the labeling function satisfies the Lipschitz condition was election conditions.",
                    "label": 1
                },
                {
                    "sent": "If two labels.",
                    "label": 0
                },
                {
                    "sent": "If two points have different labels and must be far apart.",
                    "label": 0
                },
                {
                    "sent": "So in order to satisfy the Lipschitz, a usual Lipschitz condition of the labeling function, the data must split into completely disjoint clusters.",
                    "label": 0
                },
                {
                    "sent": "This is not a very realistic scenario if the data underlying distribution, the major distribution is kind of continuous.",
                    "label": 0
                },
                {
                    "sent": "Then the only way it will satisfy Lipschitz condition is if the labeling function is constant because you otherwise you will always have two points that have different labels and their arbitrary close to each other.",
                    "label": 0
                },
                {
                    "sent": "So we relaxed these slips in this condition.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To a notion of probabilistically blip sciousness we just look at the probability that there will be a point with a close neighbor that has a different label.",
                    "label": 0
                },
                {
                    "sent": "And this probabilistic lips S can be viewed as a way of measuring how clusterable the data is.",
                    "label": 0
                },
                {
                    "sent": "So this set of bad points the wise that have a neighbor which is closer than Lambda to them but has a different label.",
                    "label": 0
                },
                {
                    "sent": "This set is the kind of the set of points that live around the boundary of the decision.",
                    "label": 1
                },
                {
                    "sent": "Student boundary, so we want to say that the probability of seeing a points around the decision boundary is small, so that's.",
                    "label": 0
                },
                {
                    "sent": "A formal way of describing that my data is clusterable, and it's more realistic than saying that it splits into disjoint clusters.",
                    "label": 0
                },
                {
                    "sent": "It is just a way of formulating the area around the boundary.",
                    "label": 0
                },
                {
                    "sent": "The decision boundaries or the weight of the points around decision boundaries is small, so we call it probabilistically Christmas and we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This assumption so for example data that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Say this is labeled.",
                    "label": 0
                },
                {
                    "sent": "Plus this is label minus.",
                    "label": 0
                },
                {
                    "sent": "This data does not satisfy Lipschitz condition because they have arbitrarily close points with opposing labels, but it does satisfy a very nice probabilistic lipnice because the probability of sync two points which are close and have different labels is very small.",
                    "label": 0
                },
                {
                    "sent": "So with such a.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assumption.",
                    "label": 0
                },
                {
                    "sent": "We can prove that the nearest neighbour algorithm is a very good predictor.",
                    "label": 0
                },
                {
                    "sent": "The nearest neighbor algorithm is not, of course, undesirable predictor, but we can use it as the first stage of our.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process so we get similar.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bounce here I will not get.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into the formulas is a similar situation is we've seen before that the unlabeled data gives you gives you advantage over labeled if you over situation where you use only label data.",
                    "label": 0
                },
                {
                    "sent": "So I want to just show graphs of two experiments that we did, so we did experiments with the basic amnesty digit recognition database.",
                    "label": 0
                },
                {
                    "sent": "So we were trying to do binary classification between the digits zero to four and which is 5 to 9.",
                    "label": 0
                },
                {
                    "sent": "With 60,000 training examples and 10,000 test examples, and.",
                    "label": 1
                },
                {
                    "sent": "Here again, our ugly classifiers were kernel based on this kernel function, which is very difficult to compute, so this is a kernel that you can use for the training, but if you need to be fast at prediction time, you cannot use this kernel because it is too computationally extensive.",
                    "label": 0
                },
                {
                    "sent": "And we want to come up with a good linear class.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sapphire, so here are the results.",
                    "label": 0
                },
                {
                    "sent": "So let me explain to you what you see, what you see here.",
                    "label": 0
                },
                {
                    "sent": "This is the number of training example labeled training examples that we use if we.",
                    "label": 0
                },
                {
                    "sent": "If we train this complex kernel, this is the way the rate the error rate goes down.",
                    "label": 0
                },
                {
                    "sent": "So it's a very good kernel.",
                    "label": 0
                },
                {
                    "sent": "The problem is you can you not use it in prediction time because it's too slow to predict.",
                    "label": 0
                },
                {
                    "sent": "If I give you a point and you want to predict you have to compute its kernel function to all the points.",
                    "label": 0
                },
                {
                    "sent": "Told the training points and that's very slow.",
                    "label": 0
                },
                {
                    "sent": "Instead, we use it to label a big unable data set and come up with a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So what you see above is the error rate of linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "The blue line is there way to fill in a classifier without unlabeled data, so it goes down much more slowly.",
                    "label": 0
                },
                {
                    "sent": "The red curve is the linear classifier.",
                    "label": 0
                },
                {
                    "sent": "When I do use unlabeled data and I use the kernel to label the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So I can get it about 2000 points.",
                    "label": 0
                },
                {
                    "sent": "I get 2000 label points.",
                    "label": 0
                },
                {
                    "sent": "For the linear classifiers I get the same accuracy as without the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "I would need in the order of 20,000 label points.",
                    "label": 0
                },
                {
                    "sent": "And we have a similar.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where is the nearest neighbor classifier with the same story, except that here the fast the fast converging rate is from nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Again nearest neighbor is not very practical, specially when you have 10s of thousands of examples, but we just use it offline to label the unlabeled points and then we can get the same kind of advantage from the use of unlabeled points to output a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was my first use that was using unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "In the situation of semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "When you have a requirement to output a predictor from a given class.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one thing you can notice here it's you can ask how many unlabeled examples do I need, and that's that's a question that can be formulated on its own, and that's the question that I have investigated in this.",
                    "label": 1
                },
                {
                    "sent": "Also 2011 two weeks ago with my son and the question is it's kind of paradoxical question you ask yourself.",
                    "label": 0
                },
                {
                    "sent": "How difficult it is to learn when you know in advance what is the labeling?",
                    "label": 0
                },
                {
                    "sent": "So what is there to learn?",
                    "label": 0
                },
                {
                    "sent": "I mean, you know the leveling in advance, but what you want to learn is what is the best predictor from your class.",
                    "label": 0
                },
                {
                    "sent": "So we want to find like we saw the picture before.",
                    "label": 0
                },
                {
                    "sent": "I want to find the best linear predictor when I know that no linear predictor is perfect.",
                    "label": 0
                },
                {
                    "sent": "So in order to find the best linear predictor I have to get some intuition about where is the weight of the unlabeled data and the question is how many unlabeled examples you need to make such decisions.",
                    "label": 1
                },
                {
                    "sent": "So we call this model the known label classifier, learning KCL.",
                    "label": 0
                },
                {
                    "sent": "So you're given.",
                    "label": 0
                },
                {
                    "sent": "Some labeling function and your input is an unlabeled sample and you want to find a predictor from a given class.",
                    "label": 1
                },
                {
                    "sent": "This class does not have a F inside.",
                    "label": 0
                },
                {
                    "sent": "You want to find the predictor that minimizes the error with spectral.",
                    "label": 0
                },
                {
                    "sent": "This unknown distribution channel distribution is disclosed to you only through the unlabeled data, and we got there.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A clean results.",
                    "label": 0
                },
                {
                    "sent": "We got combinatorial optimization of the complexity of this problem.",
                    "label": 0
                },
                {
                    "sent": "So what I'm talking about now is how many unlabeled examples I need.",
                    "label": 0
                },
                {
                    "sent": "What I showed you before is that unlabeled examples save you labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Now it's kind of side issue how many of these unlabeled examples we need, and we gotta classification trichotomy into three types of classes that we can easily distinguish.",
                    "label": 0
                },
                {
                    "sent": "In one of them, you don't need any labeled examples.",
                    "label": 0
                },
                {
                    "sent": "In the other, you need order of one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled example in the third class, which is most common, you need one over epsilon square.",
                    "label": 0
                },
                {
                    "sent": "Labeled unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "Just to learn how to get the advantage from unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So the question is how many unlabeled examples do you need?",
                    "label": 0
                },
                {
                    "sent": "If they are very cheap, you don't really care about this question.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the same supervised learning my second set up is the set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set up of domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "So what is a meditation?",
                    "label": 0
                },
                {
                    "sent": "Most of the classical machine learning assumes that the training data in the test data come from the same distribution, but.",
                    "label": 1
                },
                {
                    "sent": "It's not always a realistic assumption.",
                    "label": 0
                },
                {
                    "sent": "So in many machine learning applications it may happen that you train on one type of data, but you it's test time you have slightly different data, so here.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some examples, so say you want to learn in some way to distinguish from images of faces, men from women, and you train on images of.",
                    "label": 0
                },
                {
                    "sent": "Asian people and you want to predict on images of Caucasian people?",
                    "label": 0
                },
                {
                    "sent": "I mean it's a different distribution.",
                    "label": 0
                },
                {
                    "sent": "Is the same task you train on one you want to test another.",
                    "label": 1
                },
                {
                    "sent": "A very common one, the one I was concerned with when we wrote those papers is my daughter was learning how to drive.",
                    "label": 0
                },
                {
                    "sent": "So of course you learn to drive on one car, then at the testing you may have to drive a different car.",
                    "label": 1
                },
                {
                    "sent": "How do you adapt?",
                    "label": 0
                },
                {
                    "sent": "You do the manifestation intuitively right.",
                    "label": 1
                },
                {
                    "sent": "Another very practical application is spam filters for spam filters.",
                    "label": 0
                },
                {
                    "sent": "I mean if Google tries to come up with the spam filter, there is a big issue that they don't know what is distribution of emails that you will get.",
                    "label": 0
                },
                {
                    "sent": "They want to sell you the spam filter, but they cannot train it on the email that you will get the training on.",
                    "label": 0
                },
                {
                    "sent": "Different corpora of emails.",
                    "label": 0
                },
                {
                    "sent": "So this issue of domain adaptation.",
                    "label": 1
                },
                {
                    "sent": "Another area which is very I don't know why you would get this natural language processing and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "You have corporate off.",
                    "label": 0
                },
                {
                    "sent": "Data that you say you want to do.",
                    "label": 0
                },
                {
                    "sent": "So speech tagging, so you have data, say coming from one kind of documents that you did have training, but you want to apply your part of speech tagging to a different type of document, say one of them was legal documents.",
                    "label": 0
                },
                {
                    "sent": "You want to apply it to.",
                    "label": 0
                },
                {
                    "sent": "Medical documents it's a different distribution of words.",
                    "label": 0
                },
                {
                    "sent": "How can you adapt your classifier from one training set to a different application set so we call them source and target?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's lots of practical work on this domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "Alot of it in the natural language processing community.",
                    "label": 0
                },
                {
                    "sent": "Also in the imaging community.",
                    "label": 0
                },
                {
                    "sent": "But there's very little to vertical understanding, So what we're trying to do here is to come up with theoretical analysis.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of domain adaptation, and there are three aspects of the problem that determine what is the task at hand.",
                    "label": 0
                },
                {
                    "sent": "One of them is what assumptions you make about the relationship between the training data and test data.",
                    "label": 0
                },
                {
                    "sent": "Of course, if there is no relationship between them, there's no way you can learn on one and apply to the other.",
                    "label": 0
                },
                {
                    "sent": "So one thing you have to determine is how what is the relationship between the domain and target tasks.",
                    "label": 0
                },
                {
                    "sent": "The other thing you have to determine is what kind of data do you get.",
                    "label": 0
                },
                {
                    "sent": "Do you get only labeled examples from the training or you also have some training examples from the target?",
                    "label": 0
                },
                {
                    "sent": "What kind of input do you have and the last component is you have to determine what is the kind of prior knowledge that the learner has.",
                    "label": 0
                },
                {
                    "sent": "We know from the no free lunch theorem, no learning is possible without any prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "We want to formalize to formalize the prior knowledge here to analyze under what kind of prior knowledge can you do the meditation.",
                    "label": 0
                },
                {
                    "sent": "So these are the three components we need for doing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meditation and let me go through one by one the the way we model that we discuss here.",
                    "label": 0
                },
                {
                    "sent": "So in terms of the inputs available to the learner, we assume here that the learner has labeled data from the source domain and from the target domain.",
                    "label": 1
                },
                {
                    "sent": "It only has unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So if you think of the spam filters, I have some training domain for which someone marked marked what is spam and what is not spam.",
                    "label": 0
                },
                {
                    "sent": "So this is supervised, it's labeled, but I want to apply it to your email so maybe I can.",
                    "label": 0
                },
                {
                    "sent": "If I'm Google I can get a lot of.",
                    "label": 0
                },
                {
                    "sent": "Your emails, I don't know what you call spam or not, but so I have unlabeled data from the target domain.",
                    "label": 0
                },
                {
                    "sent": "I have labels only from the training domain, which is different, so that's the scenario that we are talking about here and we want to find a predictor that has low error with respect to the target.",
                    "label": 0
                },
                {
                    "sent": "So PT denotes the target domain distribution.",
                    "label": 1
                },
                {
                    "sent": "PS denotes the source distribution.",
                    "label": 0
                },
                {
                    "sent": "So I want to find the predictable that has low error with respect to the target distribution.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the input in terms of what assumptions do we make?",
                    "label": 0
                },
                {
                    "sent": "So in this work that many assumptions that people use to analyze the meditation.",
                    "label": 0
                },
                {
                    "sent": "Here we make two simple assumptions.",
                    "label": 0
                },
                {
                    "sent": "One of them is the covariate shift assumption to fancy feast option is the assumption that the labeling is the same in the source in the target they differ only in terms of the marginal unable distribution.",
                    "label": 1
                },
                {
                    "sent": "For example, in natural language processing, part of speech tagging it should be the same tagging regardless of whether you're talking about bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "A documents or you're talking about legal documents, but the distribution of words will be different, so that's the covariate.",
                    "label": 0
                },
                {
                    "sent": "Shift assumption and the other assumption is that there is a bounded weight ratio, the.",
                    "label": 1
                },
                {
                    "sent": "Every point that is likely to appear in the target has also some probability of appearing in the source.",
                    "label": 0
                },
                {
                    "sent": "You need some kind of an assumption like this, otherwise there will be points that only appear in the target, but you never saw them on the training and you have no way of predicting how you're going to behave there.",
                    "label": 0
                },
                {
                    "sent": "So we need some kind of such an assumption to.",
                    "label": 0
                },
                {
                    "sent": "The first assumption is about the labeling.",
                    "label": 0
                },
                {
                    "sent": "The other assumption is about relatedness between the unlabeled distributions, the marginal distributions.",
                    "label": 0
                },
                {
                    "sent": "So these are pretty standard.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Under these assumptions, people usually come up with two type of strategies strategies for the medication can be divided into two types.",
                    "label": 0
                },
                {
                    "sent": "One of them is conservative, so the conservative that patient says I will learn a good predictor for the source and then analyze when having a good predictor for the source is also good enough for the target.",
                    "label": 0
                },
                {
                    "sent": "So I'm not doing any adaptation, I'm just using what I learned for the source and hope that it will work for the target.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of analysis of I called them conservative predictors.",
                    "label": 0
                },
                {
                    "sent": "Adaptive predictors, on the other hand, are predictors for which you really use the unlabeled data, and you adapt your predictor to your knowledge about the target.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's more knowledge about the conservative because they are easier to analyze.",
                    "label": 0
                },
                {
                    "sent": "For example, these are two works that I was involved in one NIPS 2006 when we showed first.",
                    "label": 0
                },
                {
                    "sent": "To ethical results about the success of conservative, under which conditions.",
                    "label": 0
                },
                {
                    "sent": "If you learn for the source, it will be reasonably good for the target.",
                    "label": 0
                },
                {
                    "sent": "We have more recent results here in ICML 2011 that shows that under certain conditions nearest neighbor you can use nearest neighbor for the target, so you will take the labeled examples.",
                    "label": 0
                },
                {
                    "sent": "From the source and when you get the target point, you look at the nearest neighbor and copy its label.",
                    "label": 1
                },
                {
                    "sent": "So on the set of assumptions this can be used, but this is all conservative.",
                    "label": 0
                },
                {
                    "sent": "I'm not really adapting to the target, so here I want to.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use to discuss adaptation.",
                    "label": 0
                },
                {
                    "sent": "How can we utilize the unlabeled data that we have from the targets to improve our prediction?",
                    "label": 0
                },
                {
                    "sent": "So here is an proposal from this paper by it's very natural proposal.",
                    "label": 0
                },
                {
                    "sent": "Use the target unlabeled data to reweigh the training sample so you get the training sample.",
                    "label": 1
                },
                {
                    "sent": "It was generated by the underlying source distribution you want to give it.",
                    "label": 0
                },
                {
                    "sent": "New weights, weights that will reflect how the target distribution behaves.",
                    "label": 0
                },
                {
                    "sent": "And then train on the new weighted newly weighted sum.",
                    "label": 0
                },
                {
                    "sent": "So that sounds very appealing and they are discussing in a series of paper that discussing algorithmically how to do the best we waiting.",
                    "label": 0
                },
                {
                    "sent": "Alas, we could prove that it may really badly fail, I mean this.",
                    "label": 0
                },
                {
                    "sent": "We waiting.",
                    "label": 0
                },
                {
                    "sent": "In some bad scenarios, can completely mislead you so.",
                    "label": 1
                },
                {
                    "sent": "Hey can we use the antibody anyway?",
                    "label": 0
                },
                {
                    "sent": "That will probably 6.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did.",
                    "label": 0
                },
                {
                    "sent": "And I get now to the third component, so the components of the domain adaptation where what is the input?",
                    "label": 0
                },
                {
                    "sent": "What are the assumptions about relationship between the source and target distributions?",
                    "label": 0
                },
                {
                    "sent": "And now I get to the third component and 3rd component is what is the prior knowledge that the user has?",
                    "label": 1
                },
                {
                    "sent": "So we discuss here two types of prior knowledge of 1st is that the user knows some class of predictors that one of them has zero air respect to the source and the other one is that the user knows some task class of predictors HD, such as one of them has 0 error with respect to the target.",
                    "label": 1
                },
                {
                    "sent": "So this is like traditional PAC learning traditional packing.",
                    "label": 0
                },
                {
                    "sent": "In Valiant model you assume that there is some class of predictors that is guaranteed that it has the real error, but here because it's mentioned we can.",
                    "label": 0
                },
                {
                    "sent": "Really make it into two different assumptions.",
                    "label": 0
                },
                {
                    "sent": "Do we have zero inspectors?",
                    "label": 0
                },
                {
                    "sent": "Also, do you have the error respective target and are there any of these assumptions we analyze how?",
                    "label": 0
                },
                {
                    "sent": "Useful is the only.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The data, so we show that under the first assumption.",
                    "label": 1
                },
                {
                    "sent": "If you know a class which is a 0L0 approximation error with Spectra Source, the unlabeled data is useless.",
                    "label": 1
                },
                {
                    "sent": "Anything you can do with the unlabeled data you could also do without an emulator.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Who is the 2nd assumption in the scenario in which you know of a class that has zero target error?",
                    "label": 0
                },
                {
                    "sent": "We can prove that the unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really helps you.",
                    "label": 0
                },
                {
                    "sent": "We have upper and lower bounds.",
                    "label": 0
                },
                {
                    "sent": "So again, what is my algorithmic paradigm here now with MC paradigm is that I'm giving a source generated sample S which is labeled and I have an unlabeled target generated sample T. So.",
                    "label": 0
                },
                {
                    "sent": "What I do is I take my sample S and I clean from the sample S every point which is not hit by the unlabeled sample T. So we assume you have a very big unlabeled sample T you have the label sample S. Any point in S which is not hit by the sample T you throw it out, then you're left with a smaller sample, but the smaller sample becomes realizable by the target distribution, and then you can use it to learn.",
                    "label": 1
                },
                {
                    "sent": "So with this strategy.",
                    "label": 0
                },
                {
                    "sent": "So we're doing some kind of a very crude reweighting.",
                    "label": 0
                },
                {
                    "sent": "You take the sample and you just decide whether you keep a point or throw it away, and you keep a point only if it's also visited by that big, unlabeled sample and we analyzed.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is happening with this scenario and we can show that here in this case, if you have a sufficiently large target generated.",
                    "label": 1
                },
                {
                    "sent": "Samples.",
                    "label": 0
                },
                {
                    "sent": "Then with this number of labeled examples, which is pretty small, it's behaves only like one over epsilon, so small number of labeled example suffices to learn when adaptation.",
                    "label": 0
                },
                {
                    "sent": "Once you have a very big target generating sample.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you don't even have unable data, then we can show a lower bound.",
                    "label": 0
                },
                {
                    "sent": "In this case for domain adaptation you cannot learn without square root of the size of the domain points.",
                    "label": 0
                },
                {
                    "sent": "So it's a huge gap between what you can do with unlabeled data and without labeled it unable data with unlabeled data, you have a very tight, very small bound on the number of samples of labeled examples that you need.",
                    "label": 0
                },
                {
                    "sent": "Without unlabeled data, we can show a lower bound, which is huge.",
                    "label": 0
                },
                {
                    "sent": "The size of the domain.",
                    "label": 1
                },
                {
                    "sent": "So in this setting there probably the unlabeled data makes a big difference.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was my second application.",
                    "label": 0
                },
                {
                    "sent": "The domain adaptation and I'm going into my third application, so we're just trying to look at different areas in which we can probably utilize unlabeled data to save labeled examples.",
                    "label": 0
                },
                {
                    "sent": "So this third application came from a visitor head in Universal.",
                    "label": 0
                },
                {
                    "sent": "Burton wasn't better have a Center for machine learning and trying to have lots of health applications.",
                    "label": 0
                },
                {
                    "sent": "So I was talking to Russ Greiner and he explained to me this problem that are struggling with.",
                    "label": 0
                },
                {
                    "sent": "They want to design an automatic way to classify brain images.",
                    "label": 0
                },
                {
                    "sent": "So you have brain images.",
                    "label": 0
                },
                {
                    "sent": "You'll want to detect whether there is a tumor there or not, where the two tumor is benign or malignant.",
                    "label": 0
                },
                {
                    "sent": "And you want an automatic way of doing it.",
                    "label": 0
                },
                {
                    "sent": "Becausw veggie ologists are very expensive and few of them it will be very nice if you could have automatic system that does it for you.",
                    "label": 0
                },
                {
                    "sent": "The problem is that in order to train such a system you need a lot of training data, so the training data you have to generate lots of lots of images of brain images and then go to this expert of geology and tell him please sit down and classify those.",
                    "label": 0
                },
                {
                    "sent": "3000 images for me because I needed to develop some machine learning application.",
                    "label": 0
                },
                {
                    "sent": "That guy is not likely to do it for you.",
                    "label": 0
                },
                {
                    "sent": "I mean, to sit down and he's very busy.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe you have experience with them or not yet they're not going to sit for you and do things just for the sake of your nice research.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what other options do we have?",
                    "label": 0
                },
                {
                    "sent": "How do we get training data and what rights grinder suggested it?",
                    "label": 0
                },
                {
                    "sent": "Maybe you can get training data from medical students, medical students.",
                    "label": 0
                },
                {
                    "sent": "They're poor.",
                    "label": 0
                },
                {
                    "sent": "You give them $10 per marking such an image and they will be happy to do it.",
                    "label": 0
                },
                {
                    "sent": "That's more available.",
                    "label": 0
                },
                {
                    "sent": "They have lots of money, they'll be OK.",
                    "label": 0
                },
                {
                    "sent": "But of course they are prone to making errors, so this brought up this idea of.",
                    "label": 0
                },
                {
                    "sent": "How can we learn from?",
                    "label": 0
                },
                {
                    "sent": "Cheap what we call weak teachers so.",
                    "label": 0
                },
                {
                    "sent": "Classifying labeling those images.",
                    "label": 0
                },
                {
                    "sent": "By medical students we call them weak teachers and we want to see how can we learn using weak teacher labels.",
                    "label": 1
                },
                {
                    "sent": "Now the main thing to notice.",
                    "label": 0
                },
                {
                    "sent": "I mean there has been working machine learning about aronis labelings, but usually it's about either the labeling is is random noise, random noise is a very different characteristic than this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "I mean if you take medical students there are some images that will always classify correctly.",
                    "label": 0
                },
                {
                    "sent": "And we know the type, the probability of error depends on the image, how the images are likely to a queue, errors, easy ones will not take you out there.",
                    "label": 0
                },
                {
                    "sent": "Another difference is that in random labeling noise, if you repeat the same.",
                    "label": 0
                },
                {
                    "sent": "Query over and over again.",
                    "label": 0
                },
                {
                    "sent": "You can average out the noise, but here when I'm using weak teachers, if I ask this question again and again, there will persist in they give you the same wrong answer, so it is very different than random noises.",
                    "label": 0
                },
                {
                    "sent": "Also very different in malicious nodes.",
                    "label": 0
                },
                {
                    "sent": "These are the two models have been discussed before and the question is how do we model this type of week features and another example of the teachers that I mentioned in the introduction is labeling you get from Mechanical Turk labeling you get from crowdsourcing now also.",
                    "label": 0
                },
                {
                    "sent": "They contain some information, but they may have errors and those errors depend on the type of point that you give them and persistent.",
                    "label": 0
                },
                {
                    "sent": "So how do we model?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those weak teachers so.",
                    "label": 0
                },
                {
                    "sent": "My.",
                    "label": 0
                },
                {
                    "sent": "We want to learn a good labor predictor for model examples that come from two sources, strong teachers, that labels correctly weak teachers that make mistakes, but they can handle clearcut cases.",
                    "label": 1
                },
                {
                    "sent": "And the question is, how do we model the week teachers labeling and how can we utilize labeling coming from weak teachers?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we consider kind of.",
                    "label": 0
                },
                {
                    "sent": "Only requirements like sufficient requirements from week teachers that will make them useful and sufficient requirements are one of them.",
                    "label": 0
                },
                {
                    "sent": "That week teachers can label clearcut cases correctly with high probability.",
                    "label": 1
                },
                {
                    "sent": "The other requirements is that week teacher.",
                    "label": 0
                },
                {
                    "sent": "When you're getting close to the decision boundaries they make.",
                    "label": 1
                },
                {
                    "sent": "They can give you both replies so they get confused around the decision boundaries and the search requirements, and it's there are not too many points in the bottle and areas which is also natural to assume.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Under these assumptions, we model the big teachers rather than defining them.",
                    "label": 1
                },
                {
                    "sent": "We just model this requirements.",
                    "label": 1
                },
                {
                    "sent": "Any labeling rules that will satisfy this requirement can serve as a weak teacher for us and we can utilize tables we get from it.",
                    "label": 1
                },
                {
                    "sent": "So this is a more precise formalization of the requirements we saw in the previous.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slide and other door under those requirements.",
                    "label": 0
                },
                {
                    "sent": "So every rule of labeling that obey these two assumptions, we can utilize it to save queries to the strong teacher.",
                    "label": 0
                },
                {
                    "sent": "So we prove that whenever the underlying data generating distribution satisfies some elements, conditions and the labels generated by a rule that satisfies those requirements, then we can use the utilized the week teachers to get a low error prediction while saving queries to the strong teacher.",
                    "label": 1
                },
                {
                    "sent": "We can actually use these medical students to save queries to the expert radiologist and.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the algorithmic paradigm?",
                    "label": 1
                },
                {
                    "sent": "How do we do it?",
                    "label": 1
                },
                {
                    "sent": "So what we do is we take 2 random samples, we call them S&T is before we use the week teachers to label one of the samples and then we estimate the confidence of the week teacher on one of the points of each of the points of T. So a point is considered confident if all its neighbors have the same label and the content is considered.",
                    "label": 0
                },
                {
                    "sent": "I'm confident if in this neighborhood I see both labels.",
                    "label": 0
                },
                {
                    "sent": "So I estimate the confidence of every point by the homogeneity of its neighborhood, and then I pass onto the strongly teacher to the query to the strong teacher only four points that have heterogeneous neighborhoods only for points on which I'm unconfident so I can show that I can.",
                    "label": 0
                },
                {
                    "sent": "Get a good estimates of the confidence of labeling of a point and then if I have a good estimate, I can pass to the strong teacher only the points on which I don't trust the week teacher.",
                    "label": 1
                },
                {
                    "sent": "So have a good estimate of when should I trust him and why not ipass on only the points on which I don't trust and we can show that this paradigm can really save examples.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "So we want to get this is a start.",
                    "label": 1
                },
                {
                    "sent": "What happens with standard learner?",
                    "label": 1
                },
                {
                    "sent": "What is happened with what happens in with our algorithm?",
                    "label": 0
                },
                {
                    "sent": "How many strong queries like to the strong teacher will queries we need and what is the number of we queries?",
                    "label": 1
                },
                {
                    "sent": "Of course the standard learner doesn't make any queries and to achieve our epsilon we let EM epsilon be the number of.",
                    "label": 0
                },
                {
                    "sent": "Hey, queries to the strong teacher to the expert radiologist that irregular learner would do without the week teacher without an obvious advice.",
                    "label": 0
                },
                {
                    "sent": "And with an obvious advice, we see that our aero slightly deteriorates, but the number of examples improves.",
                    "label": 0
                },
                {
                    "sent": "So those parameters Phi and PSI are parameters of their niceness of distribution.",
                    "label": 0
                },
                {
                    "sent": "They measure the weight around the boundary areas and they measure how likely it is that you have a point with one label such at all of its neighborhood is truly labeled differently, like an island.",
                    "label": 0
                },
                {
                    "sent": "Of a different label than on its neighborhood, so we stood with these two parameters.",
                    "label": 1
                },
                {
                    "sent": "We show that using weak teachers for a small decrease of the accuracy we can get a significant saving in the number of queries to the strong teacher that that we need.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is these are the three applications that I wanted to discuss, so we wanted to learn.",
                    "label": 0
                },
                {
                    "sent": "To see how much unlabeled weakly labeled.",
                    "label": 1
                },
                {
                    "sent": "Examples can help you in classification learning, and we tried to give provable bounds for such savings, and we managed to do it in those three cases.",
                    "label": 0
                },
                {
                    "sent": "The proper semi supervised learning when you needed to.",
                    "label": 1
                },
                {
                    "sent": "Bring to come up with the prediction from a given class and the domain adaptation where you had labeled examples from the source, but only unlabeled examples from the target and in the learning from weak teachers.",
                    "label": 0
                },
                {
                    "sent": "In all of these three areas, we could show provable benefits of the use of unlabeled or weakly labeled examples, and.",
                    "label": 1
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the remaining open questions?",
                    "label": 0
                },
                {
                    "sent": "The main warning I should give you is of course don't be fooled.",
                    "label": 0
                },
                {
                    "sent": "This is only a theoretical research.",
                    "label": 1
                },
                {
                    "sent": "I mean we haven't run it, we haven't tried it.",
                    "label": 0
                },
                {
                    "sent": "In theory it works.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can prove that it works, but we need some real applications too.",
                    "label": 0
                },
                {
                    "sent": "And all my students now are theoretical students.",
                    "label": 0
                },
                {
                    "sent": "They don't like to run any applications, so I pass it on to you.",
                    "label": 0
                },
                {
                    "sent": "The real remaining challenges to find ways to make this paradigms really work in practice, and I hope that I will hear back from you one day with.",
                    "label": 1
                },
                {
                    "sent": "Telling me if and how these ideas can really be implemented.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So the public, the police ticklishness talks about right?",
                    "label": 0
                },
                {
                    "sent": "It's a police exception is some kind of measure of how much weight you have around the decision boundaries, and this is similar to the one of the parameters that we need here, right?",
                    "label": 0
                },
                {
                    "sent": "So you you in both cases in all of those applications you want to assume that the weight around the boundary areas between the two classes is not very high.",
                    "label": 0
                },
                {
                    "sent": "If there's lots of points that are living close to points of opposing labels, it's very difficult to do the prediction right, so it's the same parameter, right?",
                    "label": 0
                },
                {
                    "sent": "Prediction of having aliens is not subsumed by these.",
                    "label": 0
                },
                {
                    "sent": "OK, so that yeah, intuitively you can think it is subsumed we can prove we can.",
                    "label": 0
                },
                {
                    "sent": "Create artificial situations in which.",
                    "label": 0
                },
                {
                    "sent": "Those two things are separated, right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it's a very good question.",
                    "label": 0
                },
                {
                    "sent": "It's a very natural question to ask, and this is the place where it's fun to do.",
                    "label": 0
                },
                {
                    "sent": "The mathematics to come up with contrived examples in which you can show that they are independent.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's a very good question, but there are independent of parameters, right?",
                    "label": 0
                },
                {
                    "sent": "Question I didn't really understand the lower bound in the second application domain adaptation, right?",
                    "label": 0
                },
                {
                    "sent": "We wanted to have a lower bound on this.",
                    "label": 0
                },
                {
                    "sent": "There are square root of the of the size.",
                    "label": 0
                },
                {
                    "sent": "Right size was set right.",
                    "label": 0
                },
                {
                    "sent": "And there's some conditions?",
                    "label": 0
                },
                {
                    "sent": "Or is it OK so the lower bound?",
                    "label": 0
                },
                {
                    "sent": "Any algorithm, but it's for the lower bound in the sense that if the only two assumptions that you make is the covariate shift, the labeling is the same and that every point, say its probability of seeing this point in the sample, is at least half of its probability of seeing this point in the target.",
                    "label": 0
                },
                {
                    "sent": "So if these are only the only assumptions you make, although they sound pretty strong, the labeling function is the same and every point is at least half likely to be seen in their source.",
                    "label": 0
                },
                {
                    "sent": "Another target, if these are the only two assumptions that you make.",
                    "label": 0
                },
                {
                    "sent": "Then any algorithm that doesn't have unlabeled data will need square root of the size of X to get, and that's a very strong lower bound, right?",
                    "label": 0
                },
                {
                    "sent": "But understanding right, but it's a lower bound, and again, I mean for me it's fun too.",
                    "label": 0
                },
                {
                    "sent": "When I get a chance to do some clean combinatorics so clean probability and that's the lower bands are one of those things I don't know how useful they are in practice.",
                    "label": 0
                },
                {
                    "sent": "How, how relevant outta practice man?",
                    "label": 0
                },
                {
                    "sent": "Yes, I can send you says the for the case will be teachers there been research done before where you could shoot machine random noise on the teachers.",
                    "label": 0
                },
                {
                    "sent": "Then use require that the medical medical students get the results wrong half the time, no, no.",
                    "label": 0
                },
                {
                    "sent": "Well, they require is that on on points, so I did not get into the details.",
                    "label": 0
                },
                {
                    "sent": "The crucial issue isn't defining the similarity neighborhood of a point.",
                    "label": 0
                },
                {
                    "sent": "So we define the notion of the similarity neighborhood point.",
                    "label": 0
                },
                {
                    "sent": "We assume that the learner has some intuitive notion of which points are similar.",
                    "label": 0
                },
                {
                    "sent": "Right, and you want to in situations in which the similarity neighborhood of the point is homogeneous.",
                    "label": 0
                },
                {
                    "sent": "Every similar point has the same label.",
                    "label": 0
                },
                {
                    "sent": "Then you want the medical students to predict the label rather precisely.",
                    "label": 0
                },
                {
                    "sent": "If ever you can think of it as if the novice teacher sees the attributes on a very rough scale.",
                    "label": 0
                },
                {
                    "sent": "So when you so show it a point it doesn't measure, say the blood pressure in it measures it in in a quantized way.",
                    "label": 0
                },
                {
                    "sent": "So it can confuse the point with this close neighbors.",
                    "label": 0
                },
                {
                    "sent": "So if all the neighborhood is the same labeling and you wanted to predict precisely if their similarity neighborhood is at Regina's you want to have some probability for having one label in some probability for having the other label, not half and half.",
                    "label": 0
                },
                {
                    "sent": "I mean, just say bounded away from being deterministic, you want to say that on boundary areas they hesitate.",
                    "label": 0
                },
                {
                    "sent": "You don't tell them, give me half, of course, that will be too much.",
                    "label": 0
                },
                {
                    "sent": "But you guys just say, don't be.",
                    "label": 0
                },
                {
                    "sent": "I don't want learners that are completely confident around the boundary.",
                    "label": 0
                },
                {
                    "sent": "I wanted to be a little bit unconfident about the wonder if that's what I need for my algorithm.",
                    "label": 0
                },
                {
                    "sent": "Service certainly in the medical application.",
                    "label": 0
                },
                {
                    "sent": "It's often very important that false negatives tend to be a lot worse than those positives, so we might accept more false positives just as long as we avoid voice impulse negatives.",
                    "label": 0
                },
                {
                    "sent": "Would that be easy to bring into the framework?",
                    "label": 0
                },
                {
                    "sent": "So that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "We haven't considered it, but that's a very good suggestion for the next step on this right?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think this is the kind of input that I was hoping to get here, like people telling me, I mean.",
                    "label": 0
                },
                {
                    "sent": "In practice, there is a consideration here that you have.",
                    "label": 0
                },
                {
                    "sent": "Overlooked and definitely we haven't looked at this and it's an interesting point.",
                    "label": 0
                },
                {
                    "sent": "Another question about the semi supervised so.",
                    "label": 0
                },
                {
                    "sent": "Is kind of the other paradigm of 70 supervised learning is where you try and take your labeled and unlabeled some together and do the learning.",
                    "label": 0
                },
                {
                    "sent": "And in one stage somehow.",
                    "label": 0
                },
                {
                    "sent": "So maybe you are using something about manifold.",
                    "label": 0
                },
                {
                    "sent": "Learn some kind of graph structure or something, right?",
                    "label": 0
                },
                {
                    "sent": "Do you think that the the clustering assumptions and the assumptions that you are making in your theory there would apply to that type of?",
                    "label": 0
                },
                {
                    "sent": "That's OK.",
                    "label": 0
                },
                {
                    "sent": "So let me just repeat the question because it's a very good question.",
                    "label": 0
                },
                {
                    "sent": "So when semi supervised learning is really being done in practice and the problem is that we don't have theoretical support, we cannot give you guarantee that will really work.",
                    "label": 0
                },
                {
                    "sent": "And sometimes people notice that it fails.",
                    "label": 0
                },
                {
                    "sent": "Use the unlabeled data and things can deteriorate rather than improve.",
                    "label": 0
                },
                {
                    "sent": "And the question is.",
                    "label": 0
                },
                {
                    "sent": "If I may, I have now a formalization of an ocean of cluster assumption with this probabilistic sleeplessness.",
                    "label": 0
                },
                {
                    "sent": "Under these assumptions one so now we did make a step forward.",
                    "label": 0
                },
                {
                    "sent": "We know how to formulate a notion of a cluster assumption, and the data is clustered in a way which makes sense, and it's not completely artificial.",
                    "label": 0
                },
                {
                    "sent": "Can we show that the usual methods succeed under this assumption?",
                    "label": 0
                },
                {
                    "sent": "That's the main problem that we're working on right now, so.",
                    "label": 0
                },
                {
                    "sent": "Very good question.",
                    "label": 0
                },
                {
                    "sent": "And then another one at this rate, which was that you were using the.",
                    "label": 0
                },
                {
                    "sent": "This is what you call the ugly classifier to then learn very simple linear classifier on the full and they will say.",
                    "label": 0
                },
                {
                    "sent": "But it seems like maybe you know an ensemble of simple classifiers would also be cheap to compute.",
                    "label": 0
                },
                {
                    "sent": "Run sign as well and there may be that your compact complex classifier could.",
                    "label": 0
                },
                {
                    "sent": "She could easily give you a distribution over.",
                    "label": 0
                },
                {
                    "sent": "Over simple, you know, a combination of simple linear classifiers and that might give you a boost in performance.",
                    "label": 0
                },
                {
                    "sent": "There you know, OK, your data wasn't really linearly separable, but maybe a combination of few, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not, it's not.",
                    "label": 0
                },
                {
                    "sent": "This result is restricted to linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "It's it's very general to any situation in which the class of desirable classifiers is not the class that has.",
                    "label": 0
                },
                {
                    "sent": "Non zero approximation error.",
                    "label": 0
                },
                {
                    "sent": "So we have some notion of desirable classifiers and you don't have a perfectly good classifier in this class.",
                    "label": 0
                },
                {
                    "sent": "Under that scenario, we can show that you get the benefit if not necessarily half spaces, and this scenario, in which the desirable classifiers are not likely to give you the error.",
                    "label": 0
                },
                {
                    "sent": "I think it's pretty common that nice classifiers do have some inevitable error just because being nice is not reflecting the real world very well under such a scenario, we can show the benefits of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Is there something that for the medical students example, but their knowledge is somehow sort of linear in that they know roughly like they know?",
                    "label": 0
                },
                {
                    "sent": "This is roughly what achievement looks like and then the next week later, abit tighter rather than.",
                    "label": 0
                },
                {
                    "sent": "They done, you know exactly what one time machine looks like.",
                    "label": 0
                },
                {
                    "sent": "And yeah, right, yeah, so I think I think a good way of modeling this assumption is saying that the week teacher, when it looks at the attributes it quant quantized them at to Inna course way.",
                    "label": 0
                },
                {
                    "sent": "And then the better the teacher, the finer the can see the input.",
                    "label": 0
                },
                {
                    "sent": "Yeah, missing is that true for students?",
                    "label": 0
                },
                {
                    "sent": "So that's a good question.",
                    "label": 0
                },
                {
                    "sent": "That's that's a good question, no.",
                    "label": 0
                },
                {
                    "sent": "And then I think it's a very because I also think that there is not going to be a unique model for week teacher that will apply overall applications.",
                    "label": 0
                },
                {
                    "sent": "We can think of.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's very likely that we were trying to look at a mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "Just saying, you know, giving them images and asking you know where is Val?",
                    "label": 0
                },
                {
                    "sent": "Do you know that this kind of and sometimes it's easier to find him in front?",
                    "label": 0
                },
                {
                    "sent": "Sometimes how to find him?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The kind of mistakes that you get consistently are different than what you get for medical.",
                    "label": 0
                },
                {
                    "sent": "Students are trying to label Benjam, as I don't think that there's going to be a unique way of labeling of modeling week teachers.",
                    "label": 0
                },
                {
                    "sent": "I think this research will have to have a variety of options and you will have to based on your prior knowledge, decide which model is most suitable for your application.",
                    "label": 0
                },
                {
                    "sent": "A new product.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have circled it already, which would be to assess to what extent the week teachers satisfy this hesitant condition.",
                    "label": 0
                },
                {
                    "sent": "Oral to type with addictive risk that everybody is very, very sure value.",
                    "label": 0
                },
                {
                    "sent": "But being very Commission of the wrong thing.",
                    "label": 0
                },
                {
                    "sent": "Which would give no no.",
                    "label": 0
                },
                {
                    "sent": "Yeah no, that's not.",
                    "label": 0
                },
                {
                    "sent": "It would kill me only if they're very convinced on the wrong thing near a boundary.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I mean, but yeah, we did try.",
                    "label": 0
                },
                {
                    "sent": "I mean it's very difficult to get this results for the brain imaging we don't have real results for the brain imaging.",
                    "label": 0
                },
                {
                    "sent": "For kind of things like Mechanical Turk, on the very simple task, we know that it behaves slightly different than our model of like teachers.",
                    "label": 0
                },
                {
                    "sent": "And as I answered before, I think that there should be different models of like teachers, but I think that what's interesting here is just to bring forward this question.",
                    "label": 0
                },
                {
                    "sent": "I mean, because it is something that may become practical.",
                    "label": 0
                }
            ]
        }
    }
}