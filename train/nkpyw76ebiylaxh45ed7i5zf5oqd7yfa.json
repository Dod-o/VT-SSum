{
    "id": "nkpyw76ebiylaxh45ed7i5zf5oqd7yfa",
    "title": "Machine learning for cognitive science 2: Bayesian methods and statistical learning theory",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2010_scholkopf_mlfcs2/",
    "segmentation": [
        [
            "So you came here for the expecting another soothing talk of Neil Lawrence about Bayesianism.",
            "And now you're in for a surprise.",
            "We're starting with statistical learning theory this morning.",
            "Maybe because your fresh still and.",
            "Actually, yeah, I would start rather informally.",
            "So the 1st 20 minutes maybe it will be quite informal and then we have some mathematics.",
            "But actually, before starting, maybe just some personal remarks about yesterday's discussion.",
            "So I got the impression from the discussion that cognitive science, the way it's viewed right now, is that it's about studying how human or animal cognition works.",
            "Is that better?",
            "Hello OK.",
            "So I got the impression it's about studying how cognition in animals works, and I should say this is also what brought me into this field, so I was interested in theoretical neuroscience at some point, but.",
            "I greatly have come to realize that for me it's actually much more interesting to think about cookies.",
            "How could cognition could work more generally in personal animal systems in possible artificial systems, and maybe also?",
            "What's the theory that should be underlying such a field of cognitive science?",
            "So in a sense, I think statistical learning theory is one such attempts to construct the theory of some aspects of cognitive science.",
            "And for me, that's even more fascinating than studying cognition in animals or humans, even though I still think this is an amazing inspiration to look at such problems.",
            "For instance, as discussed yesterday by Chris so."
        ],
        [
            "We have course we're far from having a theory of cognition, but let's start with whatever we have and some of it is statistical learning theory, and I think even though most of you probably won't work on statistical learning theory, and actually I'm also not working on this currently myself, I still think it's something that one needs to know if one works in this field.",
            "So I'm trying to today.",
            "I'll try to give you an intuition how one can derive a statistical learning theory bound.",
            "So just to make sure that if you see them in the future, you're not.",
            "Intimidated 'cause the basic ideas are actually not that difficult.",
            "But let's start very basic.",
            "So I'm from taking it from the empirical inference Department of the Max Planck Institute for Biological Cybernetics and by empirical inference I mean the process of drawing conclusions from empirical data.",
            "So these data could be observations or measurements, and as scientists were doing empirical inference all the time, we're doing scientific inference.",
            "So we might be measuring two observables X&Y, which are related in this fashion, and we might then be willing to infer.",
            "That there's a linear law explaining these data points.",
            "But we might equally well be willing to, or actually we might.",
            "So it is likely it's already pointed out if even if we scattered points of ink randomly on a piece of paper.",
            "So for instance, by shaking quill pin, we can still find some mathematical expression that explains these data points.",
            "It might be a more complicated expression.",
            "At least it looks more complicated to us, but it's also a law.",
            "And question is why do we believe in the first long in this linear law?",
            "And why do we not believe in the second law?",
            "So what's the difference between these laws?",
            "So that's something that has been that lately has thought about another.",
            "Mathematicians have also thought about it, home and vile and cheating more recently.",
            "And it seems to be unclear what makes an equation simple and what makes it difficult.",
            "The physicist Rutherford took a pragmatic view.",
            "He said that if there's a law, it should be evident from the data roughly.",
            "So he said if your experiment needs statistics, you ought to have done a better experiment.",
            "So I will try to convince you that there are interesting inference problems that are nontrivial, so problems were better.",
            "Experiments are not enough, and I think there are lots of such problems in biology, for instance in perception.",
            "So let me show you some images of handwritten digits, so this looks like another easy problem of empirical inference where it's obvious to you which is which digit you have no problem recognizing them, but it's too.",
            "It's simple transformation of the image is just applying a fixed permutation of the pixels.",
            "So we just reorder the pixels and mathematically in computers we represent such images as vectors.",
            "So this permutation is reordering.",
            "It just means we are re labeling the axis.",
            "So for computer and from.",
            "Many recognition algorithms.",
            "These are actually the same kind of data.",
            "So for the computer this problem is not harder than this one.",
            "For you it's a lot harder.",
            "It seems to be a paradox, and the point is of course, that the original images only appear very easy to us because we have been trained on such images all our lives.",
            "So actually the two problems in some sense.",
            "Have almost the same difficulty, so we've been trained on this all our life.",
            "Our brain has been extracting statistical irregularities of this kind of data.",
            "And in the words of Horace Barlow, famous neuro physiologists, the brain is nothing but a statistical decision organ.",
            "So let's go to features.",
            "That means if we want to do brain theory, we have to do statistical learning theory and other things like that.",
            "Maybe most of them don't exist yet, but it's just this is learning theory.",
            "At least we know some things.",
            "So far it looked like.",
            "There are some sort of trivial inference problems in science, like finding this straight line, which would be obvious from the data, and there are some difficult inference problems in biology, but actually it's not the division.",
            "There's also many interesting difficult inference problems in science.",
            "And I'll show you one example, which is a problem studied in the group of Corner Edge who is applying machine learning methods in computational biology.",
            "So I don't want to go into all details on this problem, so the problem is one of classifying human DNA sequence locations into three different classes.",
            "From a pretty large data set of subsequences, local sub sequences of DNA.",
            "The data set has 50 million such sequences of length 141 and you can see here a curve learning curve, so this is the number of observations here, and this is the some measure of the accuracy, precision, recall, curve the area under the precision recall curve.",
            "But don't worry bout this for detail.",
            "For the moment.",
            "The point is that if we only trade on let's say 1000 or a few thousand training examples, that performance is close to 0.",
            "Zero would be chance level, so we're basically performing at chance level and if we use a lot more data, for instance, if we use 10 million data points, we're doing 70%, or we're doing extremely well actually in this problem.",
            "So it's a kind of regularity.",
            "It looks random if you only see a few thousand data points, but if you see a few million data points, you can do very well on this problem.",
            "So it's something that we as humans wouldn't have seen just by looking at the data.",
            "So this is an example of an interesting hard inference problem, so it has several interesting properties.",
            "One or sort of typical properties.",
            "One it's it's fairly high dimensional, so we can only find the structure of the regularity.",
            "If we consider many factors simultaneously, it's.",
            "Fairly complex, so it's it's not well or complex means different things for different problems could be nonlinear, nonstationary?",
            "It's a problem where we have relatively limited prior knowledge, so we don't understand the biological slicing mechanism in detail.",
            "We don't have a mechanistic model for it.",
            "We we have the data and maybe some idea of constraining the mechanisms, but only relatively little.",
            "And as a consequence of these three points, and there's typically a need for large datasets in such programs.",
            "This of course requires computers and it requires automatic inference methods and.",
            "And of course, that's also why Incense Rutherford was right at the time with what he said, because he didn't have these methods, he didn't even have the computer.",
            "So maybe in those days it didn't make sense to look for such complex dependencies.",
            "That days it made sense to do better experiments.",
            "And of course it still makes sense to do better experiments, but now we can do more than just better experiments.",
            "So what I want to point out here is maybe also related to yesterday's discussion.",
            "That in some cases we can already solve inference problems that humans can't solve.",
            "I think this is a kind of a quantum leave.",
            "Of course now you could say and the normal reflects whenever computer solve something you could say is actually fairly stupid.",
            "What the computer is doing, and maybe it is fairly stupid.",
            "And but nevertheless, I think it's a kind of problem where we would have been impressed 10 or 20 years ago.",
            "If someone has had told us that there are such problems.",
            "That looked like there's a complicated dependency that we don't understand and the computer can pick it up in the sense of doing the correct predictions, and we as humans can't.",
            "So I think we're already in the middle of this revolution.",
            "OK, so let's go up.",
            "It will get closer to learning theory.",
            "So what's the basic problem of learning and generalization?",
            "So let me show you this example, which I've taken roughly from Olivier Busquet, who used to be in our lab and who went to Google.",
            "Which is about observing this sequence of numbers.",
            "So suppose you see these numbers 1247 and I ask you what's the next number so you can.",
            "You can guess now 11.",
            "Any other guesses 5?",
            "Did someone say 12?",
            "How about how about 13?",
            "61 OK, so I'll give you solutions from many of them.",
            "So OK, so this is the one that most of you came up with is called the lazy caterer sequence.",
            "Because it's the number of pieces into which you can cut a cake with end cuts.",
            "So you just have to make sure that each additional cut intersects or previous cuts.",
            "So that means adding an additional pieces at this stage.",
            "Of course the pieces won't have the same size, but that doesn't matter and this is a nice sequence.",
            "There's another nice sequence which I don't think has a name.",
            "It gives 12 if you like 13, it's the triple Nachi sequence.",
            "Each number is the sum of the previous three.",
            "Here's a sequence which is nice.",
            "If your mathematician you look disease, of course is the interleaved decimal expansion of \u03c0 and E. And if you go to this website, the online Encyclopedia of integer sequences, you will find 600 hits.",
            "If you if you enter these phone numbers.",
            "So there are 600 mathematically meaningful ways of continuing this sequence.",
            "And.",
            "That seems to be a difficult problem.",
            "So if we want to know which continuation is correct.",
            "So which one generalizes then you might say.",
            "That doesn't make sense.",
            "There's no way to tell, and that would also in some sense be the answer of philosophy.",
            "It's essentially the problem of induction.",
            "So it's strange that people still construct intelligence tests that use these kind of things, so it seems like the intelligence tests are just testing whether you think the same way as the designer of the test.",
            "But let's leave that aside.",
            "So in philosophy this is the induction program.",
            "So let's ask a slightly which is considered to be unsolvable, or at least by some philosophers.",
            "Let's ask a slightly easier question, and that's the one who started learning theory.",
            "Question is not which one is correct, but how to come up with the logically.",
            "With continuations that are probably correct.",
            "And this also has it a name in philosophy is called the demarcation problem.",
            "So both these problems were discussed at length, for instance by pop up who called the first problem Humes problem and the second one cuts problem.",
            "So.",
            "We will focus on clients problem.",
            "And we will do this in the case of two class classification.",
            "So let's introduce the framework little bit.",
            "So two class classification means we want to learn functions.",
            "So this is this, this very simple case of learning that everybody told you yesterday is boring and irrelevant, but the good thing is we understand the theory for this case.",
            "So let's talk about this and once you've understood it, you can say it's boring, irrelevant.",
            "So.",
            "We went to the functions taking two values, let's call him plus or minus one.",
            "The input is some domain X and we won't learn these functions based on M observations.",
            "Each observation is a pair of input and output.",
            "And we're making the assumption that all these observations are generated independently, so this word is missing here.",
            "By sampling from the same underlying probability distribution, so it's a joint distribution that every time you query this distribution, every time you sample from it, you get a pair X&Y.",
            "So this contains a special case.",
            "For instance the situation where the X is generated randomly and the Y is just a function of X or the Y could be a function of X plus noise, or it could be more complicated.",
            "General dependency between X&Y.",
            "Now our goal is we want to minimize the expected error which is also called.",
            "The risk of this function and the risk is defined as follows.",
            "So for each pair X&Y.",
            "Sorry for each X we evaluate the function.",
            "We get some F of X is F of X is supposed to be close to the correct?",
            "Why we're measuring the closeness by evaluating this little expression here.",
            "So we take the difference between the two and take the modulus, because the difference could be either.",
            "So if you remember each of them is plus or minus one, so the difference could be plus one's very plus 2 -- 2.",
            "Or it could be 0 if they are the same.",
            "So we will take the modulus and the difference will be two or zero and then we divide by two.",
            "So the difference would be one or zero.",
            "So that's the so-called 01 loss function.",
            "Which just checks whether F of X is equal to Y or not, and then we average this over this probability distribution.",
            "So we take this into, well, these expectation with respect to the distribution and problem is we can't even compute this quantity because the probability distribution is assumed to be unknown.",
            "So we don't know this law generating the data.",
            "We only see the data so.",
            "We can't.",
            "We can't compute this quantity, and therefore we also can't minimize it.",
            "We cannot find the function which has the minimal error 'cause we can't compute the error.",
            "So we want to somehow.",
            "Maybe approximately find the minimizer of this using, not the distribution, but using what we know about the distribution, which is these data appears.",
            "And for this we need what's called an induction principle in learning theory, and one such induction principle for getting from the training training data to.",
            "The minimizer here is called empirical risk minimization.",
            "I don't know.",
            "I keep bending down here.",
            "I'm tempted to sit down, but I worry that you will all fall asleep if I sit down so.",
            "So let's let's try this.",
            "So here's the induction principle.",
            "We replaced this quantity here, which was the average over.",
            "If you want infinitely many points suffered from the unknown distribution.",
            "By the average over the 20 points, which is all the points we have, which is all we've seen.",
            "So we just compute the same error we average over the training points, and we call this a empirical risk.",
            "Or the training error.",
            "So.",
            "That's a good idea.",
            "OK, so so we're going to minimize this thing here.",
            "Anne.",
            "Over some class of functions, so we want to find the best function from sunset.",
            "In terms of explaining this and then the question is, is this consistent and consistent?",
            "Statistics, roughly speaking, means that if we increase the number of observations here that go to Infinity, of course every time we change the number of observations, we will find a different minimizer for this thing, we will find a different best function in the question is, will this function eventually do the best possible job well when we do as well?",
            "If we had directly minimized this quantity.",
            "And and the surprising answer.",
            "Is there yes and no, so it's yes under some conditions and without any additional conditions it would be no.",
            "So the answer is yes, and even independently of what's the underlying probability distribution generating the data, provided that this is 1 possible condition, provided that the VC dimension of the function class is finite.",
            "So with the VC dimension and I'll tell you more about all these issues.",
            "So now is the short intuitive version, and afterwards I'll show you some of the derivations.",
            "So we see dimension is the maximum number of points which can be classified in all possible ways using functions from your class.",
            "So this is a little bit difficult to pass out.",
            "For example, let's consider the set of linear classifiers in the plane.",
            "In this case the VC dimension is 3 and the reason for this is the following.",
            "If we take three points here in general position.",
            "We can separate them or classify them into classes plus 1 -- 1 in all possible ways using functions from our class or using linear classifiers.",
            "So for three points, there are two to the power of three such possible classifications, so eight.",
            "And here we have all eight of them.",
            "Now in the definition says the maximum number of points which can be classified all possible ways.",
            "So now we know it works for three points.",
            "Question is does it work for four points?",
            "I'm not going to prove that, but here's a little example to illustrate that if you have four points, you will always be there.",
            "Some at some point.",
            "In a situation where you want to separate two points from the other tool and you can't delete using linear functions.",
            "So it actually takes that the maximum is 3 and we see dimension is 3.",
            "And in a sense, just to connect again to problem that I mentioned before.",
            "Say this situation.",
            "So if we have a class of functions, we see dimension 3.",
            "And we only observe three points.",
            "We are in a non falsifiable situation because no matter how these three points, let's say this is our training set, no matter how they are labeled.",
            "So no matter what plus minus ones you assign, I would always be able to find a function which explains that labeling.",
            "So it's it's sort of a trivial statement.",
            "It doesn't mean anything.",
            "It doesn't mean I have learned anything from the data because I could have told him before looking at the data that I'm going to do a perfect job.",
            "So this is a case where the class of functions is non falsifiable and actually turns out.",
            "In this case we also can't generalize.",
            "On the other hand, if we have more data points, then and hopefully a lot more in the VC dimension we will be in a falsifiable situation, which is actually the good case because in that case it's.",
            "If you take such a function from such a class, and you can explain the data well, then in some sense this can't be a quince idents, and then you can get guaranteed generalization.",
            "So that's just the intuitive case and maybe just to make this more complete.",
            "It turns out in D dimensions, this generalizes.",
            "the PC dimension will be D + 1.",
            "Which, by the way, is also the number of free parameters of this function class.",
            "But there are also interesting cases.",
            "There is not the number of free parameters, so it turns out if you enforce some margin of separation between the two classes.",
            "Then the visitor mention can be a lot smaller.",
            "So especially if this margin is large in the VC dimension, can certainly be a lot smaller.",
            "And that's the basis for support vector machines, not what does learning theory.",
            "What does maybe I have to put this this one some words?",
            "So this is the kind of result that.",
            "How we get such results?",
            "So results is the test error or the risk for the expected error.",
            "Is bounded from above with the college high probabilities Hotel stick of Delta essence, very small number.",
            "For all functions of our classes, functions is bounded from above by the training error plus some quantity.",
            "Here that involves the VC dimension and the sample size.",
            "So the number of observations that we have seen.",
            "And you can see here if that we said we said image news small compared to the number of observations that we have seen in this quantity here will go to zero and this will be small.",
            "So in that case.",
            "We can guarantee that the test error is not much larger than the training error is, only could only be larger by this amount with high probability.",
            "So that sounds almost too good to be true.",
            "So suppose suppose we learn something that doesn't make sense at all, what with this bound tell us.",
            "So, for instance, suppose we try to learn the mapping from.",
            "From the name of a person in a telephone directory to their telephone number.",
            "So probably you don't believe that this makes sense even though we have a lot of training examples.",
            "So let's take a slightly different example.",
            "Suppose we went to learn the mapping from the birth date of a person to their character traits.",
            "It's actually something that a lot of people think makes sense in astrology, but it's a similar kind of program.",
            "So in that case, or maybe going back to the telephone directory again.",
            "In that case you will find that either you have a learning machine which is very large, which can actually learn by heart.",
            "The whole telephone directory.",
            "The mapping from the name to the person in that case, training error would be small, but you will.",
            "You will actually notice that this learning machine is so large that it essentially can memorize all the mappings from name 2 telephone number.",
            "In that case the VC dimension will be very large, so this quantity will be large.",
            "On the other hand, if you take a small learning machine, small VC dimension is quantity will be smaller, but in that case you won't be able to memorize all the themes, so you will have.",
            "A large training error, so in both cases you will never guarantee that the test there are small.",
            "So this doesn't guarantee that the test error for any problem will be small.",
            "It doesn't solve the induction problem, but it guarantees that if you.",
            "Follow certain methodological prescriptions so if you.",
            "Use a class of functions which has a visa dimension small compared to the sample size.",
            "Then you can guarantee that the test around the training error will not be very far apart, and if you are lucky in the training error is more than the test or is also small.",
            "So it's.",
            "In a sense, it's a formalization of this popper philosophy of falsification.",
            "Anne.",
            "It doesn't solve the induction problem 'cause we will only detect regularity's, for which we can a priori good choose good function classes.",
            "That's something that we have to do before looking at the data.",
            "And of course, nobody tells us how to do this, so machine learning always needs data plus prior knowledge in whichever way you will specify this prior knowledge.",
            "So let's move to the other slides.",
            "So if there are questions at this point, of course I should say you can interrupt me at any anytime, so I was trying to make this fairly intuitive so far, so I hope you're all still with me.",
            "And now let's do it a little bit more technical.",
            "Uh huh.",
            "Oh wait, I think I have to move this guy.",
            "OK."
        ],
        [
            "So you have.",
            "This one.",
            "Right, OK, you have already seen this night, so just to remind you of the notation.",
            "So here's all training set.",
            "Is the underlying probability distribution.",
            "Here's the test error or the risk.",
            "Here's the empirical risk.",
            "So that's the thing we're going to minimize, and that's what public intervening is studied.",
            "So they studied the consistency of minimization of the empirical."
        ],
        [
            "Risk."
        ],
        [
            "Now that's one nice thing about this.",
            "About this quantity, which is that this is an estimator of this one.",
            "And actually, this law of large numbers in statistics, which tells us that under some fairly general conditions this quantity here will converge towards this one in probability.",
            "So by this I mean that the probability of a deviation between these two guys larger than epsilon will go to zero as the number of observations goes to Infinity and it will do so no matter how small the assignment is that you choose.",
            "So let's call it."
        ],
        [
            "Surgeons in probability.",
            "Large numbers tells us that the mean of such quantity will converge to its expectation.",
            "In probability.",
            "And.",
            "That's nice and it will actually.",
            "I'll tell you afterwards how fast it will go.",
            "It actually exponentially fast.",
            "And you might be led to believe that this implies this is already solving our problems.",
            "So the question is, does this imply consistency does imply that you will get the optimal result in the limit of infinite sample size infinitely many observations?",
            "Turns out this is not the case.",
            "We need what's called a uniform version of the law of large numbers, and I'll try it."
        ],
        [
            "Motivate this in the following picture so.",
            "The classic law of large numbers tells us that."
        ],
        [
            "This quantity that I showed you before.",
            "For any fixed function here F this quantity will converge towards this."
        ],
        [
            "City.",
            "So in this diagram this means here we have a function class.",
            "It could be a high dimensional, but I'm just drawing one dimension here.",
            "We have the risk.",
            "So for each function I get a certain value of the empirical risk of the training error.",
            "Certain value of the true risk.",
            "And the law of large numbers tells us if we fix the function, so we look at one slice here, then this training error will converge towards the true error in probability.",
            "So this as we increase the number of observations, this curve here will wiggle around this one.",
            "Of course is fixed that that's the expected error, but this one will wiggle around and will it will wiggle around in such a way that for each fixed function F this point here converges towards this one in probability.",
            "Now if we ask the question of consistency, consistency means empirical risk minimization, so it it tells us we should always pick the minimize of this curve.",
            "So we take this function here.",
            "And roughly speaking, consistency that means does the minimum of this curve converge towards the minimum of the other curve?",
            "That's what we want.",
            "And surprisingly, it turns out.",
            "At this point, Wise Convergence doesn't imply consistency.",
            "So if you are mathematicians, you probably will immediately intuitively believe me.",
            "But the point is.",
            "Even if at each point we have this convergence, these convergences are all in probability and they can all happen at different speeds and things can go wrong.",
            "In the minimum, minimum might not converge.",
            "On the other hand, if you have something which is called uniform convergence in mathematics, so uniform convergence means somehow that this curve converges to the other one with the same speed everywhere, roughly speaking, then also the minima will converge, so this pointwise convergence does not imply convergence of the minima.",
            "And actually one can.",
            "So it's a uniform convergence or and I hope you would.",
            "You just believe me this.",
            "A little bit more about you.",
            "Later uniform convergence will be sufficient, and in some sense even necessary and sufficient for consistency.",
            "Now, of course, uniform convergence is something that depends on the whole function class.",
            "So it has to work for all functions, so the larger the function classes, the harder it is to get uniform convergence.",
            "So let's think about."
        ],
        [
            "What happens if we take?",
            "We take all functions from our input domain to plus minus one.",
            "So can we after all.",
            "We don't know apriori which functions the correct one, so why don't we start with all possible functions and then try to identify the correct one?",
            "So it turns out that's impossible, and that's easy to see.",
            "So suppose we have this training set here, and suppose we have some test points.",
            "Expire XI bar, where I'm assuming for simplicity that test points and the training points don't overlap, so all test points are really new points that we haven't seen before.",
            "Ben, whatever function F you give me.",
            "I can construct a function F star.",
            "Which will agree with the function F on all training points.",
            "And we'll disagree on all test points, so it's trivial to construct this function because we have we allow all functions, so whatever you give me, I will just define it.",
            "For example to find it such that it says the opposite on all test points.",
            "The opposite of your function.",
            "I suppose this is so you have a learning algorithm.",
            "You look at the training data you say.",
            "I reckon this is the correct function here.",
            "That's that's a good function.",
            "Then I will go ahead and will construct this other function F star and I'll say, well, actually in the training data it says exactly the same.",
            "Better on the test data.",
            "My function says the opposite.",
            "So based on the training data, we can't choose which functions is better and also if we look at the convergence then if it's the case that for your function empirical risk and risk are nearby.",
            "Then it's quite possible that for my functions they are quite far apart because my function only agrees on the training points with your function at all other points could be saying something different, so I can construct a function which has a very different behavior in terms of training error and tester of being nearby.",
            "So this is sort of trivial observation.",
            "It has been called the No Free lunch theorem of learning.",
            "So if.",
            "If we have no restriction on the class of functions from which we do the estimation.",
            "Then there's nothing we can do, so we need some prior knowledge before we do the learning and the prior knowledge.",
            "In the case of statistical learning theory has to consist of restricting the class of functions from which we do."
        ],
        [
            "My estimation.",
            "So that's that's one way of thinking.",
            "You talk into account prior knowledge.",
            "We would actually measure the size of these classes of functions in terms of capacity.",
            "Concepts such as the VC dimension.",
            "Of course there are other ways of doing it, for instance the Bayesian way where we place prior distributions on the class of functions."
        ],
        [
            "Just as a side note.",
            "So let's look at it a little bit in detail so.",
            "Let's introduce a shorthand Cy I for this quantity.",
            "So this was our.",
            "01 misclassification error.",
            "So zero if F of X is Y and one whenever these two things disagree.",
            "So you might remember we were interested in whether the sum over these quantities convergence to their expectation.",
            "So whether the sum converges with the integral.",
            "And these quantities are actually in statistics, the OR in probability.",
            "These are called the newly trials in their independent penalty trials.",
            "That independent because we are assuming that X&Y are sampled independently from an underlying distribution.",
            "And we're also assuming F is some fixed function here.",
            "In their Bernoulli trials with Christy, they take values zero or one.",
            "So now we are.",
            "Left with a question whether this quantity, which is now our training error.",
            "Converges towards this one, which is our test error or risk."
        ],
        [
            "Now there's a nice bound from statistics.",
            "I do too.",
            "How much of an American statistician which who I think is still alive?",
            "And so this is not that all this stuff is bounded, probably from the 60s, which tells us how fast this convergence takes place.",
            "So it tells us for such Bernoulli independent trials.",
            "So these quantities taking value 01.",
            "The probability that the training error is different from the test error by at least epsilon is bounded from above by this quantity here.",
            "And the nice thing is that this goes to zero exponentially fast as we increase the number of observations M. So this tells us how fast this convergence goes.",
            "So one example of vanderlee trials is if you flip a coin.",
            "So suppose we we we call heads zero and tails one.",
            "Then, so these quantities.",
            "These variables could measure how often do you get heads and tails.",
            "So let's say you flip the coin 100 times.",
            "You measure how often you get heads and tails.",
            "So so you sum up the tails corresponding to once.",
            "Remember correctly, suppose you get 60 tails or or.",
            "Suppose you want to ask how likely is it that I will get more than 60 or less than 40 fails.",
            "So 60 Tails, then this quantity would be.",
            "0.6 of course we assume it's a.",
            "It's a.",
            "It's an unbiased coin, so this will be 0.5.",
            "So more than 60 or less than 40 will correspond to an epsilon of 0.1 deviation either above 0.6 or below 0.4.",
            "So we set epsilon to 0.1.",
            "So here we have 0.01.",
            "Sample size is 100, so this gives US1, so it's E to the minus 2.",
            "Something like 1 / 8.",
            "So times two we get 1 / 4, so this is the probability that we get more than 60 or less than 40.",
            "Is that the numbers again?",
            "But I think it's 1 / 4, so it's it's bounded from above by 25%.",
            "Which sounds reasonable.",
            "OK, so this is a.",
            "This is a nice bound.",
            "And oh, and I also, I should say here.",
            "So yeah, talk about the probability I should be telling him what's the probability over what does it refer to him?",
            "So want random experiment and it's the random experiment of.",
            "Drawing a sample from the so-called product distribution in IID samples remains M times.",
            "I will draw a pair XY from my unknown distribution.",
            "Then I will compute this quantity and then over this procedure of drawing M times, that's the probability.",
            "So that tells us how often we go wrong.",
            "OK, and one thing we're going to need later on is rather than looking how fast this mean converges to that expectation, we can.",
            "Also, it's clear we can also derive an upper bound.",
            "How far how fast 2 means converge to each other, because each of these quantities will converge to the expectations.",
            "So if they're both close to this, they also have to be close to each other and we have to do some kind of triangle inequality and we lose a little bit over here, so we get an additional factor of two and we get.",
            "Epsilon over 2 instead of epsilon, but it's basically the same."
        ],
        [
            "Thing.",
            "So so if we translate this back into machine learning terminology.",
            "It tells us the probability of obtaining an example, so obtaining a training set well, training error and test error differ by more than epsilon is bounded by this quantity.",
            "Now I already mentioned at some point this refers to one fixed function F. So why is this the case?",
            "So what if we chose the function that directly interested in so we choose the function, giving us the minimal training error.",
            "After all, that's what we want to know is that function going to do well?",
            "Problem is this function has looked at the training, so choosing the function that has a minimum training error, we first have to look at the training data so that somehow this function knows something about all training examples.",
            "And then, unfortunately, even though."
        ],
        [
            "Even though these guys are still in dependently xiy function suddenly in depends on all of them jointly and then these sires are no longer independent.",
            "So then the channel bound doesn't apply and that's the whole problem.",
            "So that's what we have."
        ],
        [
            "To deal with now."
        ],
        [
            "Now here's something that I will not prove, and it's actually not not easy to prove one of the directions is easy.",
            "The other one is highly nontrivial.",
            "And but I've tried to motivate it a little bit before in this picture.",
            "So it turns out that.",
            "It's necessary and sufficient for a certain type of consistency, so consistency.",
            "Remember the question, do we get the best result in the limit of infinitely many data points?",
            "For this is necessary and sufficient to have a type of uniform convergence.",
            "Of training error to test error of empirical restore risk.",
            "Open the whole function class, so in this picture from before over, this whole X axis overall functions.",
            "So if we have this uniform convergence, then we have consistency and vice versa.",
            "Now, of course, if you look at this thing, he would probably say this is a little bit hard to check for a learning machine, and that's why actually we would like to have properties of classes of functions which tell us something about whether uniform convergence will take place or not.",
            "And let's capacity measures."
        ],
        [
            "So.",
            "And this will somehow show up automatically if we tried to prove such a bound.",
            "So let's take a closer look at this quantity.",
            "So remember, this is what we were interested in.",
            "So this is now the deviation between the training error and the test error.",
            "The probability that this deviation is larger than epsilon, but now it's uniformly so this is a supremum if you have never seen a supremum, just think of it as a maximum through.",
            "This is the sort of the deviation that we get for the worst possible function, and by worst I mean a function where training and test errors are as different as possible.",
            "So anyway, it's a function where measuring the training error misleads us a lot about what's the test error, so that's our worst function, so we're interested in what's the probability that for the worst function, we are misled by more than epsilon.",
            "So if we can prove that even for the worst function we are epsilon closed with high probability, then we find because then we don't care about which one is the worst one we've got just going to use the one minimizing the training, and we know we're going to be fine.",
            "So.",
            "So first of all, let's start with the simplest case.",
            "Our function class contains only one function, so in that case we can just cross out this supremum, the maximum.",
            "So we are left with the statement that we had on the previous slide in the turn of Bond.",
            "That's the job for us.",
            "So in that case the turn of bond.",
            "Tells us what's going to happen.",
            "I may be just as an aside, one application of this telephone.",
            "So even if you're, if you're a Bayesian, let's say you were busy, but not a completely Orthodox Bayesian.",
            "So you're Bayesian, who's still evaluates things on a test set as as a real Bayesian.",
            "You shouldn't be doing this, you should just put in all your prior knowledge, training the data.",
            "Then you've done.",
            "You don't have to evaluate anything, but let's say you're a sort of a reasonable Bayesian, likely Lawrence, so you are still testing your things.",
            "So let's say you have 1000 test examples that you want to know.",
            "No, I've missed my my error on this 10,000 examples.",
            "How likely is it that I'm close to the true error that I would be measuring if I had a million examples or 10,000,000?",
            "Then also this phone tells you this.",
            "'cause it tells you so.",
            "Now we have one fixed function.",
            "It's now so we have chosen the function that minimizes the training error or that maximizes the evidence or whatever.",
            "Quite here, and you want to use you have one fixed function.",
            "Now you're looking at the test set.",
            "So this function has nothing to do with test set and then again these losses.",
            "These siii on the test points.",
            "There are independent if your test set is IID.",
            "If your testing samples are drawn IID from some distribution and then this tells you how far is this empirical quantity, or my average over the test set?",
            "From the real test error that I would be measuring if I had infinitely many test examples.",
            "And this tells you that it's good to have more examples, because the difference will go exponentially with code on exponentially.",
            "So that's nice, so let's but now let's go to the slightly more complicated case where we have a function with several sorry function class with several functions and to do it very slowly, we start with two functions.",
            "So the plan will be 2 functions and then end functions.",
            "We can use something called the Union bound and then nontrivial step will be from any functions to infinitely many functions.",
            "So can we still do it for infinitely many functions?",
            "And that's there's some nice ideas of public companies that made this problem solvable.",
            "They're called symmetrization.",
            "And interest in the introduction of capacity concepts and the basic idea is that even if there are infinitely many.",
            "The capacity is small in some sense, then on a finite sample of observations they will effectively behave as if there are only finitely many Oracle.",
            "Are there?",
            "I should even say sub exponentially many, but you'll see a little bit more about that."
        ],
        [
            "So let's look at two functions.",
            "So.",
            "We have a function class containing two functions now.",
            "We will provide rewrite this thing here, so remember this is what we are interested in.",
            "The maximum of these two functions of these deviations.",
            "And so when.",
            "And.",
            "When is this?",
            "This equality here true, but we could say we write it as two events.",
            "One is the.",
            "Event that the risks.",
            "So the test around the training error for function F1.",
            "Differ by more than epsilon.",
            "So I mean, let's say I'm interested in whether one of the two functions has a risk of at least epsilon, or whether the worst function has a risk of at least steps or more than epsilon, and this will.",
            "This will take place if the first function has a risk of more than epsilon or the second one has a risk of more than website, and this is not an exclusive or they could also both be.",
            "Bibette so C One is the event that for the first function.",
            "We draw a training set such that the risks differ by more than epsilon.",
            "C2 is the same for the second function.",
            "So then we can rewrite this quantity as the probability of the Union of these two events.",
            "We can rewrite this again as the sum of these two probabilities minus the probability that both takes place.",
            "Probabilities are negative, so if we drop this term we get an upper bound which looks like this.",
            "So maybe if you haven't so personally I I don't know how much mathematics cognitive scientists typically studying, so maybe I can.",
            "I can quickly ask how many of you are sort of cognitive scientists or psychologists.",
            "OK, so that's I would say 25%.",
            "How many of you are mathematicians or physicists or mathematically minded computer scientists?",
            "So that's that's more so I think probably for you it will be easy for the other ones I'm trying to also give some intuition.",
            "So so, roughly speaking, the short version is we're interested in whether one of the two functions misleads us about the difference between training and test error.",
            "And I'm saying the probability that this will happen is upper bounded by the probability that the first one will be mislead us, plus the probability of the second one misleading us, and it's just a bound because it could be that both are misleading us, so I could be doing some double counting here.",
            "OK, so if we have this.",
            "Now we at this point, these two quantities.",
            "Now they both refer in system function classes of size 1 again, so I can use the channel font for this both these.",
            "I will get the identical term for both of them, so I will get the same result as before, which was this one only with a factor of two here.",
            "So that factor of two I don't.",
            "I don't mind that factor of two, because this thing here goes down exponentially with the number of observations, so this goes down so fast that this factor tool doesn't cost us anything.",
            "Now let's look at in functions and I think you can."
        ],
        [
            "So imagine what's going to happen even if I don't go through the details.",
            "What we will get is we will get an extra factor of N. So we can upper bound this this quantity like before.",
            "This time it's a sum of N quantities, each of them we can handle with the channel font.",
            "So we get an extra factor of N. So, so again, to make this a little bit intuitive.",
            "Suppose you're you trained learning machine with your favorite method.",
            "Now you want to evaluate on our test set.",
            "Suppose you're doing something that none of us would ever do your training.",
            "Several learning machines.",
            "You evaluate all of them in the test set.",
            "Afterwards, you take the one that does based on the test set.",
            "Maybe write a newspaper about it.",
            "So how how bad can it be?",
            "Well, actually if you take any functions and you do that, then again this bound.",
            "Sorry it's not written down here.",
            "But"
        ],
        [
            "Previous bond.",
            "So if you take 2 functions and you choose the one that does better.",
            "Then this bound here tells you how far you can be, how far your estimate of the test error of the function that you are reporting in your NIPS paper can be from the true test error.",
            "So it can be it can be away by a factor of two more, but actually it's not that bad if you have a large test set.",
            "Again, this factor of 2 is really pretty insignificant compared to how fast this goes to 0.",
            "So maybe it's not, especially I'm not saying you should be doing this, but it's I'm saying it's not so easy to cheat like that.",
            "If you really want to cheat, then you shouldn't just be training two functions, or you shouldn't just be training in functions.",
            "If you want cheap, you should have some kind of adaptive procedure.",
            "Have you changed your parameter?",
            "If you look at the test sets, you see how it's getting a little bit better.",
            "How about if I changed a little bit more?",
            "So if you do a procedure like this, and of course, you're effectively if your procedures clever, maybe you're effectively using infinitely many functions from which you are selecting.",
            "Possible still depend, even if it's infinitely many, and this will be the content of the next slides.",
            "Whether you can cheat or not depends on what's the capacity of this infinite set of functions.",
            "So let's go to capacity."
        ],
        [
            "I was seeing some interest lighting up in your eyes when I was talking about this.",
            "Explain something about your successful career.",
            "I'm not going to comment on that.",
            "OK."
        ],
        [
            "So infinite function classes, so that's the that's the real cheating.",
            "So the interesting thing here is an empirical risk.",
            "Only refers to endpoints, so if we are interested in this quantity here, so."
        ],
        [
            "So we're still always talking about this thing here.",
            "So now we went to.",
            "Look at the supremum over an infinite function class.",
            "Now, this quantity here, even if the function is infinite, we only evaluate the functions on our training points.",
            "So if we have finitely many training points, then somehow?",
            "We are factorizing this function class were saying all functions that take the same values on the training points.",
            "We will consider them equivalent if we only interested the training points.",
            "What does it matter?",
            "What other values the functions take, so that's in a sense effectively finite.",
            "But this thing is still giving us trouble, so we're trying to convert this thing into something like that.",
            "And maybe you already can guess."
        ],
        [
            "How this is gonna work?",
            "Because I've already prepared you."
        ],
        [
            "Right before.",
            "So we're going to replace.",
            "This quantity by something where both these terms are training errors.",
            "So I showed you before a channel found which is not about the convergence of a mean to an expectation, but the convergence of two means to each other.",
            "So we're doing something like that.",
            "We're using two training errors.",
            "On a double sample, so we know we take two endpoints.",
            "We are going to compute the training error on the 1st endpoints here on the second important here we are asking the question and then we are.",
            "We can actually say that the quantity that we originally were interested in will be upper bounded by the probability of these two guys deviated from each other by more than epsilon over 2.",
            "And here I have a factor of two outside.",
            "So I think this is so fairly intuitive.",
            "You probably believe me that this is the case."
        ],
        [
            "So now suddenly everything is only on finitely many happening on finitely many points over 2 endpoints, but nevertheless that's a big improvement to perform where we could have potentially infinite function classes.",
            "So let's see how large a function class can be finitely many points.",
            "So let's call this quantity calligraphic end.",
            "It depends on the function class.",
            "It depends on how many points we have is called the shattering coefficient.",
            "So what is the shattering coefficient?",
            "So remember, we're still talking about pattern recognition, so outputs plus minus one is the maximum number of different outputs.",
            "That's a all function class can generate, generate on two endpoints.",
            "So in other words, in the maximum number of ways the function class can separate two endpoints into two classes.",
            "So we have only finitely many points where we evaluate the functions.",
            "The outputs of the functions are plus or minus one.",
            "Then you can.",
            "You can imagine that only finitely many possible outputs can be observed.",
            "And of course, the largest number of such different output vectors.",
            "So so I collect all these outputs in one vector of dimension 2M.",
            "The largest number would be 2 to the power of two M because each of these quantities could be plus or minus one.",
            "Likely we are looking at the losses.",
            "It could be one or zero, but it doesn't matter is the same.",
            "So the largest size of this thing could be 2 to the power of two M. So what does that mean?",
            "So if we just briefly go back?"
        ],
        [
            "So I said before, if we have finitely many.",
            "We get a factor of extra factor of N."
        ],
        [
            "In our bones so so down here we would have fucked over end which is good.",
            "And now if we have it said if we have this shattering coefficient, I told you it could be in principle up to two to the power of two M. So if I put 2 to the power of two AM here, let's not so nice because it was about with the power of two M increases exponentially.",
            "This decreases exponentially.",
            "That's not very conclusive.",
            "We're not.",
            "We're not sure whether this will go to zero, probably will not so."
        ],
        [
            "Or 2 to the power of two M is not good, so we actually in this case if the cardinality of this set of outputs is actually 2 to the power of two M, then the function classes set to shatter two endpoints.",
            "And again, like before, this is the non falsifiable case.",
            "If we can generate all possible outputs.",
            "With functions from our class on our sample size, then we wouldn't assume that we can generalize, so that's the bad case.",
            "In this case, the function class shatters."
        ],
        [
            "Two endpoints.",
            "But let's go back to the original problem.",
            "So what we're going to do, and also we're interested in this quantity.",
            "First, we use the Symmetrization lemma.",
            "So we replaced this thing here by a difference between two training errors.",
            "So this is not probability over drawing 22M samples, but don't worry about that.",
            "So now we have this quantity.",
            "So now we're going to say.",
            "That effectively.",
            "On two endpoints or function class has only a size.",
            "A size which is measured by the shattering coefficient.",
            "So now we rewrite this quantity.",
            "So the supremum of this or this event that at least one of the all that the supremum will be larger than epsilon over 2.",
            "By the Union of the events that I get by looking at whether these individual functions are larger than this in action.",
            "Say I'm slightly cheating here, but don't worry about this now.",
            "It can be done correctly.",
            "It's just a little bit more complicated and I think for the basic idea this is fine.",
            "So now OK, we effectively only have.",
            "In calligraphic functions.",
            "So now we're going to do a union bound over these functions, so we get.",
            "The sum over all these events and now is the sum within calligraphic terms.",
            "And now for all this, we use the usual channels bound."
        ],
        [
            "And actually now we use this slightly unusual turn of bond, which is about the difference between through empirical means.",
            "If we do this, we get.",
            "In effect, while taking altogether, this was our quantity of interest, is upper bounded by this here, where this term here is sort of the usual thing only that we have slightly different constants?",
            "And here we have the shattering coefficient.",
            "So you can see here if the shattering coefficient.",
            "Does not grow exponentially in them.",
            "So if this gross up exponentially.",
            "Then this quantity he will win in the long term.",
            "For large sample size, and then we refine this thing will he will go to 0 so it means.",
            "For very large sample sizes, the training error on the test error will effectively be the same.",
            "So that's nice, and this is an example of a VC of applicable link is inequality.",
            "And maybe just all the interpretations so.",
            "And this P here as I've pointed out before, it refers to drawing training examples.",
            "So drawing an example of training points.",
            "And here this risk is an expectation.",
            "Overdrawing the test examples."
        ],
        [
            "Now it yeah so."
        ],
        [
            "What I can do?",
            "So I've already told you that this effectively this tells us that if we have many many training examples, it's unlikely that the training error will be very different from the test error, so it doesn't come as a surprise that we can rewrite this whole bound as a bound on the test error involving the training error plus something that depends on all this.",
            "To do this, we just.",
            "Set the right hand side equal to sum Delta, which we think of as a very small number, and then we solve for epsilon.",
            "And."
        ],
        [
            "To get the epsilon which corresponds that Delta, then this thing here will be absolutely epsilon.",
            "So the deviation between test error and training error would be upper bounded by this epsilon with probability of at least one minus Delta.",
            "So the probability of going wrong would be Delta at most Delta.",
            "And then this quantity here will depend on this shattering coefficient.",
            "And on the number of observations and of course on this Delta thing here, which we think of as a small number.",
            "So, and since with the bond on the previous page was a supremum over the function class, or think of it as a maximum, this bond holds true independent of the function.",
            "So in particular, holds true for the function of minimizing the training error.",
            "So this is a very similar to the bond that I showed you before, only that before we had the VC dimension in here instead of this strange shattering coefficient."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you came here for the expecting another soothing talk of Neil Lawrence about Bayesianism.",
                    "label": 0
                },
                {
                    "sent": "And now you're in for a surprise.",
                    "label": 0
                },
                {
                    "sent": "We're starting with statistical learning theory this morning.",
                    "label": 1
                },
                {
                    "sent": "Maybe because your fresh still and.",
                    "label": 0
                },
                {
                    "sent": "Actually, yeah, I would start rather informally.",
                    "label": 0
                },
                {
                    "sent": "So the 1st 20 minutes maybe it will be quite informal and then we have some mathematics.",
                    "label": 0
                },
                {
                    "sent": "But actually, before starting, maybe just some personal remarks about yesterday's discussion.",
                    "label": 0
                },
                {
                    "sent": "So I got the impression from the discussion that cognitive science, the way it's viewed right now, is that it's about studying how human or animal cognition works.",
                    "label": 0
                },
                {
                    "sent": "Is that better?",
                    "label": 0
                },
                {
                    "sent": "Hello OK.",
                    "label": 0
                },
                {
                    "sent": "So I got the impression it's about studying how cognition in animals works, and I should say this is also what brought me into this field, so I was interested in theoretical neuroscience at some point, but.",
                    "label": 0
                },
                {
                    "sent": "I greatly have come to realize that for me it's actually much more interesting to think about cookies.",
                    "label": 0
                },
                {
                    "sent": "How could cognition could work more generally in personal animal systems in possible artificial systems, and maybe also?",
                    "label": 0
                },
                {
                    "sent": "What's the theory that should be underlying such a field of cognitive science?",
                    "label": 0
                },
                {
                    "sent": "So in a sense, I think statistical learning theory is one such attempts to construct the theory of some aspects of cognitive science.",
                    "label": 0
                },
                {
                    "sent": "And for me, that's even more fascinating than studying cognition in animals or humans, even though I still think this is an amazing inspiration to look at such problems.",
                    "label": 0
                },
                {
                    "sent": "For instance, as discussed yesterday by Chris so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have course we're far from having a theory of cognition, but let's start with whatever we have and some of it is statistical learning theory, and I think even though most of you probably won't work on statistical learning theory, and actually I'm also not working on this currently myself, I still think it's something that one needs to know if one works in this field.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to today.",
                    "label": 0
                },
                {
                    "sent": "I'll try to give you an intuition how one can derive a statistical learning theory bound.",
                    "label": 0
                },
                {
                    "sent": "So just to make sure that if you see them in the future, you're not.",
                    "label": 0
                },
                {
                    "sent": "Intimidated 'cause the basic ideas are actually not that difficult.",
                    "label": 0
                },
                {
                    "sent": "But let's start very basic.",
                    "label": 0
                },
                {
                    "sent": "So I'm from taking it from the empirical inference Department of the Max Planck Institute for Biological Cybernetics and by empirical inference I mean the process of drawing conclusions from empirical data.",
                    "label": 0
                },
                {
                    "sent": "So these data could be observations or measurements, and as scientists were doing empirical inference all the time, we're doing scientific inference.",
                    "label": 0
                },
                {
                    "sent": "So we might be measuring two observables X&Y, which are related in this fashion, and we might then be willing to infer.",
                    "label": 0
                },
                {
                    "sent": "That there's a linear law explaining these data points.",
                    "label": 0
                },
                {
                    "sent": "But we might equally well be willing to, or actually we might.",
                    "label": 0
                },
                {
                    "sent": "So it is likely it's already pointed out if even if we scattered points of ink randomly on a piece of paper.",
                    "label": 0
                },
                {
                    "sent": "So for instance, by shaking quill pin, we can still find some mathematical expression that explains these data points.",
                    "label": 0
                },
                {
                    "sent": "It might be a more complicated expression.",
                    "label": 0
                },
                {
                    "sent": "At least it looks more complicated to us, but it's also a law.",
                    "label": 0
                },
                {
                    "sent": "And question is why do we believe in the first long in this linear law?",
                    "label": 0
                },
                {
                    "sent": "And why do we not believe in the second law?",
                    "label": 1
                },
                {
                    "sent": "So what's the difference between these laws?",
                    "label": 0
                },
                {
                    "sent": "So that's something that has been that lately has thought about another.",
                    "label": 0
                },
                {
                    "sent": "Mathematicians have also thought about it, home and vile and cheating more recently.",
                    "label": 0
                },
                {
                    "sent": "And it seems to be unclear what makes an equation simple and what makes it difficult.",
                    "label": 0
                },
                {
                    "sent": "The physicist Rutherford took a pragmatic view.",
                    "label": 0
                },
                {
                    "sent": "He said that if there's a law, it should be evident from the data roughly.",
                    "label": 0
                },
                {
                    "sent": "So he said if your experiment needs statistics, you ought to have done a better experiment.",
                    "label": 0
                },
                {
                    "sent": "So I will try to convince you that there are interesting inference problems that are nontrivial, so problems were better.",
                    "label": 0
                },
                {
                    "sent": "Experiments are not enough, and I think there are lots of such problems in biology, for instance in perception.",
                    "label": 0
                },
                {
                    "sent": "So let me show you some images of handwritten digits, so this looks like another easy problem of empirical inference where it's obvious to you which is which digit you have no problem recognizing them, but it's too.",
                    "label": 0
                },
                {
                    "sent": "It's simple transformation of the image is just applying a fixed permutation of the pixels.",
                    "label": 1
                },
                {
                    "sent": "So we just reorder the pixels and mathematically in computers we represent such images as vectors.",
                    "label": 0
                },
                {
                    "sent": "So this permutation is reordering.",
                    "label": 0
                },
                {
                    "sent": "It just means we are re labeling the axis.",
                    "label": 0
                },
                {
                    "sent": "So for computer and from.",
                    "label": 0
                },
                {
                    "sent": "Many recognition algorithms.",
                    "label": 0
                },
                {
                    "sent": "These are actually the same kind of data.",
                    "label": 0
                },
                {
                    "sent": "So for the computer this problem is not harder than this one.",
                    "label": 0
                },
                {
                    "sent": "For you it's a lot harder.",
                    "label": 0
                },
                {
                    "sent": "It seems to be a paradox, and the point is of course, that the original images only appear very easy to us because we have been trained on such images all our lives.",
                    "label": 0
                },
                {
                    "sent": "So actually the two problems in some sense.",
                    "label": 0
                },
                {
                    "sent": "Have almost the same difficulty, so we've been trained on this all our life.",
                    "label": 0
                },
                {
                    "sent": "Our brain has been extracting statistical irregularities of this kind of data.",
                    "label": 0
                },
                {
                    "sent": "And in the words of Horace Barlow, famous neuro physiologists, the brain is nothing but a statistical decision organ.",
                    "label": 0
                },
                {
                    "sent": "So let's go to features.",
                    "label": 0
                },
                {
                    "sent": "That means if we want to do brain theory, we have to do statistical learning theory and other things like that.",
                    "label": 0
                },
                {
                    "sent": "Maybe most of them don't exist yet, but it's just this is learning theory.",
                    "label": 0
                },
                {
                    "sent": "At least we know some things.",
                    "label": 0
                },
                {
                    "sent": "So far it looked like.",
                    "label": 0
                },
                {
                    "sent": "There are some sort of trivial inference problems in science, like finding this straight line, which would be obvious from the data, and there are some difficult inference problems in biology, but actually it's not the division.",
                    "label": 0
                },
                {
                    "sent": "There's also many interesting difficult inference problems in science.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you one example, which is a problem studied in the group of Corner Edge who is applying machine learning methods in computational biology.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to go into all details on this problem, so the problem is one of classifying human DNA sequence locations into three different classes.",
                    "label": 0
                },
                {
                    "sent": "From a pretty large data set of subsequences, local sub sequences of DNA.",
                    "label": 0
                },
                {
                    "sent": "The data set has 50 million such sequences of length 141 and you can see here a curve learning curve, so this is the number of observations here, and this is the some measure of the accuracy, precision, recall, curve the area under the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "But don't worry bout this for detail.",
                    "label": 0
                },
                {
                    "sent": "For the moment.",
                    "label": 0
                },
                {
                    "sent": "The point is that if we only trade on let's say 1000 or a few thousand training examples, that performance is close to 0.",
                    "label": 0
                },
                {
                    "sent": "Zero would be chance level, so we're basically performing at chance level and if we use a lot more data, for instance, if we use 10 million data points, we're doing 70%, or we're doing extremely well actually in this problem.",
                    "label": 0
                },
                {
                    "sent": "So it's a kind of regularity.",
                    "label": 0
                },
                {
                    "sent": "It looks random if you only see a few thousand data points, but if you see a few million data points, you can do very well on this problem.",
                    "label": 0
                },
                {
                    "sent": "So it's something that we as humans wouldn't have seen just by looking at the data.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of an interesting hard inference problem, so it has several interesting properties.",
                    "label": 0
                },
                {
                    "sent": "One or sort of typical properties.",
                    "label": 0
                },
                {
                    "sent": "One it's it's fairly high dimensional, so we can only find the structure of the regularity.",
                    "label": 0
                },
                {
                    "sent": "If we consider many factors simultaneously, it's.",
                    "label": 0
                },
                {
                    "sent": "Fairly complex, so it's it's not well or complex means different things for different problems could be nonlinear, nonstationary?",
                    "label": 0
                },
                {
                    "sent": "It's a problem where we have relatively limited prior knowledge, so we don't understand the biological slicing mechanism in detail.",
                    "label": 0
                },
                {
                    "sent": "We don't have a mechanistic model for it.",
                    "label": 0
                },
                {
                    "sent": "We we have the data and maybe some idea of constraining the mechanisms, but only relatively little.",
                    "label": 0
                },
                {
                    "sent": "And as a consequence of these three points, and there's typically a need for large datasets in such programs.",
                    "label": 0
                },
                {
                    "sent": "This of course requires computers and it requires automatic inference methods and.",
                    "label": 0
                },
                {
                    "sent": "And of course, that's also why Incense Rutherford was right at the time with what he said, because he didn't have these methods, he didn't even have the computer.",
                    "label": 0
                },
                {
                    "sent": "So maybe in those days it didn't make sense to look for such complex dependencies.",
                    "label": 0
                },
                {
                    "sent": "That days it made sense to do better experiments.",
                    "label": 0
                },
                {
                    "sent": "And of course it still makes sense to do better experiments, but now we can do more than just better experiments.",
                    "label": 0
                },
                {
                    "sent": "So what I want to point out here is maybe also related to yesterday's discussion.",
                    "label": 0
                },
                {
                    "sent": "That in some cases we can already solve inference problems that humans can't solve.",
                    "label": 0
                },
                {
                    "sent": "I think this is a kind of a quantum leave.",
                    "label": 0
                },
                {
                    "sent": "Of course now you could say and the normal reflects whenever computer solve something you could say is actually fairly stupid.",
                    "label": 0
                },
                {
                    "sent": "What the computer is doing, and maybe it is fairly stupid.",
                    "label": 0
                },
                {
                    "sent": "And but nevertheless, I think it's a kind of problem where we would have been impressed 10 or 20 years ago.",
                    "label": 0
                },
                {
                    "sent": "If someone has had told us that there are such problems.",
                    "label": 0
                },
                {
                    "sent": "That looked like there's a complicated dependency that we don't understand and the computer can pick it up in the sense of doing the correct predictions, and we as humans can't.",
                    "label": 0
                },
                {
                    "sent": "So I think we're already in the middle of this revolution.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go up.",
                    "label": 0
                },
                {
                    "sent": "It will get closer to learning theory.",
                    "label": 0
                },
                {
                    "sent": "So what's the basic problem of learning and generalization?",
                    "label": 0
                },
                {
                    "sent": "So let me show you this example, which I've taken roughly from Olivier Busquet, who used to be in our lab and who went to Google.",
                    "label": 0
                },
                {
                    "sent": "Which is about observing this sequence of numbers.",
                    "label": 0
                },
                {
                    "sent": "So suppose you see these numbers 1247 and I ask you what's the next number so you can.",
                    "label": 0
                },
                {
                    "sent": "You can guess now 11.",
                    "label": 0
                },
                {
                    "sent": "Any other guesses 5?",
                    "label": 0
                },
                {
                    "sent": "Did someone say 12?",
                    "label": 0
                },
                {
                    "sent": "How about how about 13?",
                    "label": 0
                },
                {
                    "sent": "61 OK, so I'll give you solutions from many of them.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this is the one that most of you came up with is called the lazy caterer sequence.",
                    "label": 0
                },
                {
                    "sent": "Because it's the number of pieces into which you can cut a cake with end cuts.",
                    "label": 0
                },
                {
                    "sent": "So you just have to make sure that each additional cut intersects or previous cuts.",
                    "label": 0
                },
                {
                    "sent": "So that means adding an additional pieces at this stage.",
                    "label": 0
                },
                {
                    "sent": "Of course the pieces won't have the same size, but that doesn't matter and this is a nice sequence.",
                    "label": 0
                },
                {
                    "sent": "There's another nice sequence which I don't think has a name.",
                    "label": 0
                },
                {
                    "sent": "It gives 12 if you like 13, it's the triple Nachi sequence.",
                    "label": 0
                },
                {
                    "sent": "Each number is the sum of the previous three.",
                    "label": 0
                },
                {
                    "sent": "Here's a sequence which is nice.",
                    "label": 0
                },
                {
                    "sent": "If your mathematician you look disease, of course is the interleaved decimal expansion of \u03c0 and E. And if you go to this website, the online Encyclopedia of integer sequences, you will find 600 hits.",
                    "label": 0
                },
                {
                    "sent": "If you if you enter these phone numbers.",
                    "label": 0
                },
                {
                    "sent": "So there are 600 mathematically meaningful ways of continuing this sequence.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That seems to be a difficult problem.",
                    "label": 0
                },
                {
                    "sent": "So if we want to know which continuation is correct.",
                    "label": 0
                },
                {
                    "sent": "So which one generalizes then you might say.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "There's no way to tell, and that would also in some sense be the answer of philosophy.",
                    "label": 0
                },
                {
                    "sent": "It's essentially the problem of induction.",
                    "label": 0
                },
                {
                    "sent": "So it's strange that people still construct intelligence tests that use these kind of things, so it seems like the intelligence tests are just testing whether you think the same way as the designer of the test.",
                    "label": 0
                },
                {
                    "sent": "But let's leave that aside.",
                    "label": 0
                },
                {
                    "sent": "So in philosophy this is the induction program.",
                    "label": 0
                },
                {
                    "sent": "So let's ask a slightly which is considered to be unsolvable, or at least by some philosophers.",
                    "label": 0
                },
                {
                    "sent": "Let's ask a slightly easier question, and that's the one who started learning theory.",
                    "label": 0
                },
                {
                    "sent": "Question is not which one is correct, but how to come up with the logically.",
                    "label": 0
                },
                {
                    "sent": "With continuations that are probably correct.",
                    "label": 0
                },
                {
                    "sent": "And this also has it a name in philosophy is called the demarcation problem.",
                    "label": 0
                },
                {
                    "sent": "So both these problems were discussed at length, for instance by pop up who called the first problem Humes problem and the second one cuts problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We will focus on clients problem.",
                    "label": 0
                },
                {
                    "sent": "And we will do this in the case of two class classification.",
                    "label": 0
                },
                {
                    "sent": "So let's introduce the framework little bit.",
                    "label": 0
                },
                {
                    "sent": "So two class classification means we want to learn functions.",
                    "label": 0
                },
                {
                    "sent": "So this is this, this very simple case of learning that everybody told you yesterday is boring and irrelevant, but the good thing is we understand the theory for this case.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about this and once you've understood it, you can say it's boring, irrelevant.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We went to the functions taking two values, let's call him plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "The input is some domain X and we won't learn these functions based on M observations.",
                    "label": 0
                },
                {
                    "sent": "Each observation is a pair of input and output.",
                    "label": 0
                },
                {
                    "sent": "And we're making the assumption that all these observations are generated independently, so this word is missing here.",
                    "label": 0
                },
                {
                    "sent": "By sampling from the same underlying probability distribution, so it's a joint distribution that every time you query this distribution, every time you sample from it, you get a pair X&Y.",
                    "label": 0
                },
                {
                    "sent": "So this contains a special case.",
                    "label": 0
                },
                {
                    "sent": "For instance the situation where the X is generated randomly and the Y is just a function of X or the Y could be a function of X plus noise, or it could be more complicated.",
                    "label": 0
                },
                {
                    "sent": "General dependency between X&Y.",
                    "label": 0
                },
                {
                    "sent": "Now our goal is we want to minimize the expected error which is also called.",
                    "label": 0
                },
                {
                    "sent": "The risk of this function and the risk is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "So for each pair X&Y.",
                    "label": 0
                },
                {
                    "sent": "Sorry for each X we evaluate the function.",
                    "label": 0
                },
                {
                    "sent": "We get some F of X is F of X is supposed to be close to the correct?",
                    "label": 0
                },
                {
                    "sent": "Why we're measuring the closeness by evaluating this little expression here.",
                    "label": 0
                },
                {
                    "sent": "So we take the difference between the two and take the modulus, because the difference could be either.",
                    "label": 0
                },
                {
                    "sent": "So if you remember each of them is plus or minus one, so the difference could be plus one's very plus 2 -- 2.",
                    "label": 0
                },
                {
                    "sent": "Or it could be 0 if they are the same.",
                    "label": 0
                },
                {
                    "sent": "So we will take the modulus and the difference will be two or zero and then we divide by two.",
                    "label": 0
                },
                {
                    "sent": "So the difference would be one or zero.",
                    "label": 0
                },
                {
                    "sent": "So that's the so-called 01 loss function.",
                    "label": 0
                },
                {
                    "sent": "Which just checks whether F of X is equal to Y or not, and then we average this over this probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So we take this into, well, these expectation with respect to the distribution and problem is we can't even compute this quantity because the probability distribution is assumed to be unknown.",
                    "label": 0
                },
                {
                    "sent": "So we don't know this law generating the data.",
                    "label": 0
                },
                {
                    "sent": "We only see the data so.",
                    "label": 0
                },
                {
                    "sent": "We can't.",
                    "label": 0
                },
                {
                    "sent": "We can't compute this quantity, and therefore we also can't minimize it.",
                    "label": 0
                },
                {
                    "sent": "We cannot find the function which has the minimal error 'cause we can't compute the error.",
                    "label": 0
                },
                {
                    "sent": "So we want to somehow.",
                    "label": 0
                },
                {
                    "sent": "Maybe approximately find the minimizer of this using, not the distribution, but using what we know about the distribution, which is these data appears.",
                    "label": 0
                },
                {
                    "sent": "And for this we need what's called an induction principle in learning theory, and one such induction principle for getting from the training training data to.",
                    "label": 0
                },
                {
                    "sent": "The minimizer here is called empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I keep bending down here.",
                    "label": 0
                },
                {
                    "sent": "I'm tempted to sit down, but I worry that you will all fall asleep if I sit down so.",
                    "label": 0
                },
                {
                    "sent": "So let's let's try this.",
                    "label": 0
                },
                {
                    "sent": "So here's the induction principle.",
                    "label": 1
                },
                {
                    "sent": "We replaced this quantity here, which was the average over.",
                    "label": 0
                },
                {
                    "sent": "If you want infinitely many points suffered from the unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "By the average over the 20 points, which is all the points we have, which is all we've seen.",
                    "label": 0
                },
                {
                    "sent": "So we just compute the same error we average over the training points, and we call this a empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Or the training error.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's a good idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we're going to minimize this thing here.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Over some class of functions, so we want to find the best function from sunset.",
                    "label": 0
                },
                {
                    "sent": "In terms of explaining this and then the question is, is this consistent and consistent?",
                    "label": 0
                },
                {
                    "sent": "Statistics, roughly speaking, means that if we increase the number of observations here that go to Infinity, of course every time we change the number of observations, we will find a different minimizer for this thing, we will find a different best function in the question is, will this function eventually do the best possible job well when we do as well?",
                    "label": 0
                },
                {
                    "sent": "If we had directly minimized this quantity.",
                    "label": 0
                },
                {
                    "sent": "And and the surprising answer.",
                    "label": 0
                },
                {
                    "sent": "Is there yes and no, so it's yes under some conditions and without any additional conditions it would be no.",
                    "label": 0
                },
                {
                    "sent": "So the answer is yes, and even independently of what's the underlying probability distribution generating the data, provided that this is 1 possible condition, provided that the VC dimension of the function class is finite.",
                    "label": 1
                },
                {
                    "sent": "So with the VC dimension and I'll tell you more about all these issues.",
                    "label": 0
                },
                {
                    "sent": "So now is the short intuitive version, and afterwards I'll show you some of the derivations.",
                    "label": 0
                },
                {
                    "sent": "So we see dimension is the maximum number of points which can be classified in all possible ways using functions from your class.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit difficult to pass out.",
                    "label": 0
                },
                {
                    "sent": "For example, let's consider the set of linear classifiers in the plane.",
                    "label": 0
                },
                {
                    "sent": "In this case the VC dimension is 3 and the reason for this is the following.",
                    "label": 0
                },
                {
                    "sent": "If we take three points here in general position.",
                    "label": 0
                },
                {
                    "sent": "We can separate them or classify them into classes plus 1 -- 1 in all possible ways using functions from our class or using linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "So for three points, there are two to the power of three such possible classifications, so eight.",
                    "label": 0
                },
                {
                    "sent": "And here we have all eight of them.",
                    "label": 0
                },
                {
                    "sent": "Now in the definition says the maximum number of points which can be classified all possible ways.",
                    "label": 0
                },
                {
                    "sent": "So now we know it works for three points.",
                    "label": 0
                },
                {
                    "sent": "Question is does it work for four points?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to prove that, but here's a little example to illustrate that if you have four points, you will always be there.",
                    "label": 0
                },
                {
                    "sent": "Some at some point.",
                    "label": 0
                },
                {
                    "sent": "In a situation where you want to separate two points from the other tool and you can't delete using linear functions.",
                    "label": 0
                },
                {
                    "sent": "So it actually takes that the maximum is 3 and we see dimension is 3.",
                    "label": 0
                },
                {
                    "sent": "And in a sense, just to connect again to problem that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Say this situation.",
                    "label": 0
                },
                {
                    "sent": "So if we have a class of functions, we see dimension 3.",
                    "label": 0
                },
                {
                    "sent": "And we only observe three points.",
                    "label": 0
                },
                {
                    "sent": "We are in a non falsifiable situation because no matter how these three points, let's say this is our training set, no matter how they are labeled.",
                    "label": 0
                },
                {
                    "sent": "So no matter what plus minus ones you assign, I would always be able to find a function which explains that labeling.",
                    "label": 0
                },
                {
                    "sent": "So it's it's sort of a trivial statement.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean anything.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean I have learned anything from the data because I could have told him before looking at the data that I'm going to do a perfect job.",
                    "label": 0
                },
                {
                    "sent": "So this is a case where the class of functions is non falsifiable and actually turns out.",
                    "label": 0
                },
                {
                    "sent": "In this case we also can't generalize.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we have more data points, then and hopefully a lot more in the VC dimension we will be in a falsifiable situation, which is actually the good case because in that case it's.",
                    "label": 0
                },
                {
                    "sent": "If you take such a function from such a class, and you can explain the data well, then in some sense this can't be a quince idents, and then you can get guaranteed generalization.",
                    "label": 0
                },
                {
                    "sent": "So that's just the intuitive case and maybe just to make this more complete.",
                    "label": 0
                },
                {
                    "sent": "It turns out in D dimensions, this generalizes.",
                    "label": 0
                },
                {
                    "sent": "the PC dimension will be D + 1.",
                    "label": 0
                },
                {
                    "sent": "Which, by the way, is also the number of free parameters of this function class.",
                    "label": 0
                },
                {
                    "sent": "But there are also interesting cases.",
                    "label": 0
                },
                {
                    "sent": "There is not the number of free parameters, so it turns out if you enforce some margin of separation between the two classes.",
                    "label": 0
                },
                {
                    "sent": "Then the visitor mention can be a lot smaller.",
                    "label": 0
                },
                {
                    "sent": "So especially if this margin is large in the VC dimension, can certainly be a lot smaller.",
                    "label": 1
                },
                {
                    "sent": "And that's the basis for support vector machines, not what does learning theory.",
                    "label": 0
                },
                {
                    "sent": "What does maybe I have to put this this one some words?",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of result that.",
                    "label": 0
                },
                {
                    "sent": "How we get such results?",
                    "label": 0
                },
                {
                    "sent": "So results is the test error or the risk for the expected error.",
                    "label": 0
                },
                {
                    "sent": "Is bounded from above with the college high probabilities Hotel stick of Delta essence, very small number.",
                    "label": 0
                },
                {
                    "sent": "For all functions of our classes, functions is bounded from above by the training error plus some quantity.",
                    "label": 0
                },
                {
                    "sent": "Here that involves the VC dimension and the sample size.",
                    "label": 0
                },
                {
                    "sent": "So the number of observations that we have seen.",
                    "label": 0
                },
                {
                    "sent": "And you can see here if that we said we said image news small compared to the number of observations that we have seen in this quantity here will go to zero and this will be small.",
                    "label": 0
                },
                {
                    "sent": "So in that case.",
                    "label": 0
                },
                {
                    "sent": "We can guarantee that the test error is not much larger than the training error is, only could only be larger by this amount with high probability.",
                    "label": 0
                },
                {
                    "sent": "So that sounds almost too good to be true.",
                    "label": 0
                },
                {
                    "sent": "So suppose suppose we learn something that doesn't make sense at all, what with this bound tell us.",
                    "label": 1
                },
                {
                    "sent": "So, for instance, suppose we try to learn the mapping from.",
                    "label": 0
                },
                {
                    "sent": "From the name of a person in a telephone directory to their telephone number.",
                    "label": 0
                },
                {
                    "sent": "So probably you don't believe that this makes sense even though we have a lot of training examples.",
                    "label": 0
                },
                {
                    "sent": "So let's take a slightly different example.",
                    "label": 1
                },
                {
                    "sent": "Suppose we went to learn the mapping from the birth date of a person to their character traits.",
                    "label": 0
                },
                {
                    "sent": "It's actually something that a lot of people think makes sense in astrology, but it's a similar kind of program.",
                    "label": 0
                },
                {
                    "sent": "So in that case, or maybe going back to the telephone directory again.",
                    "label": 0
                },
                {
                    "sent": "In that case you will find that either you have a learning machine which is very large, which can actually learn by heart.",
                    "label": 0
                },
                {
                    "sent": "The whole telephone directory.",
                    "label": 0
                },
                {
                    "sent": "The mapping from the name to the person in that case, training error would be small, but you will.",
                    "label": 0
                },
                {
                    "sent": "You will actually notice that this learning machine is so large that it essentially can memorize all the mappings from name 2 telephone number.",
                    "label": 0
                },
                {
                    "sent": "In that case the VC dimension will be very large, so this quantity will be large.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you take a small learning machine, small VC dimension is quantity will be smaller, but in that case you won't be able to memorize all the themes, so you will have.",
                    "label": 0
                },
                {
                    "sent": "A large training error, so in both cases you will never guarantee that the test there are small.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't guarantee that the test error for any problem will be small.",
                    "label": 0
                },
                {
                    "sent": "It doesn't solve the induction problem, but it guarantees that if you.",
                    "label": 0
                },
                {
                    "sent": "Follow certain methodological prescriptions so if you.",
                    "label": 0
                },
                {
                    "sent": "Use a class of functions which has a visa dimension small compared to the sample size.",
                    "label": 0
                },
                {
                    "sent": "Then you can guarantee that the test around the training error will not be very far apart, and if you are lucky in the training error is more than the test or is also small.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "In a sense, it's a formalization of this popper philosophy of falsification.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It doesn't solve the induction problem 'cause we will only detect regularity's, for which we can a priori good choose good function classes.",
                    "label": 0
                },
                {
                    "sent": "That's something that we have to do before looking at the data.",
                    "label": 0
                },
                {
                    "sent": "And of course, nobody tells us how to do this, so machine learning always needs data plus prior knowledge in whichever way you will specify this prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "So let's move to the other slides.",
                    "label": 0
                },
                {
                    "sent": "So if there are questions at this point, of course I should say you can interrupt me at any anytime, so I was trying to make this fairly intuitive so far, so I hope you're all still with me.",
                    "label": 0
                },
                {
                    "sent": "And now let's do it a little bit more technical.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "Oh wait, I think I have to move this guy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, you have already seen this night, so just to remind you of the notation.",
                    "label": 0
                },
                {
                    "sent": "So here's all training set.",
                    "label": 0
                },
                {
                    "sent": "Is the underlying probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Here's the test error or the risk.",
                    "label": 0
                },
                {
                    "sent": "Here's the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "So that's the thing we're going to minimize, and that's what public intervening is studied.",
                    "label": 0
                },
                {
                    "sent": "So they studied the consistency of minimization of the empirical.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Risk.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that's one nice thing about this.",
                    "label": 0
                },
                {
                    "sent": "About this quantity, which is that this is an estimator of this one.",
                    "label": 0
                },
                {
                    "sent": "And actually, this law of large numbers in statistics, which tells us that under some fairly general conditions this quantity here will converge towards this one in probability.",
                    "label": 0
                },
                {
                    "sent": "So by this I mean that the probability of a deviation between these two guys larger than epsilon will go to zero as the number of observations goes to Infinity and it will do so no matter how small the assignment is that you choose.",
                    "label": 0
                },
                {
                    "sent": "So let's call it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Surgeons in probability.",
                    "label": 0
                },
                {
                    "sent": "Large numbers tells us that the mean of such quantity will converge to its expectation.",
                    "label": 0
                },
                {
                    "sent": "In probability.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's nice and it will actually.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you afterwards how fast it will go.",
                    "label": 0
                },
                {
                    "sent": "It actually exponentially fast.",
                    "label": 0
                },
                {
                    "sent": "And you might be led to believe that this implies this is already solving our problems.",
                    "label": 0
                },
                {
                    "sent": "So the question is, does this imply consistency does imply that you will get the optimal result in the limit of infinite sample size infinitely many observations?",
                    "label": 1
                },
                {
                    "sent": "Turns out this is not the case.",
                    "label": 1
                },
                {
                    "sent": "We need what's called a uniform version of the law of large numbers, and I'll try it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motivate this in the following picture so.",
                    "label": 0
                },
                {
                    "sent": "The classic law of large numbers tells us that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This quantity that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "For any fixed function here F this quantity will converge towards this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "City.",
                    "label": 0
                },
                {
                    "sent": "So in this diagram this means here we have a function class.",
                    "label": 0
                },
                {
                    "sent": "It could be a high dimensional, but I'm just drawing one dimension here.",
                    "label": 0
                },
                {
                    "sent": "We have the risk.",
                    "label": 0
                },
                {
                    "sent": "So for each function I get a certain value of the empirical risk of the training error.",
                    "label": 0
                },
                {
                    "sent": "Certain value of the true risk.",
                    "label": 0
                },
                {
                    "sent": "And the law of large numbers tells us if we fix the function, so we look at one slice here, then this training error will converge towards the true error in probability.",
                    "label": 0
                },
                {
                    "sent": "So this as we increase the number of observations, this curve here will wiggle around this one.",
                    "label": 0
                },
                {
                    "sent": "Of course is fixed that that's the expected error, but this one will wiggle around and will it will wiggle around in such a way that for each fixed function F this point here converges towards this one in probability.",
                    "label": 0
                },
                {
                    "sent": "Now if we ask the question of consistency, consistency means empirical risk minimization, so it it tells us we should always pick the minimize of this curve.",
                    "label": 0
                },
                {
                    "sent": "So we take this function here.",
                    "label": 0
                },
                {
                    "sent": "And roughly speaking, consistency that means does the minimum of this curve converge towards the minimum of the other curve?",
                    "label": 0
                },
                {
                    "sent": "That's what we want.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly, it turns out.",
                    "label": 0
                },
                {
                    "sent": "At this point, Wise Convergence doesn't imply consistency.",
                    "label": 0
                },
                {
                    "sent": "So if you are mathematicians, you probably will immediately intuitively believe me.",
                    "label": 0
                },
                {
                    "sent": "But the point is.",
                    "label": 0
                },
                {
                    "sent": "Even if at each point we have this convergence, these convergences are all in probability and they can all happen at different speeds and things can go wrong.",
                    "label": 0
                },
                {
                    "sent": "In the minimum, minimum might not converge.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you have something which is called uniform convergence in mathematics, so uniform convergence means somehow that this curve converges to the other one with the same speed everywhere, roughly speaking, then also the minima will converge, so this pointwise convergence does not imply convergence of the minima.",
                    "label": 0
                },
                {
                    "sent": "And actually one can.",
                    "label": 0
                },
                {
                    "sent": "So it's a uniform convergence or and I hope you would.",
                    "label": 0
                },
                {
                    "sent": "You just believe me this.",
                    "label": 0
                },
                {
                    "sent": "A little bit more about you.",
                    "label": 0
                },
                {
                    "sent": "Later uniform convergence will be sufficient, and in some sense even necessary and sufficient for consistency.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, uniform convergence is something that depends on the whole function class.",
                    "label": 1
                },
                {
                    "sent": "So it has to work for all functions, so the larger the function classes, the harder it is to get uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "So let's think about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What happens if we take?",
                    "label": 0
                },
                {
                    "sent": "We take all functions from our input domain to plus minus one.",
                    "label": 0
                },
                {
                    "sent": "So can we after all.",
                    "label": 0
                },
                {
                    "sent": "We don't know apriori which functions the correct one, so why don't we start with all possible functions and then try to identify the correct one?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that's impossible, and that's easy to see.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have this training set here, and suppose we have some test points.",
                    "label": 0
                },
                {
                    "sent": "Expire XI bar, where I'm assuming for simplicity that test points and the training points don't overlap, so all test points are really new points that we haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "Ben, whatever function F you give me.",
                    "label": 0
                },
                {
                    "sent": "I can construct a function F star.",
                    "label": 0
                },
                {
                    "sent": "Which will agree with the function F on all training points.",
                    "label": 0
                },
                {
                    "sent": "And we'll disagree on all test points, so it's trivial to construct this function because we have we allow all functions, so whatever you give me, I will just define it.",
                    "label": 0
                },
                {
                    "sent": "For example to find it such that it says the opposite on all test points.",
                    "label": 0
                },
                {
                    "sent": "The opposite of your function.",
                    "label": 0
                },
                {
                    "sent": "I suppose this is so you have a learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You look at the training data you say.",
                    "label": 0
                },
                {
                    "sent": "I reckon this is the correct function here.",
                    "label": 0
                },
                {
                    "sent": "That's that's a good function.",
                    "label": 0
                },
                {
                    "sent": "Then I will go ahead and will construct this other function F star and I'll say, well, actually in the training data it says exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Better on the test data.",
                    "label": 1
                },
                {
                    "sent": "My function says the opposite.",
                    "label": 0
                },
                {
                    "sent": "So based on the training data, we can't choose which functions is better and also if we look at the convergence then if it's the case that for your function empirical risk and risk are nearby.",
                    "label": 1
                },
                {
                    "sent": "Then it's quite possible that for my functions they are quite far apart because my function only agrees on the training points with your function at all other points could be saying something different, so I can construct a function which has a very different behavior in terms of training error and tester of being nearby.",
                    "label": 1
                },
                {
                    "sent": "So this is sort of trivial observation.",
                    "label": 0
                },
                {
                    "sent": "It has been called the No Free lunch theorem of learning.",
                    "label": 1
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "If we have no restriction on the class of functions from which we do the estimation.",
                    "label": 0
                },
                {
                    "sent": "Then there's nothing we can do, so we need some prior knowledge before we do the learning and the prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "In the case of statistical learning theory has to consist of restricting the class of functions from which we do.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My estimation.",
                    "label": 0
                },
                {
                    "sent": "So that's that's one way of thinking.",
                    "label": 0
                },
                {
                    "sent": "You talk into account prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "We would actually measure the size of these classes of functions in terms of capacity.",
                    "label": 0
                },
                {
                    "sent": "Concepts such as the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Of course there are other ways of doing it, for instance the Bayesian way where we place prior distributions on the class of functions.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just as a side note.",
                    "label": 0
                },
                {
                    "sent": "So let's look at it a little bit in detail so.",
                    "label": 0
                },
                {
                    "sent": "Let's introduce a shorthand Cy I for this quantity.",
                    "label": 0
                },
                {
                    "sent": "So this was our.",
                    "label": 0
                },
                {
                    "sent": "01 misclassification error.",
                    "label": 0
                },
                {
                    "sent": "So zero if F of X is Y and one whenever these two things disagree.",
                    "label": 0
                },
                {
                    "sent": "So you might remember we were interested in whether the sum over these quantities convergence to their expectation.",
                    "label": 0
                },
                {
                    "sent": "So whether the sum converges with the integral.",
                    "label": 0
                },
                {
                    "sent": "And these quantities are actually in statistics, the OR in probability.",
                    "label": 0
                },
                {
                    "sent": "These are called the newly trials in their independent penalty trials.",
                    "label": 0
                },
                {
                    "sent": "That independent because we are assuming that X&Y are sampled independently from an underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're also assuming F is some fixed function here.",
                    "label": 0
                },
                {
                    "sent": "In their Bernoulli trials with Christy, they take values zero or one.",
                    "label": 0
                },
                {
                    "sent": "So now we are.",
                    "label": 0
                },
                {
                    "sent": "Left with a question whether this quantity, which is now our training error.",
                    "label": 0
                },
                {
                    "sent": "Converges towards this one, which is our test error or risk.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there's a nice bound from statistics.",
                    "label": 0
                },
                {
                    "sent": "I do too.",
                    "label": 0
                },
                {
                    "sent": "How much of an American statistician which who I think is still alive?",
                    "label": 0
                },
                {
                    "sent": "And so this is not that all this stuff is bounded, probably from the 60s, which tells us how fast this convergence takes place.",
                    "label": 0
                },
                {
                    "sent": "So it tells us for such Bernoulli independent trials.",
                    "label": 0
                },
                {
                    "sent": "So these quantities taking value 01.",
                    "label": 0
                },
                {
                    "sent": "The probability that the training error is different from the test error by at least epsilon is bounded from above by this quantity here.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that this goes to zero exponentially fast as we increase the number of observations M. So this tells us how fast this convergence goes.",
                    "label": 0
                },
                {
                    "sent": "So one example of vanderlee trials is if you flip a coin.",
                    "label": 0
                },
                {
                    "sent": "So suppose we we we call heads zero and tails one.",
                    "label": 0
                },
                {
                    "sent": "Then, so these quantities.",
                    "label": 0
                },
                {
                    "sent": "These variables could measure how often do you get heads and tails.",
                    "label": 0
                },
                {
                    "sent": "So let's say you flip the coin 100 times.",
                    "label": 0
                },
                {
                    "sent": "You measure how often you get heads and tails.",
                    "label": 0
                },
                {
                    "sent": "So so you sum up the tails corresponding to once.",
                    "label": 0
                },
                {
                    "sent": "Remember correctly, suppose you get 60 tails or or.",
                    "label": 0
                },
                {
                    "sent": "Suppose you want to ask how likely is it that I will get more than 60 or less than 40 fails.",
                    "label": 0
                },
                {
                    "sent": "So 60 Tails, then this quantity would be.",
                    "label": 0
                },
                {
                    "sent": "0.6 of course we assume it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's an unbiased coin, so this will be 0.5.",
                    "label": 0
                },
                {
                    "sent": "So more than 60 or less than 40 will correspond to an epsilon of 0.1 deviation either above 0.6 or below 0.4.",
                    "label": 0
                },
                {
                    "sent": "So we set epsilon to 0.1.",
                    "label": 0
                },
                {
                    "sent": "So here we have 0.01.",
                    "label": 1
                },
                {
                    "sent": "Sample size is 100, so this gives US1, so it's E to the minus 2.",
                    "label": 0
                },
                {
                    "sent": "Something like 1 / 8.",
                    "label": 0
                },
                {
                    "sent": "So times two we get 1 / 4, so this is the probability that we get more than 60 or less than 40.",
                    "label": 0
                },
                {
                    "sent": "Is that the numbers again?",
                    "label": 0
                },
                {
                    "sent": "But I think it's 1 / 4, so it's it's bounded from above by 25%.",
                    "label": 0
                },
                {
                    "sent": "Which sounds reasonable.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a nice bound.",
                    "label": 0
                },
                {
                    "sent": "And oh, and I also, I should say here.",
                    "label": 0
                },
                {
                    "sent": "So yeah, talk about the probability I should be telling him what's the probability over what does it refer to him?",
                    "label": 0
                },
                {
                    "sent": "So want random experiment and it's the random experiment of.",
                    "label": 0
                },
                {
                    "sent": "Drawing a sample from the so-called product distribution in IID samples remains M times.",
                    "label": 0
                },
                {
                    "sent": "I will draw a pair XY from my unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "Then I will compute this quantity and then over this procedure of drawing M times, that's the probability.",
                    "label": 0
                },
                {
                    "sent": "So that tells us how often we go wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, and one thing we're going to need later on is rather than looking how fast this mean converges to that expectation, we can.",
                    "label": 0
                },
                {
                    "sent": "Also, it's clear we can also derive an upper bound.",
                    "label": 0
                },
                {
                    "sent": "How far how fast 2 means converge to each other, because each of these quantities will converge to the expectations.",
                    "label": 0
                },
                {
                    "sent": "So if they're both close to this, they also have to be close to each other and we have to do some kind of triangle inequality and we lose a little bit over here, so we get an additional factor of two and we get.",
                    "label": 0
                },
                {
                    "sent": "Epsilon over 2 instead of epsilon, but it's basically the same.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing.",
                    "label": 0
                },
                {
                    "sent": "So so if we translate this back into machine learning terminology.",
                    "label": 1
                },
                {
                    "sent": "It tells us the probability of obtaining an example, so obtaining a training set well, training error and test error differ by more than epsilon is bounded by this quantity.",
                    "label": 1
                },
                {
                    "sent": "Now I already mentioned at some point this refers to one fixed function F. So why is this the case?",
                    "label": 0
                },
                {
                    "sent": "So what if we chose the function that directly interested in so we choose the function, giving us the minimal training error.",
                    "label": 0
                },
                {
                    "sent": "After all, that's what we want to know is that function going to do well?",
                    "label": 0
                },
                {
                    "sent": "Problem is this function has looked at the training, so choosing the function that has a minimum training error, we first have to look at the training data so that somehow this function knows something about all training examples.",
                    "label": 0
                },
                {
                    "sent": "And then, unfortunately, even though.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even though these guys are still in dependently xiy function suddenly in depends on all of them jointly and then these sires are no longer independent.",
                    "label": 0
                },
                {
                    "sent": "So then the channel bound doesn't apply and that's the whole problem.",
                    "label": 0
                },
                {
                    "sent": "So that's what we have.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To deal with now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here's something that I will not prove, and it's actually not not easy to prove one of the directions is easy.",
                    "label": 0
                },
                {
                    "sent": "The other one is highly nontrivial.",
                    "label": 0
                },
                {
                    "sent": "And but I've tried to motivate it a little bit before in this picture.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that.",
                    "label": 0
                },
                {
                    "sent": "It's necessary and sufficient for a certain type of consistency, so consistency.",
                    "label": 0
                },
                {
                    "sent": "Remember the question, do we get the best result in the limit of infinitely many data points?",
                    "label": 0
                },
                {
                    "sent": "For this is necessary and sufficient to have a type of uniform convergence.",
                    "label": 1
                },
                {
                    "sent": "Of training error to test error of empirical restore risk.",
                    "label": 0
                },
                {
                    "sent": "Open the whole function class, so in this picture from before over, this whole X axis overall functions.",
                    "label": 0
                },
                {
                    "sent": "So if we have this uniform convergence, then we have consistency and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if you look at this thing, he would probably say this is a little bit hard to check for a learning machine, and that's why actually we would like to have properties of classes of functions which tell us something about whether uniform convergence will take place or not.",
                    "label": 1
                },
                {
                    "sent": "And let's capacity measures.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And this will somehow show up automatically if we tried to prove such a bound.",
                    "label": 0
                },
                {
                    "sent": "So let's take a closer look at this quantity.",
                    "label": 1
                },
                {
                    "sent": "So remember, this is what we were interested in.",
                    "label": 0
                },
                {
                    "sent": "So this is now the deviation between the training error and the test error.",
                    "label": 0
                },
                {
                    "sent": "The probability that this deviation is larger than epsilon, but now it's uniformly so this is a supremum if you have never seen a supremum, just think of it as a maximum through.",
                    "label": 0
                },
                {
                    "sent": "This is the sort of the deviation that we get for the worst possible function, and by worst I mean a function where training and test errors are as different as possible.",
                    "label": 0
                },
                {
                    "sent": "So anyway, it's a function where measuring the training error misleads us a lot about what's the test error, so that's our worst function, so we're interested in what's the probability that for the worst function, we are misled by more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So if we can prove that even for the worst function we are epsilon closed with high probability, then we find because then we don't care about which one is the worst one we've got just going to use the one minimizing the training, and we know we're going to be fine.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So first of all, let's start with the simplest case.",
                    "label": 1
                },
                {
                    "sent": "Our function class contains only one function, so in that case we can just cross out this supremum, the maximum.",
                    "label": 0
                },
                {
                    "sent": "So we are left with the statement that we had on the previous slide in the turn of Bond.",
                    "label": 0
                },
                {
                    "sent": "That's the job for us.",
                    "label": 0
                },
                {
                    "sent": "So in that case the turn of bond.",
                    "label": 0
                },
                {
                    "sent": "Tells us what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "I may be just as an aside, one application of this telephone.",
                    "label": 0
                },
                {
                    "sent": "So even if you're, if you're a Bayesian, let's say you were busy, but not a completely Orthodox Bayesian.",
                    "label": 0
                },
                {
                    "sent": "So you're Bayesian, who's still evaluates things on a test set as as a real Bayesian.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't be doing this, you should just put in all your prior knowledge, training the data.",
                    "label": 0
                },
                {
                    "sent": "Then you've done.",
                    "label": 0
                },
                {
                    "sent": "You don't have to evaluate anything, but let's say you're a sort of a reasonable Bayesian, likely Lawrence, so you are still testing your things.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have 1000 test examples that you want to know.",
                    "label": 0
                },
                {
                    "sent": "No, I've missed my my error on this 10,000 examples.",
                    "label": 0
                },
                {
                    "sent": "How likely is it that I'm close to the true error that I would be measuring if I had a million examples or 10,000,000?",
                    "label": 0
                },
                {
                    "sent": "Then also this phone tells you this.",
                    "label": 0
                },
                {
                    "sent": "'cause it tells you so.",
                    "label": 0
                },
                {
                    "sent": "Now we have one fixed function.",
                    "label": 0
                },
                {
                    "sent": "It's now so we have chosen the function that minimizes the training error or that maximizes the evidence or whatever.",
                    "label": 0
                },
                {
                    "sent": "Quite here, and you want to use you have one fixed function.",
                    "label": 0
                },
                {
                    "sent": "Now you're looking at the test set.",
                    "label": 0
                },
                {
                    "sent": "So this function has nothing to do with test set and then again these losses.",
                    "label": 0
                },
                {
                    "sent": "These siii on the test points.",
                    "label": 0
                },
                {
                    "sent": "There are independent if your test set is IID.",
                    "label": 0
                },
                {
                    "sent": "If your testing samples are drawn IID from some distribution and then this tells you how far is this empirical quantity, or my average over the test set?",
                    "label": 0
                },
                {
                    "sent": "From the real test error that I would be measuring if I had infinitely many test examples.",
                    "label": 0
                },
                {
                    "sent": "And this tells you that it's good to have more examples, because the difference will go exponentially with code on exponentially.",
                    "label": 0
                },
                {
                    "sent": "So that's nice, so let's but now let's go to the slightly more complicated case where we have a function with several sorry function class with several functions and to do it very slowly, we start with two functions.",
                    "label": 1
                },
                {
                    "sent": "So the plan will be 2 functions and then end functions.",
                    "label": 0
                },
                {
                    "sent": "We can use something called the Union bound and then nontrivial step will be from any functions to infinitely many functions.",
                    "label": 0
                },
                {
                    "sent": "So can we still do it for infinitely many functions?",
                    "label": 0
                },
                {
                    "sent": "And that's there's some nice ideas of public companies that made this problem solvable.",
                    "label": 0
                },
                {
                    "sent": "They're called symmetrization.",
                    "label": 0
                },
                {
                    "sent": "And interest in the introduction of capacity concepts and the basic idea is that even if there are infinitely many.",
                    "label": 1
                },
                {
                    "sent": "The capacity is small in some sense, then on a finite sample of observations they will effectively behave as if there are only finitely many Oracle.",
                    "label": 0
                },
                {
                    "sent": "Are there?",
                    "label": 0
                },
                {
                    "sent": "I should even say sub exponentially many, but you'll see a little bit more about that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at two functions.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have a function class containing two functions now.",
                    "label": 0
                },
                {
                    "sent": "We will provide rewrite this thing here, so remember this is what we are interested in.",
                    "label": 0
                },
                {
                    "sent": "The maximum of these two functions of these deviations.",
                    "label": 0
                },
                {
                    "sent": "And so when.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When is this?",
                    "label": 0
                },
                {
                    "sent": "This equality here true, but we could say we write it as two events.",
                    "label": 0
                },
                {
                    "sent": "One is the.",
                    "label": 0
                },
                {
                    "sent": "Event that the risks.",
                    "label": 0
                },
                {
                    "sent": "So the test around the training error for function F1.",
                    "label": 0
                },
                {
                    "sent": "Differ by more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So I mean, let's say I'm interested in whether one of the two functions has a risk of at least epsilon, or whether the worst function has a risk of at least steps or more than epsilon, and this will.",
                    "label": 0
                },
                {
                    "sent": "This will take place if the first function has a risk of more than epsilon or the second one has a risk of more than website, and this is not an exclusive or they could also both be.",
                    "label": 0
                },
                {
                    "sent": "Bibette so C One is the event that for the first function.",
                    "label": 1
                },
                {
                    "sent": "We draw a training set such that the risks differ by more than epsilon.",
                    "label": 1
                },
                {
                    "sent": "C2 is the same for the second function.",
                    "label": 0
                },
                {
                    "sent": "So then we can rewrite this quantity as the probability of the Union of these two events.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this again as the sum of these two probabilities minus the probability that both takes place.",
                    "label": 0
                },
                {
                    "sent": "Probabilities are negative, so if we drop this term we get an upper bound which looks like this.",
                    "label": 0
                },
                {
                    "sent": "So maybe if you haven't so personally I I don't know how much mathematics cognitive scientists typically studying, so maybe I can.",
                    "label": 0
                },
                {
                    "sent": "I can quickly ask how many of you are sort of cognitive scientists or psychologists.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's I would say 25%.",
                    "label": 0
                },
                {
                    "sent": "How many of you are mathematicians or physicists or mathematically minded computer scientists?",
                    "label": 0
                },
                {
                    "sent": "So that's that's more so I think probably for you it will be easy for the other ones I'm trying to also give some intuition.",
                    "label": 0
                },
                {
                    "sent": "So so, roughly speaking, the short version is we're interested in whether one of the two functions misleads us about the difference between training and test error.",
                    "label": 0
                },
                {
                    "sent": "And I'm saying the probability that this will happen is upper bounded by the probability that the first one will be mislead us, plus the probability of the second one misleading us, and it's just a bound because it could be that both are misleading us, so I could be doing some double counting here.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we have this.",
                    "label": 0
                },
                {
                    "sent": "Now we at this point, these two quantities.",
                    "label": 1
                },
                {
                    "sent": "Now they both refer in system function classes of size 1 again, so I can use the channel font for this both these.",
                    "label": 0
                },
                {
                    "sent": "I will get the identical term for both of them, so I will get the same result as before, which was this one only with a factor of two here.",
                    "label": 0
                },
                {
                    "sent": "So that factor of two I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't mind that factor of two, because this thing here goes down exponentially with the number of observations, so this goes down so fast that this factor tool doesn't cost us anything.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at in functions and I think you can.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So imagine what's going to happen even if I don't go through the details.",
                    "label": 0
                },
                {
                    "sent": "What we will get is we will get an extra factor of N. So we can upper bound this this quantity like before.",
                    "label": 0
                },
                {
                    "sent": "This time it's a sum of N quantities, each of them we can handle with the channel font.",
                    "label": 0
                },
                {
                    "sent": "So we get an extra factor of N. So, so again, to make this a little bit intuitive.",
                    "label": 1
                },
                {
                    "sent": "Suppose you're you trained learning machine with your favorite method.",
                    "label": 0
                },
                {
                    "sent": "Now you want to evaluate on our test set.",
                    "label": 0
                },
                {
                    "sent": "Suppose you're doing something that none of us would ever do your training.",
                    "label": 0
                },
                {
                    "sent": "Several learning machines.",
                    "label": 1
                },
                {
                    "sent": "You evaluate all of them in the test set.",
                    "label": 0
                },
                {
                    "sent": "Afterwards, you take the one that does based on the test set.",
                    "label": 0
                },
                {
                    "sent": "Maybe write a newspaper about it.",
                    "label": 0
                },
                {
                    "sent": "So how how bad can it be?",
                    "label": 0
                },
                {
                    "sent": "Well, actually if you take any functions and you do that, then again this bound.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's not written down here.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous bond.",
                    "label": 0
                },
                {
                    "sent": "So if you take 2 functions and you choose the one that does better.",
                    "label": 0
                },
                {
                    "sent": "Then this bound here tells you how far you can be, how far your estimate of the test error of the function that you are reporting in your NIPS paper can be from the true test error.",
                    "label": 0
                },
                {
                    "sent": "So it can be it can be away by a factor of two more, but actually it's not that bad if you have a large test set.",
                    "label": 0
                },
                {
                    "sent": "Again, this factor of 2 is really pretty insignificant compared to how fast this goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's not, especially I'm not saying you should be doing this, but it's I'm saying it's not so easy to cheat like that.",
                    "label": 0
                },
                {
                    "sent": "If you really want to cheat, then you shouldn't just be training two functions, or you shouldn't just be training in functions.",
                    "label": 0
                },
                {
                    "sent": "If you want cheap, you should have some kind of adaptive procedure.",
                    "label": 0
                },
                {
                    "sent": "Have you changed your parameter?",
                    "label": 0
                },
                {
                    "sent": "If you look at the test sets, you see how it's getting a little bit better.",
                    "label": 0
                },
                {
                    "sent": "How about if I changed a little bit more?",
                    "label": 0
                },
                {
                    "sent": "So if you do a procedure like this, and of course, you're effectively if your procedures clever, maybe you're effectively using infinitely many functions from which you are selecting.",
                    "label": 0
                },
                {
                    "sent": "Possible still depend, even if it's infinitely many, and this will be the content of the next slides.",
                    "label": 0
                },
                {
                    "sent": "Whether you can cheat or not depends on what's the capacity of this infinite set of functions.",
                    "label": 0
                },
                {
                    "sent": "So let's go to capacity.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was seeing some interest lighting up in your eyes when I was talking about this.",
                    "label": 0
                },
                {
                    "sent": "Explain something about your successful career.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to comment on that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So infinite function classes, so that's the that's the real cheating.",
                    "label": 1
                },
                {
                    "sent": "So the interesting thing here is an empirical risk.",
                    "label": 1
                },
                {
                    "sent": "Only refers to endpoints, so if we are interested in this quantity here, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're still always talking about this thing here.",
                    "label": 0
                },
                {
                    "sent": "So now we went to.",
                    "label": 0
                },
                {
                    "sent": "Look at the supremum over an infinite function class.",
                    "label": 0
                },
                {
                    "sent": "Now, this quantity here, even if the function is infinite, we only evaluate the functions on our training points.",
                    "label": 0
                },
                {
                    "sent": "So if we have finitely many training points, then somehow?",
                    "label": 0
                },
                {
                    "sent": "We are factorizing this function class were saying all functions that take the same values on the training points.",
                    "label": 0
                },
                {
                    "sent": "We will consider them equivalent if we only interested the training points.",
                    "label": 0
                },
                {
                    "sent": "What does it matter?",
                    "label": 0
                },
                {
                    "sent": "What other values the functions take, so that's in a sense effectively finite.",
                    "label": 0
                },
                {
                    "sent": "But this thing is still giving us trouble, so we're trying to convert this thing into something like that.",
                    "label": 0
                },
                {
                    "sent": "And maybe you already can guess.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How this is gonna work?",
                    "label": 0
                },
                {
                    "sent": "Because I've already prepared you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right before.",
                    "label": 0
                },
                {
                    "sent": "So we're going to replace.",
                    "label": 0
                },
                {
                    "sent": "This quantity by something where both these terms are training errors.",
                    "label": 0
                },
                {
                    "sent": "So I showed you before a channel found which is not about the convergence of a mean to an expectation, but the convergence of two means to each other.",
                    "label": 0
                },
                {
                    "sent": "So we're doing something like that.",
                    "label": 0
                },
                {
                    "sent": "We're using two training errors.",
                    "label": 0
                },
                {
                    "sent": "On a double sample, so we know we take two endpoints.",
                    "label": 0
                },
                {
                    "sent": "We are going to compute the training error on the 1st endpoints here on the second important here we are asking the question and then we are.",
                    "label": 1
                },
                {
                    "sent": "We can actually say that the quantity that we originally were interested in will be upper bounded by the probability of these two guys deviated from each other by more than epsilon over 2.",
                    "label": 0
                },
                {
                    "sent": "And here I have a factor of two outside.",
                    "label": 0
                },
                {
                    "sent": "So I think this is so fairly intuitive.",
                    "label": 0
                },
                {
                    "sent": "You probably believe me that this is the case.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now suddenly everything is only on finitely many happening on finitely many points over 2 endpoints, but nevertheless that's a big improvement to perform where we could have potentially infinite function classes.",
                    "label": 0
                },
                {
                    "sent": "So let's see how large a function class can be finitely many points.",
                    "label": 0
                },
                {
                    "sent": "So let's call this quantity calligraphic end.",
                    "label": 0
                },
                {
                    "sent": "It depends on the function class.",
                    "label": 1
                },
                {
                    "sent": "It depends on how many points we have is called the shattering coefficient.",
                    "label": 1
                },
                {
                    "sent": "So what is the shattering coefficient?",
                    "label": 0
                },
                {
                    "sent": "So remember, we're still talking about pattern recognition, so outputs plus minus one is the maximum number of different outputs.",
                    "label": 0
                },
                {
                    "sent": "That's a all function class can generate, generate on two endpoints.",
                    "label": 1
                },
                {
                    "sent": "So in other words, in the maximum number of ways the function class can separate two endpoints into two classes.",
                    "label": 1
                },
                {
                    "sent": "So we have only finitely many points where we evaluate the functions.",
                    "label": 0
                },
                {
                    "sent": "The outputs of the functions are plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "Then you can.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that only finitely many possible outputs can be observed.",
                    "label": 0
                },
                {
                    "sent": "And of course, the largest number of such different output vectors.",
                    "label": 0
                },
                {
                    "sent": "So so I collect all these outputs in one vector of dimension 2M.",
                    "label": 0
                },
                {
                    "sent": "The largest number would be 2 to the power of two M because each of these quantities could be plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "Likely we are looking at the losses.",
                    "label": 0
                },
                {
                    "sent": "It could be one or zero, but it doesn't matter is the same.",
                    "label": 0
                },
                {
                    "sent": "So the largest size of this thing could be 2 to the power of two M. So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So if we just briefly go back?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I said before, if we have finitely many.",
                    "label": 0
                },
                {
                    "sent": "We get a factor of extra factor of N.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our bones so so down here we would have fucked over end which is good.",
                    "label": 0
                },
                {
                    "sent": "And now if we have it said if we have this shattering coefficient, I told you it could be in principle up to two to the power of two M. So if I put 2 to the power of two AM here, let's not so nice because it was about with the power of two M increases exponentially.",
                    "label": 0
                },
                {
                    "sent": "This decreases exponentially.",
                    "label": 0
                },
                {
                    "sent": "That's not very conclusive.",
                    "label": 0
                },
                {
                    "sent": "We're not.",
                    "label": 0
                },
                {
                    "sent": "We're not sure whether this will go to zero, probably will not so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or 2 to the power of two M is not good, so we actually in this case if the cardinality of this set of outputs is actually 2 to the power of two M, then the function classes set to shatter two endpoints.",
                    "label": 1
                },
                {
                    "sent": "And again, like before, this is the non falsifiable case.",
                    "label": 1
                },
                {
                    "sent": "If we can generate all possible outputs.",
                    "label": 0
                },
                {
                    "sent": "With functions from our class on our sample size, then we wouldn't assume that we can generalize, so that's the bad case.",
                    "label": 1
                },
                {
                    "sent": "In this case, the function class shatters.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two endpoints.",
                    "label": 0
                },
                {
                    "sent": "But let's go back to the original problem.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do, and also we're interested in this quantity.",
                    "label": 0
                },
                {
                    "sent": "First, we use the Symmetrization lemma.",
                    "label": 0
                },
                {
                    "sent": "So we replaced this thing here by a difference between two training errors.",
                    "label": 0
                },
                {
                    "sent": "So this is not probability over drawing 22M samples, but don't worry about that.",
                    "label": 0
                },
                {
                    "sent": "So now we have this quantity.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to say.",
                    "label": 0
                },
                {
                    "sent": "That effectively.",
                    "label": 0
                },
                {
                    "sent": "On two endpoints or function class has only a size.",
                    "label": 0
                },
                {
                    "sent": "A size which is measured by the shattering coefficient.",
                    "label": 1
                },
                {
                    "sent": "So now we rewrite this quantity.",
                    "label": 0
                },
                {
                    "sent": "So the supremum of this or this event that at least one of the all that the supremum will be larger than epsilon over 2.",
                    "label": 0
                },
                {
                    "sent": "By the Union of the events that I get by looking at whether these individual functions are larger than this in action.",
                    "label": 0
                },
                {
                    "sent": "Say I'm slightly cheating here, but don't worry about this now.",
                    "label": 0
                },
                {
                    "sent": "It can be done correctly.",
                    "label": 0
                },
                {
                    "sent": "It's just a little bit more complicated and I think for the basic idea this is fine.",
                    "label": 0
                },
                {
                    "sent": "So now OK, we effectively only have.",
                    "label": 0
                },
                {
                    "sent": "In calligraphic functions.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to do a union bound over these functions, so we get.",
                    "label": 0
                },
                {
                    "sent": "The sum over all these events and now is the sum within calligraphic terms.",
                    "label": 1
                },
                {
                    "sent": "And now for all this, we use the usual channels bound.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually now we use this slightly unusual turn of bond, which is about the difference between through empirical means.",
                    "label": 0
                },
                {
                    "sent": "If we do this, we get.",
                    "label": 0
                },
                {
                    "sent": "In effect, while taking altogether, this was our quantity of interest, is upper bounded by this here, where this term here is sort of the usual thing only that we have slightly different constants?",
                    "label": 0
                },
                {
                    "sent": "And here we have the shattering coefficient.",
                    "label": 1
                },
                {
                    "sent": "So you can see here if the shattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "Does not grow exponentially in them.",
                    "label": 1
                },
                {
                    "sent": "So if this gross up exponentially.",
                    "label": 0
                },
                {
                    "sent": "Then this quantity he will win in the long term.",
                    "label": 0
                },
                {
                    "sent": "For large sample size, and then we refine this thing will he will go to 0 so it means.",
                    "label": 0
                },
                {
                    "sent": "For very large sample sizes, the training error on the test error will effectively be the same.",
                    "label": 0
                },
                {
                    "sent": "So that's nice, and this is an example of a VC of applicable link is inequality.",
                    "label": 0
                },
                {
                    "sent": "And maybe just all the interpretations so.",
                    "label": 1
                },
                {
                    "sent": "And this P here as I've pointed out before, it refers to drawing training examples.",
                    "label": 0
                },
                {
                    "sent": "So drawing an example of training points.",
                    "label": 1
                },
                {
                    "sent": "And here this risk is an expectation.",
                    "label": 0
                },
                {
                    "sent": "Overdrawing the test examples.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it yeah so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I can do?",
                    "label": 0
                },
                {
                    "sent": "So I've already told you that this effectively this tells us that if we have many many training examples, it's unlikely that the training error will be very different from the test error, so it doesn't come as a surprise that we can rewrite this whole bound as a bound on the test error involving the training error plus something that depends on all this.",
                    "label": 0
                },
                {
                    "sent": "To do this, we just.",
                    "label": 0
                },
                {
                    "sent": "Set the right hand side equal to sum Delta, which we think of as a very small number, and then we solve for epsilon.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get the epsilon which corresponds that Delta, then this thing here will be absolutely epsilon.",
                    "label": 0
                },
                {
                    "sent": "So the deviation between test error and training error would be upper bounded by this epsilon with probability of at least one minus Delta.",
                    "label": 1
                },
                {
                    "sent": "So the probability of going wrong would be Delta at most Delta.",
                    "label": 0
                },
                {
                    "sent": "And then this quantity here will depend on this shattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "And on the number of observations and of course on this Delta thing here, which we think of as a small number.",
                    "label": 0
                },
                {
                    "sent": "So, and since with the bond on the previous page was a supremum over the function class, or think of it as a maximum, this bond holds true independent of the function.",
                    "label": 0
                },
                {
                    "sent": "So in particular, holds true for the function of minimizing the training error.",
                    "label": 1
                },
                {
                    "sent": "So this is a very similar to the bond that I showed you before, only that before we had the VC dimension in here instead of this strange shattering coefficient.",
                    "label": 0
                }
            ]
        }
    }
}