{
    "id": "b2opybu27dgxl5v5pavk5x4cmvtdlofz",
    "title": "Large-scale Bayesian Inference for Collaborative Filtering",
    "info": {
        "author": [
            "Ole Winther, Technical University of Denmark"
        ],
        "published": "Dec. 31, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/abi07_winther_lsb/",
    "segmentation": [
        [
            "OK, so this is a joint work with Woolridge and placed Samsung from Cambridge so my mind the kind of approximate inference I'm going to present here will not be so advanced.",
            "It would be mostly variational Bayes.",
            "I think the message I want to try to convey is that it's.",
            "That is interesting to do.",
            "Try to do Bayesian inference for large scale systems, and we're also seeing that at the conference, but I'll try to make a point here again and also.",
            "What are the problems when we use for example variational Bayes or we try to be even one patients then go to?"
        ],
        [
            "OK, next slide please.",
            "So I'll talk a little bit about the Netflix price.",
            "I'll talk about very recently about some trends about solutions.",
            "I'm not an expert on that at all.",
            "Then I will talk about.",
            "But we think is the right kind of likelihood function to use in the Netflix problem.",
            "I'm pretty sure other people have thought about this as well, and then I will talk about some approximate inference methods to use and then in the end just to keep you awake.",
            "I will also give the number on our performance so."
        ],
        [
            "Yes, OK please.",
            "In the next slide.",
            "OK, so Netflix prices.",
            "Probably a lot of you know it already that we have.",
            "We have a task of predicting how people will rank movies right?",
            "So we have an ordinal regression problem.",
            "So we have discrete classes and ordinal means that they are not just any class.",
            "They have an ordering.",
            "So I mean five is the best score you can give a movie and one is the lowest and we have in this problem we have.",
            "I did less than 20,000 movies and around half a million different users, right?",
            "So so each user has maybe maybe ranked some movies and we want to then asked in this qualifying set to come up with a prediction.",
            "On new a new entry in this huge matrix, it's a very sparsely populated matrix, so it's only like 1 1/2% of all the possible entries in this 20,000 by half a million matrix that have values.",
            "So we want to infer some of these missing value."
        ],
        [
            "OK, next slide please.",
            "So basically and there are two trends and that is near neighbor nearest neighborhood based methods like for example K nearest neighbor right?",
            "So if we have if we have some way to define the distance between rankings we could we could find the K nearest neighbor of of specific user and then we could.",
            "We could use the mean for example of the predictions from all the other from the K nearest neighbors.",
            "And then there's the other trend maintenance.",
            "That's also.",
            "I mean, there's also other things than these two, but these are things that made all maintenance that is factor factor models, and we have produced regularised factor models becausw we have.",
            "We have very nicely problem in.",
            "Predictions are very, very uncertain, right?",
            "So we have to be careful with regularization and then I think a very important point is that one model does not describe the whole story.",
            "So we have to make.",
            "Intelligent ways of combining different models.",
            "And I think the current best I'll come back to that loop in the end used combinations of these two classes."
        ],
        [
            "Connect slide please.",
            "OK so our motivation is that first of all we have patients so we like to do a basic solution of course, but our.",
            "I think also there's good points for why being based in because we know something about this problem and we know it's very noisy and we know that probably they are very complex structures in the data, so we would like to use a pretty complicated model, but will kind of careful realisation.",
            "And I think that patient approach.",
            "Of course the best for that.",
            "So first of all I mean.",
            "What is kind of the likelihood model?",
            "What is what is a model that takes an input H and then it produces an probability for one of the ranks, right?",
            "That is what we what we what we should use is kind of the first ingredient in a probabilistic model and many people have used.",
            "I mean if you just use a factor model, a linear factor model, then we have then effectively we model we model this.",
            "Likelihood with caution, but that is of course not completely right, because these are discrete probabilities.",
            "So we.",
            "So we should use ordinal regression.",
            "I mean and that's already been done in the machine learning community by Susan.",
            "And his student.",
            "And so the idea here is that we have these.",
            "These are error functions, right?",
            "So in the.",
            "In the case where this Sigma goes to 0, this every step functions that that's what I've tried to indicate here.",
            "So this is H is our model.",
            "So our model can be affective model, coaching process, whatever and then we have these P sub R which is kind of the limits.",
            "So in this case for example if we if we have rank.",
            "We have we have this one.",
            "Here is 4 for ranking two right so we have put some limits which is minus 6 and minus two.",
            "Just some choice right?",
            "And this indicate what is the probability you can see here if we if we increase Sigma, we get closer and closer to like a Goshen likelihood, at least for the kind of sender.",
            "Send ranks for the for the once 15 we get a different function, right?",
            "So this is what we should have in our model.",
            "Is this is a reasonable likelihood and then we can start thinking about how should we model this H."
        ],
        [
            "That is, on the next slide.",
            "So the idea is to use effective model or S with D or whatever it's called here so so so we have a number of parameters in this model.",
            "They are U and these so you is kind of the factors.",
            "4 Four specific movie.",
            "For moving M&DN is effective for for user in.",
            "So you can see and then we have a latent space here of dimension key.",
            "So that is the model and then we have bacon.",
            "About this we should say OK we should put some prior distributions on these these variables.",
            "And and.",
            "Then you know, turn the bench and handle.",
            "OK, so let's let's say we want to do variational Bayes.",
            "So what is not So what we have a problem as it stands now, because we have we have now a bilinear form inside this nonlinearity, right?",
            "We don't like that.",
            "But the way we can get around that is by introducing this variable H as a latent variable.",
            "And simply right?",
            "Right decompose this noise in this.",
            "Ordinal regression model into two terms and write it as an integral like this.",
            "This is an exact relation and can see now I have H stochastic variable so I can introduce.",
            "I can introduce that as a normal latent variable in like in a mixture model.",
            "And then write a variation distribution on this form.",
            "So right we have the H is here and then we have EU and the V and we write.",
            "We can write this decomposition here so we can see if we assume it factorizes completely over, or these ages, and I think that's necessary to get tractability.",
            "The choice for the for the.",
            "Choice for the prior on this U&V can be quite flexibel.",
            "I mean we can have Goshen weaken Laplace and we can treat we can treat it with practical model.",
            "I mean we can we can we can have higher parameters on these priors and we can also deal with them with their recent base.",
            "Yes please.",
            "Actually you started.",
            "You can speak now.",
            "Yeah OK, that's a good point, but you can see I mean this is actually exact, right?",
            "So I still have.",
            "I still have my havadis conditional distribution here.",
            "It has this nice this variance here, right?",
            "So basically I borrow some of the uncertainty.",
            "From from from this likelihood, right?",
            "So if this Sigma would be zero, OK, then it's deterministic.",
            "But now if it's greater than zero, I can sort of speak, borrow some of the variants.",
            "Some of the uncertainty from from my likelihood function.",
            "Are you convinced?",
            "And you variable H~ which is the deterministic variable plus some noise.",
            "Would have happened before you went through with this position.",
            "OK, yeah we can.",
            "OK, I can see your point.",
            "Yes OK, yeah OK yeah I can see that.",
            "OK, there's two different definitions, OK?"
        ],
        [
            "Yes\nPlease.",
            "OK, so.",
            "Many of you can do this.",
            "If I wake you up 5:00 o'clock in the morning, we can write down the solution.",
            "What is it I don't have?",
            "I don't have put my Clock on European time, so but it's actually a good time now for Europeans.",
            "OK, so if we just do it, make it simple herself, we just choose normal distributed, completely factorized normal distributions, priors on EU and the V. Then we and we do.",
            "10 minutes.",
            "And then we do a free form optimization, right?",
            "So we kind of find the optimal form.",
            "Of the distributions, then we get the following algorithmic recipe.",
            "Or rather, this is kind of.",
            "I mean, we know from variational approach optimization that we can kind of.",
            "We can take any we can optimize any variable and do it in any order.",
            "It will always converge to a local maximum with marginal likelihood bound.",
            "But here's a way which is kind of nice for doing large scale applications because we don't have to store moments of this latent variable.",
            "So the idea is now the solution is that we run over all movies and for each movie then we update.",
            "We update this.",
            "The variational distribution associated with this latent variable, and we only have to of course have to do that with all with the with all the users that have actually seen this movie right.",
            "So in this this gives this form.",
            "I mean this is the result of the freeform optimization.",
            "So we have we have basically the prior.",
            "Sorry, not the prior.",
            "The likelihood term is left and then we multiply by a normal term.",
            "And we can actually, as we know those of us who is done GP classification.",
            "We know we can actually calculate the normalize of this.",
            "So that means we can also get all the moments of this.",
            "And this is what we need.",
            "The next step here.",
            "We have two.",
            "Here we have OK, so I've also yeah chosen completely factorized, and then I get this update form here for the for the, for the variational distribution of a few.",
            "So that is not surprisingly also a normal distribution.",
            "So it has this expression for the mean, and then that has this variance.",
            "And what you can see here is that that that what you need to update this is that you need the the mean values of all these.",
            "And you also need only this H as associate that I calculated up here, right?",
            "OK. And this notation means that this is this.",
            "This inner product.",
            "Why subtract the case factor right?",
            "That is the kind of the normal.",
            "The variational base expressions.",
            "John made this point very nicely.",
            "OK, maybe yeah, a point.",
            "I also would like to say again, but John made it very nicely that you can see the problem.",
            "In variational base that we don't get, we ignore uncertainties, right?",
            "So we have the same.",
            "We have the same estimate for the uncertainty as we had in our original.",
            "Likelihood term right?",
            "We only we only give a sensible estimate of the mean value of this distribution."
        ],
        [
            "OK, next slide.",
            "Yes, OK, so this was what I showed in the previous slide.",
            "Was just how I would run over the movies.",
            "And of course I also need to update the user variables and I could do that in completely the same fashion.",
            "So OK, as I said already, the updates of the latent variable is local, so we don't have to store these variables and that is very nice because we actually have 100 million.",
            "Rankings that we have to calculate 1 this value 100 million times in each.",
            "In each sweep over variables.",
            "Right, so we don't have to start them.",
            "That would be a problem if we want we want to do EPL, come back to that.",
            "Another point that are already made is that that no, I didn't say that, but it is related to this uncertainty or this.",
            "This fact of VB that ignores uncertainty is that the symmetry between the two.",
            "The way I decomposed the variance has been broken.",
            "Right?",
            "And that's of course wrong because I mean in the original model there was completed symmetric.",
            "I could put 90% of the uncertainty in this variable, and 10% of this, or vice versa.",
            "It should give the same result, right?",
            "But it doesn't.",
            "OK, then I should say this.",
            "I mean the full effect arises just one choice and very convenient because you can see I don't have to do any matrix inversions or anything, but I could also take take these for each factor to take each of these to be a multivariate normal.",
            "A normal distribution and then I have to invert AK by K matrix in each update, right?",
            "So that would give an increase in complexity of K ^2.",
            "So that will probably make the algorithm too slow.",
            "With computers we have today, right?",
            "So we have to do a fully factorized in this case."
        ],
        [
            "OK please next slide.",
            "So how do we do predictions now we have?",
            "Let's say we have run now the BP algorithm.",
            "We have converged to some some values of the the Q and these two Q distributions and we want to make a prediction and what we want to minimize in the in the in the objective function in the Netflix prices is the root mean squared error.",
            "So that means that's kind of we know from from decision theory that we should shoot our best estimate.",
            "Is the posterior mean distribution right?",
            "So let's try to calculate that.",
            "So that means we have to to sum over all possibilities for R and then have the probability of our given H. Then the probability of H is written in this form and then we have two.",
            "We have to integrate here over.",
            "Over the variation of distributions.",
            "So we cannot do that becausw because basically we have this bilinear form coming from the factor model and what we can.",
            "But we can do instead is simply to say that we.",
            "Ignore this and we plug in our variational solution for H here and then we can actually calculate this so that we also again P in terms of these error functions result in terms of their functions."
        ],
        [
            "Connect slide please 5 minutes.",
            "OK, so how does it look?",
            "Let's say now that I have a different H. The mean of H is mean is.",
            "Is just the inner product between these two two factors?",
            "So if I have have a small uncertainty then then my my my is a function of H My predicted my mean ranking will be like it more like a step function this and if I put a value which is realistic in what we did in our simulation in where we got the best performance we get something more like this.",
            "So you can see it's more like.",
            "Doing, at least for the predictions, is more like doing.",
            "Doing a linear factor model with kind of a soft clipping at the edges right?",
            "So I mean what?",
            "At least what we have found so far suggests that that the Netflix problem a good parameter setting for that is more in this ballpark than in this much more noise noise, less."
        ],
        [
            "Firemint OK next slide please.",
            "OK, can we do something better?",
            "And this is also what actually it connects to.",
            "What John talked about and I don't know if this is really an excellent solution.",
            "But Italy at least kind of kind of kind of.",
            "I'm have a background in physics, so we like to kind of, you know, do a little bit of handwaving and thinking about.",
            "How how things might look if we add up many numbers and then we like to use the central limit theorem right?",
            "That says if we add up many random variables then they would probably many times converge to a normal distribution.",
            "So The thing is that.",
            "That we can say that this product is bilinear form if these two set of variables are approximately independent, then then then it might converge.",
            "This is a sum here OK, and if K is large and none of these are really dominant, then this might converge to normal distribution with this mean and then with this variance.",
            "So if we do this approximation, then of course we can do this integral over the bilinear form and what it boils down to is that we instead of just having the variance in the model, we now get an additional variance contribution coming exactly from what we want as patients from the uncertainty in our estimates of our factors, right?",
            "This is kind of what we would like to do when we do paging.",
            "Analysis Take all into uncertainty into account.",
            "Then they can of course visit problem that John mentioned with Bimodality.",
            "So maybe this approximation will breakdown.",
            "I haven't really tested that so thoroughly.",
            "OK, so how will this look if we take a fully factorized model?",
            "It has a nice intuitive form it has.",
            "Basically so it's it's case linear with K. There's of course nice because of factorizations.",
            "You can see its first as a product of the variances and then we have we have terms which couples the means to the variance of the other factor.",
            "So you can see if the variances disappear, then we actually get then this disappears and then we get back to RVP solution.",
            "So that's kind of an at least nice that we can in one limit we get back."
        ],
        [
            "To the VP OK. Next slide please.",
            "Yes, OK. How much time do I have left?",
            "OK, now I'm moving more into the shaky ground because I haven't really implemented any EP anything.",
            "I only begin thinking about it.",
            "So in EP, as we already heard it.",
            "It's the point is that we want to have consistency between some sufficient statistics.",
            "So in this case we will choose these in the Goshen family, and then we want to have consistency on the mean and the variance between two different distributions.",
            "One where we include.",
            "One observation, and then we exclude this.",
            "Corresponding variable in the in this normal factor, right?",
            "But you can see already now this and then you want to solve these equations to get this a medz.",
            "But you can see that all there is a problem.",
            "We have lots of variables here."
        ],
        [
            "Next slide, please.",
            "So OK, that's also another problem that this form here is exactly this bilinear form again, so it's not tractable so, but we can.",
            "Maybe if it works, use this central limit approximation.",
            "The other problem, which is perhaps worse is that we have too many variables, right?",
            "We have really we had on the order of the number of rankings and then times K. And we cannot do that.",
            "I mean VP here is nice.",
            "We only have have.",
            "Have two times this number here, right?",
            "Because we haven't been in a variance for each factor."
        ],
        [
            "Next slide please.",
            "OK, So what could we do?",
            "We could try to simplify EP and.",
            "The first round of EP is where we haven't seen any of the examples before then we don't have to do this correction.",
            "This dividing out the old old guesses so that we could simply do that.",
            "We have this, we get new rank, we just take them in sequentially and then we calculate the moments of this.",
            "Distribution right and then we use these new moments to match the moments of these factors and then we take a new one in and do that again and again.",
            "Of course it's not enough in Netflix to run one run iteration, so we have two somewhere come up with a method to remove the information we have already put in, so that is that maybe can be done by some kind of linear response kind of idea, OK?"
        ],
        [
            "Hope one yeah.",
            "OK, so now I've come to almost the end.",
            "The numbers I promised you, so we're kind of new into this game, so I think we can improve on these numbers.",
            "I hope so, at least.",
            "So if we had take.",
            "Take a K20 and we play a little bit around with the different parameters of the priors and so on.",
            "We get down to this number, which I think is an OK number.",
            "I mean the best so far is much better.",
            "This is a this is 8.5% improvement on the on the Netflix own system.",
            "So that means that these guys are 1 1/2% off.",
            "Being able to claim the $1,000,000.",
            "Something funky has.",
            "Pros that kind of linear factor model and has come down to this.",
            "He uses a little bit special type of regularization which is not which is not.",
            "Is a little bit.",
            "Yeah well it's I don't fully understand it, but apparently it works.",
            "Purely linear linear factor models, as far as I've seen in the literature, is a little bit worse than this, so I think it suggests a little bit at least that we win something by having a better likelihood function."
        ],
        [
            "OK, next slide last slide.",
            "So what will what we're going to do next is to go on with our lower rank decompositions and use hierarchical priors.",
            "An in general, better price.",
            "We've learned something about that at the.",
            "And it's a little bit complicated here because people haven't seen the same movies and so on.",
            "So.",
            "So that's kind of.",
            "You can put intelligence into that.",
            "And of course also you need to solve subproblems because the GP problem cannot be solved with 100 million problems.",
            "So you also have to find an intelligent way to divide your, divide your tasks and then model.",
            "Averaging is important.",
            "Maybe better approximate inference is important is also important.",
            "I think.",
            "Maybe maybe DP is good enough for me right now.",
            "OK, thank you.",
            "John story.",
            "Are you making approximation where we factorize the distribution right?",
            "Then there's also that every distribution separately doesn't handle the uncertainty very well and I'm just wondering how?",
            "How much do you win with this method over just basically treating it as a parameter or map estimate information?",
            "And if you then do a little bit of my friends sampling like your authorization, you get these numbers, they can show it as well.",
            "Yeah, so I just wanted.",
            "Variational framework, I mean, you could also turn around and see how much do I lose because it I mean the updates are not not much more complicated than doing just normal updates, so maybe I only win 1% of 2%.",
            "I don't know.",
            "But I mean the question is also whether what I loose is due to a bad approximation.",
            "Yeah, I mean a bad, uh, I mean.",
            "Approximate inference method or.",
            "What you wanna know?",
            "I don't.",
            "I just look for it there.",
            "But I mean there's I mean this complicated because there's both approximate inference and other things coming too, right?",
            "Mean different things.",
            "Yeah.",
            "Yeah.",
            "Construction.",
            "Can you?",
            "Yes I can put.",
            "I mean, we also have have kind of biases, so we can.",
            "We can move the world.",
            "I mean like, I mean, some users are always very negative, right?",
            "But basically maybe 2 users have the same rankings beside one is shifted by one and then you can model by having a bias for each user.",
            "And that's also what we are going to.",
            "We didn't have that in a model now, but that's kind of trivial to put in."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a joint work with Woolridge and placed Samsung from Cambridge so my mind the kind of approximate inference I'm going to present here will not be so advanced.",
                    "label": 0
                },
                {
                    "sent": "It would be mostly variational Bayes.",
                    "label": 0
                },
                {
                    "sent": "I think the message I want to try to convey is that it's.",
                    "label": 0
                },
                {
                    "sent": "That is interesting to do.",
                    "label": 0
                },
                {
                    "sent": "Try to do Bayesian inference for large scale systems, and we're also seeing that at the conference, but I'll try to make a point here again and also.",
                    "label": 1
                },
                {
                    "sent": "What are the problems when we use for example variational Bayes or we try to be even one patients then go to?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next slide please.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk a little bit about the Netflix price.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about very recently about some trends about solutions.",
                    "label": 1
                },
                {
                    "sent": "I'm not an expert on that at all.",
                    "label": 0
                },
                {
                    "sent": "Then I will talk about.",
                    "label": 0
                },
                {
                    "sent": "But we think is the right kind of likelihood function to use in the Netflix problem.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure other people have thought about this as well, and then I will talk about some approximate inference methods to use and then in the end just to keep you awake.",
                    "label": 1
                },
                {
                    "sent": "I will also give the number on our performance so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, OK please.",
                    "label": 0
                },
                {
                    "sent": "In the next slide.",
                    "label": 0
                },
                {
                    "sent": "OK, so Netflix prices.",
                    "label": 0
                },
                {
                    "sent": "Probably a lot of you know it already that we have.",
                    "label": 0
                },
                {
                    "sent": "We have a task of predicting how people will rank movies right?",
                    "label": 0
                },
                {
                    "sent": "So we have an ordinal regression problem.",
                    "label": 0
                },
                {
                    "sent": "So we have discrete classes and ordinal means that they are not just any class.",
                    "label": 0
                },
                {
                    "sent": "They have an ordering.",
                    "label": 0
                },
                {
                    "sent": "So I mean five is the best score you can give a movie and one is the lowest and we have in this problem we have.",
                    "label": 0
                },
                {
                    "sent": "I did less than 20,000 movies and around half a million different users, right?",
                    "label": 0
                },
                {
                    "sent": "So so each user has maybe maybe ranked some movies and we want to then asked in this qualifying set to come up with a prediction.",
                    "label": 0
                },
                {
                    "sent": "On new a new entry in this huge matrix, it's a very sparsely populated matrix, so it's only like 1 1/2% of all the possible entries in this 20,000 by half a million matrix that have values.",
                    "label": 0
                },
                {
                    "sent": "So we want to infer some of these missing value.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next slide please.",
                    "label": 0
                },
                {
                    "sent": "So basically and there are two trends and that is near neighbor nearest neighborhood based methods like for example K nearest neighbor right?",
                    "label": 1
                },
                {
                    "sent": "So if we have if we have some way to define the distance between rankings we could we could find the K nearest neighbor of of specific user and then we could.",
                    "label": 0
                },
                {
                    "sent": "We could use the mean for example of the predictions from all the other from the K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And then there's the other trend maintenance.",
                    "label": 0
                },
                {
                    "sent": "That's also.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's also other things than these two, but these are things that made all maintenance that is factor factor models, and we have produced regularised factor models becausw we have.",
                    "label": 0
                },
                {
                    "sent": "We have very nicely problem in.",
                    "label": 0
                },
                {
                    "sent": "Predictions are very, very uncertain, right?",
                    "label": 0
                },
                {
                    "sent": "So we have to be careful with regularization and then I think a very important point is that one model does not describe the whole story.",
                    "label": 0
                },
                {
                    "sent": "So we have to make.",
                    "label": 0
                },
                {
                    "sent": "Intelligent ways of combining different models.",
                    "label": 0
                },
                {
                    "sent": "And I think the current best I'll come back to that loop in the end used combinations of these two classes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Connect slide please.",
                    "label": 0
                },
                {
                    "sent": "OK so our motivation is that first of all we have patients so we like to do a basic solution of course, but our.",
                    "label": 0
                },
                {
                    "sent": "I think also there's good points for why being based in because we know something about this problem and we know it's very noisy and we know that probably they are very complex structures in the data, so we would like to use a pretty complicated model, but will kind of careful realisation.",
                    "label": 0
                },
                {
                    "sent": "And I think that patient approach.",
                    "label": 0
                },
                {
                    "sent": "Of course the best for that.",
                    "label": 0
                },
                {
                    "sent": "So first of all I mean.",
                    "label": 0
                },
                {
                    "sent": "What is kind of the likelihood model?",
                    "label": 1
                },
                {
                    "sent": "What is what is a model that takes an input H and then it produces an probability for one of the ranks, right?",
                    "label": 0
                },
                {
                    "sent": "That is what we what we what we should use is kind of the first ingredient in a probabilistic model and many people have used.",
                    "label": 1
                },
                {
                    "sent": "I mean if you just use a factor model, a linear factor model, then we have then effectively we model we model this.",
                    "label": 0
                },
                {
                    "sent": "Likelihood with caution, but that is of course not completely right, because these are discrete probabilities.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "So we should use ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "I mean and that's already been done in the machine learning community by Susan.",
                    "label": 0
                },
                {
                    "sent": "And his student.",
                    "label": 0
                },
                {
                    "sent": "And so the idea here is that we have these.",
                    "label": 0
                },
                {
                    "sent": "These are error functions, right?",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                },
                {
                    "sent": "In the case where this Sigma goes to 0, this every step functions that that's what I've tried to indicate here.",
                    "label": 0
                },
                {
                    "sent": "So this is H is our model.",
                    "label": 0
                },
                {
                    "sent": "So our model can be affective model, coaching process, whatever and then we have these P sub R which is kind of the limits.",
                    "label": 0
                },
                {
                    "sent": "So in this case for example if we if we have rank.",
                    "label": 0
                },
                {
                    "sent": "We have we have this one.",
                    "label": 0
                },
                {
                    "sent": "Here is 4 for ranking two right so we have put some limits which is minus 6 and minus two.",
                    "label": 0
                },
                {
                    "sent": "Just some choice right?",
                    "label": 0
                },
                {
                    "sent": "And this indicate what is the probability you can see here if we if we increase Sigma, we get closer and closer to like a Goshen likelihood, at least for the kind of sender.",
                    "label": 0
                },
                {
                    "sent": "Send ranks for the for the once 15 we get a different function, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what we should have in our model.",
                    "label": 0
                },
                {
                    "sent": "Is this is a reasonable likelihood and then we can start thinking about how should we model this H.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, on the next slide.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to use effective model or S with D or whatever it's called here so so so we have a number of parameters in this model.",
                    "label": 0
                },
                {
                    "sent": "They are U and these so you is kind of the factors.",
                    "label": 0
                },
                {
                    "sent": "4 Four specific movie.",
                    "label": 0
                },
                {
                    "sent": "For moving M&DN is effective for for user in.",
                    "label": 0
                },
                {
                    "sent": "So you can see and then we have a latent space here of dimension key.",
                    "label": 0
                },
                {
                    "sent": "So that is the model and then we have bacon.",
                    "label": 0
                },
                {
                    "sent": "About this we should say OK we should put some prior distributions on these these variables.",
                    "label": 0
                },
                {
                    "sent": "And and.",
                    "label": 0
                },
                {
                    "sent": "Then you know, turn the bench and handle.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's say we want to do variational Bayes.",
                    "label": 0
                },
                {
                    "sent": "So what is not So what we have a problem as it stands now, because we have we have now a bilinear form inside this nonlinearity, right?",
                    "label": 0
                },
                {
                    "sent": "We don't like that.",
                    "label": 0
                },
                {
                    "sent": "But the way we can get around that is by introducing this variable H as a latent variable.",
                    "label": 0
                },
                {
                    "sent": "And simply right?",
                    "label": 0
                },
                {
                    "sent": "Right decompose this noise in this.",
                    "label": 0
                },
                {
                    "sent": "Ordinal regression model into two terms and write it as an integral like this.",
                    "label": 0
                },
                {
                    "sent": "This is an exact relation and can see now I have H stochastic variable so I can introduce.",
                    "label": 0
                },
                {
                    "sent": "I can introduce that as a normal latent variable in like in a mixture model.",
                    "label": 0
                },
                {
                    "sent": "And then write a variation distribution on this form.",
                    "label": 0
                },
                {
                    "sent": "So right we have the H is here and then we have EU and the V and we write.",
                    "label": 0
                },
                {
                    "sent": "We can write this decomposition here so we can see if we assume it factorizes completely over, or these ages, and I think that's necessary to get tractability.",
                    "label": 0
                },
                {
                    "sent": "The choice for the for the.",
                    "label": 0
                },
                {
                    "sent": "Choice for the prior on this U&V can be quite flexibel.",
                    "label": 0
                },
                {
                    "sent": "I mean we can have Goshen weaken Laplace and we can treat we can treat it with practical model.",
                    "label": 0
                },
                {
                    "sent": "I mean we can we can we can have higher parameters on these priors and we can also deal with them with their recent base.",
                    "label": 0
                },
                {
                    "sent": "Yes please.",
                    "label": 0
                },
                {
                    "sent": "Actually you started.",
                    "label": 0
                },
                {
                    "sent": "You can speak now.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, that's a good point, but you can see I mean this is actually exact, right?",
                    "label": 0
                },
                {
                    "sent": "So I still have.",
                    "label": 0
                },
                {
                    "sent": "I still have my havadis conditional distribution here.",
                    "label": 0
                },
                {
                    "sent": "It has this nice this variance here, right?",
                    "label": 0
                },
                {
                    "sent": "So basically I borrow some of the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "From from from this likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So if this Sigma would be zero, OK, then it's deterministic.",
                    "label": 0
                },
                {
                    "sent": "But now if it's greater than zero, I can sort of speak, borrow some of the variants.",
                    "label": 0
                },
                {
                    "sent": "Some of the uncertainty from from my likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Are you convinced?",
                    "label": 0
                },
                {
                    "sent": "And you variable H~ which is the deterministic variable plus some noise.",
                    "label": 0
                },
                {
                    "sent": "Would have happened before you went through with this position.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah we can.",
                    "label": 0
                },
                {
                    "sent": "OK, I can see your point.",
                    "label": 0
                },
                {
                    "sent": "Yes OK, yeah OK yeah I can see that.",
                    "label": 0
                },
                {
                    "sent": "OK, there's two different definitions, OK?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes\nPlease.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Many of you can do this.",
                    "label": 0
                },
                {
                    "sent": "If I wake you up 5:00 o'clock in the morning, we can write down the solution.",
                    "label": 0
                },
                {
                    "sent": "What is it I don't have?",
                    "label": 0
                },
                {
                    "sent": "I don't have put my Clock on European time, so but it's actually a good time now for Europeans.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we just do it, make it simple herself, we just choose normal distributed, completely factorized normal distributions, priors on EU and the V. Then we and we do.",
                    "label": 0
                },
                {
                    "sent": "10 minutes.",
                    "label": 0
                },
                {
                    "sent": "And then we do a free form optimization, right?",
                    "label": 1
                },
                {
                    "sent": "So we kind of find the optimal form.",
                    "label": 0
                },
                {
                    "sent": "Of the distributions, then we get the following algorithmic recipe.",
                    "label": 0
                },
                {
                    "sent": "Or rather, this is kind of.",
                    "label": 0
                },
                {
                    "sent": "I mean, we know from variational approach optimization that we can kind of.",
                    "label": 0
                },
                {
                    "sent": "We can take any we can optimize any variable and do it in any order.",
                    "label": 0
                },
                {
                    "sent": "It will always converge to a local maximum with marginal likelihood bound.",
                    "label": 0
                },
                {
                    "sent": "But here's a way which is kind of nice for doing large scale applications because we don't have to store moments of this latent variable.",
                    "label": 0
                },
                {
                    "sent": "So the idea is now the solution is that we run over all movies and for each movie then we update.",
                    "label": 1
                },
                {
                    "sent": "We update this.",
                    "label": 0
                },
                {
                    "sent": "The variational distribution associated with this latent variable, and we only have to of course have to do that with all with the with all the users that have actually seen this movie right.",
                    "label": 0
                },
                {
                    "sent": "So in this this gives this form.",
                    "label": 0
                },
                {
                    "sent": "I mean this is the result of the freeform optimization.",
                    "label": 0
                },
                {
                    "sent": "So we have we have basically the prior.",
                    "label": 0
                },
                {
                    "sent": "Sorry, not the prior.",
                    "label": 0
                },
                {
                    "sent": "The likelihood term is left and then we multiply by a normal term.",
                    "label": 0
                },
                {
                    "sent": "And we can actually, as we know those of us who is done GP classification.",
                    "label": 0
                },
                {
                    "sent": "We know we can actually calculate the normalize of this.",
                    "label": 0
                },
                {
                    "sent": "So that means we can also get all the moments of this.",
                    "label": 0
                },
                {
                    "sent": "And this is what we need.",
                    "label": 0
                },
                {
                    "sent": "The next step here.",
                    "label": 0
                },
                {
                    "sent": "We have two.",
                    "label": 0
                },
                {
                    "sent": "Here we have OK, so I've also yeah chosen completely factorized, and then I get this update form here for the for the, for the variational distribution of a few.",
                    "label": 0
                },
                {
                    "sent": "So that is not surprisingly also a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So it has this expression for the mean, and then that has this variance.",
                    "label": 0
                },
                {
                    "sent": "And what you can see here is that that that what you need to update this is that you need the the mean values of all these.",
                    "label": 0
                },
                {
                    "sent": "And you also need only this H as associate that I calculated up here, right?",
                    "label": 0
                },
                {
                    "sent": "OK. And this notation means that this is this.",
                    "label": 0
                },
                {
                    "sent": "This inner product.",
                    "label": 0
                },
                {
                    "sent": "Why subtract the case factor right?",
                    "label": 0
                },
                {
                    "sent": "That is the kind of the normal.",
                    "label": 0
                },
                {
                    "sent": "The variational base expressions.",
                    "label": 0
                },
                {
                    "sent": "John made this point very nicely.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe yeah, a point.",
                    "label": 0
                },
                {
                    "sent": "I also would like to say again, but John made it very nicely that you can see the problem.",
                    "label": 0
                },
                {
                    "sent": "In variational base that we don't get, we ignore uncertainties, right?",
                    "label": 0
                },
                {
                    "sent": "So we have the same.",
                    "label": 0
                },
                {
                    "sent": "We have the same estimate for the uncertainty as we had in our original.",
                    "label": 0
                },
                {
                    "sent": "Likelihood term right?",
                    "label": 0
                },
                {
                    "sent": "We only we only give a sensible estimate of the mean value of this distribution.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next slide.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so this was what I showed in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Was just how I would run over the movies.",
                    "label": 1
                },
                {
                    "sent": "And of course I also need to update the user variables and I could do that in completely the same fashion.",
                    "label": 1
                },
                {
                    "sent": "So OK, as I said already, the updates of the latent variable is local, so we don't have to store these variables and that is very nice because we actually have 100 million.",
                    "label": 0
                },
                {
                    "sent": "Rankings that we have to calculate 1 this value 100 million times in each.",
                    "label": 0
                },
                {
                    "sent": "In each sweep over variables.",
                    "label": 0
                },
                {
                    "sent": "Right, so we don't have to start them.",
                    "label": 0
                },
                {
                    "sent": "That would be a problem if we want we want to do EPL, come back to that.",
                    "label": 1
                },
                {
                    "sent": "Another point that are already made is that that no, I didn't say that, but it is related to this uncertainty or this.",
                    "label": 0
                },
                {
                    "sent": "This fact of VB that ignores uncertainty is that the symmetry between the two.",
                    "label": 0
                },
                {
                    "sent": "The way I decomposed the variance has been broken.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And that's of course wrong because I mean in the original model there was completed symmetric.",
                    "label": 0
                },
                {
                    "sent": "I could put 90% of the uncertainty in this variable, and 10% of this, or vice versa.",
                    "label": 0
                },
                {
                    "sent": "It should give the same result, right?",
                    "label": 0
                },
                {
                    "sent": "But it doesn't.",
                    "label": 0
                },
                {
                    "sent": "OK, then I should say this.",
                    "label": 0
                },
                {
                    "sent": "I mean the full effect arises just one choice and very convenient because you can see I don't have to do any matrix inversions or anything, but I could also take take these for each factor to take each of these to be a multivariate normal.",
                    "label": 0
                },
                {
                    "sent": "A normal distribution and then I have to invert AK by K matrix in each update, right?",
                    "label": 0
                },
                {
                    "sent": "So that would give an increase in complexity of K ^2.",
                    "label": 0
                },
                {
                    "sent": "So that will probably make the algorithm too slow.",
                    "label": 0
                },
                {
                    "sent": "With computers we have today, right?",
                    "label": 0
                },
                {
                    "sent": "So we have to do a fully factorized in this case.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK please next slide.",
                    "label": 0
                },
                {
                    "sent": "So how do we do predictions now we have?",
                    "label": 0
                },
                {
                    "sent": "Let's say we have run now the BP algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have converged to some some values of the the Q and these two Q distributions and we want to make a prediction and what we want to minimize in the in the in the objective function in the Netflix prices is the root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "So that means that's kind of we know from from decision theory that we should shoot our best estimate.",
                    "label": 0
                },
                {
                    "sent": "Is the posterior mean distribution right?",
                    "label": 0
                },
                {
                    "sent": "So let's try to calculate that.",
                    "label": 0
                },
                {
                    "sent": "So that means we have to to sum over all possibilities for R and then have the probability of our given H. Then the probability of H is written in this form and then we have two.",
                    "label": 0
                },
                {
                    "sent": "We have to integrate here over.",
                    "label": 0
                },
                {
                    "sent": "Over the variation of distributions.",
                    "label": 0
                },
                {
                    "sent": "So we cannot do that becausw because basically we have this bilinear form coming from the factor model and what we can.",
                    "label": 0
                },
                {
                    "sent": "But we can do instead is simply to say that we.",
                    "label": 0
                },
                {
                    "sent": "Ignore this and we plug in our variational solution for H here and then we can actually calculate this so that we also again P in terms of these error functions result in terms of their functions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Connect slide please 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so how does it look?",
                    "label": 0
                },
                {
                    "sent": "Let's say now that I have a different H. The mean of H is mean is.",
                    "label": 0
                },
                {
                    "sent": "Is just the inner product between these two two factors?",
                    "label": 0
                },
                {
                    "sent": "So if I have have a small uncertainty then then my my my is a function of H My predicted my mean ranking will be like it more like a step function this and if I put a value which is realistic in what we did in our simulation in where we got the best performance we get something more like this.",
                    "label": 0
                },
                {
                    "sent": "So you can see it's more like.",
                    "label": 0
                },
                {
                    "sent": "Doing, at least for the predictions, is more like doing.",
                    "label": 0
                },
                {
                    "sent": "Doing a linear factor model with kind of a soft clipping at the edges right?",
                    "label": 0
                },
                {
                    "sent": "So I mean what?",
                    "label": 0
                },
                {
                    "sent": "At least what we have found so far suggests that that the Netflix problem a good parameter setting for that is more in this ballpark than in this much more noise noise, less.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Firemint OK next slide please.",
                    "label": 0
                },
                {
                    "sent": "OK, can we do something better?",
                    "label": 0
                },
                {
                    "sent": "And this is also what actually it connects to.",
                    "label": 0
                },
                {
                    "sent": "What John talked about and I don't know if this is really an excellent solution.",
                    "label": 0
                },
                {
                    "sent": "But Italy at least kind of kind of kind of.",
                    "label": 0
                },
                {
                    "sent": "I'm have a background in physics, so we like to kind of, you know, do a little bit of handwaving and thinking about.",
                    "label": 0
                },
                {
                    "sent": "How how things might look if we add up many numbers and then we like to use the central limit theorem right?",
                    "label": 1
                },
                {
                    "sent": "That says if we add up many random variables then they would probably many times converge to a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So The thing is that.",
                    "label": 0
                },
                {
                    "sent": "That we can say that this product is bilinear form if these two set of variables are approximately independent, then then then it might converge.",
                    "label": 0
                },
                {
                    "sent": "This is a sum here OK, and if K is large and none of these are really dominant, then this might converge to normal distribution with this mean and then with this variance.",
                    "label": 0
                },
                {
                    "sent": "So if we do this approximation, then of course we can do this integral over the bilinear form and what it boils down to is that we instead of just having the variance in the model, we now get an additional variance contribution coming exactly from what we want as patients from the uncertainty in our estimates of our factors, right?",
                    "label": 0
                },
                {
                    "sent": "This is kind of what we would like to do when we do paging.",
                    "label": 0
                },
                {
                    "sent": "Analysis Take all into uncertainty into account.",
                    "label": 0
                },
                {
                    "sent": "Then they can of course visit problem that John mentioned with Bimodality.",
                    "label": 0
                },
                {
                    "sent": "So maybe this approximation will breakdown.",
                    "label": 0
                },
                {
                    "sent": "I haven't really tested that so thoroughly.",
                    "label": 1
                },
                {
                    "sent": "OK, so how will this look if we take a fully factorized model?",
                    "label": 0
                },
                {
                    "sent": "It has a nice intuitive form it has.",
                    "label": 0
                },
                {
                    "sent": "Basically so it's it's case linear with K. There's of course nice because of factorizations.",
                    "label": 0
                },
                {
                    "sent": "You can see its first as a product of the variances and then we have we have terms which couples the means to the variance of the other factor.",
                    "label": 1
                },
                {
                    "sent": "So you can see if the variances disappear, then we actually get then this disappears and then we get back to RVP solution.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of an at least nice that we can in one limit we get back.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the VP OK. Next slide please.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK. How much time do I have left?",
                    "label": 0
                },
                {
                    "sent": "OK, now I'm moving more into the shaky ground because I haven't really implemented any EP anything.",
                    "label": 0
                },
                {
                    "sent": "I only begin thinking about it.",
                    "label": 0
                },
                {
                    "sent": "So in EP, as we already heard it.",
                    "label": 0
                },
                {
                    "sent": "It's the point is that we want to have consistency between some sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "So in this case we will choose these in the Goshen family, and then we want to have consistency on the mean and the variance between two different distributions.",
                    "label": 0
                },
                {
                    "sent": "One where we include.",
                    "label": 0
                },
                {
                    "sent": "One observation, and then we exclude this.",
                    "label": 0
                },
                {
                    "sent": "Corresponding variable in the in this normal factor, right?",
                    "label": 0
                },
                {
                    "sent": "But you can see already now this and then you want to solve these equations to get this a medz.",
                    "label": 0
                },
                {
                    "sent": "But you can see that all there is a problem.",
                    "label": 0
                },
                {
                    "sent": "We have lots of variables here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next slide, please.",
                    "label": 0
                },
                {
                    "sent": "So OK, that's also another problem that this form here is exactly this bilinear form again, so it's not tractable so, but we can.",
                    "label": 0
                },
                {
                    "sent": "Maybe if it works, use this central limit approximation.",
                    "label": 0
                },
                {
                    "sent": "The other problem, which is perhaps worse is that we have too many variables, right?",
                    "label": 1
                },
                {
                    "sent": "We have really we had on the order of the number of rankings and then times K. And we cannot do that.",
                    "label": 0
                },
                {
                    "sent": "I mean VP here is nice.",
                    "label": 0
                },
                {
                    "sent": "We only have have.",
                    "label": 0
                },
                {
                    "sent": "Have two times this number here, right?",
                    "label": 0
                },
                {
                    "sent": "Because we haven't been in a variance for each factor.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next slide please.",
                    "label": 0
                },
                {
                    "sent": "OK, So what could we do?",
                    "label": 0
                },
                {
                    "sent": "We could try to simplify EP and.",
                    "label": 0
                },
                {
                    "sent": "The first round of EP is where we haven't seen any of the examples before then we don't have to do this correction.",
                    "label": 1
                },
                {
                    "sent": "This dividing out the old old guesses so that we could simply do that.",
                    "label": 0
                },
                {
                    "sent": "We have this, we get new rank, we just take them in sequentially and then we calculate the moments of this.",
                    "label": 0
                },
                {
                    "sent": "Distribution right and then we use these new moments to match the moments of these factors and then we take a new one in and do that again and again.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not enough in Netflix to run one run iteration, so we have two somewhere come up with a method to remove the information we have already put in, so that is that maybe can be done by some kind of linear response kind of idea, OK?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hope one yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I've come to almost the end.",
                    "label": 0
                },
                {
                    "sent": "The numbers I promised you, so we're kind of new into this game, so I think we can improve on these numbers.",
                    "label": 0
                },
                {
                    "sent": "I hope so, at least.",
                    "label": 0
                },
                {
                    "sent": "So if we had take.",
                    "label": 0
                },
                {
                    "sent": "Take a K20 and we play a little bit around with the different parameters of the priors and so on.",
                    "label": 0
                },
                {
                    "sent": "We get down to this number, which I think is an OK number.",
                    "label": 0
                },
                {
                    "sent": "I mean the best so far is much better.",
                    "label": 0
                },
                {
                    "sent": "This is a this is 8.5% improvement on the on the Netflix own system.",
                    "label": 0
                },
                {
                    "sent": "So that means that these guys are 1 1/2% off.",
                    "label": 0
                },
                {
                    "sent": "Being able to claim the $1,000,000.",
                    "label": 0
                },
                {
                    "sent": "Something funky has.",
                    "label": 0
                },
                {
                    "sent": "Pros that kind of linear factor model and has come down to this.",
                    "label": 0
                },
                {
                    "sent": "He uses a little bit special type of regularization which is not which is not.",
                    "label": 0
                },
                {
                    "sent": "Is a little bit.",
                    "label": 0
                },
                {
                    "sent": "Yeah well it's I don't fully understand it, but apparently it works.",
                    "label": 0
                },
                {
                    "sent": "Purely linear linear factor models, as far as I've seen in the literature, is a little bit worse than this, so I think it suggests a little bit at least that we win something by having a better likelihood function.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next slide last slide.",
                    "label": 0
                },
                {
                    "sent": "So what will what we're going to do next is to go on with our lower rank decompositions and use hierarchical priors.",
                    "label": 0
                },
                {
                    "sent": "An in general, better price.",
                    "label": 1
                },
                {
                    "sent": "We've learned something about that at the.",
                    "label": 0
                },
                {
                    "sent": "And it's a little bit complicated here because people haven't seen the same movies and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of.",
                    "label": 0
                },
                {
                    "sent": "You can put intelligence into that.",
                    "label": 0
                },
                {
                    "sent": "And of course also you need to solve subproblems because the GP problem cannot be solved with 100 million problems.",
                    "label": 0
                },
                {
                    "sent": "So you also have to find an intelligent way to divide your, divide your tasks and then model.",
                    "label": 0
                },
                {
                    "sent": "Averaging is important.",
                    "label": 1
                },
                {
                    "sent": "Maybe better approximate inference is important is also important.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe DP is good enough for me right now.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "John story.",
                    "label": 0
                },
                {
                    "sent": "Are you making approximation where we factorize the distribution right?",
                    "label": 0
                },
                {
                    "sent": "Then there's also that every distribution separately doesn't handle the uncertainty very well and I'm just wondering how?",
                    "label": 0
                },
                {
                    "sent": "How much do you win with this method over just basically treating it as a parameter or map estimate information?",
                    "label": 0
                },
                {
                    "sent": "And if you then do a little bit of my friends sampling like your authorization, you get these numbers, they can show it as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I just wanted.",
                    "label": 0
                },
                {
                    "sent": "Variational framework, I mean, you could also turn around and see how much do I lose because it I mean the updates are not not much more complicated than doing just normal updates, so maybe I only win 1% of 2%.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "But I mean the question is also whether what I loose is due to a bad approximation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean a bad, uh, I mean.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference method or.",
                    "label": 0
                },
                {
                    "sent": "What you wanna know?",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                },
                {
                    "sent": "I just look for it there.",
                    "label": 0
                },
                {
                    "sent": "But I mean there's I mean this complicated because there's both approximate inference and other things coming too, right?",
                    "label": 0
                },
                {
                    "sent": "Mean different things.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Construction.",
                    "label": 0
                },
                {
                    "sent": "Can you?",
                    "label": 0
                },
                {
                    "sent": "Yes I can put.",
                    "label": 0
                },
                {
                    "sent": "I mean, we also have have kind of biases, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can move the world.",
                    "label": 0
                },
                {
                    "sent": "I mean like, I mean, some users are always very negative, right?",
                    "label": 0
                },
                {
                    "sent": "But basically maybe 2 users have the same rankings beside one is shifted by one and then you can model by having a bias for each user.",
                    "label": 0
                },
                {
                    "sent": "And that's also what we are going to.",
                    "label": 0
                },
                {
                    "sent": "We didn't have that in a model now, but that's kind of trivial to put in.",
                    "label": 0
                }
            ]
        }
    }
}