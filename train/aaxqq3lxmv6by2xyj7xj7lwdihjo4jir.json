{
    "id": "aaxqq3lxmv6by2xyj7xj7lwdihjo4jir",
    "title": "Disorder Inequality: A Combinatorial Approach to Nearest Neighbor Search",
    "info": {
        "author": [
            "Yury Lifshits, Yahoo! Research"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Web Search",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm08_lifshits_dic/",
    "segmentation": [
        [
            "OK, so this is also a paper about coping this cope, so according to Hector it's in the middle of the priorities."
        ],
        [
            "Not on the top, but still OK so nearest neighbors.",
            "That's the classical problem.",
            "As an algorithmic problem, we are given this set of objects, say pictures, and we need to preprocess them to put them to some data structure in such a way that."
        ],
        [
            "If given and you object a query, we need to determine the most similar one in the data set.",
            "I think you can do the job for this example."
        ],
        [
            "I. OK, here's the answer."
        ],
        [
            "OK, so I think that nearest neighbors is probably one of the most important algorithmic problem related to the web.",
            "So an abstract level is just as I said, given set of objects and similarity function or sometimes distance function preprocess them such a way that you're quickly find quickly means faster than in linear time.",
            "The most similar object."
        ],
        [
            "In our paper, we presented a new approach to the problem, which is basically an you assumption about data, so nobody believes that there is a general, very fast algorithm for any domain.",
            "Any similarity function, so you need to make some assumptions.",
            "And so essentially what we did.",
            "We suggested in your assumption.",
            "Then we present some new algorithms, some very preliminary experiments, and.",
            "Also, there is a website where we collected all known so far about nearest neighbors.",
            "So sounds like bibliography and tutorial."
        ],
        [
            "And things like that.",
            "OK, so."
        ],
        [
            "Let's go to some."
        ],
        [
            "Activation so you can write reformulate all these problems as a part of.",
            "Particular form of nearest neighbors like recommendations given all the movies, process them in such a way that given the person you recommend the most relevant movie to him and say ad targeting given a web page to find the most relevant advertisement to put along with this page."
        ],
        [
            "So that was done some previous previous work on nearest neighbors.",
            "So many algorithms how to choose one?",
            "But there is some problems if you try to apply actually any of that to web related applications."
        ],
        [
            "And the problem is the following.",
            "They all do some assumptions and the most basic assumption is triangle inequality.",
            "Usually it is assumed that we have distance function.",
            "We have triangle inequality and some more recent assumption was about.",
            "Dealing with datasets with small doubling dimension, which is some form of entry."
        ],
        [
            "Six dimension at unfortunately, for many datasets we have this effect.",
            "We have the data where all distances are close to maximal possible.",
            "Our let's take an example.",
            "Let's take say a Facebook, and let's define the distance based on how many joint friends we have.",
            "If you take any two people, the intersection of joint friends would be around 10 percent 20%.",
            "So 80% they are different, so the distance is more than 1/2 for any 2.",
            "If the distance is more than 1/2 of maximal, the triangle inequality is useless.",
            "You cannot do any inference based on two distances.",
            "You cannot say anything you about third distance.",
            "If you cannot say anything you about third distance, you cannot prune search.",
            "If you use some branch and bound method ALG."
        ],
        [
            "And also the doubling dimension of any datasets that has this effect also is like close to maximal possible."
        ],
        [
            "OK, so.",
            "Now let's go to the Material Framework 2 new approach."
        ],
        [
            "So first of all, instead of doing our problem easier to make some assumption, we do our problem even much harder.",
            "That's a strange step.",
            "So instead of.",
            "Assuming something new about data, we start to forget information we have.",
            "So instead of talking that we have similarities or distances, we say that we only know the order similarity order order across the points.",
            "So assume you have people and so given any particular person, say mark, we can order all the people here with by their similarity to mark.",
            "So the most similar one, the second most similar 1/3 most similar one, and so on.",
            "So we don't know actual similarity values, we just know the order.",
            "And so.",
            "Formalize that mathematically, so given any three people, we can ask who is more similar to Mark, say, Andre or or some, say myself, and the answer would be, say, Andre is more similar, and so on.",
            "So we can have only compare it."
        ],
        [
            "Information.",
            "I OK. OK.",
            "So OK, now I need one.",
            "Notation notation is rank function, so if P is marked so we can order all everybody here by similarity to mark and so they are is somebody who has the fourth position.",
            "So the rank P with respect to P from R is equal to four and the rank of mark with him with respect to himself is zero.",
            "He is the most similar person to himself, so this is just a notation.",
            "And now I'm ready to introduce an you assumption about data, which I hope holds to some extent to real web data and can replace like triangle inequality, and then can be a basis of new algorithms."
        ],
        [
            "OK, here is it.",
            "So we would like to have something like triangle inequality for ranks.",
            "So we're looking for two people, and they say ranks with respect to 3rd one and then we would like to have an upper bound on 3rd rank based on two previous."
        ],
        [
            "Thanks.",
            "So here it is formally.",
            "So we would like that third rank so it's weak triangle inequality.",
            "We would like that third rank would be bounded by some constant D. Times some of two other ranks.",
            "Something like we take the 10th nearest most similar person to Mark and 20th most similar person to Mark and we would like that.",
            "One of them with respect to another one, would be at most 30 * D. And D is a description parameter of the data set.",
            "So if the particular data set this holds for every triples with small constant D, this is easy data set.",
            "Nice data set if we need big constant.",
            "To satisfy that that that's a hard data set."
        ],
        [
            "So now I hope you have a lot of questions, so try to OK.",
            "So this is the new, just the.",
            "The new approach on a single slide, so we have a comparison Oracle, just comparative information and we assume disorder inequality with some disorder constant D. So."
        ],
        [
            "That probably the questions you have in mind now.",
            "Let me try to answer.",
            "So.",
            "Disorder of metric space, so there is no clear relation.",
            "There might be a metric, so the similarity is satisfy triangle inequality by the disorder constant is really large.",
            "There might be the opposite case, so there is no triangle inequality.",
            "But disorder constant is still small if you take the uniformly distributed data set in Euclidean space of dimension K, the disorder constant would be 2 to the K. You can think this is bad, but essentially it's not as bad as it looks like becausw.",
            "Assume you have million dimensional space and all points are lying alone small dimensional manifold.",
            "Then the disorder constant would be essentially proportional to.",
            "It would be exponential, but it would be exponential in actual intrinsic dimension dimension of this manifold, not of the underlying big space.",
            "So one particular.",
            "Caseware disorder is small.",
            "Is there is another notion called growth rate or sometimes cultural dimension so.",
            "Growth rate is essentially you take a point, say again, mark sorry, and then you take the similar some similarity distance value.",
            "So you take all people, say 5 kilometers from Mark and then you take 10 kilometers from Mark and look how much more people do have.",
            "If you take 10 kilometers.",
            "This is growth rate and there is a very short proof that if we have a.",
            "A data set with growth rate C, then disorder constant is at most C square.",
            "So every data set that has small growth rate has small disorder.",
            "Are now experiments shows that this other constant D is actually very small in average.",
            "So for most of triples this constant D needed to satisfy the inequality is small, but for some triples it is large so.",
            "We study in this paper mostly this idealistic model, where it is satisfied for all triples but.",
            "In practice, this is broken for some small fraction of cases.",
            "Oak."
        ],
        [
            "So some advantages, so we don't have triangle inequality, so we can work when separation effect is holding.",
            "So when everybody is far away from each other, it still can be true that this order is small and this comparison Oracle, and this combinatorial view that we only use the comparative information can be very useful because it can handle any data model.",
            "So if you look, say, advertisement matching, then we can have so many factors to have like history, web search history and visiting history and.",
            "Previous click through rate and text all data and time data and hyperlinks and so on and so it's difficult to come up with clean mathematical model.",
            "What is the description of the point?",
            "It's not like vector in Euclidean space.",
            "It's not like cosine similarity is something very like.",
            "Integrat like combine some.",
            "Integration over all factors.",
            "And here we carry only about comparative information, so it makes much easier.",
            "To study.",
            "And the last thing is that it is really self adapting.",
            "So if we take the 5 kilometer.",
            "Ball around mark and five kilometer ball around myself we can get very different population of number of people, but if we take combinatorial balls, communitarian ball is that when you take instead of distance you take just number of friends so they can material ball with radius 100 is simply hundreds of most similar points to me.",
            "And then I take this communal bowl about my surround myself around Mark and it's self adapting to local density of the data set that can be used somehow."
        ],
        [
            "OK, and the current main limitation is the we use worst case definition, so we assume that the disorder inequality holds for all triples, while in practice it is true only in average."
        ],
        [
            "That's further work.",
            "Yeah, so as I said, if expansion rate is limited then disorder constant is limited and with doubling dimension it is incomparable.",
            "So sometimes one is small enough is big, sometimes other way around.",
            "But there is some relation between them."
        ],
        [
            "OK, so let's go to new algorithms."
        ],
        [
            "So we tried random walk so random walk is you start from some object, say from some.",
            "So assume you match advertisement to a particular web page.",
            "So you try any advertisement and then you look around so you this advertisement.",
            "Let's say it's P1 and it's linked to four other advertisements.",
            "So you think OK, maybe one is not the most relevant to the webpage queue, so you would like to change it so you look at this four and which one you prefer, you prefer the one that is.",
            "Closer to you, right?"
        ],
        [
            "So you move there.",
            "Then you look what are the ads that were.",
            "In process."
        ],
        [
            "But I'm connected to P2.",
            "That's another choice.",
            "So you try all of them and you can do it combinatorially.",
            "So you need not actual distances, you just can compare this distance."
        ],
        [
            "You go the one that is closest to you."
        ],
        [
            "And so on."
        ],
        [
            "And so finally you, after some number of steps.",
            "At hopefully log arhythmic number of steps, you just report the last point."
        ],
        [
            "So this is very, very similar how we navigate through transportation networks in real life.",
            "So in order to come here to Wisdom conference, you start from some city.",
            "Say I started from."
        ],
        [
            "Los Angeles and then you look for Air Airlines and you choose the one that is bring me as close as possible.",
            "So I choose either San Jose or San Francisco and go there."
        ],
        [
            "Then I probably look OK.",
            "Sometimes I look for railway routes.",
            "OK, there is no OK. Then I maybe take Caltrain.",
            "OK, OK, I think."
        ],
        [
            "AT and I take it to this top, which is the closest one, and then I look for bus routes, right so and so every next time I choose the next scale, so a little bit smaller scale."
        ],
        [
            "And so.",
            "Then we in theoretical work.",
            "So if we have an objects we have log and scales."
        ],
        [
            "And So what we suggest is now the last line is about the actual algorithm.",
            "Is that for level K, we say that the scale is N / 2 to the K neighborhood.",
            "So let me rephrase this algorithm once again.",
            "I take a object and then I take the end neighborhood.",
            "That means all objects and make.",
            "Random walk, so it's random.",
            "So I make a small sample so I take say 100.",
            "Objects in the whole data set an move to the one that is close to Q.",
            "This is P2.",
            "Now I take P2 and take not the full data set, but just the half of most similar ones to be 2.",
            "Again, take a sample 100 ones.",
            "Choose the one that is close to Q, move there.",
            "This is my P3.",
            "Then I take us neighborhood of N / 4 and divided by four more similar ones to be 3.",
            "Take a sample of 100.",
            "Compare all these hundred to Q.",
            "Choose the best one.",
            "This is before then I take N / 16 and so on.",
            "And divided by 8 sorry and so on.",
            "So every time you just make the scale twice smaller, sample it, take the best one and move and that's there."
        ],
        [
            "So that's the same thing formally.",
            "So in order to have approval guarantees, we need this sampling, which is disorder constant times log log N. And then we can prove that with very high probability the rank function of our current point goes twice smaller every time.",
            "This sampling is enough to guarantee that with very high probability.",
            "Our rank is quite smaller every time."
        ],
        [
            "So here is an analysis.",
            "So unfortunately for this algorithm the processing time is space is quadratic because we need to compute ranks in advance because we need this neighborhoods in advance and the query time is really pretty fast, so it's almost D log in.",
            "So if this order constant is small, then the search time is really very fast.",
            "Are we have?"
        ],
        [
            "Creation of it where we do all random choices in advance.",
            "So instead of doing random sampling, we choose in advance and connect every point with.",
            "Few points on the big neighborhood, few points of the small neighborhood and few points in the tiny neighborhood.",
            "And so on.",
            "And then we can.",
            "Have much smaller preprocessing space like ND log in so it's not an square anymore, but still square preprocessing and.",
            "The same.",
            "Query time."
        ],
        [
            "Uh, we also did some experiment on Reuters collection of news articles using cosine similarity measure and we just take a point, take second point, Third Point, and compute the third rank divided by first and second rank and turns out that most of the cases.",
            "It's like even smaller than one.",
            "This ratio, the third rank is smaller than these two and but for some fraction it is really much larger, sometimes hundred 100 times much larger than A&B.",
            "So there are exceptions."
        ],
        [
            "Directions for."
        ],
        [
            "Research, so first of all I would like to advertise.",
            "The next result.",
            "So we already have a mixed result, which essentially improved.",
            "The largest drawback of the previous work.",
            "So in the previous work it was quadratic time.",
            "So now we have a different algorithm that it is something like D2 this seven, so it's big polynomial from D. But still polynomial anlog square ends.",
            "So we need not quadratic preprocessing and it is deterministic.",
            "Again, with the same model disorder inequality and it's tied to the four log in search time.",
            "And we also have for this Community real model algorithms for near duplicates, and this is also.",
            "Deterministic algorithms, so things like min hashing.",
            "They are inherently randomized.",
            "You make fingerprints out of documents and then compare two documents by fingerprints.",
            "And if you assume that this model is true, you have this other inequality, then you can find all pairs of near duplicates, especially in time which has two components.",
            "First is near linear and 2nd is proportional to output.",
            "Deterministically, and that's like.",
            "What you would like to have?"
        ],
        [
            "So directions for further research.",
            "So it's interesting to continue this studies within this model, like there are many, many problems in algorithm theory that starting like given N points in some metric space, do something.",
            "Now you can say given endpoints and described by comparison Oracle and assuming this order inequality is true, do something.",
            "Like you do the same with clustering with clustering things like K medians or K means you're trying to minimize the.",
            "Intercluster distances now you can try to minimize intercluster ranks.",
            "Like try to search the to cluster points in such a way that if you put somebody in the cluster you would try to put his best friend, second best friend, third best friend within the cluster and 1000 best friend outside of the cluster.",
            "Things like that.",
            "So it's interesting.",
            "How can we address except exceptions?",
            "Also important to deal with the dynamic questions, and of course, further implementations and experiments.",
            "And I think that this particular story about replacing distances by ranks is just the very first example where you start from some metric or some similarity function and trying to improve this.",
            "So you're trying to replace the existing metric by some new metric.",
            "That has some nicer property, so it has the same ordering ranks.",
            "Order the data set exactly the same as the initial distance did, but now you have much nicer distribution because you have exactly one point with rank one.",
            "Exactly one point with rank 2 from you exactly one point with rank three of you, so it's very nice distribute.",
            "It's not like everybody is equally far from you.",
            "Not like sometimes you haven't there datasets."
        ],
        [
            "OK, so there are three papers on nearest neighbors you can look my."
        ],
        [
            "Page."
        ],
        [
            "And so I presented the Dischord inequality and comparison Oracle's in your approach to nearest neighbors knew random walk algorithm and some further work.",
            "Thank you.",
            "Yep.",
            "Try to set up some some real sense.",
            "Let's let's it honestly why it's pretty small.",
            "As."
        ],
        [
            "The.",
            "One is that so here essentially you see we need this sampling which is 3D log log N. So, and this is real three, there is no another constant in front of it.",
            "Yep.",
            "Lots of huge you'll see saying.",
            "Flights.",
            "I'm sorry.",
            "We need more experiments so.",
            "Right now we have a couple of experiments, one Reuters collection, another one run independently on some biological data.",
            "It turns out that for most triples, the third rank is like times two or three more than two other ranks, so the D is like 2345, so it's really small.",
            "But it has exceptions and the question is whether what is the.",
            "How exceptions?",
            "Affect the performance of algorithm.",
            "At this should be studied experimentally.",
            "And that's the next step of research.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is also a paper about coping this cope, so according to Hector it's in the middle of the priorities.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not on the top, but still OK so nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "That's the classical problem.",
                    "label": 1
                },
                {
                    "sent": "As an algorithmic problem, we are given this set of objects, say pictures, and we need to preprocess them to put them to some data structure in such a way that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If given and you object a query, we need to determine the most similar one in the data set.",
                    "label": 0
                },
                {
                    "sent": "I think you can do the job for this example.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I. OK, here's the answer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I think that nearest neighbors is probably one of the most important algorithmic problem related to the web.",
                    "label": 1
                },
                {
                    "sent": "So an abstract level is just as I said, given set of objects and similarity function or sometimes distance function preprocess them such a way that you're quickly find quickly means faster than in linear time.",
                    "label": 0
                },
                {
                    "sent": "The most similar object.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our paper, we presented a new approach to the problem, which is basically an you assumption about data, so nobody believes that there is a general, very fast algorithm for any domain.",
                    "label": 1
                },
                {
                    "sent": "Any similarity function, so you need to make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "And so essentially what we did.",
                    "label": 0
                },
                {
                    "sent": "We suggested in your assumption.",
                    "label": 1
                },
                {
                    "sent": "Then we present some new algorithms, some very preliminary experiments, and.",
                    "label": 0
                },
                {
                    "sent": "Also, there is a website where we collected all known so far about nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "So sounds like bibliography and tutorial.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And things like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's go to some.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Activation so you can write reformulate all these problems as a part of.",
                    "label": 0
                },
                {
                    "sent": "Particular form of nearest neighbors like recommendations given all the movies, process them in such a way that given the person you recommend the most relevant movie to him and say ad targeting given a web page to find the most relevant advertisement to put along with this page.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was done some previous previous work on nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "So many algorithms how to choose one?",
                    "label": 0
                },
                {
                    "sent": "But there is some problems if you try to apply actually any of that to web related applications.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the problem is the following.",
                    "label": 0
                },
                {
                    "sent": "They all do some assumptions and the most basic assumption is triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "Usually it is assumed that we have distance function.",
                    "label": 0
                },
                {
                    "sent": "We have triangle inequality and some more recent assumption was about.",
                    "label": 1
                },
                {
                    "sent": "Dealing with datasets with small doubling dimension, which is some form of entry.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Six dimension at unfortunately, for many datasets we have this effect.",
                    "label": 0
                },
                {
                    "sent": "We have the data where all distances are close to maximal possible.",
                    "label": 0
                },
                {
                    "sent": "Our let's take an example.",
                    "label": 0
                },
                {
                    "sent": "Let's take say a Facebook, and let's define the distance based on how many joint friends we have.",
                    "label": 0
                },
                {
                    "sent": "If you take any two people, the intersection of joint friends would be around 10 percent 20%.",
                    "label": 0
                },
                {
                    "sent": "So 80% they are different, so the distance is more than 1/2 for any 2.",
                    "label": 0
                },
                {
                    "sent": "If the distance is more than 1/2 of maximal, the triangle inequality is useless.",
                    "label": 0
                },
                {
                    "sent": "You cannot do any inference based on two distances.",
                    "label": 0
                },
                {
                    "sent": "You cannot say anything you about third distance.",
                    "label": 0
                },
                {
                    "sent": "If you cannot say anything you about third distance, you cannot prune search.",
                    "label": 0
                },
                {
                    "sent": "If you use some branch and bound method ALG.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also the doubling dimension of any datasets that has this effect also is like close to maximal possible.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now let's go to the Material Framework 2 new approach.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, instead of doing our problem easier to make some assumption, we do our problem even much harder.",
                    "label": 0
                },
                {
                    "sent": "That's a strange step.",
                    "label": 0
                },
                {
                    "sent": "So instead of.",
                    "label": 0
                },
                {
                    "sent": "Assuming something new about data, we start to forget information we have.",
                    "label": 0
                },
                {
                    "sent": "So instead of talking that we have similarities or distances, we say that we only know the order similarity order order across the points.",
                    "label": 0
                },
                {
                    "sent": "So assume you have people and so given any particular person, say mark, we can order all the people here with by their similarity to mark.",
                    "label": 0
                },
                {
                    "sent": "So the most similar one, the second most similar 1/3 most similar one, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we don't know actual similarity values, we just know the order.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Formalize that mathematically, so given any three people, we can ask who is more similar to Mark, say, Andre or or some, say myself, and the answer would be, say, Andre is more similar, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we can have only compare it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "I OK. OK.",
                    "label": 0
                },
                {
                    "sent": "So OK, now I need one.",
                    "label": 0
                },
                {
                    "sent": "Notation notation is rank function, so if P is marked so we can order all everybody here by similarity to mark and so they are is somebody who has the fourth position.",
                    "label": 0
                },
                {
                    "sent": "So the rank P with respect to P from R is equal to four and the rank of mark with him with respect to himself is zero.",
                    "label": 0
                },
                {
                    "sent": "He is the most similar person to himself, so this is just a notation.",
                    "label": 0
                },
                {
                    "sent": "And now I'm ready to introduce an you assumption about data, which I hope holds to some extent to real web data and can replace like triangle inequality, and then can be a basis of new algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here is it.",
                    "label": 0
                },
                {
                    "sent": "So we would like to have something like triangle inequality for ranks.",
                    "label": 0
                },
                {
                    "sent": "So we're looking for two people, and they say ranks with respect to 3rd one and then we would like to have an upper bound on 3rd rank based on two previous.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So here it is formally.",
                    "label": 0
                },
                {
                    "sent": "So we would like that third rank so it's weak triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "We would like that third rank would be bounded by some constant D. Times some of two other ranks.",
                    "label": 0
                },
                {
                    "sent": "Something like we take the 10th nearest most similar person to Mark and 20th most similar person to Mark and we would like that.",
                    "label": 0
                },
                {
                    "sent": "One of them with respect to another one, would be at most 30 * D. And D is a description parameter of the data set.",
                    "label": 0
                },
                {
                    "sent": "So if the particular data set this holds for every triples with small constant D, this is easy data set.",
                    "label": 0
                },
                {
                    "sent": "Nice data set if we need big constant.",
                    "label": 0
                },
                {
                    "sent": "To satisfy that that that's a hard data set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I hope you have a lot of questions, so try to OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the new, just the.",
                    "label": 0
                },
                {
                    "sent": "The new approach on a single slide, so we have a comparison Oracle, just comparative information and we assume disorder inequality with some disorder constant D. So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That probably the questions you have in mind now.",
                    "label": 0
                },
                {
                    "sent": "Let me try to answer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Disorder of metric space, so there is no clear relation.",
                    "label": 0
                },
                {
                    "sent": "There might be a metric, so the similarity is satisfy triangle inequality by the disorder constant is really large.",
                    "label": 0
                },
                {
                    "sent": "There might be the opposite case, so there is no triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "But disorder constant is still small if you take the uniformly distributed data set in Euclidean space of dimension K, the disorder constant would be 2 to the K. You can think this is bad, but essentially it's not as bad as it looks like becausw.",
                    "label": 0
                },
                {
                    "sent": "Assume you have million dimensional space and all points are lying alone small dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Then the disorder constant would be essentially proportional to.",
                    "label": 0
                },
                {
                    "sent": "It would be exponential, but it would be exponential in actual intrinsic dimension dimension of this manifold, not of the underlying big space.",
                    "label": 0
                },
                {
                    "sent": "So one particular.",
                    "label": 0
                },
                {
                    "sent": "Caseware disorder is small.",
                    "label": 0
                },
                {
                    "sent": "Is there is another notion called growth rate or sometimes cultural dimension so.",
                    "label": 0
                },
                {
                    "sent": "Growth rate is essentially you take a point, say again, mark sorry, and then you take the similar some similarity distance value.",
                    "label": 0
                },
                {
                    "sent": "So you take all people, say 5 kilometers from Mark and then you take 10 kilometers from Mark and look how much more people do have.",
                    "label": 0
                },
                {
                    "sent": "If you take 10 kilometers.",
                    "label": 0
                },
                {
                    "sent": "This is growth rate and there is a very short proof that if we have a.",
                    "label": 0
                },
                {
                    "sent": "A data set with growth rate C, then disorder constant is at most C square.",
                    "label": 0
                },
                {
                    "sent": "So every data set that has small growth rate has small disorder.",
                    "label": 0
                },
                {
                    "sent": "Are now experiments shows that this other constant D is actually very small in average.",
                    "label": 0
                },
                {
                    "sent": "So for most of triples this constant D needed to satisfy the inequality is small, but for some triples it is large so.",
                    "label": 0
                },
                {
                    "sent": "We study in this paper mostly this idealistic model, where it is satisfied for all triples but.",
                    "label": 0
                },
                {
                    "sent": "In practice, this is broken for some small fraction of cases.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some advantages, so we don't have triangle inequality, so we can work when separation effect is holding.",
                    "label": 0
                },
                {
                    "sent": "So when everybody is far away from each other, it still can be true that this order is small and this comparison Oracle, and this combinatorial view that we only use the comparative information can be very useful because it can handle any data model.",
                    "label": 0
                },
                {
                    "sent": "So if you look, say, advertisement matching, then we can have so many factors to have like history, web search history and visiting history and.",
                    "label": 0
                },
                {
                    "sent": "Previous click through rate and text all data and time data and hyperlinks and so on and so it's difficult to come up with clean mathematical model.",
                    "label": 0
                },
                {
                    "sent": "What is the description of the point?",
                    "label": 0
                },
                {
                    "sent": "It's not like vector in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "It's not like cosine similarity is something very like.",
                    "label": 0
                },
                {
                    "sent": "Integrat like combine some.",
                    "label": 0
                },
                {
                    "sent": "Integration over all factors.",
                    "label": 0
                },
                {
                    "sent": "And here we carry only about comparative information, so it makes much easier.",
                    "label": 0
                },
                {
                    "sent": "To study.",
                    "label": 0
                },
                {
                    "sent": "And the last thing is that it is really self adapting.",
                    "label": 0
                },
                {
                    "sent": "So if we take the 5 kilometer.",
                    "label": 0
                },
                {
                    "sent": "Ball around mark and five kilometer ball around myself we can get very different population of number of people, but if we take combinatorial balls, communitarian ball is that when you take instead of distance you take just number of friends so they can material ball with radius 100 is simply hundreds of most similar points to me.",
                    "label": 0
                },
                {
                    "sent": "And then I take this communal bowl about my surround myself around Mark and it's self adapting to local density of the data set that can be used somehow.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the current main limitation is the we use worst case definition, so we assume that the disorder inequality holds for all triples, while in practice it is true only in average.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's further work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so as I said, if expansion rate is limited then disorder constant is limited and with doubling dimension it is incomparable.",
                    "label": 0
                },
                {
                    "sent": "So sometimes one is small enough is big, sometimes other way around.",
                    "label": 0
                },
                {
                    "sent": "But there is some relation between them.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go to new algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we tried random walk so random walk is you start from some object, say from some.",
                    "label": 0
                },
                {
                    "sent": "So assume you match advertisement to a particular web page.",
                    "label": 0
                },
                {
                    "sent": "So you try any advertisement and then you look around so you this advertisement.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's P1 and it's linked to four other advertisements.",
                    "label": 0
                },
                {
                    "sent": "So you think OK, maybe one is not the most relevant to the webpage queue, so you would like to change it so you look at this four and which one you prefer, you prefer the one that is.",
                    "label": 0
                },
                {
                    "sent": "Closer to you, right?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you move there.",
                    "label": 0
                },
                {
                    "sent": "Then you look what are the ads that were.",
                    "label": 0
                },
                {
                    "sent": "In process.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm connected to P2.",
                    "label": 0
                },
                {
                    "sent": "That's another choice.",
                    "label": 0
                },
                {
                    "sent": "So you try all of them and you can do it combinatorially.",
                    "label": 0
                },
                {
                    "sent": "So you need not actual distances, you just can compare this distance.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go the one that is closest to you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so finally you, after some number of steps.",
                    "label": 0
                },
                {
                    "sent": "At hopefully log arhythmic number of steps, you just report the last point.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is very, very similar how we navigate through transportation networks in real life.",
                    "label": 0
                },
                {
                    "sent": "So in order to come here to Wisdom conference, you start from some city.",
                    "label": 0
                },
                {
                    "sent": "Say I started from.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Los Angeles and then you look for Air Airlines and you choose the one that is bring me as close as possible.",
                    "label": 0
                },
                {
                    "sent": "So I choose either San Jose or San Francisco and go there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I probably look OK.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I look for railway routes.",
                    "label": 0
                },
                {
                    "sent": "OK, there is no OK. Then I maybe take Caltrain.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, I think.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "AT and I take it to this top, which is the closest one, and then I look for bus routes, right so and so every next time I choose the next scale, so a little bit smaller scale.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Then we in theoretical work.",
                    "label": 0
                },
                {
                    "sent": "So if we have an objects we have log and scales.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what we suggest is now the last line is about the actual algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is that for level K, we say that the scale is N / 2 to the K neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So let me rephrase this algorithm once again.",
                    "label": 0
                },
                {
                    "sent": "I take a object and then I take the end neighborhood.",
                    "label": 0
                },
                {
                    "sent": "That means all objects and make.",
                    "label": 0
                },
                {
                    "sent": "Random walk, so it's random.",
                    "label": 0
                },
                {
                    "sent": "So I make a small sample so I take say 100.",
                    "label": 0
                },
                {
                    "sent": "Objects in the whole data set an move to the one that is close to Q.",
                    "label": 0
                },
                {
                    "sent": "This is P2.",
                    "label": 0
                },
                {
                    "sent": "Now I take P2 and take not the full data set, but just the half of most similar ones to be 2.",
                    "label": 0
                },
                {
                    "sent": "Again, take a sample 100 ones.",
                    "label": 0
                },
                {
                    "sent": "Choose the one that is close to Q, move there.",
                    "label": 1
                },
                {
                    "sent": "This is my P3.",
                    "label": 0
                },
                {
                    "sent": "Then I take us neighborhood of N / 4 and divided by four more similar ones to be 3.",
                    "label": 0
                },
                {
                    "sent": "Take a sample of 100.",
                    "label": 0
                },
                {
                    "sent": "Compare all these hundred to Q.",
                    "label": 0
                },
                {
                    "sent": "Choose the best one.",
                    "label": 0
                },
                {
                    "sent": "This is before then I take N / 16 and so on.",
                    "label": 0
                },
                {
                    "sent": "And divided by 8 sorry and so on.",
                    "label": 0
                },
                {
                    "sent": "So every time you just make the scale twice smaller, sample it, take the best one and move and that's there.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the same thing formally.",
                    "label": 0
                },
                {
                    "sent": "So in order to have approval guarantees, we need this sampling, which is disorder constant times log log N. And then we can prove that with very high probability the rank function of our current point goes twice smaller every time.",
                    "label": 0
                },
                {
                    "sent": "This sampling is enough to guarantee that with very high probability.",
                    "label": 0
                },
                {
                    "sent": "Our rank is quite smaller every time.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an analysis.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately for this algorithm the processing time is space is quadratic because we need to compute ranks in advance because we need this neighborhoods in advance and the query time is really pretty fast, so it's almost D log in.",
                    "label": 0
                },
                {
                    "sent": "So if this order constant is small, then the search time is really very fast.",
                    "label": 0
                },
                {
                    "sent": "Are we have?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creation of it where we do all random choices in advance.",
                    "label": 0
                },
                {
                    "sent": "So instead of doing random sampling, we choose in advance and connect every point with.",
                    "label": 0
                },
                {
                    "sent": "Few points on the big neighborhood, few points of the small neighborhood and few points in the tiny neighborhood.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "Have much smaller preprocessing space like ND log in so it's not an square anymore, but still square preprocessing and.",
                    "label": 0
                },
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "Query time.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, we also did some experiment on Reuters collection of news articles using cosine similarity measure and we just take a point, take second point, Third Point, and compute the third rank divided by first and second rank and turns out that most of the cases.",
                    "label": 0
                },
                {
                    "sent": "It's like even smaller than one.",
                    "label": 0
                },
                {
                    "sent": "This ratio, the third rank is smaller than these two and but for some fraction it is really much larger, sometimes hundred 100 times much larger than A&B.",
                    "label": 0
                },
                {
                    "sent": "So there are exceptions.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directions for.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Research, so first of all I would like to advertise.",
                    "label": 0
                },
                {
                    "sent": "The next result.",
                    "label": 0
                },
                {
                    "sent": "So we already have a mixed result, which essentially improved.",
                    "label": 0
                },
                {
                    "sent": "The largest drawback of the previous work.",
                    "label": 0
                },
                {
                    "sent": "So in the previous work it was quadratic time.",
                    "label": 0
                },
                {
                    "sent": "So now we have a different algorithm that it is something like D2 this seven, so it's big polynomial from D. But still polynomial anlog square ends.",
                    "label": 0
                },
                {
                    "sent": "So we need not quadratic preprocessing and it is deterministic.",
                    "label": 0
                },
                {
                    "sent": "Again, with the same model disorder inequality and it's tied to the four log in search time.",
                    "label": 0
                },
                {
                    "sent": "And we also have for this Community real model algorithms for near duplicates, and this is also.",
                    "label": 0
                },
                {
                    "sent": "Deterministic algorithms, so things like min hashing.",
                    "label": 0
                },
                {
                    "sent": "They are inherently randomized.",
                    "label": 0
                },
                {
                    "sent": "You make fingerprints out of documents and then compare two documents by fingerprints.",
                    "label": 0
                },
                {
                    "sent": "And if you assume that this model is true, you have this other inequality, then you can find all pairs of near duplicates, especially in time which has two components.",
                    "label": 0
                },
                {
                    "sent": "First is near linear and 2nd is proportional to output.",
                    "label": 0
                },
                {
                    "sent": "Deterministically, and that's like.",
                    "label": 0
                },
                {
                    "sent": "What you would like to have?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So directions for further research.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting to continue this studies within this model, like there are many, many problems in algorithm theory that starting like given N points in some metric space, do something.",
                    "label": 0
                },
                {
                    "sent": "Now you can say given endpoints and described by comparison Oracle and assuming this order inequality is true, do something.",
                    "label": 0
                },
                {
                    "sent": "Like you do the same with clustering with clustering things like K medians or K means you're trying to minimize the.",
                    "label": 0
                },
                {
                    "sent": "Intercluster distances now you can try to minimize intercluster ranks.",
                    "label": 0
                },
                {
                    "sent": "Like try to search the to cluster points in such a way that if you put somebody in the cluster you would try to put his best friend, second best friend, third best friend within the cluster and 1000 best friend outside of the cluster.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting.",
                    "label": 0
                },
                {
                    "sent": "How can we address except exceptions?",
                    "label": 0
                },
                {
                    "sent": "Also important to deal with the dynamic questions, and of course, further implementations and experiments.",
                    "label": 0
                },
                {
                    "sent": "And I think that this particular story about replacing distances by ranks is just the very first example where you start from some metric or some similarity function and trying to improve this.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to replace the existing metric by some new metric.",
                    "label": 0
                },
                {
                    "sent": "That has some nicer property, so it has the same ordering ranks.",
                    "label": 0
                },
                {
                    "sent": "Order the data set exactly the same as the initial distance did, but now you have much nicer distribution because you have exactly one point with rank one.",
                    "label": 0
                },
                {
                    "sent": "Exactly one point with rank 2 from you exactly one point with rank three of you, so it's very nice distribute.",
                    "label": 0
                },
                {
                    "sent": "It's not like everybody is equally far from you.",
                    "label": 0
                },
                {
                    "sent": "Not like sometimes you haven't there datasets.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there are three papers on nearest neighbors you can look my.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Page.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I presented the Dischord inequality and comparison Oracle's in your approach to nearest neighbors knew random walk algorithm and some further work.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Try to set up some some real sense.",
                    "label": 0
                },
                {
                    "sent": "Let's let's it honestly why it's pretty small.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "One is that so here essentially you see we need this sampling which is 3D log log N. So, and this is real three, there is no another constant in front of it.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Lots of huge you'll see saying.",
                    "label": 0
                },
                {
                    "sent": "Flights.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "We need more experiments so.",
                    "label": 0
                },
                {
                    "sent": "Right now we have a couple of experiments, one Reuters collection, another one run independently on some biological data.",
                    "label": 0
                },
                {
                    "sent": "It turns out that for most triples, the third rank is like times two or three more than two other ranks, so the D is like 2345, so it's really small.",
                    "label": 0
                },
                {
                    "sent": "But it has exceptions and the question is whether what is the.",
                    "label": 0
                },
                {
                    "sent": "How exceptions?",
                    "label": 0
                },
                {
                    "sent": "Affect the performance of algorithm.",
                    "label": 0
                },
                {
                    "sent": "At this should be studied experimentally.",
                    "label": 0
                },
                {
                    "sent": "And that's the next step of research.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}