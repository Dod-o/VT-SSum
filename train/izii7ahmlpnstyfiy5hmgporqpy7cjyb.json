{
    "id": "izii7ahmlpnstyfiy5hmgporqpy7cjyb",
    "title": "Bayesian Modeling of Dependency Trees Using Hierarchical Pitman-Yor Priors",
    "info": {
        "author": [
            "Hanna M. Wallach, Department of Computer Science, University of Massachusetts Amherst"
        ],
        "published": "Aug. 11, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_wallach_bmd/",
    "segmentation": [
        [
            "OK, I'm Hannah.",
            "I'm a postdoc at UMass Amherst working with Andrew McCallum, and today I'm going to be talking about Bayesian models for dependency parsing using Pittman.",
            "Your prize and this is joint work with Charles Sutton and with."
        ],
        [
            "McAllen.",
            "Alright, so I don't know how many of you are familiar with dependency pausing, but the basic idea is that we have a bunch of sentences and we have maybe part of speech tags for the words in the sentence.",
            "What we really like to do is to get more of an idea of the syntax, which what?"
        ],
        [
            "The structure of this sentence is beyond word order, and so this is what a dependency tree looks like.",
            "We've got a sentence, and we've got this tree structure in black up here on this tree."
        ],
        [
            "Structure is actually representing relationship syntactic relationships between the words in the sentence and each node in the tree is a part of speech, tag, word, and an edge from one word to another.",
            "Word means that.",
            "The second word, the one that the edge is going to, is independent of the first word.",
            "In other words, it modifies the first word so.",
            "The nodes in our tree are these part of speech, tag cased words.",
            "We have this tree structure and we're really interested in kind of inferring these tree structures for sentences that we don't have any St tree structure."
        ],
        [
            "For.",
            "So in this talk I'm going to tell you about four models for doing this kind of thing.",
            "The first one is a Bayesian reinterpretation of a classic dependency model.",
            "This is a so if you know about dependency parsing, you will have heard about eisners generative dependency parser, and so we re interpret this in a Bayesian context and then show that having done that, we can extend the model using other priors and his various different other ways.",
            "And then because we're using a generative model, we can include other interesting latent states in the model as well.",
            "So I'm going."
        ],
        [
            "Talk to you about these four models.",
            "Alright, so I'm going to start by talking you through how eyes nose generative dependency model works.",
            "It's a generative model, so it's actually generating the words in the sentence and the tree structure all at the same time.",
            "And the basic idea is that conditioned on apparent word, the children either the left children or the right children each form of 1st order Markov chain and the final child in each direction moving outwards from the parent is going to be a special stop symbol and it's these stop symbols that enable the simultaneous generation of the words in the sentence.",
            "And the tree."
        ],
        [
            "So let's run through a simple example.",
            "So we have this artificial root node.",
            "This kind of OK.",
            "This is the top of the tree this."
        ],
        [
            "We're going to start and conditioned on that.",
            "We decide to generate no left child because we assume that route is the first word in the sentence.",
            "But we're going to generate some right child of fruit, and in this case we've generated the word hit, which has some part of speech tag verb, and then having done that, we're going to take hit and treat hit this apparent and generate some left children, some right children."
        ],
        [
            "So treating his apparent we happened to generate go with the part of speech tag noun and having done that, we say OK. Now let's treat girls apparent."
        ],
        [
            "Generate some children for that.",
            "OK, so we generate the where the part of speech tag of determiner."
        ],
        [
            "And then treating that as apparent we might, we might generate a stop and the stop means no more children on.",
            "In this case, the left hand side of the OK.",
            "So then we generate some right?"
        ],
        [
            "Children for the in this case we happen to generate stop as well, indicating that the is not going to have anymore children at."
        ],
        [
            "And we recurse back up to girl, where we generate a second left child.",
            "In this case stop."
        ],
        [
            "And then a right child."
        ],
        [
            "Also stop.",
            "And then we recurse back up to hit."
        ],
        [
            "Narrate the left stop.",
            "And then generate the right hand side of the tree hanging off of it so."
        ],
        [
            "How do we actually generate one of these tagged cased words?",
            "What we're going to do is we're going to so in this case we're looking at the situation where hit with a part of speech tagger verb is apparent.",
            "We've just generated ball as a right child, and we want to generate some other right child here.",
            "So how do we do that?",
            "Well, we can just draw from some big distribution over all possible words and part of speech tags in case values.",
            "In other words, whether these words are capitalized or not, but it might actually be worth sort of factorizing this distribution and breaking it up."
        ],
        [
            "Little bit, so we're actually going to do this.",
            "We're going to generate a part of speech tag for this word at this position, and we're going to do that by taking into account the part of speech tag, cased word that is the parent and the part of speech tag of the most recently generated sibling on that side of the children for that parent.",
            "So you see, I've grayed out the word ball here because we're not actually going to look at the word there.",
            "We're just going to look at the part of speech tag at the sibling in order to generate the part of speech tag here."
        ],
        [
            "And say we generate preposition.",
            "Now, having done that, we actually want to generate the uncased word at this position, and here what we're going to do is we're actually going to ignore the sibling entirely.",
            "We're just going to look at the part of speech tagged cased parent word, and the tags that we've just generate."
        ],
        [
            "And so in this case we then draw from a distribution over uncased words and we happen to draw width and then having done that we ignore the parent and the sibling entirely and we draw a case value for with.",
            "In this case, well, just it's a lower case word, so we drew that."
        ],
        [
            "So where did these distributions come from, and what Eisner did when he defined his original model was to take a bunch of training data, a bunch of tagged cased sentences and their trees, and account relevant occurrences in that set of training data.",
            "So, for example, if we're looking at case values, whether a word is uppercase, lowercase, mixed capitalization, or the first capitalized word in a sentence, we perhaps look at the number of times that case values see.",
            "So, for example, lower case or something.",
            "Has occurred in the context of tag S and word W, and what that means in sort of perhaps a more natural way of putting it is just simply the number of times that uncased word W with part of speech tag S has case value.",
            "See and what I did was the standard kind of thing that people did back in the mid 90s of counting relevant occurrences in the data.",
            "Using these counts to form estimators.",
            "So coming up with some kind of estimate of the probability of case value C in some context and then actually smoothing these estimators.",
            "With other estimators, and that's really a key point here.",
            "Why do you want to smooth these things well?",
            "So here we're looking at the number of times that case value C has been assigned to a word W with tag S, and then she might be quite a sparse count.",
            "We might not have actually seen that occurrence very many times, So what we want to do is smooth these estimators with estimators that are better defined.",
            "There's just more data for them because the context we're looking at is shorter, so that's what I did.",
            "Same approaches, interpolated engram, language modeling if you're."
        ],
        [
            "With that so one key difference though, is that in interpolated ngram language modeling, it's really obvious what context you want to use.",
            "If you're trying to predict a word at some position in the sentence you look at, maybe the previous three words or something, and you smooth your estimate of the probability for that context with an estimate based on maybe the previous two words, and then one that's best based on like the previous word.",
            "And you do that.",
            "It's really obvious that you take these longer sequences of words and just sort of reduce them to smaller sequences.",
            "By dropping the word that's furthest back in the history, and that's sort of an obvious thing to do, because you think that words that are further back in the sentence, or less likely to affect whatever the current word is that you're predicting.",
            "But in the case of pausing the choice of contexts in which content bit of context you drop, is actually much less obvious.",
            "So in the case of predicting case value C, so like lower case or something, we might be looking at the tag in that position, and the word in that position.",
            "Which one of these do we want to drop?",
            "Do we want to drop the word and just look at the tag, or do we want to drop the tag?",
            "And just look at the word.",
            "So I did a bunch of experiments and chose a bunch of different Contacts in Contacts reductions and I'll talk about those briefly in a second, but throughout the rest of this talk, I'm just going to use the exact same context and context reductions that I say used what he actually chose to do was to."
        ],
        [
            "2.",
            "Take the this is an estimator of the probability of case value C in the context of tag S and word W."
        ],
        [
            "And he smoothes that with an estimator.",
            "That's just the probability of case value C in the context of tag S."
        ],
        [
            "She also smooth with just a uniform probability over the case values, and throughout this I'm using the example of the case value probability because it's the simplest one of the probabilities in this model, but he uses the same approach for all."
        ],
        [
            "All of the other probabilities.",
            "And just to give you an idea of how complicated this kind of thing can get when we're looking at the probability of A tag where conditioning on the parent tag cased word, and we're looking at the sibling tag, and we're looking at the direction that's a lot of conditioning context.",
            "These counts are going to be really, really sparse, and So what we do is we actually what is needed is he actually smooths the counts from that huge context with account that's just based on the number of times that we've seen the tag in the context of the parent tag, the sibling tag.",
            "And the direction and then smooth that with just counts that are based on the number of times that the parent this tag has been seen in the context of the parent tag and the direction.",
            "So we have these really, really big context really sparse contexts, and so that's why smoothing is really important."
        ],
        [
            "So.",
            "Basically, in recent years people got sort of UN excited about generative dependency models because hey, discriminative methods are really cool, everybody likes and blah blah blah.",
            "And I said method sort of got forgotten a bit, but it turns out that you can actually redefine isnos model from a Bayesian perspective and the whole thing actually turns out to be this really nice hierarchical Bayesian model.",
            "It's not the original way that Eisner motivated the model, but it's totally equivalent, and so now what I'm going to do is, I'm going to talk you through how is this model is actually just.",
            "A hierarchical Bayesian model.",
            "So what we're going to do is we're going to treat each probability vector, so the distribution over tags or the distribution over words, or the distribution over case values as a random variable.",
            "So in this case we have a distribution over case values in the context of tag Essen, word W, and we're going to do is we're going to.",
            "We don't know what this distribution is, so we're going to draw it from a delay prior going to give it away prior and I'd originally prior has two parameters, it has a concentration parameter and a base measure, and in this case the base measure that I'm using is A tag specific base measure.",
            "And what the base measure is, it's just sort of like the mean distribution and.",
            "This concentration parameter just sort of indicates how dispersed or how concentrated these distributions over case values for each possible S&W context will be.",
            "From that base measure.",
            "So."
        ],
        [
            "Having defined interracially prior like that, well, what are we going to do about the base measures?",
            "'cause we don't know what those are either?",
            "OK, no problem.",
            "Well defined.",
            "A hierarchical directly prior, so we'll put another darish lay on each one of these tags, specific base measures, and so remember that Eisner, when he's looking at the context to use when predicting case values, he goes from the tag and the word to just the tag to the uniform distribution.",
            "So we're going to try to encode that same thing here in our hierarchical prior.",
            "By drawing on tag specific base measure from Adrish, lay with again some concentration parameter and hear a uniform based measure.",
            "And by putting these priors on the base measures of our original prior inducing a hierarchical darish way prior over our distributions.",
            "Then, having done that, we can integrate out all of these unknown probability vectors and come up with predictive distribution."
        ],
        [
            "And when we do that, what we find is that the predictive distribution over case values looks like this.",
            "Again, we have these like bees.",
            "These counts that are specific to case value C in the context of tag S and word W with smoothing, smoothing those with some counts that are specific to case values C in the context of tag S and then with this uniform.",
            "Probability here and the things to note about this are firstly that this looks really similar to eisners predictive distribution.",
            "Secondly, a point which I'm going to kind of gloss over due to time restrictions.",
            "One thing that's different is that these counts don't actually have to be raw observation counts in the duration world.",
            "These can be like type counts.",
            "So for instance, the number of different unique words that case value C and tag S have been seen in the context of, but I'm not really going to talk about that very much here.",
            "Relevant thing is just that it's worth keeping in mind that these actually in this framework don't have to be the robbs."
        ],
        [
            "Accounts so just to make it totally clear we have ice model over here.",
            "One of these predictive distributions from this model and one of the predictive distributions from the Bayesian model, and they're very, very similar.",
            "The only difference are what higher level counts were using and the values of the concentration parameters."
        ],
        [
            "So that's really interesting.",
            "We've taken ISIS model.",
            "We've reinterpreted it in this Bayesian framework.",
            "Great.",
            "So what can we do now?",
            "Well, there are at least three ways of varying the Bayesian model.",
            "Firstly, we can sample the concentration parameters.",
            "We don't have to set them to three or 1/2 or whatever values Eisner chose.",
            "We also don't have to cross validate them.",
            "We can just simply slice sample them or something like that.",
            "Also, as I was saying briefly before the counts, the higher level counts don't need to correspond to the raw observation counts.",
            "And we can also use priors other than the hierarchical drizly distribution and all of these things have the potential to improve model quality.",
            "I'm going to talk the most about actually using a different prior other than the hierarchical directly distribution."
        ],
        [
            "So I'm going to talk about using hierarchical Pitman your priors.",
            "We're going to do here is, rather than drawing, for instance, our distribution over case values from.",
            "Additionally distribution, we're going to instead draw it from a Pitman Yor process.",
            "Now I'm actually drawing it from a discrete Pitman Yor process, but if I was going to have some kind of like letter based top level prior then I could have been a continuous thing.",
            "But here I'm really going to concentrate on on discrete pit manual processes and the payment your process has the same two parameters as the duration process a concentration parameter.",
            "Database measure, but it also has a discount parameter, so we're going to.",
            "We're going to draw each of our distributions over case values and eat all of our other distributions or distributions over tags and words as well from a Pitman, your distribution, and then we're going to also put Pittman your priors on the base measures.",
            "So one thing to note is that when the discount parameters are equal to 0, the Pitman your distribution is identical to interracially distribution.",
            "So this is just a direct generalization of the distribution that gave rise to ISIS model, and in fact the Pitman Yor process gives distributions that better resemble natural language.",
            "There actually better at capturing the occurrence of really rare words that maybe occur only once or twice."
        ],
        [
            "Um?",
            "So if we actually integrate out based measures and our distributions, we end up with a predictive distribution.",
            "Again, using the example of case values that looks like this and it looks kind of similar to before, except our counts are now.",
            "I've represented them using M. There actually are old counts that we had in the duration of version of the model minus this discount parameters.",
            "Some quantity of this discount parameter.",
            "So what we're doing is we're saying rather than.",
            "Rather than actually using the counts from the model, we're subtracting some discount off of them, so we're sort of we're sort of subtracting this discount off of the account and then adding that to these distributions that we're using to smooth things with."
        ],
        [
            "Soap in your prayers have been used for language modeling with a lot of success.",
            "It turns out that conveys a nice moving, which is one of the best smoothing methods in language modeling is equivalent to setting the Alpha parameters in a pit maneuver prior to zero and using a particular approximate inference scheme that I'll refer to as a minimal path assumption.",
            "So I'm really interested in using these Pitman Yor process is for dependency parsing because unlike many other parsing models, dependency parsing actually is all based on the individual words.",
            "It's fully lexicalized and so you have really, really sparse counts, because like language modeling, you're dealing with these individual words, you're not necessarily just dealing with rules, and so I think that Pittman your prize are particularly appropriate choice for dependency models because they're a generative model for words and language."
        ],
        [
            "I'm so if this were language modeling, we'd be done.",
            "We've specified how to compute the probability of a tree or with a sentence with its corresponding tree, and if this were language modeling, we could just compute the probabilities of our sentence is and do whatever it is that one does with that.",
            "But in fact, that's not what we're interested in, and pausing in the world with dependency parsing were interested in taking real sentences that only have words tags in case values, and we want to infer the dependency trees for those sentences.",
            "So what we're going to do is we're going to use training data to learn the model, and then we're going to sample trees for test sentences were going to do this using a Metropolis Hastings algorithm that users Eisenerz dynamic program as a proposal distribution, and then decides whether to accept or reject proposal trees, and it's really similar to the Metropolis Hastings algorithm that Mark Johnson and others presented for PCF, Geez."
        ],
        [
            "OK so I wanted to compare whether the Pitman Yor prior is actually better for dependency parsing than the hierarchical directly prior.",
            "So we paused the Wall Street Journal sections of the Penn Treebank.",
            "This is the standard parsing data set that everybody uses.",
            "There's training sections sections two through 21.",
            "We've got nearly 40,000 sentences there, and then there's the testing sections, which is 2500 sentences and the way people evaluate these models is to look at parse accuracy.",
            "The percentage of parents that were correctly identified.",
            "And for comparison with other peoples methods, I'm just looking at maximum probability trees here rather than sample trees, but that's just for comparison purposes.",
            "Also, for efficiency, I, although I could infer the part of speech tags using the dependency model, I'm actually going to fix them two tags from a standard part of speech tagger.",
            "And this is just because it's faster, but it gives releases."
        ],
        [
            "The results.",
            "So here are the pause accuracies.",
            "We've got these four sets of graphs here.",
            "These two are one particular approximate inference scheme, and these two are a different approximate inference scheme, and there's not really much difference between the two approximate inference schemes.",
            "The key things to note are so this here is using fixed hyperparameters and this one here is Eisner's original model.",
            "This is entirely equivalent to Eisner's original model.",
            "This is the same setup but with the Pitman your prior, but again fixed hyperparameters and we see that using the Pitman Yor prior gives an improvement of around 3% in accuracy.",
            "Then we also have over here we have with sampled hyperparameters and the difference between the door Ashley and the Pitman Yor when we're actually sampling the hyper hyper parameters is much smaller, but overall the difference between using a Pitman yor prior with sampled hyperparameters and using Adrish lay prior with fixed hyperparameters, which is what equivalent to what I said originally did, is about sort of 4% or so, which corresponds to about a 26% error reduction over Eisner, and that's fairly significant.",
            "So that's really nice.",
            "It shows that firstly.",
            "Pittman, your prize are actually better for dependency parsing in terms of improving cause accuracy and then also we can improve performance further by sampling the hyper parameters of the model."
        ],
        [
            "OK, so.",
            "You might wonder why I'm sort of revisiting this generative dependency model when there's been so much recent success on discriminative methods for dependencies pausing and the reason why is because you can include other interesting latent variables in a generative framework, and so this is.",
            "This is kind of the second bit of the talk.",
            "And here I'm interested in including topics in the sample in the parsing model, and I'm going to look at two different types of topics.",
            "I'm going to look at semantic topics, which are the kind of topics that you guys are probably familiar with.",
            "Their, you know, semantic groups or probability distributions over semantically related words, and I'm also going to look at syntactic topics.",
            "Again, these are specialized distributions over words, but the way these words are related is by syntactic relationships, not semantic relationships, so.",
            "To investigate incorporating these kinds of latent variables into the pausing model, I'm going to use a slightly simpler pausing model.",
            "The real differences between this and the model that that I was just talking about, that the sentences are untagged, an uncased.",
            "We're just looking at lower case words were not going to look at siblings, and we're not going to take siblings into account.",
            "It's just the 1st order model and the distributions over children depends on the parent, and now some kind of latent state variable as well."
        ],
        [
            "So.",
            "Um, I'm actually going to skip over this slide.",
            "This is just talking about first order models being simpler and how they're similar, but it's."
        ],
        [
            "Not entirely relevant to the latent variable stuff, so OK syntactic latent variables.",
            "So here what I've got is I've got a parent and I've got a child and the relationship between them is now mediated by this latent state variable.",
            "I've got this latent state variable that sits between the parent and the child and tries to capture properties of the dependencies that are in these trees."
        ],
        [
            "So now the generative process is as follows.",
            "Generate a state given the parent word.",
            "So generate the state here given hit."
        ],
        [
            "And then having done that, generate a word.",
            "In this case bull given the state and these distributions over states given the parent and distributions over."
        ],
        [
            "Words given the state, we're just going to give the reshape rise as before."
        ],
        [
            "OK, semantic latent variables.",
            "What you have is something like this.",
            "We have again a parent, a child and now the child actually depends on this semantic state.",
            "Here that comes from a document specific distribution over topics."
        ],
        [
            "So again, you generate the state given the document.",
            "This is exactly as in latent richly allocation."
        ],
        [
            "And then given that.",
            "You generate the word based on the parent word and this document specific some."
        ],
        [
            "Text 8 and again we gave everything directly prior."
        ],
        [
            "So when we're looking at the syntactic topics around some experiments comparing the syntactic topic so so one thing about the syntactic topics is that we have no label data for these, or we have the dependency trees, the States themselves.",
            "These are fully unsupervised.",
            "So what I'm doing is I'm taking training data, just training trees, and I'm inferring latent semantic, latent syntactic states from these trees.",
            "Then, having done that, I'm using those.",
            "In Ferd States and the training trees to infer States and trees for test data.",
            "And when I do that, what I see is that if I fix the states to be actual part of speech tags, then I get this performance when I use sample trees I get around 55% accuracy if I use the most probable tree I get around 62 1/2% accuracy.",
            "And when I actually have the model instead learn these unsupervised latent syntactic states, we find that we actually get quite a bit better parsing accuracy.",
            "It's most noticeable for the sample trees then for the maximum probability trees.",
            "But it's definitely a noticeable performance improvement using these automatically inferred fully unsupervised latent states instead of part of speech tags.",
            "And that's kind of nice because.",
            "It's showing that what we are actually inferring is something that is relevant for do."
        ],
        [
            "Independency pausing.",
            "And when we look at the latent states, these are the latent states in Ferd using the training trees, we see really interesting stuff.",
            "They basically look like part of speech tags, but they're much finer grained.",
            "So here we have nouns.",
            "And these nouns are all basically job titles and over here we also have nouns, but these are all place names.",
            "And here we have past tense verbs.",
            "We have words over here like would will could, should, can, might a bunch of numbers over here.",
            "It's really nice.",
            "These actually.",
            "It looks like we're sort of extracting something really interesting that really summarizes what's going on in terms of the dependencies."
        ],
        [
            "So, um.",
            "Yeah, this is my last slide.",
            "So for semantic topics, the topics look very similar to what you would get from LDA, so I'm not going to show those, but what I'm going to do here is unsupervised bits per word, so the fewer the bitspower, the better.",
            "Better the model.",
            "This is a fully unsupervised model, we're using no training trees at all.",
            "We're just going through and sampling our trees in a fully unsupervised fashion and comparing.",
            "Comparing these different models as a baseline, we've got LDA here.",
            "Another baseline we've got just dependencies only, so none of these latent States and then we've got these two latent state models.",
            "We've got the one with syntactic topics, and the one with LDA is semantic topics, and we see that the best of these models, in terms of the bits per word, is using semantic topics along with the dependencies, and so this is promising.",
            "It's sort of.",
            "I mean, this is very much work in progress, but this is promising, because it shows that perhaps we are definitely gaining something in terms of model performance.",
            "By using these unsupervised."
        ],
        [
            "States.",
            "So conclusions we reinterpreted classic dependency parser using a fully Bayesian framework.",
            "We then showed that we can take that framework and improve parsing performance by using Pittman your priors rather than directly priors.",
            "Also, we can sample the hyperparameters, which improves performance too.",
            "And finally, we can incorporate latent variables into the model, either syntactic, stop it topics that cluster parent child relationships, or semantic topics as in LDA and future work, obviously.",
            "Combining syntactic and semantic topics and continuing to take a look at these latent variables in pausing."
        ],
        [
            "Thanks that's it.",
            "It's a very.",
            "The number of tables in your Chinese restaurant process.",
            "So the number of you can you go back, yeah?"
        ],
        [
            "Yep.",
            "So, OK, that's the number of tables in your Chinese restaurant Chinese restaurant for S&W, and this is the number of them that actually have observation, see.",
            "Yeah, I didn't want to get into that, but yeah.",
            "Come again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm Hannah.",
                    "label": 0
                },
                {
                    "sent": "I'm a postdoc at UMass Amherst working with Andrew McCallum, and today I'm going to be talking about Bayesian models for dependency parsing using Pittman.",
                    "label": 1
                },
                {
                    "sent": "Your prize and this is joint work with Charles Sutton and with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "McAllen.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I don't know how many of you are familiar with dependency pausing, but the basic idea is that we have a bunch of sentences and we have maybe part of speech tags for the words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "What we really like to do is to get more of an idea of the syntax, which what?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The structure of this sentence is beyond word order, and so this is what a dependency tree looks like.",
                    "label": 0
                },
                {
                    "sent": "We've got a sentence, and we've got this tree structure in black up here on this tree.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure is actually representing relationship syntactic relationships between the words in the sentence and each node in the tree is a part of speech, tag, word, and an edge from one word to another.",
                    "label": 1
                },
                {
                    "sent": "Word means that.",
                    "label": 0
                },
                {
                    "sent": "The second word, the one that the edge is going to, is independent of the first word.",
                    "label": 0
                },
                {
                    "sent": "In other words, it modifies the first word so.",
                    "label": 1
                },
                {
                    "sent": "The nodes in our tree are these part of speech, tag cased words.",
                    "label": 0
                },
                {
                    "sent": "We have this tree structure and we're really interested in kind of inferring these tree structures for sentences that we don't have any St tree structure.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'm going to tell you about four models for doing this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "The first one is a Bayesian reinterpretation of a classic dependency model.",
                    "label": 1
                },
                {
                    "sent": "This is a so if you know about dependency parsing, you will have heard about eisners generative dependency parser, and so we re interpret this in a Bayesian context and then show that having done that, we can extend the model using other priors and his various different other ways.",
                    "label": 0
                },
                {
                    "sent": "And then because we're using a generative model, we can include other interesting latent states in the model as well.",
                    "label": 0
                },
                {
                    "sent": "So I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk to you about these four models.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to start by talking you through how eyes nose generative dependency model works.",
                    "label": 1
                },
                {
                    "sent": "It's a generative model, so it's actually generating the words in the sentence and the tree structure all at the same time.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is that conditioned on apparent word, the children either the left children or the right children each form of 1st order Markov chain and the final child in each direction moving outwards from the parent is going to be a special stop symbol and it's these stop symbols that enable the simultaneous generation of the words in the sentence.",
                    "label": 1
                },
                {
                    "sent": "And the tree.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's run through a simple example.",
                    "label": 0
                },
                {
                    "sent": "So we have this artificial root node.",
                    "label": 0
                },
                {
                    "sent": "This kind of OK.",
                    "label": 0
                },
                {
                    "sent": "This is the top of the tree this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to start and conditioned on that.",
                    "label": 0
                },
                {
                    "sent": "We decide to generate no left child because we assume that route is the first word in the sentence.",
                    "label": 0
                },
                {
                    "sent": "But we're going to generate some right child of fruit, and in this case we've generated the word hit, which has some part of speech tag verb, and then having done that, we're going to take hit and treat hit this apparent and generate some left children, some right children.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So treating his apparent we happened to generate go with the part of speech tag noun and having done that, we say OK. Now let's treat girls apparent.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generate some children for that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we generate the where the part of speech tag of determiner.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then treating that as apparent we might, we might generate a stop and the stop means no more children on.",
                    "label": 0
                },
                {
                    "sent": "In this case, the left hand side of the OK.",
                    "label": 0
                },
                {
                    "sent": "So then we generate some right?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Children for the in this case we happen to generate stop as well, indicating that the is not going to have anymore children at.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we recurse back up to girl, where we generate a second left child.",
                    "label": 0
                },
                {
                    "sent": "In this case stop.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then a right child.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also stop.",
                    "label": 0
                },
                {
                    "sent": "And then we recurse back up to hit.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Narrate the left stop.",
                    "label": 0
                },
                {
                    "sent": "And then generate the right hand side of the tree hanging off of it so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we actually generate one of these tagged cased words?",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to so in this case we're looking at the situation where hit with a part of speech tagger verb is apparent.",
                    "label": 0
                },
                {
                    "sent": "We've just generated ball as a right child, and we want to generate some other right child here.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, we can just draw from some big distribution over all possible words and part of speech tags in case values.",
                    "label": 0
                },
                {
                    "sent": "In other words, whether these words are capitalized or not, but it might actually be worth sort of factorizing this distribution and breaking it up.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Little bit, so we're actually going to do this.",
                    "label": 0
                },
                {
                    "sent": "We're going to generate a part of speech tag for this word at this position, and we're going to do that by taking into account the part of speech tag, cased word that is the parent and the part of speech tag of the most recently generated sibling on that side of the children for that parent.",
                    "label": 1
                },
                {
                    "sent": "So you see, I've grayed out the word ball here because we're not actually going to look at the word there.",
                    "label": 0
                },
                {
                    "sent": "We're just going to look at the part of speech tag at the sibling in order to generate the part of speech tag here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And say we generate preposition.",
                    "label": 0
                },
                {
                    "sent": "Now, having done that, we actually want to generate the uncased word at this position, and here what we're going to do is we're actually going to ignore the sibling entirely.",
                    "label": 1
                },
                {
                    "sent": "We're just going to look at the part of speech tagged cased parent word, and the tags that we've just generate.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in this case we then draw from a distribution over uncased words and we happen to draw width and then having done that we ignore the parent and the sibling entirely and we draw a case value for with.",
                    "label": 0
                },
                {
                    "sent": "In this case, well, just it's a lower case word, so we drew that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where did these distributions come from, and what Eisner did when he defined his original model was to take a bunch of training data, a bunch of tagged cased sentences and their trees, and account relevant occurrences in that set of training data.",
                    "label": 1
                },
                {
                    "sent": "So, for example, if we're looking at case values, whether a word is uppercase, lowercase, mixed capitalization, or the first capitalized word in a sentence, we perhaps look at the number of times that case values see.",
                    "label": 0
                },
                {
                    "sent": "So, for example, lower case or something.",
                    "label": 0
                },
                {
                    "sent": "Has occurred in the context of tag S and word W, and what that means in sort of perhaps a more natural way of putting it is just simply the number of times that uncased word W with part of speech tag S has case value.",
                    "label": 1
                },
                {
                    "sent": "See and what I did was the standard kind of thing that people did back in the mid 90s of counting relevant occurrences in the data.",
                    "label": 1
                },
                {
                    "sent": "Using these counts to form estimators.",
                    "label": 0
                },
                {
                    "sent": "So coming up with some kind of estimate of the probability of case value C in some context and then actually smoothing these estimators.",
                    "label": 0
                },
                {
                    "sent": "With other estimators, and that's really a key point here.",
                    "label": 0
                },
                {
                    "sent": "Why do you want to smooth these things well?",
                    "label": 0
                },
                {
                    "sent": "So here we're looking at the number of times that case value C has been assigned to a word W with tag S, and then she might be quite a sparse count.",
                    "label": 1
                },
                {
                    "sent": "We might not have actually seen that occurrence very many times, So what we want to do is smooth these estimators with estimators that are better defined.",
                    "label": 0
                },
                {
                    "sent": "There's just more data for them because the context we're looking at is shorter, so that's what I did.",
                    "label": 0
                },
                {
                    "sent": "Same approaches, interpolated engram, language modeling if you're.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With that so one key difference though, is that in interpolated ngram language modeling, it's really obvious what context you want to use.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to predict a word at some position in the sentence you look at, maybe the previous three words or something, and you smooth your estimate of the probability for that context with an estimate based on maybe the previous two words, and then one that's best based on like the previous word.",
                    "label": 0
                },
                {
                    "sent": "And you do that.",
                    "label": 0
                },
                {
                    "sent": "It's really obvious that you take these longer sequences of words and just sort of reduce them to smaller sequences.",
                    "label": 0
                },
                {
                    "sent": "By dropping the word that's furthest back in the history, and that's sort of an obvious thing to do, because you think that words that are further back in the sentence, or less likely to affect whatever the current word is that you're predicting.",
                    "label": 0
                },
                {
                    "sent": "But in the case of pausing the choice of contexts in which content bit of context you drop, is actually much less obvious.",
                    "label": 1
                },
                {
                    "sent": "So in the case of predicting case value C, so like lower case or something, we might be looking at the tag in that position, and the word in that position.",
                    "label": 0
                },
                {
                    "sent": "Which one of these do we want to drop?",
                    "label": 0
                },
                {
                    "sent": "Do we want to drop the word and just look at the tag, or do we want to drop the tag?",
                    "label": 0
                },
                {
                    "sent": "And just look at the word.",
                    "label": 0
                },
                {
                    "sent": "So I did a bunch of experiments and chose a bunch of different Contacts in Contacts reductions and I'll talk about those briefly in a second, but throughout the rest of this talk, I'm just going to use the exact same context and context reductions that I say used what he actually chose to do was to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Take the this is an estimator of the probability of case value C in the context of tag S and word W.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And he smoothes that with an estimator.",
                    "label": 0
                },
                {
                    "sent": "That's just the probability of case value C in the context of tag S.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She also smooth with just a uniform probability over the case values, and throughout this I'm using the example of the case value probability because it's the simplest one of the probabilities in this model, but he uses the same approach for all.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of the other probabilities.",
                    "label": 0
                },
                {
                    "sent": "And just to give you an idea of how complicated this kind of thing can get when we're looking at the probability of A tag where conditioning on the parent tag cased word, and we're looking at the sibling tag, and we're looking at the direction that's a lot of conditioning context.",
                    "label": 1
                },
                {
                    "sent": "These counts are going to be really, really sparse, and So what we do is we actually what is needed is he actually smooths the counts from that huge context with account that's just based on the number of times that we've seen the tag in the context of the parent tag, the sibling tag.",
                    "label": 0
                },
                {
                    "sent": "And the direction and then smooth that with just counts that are based on the number of times that the parent this tag has been seen in the context of the parent tag and the direction.",
                    "label": 0
                },
                {
                    "sent": "So we have these really, really big context really sparse contexts, and so that's why smoothing is really important.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically, in recent years people got sort of UN excited about generative dependency models because hey, discriminative methods are really cool, everybody likes and blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "And I said method sort of got forgotten a bit, but it turns out that you can actually redefine isnos model from a Bayesian perspective and the whole thing actually turns out to be this really nice hierarchical Bayesian model.",
                    "label": 1
                },
                {
                    "sent": "It's not the original way that Eisner motivated the model, but it's totally equivalent, and so now what I'm going to do is, I'm going to talk you through how is this model is actually just.",
                    "label": 0
                },
                {
                    "sent": "A hierarchical Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to treat each probability vector, so the distribution over tags or the distribution over words, or the distribution over case values as a random variable.",
                    "label": 1
                },
                {
                    "sent": "So in this case we have a distribution over case values in the context of tag Essen, word W, and we're going to do is we're going to.",
                    "label": 0
                },
                {
                    "sent": "We don't know what this distribution is, so we're going to draw it from a delay prior going to give it away prior and I'd originally prior has two parameters, it has a concentration parameter and a base measure, and in this case the base measure that I'm using is A tag specific base measure.",
                    "label": 0
                },
                {
                    "sent": "And what the base measure is, it's just sort of like the mean distribution and.",
                    "label": 0
                },
                {
                    "sent": "This concentration parameter just sort of indicates how dispersed or how concentrated these distributions over case values for each possible S&W context will be.",
                    "label": 0
                },
                {
                    "sent": "From that base measure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having defined interracially prior like that, well, what are we going to do about the base measures?",
                    "label": 0
                },
                {
                    "sent": "'cause we don't know what those are either?",
                    "label": 0
                },
                {
                    "sent": "OK, no problem.",
                    "label": 0
                },
                {
                    "sent": "Well defined.",
                    "label": 0
                },
                {
                    "sent": "A hierarchical directly prior, so we'll put another darish lay on each one of these tags, specific base measures, and so remember that Eisner, when he's looking at the context to use when predicting case values, he goes from the tag and the word to just the tag to the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "So we're going to try to encode that same thing here in our hierarchical prior.",
                    "label": 0
                },
                {
                    "sent": "By drawing on tag specific base measure from Adrish, lay with again some concentration parameter and hear a uniform based measure.",
                    "label": 0
                },
                {
                    "sent": "And by putting these priors on the base measures of our original prior inducing a hierarchical darish way prior over our distributions.",
                    "label": 1
                },
                {
                    "sent": "Then, having done that, we can integrate out all of these unknown probability vectors and come up with predictive distribution.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when we do that, what we find is that the predictive distribution over case values looks like this.",
                    "label": 1
                },
                {
                    "sent": "Again, we have these like bees.",
                    "label": 0
                },
                {
                    "sent": "These counts that are specific to case value C in the context of tag S and word W with smoothing, smoothing those with some counts that are specific to case values C in the context of tag S and then with this uniform.",
                    "label": 1
                },
                {
                    "sent": "Probability here and the things to note about this are firstly that this looks really similar to eisners predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "Secondly, a point which I'm going to kind of gloss over due to time restrictions.",
                    "label": 0
                },
                {
                    "sent": "One thing that's different is that these counts don't actually have to be raw observation counts in the duration world.",
                    "label": 1
                },
                {
                    "sent": "These can be like type counts.",
                    "label": 1
                },
                {
                    "sent": "So for instance, the number of different unique words that case value C and tag S have been seen in the context of, but I'm not really going to talk about that very much here.",
                    "label": 0
                },
                {
                    "sent": "Relevant thing is just that it's worth keeping in mind that these actually in this framework don't have to be the robbs.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Accounts so just to make it totally clear we have ice model over here.",
                    "label": 0
                },
                {
                    "sent": "One of these predictive distributions from this model and one of the predictive distributions from the Bayesian model, and they're very, very similar.",
                    "label": 1
                },
                {
                    "sent": "The only difference are what higher level counts were using and the values of the concentration parameters.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's really interesting.",
                    "label": 0
                },
                {
                    "sent": "We've taken ISIS model.",
                    "label": 0
                },
                {
                    "sent": "We've reinterpreted it in this Bayesian framework.",
                    "label": 1
                },
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So what can we do now?",
                    "label": 0
                },
                {
                    "sent": "Well, there are at least three ways of varying the Bayesian model.",
                    "label": 1
                },
                {
                    "sent": "Firstly, we can sample the concentration parameters.",
                    "label": 0
                },
                {
                    "sent": "We don't have to set them to three or 1/2 or whatever values Eisner chose.",
                    "label": 0
                },
                {
                    "sent": "We also don't have to cross validate them.",
                    "label": 0
                },
                {
                    "sent": "We can just simply slice sample them or something like that.",
                    "label": 0
                },
                {
                    "sent": "Also, as I was saying briefly before the counts, the higher level counts don't need to correspond to the raw observation counts.",
                    "label": 0
                },
                {
                    "sent": "And we can also use priors other than the hierarchical drizly distribution and all of these things have the potential to improve model quality.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk the most about actually using a different prior other than the hierarchical directly distribution.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about using hierarchical Pitman your priors.",
                    "label": 1
                },
                {
                    "sent": "We're going to do here is, rather than drawing, for instance, our distribution over case values from.",
                    "label": 0
                },
                {
                    "sent": "Additionally distribution, we're going to instead draw it from a Pitman Yor process.",
                    "label": 0
                },
                {
                    "sent": "Now I'm actually drawing it from a discrete Pitman Yor process, but if I was going to have some kind of like letter based top level prior then I could have been a continuous thing.",
                    "label": 0
                },
                {
                    "sent": "But here I'm really going to concentrate on on discrete pit manual processes and the payment your process has the same two parameters as the duration process a concentration parameter.",
                    "label": 0
                },
                {
                    "sent": "Database measure, but it also has a discount parameter, so we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to draw each of our distributions over case values and eat all of our other distributions or distributions over tags and words as well from a Pitman, your distribution, and then we're going to also put Pittman your priors on the base measures.",
                    "label": 1
                },
                {
                    "sent": "So one thing to note is that when the discount parameters are equal to 0, the Pitman your distribution is identical to interracially distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is just a direct generalization of the distribution that gave rise to ISIS model, and in fact the Pitman Yor process gives distributions that better resemble natural language.",
                    "label": 1
                },
                {
                    "sent": "There actually better at capturing the occurrence of really rare words that maybe occur only once or twice.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So if we actually integrate out based measures and our distributions, we end up with a predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "Again, using the example of case values that looks like this and it looks kind of similar to before, except our counts are now.",
                    "label": 0
                },
                {
                    "sent": "I've represented them using M. There actually are old counts that we had in the duration of version of the model minus this discount parameters.",
                    "label": 0
                },
                {
                    "sent": "Some quantity of this discount parameter.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're saying rather than.",
                    "label": 0
                },
                {
                    "sent": "Rather than actually using the counts from the model, we're subtracting some discount off of them, so we're sort of we're sort of subtracting this discount off of the account and then adding that to these distributions that we're using to smooth things with.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Soap in your prayers have been used for language modeling with a lot of success.",
                    "label": 1
                },
                {
                    "sent": "It turns out that conveys a nice moving, which is one of the best smoothing methods in language modeling is equivalent to setting the Alpha parameters in a pit maneuver prior to zero and using a particular approximate inference scheme that I'll refer to as a minimal path assumption.",
                    "label": 1
                },
                {
                    "sent": "So I'm really interested in using these Pitman Yor process is for dependency parsing because unlike many other parsing models, dependency parsing actually is all based on the individual words.",
                    "label": 0
                },
                {
                    "sent": "It's fully lexicalized and so you have really, really sparse counts, because like language modeling, you're dealing with these individual words, you're not necessarily just dealing with rules, and so I think that Pittman your prize are particularly appropriate choice for dependency models because they're a generative model for words and language.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm so if this were language modeling, we'd be done.",
                    "label": 1
                },
                {
                    "sent": "We've specified how to compute the probability of a tree or with a sentence with its corresponding tree, and if this were language modeling, we could just compute the probabilities of our sentence is and do whatever it is that one does with that.",
                    "label": 1
                },
                {
                    "sent": "But in fact, that's not what we're interested in, and pausing in the world with dependency parsing were interested in taking real sentences that only have words tags in case values, and we want to infer the dependency trees for those sentences.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to use training data to learn the model, and then we're going to sample trees for test sentences were going to do this using a Metropolis Hastings algorithm that users Eisenerz dynamic program as a proposal distribution, and then decides whether to accept or reject proposal trees, and it's really similar to the Metropolis Hastings algorithm that Mark Johnson and others presented for PCF, Geez.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I wanted to compare whether the Pitman Yor prior is actually better for dependency parsing than the hierarchical directly prior.",
                    "label": 1
                },
                {
                    "sent": "So we paused the Wall Street Journal sections of the Penn Treebank.",
                    "label": 1
                },
                {
                    "sent": "This is the standard parsing data set that everybody uses.",
                    "label": 0
                },
                {
                    "sent": "There's training sections sections two through 21.",
                    "label": 1
                },
                {
                    "sent": "We've got nearly 40,000 sentences there, and then there's the testing sections, which is 2500 sentences and the way people evaluate these models is to look at parse accuracy.",
                    "label": 1
                },
                {
                    "sent": "The percentage of parents that were correctly identified.",
                    "label": 0
                },
                {
                    "sent": "And for comparison with other peoples methods, I'm just looking at maximum probability trees here rather than sample trees, but that's just for comparison purposes.",
                    "label": 0
                },
                {
                    "sent": "Also, for efficiency, I, although I could infer the part of speech tags using the dependency model, I'm actually going to fix them two tags from a standard part of speech tagger.",
                    "label": 0
                },
                {
                    "sent": "And this is just because it's faster, but it gives releases.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results.",
                    "label": 0
                },
                {
                    "sent": "So here are the pause accuracies.",
                    "label": 0
                },
                {
                    "sent": "We've got these four sets of graphs here.",
                    "label": 0
                },
                {
                    "sent": "These two are one particular approximate inference scheme, and these two are a different approximate inference scheme, and there's not really much difference between the two approximate inference schemes.",
                    "label": 0
                },
                {
                    "sent": "The key things to note are so this here is using fixed hyperparameters and this one here is Eisner's original model.",
                    "label": 0
                },
                {
                    "sent": "This is entirely equivalent to Eisner's original model.",
                    "label": 0
                },
                {
                    "sent": "This is the same setup but with the Pitman your prior, but again fixed hyperparameters and we see that using the Pitman Yor prior gives an improvement of around 3% in accuracy.",
                    "label": 0
                },
                {
                    "sent": "Then we also have over here we have with sampled hyperparameters and the difference between the door Ashley and the Pitman Yor when we're actually sampling the hyper hyper parameters is much smaller, but overall the difference between using a Pitman yor prior with sampled hyperparameters and using Adrish lay prior with fixed hyperparameters, which is what equivalent to what I said originally did, is about sort of 4% or so, which corresponds to about a 26% error reduction over Eisner, and that's fairly significant.",
                    "label": 1
                },
                {
                    "sent": "So that's really nice.",
                    "label": 0
                },
                {
                    "sent": "It shows that firstly.",
                    "label": 1
                },
                {
                    "sent": "Pittman, your prize are actually better for dependency parsing in terms of improving cause accuracy and then also we can improve performance further by sampling the hyper parameters of the model.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You might wonder why I'm sort of revisiting this generative dependency model when there's been so much recent success on discriminative methods for dependencies pausing and the reason why is because you can include other interesting latent variables in a generative framework, and so this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the second bit of the talk.",
                    "label": 0
                },
                {
                    "sent": "And here I'm interested in including topics in the sample in the parsing model, and I'm going to look at two different types of topics.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at semantic topics, which are the kind of topics that you guys are probably familiar with.",
                    "label": 0
                },
                {
                    "sent": "Their, you know, semantic groups or probability distributions over semantically related words, and I'm also going to look at syntactic topics.",
                    "label": 0
                },
                {
                    "sent": "Again, these are specialized distributions over words, but the way these words are related is by syntactic relationships, not semantic relationships, so.",
                    "label": 1
                },
                {
                    "sent": "To investigate incorporating these kinds of latent variables into the pausing model, I'm going to use a slightly simpler pausing model.",
                    "label": 0
                },
                {
                    "sent": "The real differences between this and the model that that I was just talking about, that the sentences are untagged, an uncased.",
                    "label": 1
                },
                {
                    "sent": "We're just looking at lower case words were not going to look at siblings, and we're not going to take siblings into account.",
                    "label": 0
                },
                {
                    "sent": "It's just the 1st order model and the distributions over children depends on the parent, and now some kind of latent state variable as well.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um, I'm actually going to skip over this slide.",
                    "label": 0
                },
                {
                    "sent": "This is just talking about first order models being simpler and how they're similar, but it's.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not entirely relevant to the latent variable stuff, so OK syntactic latent variables.",
                    "label": 1
                },
                {
                    "sent": "So here what I've got is I've got a parent and I've got a child and the relationship between them is now mediated by this latent state variable.",
                    "label": 0
                },
                {
                    "sent": "I've got this latent state variable that sits between the parent and the child and tries to capture properties of the dependencies that are in these trees.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the generative process is as follows.",
                    "label": 0
                },
                {
                    "sent": "Generate a state given the parent word.",
                    "label": 1
                },
                {
                    "sent": "So generate the state here given hit.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then having done that, generate a word.",
                    "label": 0
                },
                {
                    "sent": "In this case bull given the state and these distributions over states given the parent and distributions over.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words given the state, we're just going to give the reshape rise as before.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, semantic latent variables.",
                    "label": 0
                },
                {
                    "sent": "What you have is something like this.",
                    "label": 0
                },
                {
                    "sent": "We have again a parent, a child and now the child actually depends on this semantic state.",
                    "label": 0
                },
                {
                    "sent": "Here that comes from a document specific distribution over topics.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, you generate the state given the document.",
                    "label": 0
                },
                {
                    "sent": "This is exactly as in latent richly allocation.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then given that.",
                    "label": 0
                },
                {
                    "sent": "You generate the word based on the parent word and this document specific some.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text 8 and again we gave everything directly prior.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we're looking at the syntactic topics around some experiments comparing the syntactic topic so so one thing about the syntactic topics is that we have no label data for these, or we have the dependency trees, the States themselves.",
                    "label": 0
                },
                {
                    "sent": "These are fully unsupervised.",
                    "label": 0
                },
                {
                    "sent": "So what I'm doing is I'm taking training data, just training trees, and I'm inferring latent semantic, latent syntactic states from these trees.",
                    "label": 0
                },
                {
                    "sent": "Then, having done that, I'm using those.",
                    "label": 0
                },
                {
                    "sent": "In Ferd States and the training trees to infer States and trees for test data.",
                    "label": 0
                },
                {
                    "sent": "And when I do that, what I see is that if I fix the states to be actual part of speech tags, then I get this performance when I use sample trees I get around 55% accuracy if I use the most probable tree I get around 62 1/2% accuracy.",
                    "label": 0
                },
                {
                    "sent": "And when I actually have the model instead learn these unsupervised latent syntactic states, we find that we actually get quite a bit better parsing accuracy.",
                    "label": 1
                },
                {
                    "sent": "It's most noticeable for the sample trees then for the maximum probability trees.",
                    "label": 0
                },
                {
                    "sent": "But it's definitely a noticeable performance improvement using these automatically inferred fully unsupervised latent states instead of part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of nice because.",
                    "label": 0
                },
                {
                    "sent": "It's showing that what we are actually inferring is something that is relevant for do.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independency pausing.",
                    "label": 0
                },
                {
                    "sent": "And when we look at the latent states, these are the latent states in Ferd using the training trees, we see really interesting stuff.",
                    "label": 0
                },
                {
                    "sent": "They basically look like part of speech tags, but they're much finer grained.",
                    "label": 0
                },
                {
                    "sent": "So here we have nouns.",
                    "label": 0
                },
                {
                    "sent": "And these nouns are all basically job titles and over here we also have nouns, but these are all place names.",
                    "label": 0
                },
                {
                    "sent": "And here we have past tense verbs.",
                    "label": 0
                },
                {
                    "sent": "We have words over here like would will could, should, can, might a bunch of numbers over here.",
                    "label": 0
                },
                {
                    "sent": "It's really nice.",
                    "label": 0
                },
                {
                    "sent": "These actually.",
                    "label": 0
                },
                {
                    "sent": "It looks like we're sort of extracting something really interesting that really summarizes what's going on in terms of the dependencies.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is my last slide.",
                    "label": 0
                },
                {
                    "sent": "So for semantic topics, the topics look very similar to what you would get from LDA, so I'm not going to show those, but what I'm going to do here is unsupervised bits per word, so the fewer the bitspower, the better.",
                    "label": 1
                },
                {
                    "sent": "Better the model.",
                    "label": 0
                },
                {
                    "sent": "This is a fully unsupervised model, we're using no training trees at all.",
                    "label": 0
                },
                {
                    "sent": "We're just going through and sampling our trees in a fully unsupervised fashion and comparing.",
                    "label": 0
                },
                {
                    "sent": "Comparing these different models as a baseline, we've got LDA here.",
                    "label": 0
                },
                {
                    "sent": "Another baseline we've got just dependencies only, so none of these latent States and then we've got these two latent state models.",
                    "label": 0
                },
                {
                    "sent": "We've got the one with syntactic topics, and the one with LDA is semantic topics, and we see that the best of these models, in terms of the bits per word, is using semantic topics along with the dependencies, and so this is promising.",
                    "label": 0
                },
                {
                    "sent": "It's sort of.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is very much work in progress, but this is promising, because it shows that perhaps we are definitely gaining something in terms of model performance.",
                    "label": 0
                },
                {
                    "sent": "By using these unsupervised.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "States.",
                    "label": 0
                },
                {
                    "sent": "So conclusions we reinterpreted classic dependency parser using a fully Bayesian framework.",
                    "label": 1
                },
                {
                    "sent": "We then showed that we can take that framework and improve parsing performance by using Pittman your priors rather than directly priors.",
                    "label": 1
                },
                {
                    "sent": "Also, we can sample the hyperparameters, which improves performance too.",
                    "label": 0
                },
                {
                    "sent": "And finally, we can incorporate latent variables into the model, either syntactic, stop it topics that cluster parent child relationships, or semantic topics as in LDA and future work, obviously.",
                    "label": 1
                },
                {
                    "sent": "Combining syntactic and semantic topics and continuing to take a look at these latent variables in pausing.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks that's it.",
                    "label": 0
                },
                {
                    "sent": "It's a very.",
                    "label": 0
                },
                {
                    "sent": "The number of tables in your Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "So the number of you can you go back, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So, OK, that's the number of tables in your Chinese restaurant Chinese restaurant for S&W, and this is the number of them that actually have observation, see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I didn't want to get into that, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Come again.",
                    "label": 0
                }
            ]
        }
    }
}