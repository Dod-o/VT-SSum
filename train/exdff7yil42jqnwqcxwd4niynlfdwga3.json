{
    "id": "exdff7yil42jqnwqcxwd4niynlfdwga3",
    "title": "Reasoning, Attention and Memory",
    "info": {
        "author": [
            "Sumit Chopra, Facebook"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_chopra_attention_memory/",
    "segmentation": [
        [
            "As the title says, I'll be talking about reasoning, attention and memory so.",
            "Yeah, I don't know like just to give you a bit of a story like as a.",
            "Not as as old as your show, but I've been working in deep learning since.",
            "Since I started my PhD in 2003 and I attended a couple of machine learning summer schools when I was a student an.",
            "Used to happen in University of Toronto, then again, uh, sponsored by cipher.",
            "An room used to be full.",
            "Like as it is right now, but only composed of these two rows.",
            "So it's I mean it's fascinating to see how many how the field has taken up so much so fast and so much has been done already in two years, which looks like I mean eternity.",
            "So yeah, so as the topic of my talk as the title says, is reasoning attention and memory, where you're sure gave a really brief snapshot of things like memory, network, attention based models, etc etc which have recently basically come into the limelight and they seem to show a lot of promise in a lot of interesting, challenging problems so."
        ],
        [
            "Just to give you a bit of a motivation, deep learning as we know is pretty successful at Vision an you see the basic architecture might look something like this, where you have a convolutional network or some derivative of this network.",
            "You pass in an input image an.",
            "Which goes through a bunch of hierarchical feature transforms and in the end you get like a feature vector on top of which you plug a classifier for your final objective.",
            "And the final objective could be object detection.",
            "It could be object recognition and stuff like that and deep learning models we know are the state of the art."
        ],
        [
            "And something similar has lately happened in speech as well, where basically you have an acoustic signal which sort of is sampled at some frequency.",
            "To get your input spectrogram, which can be viewed as a 2 dimensional image and that image is plugged into again a deep network and the network essentially predicts the phones at every frame and then you can plug your language model and stuff like that to stitch these phones.",
            "The predictions of the.",
            "Work, and finally what you get is the.",
            "Outputs intense that has been spoken and again like models like these I mean of course like what goes in here.",
            "I'm not going into the detail, but the models like these are sort of.",
            "Almost state of the art in speech as well and so."
        ],
        [
            "Something similar has happened in the domain of text, although it's still a little it still catching up where basically you have an input, you feature riser and put in some way an you have some deep network which could be a feedforward net or recurrent net or like an LSD emoji.",
            "Are you an?",
            "In the end you have some objective like some task at hand.",
            "In this case I've shown a sentiment analysis task with given a sentence you want to say whether.",
            "The sentiment of the sentence is positive or negative.",
            "Or it could be any task.",
            "Any standard NLP task?"
        ],
        [
            "So so so the question is, is this enough?",
            "Well before that.",
            "So the general basically formulation is you have an input.",
            "You represent it in some way.",
            "You have an encoder or the feature extractor which extracts the features and then you plug in a classifier or regressor which we call a decoder and you optimize the whole thing using some loss function, write an."
        ],
        [
            "The question here is, is this enough basically right?",
            "So let me give you a couple of scenarios where."
        ],
        [
            "Basically.",
            "We'll see whether it's enough or not, so suppose you're given a story like a really simple story of an actor called Joanne, maybe Fred, and they do a whole bunch of tasks like moving around, picking up objects, etc.",
            "Like Joe went to the kitchen, Fred went to the kitchen.",
            "Joe picked up the milk, do travel to the office, Joe left the milk and blah blah.",
            "Now if I ask any human to answer these questions, where is the milk?",
            "I mean, it's not hard to sort of infer from this story where the milk is probably, as a human you'll see in the story where the word milk appears, and you know that the Joe had the had left the milk.",
            "And where did Joe leave the milk?",
            "Basically, Joe travel to the office in the last in previously, before leaving the milk.",
            "So highly likely the milk is in the office.",
            "And yeah, it is an likewise aware is Joe.",
            "You can infer from the storage in the bathroom an where Joe before office.",
            "And again you can infer that he was in the kitchen.",
            "So like tasks like this are fairly easy for a human too.",
            "Answer."
        ],
        [
            "Consider scenario two and this is like a sort of an exercise for you guys.",
            "Basically, you're given a story of.",
            "2020 sentences, like some snapshot of taken from some random book and but these 20 sentences are sequential and I show you the 21st sentence with one of the words missing.",
            "So I give you one minute to read the story and let me know the answer is.",
            "Right, so how many of you think the answer is Mr. Cropper?",
            "How many think the answer is Mr Baxter?",
            "OK, that was pretty bad.",
            "Come on, this is like an exam.",
            "OK, well let me give you the answer.",
            "The answer is Mr. Baxter.",
            "Right anyway, so maybe not under so much stress.",
            "If I had given you enough time, you would have read the story and probably with a very high likelihood you would have answered this correctly."
        ],
        [
            "Write an scenario #3.",
            "Now suppose you are given a whole knowledge base, a knowledge base of facts based basically as a set of triples where you have an entity, some action, an sorry, some relation an another entity.",
            "Ann, you have millions of these facts, so you want to store them and then you want to ask questions like who wrote?",
            "Kung Fu hustle."
        ],
        [
            "Or even more complicated questions like I'm interested in watching a Stephen Chow movie.",
            "Other income ferstl, can you suggest something?",
            "Right so."
        ],
        [
            "An even like yet another scenario of dialogue modeling where you have a user who has and who is chatting with the bot.",
            "An user has an intent of making a reservation in a French restaurant of table of six an.",
            "Basically the restaurant should be sort of a high end and then the bot indulges in some sort of an interaction, like where the location should be and stuff like that.",
            "And at some point the user wants to revise his or her preferences.",
            "Of going from 6 to maybe 4 and then the bot needs to keep track of whatever else it had.",
            "Sort of known about the users intent and then modify subsequently and finally make a reservation the way a normal human would do."
        ],
        [
            "Late so basically the scenarios like this for scenarios are shown so far cannot really be modeled as an input output mapping from X to Y, where X is your input potentially or stories and a question F of W is basically your standard deepnet some parametric model and you want to.",
            "So sort of give the right answer.",
            "Basically then this sort of a need to have some sort of an external memory.",
            "That sort of external persistent memory that remembers the context right?",
            "And given this memory, which could potentially be huge, your model should know how to look like given a question.",
            "The old model should know how to look, or rather where to look into this memory.",
            "And not only that, it should know what to look for in the memory and given a short list of memory slots that your model might figure out where to look at, the model needs to reason.",
            "Basically, in the example in the toy example of Joe going to the office and dropping the milk, I mean the model needs to know what does going to the office mean and what does dropping the milk mean, and so and so forth.",
            "An it essentially needs to handle like like.",
            "This context is huge.",
            "And it could potentially be changing."
        ],
        [
            "Right, so in short, we really do need some sort of a memory that is augmented to this function F of W, which can enable answer.",
            "In these complicated scenarios."
        ],
        [
            "So one might argue, given the previous talk from your show about RNS, that the hidden states of RNN, particularly LTM or grew.",
            "Are effectively memory, so why not just simply run an RNN on your external context and get some sort of a representation of your story and use that representation?",
            "To map questions to answers.",
            "Or responses, but clearly this will not scale, as you're sure mentioned, because of primarily 2 problems, one is vanishing gradient an.",
            "Extending exploring variant, but that can be handled, but primarily is the vanishing gradient, or in other words, RNS don't really have the ability to capture really long term dependencies, like if I give you knowledge base of 1,000,000 facts, there's no way an RNN can hold all the facts in its memory.",
            "One because of the vanishing gradients and two because the state of the memory is like really small.",
            "I mean, even if it's like 1000 hidden units, there's no way you can expect to store such rich information in those thousand hidden units and reason with that."
        ],
        [
            "Right, so which brings me to the topic of my talk, basically.",
            "Recently, in order to answer such questions, there have been a bunch of models that are proposed in this direction, namely memory networks, which I'll talk in quite a bit of detail.",
            "And basically I'll go over various versions of these networks.",
            "And then around the same time there was another sort of model that was both called neural Turing Machines, which does something similar, but it was shown to do.",
            "Interesting toy tasks, an not until recently it has been applied to some essentially a real world problem here on and.",
            "Then there is also another set of work which.",
            "Which is somewhat old, but recently it has again seen resurgence, namely, basically augmenting your RNS with external memories like Stacks, list and queues.",
            "So if time permits, I'll talk about the third set of models, but if not then it's OK because tomorrow probably you'll hear about them in quite a bit of detail.",
            "Right?"
        ],
        [
            "So yeah, so the general architecture of all these three classes of models.",
            "Can look something like this.",
            "Basically you have a controller which is think of a controller as like a standard deep network could be.",
            "A feedforward could be an RNN network, could be anything.",
            "Which.",
            "Has multiple tasks, basically, not only, it produces the output Y, but it also controls your read and write head into the memory.",
            "And then the main component is this memory bank which will store your context, the stories, the knowledge base etc etc.",
            "Right, so the purpose of the head, the purpose of the right head, as the name suggests, is take given the input an essentially the output of the controller, it sort of decides which fact to right where.",
            "And how like what is the purpose of the read head?",
            "As the name suggests, is given the input X and the sort of input of the controller.",
            "Which part of the memory to read and pass the information back to the controller and finally the controller wants given an input X it will.",
            "Basically read some stuff in the memory, sorry, write some stuff in the memory, read it from the memory, combine it with input X and do some processing to finally generator output Y.",
            "So all of this will become clearer with an example, but that's that's the basic architecture.",
            "So what distinguishes the three architectures shown in this slide are basically what goes inside each of these components, like what sort of memory are we using?",
            "Are we using a tape?",
            "Are we using a stack Q, whatever?",
            "And what are the functions are read in the right head an?",
            "What exactly is a controller?",
            "So memory networks is 1 version neural Turing Machines is another, and so and so forth."
        ],
        [
            "Right?",
            "So yeah, so so basically.",
            "Let's start the let's go deep into what a memory network looks like.",
            "Basically it's a class of models which combines.",
            "As I said, large scale memory with a learning component an you can read and write into the memory, and it incorporates reasoning over the memory via this.",
            "A notion of attention, so I'll talk briefly again about attention today, but you'll hear a lot more in detail in tomorrow's top.",
            "And the cool thing about memory network is that the framework is flexible enough to store rich representations of your input right, and the model is scalable in the sense that the memory can potentially be massive.",
            "And still you can efficiently do reasoning.",
            "How.",
            "Like again, I'll speak as over the course of my talk.",
            "An yeah an Lastly again, as I said, like the whole specification of the memory network model is flexible that you can store.",
            "Basically, long term memories, you can sure store short-term memories an A combination of these can help in.",
            "Solving tasks like dialogue, modeling etc.",
            "So."
        ],
        [
            "How so?",
            "Just to give you a brief overview of what the processing steps of the memory network look like basically.",
            "Given an input X, the first step is for the controller to convert the incoming input into some internal representation.",
            "So in the original paper this was done.",
            "This was denoted as the module I the input module.",
            "And then the right head based the Step 2 involves the right head updating the memories and also writing the new.",
            "Internal feature representation that you got from step one into the memory.",
            "And the step three involves given this external input and.",
            "The contents of your memory, the read head will read some portion of the memory and pass it on to the controller.",
            "And finally, the Step 4 involves the controller doing the remaining work.",
            "Basically it takes as input X.",
            "It takes as input whatever the right head.",
            "The read, read, read from the memory combines the two to finally answer your question."
        ],
        [
            "Late so.",
            "Let's be a bit more specific with respect to one simple example, which is similar to the story that I.",
            "Proposal I told earlier.",
            "Let's go into a bit more specifics using specific story, basically.",
            "The model is given this simple story.",
            "John was in the bathroom, Bob was in the office, blah blah blah.",
            "This is what's called context, right?",
            "And during training, what you're given is this question answer pair based on that story where the question is, where is John and the answer is kitchen, and in addition to that, in what the very first version of the memory networks which we call the fully supervised.",
            "In addition to these two things during training you're also given this so called supporting fact in the story where basically it tells you which sentence in the story has your answer.",
            "Or has some signal associated with your answer?",
            "Of course, this supporting factor is not given to you during test time, so.",
            "Yeah, so so.",
            "So during testing I just given the context and the question and you want to generate the answer."
        ],
        [
            "So step one of a memory network would involve storing the representation of the context.",
            "Now.",
            "What goes into the representation is entirely up to you.",
            "Like there's no, there's no sort of restriction on what it could be like.",
            "It could be individual words like you might want to store individual words of the story in each single memory slot.",
            "Or you might want to store a window words in each memory slot.",
            "Or you might want to store the whole sentence in each memory slot, where again, the representation of a sentence could be as simple as a bag of words where you essentially take the average of the embeddings of all your words.",
            "Or it could be an output of an RNN, or it could be an output of a CNN and so on and so forth.",
            "So this function F basically, which takes as input A sent a sequence of words, stores outputs of memory representation, which is stored in the slot, and this is quite flexible."
        ],
        [
            "Now you're also given a question during training, so you do exactly the same thing you represent the question.",
            "As X.",
            "Via the same same function."
        ],
        [
            "The step three of the memory network involves defining the so-called scoring function, where essentially the task of a scoring function is to measure.",
            "The sort of relevance of each of the memory slots with respect to the question.",
            "Like so the scoring should be in such a way that the memories which are relevant like.",
            "But before that, like the scoring function typically is a parametric function, it could be a neural network or anything.",
            "But the training of this parametric function should be such that in.",
            "For all the relevant memories, it scores the the score that it gives with respect to the question is higher than the score with respect to irrelevant matters.",
            "What is the difference?",
            "Context.",
            "So the four sentences are the context.",
            "The supporting fact is the subset of the context which is the most relevant.",
            "Based on your question.",
            "I don't know it is the most.",
            "Yes, you know.",
            "Yes exactly.",
            "Animations colleges are mostly for this version of memory networks which we call fully supervised, but I'll talk in detail later about another version where you do not even need that.",
            "Dealing training an where it automatically learns."
        ],
        [
            "Right, so some of the example choices of.",
            "Scoring function could be like a simple input output mapping.",
            "Why are some transformation matrix U which is basically the learnable parametric matrix?",
            "Or as I said, it could be a fully connected network that takes as input the question and some memory.",
            "Slot and generates the score again.",
            "It's up to you."
        ],
        [
            "Anne.",
            "Step 4 Finally involves designing another parametric function which Maps the current question and the relevant memory that you obtained from your read head.",
            "Basically the memories that gave you the higher score.",
            "And.",
            "Use these two information to generate your final answer.",
            "Right and in the first set of experiments, this was basically again a scoring function where basically we scored all the possible responses given the input and the memory and picked the one that gives you the that gives you the largest score."
        ],
        [
            "Right, and assuming that you've trained your function F your parametric function that is used to generate the answer, the scoring function, and basically the memories.",
            "Then given a question at Test time.",
            "You will score it against all your possible memories and you pick the highest memory and use that memory in your final parametric function to finally generate your answer."
        ],
        [
            "And this all of this is achieved by minimizing this following loss, so I'll just go briefly explain what this means.",
            "So the first term essentially says X is your question.",
            "You basically want to score the memory which is relevant to you.",
            "Which is given to you, by the way, as a supporting fact to have a higher score than some, then all the other memories F bar.",
            "By some margin gamma.",
            "And once you have the.",
            "That's the first bit of the scoring function.",
            "The second bit of the scoring function says the loss function says that given the correct the relevant memory and your input X, you have another scoring function Sr.",
            "Which essentially increases the score of your correct response R and decreases the score of all the incorrect responses are bar.",
            "Ann, as one of you asked, basically, how do we know this?",
            "So as I said before in the fully supervised setting, we are given access to this during training time, right?",
            "So that's pretty straightforward.",
            "So it's like a standard margin loss where you want to increase the score of your correct memories and decrease the score of your incorrect ones.",
            "Anne.",
            "As you would note this loss function esentially.",
            "Will work when you have a single support."
        ],
        [
            "Fact what's like?",
            "What do you do when you have multiple supporting facts where given this story, where basically you ask the question?",
            "Where is football?",
            "You first need to know that Jaune was the person who picked up the football, and also Jaune was in the playground, so the answer should be playground which essentially is encoded in these two sentences.",
            "So the previous loss function obviously will not work because you have a single supporting fact.",
            "But the cool thing about this is you can iterate."
        ],
        [
            "Or in other words, the loss function might look something like this.",
            "Now you start with the first supporting fact and you want to increase the score of your first supporting fact.",
            "Once that's done, you iterate an.",
            "Essentially, combine that with your original question an.",
            "Want to increase the score of the second supporting fact?",
            "And finally, your final the output of your controller will take as input the original question and the two supporting facts, like the memories corresponding to the two supporting facts, an increase the score of those guys.",
            "So yeah, so so basically so just to repeat, basically you have the supporting the first supporting fact, the second supporting facts."
        ],
        [
            "And finally.",
            "Iterate and this is what we call hops.",
            "And of course they need not be limited to two.",
            "You could sort of keep iterating until some threshold number of hopsan, etc."
        ],
        [
            "Right, so like a slight degression right now, basically.",
            "To test these models, so essentially we defined.",
            "Sort of 20 simulated tasks where basically the objective of these tasks was to test models that can do complex reasoning in which involves long-term memory, but in a controlled environment.",
            "So why we wanted that?",
            "Let's think of these tasks as sort of unit tests in software engineering, right?",
            "Basically we wanted to sort of.",
            "Best skills of the models which.",
            "Like this one, or at least two, maybe one or two skills in the model which sort of gives us an indication whether the model is doing any reasoning or not, right?",
            "And the skills could correspond to reasoning.",
            "It could correspond to language related skills like conjunction, coreference etc.",
            "And stuff like that.",
            "So I'll just quickly go through the collection of tasks that we came up with, which hopefully should give you an idea about sort of convince you that probably this might."
        ],
        [
            "Would be a good idea so so just to give you a bit of a background.",
            "So we essentially designed a simulator where a simulator had a bunch of objects like people.",
            "Like people and objects and they had a bunch of actions and each action was associated with an attribute like a goal would be go to a place, get will be get to an object and so and so forth.",
            "Right?"
        ],
        [
            "And based on this simulator we came up with these sort of really.",
            "Simple scenario of a world where people are just moving around to one room or the other, picking up objects, dropping objects and stuff like that.",
            "Like like Jason, Go kitchen Jason get milk and blah blah.",
            "Anne, once we are given these commands, we had sort of a very simple template ized language, which we plugged on top of this to convert these commands into natural language sentences.",
            "So again, the objective here was not to solve NLP, but to solve two things.",
            "One is basically to solve the reasoning, but given these commands.",
            "And also to have some sort of.",
            "Sort of.",
            "Language understanding skills basically.",
            "Yeah.",
            "An we wanted all of this in a very controlled environment and hence the simple simulation."
        ],
        [
            "So just so, just to give you a simple examples the.",
            "Like the simplest of the of the example, is basically a factoid based question answering which uses only a single supporting fact right where.",
            "Again, the objective was, as we've seen so far to answer a question given this story, which has just a single supporting."
        ],
        [
            "And then an extension of that was a story which.",
            "Can only be answered a question which can only be answered with two supporting facts.",
            "Or a question which can only be answered with three supporting facts."
        ],
        [
            "And there was another task where basically we we wanted to see if the model can learn the difference between 2 two things, subject and object where basically if the story looked something like the office is North of the bedroom, the bedroom is North of the bathroom.",
            "What is North of the bedroom or what is the bedroom North of?",
            "So if you see the two sentences have exactly the same words.",
            "But reordering them results in a different answer, so there was something we wanted to test whether the model can actually differentiate between North Ann bedroom."
        ],
        [
            "And then similar to that we had tasks related to three argument relation where basically have a story like Mary gave the cake to Fred.",
            "Fred gave the key to Bill.",
            "Jeff was Jeff was given the bail and blah blah.",
            "And who gave the cake to?"
        ],
        [
            "And then a simple true false questions."
        ],
        [
            "An questions pertaining to counting like simple counting tasks like Daniel picked up the football, he picked up the football.",
            "Drop the football.",
            "You got the milk.",
            "You got the Apple.",
            "How many objects is done in holding?",
            "So here you see like if for the model to answer this the model really needs to understand what's going on here.",
            "It's not like a simple input output map in a black box where you expect this thing to be solved."
        ],
        [
            "And then there was the last related to indefinite knowledgeware.",
            "Basically, you're not given certain like you're given some sort of a nuns uncertain facts.",
            "Ann, you're expected to answer your enhancer answers.",
            "Could be something like.",
            "Maybe like John is in the classroom or the playground and you ask, is John in the classroom?",
            "So things like that."
        ],
        [
            "Some language related tasks, like basic coreference, Daniel was in the kitchen, then he went to the studio.",
            "Where is Daniel?",
            "Right, so it needs to understand what he meant here and compound coreference.",
            "Daniel and Sandra Journey to the office.",
            "Then they went to the Garden Ann.",
            "Then you want to answer, where is Daniels even those Sandra had moved to somewhere else?"
        ],
        [
            "An like similar to that we had other set of tasks like time manipulation, basic deduction etc.",
            "But probably in the interest of time out."
        ],
        [
            "Move forward quickly and then, like positional reasoning, reasoning about."
        ],
        [
            "Aces etc anpac."
        ],
        [
            "Training and stuff like that."
        ],
        [
            "So.",
            "Yeah, so this gave us basically 20 tasks.",
            "Each task came with like a short story of the simulated world an the model was expected to answer and the questions were made so that basically you could evaluate with the model loans or not.",
            "And to compare.",
            "Our hypothesis that we really need an external context or an external memory.",
            "We compared with three sets of standard baseline.",
            "One is basically a standard structured SVM where you have a whole bunch of hand coded features from your story and your question and you just want to predict an answer using an SVM.",
            "And we also used an LSD M where basically we ran the SDM over the entire story on the question, and the objective was to use the last hidden state of the STM to predict the final answer and then a similar was an engram classifier with again, we took the story and the questions represented as a bag of ngrams and then train the classifier on top."
        ],
        [
            "So I guess you can see them so you can see from this table other than I mean the 1st and the 3rd column were basically bound to be hopeless.",
            "Basically the interesting bit is the STM an as expected since the story is long and it's the memory of the LCM is fairly restricted to just one hidden vector.",
            "I mean it did not do very well on most of the tasks.",
            "Right?",
            "And compared to a memory network which basically was the first version of the memory network consisting of one or two hops.",
            "So it does well on single supporting facts to supporting fact, but does really poorly on the three, supporting facts and so and so forth.",
            "And so in some of the tasks it works of them.",
            "It does not work.",
            "And then as I said, there's a lot of freedom in how you."
        ],
        [
            "Structure The memory network model that you can play around with it.",
            "So the second version of this was with so called adaptive memory, where essentially we do not restrict to two hops, but we define a special memory slot called the null memory slot and keep repeating the inference until it fetches the null memory slot and then generate an answer.",
            "And that, as expected, helps on three supporting facts an also on basic induction task, which I did not go into.",
            "But yeah, whatever."
        ],
        [
            "And then, as I said, like again, these are the next two are again variants of the memory networks where how you encode the features instead of encoding as the whole sentence.",
            "We essentially treated a sentence as a bag of ngrams.",
            "Anne used adaptive."
        ],
        [
            "Memory and here we used adaptive memory Ana nonlinear classifier instead of a simple scoring function at the end."
        ],
        [
            "And finally, a combination of all the three and we see as we increase the complexity of the network, the the number of tasks it has, it is able to solve successively.",
            "Increases an each of these tasks are basically.",
            "More complex than the other."
        ],
        [
            "Late so.",
            "That that was basically the first version of memory network that we proposed an.",
            "As I said, it's basically a fully supervised version.",
            "Which means that during training you are in addition to the question answer in the context, you're also given a supporting fact, so let's look a little deeper into what's happening here, right?",
            "Basically, we said that in the first step we embed or yeah, let's call this embed.",
            "We embed the story into the memories.",
            "And we also embed the given question into the memory, right?",
            "And then what we do is we match this question representation with all the slots in the memory to get these scores.",
            "Like these so called relevant scores.",
            "Now think of the relevant scores like.",
            "Think of these scores as how relevant each of these guys is to your current question, right or?",
            "And then you're given a supporting fact.",
            "And basically you take that.",
            "You take the slot corresponding to the correct supporting facts and what you want is to increase.",
            "To retrieve this memory and to increase the score of this supporting fact, an decrease the score of each of the other guys.",
            "Right so.",
            "And basically this is like a notion of hard attention.",
            "Right, so each of these cores can be thought of as nothing but some sort of yeah.",
            "Sorry.",
            "So as I said in the fully supervised setting during training, you do have that right.",
            "So not during test.",
            "So.",
            "Yeah, so so.",
            "Basically each of these scores, as I said, can be represented as some sort of like how relevant that slot is to your current current set of questions.",
            "Right, and can be interpreted as some sort of in attention score if you want.",
            "Like how well you are confident that you really want to attend to that part of the memory.",
            "And then given the correct supporting fact, your ascentia Lee know that this score has to be higher and you just pick that memory an push the score higher Ann and decrease the other scores.",
            "So that that's basically the idea of what, yeah?",
            "Yes.",
            "Yes.",
            "Why?",
            "Yeah, so bot finding task is basically.",
            "Like you're given a sequence of events.",
            "Basically not really adventurous sensually given a layout of a sequence of rooms if you want.",
            "Like bathroom is not of the kitchen.",
            "Bedroom is South of the bathroom and so and so forth.",
            "So that's the set of sentences you're given.",
            "And the question you are asked is how do I go from bathroom to the kitchen?",
            "Right, so essentially I mean, it's inherently a hard task bar because it really requires a lot of planning and search.",
            "Write an and sure enough, memory networks, at least the first version, are really bad at that.",
            "Or for that matter, like like search is hard."
        ],
        [
            "Yeah, so so as I said, as most of you have been sort of implicitly complaining via questions is what the hell are we doing with this full, fully supervised like we don't really know during training time what the supporting factors, right?",
            "That's certainly not a natural scenario in a natural scenario.",
            "All you're given is a question or story and you want.",
            "You essentially want an answer, or in other words, you want to learn which part of the memory is relevant to you, even during training time, right?"
        ],
        [
            "So sure enough, the second model, which sort of tries to answer those questions, is what we call an end to end memory network, where the.",
            "Where in this architecture the primary difference is that you're not given the initial supporting fact.",
            "And it actually learns the part of the memory which are relevant even during training an this is achieved using this so called soft attention."
        ],
        [
            "Like again by soft and hard.",
            "I mean in this in the hard case you since you were given what the correct memory is, which will correspond to the answer, you just pick that single memory.",
            "We"
        ],
        [
            "As in the case of soft soft, you'll probably have some sort of a score over your entire memory.",
            "Annual do some sort of a weighted combination, an stuff like that.",
            "So again, like if attention is not super clear, I can definitely talk to you guys offline and plus you'll hear a lot more about it in the in the in tomorrow stop.",
            "So and the cool thing about this is that the whole architecture is end to end differentiable an it only needs supervision at the final output."
        ],
        [
            "So here's the architecture like the this is the single layer.",
            "So what you have is a question.",
            "As usual, you embed the question in whichever way you want.",
            "I mean, again, bag of words output of an RNN, blah blah.",
            "Let's denote it by U.",
            "And also you have your collection of sentences from the story which are embedded in your memory.",
            "Again, whichever way you want.",
            "You sort of match your U with these embeddings amyes to get the Scorpii where P is basically a softmax over your score the scoring vector.",
            "So way to interpret the values of Pi is basically how how confident are you that this memory that the it's memory slot is.",
            "Is relevant to your question.",
            "Right and then you have the same set of sentence is also embedded in this so called output memory.",
            "Again, it's free for you to choose, you could might as well share the parameters of the output and the input, in which case will be the same matrix.",
            "If you want, you can keep it different, but again like that's more parameters in a model to learn, but whatever.",
            "So what you do is you use the peas and the output of your and your collection of output memories to get your final output, which is basically a weighted sum of these memories weighted by the likelihood that.",
            "That that that you were confident that the Earth memory is your relevant memory.",
            "So that's your output.",
            "And finally you combine your oh with your question representation U and pass it through some other parametric function to give you an answer.",
            "Right, so if you look carefully at this architecture, this is basically a feedforward net.",
            "I mean, you're going this way all the way up, and then you compute the loss you go back.",
            "And during the process you're learning, you're use.",
            "You're learning your memories and you're learning the scoring.",
            "The matching scoring function so that peas are such that it picks up the relevant memory.",
            "And of course, you're learning the final parametric function to give you the answer.",
            "So that's the end to end bit.",
            "Is it clear?",
            "Yeah.",
            "Yes, you is the question right now.",
            "Yeah.",
            "Again, it's just a dot product here.",
            "It could be anything you could have a neural network, I mean, as long as.",
            "Yes, over the memories, so that's the soft bit of the attention.",
            "Since you're getting like a distribution of your memories."
        ],
        [
            "So the cool thing about this is you can iterate.",
            "Where this is what I showed you in the previous slide, you have your question vector.",
            "You compute the output vector.",
            "You could combine the output vector with the previous question vector again and do an iterate.",
            "Right?",
            "And keep going up until you want to go an each of these steps is called a hop and then you get a final output vector which you combine with the final question vector to generate an answer.",
            "So I mean you need this iteration again because of the fact that this will only select.",
            "11 fact, although it will be in a soft manner, but it will only select one fact.",
            "But you probably want to select multiple facts.",
            "And now if you look at it, basically this is nothing but an RNN turned sideways.",
            "Right, like if these matrices are shared?",
            "Then this is nothing but an RNN, which is turned on on its side.",
            "Except for the fact that there is no limitation on the size of your memory.",
            "You're actually selectively picking memory or the bits of your RNN or your recurrent matrix which are most relevant to you for that time step.",
            "Right?",
            "So that's the multiple hop end to end memory network.",
            "So here yeah.",
            "I see you are reasoning or mental.",
            "Yeah, but.",
            "So this is just making that thing explicit like the the single case almost always has the likelihood of picking only one slot.",
            "In practice.",
            "Sure, I mean so think of it right.",
            "Like consider that sequential statement John went to the playground.",
            "John dropped the ball and your question is where is?",
            "Where is the ball?",
            "Right, so there in order to answer, where is the ball you really need to have some sort of a sequential processing.",
            "First you find out where the ball is.",
            "Then you could then you connect that ball with Jaune.",
            "Write an in."
        ],
        [
            "A single case, the softmax, is taken in dependently over all the memories.",
            "Right, so the hope is that in the first case you will pick one element of the sequence and then use that element to attend to the second element.",
            "Yep.",
            "Yes.",
            "Maybe a good idea for them to consider correlations between that."
        ],
        [
            "Whereas I just explained that's where the multiple hop things come in right?",
            "I mean where.",
            "Essentially you're tying your previous attention with the newer attention.",
            "Uh."
        ],
        [
            "Yeah.",
            "Bob likes Donuts.",
            "Library.",
            "We don't.",
            "That's that's that's sort of 1 drawback.",
            "I mean there are quite a few drawbacks, which I'll talk about later.",
            "No, I'm sorry, I thought you said how do we deal with multiple answers or like like?",
            "Like a list of answers.",
            "So I mean, right now we just pretend that a list of answer is a single answer.",
            "So if like if there are key answers, then you have like lots of potential answers and then we try to train.",
            "So that's obviously not the best, but that's that's hard.",
            "That's really hard.",
            "Yep.",
            "Separate input output.",
            "Oh, so it will be clearer later on where basically we.",
            "Let's"
        ],
        [
            "So I mean, the motivation is that the input memories are trained.",
            "To generate your attention scores and the output matrix is trained to predict the answer given the attention scores.",
            "Right in some sense.",
            "The hope is that there will be some sort of a communication between the two.",
            "That said, in most of our experiments we shared them, but.",
            "That was the motivation.",
            "Because this is, this never sees the final answer right.",
            "This set of memories never see the final answer.",
            "And also I'll talk about later in this so called key value.",
            "Pair, key value, memory networks.",
            "I don't know whether I'll be able to talk because it's I'm fairly late here, but the idea is this bit is used primarily for adressing.",
            "Which part of the memory is relevant and this bit is used to actually given that address.",
            "This bit is used to predict the answer.",
            "So you could store a different.",
            "Things here and a different thing in the corresponding ad slot in the output memory, which is more predicted to the answer.",
            "I think I should move ahead because there's a lot to go.",
            "So."
        ],
        [
            "Yeah, so so.",
            "As I said, the architectures were fairly flexible.",
            "You could use bag of words.",
            "You could use RNN style reading of words or characters to embed your questions and statements into the memory and things like that."
        ],
        [
            "And this will give you the set of experiments on the previous task.",
            "Ma'am and two NR again far better than LS teams, but as expected not as well as a fully supervised memory networks.",
            "Because of course there's less supervision.",
            "But still.",
            "I mean, they're pretty good.",
            "But like, but still, as I said, like being not fully supervised.",
            "There are some tasks where it fails compared to the full supervision."
        ],
        [
            "And just to answer your question, why did we need multiple hops?",
            "Because consider this basic induction task right.",
            "Brian is a frog Lily's grave brighness yellow Julius is green.",
            "Greg is a frog.",
            "What color is Greg?",
            "So first you need to know what basically what Greg is.",
            "So it attends in the first hop on what Greg is basically now it finds out it's OK, it's a frog.",
            "And then it ends what Brian is like.",
            "Who else is a frog?",
            "He says, OK, fine, Brian is the frog, so it retains that.",
            "And then it says Brian is yellow, so that's the third of where it is.",
            "So it needs that sequential reasoning.",
            "And of course it gives the correct answer.",
            "So.",
            "And likewise on other tasks, right?"
        ],
        [
            "So that was the sort of the performance on the so called baby tasks.",
            "But as I said, it's basically an RNN turned on its side, so you might as well do language modeling and see whether it works or not.",
            "Late and basically the.",
            "The setup for the language modeling form eminens is.",
            "You're given a sequence of words.",
            "Say the previous 100 words used or each word in a memory slot.",
            "And you start with a null question vector.",
            "And then you do multiple hops.",
            "Basically another question vector.",
            "You do one hop, you get something you concatenate with the question vector, you do another hop, and so and so forth.",
            "Finally you get after what six or seven hops you get some feature vector.",
            "Then you use that feature vector to predict the next word.",
            "Right so.",
            "Sort of putting this in RN in terms what RN is doing in.",
            "Like if you're aware or like how the language modeling that is done in RNN, you're given the current word.",
            "You do a single hop and produce the next word.",
            "Here the flexibility is that you can basically look at your context multiple times to generate your next word.",
            "An on standard datasets, namely Penn, Treebank and Text 8, which is a subset of Wikipedia, we show that.",
            "It performs fairly similar to LS teams.",
            "Which is the state of the art.",
            "Number.",
            "Yes we did I mean not in this experiment, but for the baby task, yes.",
            "For the baby yes yeah.",
            "As I said, like by we control the adoption by the.",
            "Ability of the model to fetch the null memory once it fetches the null memory, we stopped."
        ],
        [
            "Uh.",
            "So yeah, so so the conclusion from this experiment is that, at least for language modeling, we may not really need long term dependencies.",
            "Becausw quite often in the case of language modeling, mostly it's like like common words that are that you're predicting an you're computing the entropy over, but maybe once you're doing more.",
            "Once you evaluate on nouns or entities, you might need long term dependencies, right?",
            "Which is highly dependent on the."
        ],
        [
            "You're reading.",
            "So just to give you a brief overview of literature as you mention that recently there's been a lot of work in these attention based on memory based models for numerous applications like.",
            "Solution I mean for generating sort of strokes of characters.",
            "And then there's also the neural Turing machine, which I'll talk about.",
            "Given that, I'm assuming I have that I'm an, then there was also work on stack based memories.",
            "Which was which is valuable."
        ],
        [
            "Let's move a bit quicker because there's a lot to cover.",
            "So then then as I said, like I started off motivating how to deal with context, which is super large right?",
            "When suppose you have a knowledge base which has millions and millions of facts.",
            "So the cool thing about memory network."
        ],
        [
            "Again, is that.",
            "The architecture is fairly flexible.",
            "I never really mentioned how we are storing the memory.",
            "I mean, we can actually do some intelligence while storing the memory.",
            "In particular, you could hash the memories.",
            "You could hash the memories based on the words in the statement in the sense.",
            "Prince"
        ],
        [
            "In the previous case, all the memories corresponding to Shaolin Soccer would be in one bucket.",
            "All the memory scope corresponding to the God of cookery will be in another bucket, and."
        ],
        [
            "And so forth, right?",
            "Because if you don't hash them, then there's like a million memory slots and it's kind of hard to do inference over.",
            "Right, and the hashing could not only be with respect to words, but also could.",
            "It could be with respect to the embeddings of the words."
        ],
        [
            "And so and so forth and.",
            "Via this we tried the model on actual question answering data set which has about 14,000,000 facts which are stored in basically 14,000,000 memory slots.",
            "And the statements are fairly diverse.",
            "Basically you have stuff like this and you have stuff like.",
            "Like factual stuff, when you have stuff like I mean reasoning based.",
            "Anne."
        ],
        [
            "Yeah, and then basically we showed that meman ends.",
            "With some fancy featurization basically gives you the state of the art, even on the large scale data set."
        ],
        [
            "An as I said, so without hashing the, you're basically.",
            "You have around 14,000,000 memory slots.",
            "It gives you the best performance, but it's extremely slow at inference time because you really need to score 14 million slots and pick the maximum.",
            "So if you hash by words or by clusters you degrade in performance, but it's like significantly faster.",
            "So specially with clustering you don't really degrade in performance much, but it's.",
            "Way faster."
        ],
        [
            "And then we also tried multitasking by using the real world question, answering data set described in the previous slide an the baby tasks and then at Test time we gave this cooked up story.",
            "And it could answer questions very in an interesting manner like some like simple factual questions like where is the story based?",
            "Question like where is the milk and it says office and will answer questions like where is the milk come from?",
            "It comes from cow but in some cases are like the answer nonsensical as well.",
            "So.",
            "Yeah, so so.",
            "I guess my point over here is that the architecture is fairly flexible that what goes inside the memory is absolutely.",
            "Not."
        ],
        [
            "Addictive so another thing I want to go into detail is this so called closee style question answering so the motivation is you want to build machines that can actually understand language.",
            "Late basically and one way to sort of know that a machine is understanding language is making it reader comprehension an ass.",
            "Making it answer questions that pertain only to the comprehension, or in other words, the machine should not be able to answer questions based on some outside knowledge, so you want to restrict your questions in such a way that the answer exists.",
            "In this story, I think it was so an Unfortunately there was no data set until recently that did this.",
            "No large scale data set, in fact, which you could use to train.",
            "I mean there is a small data set like MMC test and stuff, but it's really small and you can only use it for testing.",
            "So yeah, so I think this is an interesting problem.",
            "An recently there's been a couple of works in this direction, one from our lab and one from Deep Mind, and where.",
            "We've built the data set an models that does this and they don't do a perfect job, but I think it's a step in the right direction."
        ],
        [
            "So in particular we released this children's book data set where we took basically 118 books which were freely available, an used snippets from this book to generate our comprehension an the question in particular, we took the random 20 sentences from the book.",
            "From anywhere that's the context and the 21st sentence is your question."
        ],
        [
            "An in that question where we basically remove a random word.",
            "And the ask here is, given this story, fill in the blank.",
            "And as part of our training, we also provide a set of candidates to make things a little easier for the machine.",
            "An yeah so so so the thing here is that you really want to remove the words which are not.",
            "You really want to remove the words in a way so that simple models like language modeling type models do are not able to answer these right.",
            "In particular, the interesting set of words to remove from here would be entities that only appear here, or the nouns that only appear here and so and so forth."
        ],
        [
            "Right, and the model is as usual.",
            "Basically have the story you store the story in the memory you have the question without with the blank you represent this in some way and you score the memories an basically evaluator.",
            "Finally, you scored the candidates an the novelty here."
        ],
        [
            "Is basically how we stored the memories.",
            "You could store the entire sentence, which we'll see in a bit was pretty bad.",
            "You could store wordwise like each.",
            "Each word has its own slot, which is also bad.",
            "The best thing that worked was basically store windows."
        ],
        [
            "Words.",
            "An basically these are the results as you'll see like language.",
            "So we split the columns in by the category of words we removed like either the entity or common nouns or verbs or prepositions.",
            "So as you can see models like STM which essentially run through the whole story and predict the final answer after the story in the question.",
            "They do pretty well adverbs and prepositions, because that's what language models do, but they are fairly bad at.",
            "Basically predicting the entities an nouns which are highly story dependent.",
            "Humans, by their do a very good job.",
            "Which apparently was not evident from the.",
            "Earlier on in my talk, but it's OK.",
            "So, so these humans were basically annotators from Facebook so.",
            "We showed them the story and they were given plenty of time to answer them."
        ],
        [
            "Only, that's why they're doing good job.",
            "And men's are not there yet, but yeah."
        ],
        [
            "So what I'll think about cell?"
        ],
        [
            "Supervision for now.",
            "And we tried the same model on the data set from DeepMind, which as I said is another very interesting work in this direction where basically they.",
            "They use the news articles from CNN and Daily Mail.",
            "Probably you'll hear a lot more about this tomorrow as well an each of these news articles comes with a summary sentence is like snippets from the article and what they do is again from that summary.",
            "They remove one word an.",
            "Then the setup is the same that you want to predict the word given the news article and that summary."
        ],
        [
            "So this is what it looks like.",
            "You're given the article you are given the query where you remove some word and to make the task harder so that language models fail.",
            "They actually anonymize these entities.",
            "So you don't really learn the model.",
            "the BBC.",
            "You basically learn the model, the blah, and that entity.",
            "This entity 381 will never appear anywhere else in the data.",
            "Right, so you can't really learn though BC so."
        ],
        [
            "And yeah, so here again.",
            "Basically mem ends do a decent job until recently where there were other results, which sort of are better than this."
        ],
        [
            "Then as I said, this dialogue modeling, probably I'll skip dialogue modeling becausw."
        ],
        [
            "The idea is similar like we released another data set in this direction where you could do question answering over."
        ],
        [
            "Movies recommendation?"
        ],
        [
            "How are movies combining question answer?"
        ],
        [
            "In movie recommendation and also a subset of the Reddit data which does."
        ],
        [
            "Free freeform dialogue modeling here, basically the.",
            "They call the key point is.",
            "Basically you have two types of memories.",
            "One is this long term memory which is essentially storing the background.",
            "The knowledge base, and then there's a short term memory which pertains to the dialogue that the model is having with the with the user.",
            "So you really need to combine this short term memory with this long term memory to answer questions that the user has asked.",
            "So details are in the page."
        ],
        [
            "Sure, but I'll probably speed up."
        ],
        [
            "Lastly, as I said, there was this key value memory networks."
        ],
        [
            "At.",
            "As I said before, the two matrices need not store the same thing.",
            "You could repres."
        ],
        [
            "And your triple the knowledge base.",
            "Triple, which is basically subject relation object as a key value pair where you define the key as subject in a relation and the value is your object because in the end if somebody asks you a question relating to subject in relation, you really want the object to help you in answering and not the whole."
        ],
        [
            "So you could store a different thing here.",
            "Different thing here an you use this as your addressing mechanism to attention to focus towards which.",
            "Which object is correct and then use that addressing mechanism to?",
            "Fetch the actual object and use that to answer.",
            "So this is fairly recent actually, so I mean."
        ],
        [
            "Anne."
        ],
        [
            "So it does what it does, and then there was another paper called Dynamic Memory Networks which sort of emphasizes my point, which I've said that multiple times that memory network architecture is fairly flexible, so this dynamic memory and what they do is.",
            "They really basically hammer the memory networks with recommend networks everywhere.",
            "In"
        ],
        [
            "Regular, this is what the intimidating figure looks like.",
            "You have a question.",
            "You represent it with an RNN.",
            "You have the statements which also are represented by an RNN.",
            "Well, the output of the RNN and then given the question you sort of score these representation to generate.",
            "What would you call these episodes?",
            "And that episode Vector is combined with this so called internal memory vector.",
            "Again, why an RNN to generate the next memory vector?",
            "Uh.",
            "And you iterate.",
            "Using the current memory vector, the current question you iterate to find another episode, an plug that into the RNN, and so and so forth.",
            "Finally, you get the final.",
            "Representation which you use to generate your answer.",
            "So like if you think carefully, it's basically the same architecture but with.",
            "A lot more machinery into how you want to encode your data.",
            "Right?"
        ],
        [
            "Yeah, so so."
        ],
        [
            "And then basically they do reasonably well.",
            "With such heavy machinery on the baby data set."
        ],
        [
            "Right, so I will and then they apply this to task like sentiment and."
        ],
        [
            "This is an power, speech tagging and stuff like that, so I mean, I'm happy to discuss these offline after the talk."
        ],
        [
            "You guys are interested.",
            "So just to summarize.",
            "So basically memory networks are basically deep learning models that that AUGMENT external memory which is readable and writable.",
            "Anne, these memories are learned an are effectively used for reasoning tasks for language, understanding tasks and stuff like that.",
            "And the architecture is quite flexible as you know by now.",
            "To basically play around with stuff."
        ],
        [
            "The shortcomings are with the memory network model that I proposed, which I also spoke a bit about earlier is that we don't really write into the memories intelligently.",
            "Right in the sense we do not erase any memory.",
            "If we have a certain number of slots of memory.",
            "Once the slots get full, we cycle.",
            "And we cannot compress memories like if you have like, some know some long-term knowledge, there's no way to.",
            "Efficiently encoded so that later on it can be used in a.",
            "An efficient manner, it's just that every knowledge is treated the same.",
            "How recent or long term back it?"
        ],
        [
            "So neural Turing machines, which I will talk briefly in 5 minutes, is basically an answer to some of the shortcomings of a memory network where the architecture is as I said, the same.",
            "The primary differences in the way it writes into the memory."
        ],
        [
            "Right, so the reading better is very similar.",
            "Basically you have a weight vector which is computed via your controller at time T. And at this rate is like your attention vector.",
            "And then for reading bit, all you're doing is taking all your memories an.",
            "Weighting them by the weight vector to get a final read vector output as output into your controller, which again which the controller will use for your final object for final task."
        ],
        [
            "The right mechanism is a little more complicated where basically you have the weight vector generated by the right module as before.",
            "Then it also generates two additional vectors, the erase and the ad vector.",
            "So the first step in the right module, what the right head does, is basically for every memory slot I.",
            "It multiplies it by this factor.",
            "So what Ascentia Lee this is saying is if the weight vector at I is 1.",
            "And it is vector is also one, then this memory slot is 0.",
            "So your whole your current memory slot then becomes zero like you erase your memory.",
            "If the weight vector is 0 then you keep the previous memory and then in the second step with this M~ I had.",
            "You essentially add your ad vector, again, weighted by that, that that set of weights.",
            "So it's like a two step process where in the first step it has erased the part of the memory that it wanted to an added the new information.",
            "In the second step.",
            "Right, so yeah, so the question here is how do we compute the weight vectors?"
        ],
        [
            "The cool thing about neural Turing machines is that the weight vectors are computed in a way that basically allows you to.",
            "Not just read or write at the next memory slot, but anywhere in your memory.",
            "Module and that's achieved by combining the so called content based addressing and the location based ad dressing."
        ],
        [
            "So the content based our dressing is basically the usual stuff that we've been we've seen so far.",
            "Basically you have the output of your controller, Katie.",
            "Which could be your question.",
            "For instance, an you have the memory.",
            "Then you're essentially scoring each of your memories via the softmax.",
            "That gives you the weights of basically the content based scoring.",
            "Anne."
        ],
        [
            "The location based coding is a little more involved than it.",
            "Most three steps.",
            "The first step involves interpolating the previously computed content based scoring.",
            "The content based weighting in the previous slide with the previous weights by through some vector GT.",
            "Again, so this GT is the vector that your controller MX.",
            "And the second thing is convolving this WTG with some shift vector.",
            "So again, this shift vector is also the output of your controller.",
            "Which essentially what this does if you look carefully is like if you allow shifts, say.",
            "Ascentia Lee all it's doing is shifting the weights from Kate from The Jets location to its location if you want.",
            "Right?",
            "And Lastly, since it's a convolution operation, an if the shift vector is basically smooth, then what this ends up doing is blurring the weight vector.",
            "So you probably need to sharpen it as well.",
            "An so in short, what these three steps lead 2 is?",
            "A set of weight vectors I for the right module that could.",
            "Effectively right into.",
            "Random memory slot, which probably had already written to.",
            "Or it could be a new slot overtime."
        ],
        [
            "So, so that's that's pretty much it as far as the neural Turing machines is concerned.",
            "An they original paper used these models on simple tasks like copy, where you're given a sequence of vectors.",
            "The machine is expected to read the sequence stored sequence in the memory, an sort of remember the sequence by generating it back again.",
            "So you can see that the LS teams take quite awhile, so if these are the number of epochs on the X axis.",
            "So it takes quite awhile to copy the.",
            "Even short sequences, I think this was tried.",
            "This graph is for a sequence of length 20 I believe."
        ],
        [
            "Right?"
        ],
        [
            "An the repeat copy is, as the name suggests, is.",
            "Basically, you read the sequence.",
            "Once an repeated K times.",
            "And here again, I mean the neural Turing machines basically converge a lot faster and whereas in the case of LSD pretty much fails when it for large values of North being the number of times you want to repeat."
        ],
        [
            "Right, and so the sorting experiment was another one where basically a given a collection of vectors, an set of priorities for the vectors, and you want to sort the vectors in the order of priority.",
            "So here again, basically neural Turing machines seem to successfully do that."
        ],
        [
            "And whereas LSD comes pretty much failed because, like the entropy of The thing is quite high."
        ],
        [
            "So yeah, so to summarize, a neural Turing machines basically again sort of deep network augmented with memory where you can write into the memory much more intelligently that than memory networks.",
            "An unfortunately though, so far it has only had only been used for toy problems like copy blah blah, and it's not.",
            "It was not scalable, so until recently there's been some work.",
            "Your shows Labahn, also probably another one from Deep Mind where they've used it on.",
            "On the baby tasks, at least ensure that the neural Turing machines do work there."
        ],
        [
            "And stack augmented R and ends.",
            "I will skip becausw you."
        ],
        [
            "We'll hear."
        ],
        [
            "A lot more."
        ],
        [
            "More detail."
        ],
        [
            "Label them.",
            "Tomorrow and so, just to wrap up.",
            "So we discussed memory networks, which is basically and we discussed the Turing machines, not so much the stack augmented RNS, and we sort of gave you a quick.",
            "Overview of attention both the soft and hard version an but that's I think one of the cool things that has come out."
        ],
        [
            "Recently but still, this is quite a bit larger, lacking as far as these models are concerned, like we don't yet know what to write and what not to write.",
            "I mean, if you read a story, there may be so much of stuff that probably for us as humans.",
            "If we read the story is sort of irrelevant an you really do want to keep that in your memory.",
            "Only the selective bits.",
            "An what type of memory to use?",
            "Probably we have some answers in this direction, but not so much.",
            "How to forget how to compress the old knowledge we don't yet know an basically how to build hierarchical memories, where basically you can do multi scale attention.",
            "For instance an how to compose.",
            "Sort of learn intelligent composition functions for hierarchical reasoning and stuff like that.",
            "Those are things that are still unanswered, Ann.",
            "Basically open questions.",
            "So yeah, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As the title says, I'll be talking about reasoning, attention and memory so.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I don't know like just to give you a bit of a story like as a.",
                    "label": 0
                },
                {
                    "sent": "Not as as old as your show, but I've been working in deep learning since.",
                    "label": 0
                },
                {
                    "sent": "Since I started my PhD in 2003 and I attended a couple of machine learning summer schools when I was a student an.",
                    "label": 0
                },
                {
                    "sent": "Used to happen in University of Toronto, then again, uh, sponsored by cipher.",
                    "label": 0
                },
                {
                    "sent": "An room used to be full.",
                    "label": 0
                },
                {
                    "sent": "Like as it is right now, but only composed of these two rows.",
                    "label": 0
                },
                {
                    "sent": "So it's I mean it's fascinating to see how many how the field has taken up so much so fast and so much has been done already in two years, which looks like I mean eternity.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so as the topic of my talk as the title says, is reasoning attention and memory, where you're sure gave a really brief snapshot of things like memory, network, attention based models, etc etc which have recently basically come into the limelight and they seem to show a lot of promise in a lot of interesting, challenging problems so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you a bit of a motivation, deep learning as we know is pretty successful at Vision an you see the basic architecture might look something like this, where you have a convolutional network or some derivative of this network.",
                    "label": 0
                },
                {
                    "sent": "You pass in an input image an.",
                    "label": 0
                },
                {
                    "sent": "Which goes through a bunch of hierarchical feature transforms and in the end you get like a feature vector on top of which you plug a classifier for your final objective.",
                    "label": 0
                },
                {
                    "sent": "And the final objective could be object detection.",
                    "label": 0
                },
                {
                    "sent": "It could be object recognition and stuff like that and deep learning models we know are the state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And something similar has lately happened in speech as well, where basically you have an acoustic signal which sort of is sampled at some frequency.",
                    "label": 0
                },
                {
                    "sent": "To get your input spectrogram, which can be viewed as a 2 dimensional image and that image is plugged into again a deep network and the network essentially predicts the phones at every frame and then you can plug your language model and stuff like that to stitch these phones.",
                    "label": 0
                },
                {
                    "sent": "The predictions of the.",
                    "label": 0
                },
                {
                    "sent": "Work, and finally what you get is the.",
                    "label": 0
                },
                {
                    "sent": "Outputs intense that has been spoken and again like models like these I mean of course like what goes in here.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into the detail, but the models like these are sort of.",
                    "label": 0
                },
                {
                    "sent": "Almost state of the art in speech as well and so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something similar has happened in the domain of text, although it's still a little it still catching up where basically you have an input, you feature riser and put in some way an you have some deep network which could be a feedforward net or recurrent net or like an LSD emoji.",
                    "label": 0
                },
                {
                    "sent": "Are you an?",
                    "label": 0
                },
                {
                    "sent": "In the end you have some objective like some task at hand.",
                    "label": 0
                },
                {
                    "sent": "In this case I've shown a sentiment analysis task with given a sentence you want to say whether.",
                    "label": 0
                },
                {
                    "sent": "The sentiment of the sentence is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "Or it could be any task.",
                    "label": 0
                },
                {
                    "sent": "Any standard NLP task?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so so the question is, is this enough?",
                    "label": 0
                },
                {
                    "sent": "Well before that.",
                    "label": 0
                },
                {
                    "sent": "So the general basically formulation is you have an input.",
                    "label": 0
                },
                {
                    "sent": "You represent it in some way.",
                    "label": 0
                },
                {
                    "sent": "You have an encoder or the feature extractor which extracts the features and then you plug in a classifier or regressor which we call a decoder and you optimize the whole thing using some loss function, write an.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The question here is, is this enough basically right?",
                    "label": 0
                },
                {
                    "sent": "So let me give you a couple of scenarios where.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "We'll see whether it's enough or not, so suppose you're given a story like a really simple story of an actor called Joanne, maybe Fred, and they do a whole bunch of tasks like moving around, picking up objects, etc.",
                    "label": 0
                },
                {
                    "sent": "Like Joe went to the kitchen, Fred went to the kitchen.",
                    "label": 1
                },
                {
                    "sent": "Joe picked up the milk, do travel to the office, Joe left the milk and blah blah.",
                    "label": 1
                },
                {
                    "sent": "Now if I ask any human to answer these questions, where is the milk?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not hard to sort of infer from this story where the milk is probably, as a human you'll see in the story where the word milk appears, and you know that the Joe had the had left the milk.",
                    "label": 1
                },
                {
                    "sent": "And where did Joe leave the milk?",
                    "label": 1
                },
                {
                    "sent": "Basically, Joe travel to the office in the last in previously, before leaving the milk.",
                    "label": 0
                },
                {
                    "sent": "So highly likely the milk is in the office.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it is an likewise aware is Joe.",
                    "label": 0
                },
                {
                    "sent": "You can infer from the storage in the bathroom an where Joe before office.",
                    "label": 0
                },
                {
                    "sent": "And again you can infer that he was in the kitchen.",
                    "label": 0
                },
                {
                    "sent": "So like tasks like this are fairly easy for a human too.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider scenario two and this is like a sort of an exercise for you guys.",
                    "label": 0
                },
                {
                    "sent": "Basically, you're given a story of.",
                    "label": 0
                },
                {
                    "sent": "2020 sentences, like some snapshot of taken from some random book and but these 20 sentences are sequential and I show you the 21st sentence with one of the words missing.",
                    "label": 0
                },
                {
                    "sent": "So I give you one minute to read the story and let me know the answer is.",
                    "label": 0
                },
                {
                    "sent": "Right, so how many of you think the answer is Mr. Cropper?",
                    "label": 0
                },
                {
                    "sent": "How many think the answer is Mr Baxter?",
                    "label": 0
                },
                {
                    "sent": "OK, that was pretty bad.",
                    "label": 0
                },
                {
                    "sent": "Come on, this is like an exam.",
                    "label": 0
                },
                {
                    "sent": "OK, well let me give you the answer.",
                    "label": 0
                },
                {
                    "sent": "The answer is Mr. Baxter.",
                    "label": 0
                },
                {
                    "sent": "Right anyway, so maybe not under so much stress.",
                    "label": 0
                },
                {
                    "sent": "If I had given you enough time, you would have read the story and probably with a very high likelihood you would have answered this correctly.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Write an scenario #3.",
                    "label": 0
                },
                {
                    "sent": "Now suppose you are given a whole knowledge base, a knowledge base of facts based basically as a set of triples where you have an entity, some action, an sorry, some relation an another entity.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have millions of these facts, so you want to store them and then you want to ask questions like who wrote?",
                    "label": 0
                },
                {
                    "sent": "Kung Fu hustle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or even more complicated questions like I'm interested in watching a Stephen Chow movie.",
                    "label": 1
                },
                {
                    "sent": "Other income ferstl, can you suggest something?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An even like yet another scenario of dialogue modeling where you have a user who has and who is chatting with the bot.",
                    "label": 0
                },
                {
                    "sent": "An user has an intent of making a reservation in a French restaurant of table of six an.",
                    "label": 0
                },
                {
                    "sent": "Basically the restaurant should be sort of a high end and then the bot indulges in some sort of an interaction, like where the location should be and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And at some point the user wants to revise his or her preferences.",
                    "label": 0
                },
                {
                    "sent": "Of going from 6 to maybe 4 and then the bot needs to keep track of whatever else it had.",
                    "label": 0
                },
                {
                    "sent": "Sort of known about the users intent and then modify subsequently and finally make a reservation the way a normal human would do.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Late so basically the scenarios like this for scenarios are shown so far cannot really be modeled as an input output mapping from X to Y, where X is your input potentially or stories and a question F of W is basically your standard deepnet some parametric model and you want to.",
                    "label": 0
                },
                {
                    "sent": "So sort of give the right answer.",
                    "label": 0
                },
                {
                    "sent": "Basically then this sort of a need to have some sort of an external memory.",
                    "label": 0
                },
                {
                    "sent": "That sort of external persistent memory that remembers the context right?",
                    "label": 0
                },
                {
                    "sent": "And given this memory, which could potentially be huge, your model should know how to look like given a question.",
                    "label": 0
                },
                {
                    "sent": "The old model should know how to look, or rather where to look into this memory.",
                    "label": 0
                },
                {
                    "sent": "And not only that, it should know what to look for in the memory and given a short list of memory slots that your model might figure out where to look at, the model needs to reason.",
                    "label": 1
                },
                {
                    "sent": "Basically, in the example in the toy example of Joe going to the office and dropping the milk, I mean the model needs to know what does going to the office mean and what does dropping the milk mean, and so and so forth.",
                    "label": 1
                },
                {
                    "sent": "An it essentially needs to handle like like.",
                    "label": 0
                },
                {
                    "sent": "This context is huge.",
                    "label": 0
                },
                {
                    "sent": "And it could potentially be changing.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so in short, we really do need some sort of a memory that is augmented to this function F of W, which can enable answer.",
                    "label": 0
                },
                {
                    "sent": "In these complicated scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one might argue, given the previous talk from your show about RNS, that the hidden states of RNN, particularly LTM or grew.",
                    "label": 1
                },
                {
                    "sent": "Are effectively memory, so why not just simply run an RNN on your external context and get some sort of a representation of your story and use that representation?",
                    "label": 1
                },
                {
                    "sent": "To map questions to answers.",
                    "label": 0
                },
                {
                    "sent": "Or responses, but clearly this will not scale, as you're sure mentioned, because of primarily 2 problems, one is vanishing gradient an.",
                    "label": 0
                },
                {
                    "sent": "Extending exploring variant, but that can be handled, but primarily is the vanishing gradient, or in other words, RNS don't really have the ability to capture really long term dependencies, like if I give you knowledge base of 1,000,000 facts, there's no way an RNN can hold all the facts in its memory.",
                    "label": 0
                },
                {
                    "sent": "One because of the vanishing gradients and two because the state of the memory is like really small.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if it's like 1000 hidden units, there's no way you can expect to store such rich information in those thousand hidden units and reason with that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so which brings me to the topic of my talk, basically.",
                    "label": 0
                },
                {
                    "sent": "Recently, in order to answer such questions, there have been a bunch of models that are proposed in this direction, namely memory networks, which I'll talk in quite a bit of detail.",
                    "label": 0
                },
                {
                    "sent": "And basically I'll go over various versions of these networks.",
                    "label": 0
                },
                {
                    "sent": "And then around the same time there was another sort of model that was both called neural Turing Machines, which does something similar, but it was shown to do.",
                    "label": 1
                },
                {
                    "sent": "Interesting toy tasks, an not until recently it has been applied to some essentially a real world problem here on and.",
                    "label": 0
                },
                {
                    "sent": "Then there is also another set of work which.",
                    "label": 0
                },
                {
                    "sent": "Which is somewhat old, but recently it has again seen resurgence, namely, basically augmenting your RNS with external memories like Stacks, list and queues.",
                    "label": 0
                },
                {
                    "sent": "So if time permits, I'll talk about the third set of models, but if not then it's OK because tomorrow probably you'll hear about them in quite a bit of detail.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so the general architecture of all these three classes of models.",
                    "label": 0
                },
                {
                    "sent": "Can look something like this.",
                    "label": 0
                },
                {
                    "sent": "Basically you have a controller which is think of a controller as like a standard deep network could be.",
                    "label": 0
                },
                {
                    "sent": "A feedforward could be an RNN network, could be anything.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Has multiple tasks, basically, not only, it produces the output Y, but it also controls your read and write head into the memory.",
                    "label": 1
                },
                {
                    "sent": "And then the main component is this memory bank which will store your context, the stories, the knowledge base etc etc.",
                    "label": 0
                },
                {
                    "sent": "Right, so the purpose of the head, the purpose of the right head, as the name suggests, is take given the input an essentially the output of the controller, it sort of decides which fact to right where.",
                    "label": 0
                },
                {
                    "sent": "And how like what is the purpose of the read head?",
                    "label": 0
                },
                {
                    "sent": "As the name suggests, is given the input X and the sort of input of the controller.",
                    "label": 0
                },
                {
                    "sent": "Which part of the memory to read and pass the information back to the controller and finally the controller wants given an input X it will.",
                    "label": 0
                },
                {
                    "sent": "Basically read some stuff in the memory, sorry, write some stuff in the memory, read it from the memory, combine it with input X and do some processing to finally generator output Y.",
                    "label": 0
                },
                {
                    "sent": "So all of this will become clearer with an example, but that's that's the basic architecture.",
                    "label": 0
                },
                {
                    "sent": "So what distinguishes the three architectures shown in this slide are basically what goes inside each of these components, like what sort of memory are we using?",
                    "label": 1
                },
                {
                    "sent": "Are we using a tape?",
                    "label": 0
                },
                {
                    "sent": "Are we using a stack Q, whatever?",
                    "label": 0
                },
                {
                    "sent": "And what are the functions are read in the right head an?",
                    "label": 0
                },
                {
                    "sent": "What exactly is a controller?",
                    "label": 0
                },
                {
                    "sent": "So memory networks is 1 version neural Turing Machines is another, and so and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so so basically.",
                    "label": 0
                },
                {
                    "sent": "Let's start the let's go deep into what a memory network looks like.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a class of models which combines.",
                    "label": 1
                },
                {
                    "sent": "As I said, large scale memory with a learning component an you can read and write into the memory, and it incorporates reasoning over the memory via this.",
                    "label": 1
                },
                {
                    "sent": "A notion of attention, so I'll talk briefly again about attention today, but you'll hear a lot more in detail in tomorrow's top.",
                    "label": 0
                },
                {
                    "sent": "And the cool thing about memory network is that the framework is flexible enough to store rich representations of your input right, and the model is scalable in the sense that the memory can potentially be massive.",
                    "label": 1
                },
                {
                    "sent": "And still you can efficiently do reasoning.",
                    "label": 0
                },
                {
                    "sent": "How.",
                    "label": 0
                },
                {
                    "sent": "Like again, I'll speak as over the course of my talk.",
                    "label": 0
                },
                {
                    "sent": "An yeah an Lastly again, as I said, like the whole specification of the memory network model is flexible that you can store.",
                    "label": 0
                },
                {
                    "sent": "Basically, long term memories, you can sure store short-term memories an A combination of these can help in.",
                    "label": 0
                },
                {
                    "sent": "Solving tasks like dialogue, modeling etc.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How so?",
                    "label": 0
                },
                {
                    "sent": "Just to give you a brief overview of what the processing steps of the memory network look like basically.",
                    "label": 0
                },
                {
                    "sent": "Given an input X, the first step is for the controller to convert the incoming input into some internal representation.",
                    "label": 0
                },
                {
                    "sent": "So in the original paper this was done.",
                    "label": 0
                },
                {
                    "sent": "This was denoted as the module I the input module.",
                    "label": 0
                },
                {
                    "sent": "And then the right head based the Step 2 involves the right head updating the memories and also writing the new.",
                    "label": 1
                },
                {
                    "sent": "Internal feature representation that you got from step one into the memory.",
                    "label": 0
                },
                {
                    "sent": "And the step three involves given this external input and.",
                    "label": 0
                },
                {
                    "sent": "The contents of your memory, the read head will read some portion of the memory and pass it on to the controller.",
                    "label": 1
                },
                {
                    "sent": "And finally, the Step 4 involves the controller doing the remaining work.",
                    "label": 0
                },
                {
                    "sent": "Basically it takes as input X.",
                    "label": 0
                },
                {
                    "sent": "It takes as input whatever the right head.",
                    "label": 0
                },
                {
                    "sent": "The read, read, read from the memory combines the two to finally answer your question.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Late so.",
                    "label": 0
                },
                {
                    "sent": "Let's be a bit more specific with respect to one simple example, which is similar to the story that I.",
                    "label": 1
                },
                {
                    "sent": "Proposal I told earlier.",
                    "label": 0
                },
                {
                    "sent": "Let's go into a bit more specifics using specific story, basically.",
                    "label": 0
                },
                {
                    "sent": "The model is given this simple story.",
                    "label": 0
                },
                {
                    "sent": "John was in the bathroom, Bob was in the office, blah blah blah.",
                    "label": 1
                },
                {
                    "sent": "This is what's called context, right?",
                    "label": 1
                },
                {
                    "sent": "And during training, what you're given is this question answer pair based on that story where the question is, where is John and the answer is kitchen, and in addition to that, in what the very first version of the memory networks which we call the fully supervised.",
                    "label": 0
                },
                {
                    "sent": "In addition to these two things during training you're also given this so called supporting fact in the story where basically it tells you which sentence in the story has your answer.",
                    "label": 1
                },
                {
                    "sent": "Or has some signal associated with your answer?",
                    "label": 0
                },
                {
                    "sent": "Of course, this supporting factor is not given to you during test time, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "So during testing I just given the context and the question and you want to generate the answer.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So step one of a memory network would involve storing the representation of the context.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What goes into the representation is entirely up to you.",
                    "label": 0
                },
                {
                    "sent": "Like there's no, there's no sort of restriction on what it could be like.",
                    "label": 0
                },
                {
                    "sent": "It could be individual words like you might want to store individual words of the story in each single memory slot.",
                    "label": 1
                },
                {
                    "sent": "Or you might want to store a window words in each memory slot.",
                    "label": 0
                },
                {
                    "sent": "Or you might want to store the whole sentence in each memory slot, where again, the representation of a sentence could be as simple as a bag of words where you essentially take the average of the embeddings of all your words.",
                    "label": 1
                },
                {
                    "sent": "Or it could be an output of an RNN, or it could be an output of a CNN and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "So this function F basically, which takes as input A sent a sequence of words, stores outputs of memory representation, which is stored in the slot, and this is quite flexible.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you're also given a question during training, so you do exactly the same thing you represent the question.",
                    "label": 0
                },
                {
                    "sent": "As X.",
                    "label": 0
                },
                {
                    "sent": "Via the same same function.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The step three of the memory network involves defining the so-called scoring function, where essentially the task of a scoring function is to measure.",
                    "label": 1
                },
                {
                    "sent": "The sort of relevance of each of the memory slots with respect to the question.",
                    "label": 0
                },
                {
                    "sent": "Like so the scoring should be in such a way that the memories which are relevant like.",
                    "label": 0
                },
                {
                    "sent": "But before that, like the scoring function typically is a parametric function, it could be a neural network or anything.",
                    "label": 0
                },
                {
                    "sent": "But the training of this parametric function should be such that in.",
                    "label": 1
                },
                {
                    "sent": "For all the relevant memories, it scores the the score that it gives with respect to the question is higher than the score with respect to irrelevant matters.",
                    "label": 1
                },
                {
                    "sent": "What is the difference?",
                    "label": 0
                },
                {
                    "sent": "Context.",
                    "label": 0
                },
                {
                    "sent": "So the four sentences are the context.",
                    "label": 0
                },
                {
                    "sent": "The supporting fact is the subset of the context which is the most relevant.",
                    "label": 0
                },
                {
                    "sent": "Based on your question.",
                    "label": 0
                },
                {
                    "sent": "I don't know it is the most.",
                    "label": 0
                },
                {
                    "sent": "Yes, you know.",
                    "label": 0
                },
                {
                    "sent": "Yes exactly.",
                    "label": 0
                },
                {
                    "sent": "Animations colleges are mostly for this version of memory networks which we call fully supervised, but I'll talk in detail later about another version where you do not even need that.",
                    "label": 0
                },
                {
                    "sent": "Dealing training an where it automatically learns.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so some of the example choices of.",
                    "label": 0
                },
                {
                    "sent": "Scoring function could be like a simple input output mapping.",
                    "label": 0
                },
                {
                    "sent": "Why are some transformation matrix U which is basically the learnable parametric matrix?",
                    "label": 0
                },
                {
                    "sent": "Or as I said, it could be a fully connected network that takes as input the question and some memory.",
                    "label": 0
                },
                {
                    "sent": "Slot and generates the score again.",
                    "label": 0
                },
                {
                    "sent": "It's up to you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Step 4 Finally involves designing another parametric function which Maps the current question and the relevant memory that you obtained from your read head.",
                    "label": 0
                },
                {
                    "sent": "Basically the memories that gave you the higher score.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Use these two information to generate your final answer.",
                    "label": 0
                },
                {
                    "sent": "Right and in the first set of experiments, this was basically again a scoring function where basically we scored all the possible responses given the input and the memory and picked the one that gives you the that gives you the largest score.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and assuming that you've trained your function F your parametric function that is used to generate the answer, the scoring function, and basically the memories.",
                    "label": 1
                },
                {
                    "sent": "Then given a question at Test time.",
                    "label": 0
                },
                {
                    "sent": "You will score it against all your possible memories and you pick the highest memory and use that memory in your final parametric function to finally generate your answer.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this all of this is achieved by minimizing this following loss, so I'll just go briefly explain what this means.",
                    "label": 1
                },
                {
                    "sent": "So the first term essentially says X is your question.",
                    "label": 0
                },
                {
                    "sent": "You basically want to score the memory which is relevant to you.",
                    "label": 1
                },
                {
                    "sent": "Which is given to you, by the way, as a supporting fact to have a higher score than some, then all the other memories F bar.",
                    "label": 0
                },
                {
                    "sent": "By some margin gamma.",
                    "label": 0
                },
                {
                    "sent": "And once you have the.",
                    "label": 0
                },
                {
                    "sent": "That's the first bit of the scoring function.",
                    "label": 0
                },
                {
                    "sent": "The second bit of the scoring function says the loss function says that given the correct the relevant memory and your input X, you have another scoring function Sr.",
                    "label": 1
                },
                {
                    "sent": "Which essentially increases the score of your correct response R and decreases the score of all the incorrect responses are bar.",
                    "label": 0
                },
                {
                    "sent": "Ann, as one of you asked, basically, how do we know this?",
                    "label": 0
                },
                {
                    "sent": "So as I said before in the fully supervised setting, we are given access to this during training time, right?",
                    "label": 1
                },
                {
                    "sent": "So that's pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "So it's like a standard margin loss where you want to increase the score of your correct memories and decrease the score of your incorrect ones.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "As you would note this loss function esentially.",
                    "label": 0
                },
                {
                    "sent": "Will work when you have a single support.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fact what's like?",
                    "label": 0
                },
                {
                    "sent": "What do you do when you have multiple supporting facts where given this story, where basically you ask the question?",
                    "label": 0
                },
                {
                    "sent": "Where is football?",
                    "label": 0
                },
                {
                    "sent": "You first need to know that Jaune was the person who picked up the football, and also Jaune was in the playground, so the answer should be playground which essentially is encoded in these two sentences.",
                    "label": 1
                },
                {
                    "sent": "So the previous loss function obviously will not work because you have a single supporting fact.",
                    "label": 0
                },
                {
                    "sent": "But the cool thing about this is you can iterate.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or in other words, the loss function might look something like this.",
                    "label": 1
                },
                {
                    "sent": "Now you start with the first supporting fact and you want to increase the score of your first supporting fact.",
                    "label": 1
                },
                {
                    "sent": "Once that's done, you iterate an.",
                    "label": 0
                },
                {
                    "sent": "Essentially, combine that with your original question an.",
                    "label": 1
                },
                {
                    "sent": "Want to increase the score of the second supporting fact?",
                    "label": 0
                },
                {
                    "sent": "And finally, your final the output of your controller will take as input the original question and the two supporting facts, like the memories corresponding to the two supporting facts, an increase the score of those guys.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so so basically so just to repeat, basically you have the supporting the first supporting fact, the second supporting facts.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally.",
                    "label": 0
                },
                {
                    "sent": "Iterate and this is what we call hops.",
                    "label": 0
                },
                {
                    "sent": "And of course they need not be limited to two.",
                    "label": 0
                },
                {
                    "sent": "You could sort of keep iterating until some threshold number of hopsan, etc.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so like a slight degression right now, basically.",
                    "label": 0
                },
                {
                    "sent": "To test these models, so essentially we defined.",
                    "label": 0
                },
                {
                    "sent": "Sort of 20 simulated tasks where basically the objective of these tasks was to test models that can do complex reasoning in which involves long-term memory, but in a controlled environment.",
                    "label": 1
                },
                {
                    "sent": "So why we wanted that?",
                    "label": 1
                },
                {
                    "sent": "Let's think of these tasks as sort of unit tests in software engineering, right?",
                    "label": 1
                },
                {
                    "sent": "Basically we wanted to sort of.",
                    "label": 0
                },
                {
                    "sent": "Best skills of the models which.",
                    "label": 0
                },
                {
                    "sent": "Like this one, or at least two, maybe one or two skills in the model which sort of gives us an indication whether the model is doing any reasoning or not, right?",
                    "label": 0
                },
                {
                    "sent": "And the skills could correspond to reasoning.",
                    "label": 1
                },
                {
                    "sent": "It could correspond to language related skills like conjunction, coreference etc.",
                    "label": 0
                },
                {
                    "sent": "And stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So I'll just quickly go through the collection of tasks that we came up with, which hopefully should give you an idea about sort of convince you that probably this might.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be a good idea so so just to give you a bit of a background.",
                    "label": 0
                },
                {
                    "sent": "So we essentially designed a simulator where a simulator had a bunch of objects like people.",
                    "label": 0
                },
                {
                    "sent": "Like people and objects and they had a bunch of actions and each action was associated with an attribute like a goal would be go to a place, get will be get to an object and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And based on this simulator we came up with these sort of really.",
                    "label": 0
                },
                {
                    "sent": "Simple scenario of a world where people are just moving around to one room or the other, picking up objects, dropping objects and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Like like Jason, Go kitchen Jason get milk and blah blah.",
                    "label": 0
                },
                {
                    "sent": "Anne, once we are given these commands, we had sort of a very simple template ized language, which we plugged on top of this to convert these commands into natural language sentences.",
                    "label": 0
                },
                {
                    "sent": "So again, the objective here was not to solve NLP, but to solve two things.",
                    "label": 0
                },
                {
                    "sent": "One is basically to solve the reasoning, but given these commands.",
                    "label": 0
                },
                {
                    "sent": "And also to have some sort of.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Language understanding skills basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "An we wanted all of this in a very controlled environment and hence the simple simulation.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just so, just to give you a simple examples the.",
                    "label": 0
                },
                {
                    "sent": "Like the simplest of the of the example, is basically a factoid based question answering which uses only a single supporting fact right where.",
                    "label": 1
                },
                {
                    "sent": "Again, the objective was, as we've seen so far to answer a question given this story, which has just a single supporting.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then an extension of that was a story which.",
                    "label": 0
                },
                {
                    "sent": "Can only be answered a question which can only be answered with two supporting facts.",
                    "label": 1
                },
                {
                    "sent": "Or a question which can only be answered with three supporting facts.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there was another task where basically we we wanted to see if the model can learn the difference between 2 two things, subject and object where basically if the story looked something like the office is North of the bedroom, the bedroom is North of the bathroom.",
                    "label": 1
                },
                {
                    "sent": "What is North of the bedroom or what is the bedroom North of?",
                    "label": 1
                },
                {
                    "sent": "So if you see the two sentences have exactly the same words.",
                    "label": 0
                },
                {
                    "sent": "But reordering them results in a different answer, so there was something we wanted to test whether the model can actually differentiate between North Ann bedroom.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then similar to that we had tasks related to three argument relation where basically have a story like Mary gave the cake to Fred.",
                    "label": 1
                },
                {
                    "sent": "Fred gave the key to Bill.",
                    "label": 0
                },
                {
                    "sent": "Jeff was Jeff was given the bail and blah blah.",
                    "label": 1
                },
                {
                    "sent": "And who gave the cake to?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then a simple true false questions.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An questions pertaining to counting like simple counting tasks like Daniel picked up the football, he picked up the football.",
                    "label": 1
                },
                {
                    "sent": "Drop the football.",
                    "label": 0
                },
                {
                    "sent": "You got the milk.",
                    "label": 0
                },
                {
                    "sent": "You got the Apple.",
                    "label": 1
                },
                {
                    "sent": "How many objects is done in holding?",
                    "label": 0
                },
                {
                    "sent": "So here you see like if for the model to answer this the model really needs to understand what's going on here.",
                    "label": 0
                },
                {
                    "sent": "It's not like a simple input output map in a black box where you expect this thing to be solved.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then there was the last related to indefinite knowledgeware.",
                    "label": 1
                },
                {
                    "sent": "Basically, you're not given certain like you're given some sort of a nuns uncertain facts.",
                    "label": 0
                },
                {
                    "sent": "Ann, you're expected to answer your enhancer answers.",
                    "label": 0
                },
                {
                    "sent": "Could be something like.",
                    "label": 0
                },
                {
                    "sent": "Maybe like John is in the classroom or the playground and you ask, is John in the classroom?",
                    "label": 1
                },
                {
                    "sent": "So things like that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some language related tasks, like basic coreference, Daniel was in the kitchen, then he went to the studio.",
                    "label": 0
                },
                {
                    "sent": "Where is Daniel?",
                    "label": 0
                },
                {
                    "sent": "Right, so it needs to understand what he meant here and compound coreference.",
                    "label": 0
                },
                {
                    "sent": "Daniel and Sandra Journey to the office.",
                    "label": 0
                },
                {
                    "sent": "Then they went to the Garden Ann.",
                    "label": 0
                },
                {
                    "sent": "Then you want to answer, where is Daniels even those Sandra had moved to somewhere else?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An like similar to that we had other set of tasks like time manipulation, basic deduction etc.",
                    "label": 0
                },
                {
                    "sent": "But probably in the interest of time out.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move forward quickly and then, like positional reasoning, reasoning about.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aces etc anpac.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training and stuff like that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this gave us basically 20 tasks.",
                    "label": 0
                },
                {
                    "sent": "Each task came with like a short story of the simulated world an the model was expected to answer and the questions were made so that basically you could evaluate with the model loans or not.",
                    "label": 0
                },
                {
                    "sent": "And to compare.",
                    "label": 0
                },
                {
                    "sent": "Our hypothesis that we really need an external context or an external memory.",
                    "label": 0
                },
                {
                    "sent": "We compared with three sets of standard baseline.",
                    "label": 0
                },
                {
                    "sent": "One is basically a standard structured SVM where you have a whole bunch of hand coded features from your story and your question and you just want to predict an answer using an SVM.",
                    "label": 1
                },
                {
                    "sent": "And we also used an LSD M where basically we ran the SDM over the entire story on the question, and the objective was to use the last hidden state of the STM to predict the final answer and then a similar was an engram classifier with again, we took the story and the questions represented as a bag of ngrams and then train the classifier on top.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I guess you can see them so you can see from this table other than I mean the 1st and the 3rd column were basically bound to be hopeless.",
                    "label": 0
                },
                {
                    "sent": "Basically the interesting bit is the STM an as expected since the story is long and it's the memory of the LCM is fairly restricted to just one hidden vector.",
                    "label": 0
                },
                {
                    "sent": "I mean it did not do very well on most of the tasks.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And compared to a memory network which basically was the first version of the memory network consisting of one or two hops.",
                    "label": 0
                },
                {
                    "sent": "So it does well on single supporting facts to supporting fact, but does really poorly on the three, supporting facts and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "And so in some of the tasks it works of them.",
                    "label": 0
                },
                {
                    "sent": "It does not work.",
                    "label": 0
                },
                {
                    "sent": "And then as I said, there's a lot of freedom in how you.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure The memory network model that you can play around with it.",
                    "label": 0
                },
                {
                    "sent": "So the second version of this was with so called adaptive memory, where essentially we do not restrict to two hops, but we define a special memory slot called the null memory slot and keep repeating the inference until it fetches the null memory slot and then generate an answer.",
                    "label": 0
                },
                {
                    "sent": "And that, as expected, helps on three supporting facts an also on basic induction task, which I did not go into.",
                    "label": 0
                },
                {
                    "sent": "But yeah, whatever.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then, as I said, like again, these are the next two are again variants of the memory networks where how you encode the features instead of encoding as the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "We essentially treated a sentence as a bag of ngrams.",
                    "label": 0
                },
                {
                    "sent": "Anne used adaptive.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Memory and here we used adaptive memory Ana nonlinear classifier instead of a simple scoring function at the end.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, a combination of all the three and we see as we increase the complexity of the network, the the number of tasks it has, it is able to solve successively.",
                    "label": 0
                },
                {
                    "sent": "Increases an each of these tasks are basically.",
                    "label": 0
                },
                {
                    "sent": "More complex than the other.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Late so.",
                    "label": 0
                },
                {
                    "sent": "That that was basically the first version of memory network that we proposed an.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's basically a fully supervised version.",
                    "label": 0
                },
                {
                    "sent": "Which means that during training you are in addition to the question answer in the context, you're also given a supporting fact, so let's look a little deeper into what's happening here, right?",
                    "label": 0
                },
                {
                    "sent": "Basically, we said that in the first step we embed or yeah, let's call this embed.",
                    "label": 0
                },
                {
                    "sent": "We embed the story into the memories.",
                    "label": 0
                },
                {
                    "sent": "And we also embed the given question into the memory, right?",
                    "label": 0
                },
                {
                    "sent": "And then what we do is we match this question representation with all the slots in the memory to get these scores.",
                    "label": 0
                },
                {
                    "sent": "Like these so called relevant scores.",
                    "label": 0
                },
                {
                    "sent": "Now think of the relevant scores like.",
                    "label": 0
                },
                {
                    "sent": "Think of these scores as how relevant each of these guys is to your current question, right or?",
                    "label": 0
                },
                {
                    "sent": "And then you're given a supporting fact.",
                    "label": 1
                },
                {
                    "sent": "And basically you take that.",
                    "label": 0
                },
                {
                    "sent": "You take the slot corresponding to the correct supporting facts and what you want is to increase.",
                    "label": 1
                },
                {
                    "sent": "To retrieve this memory and to increase the score of this supporting fact, an decrease the score of each of the other guys.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "And basically this is like a notion of hard attention.",
                    "label": 1
                },
                {
                    "sent": "Right, so each of these cores can be thought of as nothing but some sort of yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So as I said in the fully supervised setting during training, you do have that right.",
                    "label": 0
                },
                {
                    "sent": "So not during test.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "Basically each of these scores, as I said, can be represented as some sort of like how relevant that slot is to your current current set of questions.",
                    "label": 0
                },
                {
                    "sent": "Right, and can be interpreted as some sort of in attention score if you want.",
                    "label": 1
                },
                {
                    "sent": "Like how well you are confident that you really want to attend to that part of the memory.",
                    "label": 0
                },
                {
                    "sent": "And then given the correct supporting fact, your ascentia Lee know that this score has to be higher and you just pick that memory an push the score higher Ann and decrease the other scores.",
                    "label": 0
                },
                {
                    "sent": "So that that's basically the idea of what, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 1
                },
                {
                    "sent": "Yeah, so bot finding task is basically.",
                    "label": 1
                },
                {
                    "sent": "Like you're given a sequence of events.",
                    "label": 0
                },
                {
                    "sent": "Basically not really adventurous sensually given a layout of a sequence of rooms if you want.",
                    "label": 0
                },
                {
                    "sent": "Like bathroom is not of the kitchen.",
                    "label": 0
                },
                {
                    "sent": "Bedroom is South of the bathroom and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "So that's the set of sentences you're given.",
                    "label": 0
                },
                {
                    "sent": "And the question you are asked is how do I go from bathroom to the kitchen?",
                    "label": 0
                },
                {
                    "sent": "Right, so essentially I mean, it's inherently a hard task bar because it really requires a lot of planning and search.",
                    "label": 0
                },
                {
                    "sent": "Write an and sure enough, memory networks, at least the first version, are really bad at that.",
                    "label": 0
                },
                {
                    "sent": "Or for that matter, like like search is hard.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so so as I said, as most of you have been sort of implicitly complaining via questions is what the hell are we doing with this full, fully supervised like we don't really know during training time what the supporting factors, right?",
                    "label": 0
                },
                {
                    "sent": "That's certainly not a natural scenario in a natural scenario.",
                    "label": 0
                },
                {
                    "sent": "All you're given is a question or story and you want.",
                    "label": 0
                },
                {
                    "sent": "You essentially want an answer, or in other words, you want to learn which part of the memory is relevant to you, even during training time, right?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sure enough, the second model, which sort of tries to answer those questions, is what we call an end to end memory network, where the.",
                    "label": 0
                },
                {
                    "sent": "Where in this architecture the primary difference is that you're not given the initial supporting fact.",
                    "label": 0
                },
                {
                    "sent": "And it actually learns the part of the memory which are relevant even during training an this is achieved using this so called soft attention.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like again by soft and hard.",
                    "label": 0
                },
                {
                    "sent": "I mean in this in the hard case you since you were given what the correct memory is, which will correspond to the answer, you just pick that single memory.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As in the case of soft soft, you'll probably have some sort of a score over your entire memory.",
                    "label": 0
                },
                {
                    "sent": "Annual do some sort of a weighted combination, an stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So again, like if attention is not super clear, I can definitely talk to you guys offline and plus you'll hear a lot more about it in the in the in tomorrow stop.",
                    "label": 0
                },
                {
                    "sent": "So and the cool thing about this is that the whole architecture is end to end differentiable an it only needs supervision at the final output.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the architecture like the this is the single layer.",
                    "label": 0
                },
                {
                    "sent": "So what you have is a question.",
                    "label": 0
                },
                {
                    "sent": "As usual, you embed the question in whichever way you want.",
                    "label": 0
                },
                {
                    "sent": "I mean, again, bag of words output of an RNN, blah blah.",
                    "label": 0
                },
                {
                    "sent": "Let's denote it by U.",
                    "label": 0
                },
                {
                    "sent": "And also you have your collection of sentences from the story which are embedded in your memory.",
                    "label": 0
                },
                {
                    "sent": "Again, whichever way you want.",
                    "label": 0
                },
                {
                    "sent": "You sort of match your U with these embeddings amyes to get the Scorpii where P is basically a softmax over your score the scoring vector.",
                    "label": 0
                },
                {
                    "sent": "So way to interpret the values of Pi is basically how how confident are you that this memory that the it's memory slot is.",
                    "label": 0
                },
                {
                    "sent": "Is relevant to your question.",
                    "label": 0
                },
                {
                    "sent": "Right and then you have the same set of sentence is also embedded in this so called output memory.",
                    "label": 0
                },
                {
                    "sent": "Again, it's free for you to choose, you could might as well share the parameters of the output and the input, in which case will be the same matrix.",
                    "label": 0
                },
                {
                    "sent": "If you want, you can keep it different, but again like that's more parameters in a model to learn, but whatever.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you use the peas and the output of your and your collection of output memories to get your final output, which is basically a weighted sum of these memories weighted by the likelihood that.",
                    "label": 0
                },
                {
                    "sent": "That that that you were confident that the Earth memory is your relevant memory.",
                    "label": 0
                },
                {
                    "sent": "So that's your output.",
                    "label": 0
                },
                {
                    "sent": "And finally you combine your oh with your question representation U and pass it through some other parametric function to give you an answer.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you look carefully at this architecture, this is basically a feedforward net.",
                    "label": 0
                },
                {
                    "sent": "I mean, you're going this way all the way up, and then you compute the loss you go back.",
                    "label": 0
                },
                {
                    "sent": "And during the process you're learning, you're use.",
                    "label": 0
                },
                {
                    "sent": "You're learning your memories and you're learning the scoring.",
                    "label": 0
                },
                {
                    "sent": "The matching scoring function so that peas are such that it picks up the relevant memory.",
                    "label": 0
                },
                {
                    "sent": "And of course, you're learning the final parametric function to give you the answer.",
                    "label": 0
                },
                {
                    "sent": "So that's the end to end bit.",
                    "label": 0
                },
                {
                    "sent": "Is it clear?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, you is the question right now.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Again, it's just a dot product here.",
                    "label": 0
                },
                {
                    "sent": "It could be anything you could have a neural network, I mean, as long as.",
                    "label": 0
                },
                {
                    "sent": "Yes, over the memories, so that's the soft bit of the attention.",
                    "label": 0
                },
                {
                    "sent": "Since you're getting like a distribution of your memories.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the cool thing about this is you can iterate.",
                    "label": 0
                },
                {
                    "sent": "Where this is what I showed you in the previous slide, you have your question vector.",
                    "label": 0
                },
                {
                    "sent": "You compute the output vector.",
                    "label": 0
                },
                {
                    "sent": "You could combine the output vector with the previous question vector again and do an iterate.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And keep going up until you want to go an each of these steps is called a hop and then you get a final output vector which you combine with the final question vector to generate an answer.",
                    "label": 0
                },
                {
                    "sent": "So I mean you need this iteration again because of the fact that this will only select.",
                    "label": 0
                },
                {
                    "sent": "11 fact, although it will be in a soft manner, but it will only select one fact.",
                    "label": 0
                },
                {
                    "sent": "But you probably want to select multiple facts.",
                    "label": 0
                },
                {
                    "sent": "And now if you look at it, basically this is nothing but an RNN turned sideways.",
                    "label": 0
                },
                {
                    "sent": "Right, like if these matrices are shared?",
                    "label": 0
                },
                {
                    "sent": "Then this is nothing but an RNN, which is turned on on its side.",
                    "label": 0
                },
                {
                    "sent": "Except for the fact that there is no limitation on the size of your memory.",
                    "label": 0
                },
                {
                    "sent": "You're actually selectively picking memory or the bits of your RNN or your recurrent matrix which are most relevant to you for that time step.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that's the multiple hop end to end memory network.",
                    "label": 0
                },
                {
                    "sent": "So here yeah.",
                    "label": 0
                },
                {
                    "sent": "I see you are reasoning or mental.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but.",
                    "label": 0
                },
                {
                    "sent": "So this is just making that thing explicit like the the single case almost always has the likelihood of picking only one slot.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "Sure, I mean so think of it right.",
                    "label": 0
                },
                {
                    "sent": "Like consider that sequential statement John went to the playground.",
                    "label": 0
                },
                {
                    "sent": "John dropped the ball and your question is where is?",
                    "label": 0
                },
                {
                    "sent": "Where is the ball?",
                    "label": 0
                },
                {
                    "sent": "Right, so there in order to answer, where is the ball you really need to have some sort of a sequential processing.",
                    "label": 0
                },
                {
                    "sent": "First you find out where the ball is.",
                    "label": 0
                },
                {
                    "sent": "Then you could then you connect that ball with Jaune.",
                    "label": 0
                },
                {
                    "sent": "Write an in.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A single case, the softmax, is taken in dependently over all the memories.",
                    "label": 0
                },
                {
                    "sent": "Right, so the hope is that in the first case you will pick one element of the sequence and then use that element to attend to the second element.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Maybe a good idea for them to consider correlations between that.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whereas I just explained that's where the multiple hop things come in right?",
                    "label": 0
                },
                {
                    "sent": "I mean where.",
                    "label": 0
                },
                {
                    "sent": "Essentially you're tying your previous attention with the newer attention.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Bob likes Donuts.",
                    "label": 0
                },
                {
                    "sent": "Library.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "That's that's that's sort of 1 drawback.",
                    "label": 0
                },
                {
                    "sent": "I mean there are quite a few drawbacks, which I'll talk about later.",
                    "label": 0
                },
                {
                    "sent": "No, I'm sorry, I thought you said how do we deal with multiple answers or like like?",
                    "label": 0
                },
                {
                    "sent": "Like a list of answers.",
                    "label": 0
                },
                {
                    "sent": "So I mean, right now we just pretend that a list of answer is a single answer.",
                    "label": 0
                },
                {
                    "sent": "So if like if there are key answers, then you have like lots of potential answers and then we try to train.",
                    "label": 0
                },
                {
                    "sent": "So that's obviously not the best, but that's that's hard.",
                    "label": 0
                },
                {
                    "sent": "That's really hard.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Separate input output.",
                    "label": 0
                },
                {
                    "sent": "Oh, so it will be clearer later on where basically we.",
                    "label": 0
                },
                {
                    "sent": "Let's",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mean, the motivation is that the input memories are trained.",
                    "label": 0
                },
                {
                    "sent": "To generate your attention scores and the output matrix is trained to predict the answer given the attention scores.",
                    "label": 0
                },
                {
                    "sent": "Right in some sense.",
                    "label": 0
                },
                {
                    "sent": "The hope is that there will be some sort of a communication between the two.",
                    "label": 0
                },
                {
                    "sent": "That said, in most of our experiments we shared them, but.",
                    "label": 0
                },
                {
                    "sent": "That was the motivation.",
                    "label": 0
                },
                {
                    "sent": "Because this is, this never sees the final answer right.",
                    "label": 0
                },
                {
                    "sent": "This set of memories never see the final answer.",
                    "label": 0
                },
                {
                    "sent": "And also I'll talk about later in this so called key value.",
                    "label": 0
                },
                {
                    "sent": "Pair, key value, memory networks.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether I'll be able to talk because it's I'm fairly late here, but the idea is this bit is used primarily for adressing.",
                    "label": 0
                },
                {
                    "sent": "Which part of the memory is relevant and this bit is used to actually given that address.",
                    "label": 0
                },
                {
                    "sent": "This bit is used to predict the answer.",
                    "label": 0
                },
                {
                    "sent": "So you could store a different.",
                    "label": 0
                },
                {
                    "sent": "Things here and a different thing in the corresponding ad slot in the output memory, which is more predicted to the answer.",
                    "label": 0
                },
                {
                    "sent": "I think I should move ahead because there's a lot to go.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "As I said, the architectures were fairly flexible.",
                    "label": 1
                },
                {
                    "sent": "You could use bag of words.",
                    "label": 0
                },
                {
                    "sent": "You could use RNN style reading of words or characters to embed your questions and statements into the memory and things like that.",
                    "label": 1
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this will give you the set of experiments on the previous task.",
                    "label": 0
                },
                {
                    "sent": "Ma'am and two NR again far better than LS teams, but as expected not as well as a fully supervised memory networks.",
                    "label": 0
                },
                {
                    "sent": "Because of course there's less supervision.",
                    "label": 0
                },
                {
                    "sent": "But still.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're pretty good.",
                    "label": 0
                },
                {
                    "sent": "But like, but still, as I said, like being not fully supervised.",
                    "label": 0
                },
                {
                    "sent": "There are some tasks where it fails compared to the full supervision.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to answer your question, why did we need multiple hops?",
                    "label": 0
                },
                {
                    "sent": "Because consider this basic induction task right.",
                    "label": 0
                },
                {
                    "sent": "Brian is a frog Lily's grave brighness yellow Julius is green.",
                    "label": 0
                },
                {
                    "sent": "Greg is a frog.",
                    "label": 0
                },
                {
                    "sent": "What color is Greg?",
                    "label": 0
                },
                {
                    "sent": "So first you need to know what basically what Greg is.",
                    "label": 0
                },
                {
                    "sent": "So it attends in the first hop on what Greg is basically now it finds out it's OK, it's a frog.",
                    "label": 0
                },
                {
                    "sent": "And then it ends what Brian is like.",
                    "label": 0
                },
                {
                    "sent": "Who else is a frog?",
                    "label": 0
                },
                {
                    "sent": "He says, OK, fine, Brian is the frog, so it retains that.",
                    "label": 0
                },
                {
                    "sent": "And then it says Brian is yellow, so that's the third of where it is.",
                    "label": 0
                },
                {
                    "sent": "So it needs that sequential reasoning.",
                    "label": 0
                },
                {
                    "sent": "And of course it gives the correct answer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And likewise on other tasks, right?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was the sort of the performance on the so called baby tasks.",
                    "label": 0
                },
                {
                    "sent": "But as I said, it's basically an RNN turned on its side, so you might as well do language modeling and see whether it works or not.",
                    "label": 0
                },
                {
                    "sent": "Late and basically the.",
                    "label": 0
                },
                {
                    "sent": "The setup for the language modeling form eminens is.",
                    "label": 1
                },
                {
                    "sent": "You're given a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "Say the previous 100 words used or each word in a memory slot.",
                    "label": 1
                },
                {
                    "sent": "And you start with a null question vector.",
                    "label": 1
                },
                {
                    "sent": "And then you do multiple hops.",
                    "label": 0
                },
                {
                    "sent": "Basically another question vector.",
                    "label": 0
                },
                {
                    "sent": "You do one hop, you get something you concatenate with the question vector, you do another hop, and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "Finally you get after what six or seven hops you get some feature vector.",
                    "label": 0
                },
                {
                    "sent": "Then you use that feature vector to predict the next word.",
                    "label": 1
                },
                {
                    "sent": "Right so.",
                    "label": 1
                },
                {
                    "sent": "Sort of putting this in RN in terms what RN is doing in.",
                    "label": 0
                },
                {
                    "sent": "Like if you're aware or like how the language modeling that is done in RNN, you're given the current word.",
                    "label": 0
                },
                {
                    "sent": "You do a single hop and produce the next word.",
                    "label": 0
                },
                {
                    "sent": "Here the flexibility is that you can basically look at your context multiple times to generate your next word.",
                    "label": 0
                },
                {
                    "sent": "An on standard datasets, namely Penn, Treebank and Text 8, which is a subset of Wikipedia, we show that.",
                    "label": 1
                },
                {
                    "sent": "It performs fairly similar to LS teams.",
                    "label": 0
                },
                {
                    "sent": "Which is the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "Yes we did I mean not in this experiment, but for the baby task, yes.",
                    "label": 0
                },
                {
                    "sent": "For the baby yes yeah.",
                    "label": 0
                },
                {
                    "sent": "As I said, like by we control the adoption by the.",
                    "label": 0
                },
                {
                    "sent": "Ability of the model to fetch the null memory once it fetches the null memory, we stopped.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so so the conclusion from this experiment is that, at least for language modeling, we may not really need long term dependencies.",
                    "label": 1
                },
                {
                    "sent": "Becausw quite often in the case of language modeling, mostly it's like like common words that are that you're predicting an you're computing the entropy over, but maybe once you're doing more.",
                    "label": 1
                },
                {
                    "sent": "Once you evaluate on nouns or entities, you might need long term dependencies, right?",
                    "label": 1
                },
                {
                    "sent": "Which is highly dependent on the.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're reading.",
                    "label": 0
                },
                {
                    "sent": "So just to give you a brief overview of literature as you mention that recently there's been a lot of work in these attention based on memory based models for numerous applications like.",
                    "label": 0
                },
                {
                    "sent": "Solution I mean for generating sort of strokes of characters.",
                    "label": 0
                },
                {
                    "sent": "And then there's also the neural Turing machine, which I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "Given that, I'm assuming I have that I'm an, then there was also work on stack based memories.",
                    "label": 0
                },
                {
                    "sent": "Which was which is valuable.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's move a bit quicker because there's a lot to cover.",
                    "label": 0
                },
                {
                    "sent": "So then then as I said, like I started off motivating how to deal with context, which is super large right?",
                    "label": 0
                },
                {
                    "sent": "When suppose you have a knowledge base which has millions and millions of facts.",
                    "label": 0
                },
                {
                    "sent": "So the cool thing about memory network.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, is that.",
                    "label": 0
                },
                {
                    "sent": "The architecture is fairly flexible.",
                    "label": 0
                },
                {
                    "sent": "I never really mentioned how we are storing the memory.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can actually do some intelligence while storing the memory.",
                    "label": 0
                },
                {
                    "sent": "In particular, you could hash the memories.",
                    "label": 1
                },
                {
                    "sent": "You could hash the memories based on the words in the statement in the sense.",
                    "label": 1
                },
                {
                    "sent": "Prince",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous case, all the memories corresponding to Shaolin Soccer would be in one bucket.",
                    "label": 0
                },
                {
                    "sent": "All the memory scope corresponding to the God of cookery will be in another bucket, and.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so forth, right?",
                    "label": 0
                },
                {
                    "sent": "Because if you don't hash them, then there's like a million memory slots and it's kind of hard to do inference over.",
                    "label": 0
                },
                {
                    "sent": "Right, and the hashing could not only be with respect to words, but also could.",
                    "label": 1
                },
                {
                    "sent": "It could be with respect to the embeddings of the words.",
                    "label": 1
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so and so forth and.",
                    "label": 0
                },
                {
                    "sent": "Via this we tried the model on actual question answering data set which has about 14,000,000 facts which are stored in basically 14,000,000 memory slots.",
                    "label": 0
                },
                {
                    "sent": "And the statements are fairly diverse.",
                    "label": 0
                },
                {
                    "sent": "Basically you have stuff like this and you have stuff like.",
                    "label": 0
                },
                {
                    "sent": "Like factual stuff, when you have stuff like I mean reasoning based.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and then basically we showed that meman ends.",
                    "label": 0
                },
                {
                    "sent": "With some fancy featurization basically gives you the state of the art, even on the large scale data set.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An as I said, so without hashing the, you're basically.",
                    "label": 0
                },
                {
                    "sent": "You have around 14,000,000 memory slots.",
                    "label": 0
                },
                {
                    "sent": "It gives you the best performance, but it's extremely slow at inference time because you really need to score 14 million slots and pick the maximum.",
                    "label": 0
                },
                {
                    "sent": "So if you hash by words or by clusters you degrade in performance, but it's like significantly faster.",
                    "label": 0
                },
                {
                    "sent": "So specially with clustering you don't really degrade in performance much, but it's.",
                    "label": 0
                },
                {
                    "sent": "Way faster.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we also tried multitasking by using the real world question, answering data set described in the previous slide an the baby tasks and then at Test time we gave this cooked up story.",
                    "label": 0
                },
                {
                    "sent": "And it could answer questions very in an interesting manner like some like simple factual questions like where is the story based?",
                    "label": 0
                },
                {
                    "sent": "Question like where is the milk and it says office and will answer questions like where is the milk come from?",
                    "label": 1
                },
                {
                    "sent": "It comes from cow but in some cases are like the answer nonsensical as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "I guess my point over here is that the architecture is fairly flexible that what goes inside the memory is absolutely.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addictive so another thing I want to go into detail is this so called closee style question answering so the motivation is you want to build machines that can actually understand language.",
                    "label": 1
                },
                {
                    "sent": "Late basically and one way to sort of know that a machine is understanding language is making it reader comprehension an ass.",
                    "label": 1
                },
                {
                    "sent": "Making it answer questions that pertain only to the comprehension, or in other words, the machine should not be able to answer questions based on some outside knowledge, so you want to restrict your questions in such a way that the answer exists.",
                    "label": 1
                },
                {
                    "sent": "In this story, I think it was so an Unfortunately there was no data set until recently that did this.",
                    "label": 0
                },
                {
                    "sent": "No large scale data set, in fact, which you could use to train.",
                    "label": 0
                },
                {
                    "sent": "I mean there is a small data set like MMC test and stuff, but it's really small and you can only use it for testing.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I think this is an interesting problem.",
                    "label": 0
                },
                {
                    "sent": "An recently there's been a couple of works in this direction, one from our lab and one from Deep Mind, and where.",
                    "label": 0
                },
                {
                    "sent": "We've built the data set an models that does this and they don't do a perfect job, but I think it's a step in the right direction.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular we released this children's book data set where we took basically 118 books which were freely available, an used snippets from this book to generate our comprehension an the question in particular, we took the random 20 sentences from the book.",
                    "label": 0
                },
                {
                    "sent": "From anywhere that's the context and the 21st sentence is your question.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An in that question where we basically remove a random word.",
                    "label": 0
                },
                {
                    "sent": "And the ask here is, given this story, fill in the blank.",
                    "label": 0
                },
                {
                    "sent": "And as part of our training, we also provide a set of candidates to make things a little easier for the machine.",
                    "label": 0
                },
                {
                    "sent": "An yeah so so so the thing here is that you really want to remove the words which are not.",
                    "label": 0
                },
                {
                    "sent": "You really want to remove the words in a way so that simple models like language modeling type models do are not able to answer these right.",
                    "label": 0
                },
                {
                    "sent": "In particular, the interesting set of words to remove from here would be entities that only appear here, or the nouns that only appear here and so and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and the model is as usual.",
                    "label": 0
                },
                {
                    "sent": "Basically have the story you store the story in the memory you have the question without with the blank you represent this in some way and you score the memories an basically evaluator.",
                    "label": 0
                },
                {
                    "sent": "Finally, you scored the candidates an the novelty here.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is basically how we stored the memories.",
                    "label": 0
                },
                {
                    "sent": "You could store the entire sentence, which we'll see in a bit was pretty bad.",
                    "label": 0
                },
                {
                    "sent": "You could store wordwise like each.",
                    "label": 0
                },
                {
                    "sent": "Each word has its own slot, which is also bad.",
                    "label": 0
                },
                {
                    "sent": "The best thing that worked was basically store windows.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "An basically these are the results as you'll see like language.",
                    "label": 0
                },
                {
                    "sent": "So we split the columns in by the category of words we removed like either the entity or common nouns or verbs or prepositions.",
                    "label": 0
                },
                {
                    "sent": "So as you can see models like STM which essentially run through the whole story and predict the final answer after the story in the question.",
                    "label": 0
                },
                {
                    "sent": "They do pretty well adverbs and prepositions, because that's what language models do, but they are fairly bad at.",
                    "label": 0
                },
                {
                    "sent": "Basically predicting the entities an nouns which are highly story dependent.",
                    "label": 0
                },
                {
                    "sent": "Humans, by their do a very good job.",
                    "label": 0
                },
                {
                    "sent": "Which apparently was not evident from the.",
                    "label": 0
                },
                {
                    "sent": "Earlier on in my talk, but it's OK.",
                    "label": 0
                },
                {
                    "sent": "So, so these humans were basically annotators from Facebook so.",
                    "label": 0
                },
                {
                    "sent": "We showed them the story and they were given plenty of time to answer them.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only, that's why they're doing good job.",
                    "label": 0
                },
                {
                    "sent": "And men's are not there yet, but yeah.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'll think about cell?",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supervision for now.",
                    "label": 0
                },
                {
                    "sent": "And we tried the same model on the data set from DeepMind, which as I said is another very interesting work in this direction where basically they.",
                    "label": 0
                },
                {
                    "sent": "They use the news articles from CNN and Daily Mail.",
                    "label": 0
                },
                {
                    "sent": "Probably you'll hear a lot more about this tomorrow as well an each of these news articles comes with a summary sentence is like snippets from the article and what they do is again from that summary.",
                    "label": 0
                },
                {
                    "sent": "They remove one word an.",
                    "label": 0
                },
                {
                    "sent": "Then the setup is the same that you want to predict the word given the news article and that summary.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "You're given the article you are given the query where you remove some word and to make the task harder so that language models fail.",
                    "label": 0
                },
                {
                    "sent": "They actually anonymize these entities.",
                    "label": 0
                },
                {
                    "sent": "So you don't really learn the model.",
                    "label": 0
                },
                {
                    "sent": "the BBC.",
                    "label": 0
                },
                {
                    "sent": "You basically learn the model, the blah, and that entity.",
                    "label": 0
                },
                {
                    "sent": "This entity 381 will never appear anywhere else in the data.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can't really learn though BC so.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, so here again.",
                    "label": 0
                },
                {
                    "sent": "Basically mem ends do a decent job until recently where there were other results, which sort of are better than this.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then as I said, this dialogue modeling, probably I'll skip dialogue modeling becausw.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is similar like we released another data set in this direction where you could do question answering over.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Movies recommendation?",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How are movies combining question answer?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In movie recommendation and also a subset of the Reddit data which does.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free freeform dialogue modeling here, basically the.",
                    "label": 0
                },
                {
                    "sent": "They call the key point is.",
                    "label": 0
                },
                {
                    "sent": "Basically you have two types of memories.",
                    "label": 0
                },
                {
                    "sent": "One is this long term memory which is essentially storing the background.",
                    "label": 0
                },
                {
                    "sent": "The knowledge base, and then there's a short term memory which pertains to the dialogue that the model is having with the with the user.",
                    "label": 0
                },
                {
                    "sent": "So you really need to combine this short term memory with this long term memory to answer questions that the user has asked.",
                    "label": 0
                },
                {
                    "sent": "So details are in the page.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, but I'll probably speed up.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lastly, as I said, there was this key value memory networks.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "As I said before, the two matrices need not store the same thing.",
                    "label": 0
                },
                {
                    "sent": "You could repres.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And your triple the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Triple, which is basically subject relation object as a key value pair where you define the key as subject in a relation and the value is your object because in the end if somebody asks you a question relating to subject in relation, you really want the object to help you in answering and not the whole.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you could store a different thing here.",
                    "label": 0
                },
                {
                    "sent": "Different thing here an you use this as your addressing mechanism to attention to focus towards which.",
                    "label": 0
                },
                {
                    "sent": "Which object is correct and then use that addressing mechanism to?",
                    "label": 0
                },
                {
                    "sent": "Fetch the actual object and use that to answer.",
                    "label": 0
                },
                {
                    "sent": "So this is fairly recent actually, so I mean.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it does what it does, and then there was another paper called Dynamic Memory Networks which sort of emphasizes my point, which I've said that multiple times that memory network architecture is fairly flexible, so this dynamic memory and what they do is.",
                    "label": 0
                },
                {
                    "sent": "They really basically hammer the memory networks with recommend networks everywhere.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regular, this is what the intimidating figure looks like.",
                    "label": 0
                },
                {
                    "sent": "You have a question.",
                    "label": 0
                },
                {
                    "sent": "You represent it with an RNN.",
                    "label": 0
                },
                {
                    "sent": "You have the statements which also are represented by an RNN.",
                    "label": 0
                },
                {
                    "sent": "Well, the output of the RNN and then given the question you sort of score these representation to generate.",
                    "label": 0
                },
                {
                    "sent": "What would you call these episodes?",
                    "label": 0
                },
                {
                    "sent": "And that episode Vector is combined with this so called internal memory vector.",
                    "label": 0
                },
                {
                    "sent": "Again, why an RNN to generate the next memory vector?",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "And you iterate.",
                    "label": 0
                },
                {
                    "sent": "Using the current memory vector, the current question you iterate to find another episode, an plug that into the RNN, and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "Finally, you get the final.",
                    "label": 0
                },
                {
                    "sent": "Representation which you use to generate your answer.",
                    "label": 0
                },
                {
                    "sent": "So like if you think carefully, it's basically the same architecture but with.",
                    "label": 0
                },
                {
                    "sent": "A lot more machinery into how you want to encode your data.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then basically they do reasonably well.",
                    "label": 0
                },
                {
                    "sent": "With such heavy machinery on the baby data set.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so I will and then they apply this to task like sentiment and.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an power, speech tagging and stuff like that, so I mean, I'm happy to discuss these offline after the talk.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You guys are interested.",
                    "label": 0
                },
                {
                    "sent": "So just to summarize.",
                    "label": 0
                },
                {
                    "sent": "So basically memory networks are basically deep learning models that that AUGMENT external memory which is readable and writable.",
                    "label": 1
                },
                {
                    "sent": "Anne, these memories are learned an are effectively used for reasoning tasks for language, understanding tasks and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And the architecture is quite flexible as you know by now.",
                    "label": 1
                },
                {
                    "sent": "To basically play around with stuff.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The shortcomings are with the memory network model that I proposed, which I also spoke a bit about earlier is that we don't really write into the memories intelligently.",
                    "label": 1
                },
                {
                    "sent": "Right in the sense we do not erase any memory.",
                    "label": 0
                },
                {
                    "sent": "If we have a certain number of slots of memory.",
                    "label": 0
                },
                {
                    "sent": "Once the slots get full, we cycle.",
                    "label": 1
                },
                {
                    "sent": "And we cannot compress memories like if you have like, some know some long-term knowledge, there's no way to.",
                    "label": 0
                },
                {
                    "sent": "Efficiently encoded so that later on it can be used in a.",
                    "label": 0
                },
                {
                    "sent": "An efficient manner, it's just that every knowledge is treated the same.",
                    "label": 0
                },
                {
                    "sent": "How recent or long term back it?",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So neural Turing machines, which I will talk briefly in 5 minutes, is basically an answer to some of the shortcomings of a memory network where the architecture is as I said, the same.",
                    "label": 0
                },
                {
                    "sent": "The primary differences in the way it writes into the memory.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so the reading better is very similar.",
                    "label": 0
                },
                {
                    "sent": "Basically you have a weight vector which is computed via your controller at time T. And at this rate is like your attention vector.",
                    "label": 1
                },
                {
                    "sent": "And then for reading bit, all you're doing is taking all your memories an.",
                    "label": 1
                },
                {
                    "sent": "Weighting them by the weight vector to get a final read vector output as output into your controller, which again which the controller will use for your final object for final task.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The right mechanism is a little more complicated where basically you have the weight vector generated by the right module as before.",
                    "label": 1
                },
                {
                    "sent": "Then it also generates two additional vectors, the erase and the ad vector.",
                    "label": 0
                },
                {
                    "sent": "So the first step in the right module, what the right head does, is basically for every memory slot I.",
                    "label": 0
                },
                {
                    "sent": "It multiplies it by this factor.",
                    "label": 0
                },
                {
                    "sent": "So what Ascentia Lee this is saying is if the weight vector at I is 1.",
                    "label": 1
                },
                {
                    "sent": "And it is vector is also one, then this memory slot is 0.",
                    "label": 0
                },
                {
                    "sent": "So your whole your current memory slot then becomes zero like you erase your memory.",
                    "label": 0
                },
                {
                    "sent": "If the weight vector is 0 then you keep the previous memory and then in the second step with this M~ I had.",
                    "label": 0
                },
                {
                    "sent": "You essentially add your ad vector, again, weighted by that, that that set of weights.",
                    "label": 0
                },
                {
                    "sent": "So it's like a two step process where in the first step it has erased the part of the memory that it wanted to an added the new information.",
                    "label": 0
                },
                {
                    "sent": "In the second step.",
                    "label": 0
                },
                {
                    "sent": "Right, so yeah, so the question here is how do we compute the weight vectors?",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The cool thing about neural Turing machines is that the weight vectors are computed in a way that basically allows you to.",
                    "label": 1
                },
                {
                    "sent": "Not just read or write at the next memory slot, but anywhere in your memory.",
                    "label": 0
                },
                {
                    "sent": "Module and that's achieved by combining the so called content based addressing and the location based ad dressing.",
                    "label": 1
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the content based our dressing is basically the usual stuff that we've been we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "Basically you have the output of your controller, Katie.",
                    "label": 0
                },
                {
                    "sent": "Which could be your question.",
                    "label": 0
                },
                {
                    "sent": "For instance, an you have the memory.",
                    "label": 0
                },
                {
                    "sent": "Then you're essentially scoring each of your memories via the softmax.",
                    "label": 0
                },
                {
                    "sent": "That gives you the weights of basically the content based scoring.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The location based coding is a little more involved than it.",
                    "label": 1
                },
                {
                    "sent": "Most three steps.",
                    "label": 1
                },
                {
                    "sent": "The first step involves interpolating the previously computed content based scoring.",
                    "label": 0
                },
                {
                    "sent": "The content based weighting in the previous slide with the previous weights by through some vector GT.",
                    "label": 1
                },
                {
                    "sent": "Again, so this GT is the vector that your controller MX.",
                    "label": 1
                },
                {
                    "sent": "And the second thing is convolving this WTG with some shift vector.",
                    "label": 0
                },
                {
                    "sent": "So again, this shift vector is also the output of your controller.",
                    "label": 0
                },
                {
                    "sent": "Which essentially what this does if you look carefully is like if you allow shifts, say.",
                    "label": 0
                },
                {
                    "sent": "Ascentia Lee all it's doing is shifting the weights from Kate from The Jets location to its location if you want.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And Lastly, since it's a convolution operation, an if the shift vector is basically smooth, then what this ends up doing is blurring the weight vector.",
                    "label": 1
                },
                {
                    "sent": "So you probably need to sharpen it as well.",
                    "label": 0
                },
                {
                    "sent": "An so in short, what these three steps lead 2 is?",
                    "label": 0
                },
                {
                    "sent": "A set of weight vectors I for the right module that could.",
                    "label": 0
                },
                {
                    "sent": "Effectively right into.",
                    "label": 0
                },
                {
                    "sent": "Random memory slot, which probably had already written to.",
                    "label": 0
                },
                {
                    "sent": "Or it could be a new slot overtime.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so that's that's pretty much it as far as the neural Turing machines is concerned.",
                    "label": 0
                },
                {
                    "sent": "An they original paper used these models on simple tasks like copy, where you're given a sequence of vectors.",
                    "label": 0
                },
                {
                    "sent": "The machine is expected to read the sequence stored sequence in the memory, an sort of remember the sequence by generating it back again.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the LS teams take quite awhile, so if these are the number of epochs on the X axis.",
                    "label": 0
                },
                {
                    "sent": "So it takes quite awhile to copy the.",
                    "label": 0
                },
                {
                    "sent": "Even short sequences, I think this was tried.",
                    "label": 0
                },
                {
                    "sent": "This graph is for a sequence of length 20 I believe.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An the repeat copy is, as the name suggests, is.",
                    "label": 0
                },
                {
                    "sent": "Basically, you read the sequence.",
                    "label": 0
                },
                {
                    "sent": "Once an repeated K times.",
                    "label": 0
                },
                {
                    "sent": "And here again, I mean the neural Turing machines basically converge a lot faster and whereas in the case of LSD pretty much fails when it for large values of North being the number of times you want to repeat.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and so the sorting experiment was another one where basically a given a collection of vectors, an set of priorities for the vectors, and you want to sort the vectors in the order of priority.",
                    "label": 0
                },
                {
                    "sent": "So here again, basically neural Turing machines seem to successfully do that.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And whereas LSD comes pretty much failed because, like the entropy of The thing is quite high.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so to summarize, a neural Turing machines basically again sort of deep network augmented with memory where you can write into the memory much more intelligently that than memory networks.",
                    "label": 1
                },
                {
                    "sent": "An unfortunately though, so far it has only had only been used for toy problems like copy blah blah, and it's not.",
                    "label": 1
                },
                {
                    "sent": "It was not scalable, so until recently there's been some work.",
                    "label": 1
                },
                {
                    "sent": "Your shows Labahn, also probably another one from Deep Mind where they've used it on.",
                    "label": 0
                },
                {
                    "sent": "On the baby tasks, at least ensure that the neural Turing machines do work there.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And stack augmented R and ends.",
                    "label": 0
                },
                {
                    "sent": "I will skip becausw you.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll hear.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot more.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More detail.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Label them.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow and so, just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "So we discussed memory networks, which is basically and we discussed the Turing machines, not so much the stack augmented RNS, and we sort of gave you a quick.",
                    "label": 0
                },
                {
                    "sent": "Overview of attention both the soft and hard version an but that's I think one of the cool things that has come out.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently but still, this is quite a bit larger, lacking as far as these models are concerned, like we don't yet know what to write and what not to write.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you read a story, there may be so much of stuff that probably for us as humans.",
                    "label": 0
                },
                {
                    "sent": "If we read the story is sort of irrelevant an you really do want to keep that in your memory.",
                    "label": 0
                },
                {
                    "sent": "Only the selective bits.",
                    "label": 0
                },
                {
                    "sent": "An what type of memory to use?",
                    "label": 0
                },
                {
                    "sent": "Probably we have some answers in this direction, but not so much.",
                    "label": 0
                },
                {
                    "sent": "How to forget how to compress the old knowledge we don't yet know an basically how to build hierarchical memories, where basically you can do multi scale attention.",
                    "label": 0
                },
                {
                    "sent": "For instance an how to compose.",
                    "label": 0
                },
                {
                    "sent": "Sort of learn intelligent composition functions for hierarchical reasoning and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Those are things that are still unanswered, Ann.",
                    "label": 0
                },
                {
                    "sent": "Basically open questions.",
                    "label": 0
                },
                {
                    "sent": "So yeah, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}