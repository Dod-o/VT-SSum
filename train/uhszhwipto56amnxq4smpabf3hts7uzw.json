{
    "id": "uhszhwipto56amnxq4smpabf3hts7uzw",
    "title": "Online Learning for Time Series Prediction",
    "info": {
        "author": [
            "Elad Hazan, Technion - Israel Institute of Technology"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_hazan_learning/",
    "segmentation": [
        [
            "So good morning, my name is lacrosse and this is joint work.",
            "With Warren's are my student was supposed to give the talk but didn't come 'cause he had a child last week and our colleagues Shaman or from the Technion and Ohio Shamir with here from vitamin Institute.",
            "And this talk is about time series prediction, which is a topic from statistics and an online learning approach to it.",
            "So this is a play."
        ],
        [
            "Lot of average water temperatures overtime measured over.",
            "I think average overall oceans and you can clearly see global warming here.",
            "But the point is that so this is what we call a time series.",
            "It is a sequence of measurements of real numbers that are taken over successive interval in time and we assume that the measurements are taken over.",
            "So uniform intervals.",
            "So every month or every week or every the same times time measurement, same time sequence is passed through every measurement.",
            "And this is so this kind of creature has been studied in statistics for a long time."
        ],
        [
            "Starting in the 1920s, in the context of indeed weather prediction and many other applications as well.",
            "So angle I'll just so there's.",
            "There's a very long history, so this study, which I will not survey, but in 1982 Angle received the Nobel Prize for studying Time series in the context of finance, and he devised a model called Arch, which is useful for measuring certain type of financial time series.",
            "So in this statistic literature there have been many models called a Arthur autoregressive MA moving average Arma autoregressive moving average and so on.",
            "And it has been for the statistical literature is focused on modeling the time series.",
            "So trying to come up with models that explain how these time series are created and estimation of parameters.",
            "So trying to come up with parameters that will explain the data as it is seen.",
            "And of course the natural question in.",
            "We have machine learners try to is forecasting.",
            "Can we looking at a time series can we predict the next measurement with sufficiently high accuracy?",
            "Actually, not much of the statistical literature is focused on forecasting, but it has been considered the criteria that has been considering statistics at least the little statistic that I know is the mean square error criterion only.",
            "And that is because the mean square error criterion allows you to make.",
            "Assumptions that are otherwise not true.",
            "For example, you can assume that the noise is IID Gaussian so that you can assume if you measure according to the mean square.",
            "Oh, and the reason is the cause, the squirrel is a function which has only a second derivative, so you only care about the second moment of the noise and all other moments vanish.",
            "So you can make this assumption.",
            "And what we would like to do is come up.",
            "Learning we don't like to make assumptions about Gaussian IID noise.",
            "We would like to consider general functions that might be more appropriate for finance, for example, and in short, come up with more general models and faster algorithms."
        ],
        [
            "Let me talk about.",
            "We consider here only the armor model, which is, I would say one of the two main models considered in time series, the other being Arch or garch.",
            "And in Arma first we have the auto regressive model called AR, which assumes that the signal is created via linear combination of the previous measurements with some noise and the noise as I said in statistics, is usually assumed to be ID Gaussian.",
            "And there is a further enhancement of the album of the R model called A RMA stands for Moving Average which includes another component of a linear combination of the previous noise terms.",
            "So actually it has been known in signal processing that both of these models can describe essentially the same set of signals.",
            "The set of all signals that have a bounded spectrum in some sense, and indeed the main difficulty or the main point in these models is estimating the frequencies or the spectrum of signals, because we would like to.",
            "So these models are very good at predicting, figuring out frequencies or long-term cyclic behavior of signals.",
            "Gaming cave.",
            "You do gain, so it is also known that as you increase K, you have a larger and larger family of signals that you can represent, and it is known that for K equals Infinity and K = = 2 equals Infinity, it is the same group of signals.",
            "But that's all that is known.",
            "I haven't seen any literature on what happens for a finite key.",
            "So thank you for the question.",
            "And what will?"
        ],
        [
            "We like to do in terms of predicting some serious serious.",
            "So here's the protocol that should be familiar to all of you, I hope.",
            "So let's look at it from a game theoretic point of view of adverse aerial POV.",
            "Let's say that we have an adversary that fixes parameters Alpha and beta for the album model.",
            "Then at iteration T, the adversary samples noise.",
            "According to some arbitrary distribution, and generates the next measurement according to a linear combination of the previous measurements and the previous noise terms, and that that's going to be the next measurement.",
            "That is that he is going to, or she is going to provide to the learner.",
            "The online player predicts.",
            "Some why some real number Y and suffer the loss according to some convex loss function.",
            "Alex line.",
            "And what we would like to do the standard metric in performance in online learning is minimized regret, which in this case is the sum of loss of the online player.",
            "The difference between that and the loss of of of optimal player in the sense that he can minimize overall possible Alpha and beta to minimize her loss.",
            "So this is the setting that we consider.",
            "Notice that the player in no stage of this game she can never observe the epsilons.",
            "The noise terms, right?",
            "So that's never revealed to the player."
        ],
        [
            "So in this setting we have the following results.",
            "If we assume that the loss functions are convex, then we have one algorithm that attends the regret bound or average regret bound that goes to zero at the rate of roughly 1 / sqrt T. And if we assume some further assumptions about.",
            "The structure of the loss functions, such as it being explored cave, so this, for example, is very useful when you have quadratic loss square loss, the mean square error, for example.",
            "Then you can have a better rate that looks like log squared over T, and what assumptions do we make?",
            "So the assumptions are much weaker than the usual ones that are made in statistics.",
            "So we assume that the parameters Alpha are bounded.",
            "That is completely I would say, doesn't reduce generality at all.",
            "We assume that the parameters better, so if you recall Alpha or the linear combination of the previous measurements better than previously known combination of noise terms.",
            "So we assume that these batteries are in the unit ball at the vector, so this is the strongest assumption that we make and one can show that if you don't assume it, the signal can explode to Infinity.",
            "So in some sense it is.",
            "Reasonable to assume, but it is not strictly necessary, so this is what I would say is the weak point of the result.",
            "And we assume again that the loss functions are elections and bounded.",
            "That's very standard assumption.",
            "OK, so these are the results in comparison to previous results."
        ],
        [
            "So actually it's kind of hard to compare to previous results because statistical literature doesn't consider regret as a criteria.",
            "It considers convergence to the parameters.",
            "So what statistical papers usually look at is how you try to estimate Alpha and beta, assuming that indeed the signal comes from an ARMA generated model and then they bound the convergence in terms of, let's say the Euclidean distance from the Alpha that you estimate to the real Alpha, and similarly for better, yeah, sure.",
            "Monitors in agnostic sense of convergence to the best possible parameters without the assumption that we we realized right, so such a result doesn't exist in the statistical literature, we give such a result in a limited way.",
            "Our regret criteria has this kind of flavor.",
            "It is agnostic, but it is not completely agnostic for a reason which I will describe in a minute, but it is much much stronger than previous results, which is not exhaustive at all.",
            "It's in a proper model.",
            "So using the previous convergence behavior results you can derive regret bounds for very frustrated setting that look like almost the same as what we get logged to the two plus epsilon T / T where we improve a little bit.",
            "This is for the square loss, but again, we have weaker assumptions and in the general off there are no you cannot do it cause the statistical reduction from ID noise to Gaussian noise doesn't work anymore when you have loss functions that are not squared.",
            "OK, by the way, if there are any questions, I know she's feel free to ask me, but.",
            "As you have it on the slide, yeah, so that's right.",
            "So there is something called the Yule Walker equations.",
            "You can.",
            "You can write down system of linear equations and estimated parameters.",
            "It's a very clever algorithm to estimate Alpha and beta using the sequence that you see and that takes time proportional to solving systems linear equations.",
            "So Ncube.",
            "OK so CU running time.",
            "Our algorithms are all this number of samples, yeah?",
            "Let's say KQRKQ, the running time is K plus que cube that is the running time of the Yule Walker algorithm.",
            "It's like a sliding window algorithm, so every time you look at the previous K plus you samples and you solve system of linear equations.",
            "So that's cubic running time, whereas our algorithms are linear time for the square root result and square squared time.",
            "So more efficient also.",
            "OK, so good.",
            "So how do our algorithms look like?",
            "And here I'm giving the first talk of today so I have to say something which I think most of you know, and maybe everyone that comes after we will no longer have to say so.",
            "I'm giving a service to everyone that comes after me I hope.",
            "So here is the."
        ],
        [
            "I would say by now with standard framework for learning, it is called online convex optimization.",
            "And in this framework, a decision maker picks a point in a convex set.",
            "And incurs a loss which is proportional to a convex function applied to the point that she chose here, which is a linear function but doesn't have to be linear, can be any convex function.",
            "And this is repeated again and again.",
            "And finally.",
            "We measured the regret which is the difference between what the decision maker in curd, the losses incurred throughout all iterations and the loss she would have incurred having picked the best decision in hindsight.",
            "Which is the best point in the in the set?",
            "So just to, how would this fit into the Arma model that I described?",
            "So you would think of the X of the decisions that decision Maker has to have to take as parameters Alpha and beta and try to apply them to the previous signal.",
            "That's how I'm being a little brief here.",
            "If there are any questions, I'd be happy to answer.",
            "Ann and I. Yeah, so so algorithms that minimize regret in this in this setting.",
            "And most of them look like the following general template."
        ],
        [
            "Like all angry dissent, you would have some decision, which is a point in your convex set.",
            "And given a lot function, you would move in the direction of the gradient of the loss function that the lock of the lawsuit just in curd.",
            "And moving in the direction of the gradient might take you outside the convex set, so then you would have to project which means to find the closest point in the convex set to the point that you chose.",
            "So this is a simple gradient algorithm and encourage proved in 03 that by tuning the parameters this algorithm always attains the regret bounded by square root.",
            "The number of iterations.",
            "And there's a very efficient algorithm.",
            "It takes linear time aside from the projection operation.",
            "Yeah.",
            "So that's what I have to say about this setting.",
            "I'm being very brief because I assume that most of you know this very well.",
            "And there are generalizations for this framework that look like Milana Mirror."
        ],
        [
            "Sent that I'm not going to."
        ],
        [
            "Grover, OK, so we would like to apply this learning framework, which is very general.",
            "It is agnostic to learning time series.",
            "So to learning in the ARMA model.",
            "Now how would we try the naive way to model loss in the Arma setting would be to write down the loss function.",
            "It looks like this.",
            "It depends on Alpha and beta.",
            "And it looks like the loss of the real measurement and the linear combination of the previous measurement plus noise terms applied to the previous signals.",
            "So that's what we would like to do, and there are two issues here.",
            "First of all, we don't know the noise terms, so that's an obvious difficulty that prohibits us from using online convex optimization.",
            "And second of all, this function is not convex in general.",
            "OK, so."
        ],
        [
            "So what do we do?",
            "OK, so here is the first thing we would like to cope with.",
            "Is not knowing the noise so we can do the following trick.",
            "We can write down an estimate for the noise.",
            "Oh OK, so we can write down an estimate for the noise using all previous.",
            "All previous signals, how do we estimate the noise?",
            "Just write down the linear component given all previous noise estimates, you can write down the recursive definition and estimate the noise term.",
            "In the way that I've written here, this, by the way, has been done in statistics, so this is a standard but not not in an agnostic setting.",
            "And then you can write down a new loss function which takes into account this new estimate of the loss.",
            "So we call it F Infinity of Alpha and beta that now we have no longer have noise terms.",
            "But there are other difficulties that come into play.",
            "First of all, this new function has infinite memory.",
            "It needs to know all the previous measurements.",
            "To define the loss for the next iteration and second of all, it is still not convex.",
            "So here is the main."
        ],
        [
            "The the main idea.",
            "So what we do is replace the Alpha and beta with a single parameter gamma.",
            "And right the last the last terms instead of having these coefficients that depend on both Alpha and beta.",
            "And this can be very complex coefficients, right?",
            "Because you open it up recursively by new coefficients, so this is called non proper learning.",
            "We use one model to learn another model.",
            "One model which is more robust or more general.",
            "And the second thing is that we will clip the memory having infinite memories too restrictive.",
            "So we clip the memory down to some finite number.",
            "And the main observation hour."
        ],
        [
            "Our work is the following, so this is a quantitative theorem that corresponds to the non Quantitive theorem that she asked about.",
            "We can prove that armor cake you this set of signals that can be represented by Cape Alpha Permitieron Que better parameters is included in a regret mean regret sense by the set of all a are signals of a larger number M where M behaves like K plus que log T. OK, so by taking a longer sequence of measurements we can minimize regret as if we had.",
            "A stronger model but less coefficient, and so this inclusion is of course not precise.",
            "What I mean by that is that minimizing regret over the Big Blue ellipse gives you regret bounds with respect to the smaller ellipse."
        ],
        [
            "And now so now we can use online convex optimization.",
            "So once we move to our model, everything is convex.",
            "It's fine, it's bounded memory.",
            "We can apply the gradient descent type algorithms or more sophisticated 2nd order algorithms and obtain regret bounds that follow by.",
            "But just previous theorems in Kevin's theorem and so on.",
            "And how does the algorithm look like?"
        ],
        [
            "When it's a gradient algorithm on top of MM is a larger, larger number than what we started off with coefficients of the type of gamma.",
            "So given a new loss function, you would move in the direction of the gradient and project projection.",
            "In this case is very simple.",
            "We project onto the cube with whom we have a bounded set of parameters.",
            "And via the gradient algorithms will obtain regret.",
            "It looks like square root.",
            "The number of iterations and if you apply more sophisticated algorithm that takes into account also the second derivative of the loss function you can obtain.",
            "Slightly better regret bounds.",
            "Alright then.",
            "So here are some."
        ],
        [
            "Experiments that we have conducted and this is first just a sanity check.",
            "So we generated the time series and by just some bubbles.",
            "And we picked Alpha and Beta, an generated noise generated a signal this way and apply their algorithms, and this experiment is just to show you that the algorithms do something reasonable.",
            "And indeed if we have Gaussian noise, you can see here this is.",
            "This is the average error, so all algorithms converge to a small average error even though you Walker equations that I talked about.",
            "This is the previous state of the art.",
            "But if now you have, what happens if you look at more robust signals but would like to have some advantage because it's an agnostic setting rather than assuming a strict ARMA model, and then indeed previous methods do not converge to a nice average error, whereas this gradient algorithms are more robust.",
            "Anne."
        ],
        [
            "Similar things happen when we look at what happens when there is an abrupt change in parameters.",
            "So the signal all of the sudden.",
            "Changes in terms of the parameters.",
            "And then when we look at more general models of noise, such as correlated noise, an in both settings, these are settings that have been studied in online learning.",
            "So in some respect we expect these results and you can just take known results from online learning and plug them into the gradient algorithm that we come up with and have this guarantee automatically without working too hard.",
            "OK this."
        ],
        [
            "What happens with real data?",
            "And this is the first plot that I showed you of the average temperature of the oceans.",
            "Are algorithms obtain a better error in terms of predicting the next?",
            "And the next temperature and all algorithms, by the way, predicted global warming.",
            "And in terms of stock market data, no one does well.",
            "Which is also as we expect.",
            "So we need to show here."
        ],
        [
            "So just to conclude.",
            "So we have research and approach to time series analysis and forecasting that is based on online learning.",
            "The technology that we all know and love and we're able to predict this best of the best ARMA model under some assumptions which are weaker than previously.",
            "Staind and there are some nice interesting open questions here.",
            "First of all, can we assumption relax the assumption on the noise coefficient?",
            "This is, I think, a very nice technical question.",
            "Can we come up with regret bounds without assuming that the noise the coefficient vector lies inside the unit ball, which is not a necessary condition in general?",
            "And second of all, we would like to tackle more general models.",
            "In particular the model proposed by angle studying financial time series, which is called the Archer Gouge.",
            "So that would be very interesting as well.",
            "And I will conclude here.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good morning, my name is lacrosse and this is joint work.",
                    "label": 0
                },
                {
                    "sent": "With Warren's are my student was supposed to give the talk but didn't come 'cause he had a child last week and our colleagues Shaman or from the Technion and Ohio Shamir with here from vitamin Institute.",
                    "label": 0
                },
                {
                    "sent": "And this talk is about time series prediction, which is a topic from statistics and an online learning approach to it.",
                    "label": 1
                },
                {
                    "sent": "So this is a play.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lot of average water temperatures overtime measured over.",
                    "label": 0
                },
                {
                    "sent": "I think average overall oceans and you can clearly see global warming here.",
                    "label": 0
                },
                {
                    "sent": "But the point is that so this is what we call a time series.",
                    "label": 1
                },
                {
                    "sent": "It is a sequence of measurements of real numbers that are taken over successive interval in time and we assume that the measurements are taken over.",
                    "label": 0
                },
                {
                    "sent": "So uniform intervals.",
                    "label": 0
                },
                {
                    "sent": "So every month or every week or every the same times time measurement, same time sequence is passed through every measurement.",
                    "label": 0
                },
                {
                    "sent": "And this is so this kind of creature has been studied in statistics for a long time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Starting in the 1920s, in the context of indeed weather prediction and many other applications as well.",
                    "label": 0
                },
                {
                    "sent": "So angle I'll just so there's.",
                    "label": 0
                },
                {
                    "sent": "There's a very long history, so this study, which I will not survey, but in 1982 Angle received the Nobel Prize for studying Time series in the context of finance, and he devised a model called Arch, which is useful for measuring certain type of financial time series.",
                    "label": 0
                },
                {
                    "sent": "So in this statistic literature there have been many models called a Arthur autoregressive MA moving average Arma autoregressive moving average and so on.",
                    "label": 0
                },
                {
                    "sent": "And it has been for the statistical literature is focused on modeling the time series.",
                    "label": 0
                },
                {
                    "sent": "So trying to come up with models that explain how these time series are created and estimation of parameters.",
                    "label": 0
                },
                {
                    "sent": "So trying to come up with parameters that will explain the data as it is seen.",
                    "label": 0
                },
                {
                    "sent": "And of course the natural question in.",
                    "label": 0
                },
                {
                    "sent": "We have machine learners try to is forecasting.",
                    "label": 0
                },
                {
                    "sent": "Can we looking at a time series can we predict the next measurement with sufficiently high accuracy?",
                    "label": 0
                },
                {
                    "sent": "Actually, not much of the statistical literature is focused on forecasting, but it has been considered the criteria that has been considering statistics at least the little statistic that I know is the mean square error criterion only.",
                    "label": 0
                },
                {
                    "sent": "And that is because the mean square error criterion allows you to make.",
                    "label": 1
                },
                {
                    "sent": "Assumptions that are otherwise not true.",
                    "label": 0
                },
                {
                    "sent": "For example, you can assume that the noise is IID Gaussian so that you can assume if you measure according to the mean square.",
                    "label": 0
                },
                {
                    "sent": "Oh, and the reason is the cause, the squirrel is a function which has only a second derivative, so you only care about the second moment of the noise and all other moments vanish.",
                    "label": 0
                },
                {
                    "sent": "So you can make this assumption.",
                    "label": 0
                },
                {
                    "sent": "And what we would like to do is come up.",
                    "label": 0
                },
                {
                    "sent": "Learning we don't like to make assumptions about Gaussian IID noise.",
                    "label": 0
                },
                {
                    "sent": "We would like to consider general functions that might be more appropriate for finance, for example, and in short, come up with more general models and faster algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me talk about.",
                    "label": 0
                },
                {
                    "sent": "We consider here only the armor model, which is, I would say one of the two main models considered in time series, the other being Arch or garch.",
                    "label": 0
                },
                {
                    "sent": "And in Arma first we have the auto regressive model called AR, which assumes that the signal is created via linear combination of the previous measurements with some noise and the noise as I said in statistics, is usually assumed to be ID Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And there is a further enhancement of the album of the R model called A RMA stands for Moving Average which includes another component of a linear combination of the previous noise terms.",
                    "label": 0
                },
                {
                    "sent": "So actually it has been known in signal processing that both of these models can describe essentially the same set of signals.",
                    "label": 0
                },
                {
                    "sent": "The set of all signals that have a bounded spectrum in some sense, and indeed the main difficulty or the main point in these models is estimating the frequencies or the spectrum of signals, because we would like to.",
                    "label": 0
                },
                {
                    "sent": "So these models are very good at predicting, figuring out frequencies or long-term cyclic behavior of signals.",
                    "label": 0
                },
                {
                    "sent": "Gaming cave.",
                    "label": 0
                },
                {
                    "sent": "You do gain, so it is also known that as you increase K, you have a larger and larger family of signals that you can represent, and it is known that for K equals Infinity and K = = 2 equals Infinity, it is the same group of signals.",
                    "label": 0
                },
                {
                    "sent": "But that's all that is known.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen any literature on what happens for a finite key.",
                    "label": 0
                },
                {
                    "sent": "So thank you for the question.",
                    "label": 0
                },
                {
                    "sent": "And what will?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We like to do in terms of predicting some serious serious.",
                    "label": 0
                },
                {
                    "sent": "So here's the protocol that should be familiar to all of you, I hope.",
                    "label": 0
                },
                {
                    "sent": "So let's look at it from a game theoretic point of view of adverse aerial POV.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have an adversary that fixes parameters Alpha and beta for the album model.",
                    "label": 0
                },
                {
                    "sent": "Then at iteration T, the adversary samples noise.",
                    "label": 1
                },
                {
                    "sent": "According to some arbitrary distribution, and generates the next measurement according to a linear combination of the previous measurements and the previous noise terms, and that that's going to be the next measurement.",
                    "label": 0
                },
                {
                    "sent": "That is that he is going to, or she is going to provide to the learner.",
                    "label": 0
                },
                {
                    "sent": "The online player predicts.",
                    "label": 0
                },
                {
                    "sent": "Some why some real number Y and suffer the loss according to some convex loss function.",
                    "label": 0
                },
                {
                    "sent": "Alex line.",
                    "label": 0
                },
                {
                    "sent": "And what we would like to do the standard metric in performance in online learning is minimized regret, which in this case is the sum of loss of the online player.",
                    "label": 0
                },
                {
                    "sent": "The difference between that and the loss of of of optimal player in the sense that he can minimize overall possible Alpha and beta to minimize her loss.",
                    "label": 0
                },
                {
                    "sent": "So this is the setting that we consider.",
                    "label": 0
                },
                {
                    "sent": "Notice that the player in no stage of this game she can never observe the epsilons.",
                    "label": 0
                },
                {
                    "sent": "The noise terms, right?",
                    "label": 0
                },
                {
                    "sent": "So that's never revealed to the player.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this setting we have the following results.",
                    "label": 0
                },
                {
                    "sent": "If we assume that the loss functions are convex, then we have one algorithm that attends the regret bound or average regret bound that goes to zero at the rate of roughly 1 / sqrt T. And if we assume some further assumptions about.",
                    "label": 0
                },
                {
                    "sent": "The structure of the loss functions, such as it being explored cave, so this, for example, is very useful when you have quadratic loss square loss, the mean square error, for example.",
                    "label": 0
                },
                {
                    "sent": "Then you can have a better rate that looks like log squared over T, and what assumptions do we make?",
                    "label": 0
                },
                {
                    "sent": "So the assumptions are much weaker than the usual ones that are made in statistics.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the parameters Alpha are bounded.",
                    "label": 0
                },
                {
                    "sent": "That is completely I would say, doesn't reduce generality at all.",
                    "label": 0
                },
                {
                    "sent": "We assume that the parameters better, so if you recall Alpha or the linear combination of the previous measurements better than previously known combination of noise terms.",
                    "label": 0
                },
                {
                    "sent": "So we assume that these batteries are in the unit ball at the vector, so this is the strongest assumption that we make and one can show that if you don't assume it, the signal can explode to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So in some sense it is.",
                    "label": 0
                },
                {
                    "sent": "Reasonable to assume, but it is not strictly necessary, so this is what I would say is the weak point of the result.",
                    "label": 0
                },
                {
                    "sent": "And we assume again that the loss functions are elections and bounded.",
                    "label": 1
                },
                {
                    "sent": "That's very standard assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the results in comparison to previous results.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually it's kind of hard to compare to previous results because statistical literature doesn't consider regret as a criteria.",
                    "label": 0
                },
                {
                    "sent": "It considers convergence to the parameters.",
                    "label": 0
                },
                {
                    "sent": "So what statistical papers usually look at is how you try to estimate Alpha and beta, assuming that indeed the signal comes from an ARMA generated model and then they bound the convergence in terms of, let's say the Euclidean distance from the Alpha that you estimate to the real Alpha, and similarly for better, yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Monitors in agnostic sense of convergence to the best possible parameters without the assumption that we we realized right, so such a result doesn't exist in the statistical literature, we give such a result in a limited way.",
                    "label": 0
                },
                {
                    "sent": "Our regret criteria has this kind of flavor.",
                    "label": 0
                },
                {
                    "sent": "It is agnostic, but it is not completely agnostic for a reason which I will describe in a minute, but it is much much stronger than previous results, which is not exhaustive at all.",
                    "label": 0
                },
                {
                    "sent": "It's in a proper model.",
                    "label": 0
                },
                {
                    "sent": "So using the previous convergence behavior results you can derive regret bounds for very frustrated setting that look like almost the same as what we get logged to the two plus epsilon T / T where we improve a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is for the square loss, but again, we have weaker assumptions and in the general off there are no you cannot do it cause the statistical reduction from ID noise to Gaussian noise doesn't work anymore when you have loss functions that are not squared.",
                    "label": 1
                },
                {
                    "sent": "OK, by the way, if there are any questions, I know she's feel free to ask me, but.",
                    "label": 0
                },
                {
                    "sent": "As you have it on the slide, yeah, so that's right.",
                    "label": 0
                },
                {
                    "sent": "So there is something called the Yule Walker equations.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can write down system of linear equations and estimated parameters.",
                    "label": 0
                },
                {
                    "sent": "It's a very clever algorithm to estimate Alpha and beta using the sequence that you see and that takes time proportional to solving systems linear equations.",
                    "label": 0
                },
                {
                    "sent": "So Ncube.",
                    "label": 0
                },
                {
                    "sent": "OK so CU running time.",
                    "label": 0
                },
                {
                    "sent": "Our algorithms are all this number of samples, yeah?",
                    "label": 0
                },
                {
                    "sent": "Let's say KQRKQ, the running time is K plus que cube that is the running time of the Yule Walker algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's like a sliding window algorithm, so every time you look at the previous K plus you samples and you solve system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "So that's cubic running time, whereas our algorithms are linear time for the square root result and square squared time.",
                    "label": 0
                },
                {
                    "sent": "So more efficient also.",
                    "label": 0
                },
                {
                    "sent": "OK, so good.",
                    "label": 0
                },
                {
                    "sent": "So how do our algorithms look like?",
                    "label": 0
                },
                {
                    "sent": "And here I'm giving the first talk of today so I have to say something which I think most of you know, and maybe everyone that comes after we will no longer have to say so.",
                    "label": 0
                },
                {
                    "sent": "I'm giving a service to everyone that comes after me I hope.",
                    "label": 0
                },
                {
                    "sent": "So here is the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would say by now with standard framework for learning, it is called online convex optimization.",
                    "label": 1
                },
                {
                    "sent": "And in this framework, a decision maker picks a point in a convex set.",
                    "label": 0
                },
                {
                    "sent": "And incurs a loss which is proportional to a convex function applied to the point that she chose here, which is a linear function but doesn't have to be linear, can be any convex function.",
                    "label": 0
                },
                {
                    "sent": "And this is repeated again and again.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                },
                {
                    "sent": "We measured the regret which is the difference between what the decision maker in curd, the losses incurred throughout all iterations and the loss she would have incurred having picked the best decision in hindsight.",
                    "label": 0
                },
                {
                    "sent": "Which is the best point in the in the set?",
                    "label": 0
                },
                {
                    "sent": "So just to, how would this fit into the Arma model that I described?",
                    "label": 0
                },
                {
                    "sent": "So you would think of the X of the decisions that decision Maker has to have to take as parameters Alpha and beta and try to apply them to the previous signal.",
                    "label": 0
                },
                {
                    "sent": "That's how I'm being a little brief here.",
                    "label": 0
                },
                {
                    "sent": "If there are any questions, I'd be happy to answer.",
                    "label": 0
                },
                {
                    "sent": "Ann and I. Yeah, so so algorithms that minimize regret in this in this setting.",
                    "label": 0
                },
                {
                    "sent": "And most of them look like the following general template.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like all angry dissent, you would have some decision, which is a point in your convex set.",
                    "label": 0
                },
                {
                    "sent": "And given a lot function, you would move in the direction of the gradient of the loss function that the lock of the lawsuit just in curd.",
                    "label": 1
                },
                {
                    "sent": "And moving in the direction of the gradient might take you outside the convex set, so then you would have to project which means to find the closest point in the convex set to the point that you chose.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple gradient algorithm and encourage proved in 03 that by tuning the parameters this algorithm always attains the regret bounded by square root.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations.",
                    "label": 0
                },
                {
                    "sent": "And there's a very efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "It takes linear time aside from the projection operation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So that's what I have to say about this setting.",
                    "label": 0
                },
                {
                    "sent": "I'm being very brief because I assume that most of you know this very well.",
                    "label": 0
                },
                {
                    "sent": "And there are generalizations for this framework that look like Milana Mirror.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent that I'm not going to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grover, OK, so we would like to apply this learning framework, which is very general.",
                    "label": 0
                },
                {
                    "sent": "It is agnostic to learning time series.",
                    "label": 0
                },
                {
                    "sent": "So to learning in the ARMA model.",
                    "label": 0
                },
                {
                    "sent": "Now how would we try the naive way to model loss in the Arma setting would be to write down the loss function.",
                    "label": 0
                },
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "It depends on Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "And it looks like the loss of the real measurement and the linear combination of the previous measurement plus noise terms applied to the previous signals.",
                    "label": 0
                },
                {
                    "sent": "So that's what we would like to do, and there are two issues here.",
                    "label": 0
                },
                {
                    "sent": "First of all, we don't know the noise terms, so that's an obvious difficulty that prohibits us from using online convex optimization.",
                    "label": 0
                },
                {
                    "sent": "And second of all, this function is not convex in general.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the first thing we would like to cope with.",
                    "label": 0
                },
                {
                    "sent": "Is not knowing the noise so we can do the following trick.",
                    "label": 0
                },
                {
                    "sent": "We can write down an estimate for the noise.",
                    "label": 1
                },
                {
                    "sent": "Oh OK, so we can write down an estimate for the noise using all previous.",
                    "label": 0
                },
                {
                    "sent": "All previous signals, how do we estimate the noise?",
                    "label": 0
                },
                {
                    "sent": "Just write down the linear component given all previous noise estimates, you can write down the recursive definition and estimate the noise term.",
                    "label": 0
                },
                {
                    "sent": "In the way that I've written here, this, by the way, has been done in statistics, so this is a standard but not not in an agnostic setting.",
                    "label": 0
                },
                {
                    "sent": "And then you can write down a new loss function which takes into account this new estimate of the loss.",
                    "label": 0
                },
                {
                    "sent": "So we call it F Infinity of Alpha and beta that now we have no longer have noise terms.",
                    "label": 0
                },
                {
                    "sent": "But there are other difficulties that come into play.",
                    "label": 1
                },
                {
                    "sent": "First of all, this new function has infinite memory.",
                    "label": 0
                },
                {
                    "sent": "It needs to know all the previous measurements.",
                    "label": 0
                },
                {
                    "sent": "To define the loss for the next iteration and second of all, it is still not convex.",
                    "label": 1
                },
                {
                    "sent": "So here is the main.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the main idea.",
                    "label": 0
                },
                {
                    "sent": "So what we do is replace the Alpha and beta with a single parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "And right the last the last terms instead of having these coefficients that depend on both Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "And this can be very complex coefficients, right?",
                    "label": 0
                },
                {
                    "sent": "Because you open it up recursively by new coefficients, so this is called non proper learning.",
                    "label": 0
                },
                {
                    "sent": "We use one model to learn another model.",
                    "label": 0
                },
                {
                    "sent": "One model which is more robust or more general.",
                    "label": 1
                },
                {
                    "sent": "And the second thing is that we will clip the memory having infinite memories too restrictive.",
                    "label": 0
                },
                {
                    "sent": "So we clip the memory down to some finite number.",
                    "label": 0
                },
                {
                    "sent": "And the main observation hour.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our work is the following, so this is a quantitative theorem that corresponds to the non Quantitive theorem that she asked about.",
                    "label": 0
                },
                {
                    "sent": "We can prove that armor cake you this set of signals that can be represented by Cape Alpha Permitieron Que better parameters is included in a regret mean regret sense by the set of all a are signals of a larger number M where M behaves like K plus que log T. OK, so by taking a longer sequence of measurements we can minimize regret as if we had.",
                    "label": 0
                },
                {
                    "sent": "A stronger model but less coefficient, and so this inclusion is of course not precise.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that is that minimizing regret over the Big Blue ellipse gives you regret bounds with respect to the smaller ellipse.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now so now we can use online convex optimization.",
                    "label": 1
                },
                {
                    "sent": "So once we move to our model, everything is convex.",
                    "label": 0
                },
                {
                    "sent": "It's fine, it's bounded memory.",
                    "label": 0
                },
                {
                    "sent": "We can apply the gradient descent type algorithms or more sophisticated 2nd order algorithms and obtain regret bounds that follow by.",
                    "label": 0
                },
                {
                    "sent": "But just previous theorems in Kevin's theorem and so on.",
                    "label": 0
                },
                {
                    "sent": "And how does the algorithm look like?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When it's a gradient algorithm on top of MM is a larger, larger number than what we started off with coefficients of the type of gamma.",
                    "label": 0
                },
                {
                    "sent": "So given a new loss function, you would move in the direction of the gradient and project projection.",
                    "label": 0
                },
                {
                    "sent": "In this case is very simple.",
                    "label": 0
                },
                {
                    "sent": "We project onto the cube with whom we have a bounded set of parameters.",
                    "label": 0
                },
                {
                    "sent": "And via the gradient algorithms will obtain regret.",
                    "label": 0
                },
                {
                    "sent": "It looks like square root.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations and if you apply more sophisticated algorithm that takes into account also the second derivative of the loss function you can obtain.",
                    "label": 0
                },
                {
                    "sent": "Slightly better regret bounds.",
                    "label": 0
                },
                {
                    "sent": "Alright then.",
                    "label": 0
                },
                {
                    "sent": "So here are some.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments that we have conducted and this is first just a sanity check.",
                    "label": 1
                },
                {
                    "sent": "So we generated the time series and by just some bubbles.",
                    "label": 0
                },
                {
                    "sent": "And we picked Alpha and Beta, an generated noise generated a signal this way and apply their algorithms, and this experiment is just to show you that the algorithms do something reasonable.",
                    "label": 1
                },
                {
                    "sent": "And indeed if we have Gaussian noise, you can see here this is.",
                    "label": 0
                },
                {
                    "sent": "This is the average error, so all algorithms converge to a small average error even though you Walker equations that I talked about.",
                    "label": 0
                },
                {
                    "sent": "This is the previous state of the art.",
                    "label": 0
                },
                {
                    "sent": "But if now you have, what happens if you look at more robust signals but would like to have some advantage because it's an agnostic setting rather than assuming a strict ARMA model, and then indeed previous methods do not converge to a nice average error, whereas this gradient algorithms are more robust.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar things happen when we look at what happens when there is an abrupt change in parameters.",
                    "label": 0
                },
                {
                    "sent": "So the signal all of the sudden.",
                    "label": 0
                },
                {
                    "sent": "Changes in terms of the parameters.",
                    "label": 0
                },
                {
                    "sent": "And then when we look at more general models of noise, such as correlated noise, an in both settings, these are settings that have been studied in online learning.",
                    "label": 0
                },
                {
                    "sent": "So in some respect we expect these results and you can just take known results from online learning and plug them into the gradient algorithm that we come up with and have this guarantee automatically without working too hard.",
                    "label": 0
                },
                {
                    "sent": "OK this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens with real data?",
                    "label": 0
                },
                {
                    "sent": "And this is the first plot that I showed you of the average temperature of the oceans.",
                    "label": 0
                },
                {
                    "sent": "Are algorithms obtain a better error in terms of predicting the next?",
                    "label": 0
                },
                {
                    "sent": "And the next temperature and all algorithms, by the way, predicted global warming.",
                    "label": 0
                },
                {
                    "sent": "And in terms of stock market data, no one does well.",
                    "label": 0
                },
                {
                    "sent": "Which is also as we expect.",
                    "label": 0
                },
                {
                    "sent": "So we need to show here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to conclude.",
                    "label": 0
                },
                {
                    "sent": "So we have research and approach to time series analysis and forecasting that is based on online learning.",
                    "label": 1
                },
                {
                    "sent": "The technology that we all know and love and we're able to predict this best of the best ARMA model under some assumptions which are weaker than previously.",
                    "label": 0
                },
                {
                    "sent": "Staind and there are some nice interesting open questions here.",
                    "label": 1
                },
                {
                    "sent": "First of all, can we assumption relax the assumption on the noise coefficient?",
                    "label": 0
                },
                {
                    "sent": "This is, I think, a very nice technical question.",
                    "label": 0
                },
                {
                    "sent": "Can we come up with regret bounds without assuming that the noise the coefficient vector lies inside the unit ball, which is not a necessary condition in general?",
                    "label": 0
                },
                {
                    "sent": "And second of all, we would like to tackle more general models.",
                    "label": 0
                },
                {
                    "sent": "In particular the model proposed by angle studying financial time series, which is called the Archer Gouge.",
                    "label": 0
                },
                {
                    "sent": "So that would be very interesting as well.",
                    "label": 0
                },
                {
                    "sent": "And I will conclude here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}