{
    "id": "hoiosi2ossabkywwbzsjqhokfaainqph",
    "title": "Function class complexity and cluster structure with applications to transduction",
    "info": {
        "author": [
            "Guy Lever, Department of Computer Science, University College London"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2010_lever_fcc/",
    "segmentation": [
        [
            "OK, so in this work this works about relating function class complexity to cluster structure with a particular emphasis on some applications to transduction.",
            "So quick."
        ],
        [
            "I love you.",
            "I'm going to relate complexity function classes to cluster structure in the input space.",
            "So in the domain of the function class and the idea here really is that the learning process really depends to a large extent on the structure defined by the data generating distribution and maybe current analysis don't really capture that, or at least some analysis might improve by with a better understanding and a better analysis of that structure.",
            "And so I'm going to develop some kind of risk bounds relative to the cluster structure in data.",
            "And in particular, I'm going to investigate the complexity of learning functions defined over a graph.",
            "So specialized to this transductive setting of graph learning, and in that case I'm going to give transductive and semi supervised bounds relative to the cluster structure in what's called the resistance metric and this is nice 'cause it relates learning to kind of geometry that's defined by the data and kind of intrinsic structure of data."
        ],
        [
            "So my motivation for doing this really comes from this graph labeling problem.",
            "So imagine the problem of predicting a partially labeled graph like this one.",
            "And a key goal is to kind of understand the complexity of function classes defined over a graph, and clearly that's very heavily dependent on the structure of the graph, But that's something that's poorly understood from a learning theory perspective.",
            "That structure of a graph and existing analysis is sometimes only fairly weakly dependent on that structure.",
            "And so a goal is to kind of understand exactly how that structure affects the learning process and how it should appear in the bounds and analysis, and the approach that I take here is really inspired by recent online bounds by Mark Herbster in the online setting, so he proves some bounds relative to recovering number of the graph in a what's called the resistance metric relative to covering number of the graph of resistance metric and the smoothness of the true.",
            "Classifier on the graph and the kind of general goal is to kind of understand the role that this structure of the data generating distribution plays in."
        ],
        [
            "Learning in general.",
            "So I'm gonna try and relate learning to cluster structure, so just some quick definitions regarding clustering.",
            "So if we go into the metric space, clustering of any sample of points from that metric spaces, just any partition of that set, we just define the center of a cluster just to be the point in the space which minimizes the sum of the squared distances within the cluster, and for each point in the set we can just.",
            "We'll just do know it's corresponding sent."
        ],
        [
            "If I see a vex.",
            "Some preliminaries for the graph labeling problem then, so this is typically viewed by identifying each vertex with a standard basis vector in RN, so that any real valued function over the graph could just be identified with a vector in our end.",
            "It's just evaluates by the dot product.",
            "By taking a sign you just get a classification.",
            "And so this this class of functions can be equipped with this smoothness functional, which arises from the graph Laplacian.",
            "So L here is what's called the graph Laplacian and a is an adjacency matrix which captures similarity between points and this movement functional then measures in a nice way smoothness of a function over a graph will just do it by H5, the set of binary classifiers to smoothness is."
        ],
        [
            "Greater than five, so we're going to try and capture the capacity of that Class H. Viren function classes in general, so we just recall the definition of Rademacher complexity, so this just captures how well a function class can fit random noise, which is given by the Rademacher Variable Sigma.",
            "So this is the Rademacher variable on the with respect to a sample, and if we take expectations we get the Rademacher complexity with respect to the underlying data generating distribution.",
            "So this gives bounds a typically sharper than VC bounds, because it's data dependent.",
            "So for example, in our graph labeling problem.",
            "So imagine trying to understand the capacity of.",
            "This function Class H fi on this end Rutan Lollipop graph so this is a graph with N vertices in a clique and root N vertices in the path section.",
            "Then the Rademacher complexity would essentially measure the capacity of the graph of the function class on the clique, 'cause that's where most of the samples are coming from.",
            "But the VC dimension and measure the worst case and the capacity on the path graph and that can be much larger.",
            "So I think the Rademacher bound is probably prep."
        ],
        [
            "So in general, and certainly in this case, there's a strong argument for it there, so we're going to try and relate this capacity to cluster structure.",
            "So we need some kind of metric space structure on our input space, but I think that the right way to go about doing this is to just observe that there's a natural duality between a norm on the hypothesis class and a distance on the input space.",
            "So given any normanna hypothesis class, we can just define this implied metric on our input space, which just arises from the dual norm to that normal hypothesis class.",
            "So the intuition is that if that norm is measuring complexity in some way, then if two functions can be distinctly classified, two points in our space can be distinctly classified by very simple function than their distant.",
            "And Conversely, flick can be distinctly classified only by complicated functions, and they're very close, and that's the metric we use to measure cluster structure.",
            "And the simple example is if.",
            "Hypothesis classes narke chess, that's just."
        ],
        [
            "The feature space distance but another really interesting example comes from graph labeling examples.",
            "So file hypothesis classes functions over a graph equipped with the norm, which is the smoothness on the graph.",
            "Then the implied metric in this case is actually equivalent to resistance distance.",
            "So this arises by viewing the graph as an electrical network, where each edge is a resistor of conductance equal to the edge weight.",
            "And this defines a family of effective resistances between all of the vertices.",
            "So, for example, that the effective resistance in B&C is much smaller than between A&B, 'cause there are lots of parallel disjoint path for current to flow, so it captures connectivity and distance in a graph and in the right kind of nice way.",
            "So the interesting thing is that if we learning on a graph under these smoothness typical smoothness assumptions, this is the metric space structure that we're kind of implicitly implying on our input space, and I've given that the formula for the effective resistance in terms of the pseudo inverse of the graph laplacians."
        ],
        [
            "So this is the bound which relates complexity to the cluster structure in the input space, so it's an extension of a recent nice bound by Cacador and Co.",
            "Authors who use a kind of convex duality argument to upper bound, the Rademacher complexity.",
            "And I think Tom Young as well had a paper on this guy thing.",
            "So essentially, if we're given a function F which measures complexity of our hypothesis, and this is strongly convex with respect some normanna hypothesis class, and we just denoted by H. Alpha, the class of functions whose complexity is no greater than Alpha and the result says that for any sample of points from our input space and for all clusterings of those points, and the writer market complexity of this Class H. Alpha.",
            "Can be upper bounded by a term in the number of clusters.",
            "Determine Alpha, which is the complexity plus this row S term, which is essentially the average of the squared distances to the cluster centers.",
            "So K mean objective that clustering in this implied metric.",
            "So we've got a complexity function F which is strongly convex with respect some norm.",
            "The cluster structures measured with respect to.",
            "The norm on the metric on the input space implied by the jewel to that norm on the hypothesis class is that relates learning to cross structure indent in general and obviously"
        ],
        [
            "Is optimized by good K means clustering by taking expectations.",
            "With that theorem we just get.",
            "A bound for the Rademacher complexity, in terms of the cluster structure of the underlying data generating distribution."
        ],
        [
            "OK, so a natural question to ask is is is doing that kind of clustering an improvement over a typical analysis?",
            "So I think 1.",
            "Nice example where it's really a definite strong improvement is is this resistive geometry.",
            "So the resistance is essentially really sensitive to clustering.",
            "So if you imagine two points in our input space A&B."
        ],
        [
            "And we continue to."
        ],
        [
            "Add."
        ],
        [
            "Points nearby to create."
        ],
        [
            "A cluster clique.",
            "The resistance between those points essentially decreasing at a rate of one over the number of points.",
            "So it's essentially defined by the density of the data distribution, so it's really sensitive to clustering, and you wouldn't get that kind of behavior with any kind of non empirical metric, since they wouldn't just just wouldn't be defined by the the data distribution."
        ],
        [
            "So it seems like that's a really nice example in which to kind of look at this analysis.",
            "So we specialize at this theorem to the case of transduction.",
            "So the transaction is where we're given our unlabeled test set at the start of the learning process, and we view our training samples drawn uniformly without replacement from the whole collection of data.",
            "And we can view this is from as defining a graph and again from this hypothesis class H5 binary functions.",
            "So smoothness is no greater than five.",
            "And the the fair and specializes to the following.",
            "So given any clustering of our data or clustering vertices on our graph.",
            "The transductive Rademacher complexity of H5, so that's just emphasizing we're in the transductive setting and know something about the distribution of the sample.",
            "Can be bounded by.",
            "Again, this term in the number of clusters or the expected number of clusters in the sample.",
            "Plus determine if I so the smoothness and this term row, which is the essentially K, means objective of the clustering in the resistance metric.",
            "So this resembles the bound of Herbster in the from the online setting that I mentioned earlier, and this relates learning on a graph to clustering."
        ],
        [
            "Resistance generally.",
            "So there exists other bounds on the capacity of that function class.",
            "So for example, Kleinberg gives a bound on the VC dimension which is order five or five star with five stars, the edge connectivity, so the minimum number of edges required to disconnect the graph.",
            "So it's compared to that.",
            "So on the lollipop type constructions from earlier or really any graph where the majority of vertices are in a kind of highly cluster structured.",
            "Look a lot of cluster structure, but where there is also some some kind of more sparsely structured elements, then typically the Rademacher Bell will be better.",
            "Another example comes from N barbell graph, so this is 2 N cliques connected by bridge and we can evaluate the Rademacher and VC bounds exactly in this case, and the right comparison to choose the right amacher bound compared to the root VC bound over M. And we're seeing the Rademacher band we pick up an extra term end in the second.",
            "An extra factor of N in the denominator of the second term.",
            "So that comes from the fact that the resistance within these cliques is so small.",
            "So it seems like a big improvement in that case and.",
            "You can, you can relax the connectivity within those cliques quite a lot and still maintain a good bound and this really shows the kind of advantage of doing this clustering as well, because the resistance between clusters is is really large and this whole analysis holds for weighted graphs, and that distance can become arbitrarily large."
        ],
        [
            "It's not a uniform improvement though, unfortunately, so.",
            "And the opposite end of the spectrum, where there's really very little cluster structure, so the VC bound is better.",
            "So on a path graph, the VC bound is tight, but the Rademacher bounds actually vacuous.",
            "Actually, this is improved in a really interesting way, so it's improved by passing to what's called P resistance.",
            "So essentially there's a family of P norms you can define on the space of graph labelings, which generalize the smoothness, functional, and they imply a different kind of geometry on the graph, which is P resistance and essentially the Rademacher bound holds for all values of P simultaneously, and values of P closer to one or smaller values of P are more suitable for analysis on sparse graphs, so that really improves it doesn't quite catch up with the VC ban.",
            "I think that's still better on the path graph."
        ],
        [
            "OK, so we have what looks like a nice bound on the capacity of that function class.",
            "So let's just arrived some risk analysis from it.",
            "So just define the transductive risk to be the risk on the unlabeled set.",
            "The risk on the test set, and the result that we can derive essentially says that for any clustering of our data of our graph.",
            "I'm.",
            "And for any, essentially for any binary labeling of this data that we can bound the deviation between the transductive risk and the empirical risk, again by a term in the number of clusters, determine the smoothness of that hypothesis.",
            "And again, this term Roe, which is the objective of the clustering.",
            "So the sum of the average of the squared distances in the resistance metric.",
            "So that relates risk in transduction to this clustering of the data in this nice natural resistance metric quite generally, and so that suitable for.",
            "Any algorithm, essentially the outputs are binary labeling of a graph, so there are lots of those.",
            "I've just listed a few there, but also maybe it suggests some new algorithms obtained by minimizing that bound or minimizing a relaxation of that bound somehow over clusterings, classifiers, and maybe even P, which is the parameter of.",
            "The geometry you want to work with on the graph, essentially and.",
            "If there are various ways of doing that, one way you would bench essentially arrive at a kind of Laplacian regularization kind of algorithm, where the regularization parameters are determined by quite a detailed decomposition of the data, essentially, which feels kind of like, you know, you should be able to learn that parameter from essentially from the data, especially in transit."
        ],
        [
            "Auction.",
            "So quick comparison to other bands in the literature so Hanukkah.",
            "Gives a similar bound for the transductive risk, which is again related to the smoothness of the classifier and essentially this is the kind of bound you'd get if you could plug in the VC bound.",
            "So this is better than the bound of just presented, and whenever the VC bound is better, so when there's a when you're in that kind of case of a high degree of sparsity, and certainly on something like a path graph, this band would still be better and at the other end of the spectrum the huge amount of cluster structure.",
            "The Rademacher Bounds definitely better.",
            "But somewhere in the middle that meet."
        ],
        [
            "Pokemons and short tail are given.",
            "Analysis of risk relative to the spectrum of the graph Laplacian.",
            "So I think this is really nice and I think that this spectrum of passing ought to be kind of very good way of capturing the structure in a graph that's relevant to learning, but it's a bit more difficult to.",
            "Compare that to the analytically, it's less tangible, but in the paper I give a kind of attempt at doing that."
        ],
        [
            "Empirically.",
            "OK, and finally ioffer an extension.",
            "Semi supervised learning.",
            "So here we just given some unlabeled data at the start of the learning process, but have to kind of learn an out of sample classifier.",
            "So the point is, here we can again relate the risk to the cluster structure in all the labeled and unlabeled data, and one way to do this is essentially to define two structuring of your hypothesis class.",
            "So one structuring which is very data dependent and another structuring which isn't informed by the data sample at all.",
            "And you can bound the risk essentially by the transductive Rademacher complexity of some data defined class and the inductive Rademacher complexity of another non dated fine class.",
            "And the point really is that.",
            "The Rademacher the transductive Rademacher term should be small because you view this essentially an empirically defined hypothesis class.",
            "And if you used all of this extra data to form that class, and so you ought to be able to to make the search space quite small and find a good classifier in a fairly small hypothesis class.",
            "The inductive term ought to be small because it's essentially decaying at a rate of order one over.",
            "The square root of the total number of samples.",
            "So if you've got a lot of unlabeled data, there's essentially very little loss in going from there.",
            "Transductive risk today.",
            "General.",
            "Inductive risk."
        ],
        [
            "OK, so that's it.",
            "So I've related complexity to cluster structure to the clustering data generally.",
            "Particular trust specialized to clustering in resistance geometry.",
            "And given a risk analysis for transduction with respect to that geometry.",
            "Maybe this suggests some new kind of algorithms for for learning, in particular transductive setting.",
            "And there's I mean, there's still definite open problems here.",
            "So I mean, I don't think that I've kind of solved this problem particularly.",
            "I think you know understanding the structure of a graph of data more generally and how it should appear in the learning process is still, you know, I've offered one kind of analysis of the structure, and I think they ought to be more general analysis, and there are lots of ways to capture that structure, and.",
            "You know there are lots of interesting approaches to it.",
            "OK, that's it, thanks."
        ],
        [
            "Question.",
            "I think I've told you about this approximation results with Mcavoy's that works just in a Hilbert space.",
            "No, I don't think so.",
            "I've never had it.",
            "Approximation in a Hilbert space that relates the number of terms and the basis expansion to the cluster structure in a way right.",
            "It actually gives a more fine grained view of the cluster structure that you use, but but it's restricted to the space case.",
            "There might be something worth comparing.",
            "2 alright.",
            "Yeah, I start exactly using that technique shows that if your clusters are robust to small cultivations then you can cluster them efficiently with exactly that it was OK.",
            "I'm not aware of that.",
            "Cluster data alright.",
            "Nations, then you can have prototype clustering OK.",
            "Right, I'll get the references later.",
            "Yeah, cool thanks.",
            "So thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in this work this works about relating function class complexity to cluster structure with a particular emphasis on some applications to transduction.",
                    "label": 0
                },
                {
                    "sent": "So quick.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I love you.",
                    "label": 0
                },
                {
                    "sent": "I'm going to relate complexity function classes to cluster structure in the input space.",
                    "label": 1
                },
                {
                    "sent": "So in the domain of the function class and the idea here really is that the learning process really depends to a large extent on the structure defined by the data generating distribution and maybe current analysis don't really capture that, or at least some analysis might improve by with a better understanding and a better analysis of that structure.",
                    "label": 1
                },
                {
                    "sent": "And so I'm going to develop some kind of risk bounds relative to the cluster structure in data.",
                    "label": 0
                },
                {
                    "sent": "And in particular, I'm going to investigate the complexity of learning functions defined over a graph.",
                    "label": 1
                },
                {
                    "sent": "So specialized to this transductive setting of graph learning, and in that case I'm going to give transductive and semi supervised bounds relative to the cluster structure in what's called the resistance metric and this is nice 'cause it relates learning to kind of geometry that's defined by the data and kind of intrinsic structure of data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my motivation for doing this really comes from this graph labeling problem.",
                    "label": 0
                },
                {
                    "sent": "So imagine the problem of predicting a partially labeled graph like this one.",
                    "label": 0
                },
                {
                    "sent": "And a key goal is to kind of understand the complexity of function classes defined over a graph, and clearly that's very heavily dependent on the structure of the graph, But that's something that's poorly understood from a learning theory perspective.",
                    "label": 1
                },
                {
                    "sent": "That structure of a graph and existing analysis is sometimes only fairly weakly dependent on that structure.",
                    "label": 0
                },
                {
                    "sent": "And so a goal is to kind of understand exactly how that structure affects the learning process and how it should appear in the bounds and analysis, and the approach that I take here is really inspired by recent online bounds by Mark Herbster in the online setting, so he proves some bounds relative to recovering number of the graph in a what's called the resistance metric relative to covering number of the graph of resistance metric and the smoothness of the true.",
                    "label": 0
                },
                {
                    "sent": "Classifier on the graph and the kind of general goal is to kind of understand the role that this structure of the data generating distribution plays in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning in general.",
                    "label": 0
                },
                {
                    "sent": "So I'm gonna try and relate learning to cluster structure, so just some quick definitions regarding clustering.",
                    "label": 0
                },
                {
                    "sent": "So if we go into the metric space, clustering of any sample of points from that metric spaces, just any partition of that set, we just define the center of a cluster just to be the point in the space which minimizes the sum of the squared distances within the cluster, and for each point in the set we can just.",
                    "label": 1
                },
                {
                    "sent": "We'll just do know it's corresponding sent.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If I see a vex.",
                    "label": 0
                },
                {
                    "sent": "Some preliminaries for the graph labeling problem then, so this is typically viewed by identifying each vertex with a standard basis vector in RN, so that any real valued function over the graph could just be identified with a vector in our end.",
                    "label": 0
                },
                {
                    "sent": "It's just evaluates by the dot product.",
                    "label": 0
                },
                {
                    "sent": "By taking a sign you just get a classification.",
                    "label": 0
                },
                {
                    "sent": "And so this this class of functions can be equipped with this smoothness functional, which arises from the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "So L here is what's called the graph Laplacian and a is an adjacency matrix which captures similarity between points and this movement functional then measures in a nice way smoothness of a function over a graph will just do it by H5, the set of binary classifiers to smoothness is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Greater than five, so we're going to try and capture the capacity of that Class H. Viren function classes in general, so we just recall the definition of Rademacher complexity, so this just captures how well a function class can fit random noise, which is given by the Rademacher Variable Sigma.",
                    "label": 0
                },
                {
                    "sent": "So this is the Rademacher variable on the with respect to a sample, and if we take expectations we get the Rademacher complexity with respect to the underlying data generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So this gives bounds a typically sharper than VC bounds, because it's data dependent.",
                    "label": 1
                },
                {
                    "sent": "So for example, in our graph labeling problem.",
                    "label": 0
                },
                {
                    "sent": "So imagine trying to understand the capacity of.",
                    "label": 0
                },
                {
                    "sent": "This function Class H fi on this end Rutan Lollipop graph so this is a graph with N vertices in a clique and root N vertices in the path section.",
                    "label": 0
                },
                {
                    "sent": "Then the Rademacher complexity would essentially measure the capacity of the graph of the function class on the clique, 'cause that's where most of the samples are coming from.",
                    "label": 0
                },
                {
                    "sent": "But the VC dimension and measure the worst case and the capacity on the path graph and that can be much larger.",
                    "label": 0
                },
                {
                    "sent": "So I think the Rademacher bound is probably prep.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, and certainly in this case, there's a strong argument for it there, so we're going to try and relate this capacity to cluster structure.",
                    "label": 0
                },
                {
                    "sent": "So we need some kind of metric space structure on our input space, but I think that the right way to go about doing this is to just observe that there's a natural duality between a norm on the hypothesis class and a distance on the input space.",
                    "label": 0
                },
                {
                    "sent": "So given any normanna hypothesis class, we can just define this implied metric on our input space, which just arises from the dual norm to that normal hypothesis class.",
                    "label": 1
                },
                {
                    "sent": "So the intuition is that if that norm is measuring complexity in some way, then if two functions can be distinctly classified, two points in our space can be distinctly classified by very simple function than their distant.",
                    "label": 0
                },
                {
                    "sent": "And Conversely, flick can be distinctly classified only by complicated functions, and they're very close, and that's the metric we use to measure cluster structure.",
                    "label": 1
                },
                {
                    "sent": "And the simple example is if.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis classes narke chess, that's just.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The feature space distance but another really interesting example comes from graph labeling examples.",
                    "label": 0
                },
                {
                    "sent": "So file hypothesis classes functions over a graph equipped with the norm, which is the smoothness on the graph.",
                    "label": 1
                },
                {
                    "sent": "Then the implied metric in this case is actually equivalent to resistance distance.",
                    "label": 1
                },
                {
                    "sent": "So this arises by viewing the graph as an electrical network, where each edge is a resistor of conductance equal to the edge weight.",
                    "label": 0
                },
                {
                    "sent": "And this defines a family of effective resistances between all of the vertices.",
                    "label": 0
                },
                {
                    "sent": "So, for example, that the effective resistance in B&C is much smaller than between A&B, 'cause there are lots of parallel disjoint path for current to flow, so it captures connectivity and distance in a graph and in the right kind of nice way.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing is that if we learning on a graph under these smoothness typical smoothness assumptions, this is the metric space structure that we're kind of implicitly implying on our input space, and I've given that the formula for the effective resistance in terms of the pseudo inverse of the graph laplacians.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the bound which relates complexity to the cluster structure in the input space, so it's an extension of a recent nice bound by Cacador and Co.",
                    "label": 1
                },
                {
                    "sent": "Authors who use a kind of convex duality argument to upper bound, the Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "And I think Tom Young as well had a paper on this guy thing.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if we're given a function F which measures complexity of our hypothesis, and this is strongly convex with respect some normanna hypothesis class, and we just denoted by H. Alpha, the class of functions whose complexity is no greater than Alpha and the result says that for any sample of points from our input space and for all clusterings of those points, and the writer market complexity of this Class H. Alpha.",
                    "label": 0
                },
                {
                    "sent": "Can be upper bounded by a term in the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Determine Alpha, which is the complexity plus this row S term, which is essentially the average of the squared distances to the cluster centers.",
                    "label": 1
                },
                {
                    "sent": "So K mean objective that clustering in this implied metric.",
                    "label": 0
                },
                {
                    "sent": "So we've got a complexity function F which is strongly convex with respect some norm.",
                    "label": 0
                },
                {
                    "sent": "The cluster structures measured with respect to.",
                    "label": 1
                },
                {
                    "sent": "The norm on the metric on the input space implied by the jewel to that norm on the hypothesis class is that relates learning to cross structure indent in general and obviously",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is optimized by good K means clustering by taking expectations.",
                    "label": 0
                },
                {
                    "sent": "With that theorem we just get.",
                    "label": 0
                },
                {
                    "sent": "A bound for the Rademacher complexity, in terms of the cluster structure of the underlying data generating distribution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so a natural question to ask is is is doing that kind of clustering an improvement over a typical analysis?",
                    "label": 0
                },
                {
                    "sent": "So I think 1.",
                    "label": 0
                },
                {
                    "sent": "Nice example where it's really a definite strong improvement is is this resistive geometry.",
                    "label": 0
                },
                {
                    "sent": "So the resistance is essentially really sensitive to clustering.",
                    "label": 1
                },
                {
                    "sent": "So if you imagine two points in our input space A&B.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we continue to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points nearby to create.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A cluster clique.",
                    "label": 0
                },
                {
                    "sent": "The resistance between those points essentially decreasing at a rate of one over the number of points.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially defined by the density of the data distribution, so it's really sensitive to clustering, and you wouldn't get that kind of behavior with any kind of non empirical metric, since they wouldn't just just wouldn't be defined by the the data distribution.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it seems like that's a really nice example in which to kind of look at this analysis.",
                    "label": 0
                },
                {
                    "sent": "So we specialize at this theorem to the case of transduction.",
                    "label": 0
                },
                {
                    "sent": "So the transaction is where we're given our unlabeled test set at the start of the learning process, and we view our training samples drawn uniformly without replacement from the whole collection of data.",
                    "label": 1
                },
                {
                    "sent": "And we can view this is from as defining a graph and again from this hypothesis class H5 binary functions.",
                    "label": 0
                },
                {
                    "sent": "So smoothness is no greater than five.",
                    "label": 1
                },
                {
                    "sent": "And the the fair and specializes to the following.",
                    "label": 0
                },
                {
                    "sent": "So given any clustering of our data or clustering vertices on our graph.",
                    "label": 0
                },
                {
                    "sent": "The transductive Rademacher complexity of H5, so that's just emphasizing we're in the transductive setting and know something about the distribution of the sample.",
                    "label": 0
                },
                {
                    "sent": "Can be bounded by.",
                    "label": 0
                },
                {
                    "sent": "Again, this term in the number of clusters or the expected number of clusters in the sample.",
                    "label": 1
                },
                {
                    "sent": "Plus determine if I so the smoothness and this term row, which is the essentially K, means objective of the clustering in the resistance metric.",
                    "label": 0
                },
                {
                    "sent": "So this resembles the bound of Herbster in the from the online setting that I mentioned earlier, and this relates learning on a graph to clustering.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Resistance generally.",
                    "label": 0
                },
                {
                    "sent": "So there exists other bounds on the capacity of that function class.",
                    "label": 0
                },
                {
                    "sent": "So for example, Kleinberg gives a bound on the VC dimension which is order five or five star with five stars, the edge connectivity, so the minimum number of edges required to disconnect the graph.",
                    "label": 1
                },
                {
                    "sent": "So it's compared to that.",
                    "label": 0
                },
                {
                    "sent": "So on the lollipop type constructions from earlier or really any graph where the majority of vertices are in a kind of highly cluster structured.",
                    "label": 0
                },
                {
                    "sent": "Look a lot of cluster structure, but where there is also some some kind of more sparsely structured elements, then typically the Rademacher Bell will be better.",
                    "label": 0
                },
                {
                    "sent": "Another example comes from N barbell graph, so this is 2 N cliques connected by bridge and we can evaluate the Rademacher and VC bounds exactly in this case, and the right comparison to choose the right amacher bound compared to the root VC bound over M. And we're seeing the Rademacher band we pick up an extra term end in the second.",
                    "label": 0
                },
                {
                    "sent": "An extra factor of N in the denominator of the second term.",
                    "label": 0
                },
                {
                    "sent": "So that comes from the fact that the resistance within these cliques is so small.",
                    "label": 0
                },
                {
                    "sent": "So it seems like a big improvement in that case and.",
                    "label": 0
                },
                {
                    "sent": "You can, you can relax the connectivity within those cliques quite a lot and still maintain a good bound and this really shows the kind of advantage of doing this clustering as well, because the resistance between clusters is is really large and this whole analysis holds for weighted graphs, and that distance can become arbitrarily large.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's not a uniform improvement though, unfortunately, so.",
                    "label": 0
                },
                {
                    "sent": "And the opposite end of the spectrum, where there's really very little cluster structure, so the VC bound is better.",
                    "label": 0
                },
                {
                    "sent": "So on a path graph, the VC bound is tight, but the Rademacher bounds actually vacuous.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is improved in a really interesting way, so it's improved by passing to what's called P resistance.",
                    "label": 1
                },
                {
                    "sent": "So essentially there's a family of P norms you can define on the space of graph labelings, which generalize the smoothness, functional, and they imply a different kind of geometry on the graph, which is P resistance and essentially the Rademacher bound holds for all values of P simultaneously, and values of P closer to one or smaller values of P are more suitable for analysis on sparse graphs, so that really improves it doesn't quite catch up with the VC ban.",
                    "label": 0
                },
                {
                    "sent": "I think that's still better on the path graph.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have what looks like a nice bound on the capacity of that function class.",
                    "label": 0
                },
                {
                    "sent": "So let's just arrived some risk analysis from it.",
                    "label": 0
                },
                {
                    "sent": "So just define the transductive risk to be the risk on the unlabeled set.",
                    "label": 1
                },
                {
                    "sent": "The risk on the test set, and the result that we can derive essentially says that for any clustering of our data of our graph.",
                    "label": 1
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And for any, essentially for any binary labeling of this data that we can bound the deviation between the transductive risk and the empirical risk, again by a term in the number of clusters, determine the smoothness of that hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And again, this term Roe, which is the objective of the clustering.",
                    "label": 0
                },
                {
                    "sent": "So the sum of the average of the squared distances in the resistance metric.",
                    "label": 0
                },
                {
                    "sent": "So that relates risk in transduction to this clustering of the data in this nice natural resistance metric quite generally, and so that suitable for.",
                    "label": 0
                },
                {
                    "sent": "Any algorithm, essentially the outputs are binary labeling of a graph, so there are lots of those.",
                    "label": 0
                },
                {
                    "sent": "I've just listed a few there, but also maybe it suggests some new algorithms obtained by minimizing that bound or minimizing a relaxation of that bound somehow over clusterings, classifiers, and maybe even P, which is the parameter of.",
                    "label": 1
                },
                {
                    "sent": "The geometry you want to work with on the graph, essentially and.",
                    "label": 0
                },
                {
                    "sent": "If there are various ways of doing that, one way you would bench essentially arrive at a kind of Laplacian regularization kind of algorithm, where the regularization parameters are determined by quite a detailed decomposition of the data, essentially, which feels kind of like, you know, you should be able to learn that parameter from essentially from the data, especially in transit.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Auction.",
                    "label": 0
                },
                {
                    "sent": "So quick comparison to other bands in the literature so Hanukkah.",
                    "label": 0
                },
                {
                    "sent": "Gives a similar bound for the transductive risk, which is again related to the smoothness of the classifier and essentially this is the kind of bound you'd get if you could plug in the VC bound.",
                    "label": 0
                },
                {
                    "sent": "So this is better than the bound of just presented, and whenever the VC bound is better, so when there's a when you're in that kind of case of a high degree of sparsity, and certainly on something like a path graph, this band would still be better and at the other end of the spectrum the huge amount of cluster structure.",
                    "label": 0
                },
                {
                    "sent": "The Rademacher Bounds definitely better.",
                    "label": 0
                },
                {
                    "sent": "But somewhere in the middle that meet.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pokemons and short tail are given.",
                    "label": 0
                },
                {
                    "sent": "Analysis of risk relative to the spectrum of the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So I think this is really nice and I think that this spectrum of passing ought to be kind of very good way of capturing the structure in a graph that's relevant to learning, but it's a bit more difficult to.",
                    "label": 0
                },
                {
                    "sent": "Compare that to the analytically, it's less tangible, but in the paper I give a kind of attempt at doing that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Empirically.",
                    "label": 0
                },
                {
                    "sent": "OK, and finally ioffer an extension.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So here we just given some unlabeled data at the start of the learning process, but have to kind of learn an out of sample classifier.",
                    "label": 0
                },
                {
                    "sent": "So the point is, here we can again relate the risk to the cluster structure in all the labeled and unlabeled data, and one way to do this is essentially to define two structuring of your hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So one structuring which is very data dependent and another structuring which isn't informed by the data sample at all.",
                    "label": 0
                },
                {
                    "sent": "And you can bound the risk essentially by the transductive Rademacher complexity of some data defined class and the inductive Rademacher complexity of another non dated fine class.",
                    "label": 0
                },
                {
                    "sent": "And the point really is that.",
                    "label": 0
                },
                {
                    "sent": "The Rademacher the transductive Rademacher term should be small because you view this essentially an empirically defined hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "And if you used all of this extra data to form that class, and so you ought to be able to to make the search space quite small and find a good classifier in a fairly small hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "The inductive term ought to be small because it's essentially decaying at a rate of order one over.",
                    "label": 0
                },
                {
                    "sent": "The square root of the total number of samples.",
                    "label": 0
                },
                {
                    "sent": "So if you've got a lot of unlabeled data, there's essentially very little loss in going from there.",
                    "label": 0
                },
                {
                    "sent": "Transductive risk today.",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                },
                {
                    "sent": "Inductive risk.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "So I've related complexity to cluster structure to the clustering data generally.",
                    "label": 1
                },
                {
                    "sent": "Particular trust specialized to clustering in resistance geometry.",
                    "label": 1
                },
                {
                    "sent": "And given a risk analysis for transduction with respect to that geometry.",
                    "label": 0
                },
                {
                    "sent": "Maybe this suggests some new kind of algorithms for for learning, in particular transductive setting.",
                    "label": 0
                },
                {
                    "sent": "And there's I mean, there's still definite open problems here.",
                    "label": 0
                },
                {
                    "sent": "So I mean, I don't think that I've kind of solved this problem particularly.",
                    "label": 0
                },
                {
                    "sent": "I think you know understanding the structure of a graph of data more generally and how it should appear in the learning process is still, you know, I've offered one kind of analysis of the structure, and I think they ought to be more general analysis, and there are lots of ways to capture that structure, and.",
                    "label": 0
                },
                {
                    "sent": "You know there are lots of interesting approaches to it.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "I think I've told you about this approximation results with Mcavoy's that works just in a Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "No, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "I've never had it.",
                    "label": 0
                },
                {
                    "sent": "Approximation in a Hilbert space that relates the number of terms and the basis expansion to the cluster structure in a way right.",
                    "label": 0
                },
                {
                    "sent": "It actually gives a more fine grained view of the cluster structure that you use, but but it's restricted to the space case.",
                    "label": 0
                },
                {
                    "sent": "There might be something worth comparing.",
                    "label": 0
                },
                {
                    "sent": "2 alright.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I start exactly using that technique shows that if your clusters are robust to small cultivations then you can cluster them efficiently with exactly that it was OK.",
                    "label": 0
                },
                {
                    "sent": "I'm not aware of that.",
                    "label": 0
                },
                {
                    "sent": "Cluster data alright.",
                    "label": 0
                },
                {
                    "sent": "Nations, then you can have prototype clustering OK.",
                    "label": 0
                },
                {
                    "sent": "Right, I'll get the references later.",
                    "label": 0
                },
                {
                    "sent": "Yeah, cool thanks.",
                    "label": 0
                },
                {
                    "sent": "So thank you again.",
                    "label": 0
                }
            ]
        }
    }
}