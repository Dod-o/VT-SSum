{
    "id": "z27m6sys5ppxjrodznhbl3rxs7tq45vq",
    "title": "L1-based relaxations for sparsity recovery and graphical model selection in the high-dimensional regime",
    "info": {
        "author": [
            "Martin J. Wainwright, UC Berkeley"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_wainwright_brsrg/",
    "segmentation": [
        [
            "Will come back.",
            "So we're glad to have Doctor Martin Wainwright to give us the research talk, and so let's welcome.",
            "OK, so I'd like to talk a bit about the use of L1 regularization.",
            "And in particular, how it's useful for sparsity recovery and also for the problem with model selection.",
            "So the work on model selection is based on joint work with John Lafferty and probably Praveen Kumar at CMU."
        ],
        [
            "You OK, so at a high level, the problem of sparsity recovery is is how do you recover as a signal.",
            "Let's think about it.",
            "This is some vector beta star that's suitably sparse.",
            "I'll be precise about what suitably sparse means in a minute.",
            "On the basis of a bunch of noisy observations.",
            "So it's a.",
            "It's a fairly simple problem, but it's one that actually has a pretty broad range of applications.",
            "Perhaps the most immediately obvious one is subset selection in regression, and I'll talk a bit about that on the next slide.",
            "It's also been studied a lot in the context of signal denoising and sparse constructive approximation, and you can also think about the problem of graphical model selection.",
            "Actually is a problem of recovering the sparse signal.",
            "So the thing with sparsity, the natural way to think about it in terms of an optimization problem is in terms of what you might call an L0 norm norm here is in.",
            "Quotation marks because it's not really a norm by this norm, we mean just the number.",
            "Just something that counts the number of elements in a vector that are non 0.",
            "So you're just looking at the size of the support of the vector.",
            "Typically you can formulate problems like this in terms of constraints that involve L zero constraints.",
            "But those L 0 problems are well known to be NP hard in general, so that's what motivates looking for methods that are actually computationally tractable, but hopefully."
        ],
        [
            "Work as well.",
            "So let's just start with subset selection in regression.",
            "This is a very classical problem in statistics.",
            "Just to keep things simple, let's let's just look at the standard linear regression model you've got observed data.",
            "XK and.",
            "Interesting, I see what you mean now.",
            "It's got features just like Microsoft.",
            "OK. OK, anyway we've got observed data pairs XK.",
            "These are vectors and YC.",
            "In this case is just a scalar and what you're assuming is that they are generated by the standard linear regression.",
            "So you're just taking the inner product between the data and some fixed vector beta star.",
            "This is the sparse thing that we're going to be after and things were contaminated by some Gaussian noise.",
            "What's interesting is there lots of applications where you can think that X includes a large number of variables that are actually completely irrelevant, so there's sort of nuisance variables that distract you and what you'd like to do is thank you.",
            "Hopefully the third time was lucky.",
            "Good, that's great, So what you'd like to do is sort of recover this subset of industries and beta star that correspond to elements of exit are actually relevant.",
            "And at the same time you'd like to suppress or ignore.",
            "You'd like to get rid of all the irrelevant variables.",
            "OK. Moving up in the world.",
            "Right, so you want to think about taxes being very long.",
            "X might have you know 100,000 components, but there might be a very small subset, maybe 100 or 10 that are relevant.",
            "So problems like this arise a lot in bioinformatics.",
            "For instance, with gene problems may also arise very naturally in sparse representations in signal processing.",
            "Actually historically has been a fair bit of work on the."
        ],
        [
            "Just to give you an illustration.",
            "A lot of what people sort of area is pretty active in signal processing.",
            "Is looking at how do you reconstruct signals optimally when you have overcomplete bases.",
            "So if you remember a basis is just a linearly independent and complete subset of a vector space.",
            "So for instance the Fourier basis is 1 basis here.",
            "Also the basis of spikes that this samples this.",
            "I'm looking at a signal of length 256 that would be another basis of this signal.",
            "So what's interesting is that you can have signals like this one here.",
            "Is very simple to set toys such a sinusoid here plus a few spikes at some locations so you can see that it's not sparse in either one of the two bases alone.",
            "If you just trying to reconstruct it using spikes alone, you have to use all 256 pairs.",
            "And if you try and reconstruct it in a Fourier basis, the fact that you have these little spikes that gives you high frequency or it gives you broadband frequency information, so it's not at all sparse in the Fourier basis.",
            "But the thing here is that it's very obviously it's sparse.",
            "If I was allowed to use a combination, this is called dictionary selection.",
            "If I was allowed to choose a subset of Fourier coefficients and a subset of spiced coefficients, then I could in fact reconstruct this signal exactly very sparsely.",
            "I could just use one Fourier coefficient and a very small subset of spikes.",
            "So this is this is a problem with sparse approximation and here it's sort of obvious.",
            "You can just look at it, but if you think about this for real signals, it's quite a challenging problem.",
            "You have to search over all subsets of your dictionary to try and find the best subset to approximate your signal, and that's not an easy problem engine."
        ],
        [
            "Graphical model selection.",
            "Here you want to imagine that you were given some samples.",
            "Let's say have a.",
            "Sorry this should be M. Here.",
            "Samples of an M dimensional random vector and you want to imagine that you would like to fit a Markov random field to the data.",
            "But if you don't know the graph in advance, then it's really a question of.",
            "How do you figure out which edges should be included in the graph you want to search over the space of all graphs, but there's a huge number of possible graphs.",
            "In particular there M choose two possible edges that you could include an act or exclude, so there's sort of an exponential number of possible subgraphs and what you'd like to do is, for instance use the data to try and figure out what is the appropriate graph, the appropriate subset of edges that you should include.",
            "So there are classical techniques that suggest in principle how you could do this.",
            "If you look at model selection criteria like the AIC and the BI, See the.",
            "These are two kinds of information criteria.",
            "What they suggest is that you should look at likelihood, but penalize it with some kind of L0 penalty.",
            "So again, in principle, these methods have certain guarantees associated with them, but because of the L0 penalty, they're not easy to actually compute efficiently in practice.",
            "So in all of these cases, essentially what you have is you've got some kind of sparse problem.",
            "There's some kind of L0 constraint that's arising and causing you problems, and what you'd like to do is find computationally efficient ways to relax those constraints."
        ],
        [
            "So the natural thing is something that's been studied a fair bit in both signal processing and statistics over the past decade or so.",
            "Is to think about instead of looking at L0 constraints, why don't we just relax to L1 constraints?",
            "The motivation being here that L1 is sort of the closest LP norm that's convex, but it's going to induce more sparsity.",
            "For instance, in an L2 norm.",
            "So at a high level, there's sort of been working on two classes of problems here.",
            "The first class is sort of a special case of that regression model.",
            "People assume that you're actually given perfect observations, so beta star here is an unknown vector that you're assuming to be sparse, and we're assuming you have some set of linear observations that look like this.",
            "And the problem you'd like to solve is this problem L 0 here you sort of like to search over vectors that satisfy these constraints, But you want to search over ones that have small L zero norm that have a small number of active coefficients, so that would correspond to getting the sparsest possible reconstruction of the signal.",
            "So what people have studied is this the natural L1 relaxation?",
            "It's the same problem this change this 021 and what's nice about this problem is that it's certainly convex.",
            "And in fact, it's actually a linear program.",
            "If you stare at it a bit and play some games, you can see that it's simply a linear program.",
            "So this was pioneered by Chen, Donoho and Sanders.",
            "It's a method called basis pursuit in signal processing.",
            "More in my opinion, more relevant cases where you actually have noise, so again, just focusing on the simple case of linear regression.",
            "Just adding a Gaussian noise term here.",
            "Here the natural problem to solve would be looking at a sum of squared differences corresponding to the observed data minus signal generation, and then again you penalize with an L0 norm and so the natural relaxation.",
            "This is something also referred to as the lasun.",
            "Statistics would be just to use the L1 norm here, so it's just a quadratic program with L1 norm constraints."
        ],
        [
            "OK, so there's a lot of work on these problems, so I'm not going to in a short talk, be able to give a complete overview, but just to sort of set the stage of it.",
            "As I mentioned, this basis pursuit that linear programming method was pioneered by Chen, Donahoe, and Sanders, and in the past five to 10 years there's been a lot of work on figuring out when does it actually work.",
            "So by that I mean this is a problem that you'd like to solve but can't solve.",
            "This is a problem that you can solve, but it's a relaxation.",
            "When does solving this problem give you the same answer as solving this problem?",
            "And there's been a lot of nice work by by various people.",
            "Just a partial list here, and they can now give pretty tight conditions on when it actually works.",
            "It doesn't always work, but it works for a reasonably broad class of problems, and so it is quite a useful method in practice.",
            "What I'll be focusing on more in this talk is the noisy case.",
            "The second kind of relaxation where you move from a QP with L0 norm.",
            "2A QP with L1.",
            "And this again has been studied a fair bit.",
            "I guess one of the earlier people to suggest it was tipsy.",
            "Ronnie, there's some nice work on the asymptotic's of this actually of LP regularization more generally by night in Foo.",
            "This is actually been used to for Gaussian graphical model selection.",
            "Some recent work by my house in Film and Joe and you.",
            "And I'll mention some links to that a bit later.",
            "We'll also see that the work I'll discuss has links to to some interesting recent work by both Donahoe and as well as Candace and Tao, in which they do randomized analysis of Gaussian ensembles, and so they don't characterize the behavior for a particular problem, but they give with high probability results over random classes of problems, so the results that all give some of them will be of that flavor too."
        ],
        [
            "OK, so this would be a bit more precise, So what we're thinking is we've got a fixed but unknown vector.",
            "Some beta star in our the P and we're going to be interested in its support, so it's support is just the subset of elements for which the vectors non 0 so that little SB the size of that support.",
            "So I'm going to refer to P. Here is the ambient dimension of the problem.",
            "It's sort of where the problem naturally sits, but as here is the sparsity index, we're going to see the taxes in some sense the more natural dimension of the problem.",
            "And typically, we're going to be interested in cases where P is much larger than S. That means just that you're looking at a pretty sparse problem.",
            "So P might be a small fraction of SP could even be sorry, as could be a small fraction of P, as might even be a vanishingly small fraction of P. And then we're going to be given a set of N observations of this form.",
            "So we're just looking at a standard linear regression, and the question will first tackle is for what sequences we're going to look at sequences of number of observations N. That's the number of these guys.",
            "You get the ambient dimension P and the sparsity S. And if you know time writing P as a function of N&S as a function of N. So we're going to look at sort of the interesting.",
            "The non classical setting in which both the size of your problem and the amount of data grows.",
            "So in classical asymptotically you sort of think about P&S is being fixed and just end the number of observation grows, but in many modern settings where you're getting lots of data an your samples might be might be very hard to come by with.",
            "Samples you're getting very high dimensional data that you have few samples.",
            "It's more natural to think of both and the number of observations and the size of the problem is actually growing in some way.",
            "So we'll be interested in how how does this triplet have to scale?",
            "How essentially, how quickly does N have to grow so that our method can actually recover this support?"
        ],
        [
            "In a reliable way.",
            "So what I'm going to look at is I'm going to look at this problem for a random set of problems.",
            "And what will establish is it was a result that holds with high probability over this set of random problems.",
            "So we're going to think about each one of these vectors.",
            "These are the vectors X one X2.",
            "This is a set of observations we think of.",
            "Each one of these is being drawn as some random Gaussian vector with a covariance matrix Sigma.",
            "And we're going to have to impose some conditions on on this matrix.",
            "These conditions actually have quite natural geometric interpretations.",
            "One thing you need is you need some kind of control on how dependent these vectors become.",
            "So this first condition says that you need control on the eigenvalues.",
            "They can't get too close to 0, or they can't get too large and the second condition is quite interesting.",
            "Essentially what it says is that this matrix here.",
            "This matrix is essentially a conditional covariance matrix, so as here are that's the subset of betas that are non zero and S complement.",
            "That's the subset that are zero and so this condition what it's telling you is that you can't have too much dependence.",
            "You can't have the irrelevant variables exerting too much of an influence on the relevant variables and so having a little Delta that bounds this away from one that turns out to be a key condition here."
        ],
        [
            "Just to give you some examples of things that satisfy this, what's been analyzed a lot in previous work is sort of a quite special case that when you just look at the uniform, you just choose the Rose uniformly from an identity matrix.",
            "You can also look at things like toplitz ensembles like this looking at certain kinds of bounded correlation models.",
            "More generally, any kind of diagonally dominant matrix.",
            "All of these classes will satisfy these conditions.",
            "Not all matrices will, but it's still a reasonably broad class.",
            "And what you want to be clear about is that these are conditions on the sample covariance.",
            "These are conditions on this, but the actual matrix that we sample.",
            "This is a random quantity.",
            "It can look quite different.",
            "It can have very different structure."
        ],
        [
            "So for instance, just as a simple illustration, if you look at just an identity matrix.",
            "This doesn't maybe show up so well, but a random sample, a random wish heart matrix has a strong diagonal, but maybe you can't see it's got actually a lot of non zeros.",
            "It's got a lot of structure off the diagonal and if you look at the eigen spectrum you can see the random matrix has a very different eigen spectrum than the.",
            "Then the identity matrix and the reason this is happening is of course, because we're letting both the size of the matrix and the number of observations grow, so it's because we're looking at this non classical limit where both the problem size and the number of observation grows."
        ],
        [
            "OK, so the main result here is the following.",
            "So, just to recap, we're thinking about this observation model.",
            "You have N such observations.",
            "Beta star is some fixed but unknown vector that lies in some dimensional space P, and we're assuming that it has a support.",
            "It's sparse with a supportive size S. And what this says is this.",
            "This gives a sort of threshold result that tells you when will assume will work and when it will fail.",
            "And basically it says that N has to scale essentially like this in terms of the sparsity index and in terms of the ambient dimension.",
            "So this is this is a more precise statement of the threshold result.",
            "So what you want to see here is that it is actually surprising in some ways.",
            "'cause it says you need relatively little data because of this log term here P. So the overall size of your vector can actually be extremely large.",
            "It could be exponentially large in N and as long as the sparsity was low enough, this would still succeed.",
            "So that's what that condition is saying, and it's also interesting that it's in some sense if and only if you don't have that much data, then there's no way you'll succeed."
        ],
        [
            "So just looking at some simulations just to confirm that theory, if you sort of plot the control parameter so the threshold in this case is predicted to happen at one and all I'm doing here is ramping up the amount of data.",
            "So looking at a number of problems, and this is with less data, the amount of data increases and what I'm plotting is the probability that the technique works, that it recovers successfully.",
            "You can see that it's going to fail, fail, fail, and then at a certain point it jumps up.",
            "And then here, it's essentially succeeding with probability one, and each of these is for a different kind of sparsity.",
            "This is for where the star sees a linear fraction.",
            "This is where the sparsity is something like a square root fraction."
        ],
        [
            "So just a couple of corral reasons that result.",
            "One is that suppose that you only you have a sort of linearly undetermined problem.",
            "This is something that people have looked at a lot in the noiseless setting, because it's it's sort of the interesting regime when you don't have noise an what they showed.",
            "For instance, Donahoe Candace Tao is they showed that if you don't have noise, then if you just have this much data, it's not actually much data.",
            "You can still recover all sparsity patterns up to some constant fractional linear fraction of the total length can be recovered.",
            "What the take home message here is that if you have noise, then you actually lose a fair bit.",
            "Instead of getting a linear fraction, you only get P over log of P, so this is sort of decaying to 0 as a fraction of P. So saying that you lose a fair bit when there's noise in your data at some level, that's not surprising.",
            "Another corollary, this is a result that was established previously by these people is that if for instance P can scale as quickly as some exponential of the number of observations, so it's saying your problem size can grow exponentially in the amount of data, and as long as the sparsity is relatively small then you will always recover with probability one.",
            "So this result here just these qualities followed by sort of specializing this kind of scaling to those two particular cases."
        ],
        [
            "OK, so in the last few minutes let me just talk about some extensions to graphical model selection.",
            "Up until now I was talking about.",
            "Simply about linear regression.",
            "But the basic idea is actually carry over in a pretty natural way to thinking, thinking about graphical model selection.",
            "And just to be clear, these two authors in fact did consider the case of Gaussian graphical model selection.",
            "But we're going to consider the case of more general graphical models and just to keep things concrete, it's not.",
            "It's convenient to look at a case where you just have, for instance, a pairwise Markov random field.",
            "And let's assume that our random variables are actually 01 valued.",
            "So the basic observation here is that if you have a distribution like this.",
            "If you fix one variable, say you fix Z1.",
            "And you look at what's its conditional distribution conditioned on the rest of the variables Z2 through ZM.",
            "Then in fact, you can show that in this case it has this kind of logistic distribution.",
            "That's something we saw before this morning.",
            "So what this suggests what's key here is that in this logistic, so N of 1 here that denotes the neighborhood of node one.",
            "That's what we're trying to recover.",
            "That's the neighborhood structure of the graph, and so what's key is that this sum is only over the neighborhood structure means that this beta is actually zero everywhere else.",
            "So in fact, the support of the vector beta tells you what the neighborhood structure of the graph.",
            "If you do this local kind of regression.",
            "So that sort of puts it in the framework we've been talking about.",
            "You can start trying to recover the support of exactly this vector, and if you can do that, then you can recover the neighborhood structure.",
            "So our strategy is a natural one.",
            "Given this kind of conditional distribution.",
            "What it suggests you should do is perform logistic regression.",
            "And so instead of ordinary linear regression, will do logistic regression, but will penalize the logistic regression again with an L1 norm."
        ],
        [
            "It's a bit more precisely we're going to be given some samples Z1 through ZM.",
            "That's a vector K is indexing the sample number.",
            "And for each node, what we're going to do is we're going to take the regression of ZI the variable at node I on the remaining set of variables.",
            "This is just notation for the remaining set.",
            "So what we're doing is solving this optimization problem.",
            "This is the logistic regression term, and here is the L1 penalty and right there.",
            "Lambda N is a regularization term that will be chosen.",
            "So once we've done this, we are going to get some vector.",
            "It's going to have some zero somewhere in some non zeros and will simply estimate the neighborhood of the node is where the nonzeros are.",
            "And we're actually going to do this simultaneously over all nodes of the graph at once.",
            "What turns out to be key here is actually to look at the Fisher information matrix, just like in the linear regression case we saw that it was the structure of the covariance matrix there that controlled how it behaved.",
            "Here it turns out to be the Fisher Information matrix, which is the analogous quantity, so this don't worry about the details.",
            "But this is just the Fisher information matrix of this particular regression model at node I, and we're going to let Q. SS denote the submatrix that's corresponds to the support of that node, so that's the submatrix that corresponds to the non zero entries.",
            "The one that correspond."
        ],
        [
            "The edges.",
            "So the assumptions we need are essentially the same, essentially analogous.",
            "The first to the ones we had before.",
            "You need a control on the eigenvalues of this Fisher information.",
            "This is the same kind of incoherence condition that we required before, and.",
            "In this case, we again need a kind of we need end to grow roughly logarithmically here, logarithmically in the ambient dimension of the problem, and it also depends on, for instance, the maximum degree of the graph.",
            "So this condition is a bit weaker than what we had before, but this is a sufficient can."
        ],
        [
            "And So what you can prove in this case is that if you follow this procedure, you're given samples for every node you're going to perform the logistic regression of that node on its neighbors.",
            "You're going to use the support of the vector that you get.",
            "You're getting some vector out of that optimization problem.",
            "Use that support to estimate the neighborhood structure of that node, and in the end you're going to have one estimate for every node in the graph.",
            "So you need to combine these in some way.",
            "It doesn't really matter how you do it.",
            "You can either take ends between them or ores.",
            "And so if you follow this procedure again, this is this is computationally efficient since it just involves solving a sequence of logistic regressions that can be solved very quickly again by dual methods.",
            "And what we're guaranteed is that if our triple, if the previous assumptions are satisfied and we choose our regularization parameter in such a way that these two conditions hold, then what you're guaranteed is that with probability one.",
            "You choose the correct neighborhood at every node of the graph as the amount of data goes to Infinity.",
            "So what this is saying is that this is a computationally efficient method for doing graphical model selection.",
            "Actually, in the non classical setting, both the size of the graph, possibly the sizes of the neighborhoods and the amount of data, all three things can grow at once, so this sort of allows you to deal with pretty general set of problems.",
            "We do of course need these assumptions to be clear, so these assumptions will not hold for any graphical model, but it's certainly a reasonable subset of graphical models for which this holds."
        ],
        [
            "OK, so just to wrap up.",
            "So what I discussed sort of two kinds of problems involving L1 regularization.",
            "First, looking at the simpler case of linear regression, we gave sharp thresholds for when it when it works when you can recover the sparsity.",
            "In the case of the identity matrix, results are actually are sharp.",
            "They can't be improved because both the theory and practice says that there's exactly a threshold.",
            "At this point, one and you can see that's happening.",
            "In the more general case, the theory, it should be possible to improve it right now.",
            "Our theory says that the threshold is certainly above this line an it's below this line, but you can see there's quite a big gap between those two lines.",
            "It should be possible to figure out analytically more precisely that it's somewhere around there, but that sort of requires, I think, some finer analysis.",
            "In the case of logistic regression, we've given so far sufficient conditions for it to work.",
            "But again, it's an interesting set of sufficient conditions.",
            "You're now guaranteed to have a consistent procedure for model selection, even in the high dimensional setting.",
            "Just a couple of remarks and open questions I talked about logistic regression and basically binary variables, But basically the same story can be carried over to more general graphical models.",
            "You're just going to get a more general kind of regression for Gaussians.",
            "It's linear regression.",
            "For binary.",
            "It's logistic regression.",
            "For other models, it's going to be another kind of generalized linear model and a different kind of regression.",
            "What I think are interesting is our results depend pretty critically on these so called mutual incoherence conditions.",
            "Just to recap, these are conditions of this form which basically say there's the irrelevant variables in this subset.",
            "Don't have too much of an effect on the relevant ones, so that's probably the strongest constraint that we have, and it's interesting to think cannot be weakened or eliminated.",
            "Or is that really essential?",
            "And the last thing that I think is interesting, and I believe some other people are working on, is actually to figure out forgetting about computation.",
            "Just figuring out what are the fundamental limits.",
            "When can you ever expect to recover sparsity?",
            "What's the minimum amount of data that any method, even an Oracle, that could search the entire space could possibly recover?",
            "So those are sort of interesting questions to figure out how close these approximate methods are coming to what the fundamental limits are.",
            "OK thanks.",
            "Yep.",
            "Senator results on the rating system of graphical models.",
            "At this point we do not.",
            "Rather, Pradeep does not.",
            "He's probably in the middle of doing it as we speak, but.",
            "Will be interested to see that, but I don't expect there to be any issues that the theory seems solid and the agreement in the linear case is pretty clean, so.",
            "Sorry.",
            "OK yeah, how about using?",
            "Playing through step one, is there any?",
            "Conversational when Q is less than one or larger than one.",
            "You'll be nice to use Q less than one, but LQ norm for Q less than one or non convex so there would be pretty serious computational issues.",
            "In fact, night and Fu the paper I mentioned, they have very nice results saying that in theory if you could do LQ for Q less than one, you get very good sparsity selection.",
            "It's it's more powerful than L1, but it's nonconvex, so that's sort of why we're stopping at L1.",
            "Yep.",
            "Can you read about methods for doing the other one?",
            "I wonder if your eyes.",
            "For actually solving L1 problems.",
            "So I guess in the linear case there's been a fair bit of work group of statisticians at Stanford have an algorithm called Lars that sort of sequential algorithm will solve L1 constraint QPS.",
            "In practice, I found that you can solve a lot of the L1 constraint QPS simply by dualizing them.",
            "The reason that's helpful is that L1 constraints when you dualize become L, Infinity or box constraints, so you can just use a gradient projection method.",
            "So actually the experiments I did, that's all I did.",
            "It was very fast and easy to program.",
            "In terms of the logistic regression, probably one of the best ways to solve it is again dualize.",
            "If you dualize this as you might expect from this morning, you get an entropy, so the dual of this is actually a maximum entropy with an L Infinity constraint, so you can do allies and plug it into something like what's that convex program solver named.",
            "Nothing you see Plex, but that's an LP solver.",
            "Slipping my mind, but I'll get back to you on it, yeah so.",
            "Pardon me.",
            "El peasel but that's for LP's.",
            "I want something for more general convex.",
            "I'm thinking not have said you me, that's for SDP's.",
            "Which, OK?",
            "Anyway, it suffice to say it's a nice convex problem, and it's sort of in a Canonical family, so there are a lot of techniques to solve it.",
            "I still think probably you can make you know in general whatever structure you should have, you should exploit to try and get faster methods, and I think for solving very large scale problems there's still probably would be a bit of work to figure out how to solve this in the most efficient way possible.",
            "No more question.",
            "Appreciation of the contribution of Doctor Martin Wainwright regarding pre things keeps improving.",
            "Do I get a Chinese name?",
            "Yes, OK. Can you read?",
            "This is Marty.",
            "Thank you.",
            "Thanks very much for organizing, much appreciated."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will come back.",
                    "label": 0
                },
                {
                    "sent": "So we're glad to have Doctor Martin Wainwright to give us the research talk, and so let's welcome.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'd like to talk a bit about the use of L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "And in particular, how it's useful for sparsity recovery and also for the problem with model selection.",
                    "label": 1
                },
                {
                    "sent": "So the work on model selection is based on joint work with John Lafferty and probably Praveen Kumar at CMU.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You OK, so at a high level, the problem of sparsity recovery is is how do you recover as a signal.",
                    "label": 0
                },
                {
                    "sent": "Let's think about it.",
                    "label": 0
                },
                {
                    "sent": "This is some vector beta star that's suitably sparse.",
                    "label": 1
                },
                {
                    "sent": "I'll be precise about what suitably sparse means in a minute.",
                    "label": 1
                },
                {
                    "sent": "On the basis of a bunch of noisy observations.",
                    "label": 1
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly simple problem, but it's one that actually has a pretty broad range of applications.",
                    "label": 1
                },
                {
                    "sent": "Perhaps the most immediately obvious one is subset selection in regression, and I'll talk a bit about that on the next slide.",
                    "label": 0
                },
                {
                    "sent": "It's also been studied a lot in the context of signal denoising and sparse constructive approximation, and you can also think about the problem of graphical model selection.",
                    "label": 1
                },
                {
                    "sent": "Actually is a problem of recovering the sparse signal.",
                    "label": 0
                },
                {
                    "sent": "So the thing with sparsity, the natural way to think about it in terms of an optimization problem is in terms of what you might call an L0 norm norm here is in.",
                    "label": 0
                },
                {
                    "sent": "Quotation marks because it's not really a norm by this norm, we mean just the number.",
                    "label": 0
                },
                {
                    "sent": "Just something that counts the number of elements in a vector that are non 0.",
                    "label": 0
                },
                {
                    "sent": "So you're just looking at the size of the support of the vector.",
                    "label": 0
                },
                {
                    "sent": "Typically you can formulate problems like this in terms of constraints that involve L zero constraints.",
                    "label": 0
                },
                {
                    "sent": "But those L 0 problems are well known to be NP hard in general, so that's what motivates looking for methods that are actually computationally tractable, but hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work as well.",
                    "label": 0
                },
                {
                    "sent": "So let's just start with subset selection in regression.",
                    "label": 1
                },
                {
                    "sent": "This is a very classical problem in statistics.",
                    "label": 0
                },
                {
                    "sent": "Just to keep things simple, let's let's just look at the standard linear regression model you've got observed data.",
                    "label": 1
                },
                {
                    "sent": "XK and.",
                    "label": 0
                },
                {
                    "sent": "Interesting, I see what you mean now.",
                    "label": 0
                },
                {
                    "sent": "It's got features just like Microsoft.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, anyway we've got observed data pairs XK.",
                    "label": 0
                },
                {
                    "sent": "These are vectors and YC.",
                    "label": 0
                },
                {
                    "sent": "In this case is just a scalar and what you're assuming is that they are generated by the standard linear regression.",
                    "label": 0
                },
                {
                    "sent": "So you're just taking the inner product between the data and some fixed vector beta star.",
                    "label": 0
                },
                {
                    "sent": "This is the sparse thing that we're going to be after and things were contaminated by some Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "What's interesting is there lots of applications where you can think that X includes a large number of variables that are actually completely irrelevant, so there's sort of nuisance variables that distract you and what you'd like to do is thank you.",
                    "label": 0
                },
                {
                    "sent": "Hopefully the third time was lucky.",
                    "label": 0
                },
                {
                    "sent": "Good, that's great, So what you'd like to do is sort of recover this subset of industries and beta star that correspond to elements of exit are actually relevant.",
                    "label": 0
                },
                {
                    "sent": "And at the same time you'd like to suppress or ignore.",
                    "label": 0
                },
                {
                    "sent": "You'd like to get rid of all the irrelevant variables.",
                    "label": 0
                },
                {
                    "sent": "OK. Moving up in the world.",
                    "label": 0
                },
                {
                    "sent": "Right, so you want to think about taxes being very long.",
                    "label": 0
                },
                {
                    "sent": "X might have you know 100,000 components, but there might be a very small subset, maybe 100 or 10 that are relevant.",
                    "label": 0
                },
                {
                    "sent": "So problems like this arise a lot in bioinformatics.",
                    "label": 1
                },
                {
                    "sent": "For instance, with gene problems may also arise very naturally in sparse representations in signal processing.",
                    "label": 0
                },
                {
                    "sent": "Actually historically has been a fair bit of work on the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you an illustration.",
                    "label": 0
                },
                {
                    "sent": "A lot of what people sort of area is pretty active in signal processing.",
                    "label": 0
                },
                {
                    "sent": "Is looking at how do you reconstruct signals optimally when you have overcomplete bases.",
                    "label": 0
                },
                {
                    "sent": "So if you remember a basis is just a linearly independent and complete subset of a vector space.",
                    "label": 0
                },
                {
                    "sent": "So for instance the Fourier basis is 1 basis here.",
                    "label": 0
                },
                {
                    "sent": "Also the basis of spikes that this samples this.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at a signal of length 256 that would be another basis of this signal.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting is that you can have signals like this one here.",
                    "label": 0
                },
                {
                    "sent": "Is very simple to set toys such a sinusoid here plus a few spikes at some locations so you can see that it's not sparse in either one of the two bases alone.",
                    "label": 0
                },
                {
                    "sent": "If you just trying to reconstruct it using spikes alone, you have to use all 256 pairs.",
                    "label": 0
                },
                {
                    "sent": "And if you try and reconstruct it in a Fourier basis, the fact that you have these little spikes that gives you high frequency or it gives you broadband frequency information, so it's not at all sparse in the Fourier basis.",
                    "label": 0
                },
                {
                    "sent": "But the thing here is that it's very obviously it's sparse.",
                    "label": 0
                },
                {
                    "sent": "If I was allowed to use a combination, this is called dictionary selection.",
                    "label": 0
                },
                {
                    "sent": "If I was allowed to choose a subset of Fourier coefficients and a subset of spiced coefficients, then I could in fact reconstruct this signal exactly very sparsely.",
                    "label": 0
                },
                {
                    "sent": "I could just use one Fourier coefficient and a very small subset of spikes.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a problem with sparse approximation and here it's sort of obvious.",
                    "label": 0
                },
                {
                    "sent": "You can just look at it, but if you think about this for real signals, it's quite a challenging problem.",
                    "label": 0
                },
                {
                    "sent": "You have to search over all subsets of your dictionary to try and find the best subset to approximate your signal, and that's not an easy problem engine.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphical model selection.",
                    "label": 0
                },
                {
                    "sent": "Here you want to imagine that you were given some samples.",
                    "label": 0
                },
                {
                    "sent": "Let's say have a.",
                    "label": 0
                },
                {
                    "sent": "Sorry this should be M. Here.",
                    "label": 0
                },
                {
                    "sent": "Samples of an M dimensional random vector and you want to imagine that you would like to fit a Markov random field to the data.",
                    "label": 0
                },
                {
                    "sent": "But if you don't know the graph in advance, then it's really a question of.",
                    "label": 0
                },
                {
                    "sent": "How do you figure out which edges should be included in the graph you want to search over the space of all graphs, but there's a huge number of possible graphs.",
                    "label": 0
                },
                {
                    "sent": "In particular there M choose two possible edges that you could include an act or exclude, so there's sort of an exponential number of possible subgraphs and what you'd like to do is, for instance use the data to try and figure out what is the appropriate graph, the appropriate subset of edges that you should include.",
                    "label": 0
                },
                {
                    "sent": "So there are classical techniques that suggest in principle how you could do this.",
                    "label": 0
                },
                {
                    "sent": "If you look at model selection criteria like the AIC and the BI, See the.",
                    "label": 0
                },
                {
                    "sent": "These are two kinds of information criteria.",
                    "label": 0
                },
                {
                    "sent": "What they suggest is that you should look at likelihood, but penalize it with some kind of L0 penalty.",
                    "label": 0
                },
                {
                    "sent": "So again, in principle, these methods have certain guarantees associated with them, but because of the L0 penalty, they're not easy to actually compute efficiently in practice.",
                    "label": 0
                },
                {
                    "sent": "So in all of these cases, essentially what you have is you've got some kind of sparse problem.",
                    "label": 0
                },
                {
                    "sent": "There's some kind of L0 constraint that's arising and causing you problems, and what you'd like to do is find computationally efficient ways to relax those constraints.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the natural thing is something that's been studied a fair bit in both signal processing and statistics over the past decade or so.",
                    "label": 0
                },
                {
                    "sent": "Is to think about instead of looking at L0 constraints, why don't we just relax to L1 constraints?",
                    "label": 0
                },
                {
                    "sent": "The motivation being here that L1 is sort of the closest LP norm that's convex, but it's going to induce more sparsity.",
                    "label": 0
                },
                {
                    "sent": "For instance, in an L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So at a high level, there's sort of been working on two classes of problems here.",
                    "label": 0
                },
                {
                    "sent": "The first class is sort of a special case of that regression model.",
                    "label": 0
                },
                {
                    "sent": "People assume that you're actually given perfect observations, so beta star here is an unknown vector that you're assuming to be sparse, and we're assuming you have some set of linear observations that look like this.",
                    "label": 0
                },
                {
                    "sent": "And the problem you'd like to solve is this problem L 0 here you sort of like to search over vectors that satisfy these constraints, But you want to search over ones that have small L zero norm that have a small number of active coefficients, so that would correspond to getting the sparsest possible reconstruction of the signal.",
                    "label": 0
                },
                {
                    "sent": "So what people have studied is this the natural L1 relaxation?",
                    "label": 0
                },
                {
                    "sent": "It's the same problem this change this 021 and what's nice about this problem is that it's certainly convex.",
                    "label": 0
                },
                {
                    "sent": "And in fact, it's actually a linear program.",
                    "label": 0
                },
                {
                    "sent": "If you stare at it a bit and play some games, you can see that it's simply a linear program.",
                    "label": 0
                },
                {
                    "sent": "So this was pioneered by Chen, Donoho and Sanders.",
                    "label": 0
                },
                {
                    "sent": "It's a method called basis pursuit in signal processing.",
                    "label": 0
                },
                {
                    "sent": "More in my opinion, more relevant cases where you actually have noise, so again, just focusing on the simple case of linear regression.",
                    "label": 0
                },
                {
                    "sent": "Just adding a Gaussian noise term here.",
                    "label": 0
                },
                {
                    "sent": "Here the natural problem to solve would be looking at a sum of squared differences corresponding to the observed data minus signal generation, and then again you penalize with an L0 norm and so the natural relaxation.",
                    "label": 0
                },
                {
                    "sent": "This is something also referred to as the lasun.",
                    "label": 0
                },
                {
                    "sent": "Statistics would be just to use the L1 norm here, so it's just a quadratic program with L1 norm constraints.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's a lot of work on these problems, so I'm not going to in a short talk, be able to give a complete overview, but just to sort of set the stage of it.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, this basis pursuit that linear programming method was pioneered by Chen, Donahoe, and Sanders, and in the past five to 10 years there's been a lot of work on figuring out when does it actually work.",
                    "label": 0
                },
                {
                    "sent": "So by that I mean this is a problem that you'd like to solve but can't solve.",
                    "label": 0
                },
                {
                    "sent": "This is a problem that you can solve, but it's a relaxation.",
                    "label": 0
                },
                {
                    "sent": "When does solving this problem give you the same answer as solving this problem?",
                    "label": 0
                },
                {
                    "sent": "And there's been a lot of nice work by by various people.",
                    "label": 0
                },
                {
                    "sent": "Just a partial list here, and they can now give pretty tight conditions on when it actually works.",
                    "label": 0
                },
                {
                    "sent": "It doesn't always work, but it works for a reasonably broad class of problems, and so it is quite a useful method in practice.",
                    "label": 0
                },
                {
                    "sent": "What I'll be focusing on more in this talk is the noisy case.",
                    "label": 0
                },
                {
                    "sent": "The second kind of relaxation where you move from a QP with L0 norm.",
                    "label": 0
                },
                {
                    "sent": "2A QP with L1.",
                    "label": 0
                },
                {
                    "sent": "And this again has been studied a fair bit.",
                    "label": 0
                },
                {
                    "sent": "I guess one of the earlier people to suggest it was tipsy.",
                    "label": 0
                },
                {
                    "sent": "Ronnie, there's some nice work on the asymptotic's of this actually of LP regularization more generally by night in Foo.",
                    "label": 0
                },
                {
                    "sent": "This is actually been used to for Gaussian graphical model selection.",
                    "label": 1
                },
                {
                    "sent": "Some recent work by my house in Film and Joe and you.",
                    "label": 0
                },
                {
                    "sent": "And I'll mention some links to that a bit later.",
                    "label": 0
                },
                {
                    "sent": "We'll also see that the work I'll discuss has links to to some interesting recent work by both Donahoe and as well as Candace and Tao, in which they do randomized analysis of Gaussian ensembles, and so they don't characterize the behavior for a particular problem, but they give with high probability results over random classes of problems, so the results that all give some of them will be of that flavor too.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this would be a bit more precise, So what we're thinking is we've got a fixed but unknown vector.",
                    "label": 1
                },
                {
                    "sent": "Some beta star in our the P and we're going to be interested in its support, so it's support is just the subset of elements for which the vectors non 0 so that little SB the size of that support.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to refer to P. Here is the ambient dimension of the problem.",
                    "label": 1
                },
                {
                    "sent": "It's sort of where the problem naturally sits, but as here is the sparsity index, we're going to see the taxes in some sense the more natural dimension of the problem.",
                    "label": 0
                },
                {
                    "sent": "And typically, we're going to be interested in cases where P is much larger than S. That means just that you're looking at a pretty sparse problem.",
                    "label": 0
                },
                {
                    "sent": "So P might be a small fraction of SP could even be sorry, as could be a small fraction of P, as might even be a vanishingly small fraction of P. And then we're going to be given a set of N observations of this form.",
                    "label": 0
                },
                {
                    "sent": "So we're just looking at a standard linear regression, and the question will first tackle is for what sequences we're going to look at sequences of number of observations N. That's the number of these guys.",
                    "label": 0
                },
                {
                    "sent": "You get the ambient dimension P and the sparsity S. And if you know time writing P as a function of N&S as a function of N. So we're going to look at sort of the interesting.",
                    "label": 0
                },
                {
                    "sent": "The non classical setting in which both the size of your problem and the amount of data grows.",
                    "label": 0
                },
                {
                    "sent": "So in classical asymptotically you sort of think about P&S is being fixed and just end the number of observation grows, but in many modern settings where you're getting lots of data an your samples might be might be very hard to come by with.",
                    "label": 0
                },
                {
                    "sent": "Samples you're getting very high dimensional data that you have few samples.",
                    "label": 0
                },
                {
                    "sent": "It's more natural to think of both and the number of observations and the size of the problem is actually growing in some way.",
                    "label": 0
                },
                {
                    "sent": "So we'll be interested in how how does this triplet have to scale?",
                    "label": 0
                },
                {
                    "sent": "How essentially, how quickly does N have to grow so that our method can actually recover this support?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a reliable way.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to look at is I'm going to look at this problem for a random set of problems.",
                    "label": 0
                },
                {
                    "sent": "And what will establish is it was a result that holds with high probability over this set of random problems.",
                    "label": 0
                },
                {
                    "sent": "So we're going to think about each one of these vectors.",
                    "label": 0
                },
                {
                    "sent": "These are the vectors X one X2.",
                    "label": 0
                },
                {
                    "sent": "This is a set of observations we think of.",
                    "label": 0
                },
                {
                    "sent": "Each one of these is being drawn as some random Gaussian vector with a covariance matrix Sigma.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have to impose some conditions on on this matrix.",
                    "label": 0
                },
                {
                    "sent": "These conditions actually have quite natural geometric interpretations.",
                    "label": 0
                },
                {
                    "sent": "One thing you need is you need some kind of control on how dependent these vectors become.",
                    "label": 0
                },
                {
                    "sent": "So this first condition says that you need control on the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "They can't get too close to 0, or they can't get too large and the second condition is quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Essentially what it says is that this matrix here.",
                    "label": 0
                },
                {
                    "sent": "This matrix is essentially a conditional covariance matrix, so as here are that's the subset of betas that are non zero and S complement.",
                    "label": 0
                },
                {
                    "sent": "That's the subset that are zero and so this condition what it's telling you is that you can't have too much dependence.",
                    "label": 0
                },
                {
                    "sent": "You can't have the irrelevant variables exerting too much of an influence on the relevant variables and so having a little Delta that bounds this away from one that turns out to be a key condition here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to give you some examples of things that satisfy this, what's been analyzed a lot in previous work is sort of a quite special case that when you just look at the uniform, you just choose the Rose uniformly from an identity matrix.",
                    "label": 0
                },
                {
                    "sent": "You can also look at things like toplitz ensembles like this looking at certain kinds of bounded correlation models.",
                    "label": 1
                },
                {
                    "sent": "More generally, any kind of diagonally dominant matrix.",
                    "label": 0
                },
                {
                    "sent": "All of these classes will satisfy these conditions.",
                    "label": 0
                },
                {
                    "sent": "Not all matrices will, but it's still a reasonably broad class.",
                    "label": 0
                },
                {
                    "sent": "And what you want to be clear about is that these are conditions on the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "These are conditions on this, but the actual matrix that we sample.",
                    "label": 0
                },
                {
                    "sent": "This is a random quantity.",
                    "label": 0
                },
                {
                    "sent": "It can look quite different.",
                    "label": 1
                },
                {
                    "sent": "It can have very different structure.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance, just as a simple illustration, if you look at just an identity matrix.",
                    "label": 0
                },
                {
                    "sent": "This doesn't maybe show up so well, but a random sample, a random wish heart matrix has a strong diagonal, but maybe you can't see it's got actually a lot of non zeros.",
                    "label": 0
                },
                {
                    "sent": "It's got a lot of structure off the diagonal and if you look at the eigen spectrum you can see the random matrix has a very different eigen spectrum than the.",
                    "label": 0
                },
                {
                    "sent": "Then the identity matrix and the reason this is happening is of course, because we're letting both the size of the matrix and the number of observations grow, so it's because we're looking at this non classical limit where both the problem size and the number of observation grows.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the main result here is the following.",
                    "label": 0
                },
                {
                    "sent": "So, just to recap, we're thinking about this observation model.",
                    "label": 0
                },
                {
                    "sent": "You have N such observations.",
                    "label": 0
                },
                {
                    "sent": "Beta star is some fixed but unknown vector that lies in some dimensional space P, and we're assuming that it has a support.",
                    "label": 0
                },
                {
                    "sent": "It's sparse with a supportive size S. And what this says is this.",
                    "label": 0
                },
                {
                    "sent": "This gives a sort of threshold result that tells you when will assume will work and when it will fail.",
                    "label": 0
                },
                {
                    "sent": "And basically it says that N has to scale essentially like this in terms of the sparsity index and in terms of the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a more precise statement of the threshold result.",
                    "label": 0
                },
                {
                    "sent": "So what you want to see here is that it is actually surprising in some ways.",
                    "label": 0
                },
                {
                    "sent": "'cause it says you need relatively little data because of this log term here P. So the overall size of your vector can actually be extremely large.",
                    "label": 0
                },
                {
                    "sent": "It could be exponentially large in N and as long as the sparsity was low enough, this would still succeed.",
                    "label": 0
                },
                {
                    "sent": "So that's what that condition is saying, and it's also interesting that it's in some sense if and only if you don't have that much data, then there's no way you'll succeed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just looking at some simulations just to confirm that theory, if you sort of plot the control parameter so the threshold in this case is predicted to happen at one and all I'm doing here is ramping up the amount of data.",
                    "label": 0
                },
                {
                    "sent": "So looking at a number of problems, and this is with less data, the amount of data increases and what I'm plotting is the probability that the technique works, that it recovers successfully.",
                    "label": 0
                },
                {
                    "sent": "You can see that it's going to fail, fail, fail, and then at a certain point it jumps up.",
                    "label": 0
                },
                {
                    "sent": "And then here, it's essentially succeeding with probability one, and each of these is for a different kind of sparsity.",
                    "label": 0
                },
                {
                    "sent": "This is for where the star sees a linear fraction.",
                    "label": 0
                },
                {
                    "sent": "This is where the sparsity is something like a square root fraction.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a couple of corral reasons that result.",
                    "label": 0
                },
                {
                    "sent": "One is that suppose that you only you have a sort of linearly undetermined problem.",
                    "label": 1
                },
                {
                    "sent": "This is something that people have looked at a lot in the noiseless setting, because it's it's sort of the interesting regime when you don't have noise an what they showed.",
                    "label": 1
                },
                {
                    "sent": "For instance, Donahoe Candace Tao is they showed that if you don't have noise, then if you just have this much data, it's not actually much data.",
                    "label": 1
                },
                {
                    "sent": "You can still recover all sparsity patterns up to some constant fractional linear fraction of the total length can be recovered.",
                    "label": 0
                },
                {
                    "sent": "What the take home message here is that if you have noise, then you actually lose a fair bit.",
                    "label": 0
                },
                {
                    "sent": "Instead of getting a linear fraction, you only get P over log of P, so this is sort of decaying to 0 as a fraction of P. So saying that you lose a fair bit when there's noise in your data at some level, that's not surprising.",
                    "label": 0
                },
                {
                    "sent": "Another corollary, this is a result that was established previously by these people is that if for instance P can scale as quickly as some exponential of the number of observations, so it's saying your problem size can grow exponentially in the amount of data, and as long as the sparsity is relatively small then you will always recover with probability one.",
                    "label": 0
                },
                {
                    "sent": "So this result here just these qualities followed by sort of specializing this kind of scaling to those two particular cases.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in the last few minutes let me just talk about some extensions to graphical model selection.",
                    "label": 0
                },
                {
                    "sent": "Up until now I was talking about.",
                    "label": 0
                },
                {
                    "sent": "Simply about linear regression.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is actually carry over in a pretty natural way to thinking, thinking about graphical model selection.",
                    "label": 0
                },
                {
                    "sent": "And just to be clear, these two authors in fact did consider the case of Gaussian graphical model selection.",
                    "label": 1
                },
                {
                    "sent": "But we're going to consider the case of more general graphical models and just to keep things concrete, it's not.",
                    "label": 1
                },
                {
                    "sent": "It's convenient to look at a case where you just have, for instance, a pairwise Markov random field.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that our random variables are actually 01 valued.",
                    "label": 0
                },
                {
                    "sent": "So the basic observation here is that if you have a distribution like this.",
                    "label": 0
                },
                {
                    "sent": "If you fix one variable, say you fix Z1.",
                    "label": 0
                },
                {
                    "sent": "And you look at what's its conditional distribution conditioned on the rest of the variables Z2 through ZM.",
                    "label": 1
                },
                {
                    "sent": "Then in fact, you can show that in this case it has this kind of logistic distribution.",
                    "label": 0
                },
                {
                    "sent": "That's something we saw before this morning.",
                    "label": 0
                },
                {
                    "sent": "So what this suggests what's key here is that in this logistic, so N of 1 here that denotes the neighborhood of node one.",
                    "label": 0
                },
                {
                    "sent": "That's what we're trying to recover.",
                    "label": 0
                },
                {
                    "sent": "That's the neighborhood structure of the graph, and so what's key is that this sum is only over the neighborhood structure means that this beta is actually zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So in fact, the support of the vector beta tells you what the neighborhood structure of the graph.",
                    "label": 0
                },
                {
                    "sent": "If you do this local kind of regression.",
                    "label": 0
                },
                {
                    "sent": "So that sort of puts it in the framework we've been talking about.",
                    "label": 0
                },
                {
                    "sent": "You can start trying to recover the support of exactly this vector, and if you can do that, then you can recover the neighborhood structure.",
                    "label": 0
                },
                {
                    "sent": "So our strategy is a natural one.",
                    "label": 1
                },
                {
                    "sent": "Given this kind of conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "What it suggests you should do is perform logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And so instead of ordinary linear regression, will do logistic regression, but will penalize the logistic regression again with an L1 norm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a bit more precisely we're going to be given some samples Z1 through ZM.",
                    "label": 0
                },
                {
                    "sent": "That's a vector K is indexing the sample number.",
                    "label": 0
                },
                {
                    "sent": "And for each node, what we're going to do is we're going to take the regression of ZI the variable at node I on the remaining set of variables.",
                    "label": 1
                },
                {
                    "sent": "This is just notation for the remaining set.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is solving this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This is the logistic regression term, and here is the L1 penalty and right there.",
                    "label": 0
                },
                {
                    "sent": "Lambda N is a regularization term that will be chosen.",
                    "label": 0
                },
                {
                    "sent": "So once we've done this, we are going to get some vector.",
                    "label": 0
                },
                {
                    "sent": "It's going to have some zero somewhere in some non zeros and will simply estimate the neighborhood of the node is where the nonzeros are.",
                    "label": 0
                },
                {
                    "sent": "And we're actually going to do this simultaneously over all nodes of the graph at once.",
                    "label": 0
                },
                {
                    "sent": "What turns out to be key here is actually to look at the Fisher information matrix, just like in the linear regression case we saw that it was the structure of the covariance matrix there that controlled how it behaved.",
                    "label": 0
                },
                {
                    "sent": "Here it turns out to be the Fisher Information matrix, which is the analogous quantity, so this don't worry about the details.",
                    "label": 0
                },
                {
                    "sent": "But this is just the Fisher information matrix of this particular regression model at node I, and we're going to let Q. SS denote the submatrix that's corresponds to the support of that node, so that's the submatrix that corresponds to the non zero entries.",
                    "label": 1
                },
                {
                    "sent": "The one that correspond.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The edges.",
                    "label": 0
                },
                {
                    "sent": "So the assumptions we need are essentially the same, essentially analogous.",
                    "label": 0
                },
                {
                    "sent": "The first to the ones we had before.",
                    "label": 0
                },
                {
                    "sent": "You need a control on the eigenvalues of this Fisher information.",
                    "label": 0
                },
                {
                    "sent": "This is the same kind of incoherence condition that we required before, and.",
                    "label": 0
                },
                {
                    "sent": "In this case, we again need a kind of we need end to grow roughly logarithmically here, logarithmically in the ambient dimension of the problem, and it also depends on, for instance, the maximum degree of the graph.",
                    "label": 1
                },
                {
                    "sent": "So this condition is a bit weaker than what we had before, but this is a sufficient can.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what you can prove in this case is that if you follow this procedure, you're given samples for every node you're going to perform the logistic regression of that node on its neighbors.",
                    "label": 1
                },
                {
                    "sent": "You're going to use the support of the vector that you get.",
                    "label": 1
                },
                {
                    "sent": "You're getting some vector out of that optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Use that support to estimate the neighborhood structure of that node, and in the end you're going to have one estimate for every node in the graph.",
                    "label": 0
                },
                {
                    "sent": "So you need to combine these in some way.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter how you do it.",
                    "label": 0
                },
                {
                    "sent": "You can either take ends between them or ores.",
                    "label": 1
                },
                {
                    "sent": "And so if you follow this procedure again, this is this is computationally efficient since it just involves solving a sequence of logistic regressions that can be solved very quickly again by dual methods.",
                    "label": 1
                },
                {
                    "sent": "And what we're guaranteed is that if our triple, if the previous assumptions are satisfied and we choose our regularization parameter in such a way that these two conditions hold, then what you're guaranteed is that with probability one.",
                    "label": 0
                },
                {
                    "sent": "You choose the correct neighborhood at every node of the graph as the amount of data goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is that this is a computationally efficient method for doing graphical model selection.",
                    "label": 0
                },
                {
                    "sent": "Actually, in the non classical setting, both the size of the graph, possibly the sizes of the neighborhoods and the amount of data, all three things can grow at once, so this sort of allows you to deal with pretty general set of problems.",
                    "label": 0
                },
                {
                    "sent": "We do of course need these assumptions to be clear, so these assumptions will not hold for any graphical model, but it's certainly a reasonable subset of graphical models for which this holds.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "So what I discussed sort of two kinds of problems involving L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "First, looking at the simpler case of linear regression, we gave sharp thresholds for when it when it works when you can recover the sparsity.",
                    "label": 1
                },
                {
                    "sent": "In the case of the identity matrix, results are actually are sharp.",
                    "label": 1
                },
                {
                    "sent": "They can't be improved because both the theory and practice says that there's exactly a threshold.",
                    "label": 0
                },
                {
                    "sent": "At this point, one and you can see that's happening.",
                    "label": 0
                },
                {
                    "sent": "In the more general case, the theory, it should be possible to improve it right now.",
                    "label": 0
                },
                {
                    "sent": "Our theory says that the threshold is certainly above this line an it's below this line, but you can see there's quite a big gap between those two lines.",
                    "label": 1
                },
                {
                    "sent": "It should be possible to figure out analytically more precisely that it's somewhere around there, but that sort of requires, I think, some finer analysis.",
                    "label": 0
                },
                {
                    "sent": "In the case of logistic regression, we've given so far sufficient conditions for it to work.",
                    "label": 0
                },
                {
                    "sent": "But again, it's an interesting set of sufficient conditions.",
                    "label": 0
                },
                {
                    "sent": "You're now guaranteed to have a consistent procedure for model selection, even in the high dimensional setting.",
                    "label": 0
                },
                {
                    "sent": "Just a couple of remarks and open questions I talked about logistic regression and basically binary variables, But basically the same story can be carried over to more general graphical models.",
                    "label": 1
                },
                {
                    "sent": "You're just going to get a more general kind of regression for Gaussians.",
                    "label": 0
                },
                {
                    "sent": "It's linear regression.",
                    "label": 0
                },
                {
                    "sent": "For binary.",
                    "label": 0
                },
                {
                    "sent": "It's logistic regression.",
                    "label": 0
                },
                {
                    "sent": "For other models, it's going to be another kind of generalized linear model and a different kind of regression.",
                    "label": 0
                },
                {
                    "sent": "What I think are interesting is our results depend pretty critically on these so called mutual incoherence conditions.",
                    "label": 0
                },
                {
                    "sent": "Just to recap, these are conditions of this form which basically say there's the irrelevant variables in this subset.",
                    "label": 1
                },
                {
                    "sent": "Don't have too much of an effect on the relevant ones, so that's probably the strongest constraint that we have, and it's interesting to think cannot be weakened or eliminated.",
                    "label": 0
                },
                {
                    "sent": "Or is that really essential?",
                    "label": 0
                },
                {
                    "sent": "And the last thing that I think is interesting, and I believe some other people are working on, is actually to figure out forgetting about computation.",
                    "label": 0
                },
                {
                    "sent": "Just figuring out what are the fundamental limits.",
                    "label": 0
                },
                {
                    "sent": "When can you ever expect to recover sparsity?",
                    "label": 1
                },
                {
                    "sent": "What's the minimum amount of data that any method, even an Oracle, that could search the entire space could possibly recover?",
                    "label": 0
                },
                {
                    "sent": "So those are sort of interesting questions to figure out how close these approximate methods are coming to what the fundamental limits are.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Senator results on the rating system of graphical models.",
                    "label": 0
                },
                {
                    "sent": "At this point we do not.",
                    "label": 0
                },
                {
                    "sent": "Rather, Pradeep does not.",
                    "label": 0
                },
                {
                    "sent": "He's probably in the middle of doing it as we speak, but.",
                    "label": 0
                },
                {
                    "sent": "Will be interested to see that, but I don't expect there to be any issues that the theory seems solid and the agreement in the linear case is pretty clean, so.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, how about using?",
                    "label": 0
                },
                {
                    "sent": "Playing through step one, is there any?",
                    "label": 1
                },
                {
                    "sent": "Conversational when Q is less than one or larger than one.",
                    "label": 0
                },
                {
                    "sent": "You'll be nice to use Q less than one, but LQ norm for Q less than one or non convex so there would be pretty serious computational issues.",
                    "label": 0
                },
                {
                    "sent": "In fact, night and Fu the paper I mentioned, they have very nice results saying that in theory if you could do LQ for Q less than one, you get very good sparsity selection.",
                    "label": 0
                },
                {
                    "sent": "It's it's more powerful than L1, but it's nonconvex, so that's sort of why we're stopping at L1.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Can you read about methods for doing the other one?",
                    "label": 0
                },
                {
                    "sent": "I wonder if your eyes.",
                    "label": 0
                },
                {
                    "sent": "For actually solving L1 problems.",
                    "label": 0
                },
                {
                    "sent": "So I guess in the linear case there's been a fair bit of work group of statisticians at Stanford have an algorithm called Lars that sort of sequential algorithm will solve L1 constraint QPS.",
                    "label": 0
                },
                {
                    "sent": "In practice, I found that you can solve a lot of the L1 constraint QPS simply by dualizing them.",
                    "label": 0
                },
                {
                    "sent": "The reason that's helpful is that L1 constraints when you dualize become L, Infinity or box constraints, so you can just use a gradient projection method.",
                    "label": 0
                },
                {
                    "sent": "So actually the experiments I did, that's all I did.",
                    "label": 0
                },
                {
                    "sent": "It was very fast and easy to program.",
                    "label": 0
                },
                {
                    "sent": "In terms of the logistic regression, probably one of the best ways to solve it is again dualize.",
                    "label": 0
                },
                {
                    "sent": "If you dualize this as you might expect from this morning, you get an entropy, so the dual of this is actually a maximum entropy with an L Infinity constraint, so you can do allies and plug it into something like what's that convex program solver named.",
                    "label": 0
                },
                {
                    "sent": "Nothing you see Plex, but that's an LP solver.",
                    "label": 1
                },
                {
                    "sent": "Slipping my mind, but I'll get back to you on it, yeah so.",
                    "label": 0
                },
                {
                    "sent": "Pardon me.",
                    "label": 0
                },
                {
                    "sent": "El peasel but that's for LP's.",
                    "label": 0
                },
                {
                    "sent": "I want something for more general convex.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking not have said you me, that's for SDP's.",
                    "label": 0
                },
                {
                    "sent": "Which, OK?",
                    "label": 0
                },
                {
                    "sent": "Anyway, it suffice to say it's a nice convex problem, and it's sort of in a Canonical family, so there are a lot of techniques to solve it.",
                    "label": 0
                },
                {
                    "sent": "I still think probably you can make you know in general whatever structure you should have, you should exploit to try and get faster methods, and I think for solving very large scale problems there's still probably would be a bit of work to figure out how to solve this in the most efficient way possible.",
                    "label": 0
                },
                {
                    "sent": "No more question.",
                    "label": 0
                },
                {
                    "sent": "Appreciation of the contribution of Doctor Martin Wainwright regarding pre things keeps improving.",
                    "label": 0
                },
                {
                    "sent": "Do I get a Chinese name?",
                    "label": 0
                },
                {
                    "sent": "Yes, OK. Can you read?",
                    "label": 0
                },
                {
                    "sent": "This is Marty.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much for organizing, much appreciated.",
                    "label": 0
                }
            ]
        }
    }
}