{
    "id": "udpplamhjpczvvhz7t53p4665v7h5cc7",
    "title": "People In Motion: Pose, Action and Communication",
    "info": {
        "author": [
            "Stan Sclaroff, Department of Computer Science, Boston University"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision->Motion and Tracking"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_sclaroff_communication/",
    "segmentation": [
        [
            "Thanks for inviting me.",
            "As I said, I'll cover three main topics in my talk to."
        ],
        [
            "What I'll do is I'll first describe some of our work and pose in matching methods that allow efficient exact inference with loopy models focusing on pose estimation.",
            "2nd, I'll present.",
            "Two projects related to loose, loosely supervised learning of actions from web video.",
            "An image is an 3rd I'll talk about a collaborative project with linguists related to sign language recognition and retrieval.",
            "Now I'd like to emphasize that in this talk I'm going to give you a high level overview of a number of different projects in my group and also with collaborators of mine, so I may not go very deep in some areas mathematically, so please forgive me.",
            "What I'd like to do is convey.",
            "Some of the basic insights or observations that we've had along the way, and also I'd like to apologize upfront for not citing all of the related work that I could be citing, including many people in this room whose work I site and respect in the papers that we've written.",
            "We cite many people."
        ],
        [
            "MVC.",
            "So to start what I'd like to do is describe some work on a familiar topic of popular topic, which is pose estimation and matching and.",
            "We're going to be using graph based affordable part models and we're interested in methods that enable efficient exact inference when the graphs have loops.",
            "This is collaborative work with typing, Tienen a graduate student at BU who's now graduated, and how Zhang who's at Boston College."
        ],
        [
            "So what I'd like to do is focus on the human parsing problem, but the issues permeate other uses of graph based deformable part models, but I'll focus on human parsing and and we care about this problem as you'll see later when we're looking at action recognition, we focus on methods that use human pose distinctive poses to help us recognize actions in videos and also an image retrieval."
        ],
        [
            "So the standard and familiar tree based model for human pose is shown on the left and the torso is the root of the tree and the head and the arms and the legs are represented by simple rectangles and on the left hand side you see what is a common result that you obtained with a tree based model, where there's overcounting where the left and the right leg of the model or both are coincident on the image, and that's a valid interpretation.",
            "Of the image and explained by a tree model, now non tree models can incorporate higher order constraints.",
            "For instance appearance symmetry of the limbs.",
            "So you might expect that the person is wearing clothing that symmetric and so you might want to have a model that tends to reward interpretations of images that show symmetric appearance of the limbs.",
            "And also you may want to for a frontal pose enforce the constraint that.",
            "The left and the right leg tend not to overlap very very often, and so an uncommon interpretation is that the legs overlap and as a result what you can get is.",
            "This."
        ],
        [
            "Interpretation here, which looks better?",
            "And indeed, if you use pairwise constraints, here we have some examples from the image parsing data set of Ramadan.",
            "In the upper row of this figure you see the results that are obtained using a simple kinematic structure in a tree based model an in the bottom row you see results that are obtained when you use a simple pairwise appearance constraint for the arms and."
        ],
        [
            "Legs.",
            "But there's a catch.",
            "So when you use a non tree model then the computational complexity of inference is increased and.",
            "So if we have a tree graph.",
            "I we can use dynamic programming and the complexity is shown here.",
            "In this complexity is reduced by using what's called the distance transform Pedro Felzenszwalb and Dan Huttenlocher, and for the data set that I just showed you, on average, the time that it takes to do the pose inference for those images is about 112 seconds.",
            "Now for for non tree models if we use approximate methods it can take up to an hour or more.",
            "On average.",
            "Even then, for exact inference, it's also quite time intensive.",
            "So what I wanted to describe for you right now is our our."
        ],
        [
            "Ocean, which is slightly slower than on average, slightly slower in practice, then inference with the tree model, and permits exact inference on a non tree model.",
            "So here's the trick.",
            "We use branch inbound.",
            "Now.",
            "Branching bound is a familiar strategy.",
            "I'm not going to claim that it's new, but the idea here is to partition the solution space for searching for the optimal pose alignment with an input image by using a a search space Omega.",
            "What we're going to do is partition it and compute a lower bound for each partition and the lower bound is going to be based on that remodel cost.",
            "For matching the model to the image an our original model cost is shown here.",
            "This cost you so when we have a Singleton set, we actually evaluate the full model cost, but In interim steps what we'll do is we'll evaluate the lower bound and this is you can see that throughout the talk I'll.",
            "I'm not going to tell you very much about the works that are cited, but you'll see related works fly by in the bottom of slides when."
        ],
        [
            "And it's appropriate.",
            "So here's the basic idea for branching bound.",
            "And the key thing is that there's an inner loop, and in that inner loop, what's happening is we're doing splits of the search space, and as we split the search space, we need to recompute the lower bound and the challenges that this inner loop can be executed many times.",
            "Many, many times.",
            "And so efficiency of the computation of the lower bound is critical."
        ],
        [
            "But what we can note is because of the structure of our problem.",
            "If we look at message passing within the model, in fact messages are unaffected by the splitting at the lower notes.",
            "So what happens is we can actually reuse the messages that were pre computed in a previous computation of the lower bound.",
            "And we reuse those by using a memoized dynamic programming table.",
            "So we're using dynamic programming to compute the lower bound.",
            "OK, we're using the distance program distance transform to reduce the complexity further, and then we're memorizing these messages to further reduce the complexity.",
            "Then we also note that when we parted."
        ],
        [
            "And the search space Omega.",
            "In fact we've computed.",
            "The we've computed the values of each of the entries in this array here, and we can reuse them because the partitions are actually referring to minimums over different ranges.",
            "So when we do the message passing, we find them in finding the minimum of the messages over a particular range, and we can do this repeatedly, and this can be done quite efficiently using range minimum queries.",
            "So to summarize, we can reduce the time complexity of computing the lower bound by exploiting the structure of the problem, and that's the main insight so."
        ],
        [
            "By having that inner loop where we're repeatedly computing the lower bound by partitioning the search space, if we recognize that, then what we can do is we can actually reduce the computational complexity to order one."
        ],
        [
            "Alright, so that's the first message.",
            "So in a related work we looked at pose estimation for multi aspect views of the human.",
            "So that previous work that I just talked about looked at frontal views.",
            "And I won't talk about that work here, but I just want to note that a common solution for modeling pose is to use multiple models, for instance one for the frontal pose, one for the side pose or for object recognition you might have multiple models.",
            "The problem with this, of course, is that you've got a continuous space of post of camera orientation, and you're imposing some quantization over that space."
        ],
        [
            "So what the common solution is to quantize the rotation and angle and scale?",
            "I'm going to present this sort of as a general problem rather than just concentrate on pose for the moment.",
            "So the question is what range to sample if I'm if I'm looking at a scaling my model or how many samples to take in terms of the samples over rotation and if you look at this within the context of tree based deformable part models, this explodes the search space because we have to quantize over these different variables and search over them."
        ],
        [
            "And so here we have a non tree model and the tree constraints are shown in red.",
            "And the non tree constraints are shown in green.",
            "But the non tree edges can encode scale, consistency or rotation consistency between the parts of the model.",
            "The red constraints can be the kinematic constraints for instance.",
            "And then we have a hyperedge which can define properties for the whole model."
        ],
        [
            "Overall.",
            "So a strategy that we're going to employ is to to have any constraints on the tree part of the model, the red links and then not will have linear constraints on the non tree links in the model.",
            "This is called a linearly augmented tree.",
            "And this is described in the publication shown here.",
            "CPR 2011."
        ],
        [
            "So here's the basic equation, so we've got it.",
            "We've got an optimization problem and we want to do invariant matching invariant to rotation and scale, for instance.",
            "And so we've got the unary cost term, which is looking at the matching of a template point.",
            "To a target point in an image.",
            "And there's a mapping function F which is describing the mapping from the target to the from the target to the template."
        ],
        [
            "And this can be described as a simple linear form using a binary assignment matrix and a local cost matrix."
        ],
        [
            "Then we have rotation and scale consistency.",
            "And there are the parameters are rotation and scale of course, and we're looking for consistent mappings of pairs of points from the target.",
            "It's a template to the target."
        ],
        [
            "And here what we can do is we can we can write this term and linearize it as follows.",
            "We have a pairwise assignment matrix for each pair PQ and in the template in the target.",
            "We've got these rotation matrices that are written here in terms of the cosine and the sign of the angles.",
            "And we also linearize in terms of scale.",
            "OK, so we've got this.",
            "Rotation angle matrix and a scale matrix and some variables as shown here."
        ],
        [
            "So what this yields is this mixed integer optimization, which is a complex problem to solve in general, but but the special structure is going to help us.",
            "I want to note that there can also be global terms here and they should be linear or L1 and L1 can be linearized through some auxiliary auxiliary variable techniques, so this subject to some constraints as I'll show in a moment."
        ],
        [
            "In the next slide so we can relax this.",
            "To solve it and.",
            "There's our objective function.",
            "And.",
            "These are our non tree constraints.",
            "It looks kind of complex.",
            "So I'll call.",
            "Those are hard constraints and as in they are difficult to work with, not in another sense, so that I'll put those in red.",
            "The remaining constraints and bounds look like tree constraints.",
            "OK, so we've got this problem now where we've got our linearly augmented tree where the augmented constraints are in red.",
            "And the tree constraints are in green.",
            "So this is a problem again with the special structure."
        ],
        [
            "So here's the basic idea.",
            "We can actually use the Danzig Wolfe decomposition to solve our problem by using the the solutions for the constraints in the green box as proposals for finding the optimal solution which lives on the combined solution with the hard constraints.",
            "OK, so this is also known as delayed column generation, where the proposals are being generated using a dynamic programming technique for the tree constraints, which were shown in.",
            "Green"
        ],
        [
            "And there are simple, simpler to compute.",
            "And then what we can do is we use linear combinations of those similar problems to find the solution to the harder problem."
        ],
        [
            "So I'm.",
            "At each step we choose a new proposal that optimizes some game game.",
            "Sorry, which I'm not going to describe here.",
            "I just want to give the idea.",
            "So here again, the key concept is they look at.",
            "If you choose the right structure for the problem.",
            "In this case by using linearly augmented trees.",
            "What you get is initially a problem that looks like it's very difficult to solve, but you recognize this structure.",
            "In fact, their proposal generation is cheap.",
            "It can be done via dynamic programming OK.",
            "So in this paper that I that I cite below."
        ],
        [
            "So one of the example applications of this technique was to use it for matching templates of regions to video sequences where the regions are extracted roughly using a superpixel representation."
        ],
        [
            "So here we've got a template image.",
            "Here, and this is the original image and the segmentation and the matching note here.",
            "This is the same template but now later in the video sequence it's rotated OK and here we have another template and a target image in varying scale and rotation.",
            "OK, so key point is we don't have to quantize.",
            "Over rotation and scale.",
            "To find these solutions.",
            "So here are some example videos and showing the matchings that are obtained and the CPU time in matching per frame in these sequences varies between .02 and .42 seconds per frame on a regular personal computer.",
            "And the way the experiment was run is so you pick one of the frames as randomly as a template.",
            "Use the regions from that you select the regions that are relevant to tracking and then you track it forward through the whole sequence OK."
        ],
        [
            "Alright, so some observations.",
            "So.",
            "The branch and bound and exploiting the problem structure allowed us to to to do exact post inference with non tree graphs.",
            "For the majority of cases now I want to emphasize two things, one branch and bound, in practice gave very good performance, but still the worst case complexity is there and occasionally you will get a case which will go toward the worst case of complexity.",
            "But on average it does much better.",
            "And in practice, for us, it works quite well.",
            "The other thing I want to note is that if you're thinking about using a tree based lower bound, you need to think about how well the lower bound cost fits how close the lower bound is, or what how good a lower bound the tree tree cost is for the overall cost function.",
            "In our problem, we chose the tree based lower bound so that we could take advantage of the distance transform and that further reduce the complexity of computing the lower bound at each iteration.",
            "So you can think of finding the spanning tree of a.",
            "Of a non tree graph and using that as the lower bound.",
            "But you have to be careful in how you choose it, so it's very well so suited for pose estimation.",
            "Future work will tell us whether it's suited for other applications.",
            "And linearly augmented trees, scale and rotation invariant matching does not require quantization of the continuous variables and the special structure permits.",
            "Permits use of the Doncic Wolfe decomposition and dynamic programming for proposal."
        ],
        [
            "Rayshon alright, so in in the second set of projects what I'd like to talk about is learning of actions from loose supervision by loose supervision I mean that you're given an image an you're told that there's an action taking place in that image, but you don't know who's doing that action.",
            "Or where they are, or you're given a video and you're told that there is someone playing basket in that basketball.",
            "In that video, you don't know who's playing video, where in the video they appear.",
            "There may be other people present in the video, etc.",
            "We'll also be interested in learning what scene features are relevant and what object features are relevant in the scene with the action, and we don't know which objects might be relevant, for instance, or maybe a basketball, but there may also be other objects moving around in the scene, and maybe the basketball is relevant, but the others are not."
        ],
        [
            "So really, we're interested in action recognition and uncontrolled videos, and this is becoming more and more popular.",
            "Moving away from the staged action recognition datasets more towards.",
            "Uncontrolled video YouTube home movies.",
            "These these sequences are are typified by.",
            "They are difficulty moving cameras, cluttered scenes, dynamic backgrounds.",
            "And there's also great variation in the actions.",
            "The styles of those actions, the dynamics, the body, proportions of the people performing those actions, the clothing variation, etc.",
            "And then, on top of that, there are compression artifacts.",
            "Due to the compression standards used in compressing the video."
        ],
        [
            "So in this work, what we've been doing is we're looking at learning actions from the web, and this has been.",
            "Internet vision has been quite successfully used in the object recognition communities, so we're taking our inspiration from that.",
            "And the beauty, of course, is that there are lots of action images on the web.",
            "We're going to use images of human poses.",
            "In recognizing actions actions not only in images but also in video.",
            "So the nice thing about these images that they can help capture the diversity of the nature of the actions that were trying to recognize, and they also may reduce the training bias that might be encountered in staged datasets.",
            "For action recognition, there are uncontrolled poses.",
            "There are various people, clothing, body proportions, but the same time this data set or if you consider the web, the web imagery presents a challenge due to its diversity.",
            "As well.",
            "And there are two papers here that I've cited.",
            "One is from my CV.",
            "Another is more recently from ACM Multimedia, so here's the basic idea."
        ],
        [
            "So system overview.",
            "So the.",
            "The basic system works as follows.",
            "We specify a text query.",
            "To retrieve images from the web.",
            "Using a standard search engine like Google.",
            "And we get the first page of the results due to the text processing that's inherent in these search engines and the tuning of the image search engine engine parameters.",
            "The results have pretty high precision, but still there are images which don't show people running.",
            "But the first page tends to have pretty high precision, and that many of the images are relevant.",
            "By relevant, I mean there's at least one person in the picture doing that action.",
            "Nevertheless, the the we find that actually there's about 4040% label noise and some of the retrieval results that we get, and specifying the text keywords that are used in the query turns out to be somewhat critical.",
            "Nevertheless, what we're doing, I'll call it loop loosely supervised learning.",
            "In that all we do to train the system as we give it some keywords, and then we run some searches on Yahoo and Google another image, search engines and what we get in the first page is a set of images and we run.",
            "A person detector over those images.",
            "The felzenszwalb at all person detector will do and for all of the images for which a person is detected.",
            "We save those images and we get rid of the other images right off the bat.",
            "And given that first set of images we we do some outlier rejection further by fitting a model to them that I'll describe in a second.",
            "And then using that model we retrieve the next page of results, which tend to be a bit noisier, and using the posterior of the model that we just estimated we reject images which are below a threshold in terms of the posterior we include the images which are thought to be likely depicting of that action, or consistent with the model we've learned, and we iterate.",
            "Then, given that clean data set, we we learn a classifier for each of the poses for that action and then use those key poses to label actions in video."
        ],
        [
            "So the first step is the action image retrieval step.",
            "And what you see in this slide?",
            "Are retrieved images for five actions running in the top row.",
            "Walking sitting.",
            "Playing golf.",
            "And dancing.",
            "The red boxes show outliers.",
            "So outliers could be a person not engaged in that action.",
            "It could be not a person.",
            "'cause sometimes the detector doesn't give correct result.",
            "And our goal is to remove as many of these as possible."
        ],
        [
            "So the image representation that we're using for the windows of the detection we could use what's called a hog descriptor.",
            "The histogram of oriented oriented gradients.",
            "And that descriptor is shown here.",
            "So you compute the gradients and the orientation of the gradients and then a histogram instead.",
            "We've found that using a probability of boundary image and works better for us, and the reason is that.",
            "The there is parts of the background lie within the detection window.",
            "It's a persons here, but there's also a fence in the background, and what we'd like to do is focus on the human in the foreground and less on the background.",
            "And we found that the probability of boundary measure works somewhat better in reducing the effect of the background within the detection window."
        ],
        [
            "And so here's the incremental procedure and I'll describe it step by step, just briefly so you run a query.",
            "You preprocess the retrieved images as I've described.",
            "We do an alignment of the head positions because the head head positions tend to be somewhat more stable.",
            "We extract the PB Hog features and then we build an action model and the way we build the action model is we use a logistic regression.",
            "And one of the insights here is that keeping the model simple and the outlier rejection method simple tends to work better than using a more complicated procedure.",
            "We've tried many and this seems to work the best.",
            "And then there's a loop.",
            "So while more images remain, we compute the posterior.",
            "We include all of those where the posterior is greater than a threshold.",
            "We will train the logistic regression.",
            "And continue.",
            "Alright."
        ],
        [
            "So this can be used to improve the precision of the retrieved results.",
            "So here we've collected images from Google and Yahoo Image Search and you can see the categories and the number of images that were in the final set.",
            "The blue bars on this chart showed the precision of just the keyword retrieved result.",
            "The green bars show the result using our our iterative technique to refine the set, but using hog features and then red shows the result using PB hogs and you can see that over eight of the 11 actions the results are better using PB hogs in improving the precision.",
            "But there are.",
            "There are three actions where the.",
            "The precision is actually slightly.",
            "Better for hogs, so that would be riding a horse.",
            "Swinging a Golf Club.",
            "And jumping.",
            "And we believe that this might be due to the fact that in those particular images, the background is is a useful feature.",
            "So after we improve."
        ],
        [
            "The precision of the retrieved data set.",
            "So we've got one collection of images, let's say for running we apply non negative matrix factorization.",
            "To find the modes in the data and here are some examples, this is these are five modes for running you could see a side pose three quarter poses front pose, and I think this is roughly a leg up and both legs down.",
            "So using these, what we're going to do is then train a separate logistic regression classifier for each pose mode, so you can think of these as key poses.",
            "OK."
        ],
        [
            "And we're going to use these in action recognition in video.",
            "It's going to be a simple scheme.",
            "It's tracking by detection.",
            "So you run a detector over the video sequence that's input.",
            "And we use simple bounding box tracking where we initialize the tracker on the 1st frame using the detector and then consecutive frames giving given the results of the detection we link them together based on proximity.",
            "If no track is associated to a new detection, that would create a new track and then on top of that we run our classifier and our classifier works as follows.",
            "We're going to use a very simple 1st order model where the action is allowed to move from one pose to another pose within the same action with a low cost.",
            "And you're allowed to move from one action to another action with a higher cost.",
            "So it's a very simple dynamic programming problem and we can solve for the the resulting classification."
        ],
        [
            "So here is an example of a YouTube video from video data set of nebulous at all.",
            "And the red boxes show incorrect classification.",
            "And what we find is that some of the poses are shared across actions.",
            "So if we're dancing, you'll find poses sometimes that look like poses that you would find in walking.",
            "Nevertheless I this is, uh, I've enlarged this video.",
            "It looks kind of blurry, and in fact it is."
        ],
        [
            "Here are some other results.",
            "For this data set and again, the red boxes show some.",
            "False classifications this is thought to be dancing, I think may because of hands on hip playing golf.",
            "Walking, dancing.",
            "Etc So it works pretty well."
        ],
        [
            "So we've extended this, so now let's say I've given you a technique where I've learned poses from images.",
            "What if I combined web images and web video?",
            "So let's say I've only got, you know, labeling videos is relatively tedious expensive, so maybe I'll give you a few videos where I tell you, OK, there's a person doing this action, and I. I asked, OK, what happens if I combine the OR I bootstrap?",
            "I use the images to 1st labeled the first set of videos and then I add more and more videos and as I add videos, how does the performance change?",
            "So in this experiment what we did was we took the JCF YouTube video data set.",
            "We grouped the videos into videos that showed similar actions, and we chose at random.",
            "One example from each one of those eight clusters, and that's where we started, and we kept adding more and more videos.",
            "So here the green curve is web poses, plus video poses, and So what you see is that when you have only a few videos, you can do quite well by starting with the image based model.",
            "But as you get more and more videos, eventually the video video based model overtake the hybrid model."
        ],
        [
            "Another thing that we examined was the ordering of the poses.",
            "The model that I described earlier that was based on the dynamic programming didn't look at the ordering, so we used a simple model that's like a bigram model where.",
            "You look at the order of 1 pose after another within within the action.",
            "And as you would expect, this also improves our classification.",
            "Accuracy, so if you look at the number of training videos in this direction.",
            "This red curve is web poses plus video poses.",
            "This blue curve is with the ordered pose pairs so you can see that it improves as you would expect.",
            "Alright.",
            "So that was."
        ],
        [
            "One project that looked at using web images and video for training action models in a loosely supervised way and what I'd like to do is briefly describe another approach that we've developed that looks at leveraging scene properties, object properties, and human motion and poses.",
            "Together to improve the accuracy of action classification.",
            "And the basic idea is a familiar one.",
            "The context of the video will tell, or of an image will tell us something about what actions we would expect are likely there.",
            "For instance, we don't expect golf to be played in this image.",
            "And here we probably wouldn't expect to see swimming.",
            "And here the presence of a basketball moving in the air might tell us that it's likely that we would observe basketball or it would be unlikely to see swimming.",
            "Maybe water polo."
        ],
        [
            "So the approach that we're going to take is to recognize that single features that we're going to extract in these videos are going to be quite noisy.",
            "The detections are going to be error prone for object detections.",
            "We're going to get many moving objects which aren't relevant to an action.",
            "So given these noisy features, we need an approach which is going to be robust.",
            "And given many non relevant tracks including other people performing not performing the action of interest, irrelevant objects, parts of the sequence that don't show that action, if you've got a video, that's a clip and part of the clip shows that action, but part of it does not.",
            "You've got what I'll call a multiple, but can be framed as a multiple instance learning problem.",
            "So multiple instance learning.",
            "In this case, each video is a bag.",
            "The instances in the bag include the moving objects, the people in the scene features.",
            "We don't know which of the instances inside these each of these bags is relevant to the action.",
            "That appears.",
            "OK, so I hope people are familiar with multiple instance learning.",
            "It's a very simple concept.",
            "I'll say very briefly, so let's say I want to teach you how to classify objects and all I do is I give you bags.",
            "I say this is a positive bag.",
            "And inside the bag there are some examples of the positive class and also some examples of the negative class.",
            "But I just say this is a positive bag, one at least one of the items in this bag.",
            "Is the of the positive class in our case, the bag is a video right?",
            "Somewhere in this video there's a person performing this action somewhere.",
            "In this video there may be objects whose motions are relevant to this action, but you don't know which ones there are and the learning process must discover those.",
            "I give you negative bags and I say in this in this bag there's no instance of this action, so basketball does not occur here.",
            "So given a collection of these bags, your job is to learn.",
            "For the positive bags, which instances are relevant?",
            "Which body poses are relevant?",
            "Which scene features are relevant?",
            "Which objects are relevant?",
            "And the nice thing about this is that some of the detections can be wrong, and those instances we hope will be filtered out as a process as part of the learning process.",
            "OK, so that's the basic idea."
        ],
        [
            "So here's an overview with pictures.",
            "First, we stabilize an input video using a basic technique that stabilizes the video.",
            "Then we extract the moving object tracks.",
            "And.",
            "We also extract the person tracks by using a person detector, much like I described earlier.",
            "And then what we've got are these multiple channels.",
            "We have candidate objects.",
            "Candidate person detections.",
            "And we also sample features of the scene at randomly selected sets of frames within the video, OK?",
            "So these are our bags.",
            "We have bags across multiple channels.",
            "And then within those channels we compute multiple features, for instance object hogs.",
            "Object upflow for the moving objects.",
            "Person Hogson person optic flow for the person detection's.",
            "Just features and also color features for the scene."
        ],
        [
            "So using this approach, I'm not going to describe the mathematical formulation, but the mathematical formulation in brief, learns the weight relative weighting of those feature channels for each of the actions, and also can find out or can discover which instances are relevant in the training examples for each of the actions, and most important for discriminating between pairs of action classes.",
            "So we'll evaluate our system using the UC F YouTube data set with 11 actions as shown and will use leave one out cross validation."
        ],
        [
            "And this is a big chart.",
            "So what I want to do is focus your attention.",
            "So focus your attention on the Gray area.",
            "So the.",
            "The combination of features is what we're most interested in.",
            "So in this table, what you see across the top are the action classes present in the data set, and then these labels are just the individual feature channels.",
            "And if I were to develop a classifier that just use that particular feature channel, what accuracy would I get in the classification of those actions in that data set?",
            "And so the best the best feature on average.",
            "If you use it just alone, is the gist feature, which is a scene.",
            "Feature for scene context and what this is saying is that for the UC F data set, there's actually seen features.",
            "A scene features alone can get you pretty good classification of the actions.",
            "Then if you look at the overall best performance, what we get is by combining the features using the multiple instance learning technique, we do better significantly better on average.",
            "OK. We compared against a work by Lou at all and we also do better than they did on this data set.",
            "So."
        ],
        [
            "Here are some visual examples.",
            "The interesting thing is to see for the cases that it gets right.",
            "So T means the test and C means the classification.",
            "So here the test case was diving and the class that it was assigned was diving and the system found the blue is shows the object that it thought it was relevant and the green is the person detection that it thought was relevant.",
            "So it's both classifying and saying which objects and elements of the scene are relevant.",
            "And for other examples tennis, it detects a racket.",
            "For golf, it detects a person at some part of the Golf Club.",
            "I.",
            "And there are also cases where it doesn't do the right thing, so here's an example where.",
            "I can't, I can't read that.",
            "Basketball, I hope you can read it so.",
            "Basketball shooting.",
            "So it detected the relevant people, but it classified it as a volleyball spike, which is not not that bad, right?",
            "If you think about where volleyball and basketball are played and the kinds of moves you might find.",
            "But there are also others, which, for instance here we have a bike and here we have.",
            "It's classified as walking and the pose of the person looks a little bit like walking, but still it's biking.",
            "So it got it wrong.",
            "Alright, so here are some observations about the work."
        ],
        [
            "I've just described.",
            "So the nice thing about learning actions from the web, but is that the action classes are easily extensible, you can just run a text based query and extend your model right with a new action.",
            "Data is plentiful, plentiful, diverse, but noisy.",
            "Using the multiple instance learning approach it.",
            "It was possible to leverage scene objects and people features in the scene despite somewhat noisy tracking and detection.",
            "And it also selected the relevant features for the actions that we were classifying.",
            "So finally the third part."
        ],
        [
            "My talk and I'll be brief here.",
            "With the sign language stuff I is communication and there's a longstanding collaboration that I've had it be you with Carol Neidle and her group.",
            "Carol is a linguist at a specialist in American Sign Language.",
            "This collaboration that's been going on for about 10 years and.",
            "There's also a graduate student of Mindless Vassilis defeat sauces now an associate professor at University of Texas was a collaborator on this project for a long time, so I wanted knowledge.",
            "Their work and contributions in this effort."
        ],
        [
            "So our current our current effort is focusing on the following problem.",
            "How to look up a sign, let's say in an ASL dictionary, or on a sign language YouTube channel.",
            "I buy using.",
            "Your native language.",
            "Sign language, let's say or.",
            "Let's say you're learning sign language and you want to look up a sign.",
            "You pick the relevant piece of clip and you say the sign occurs.",
            "Here you submit it to the search tool and then it returns the result which is.",
            "A ranked set of of retrieved sign."
        ],
        [
            "Lines where the most similar appears first.",
            "I don't have a live demo unfortunately, so this is work in progress.",
            "So let's say I submit an input sign, and then it retrieves some results based on the similarity to that sign and presents some other information.",
            "So this in this case these are entries from lexecon of signs, but it could also be examples of videos that are present in some collection of videos that you've indexed."
        ],
        [
            "So a major part of this effort has been data collection.",
            "This is a data driven, an linguistics driven project.",
            "So.",
            "The data collection that we've engaged in for this project involves working with native signers in a video capture lab.",
            "And collecting over 3000 signs that are present in the Gallaudet Dictionary and also adding additional signs to that collection and the elicitation was led by our linguistics collaborator Carol Neidle."
        ],
        [
            "And this data set, which will soon be made available, is pretty high quality, high resolution both in space and time.",
            "So it's 60 frames per second for some of these videos, and it also includes bounding boxes for the faces and hands for some of the video clips in the data set.",
            "The linguistics team has been annotating this data, and this is a quite involved process that, as a result of labeling the start and the end handshapes in each of the signs that are so.",
            "Imagine going through a video labeling the start and the end point of every sign.",
            "The start and the end handshape of every sign labeling other relevant features.",
            "This is a process that actually sometimes exposes interesting questions about the nature of sign language and also raises questions about exactly which hand shape is being used and what you find is that there's variation, just natural variation, just like there's natural variation in pronunciation in spoken speech, there's pronunciation variation in sign language.",
            "So."
        ],
        [
            "Part of the data set includes labeling the handshapes, and here you see the handshape set classes that are used in the data set.",
            "And in labeling, this is time consuming.",
            "And also it needs to be verified before it's released.",
            "That's why it's taking us awhile beef."
        ],
        [
            "Or it's available?",
            "Alright, so we're exploiting this data set in some of the work related to the retrieval, and I'll tell you how that works, so we're.",
            "We're looking at ways to constrain the retrieved set that I showed.",
            "You have relevant signs that you get back based on the start and the end handshapes, and also what we'd like to do is improve our hand shape recognition by leveraging the training, leveraging the data that we've collected as a training set."
        ],
        [
            "Here's an overview of what I mean.",
            "So, given the beginning of a sign at the end of the sign and hand detections which are found.",
            "What we want to do is we want to classify the hand shape and the hand shape.",
            "Of course is is 1 relevant feature of recognizing assign hand orientation is another in hand motion as another.",
            "So these are features of signs that we need to consider in order to be able to do accurate sign language retrieval.",
            "So, given the the input is the start and end frame, we want to find the nearest neighbor handshapes from the training data set that we've got.",
            "And then what we use is what we call a hand shape inference or handshape Bayesian network HBN.",
            "And the the HBN is structured as follows, the lowest level it's shown here for one handed signs where you have a start hand shape and an end hand shape, and you have the observed produced hand shape.",
            "So here are the image features.",
            "The lowest node, of course is the image features.",
            "The blue note.",
            "The middle node is the produced handshapes.",
            "The this higher node here is an observed and that would be there's some underlying process where their natural variations in the hand shapes that are actually produced.",
            "These are equivalent to allophones and speech in some ways, so you have just natural variation in the hand shapes that are produced.",
            "These these actually variations are follow our general and are occur across signs.",
            "It's not assigned specific phenomenon, there are certain.",
            "Variations that occur, and there's subsets of of shapes that that can vary.",
            "Together so we want to exploit that we're going to learn that from the data set and build it into our model.",
            "Here I show the model for one handed signs.",
            "There are models for two handed symmetric signs like book and two handed nonsymmetric signs like assigned to give OK, so that's the basic idea."
        ],
        [
            "So we've used this model in.",
            "Hand held shape classification or given a training set which is different a different signer than the training set is different signers than the test set.",
            "So it's signer independent and.",
            "We're given signs for model evaluation that are taken from our data set as follows, and as you would expect by using the model for inference, we get better accuracy in classifying the handshapes.",
            "So this is just one subsystem within the retrieval system that I described earlier."
        ],
        [
            "In a completely different project, this is future work.",
            "I'm almost I'm almost done so.",
            "In a completely different project, unrelated to the sign Language project that I just described, we're looking at children who are learning sign language in sessions with.",
            "With an adult where these are children who have have delays or communication problems and they need to learn sign language in order to communicate their what they know.",
            "For instance that this child wants popcorn.",
            "So there's a.",
            "A person here who's showing the child.",
            "That sign as an example and then helping the child.",
            "And now she's showing the child the sign for cracker.",
            "Actually, it's around handshake cracker, so I don't speak sign language unfortunately, so the basic idea though is that.",
            "We've got a session here with a child, and you've got a person touching the hands of the child, showing them how to form the right hand shape.",
            "So we want to make sure that the hand shape is correct.",
            "We want to make sure that the shape is correct.",
            "And as part of this session, what's happening is that the child gets a reward of the popcorn when the sign is produced, and by repeatedly performing this training overtime, the signing will improve, and what we want to be able to do is to measure that improvement overtime automatically.",
            "'cause this is a very.",
            "Time consuming process an it happens there sessions multiple times per week and from session to session.",
            "You want to be able to pick up where you left off and continue what's called the behavior shaping which is providing moving the target so as the child gets closer and closer to the target sign that you withhold the reward until they get a little bit better.",
            "So this is part."
        ],
        [
            "A larger project.",
            "The data that I showed you was collected at Georgia Tech, which is the lead in this collaboration.",
            "It's a multi institutional project that's looking very broadly at computational methods for analyzing face to face communication.",
            "And it's looking at speech.",
            "It's looking at autonomic signals.",
            "First person, computer vision.",
            "That what I just showed you is just a video that was collected for the sign language part of the project.",
            "There's much more going on in this project and the target.",
            "Application area that we're looking at is supporting the diagnosis and treatment of autism and other behavioral developmental disorders, but really, the tools that are being developed will be could be useful in many other many other disciplines, such as education, advertising, customer relations, and what we're looking to do is to develop quantitative tools that enable analysis of human behavior.",
            "So."
        ],
        [
            "Some observations about the sign language work, data driven and linguistics driven methods are promising and I want to emphasize that sign language recognition in unconstrained settings is remains challenging.",
            "I'll leave it at that."
        ],
        [
            "Alright.",
            "So to recap, I told you about these three projects."
        ],
        [
            "And I want to thank my collaborators who without them especially being a Department chair without collaborators, I'd be subbed, but generally speaking, I'm indebted to the collaborators here, particularly for the linguistics work.",
            "Carol Neidle an for the search engine work of Bacillus thit sauce and nasli for the action recognition work an how and TN for the work on the on the inference of pose.",
            "And matching."
        ],
        [
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for inviting me.",
                    "label": 0
                },
                {
                    "sent": "As I said, I'll cover three main topics in my talk to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'll do is I'll first describe some of our work and pose in matching methods that allow efficient exact inference with loopy models focusing on pose estimation.",
                    "label": 1
                },
                {
                    "sent": "2nd, I'll present.",
                    "label": 0
                },
                {
                    "sent": "Two projects related to loose, loosely supervised learning of actions from web video.",
                    "label": 0
                },
                {
                    "sent": "An image is an 3rd I'll talk about a collaborative project with linguists related to sign language recognition and retrieval.",
                    "label": 1
                },
                {
                    "sent": "Now I'd like to emphasize that in this talk I'm going to give you a high level overview of a number of different projects in my group and also with collaborators of mine, so I may not go very deep in some areas mathematically, so please forgive me.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to do is convey.",
                    "label": 0
                },
                {
                    "sent": "Some of the basic insights or observations that we've had along the way, and also I'd like to apologize upfront for not citing all of the related work that I could be citing, including many people in this room whose work I site and respect in the papers that we've written.",
                    "label": 0
                },
                {
                    "sent": "We cite many people.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "MVC.",
                    "label": 0
                },
                {
                    "sent": "So to start what I'd like to do is describe some work on a familiar topic of popular topic, which is pose estimation and matching and.",
                    "label": 0
                },
                {
                    "sent": "We're going to be using graph based affordable part models and we're interested in methods that enable efficient exact inference when the graphs have loops.",
                    "label": 0
                },
                {
                    "sent": "This is collaborative work with typing, Tienen a graduate student at BU who's now graduated, and how Zhang who's at Boston College.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'd like to do is focus on the human parsing problem, but the issues permeate other uses of graph based deformable part models, but I'll focus on human parsing and and we care about this problem as you'll see later when we're looking at action recognition, we focus on methods that use human pose distinctive poses to help us recognize actions in videos and also an image retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the standard and familiar tree based model for human pose is shown on the left and the torso is the root of the tree and the head and the arms and the legs are represented by simple rectangles and on the left hand side you see what is a common result that you obtained with a tree based model, where there's overcounting where the left and the right leg of the model or both are coincident on the image, and that's a valid interpretation.",
                    "label": 0
                },
                {
                    "sent": "Of the image and explained by a tree model, now non tree models can incorporate higher order constraints.",
                    "label": 0
                },
                {
                    "sent": "For instance appearance symmetry of the limbs.",
                    "label": 0
                },
                {
                    "sent": "So you might expect that the person is wearing clothing that symmetric and so you might want to have a model that tends to reward interpretations of images that show symmetric appearance of the limbs.",
                    "label": 0
                },
                {
                    "sent": "And also you may want to for a frontal pose enforce the constraint that.",
                    "label": 0
                },
                {
                    "sent": "The left and the right leg tend not to overlap very very often, and so an uncommon interpretation is that the legs overlap and as a result what you can get is.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interpretation here, which looks better?",
                    "label": 0
                },
                {
                    "sent": "And indeed, if you use pairwise constraints, here we have some examples from the image parsing data set of Ramadan.",
                    "label": 1
                },
                {
                    "sent": "In the upper row of this figure you see the results that are obtained using a simple kinematic structure in a tree based model an in the bottom row you see results that are obtained when you use a simple pairwise appearance constraint for the arms and.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Legs.",
                    "label": 0
                },
                {
                    "sent": "But there's a catch.",
                    "label": 0
                },
                {
                    "sent": "So when you use a non tree model then the computational complexity of inference is increased and.",
                    "label": 0
                },
                {
                    "sent": "So if we have a tree graph.",
                    "label": 0
                },
                {
                    "sent": "I we can use dynamic programming and the complexity is shown here.",
                    "label": 0
                },
                {
                    "sent": "In this complexity is reduced by using what's called the distance transform Pedro Felzenszwalb and Dan Huttenlocher, and for the data set that I just showed you, on average, the time that it takes to do the pose inference for those images is about 112 seconds.",
                    "label": 0
                },
                {
                    "sent": "Now for for non tree models if we use approximate methods it can take up to an hour or more.",
                    "label": 0
                },
                {
                    "sent": "On average.",
                    "label": 0
                },
                {
                    "sent": "Even then, for exact inference, it's also quite time intensive.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to describe for you right now is our our.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean, which is slightly slower than on average, slightly slower in practice, then inference with the tree model, and permits exact inference on a non tree model.",
                    "label": 0
                },
                {
                    "sent": "So here's the trick.",
                    "label": 0
                },
                {
                    "sent": "We use branch inbound.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Branching bound is a familiar strategy.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to claim that it's new, but the idea here is to partition the solution space for searching for the optimal pose alignment with an input image by using a a search space Omega.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is partition it and compute a lower bound for each partition and the lower bound is going to be based on that remodel cost.",
                    "label": 0
                },
                {
                    "sent": "For matching the model to the image an our original model cost is shown here.",
                    "label": 0
                },
                {
                    "sent": "This cost you so when we have a Singleton set, we actually evaluate the full model cost, but In interim steps what we'll do is we'll evaluate the lower bound and this is you can see that throughout the talk I'll.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to tell you very much about the works that are cited, but you'll see related works fly by in the bottom of slides when.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's appropriate.",
                    "label": 0
                },
                {
                    "sent": "So here's the basic idea for branching bound.",
                    "label": 0
                },
                {
                    "sent": "And the key thing is that there's an inner loop, and in that inner loop, what's happening is we're doing splits of the search space, and as we split the search space, we need to recompute the lower bound and the challenges that this inner loop can be executed many times.",
                    "label": 0
                },
                {
                    "sent": "Many, many times.",
                    "label": 0
                },
                {
                    "sent": "And so efficiency of the computation of the lower bound is critical.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what we can note is because of the structure of our problem.",
                    "label": 0
                },
                {
                    "sent": "If we look at message passing within the model, in fact messages are unaffected by the splitting at the lower notes.",
                    "label": 0
                },
                {
                    "sent": "So what happens is we can actually reuse the messages that were pre computed in a previous computation of the lower bound.",
                    "label": 0
                },
                {
                    "sent": "And we reuse those by using a memoized dynamic programming table.",
                    "label": 0
                },
                {
                    "sent": "So we're using dynamic programming to compute the lower bound.",
                    "label": 0
                },
                {
                    "sent": "OK, we're using the distance program distance transform to reduce the complexity further, and then we're memorizing these messages to further reduce the complexity.",
                    "label": 0
                },
                {
                    "sent": "Then we also note that when we parted.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the search space Omega.",
                    "label": 0
                },
                {
                    "sent": "In fact we've computed.",
                    "label": 0
                },
                {
                    "sent": "The we've computed the values of each of the entries in this array here, and we can reuse them because the partitions are actually referring to minimums over different ranges.",
                    "label": 0
                },
                {
                    "sent": "So when we do the message passing, we find them in finding the minimum of the messages over a particular range, and we can do this repeatedly, and this can be done quite efficiently using range minimum queries.",
                    "label": 1
                },
                {
                    "sent": "So to summarize, we can reduce the time complexity of computing the lower bound by exploiting the structure of the problem, and that's the main insight so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By having that inner loop where we're repeatedly computing the lower bound by partitioning the search space, if we recognize that, then what we can do is we can actually reduce the computational complexity to order one.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so that's the first message.",
                    "label": 0
                },
                {
                    "sent": "So in a related work we looked at pose estimation for multi aspect views of the human.",
                    "label": 1
                },
                {
                    "sent": "So that previous work that I just talked about looked at frontal views.",
                    "label": 0
                },
                {
                    "sent": "And I won't talk about that work here, but I just want to note that a common solution for modeling pose is to use multiple models, for instance one for the frontal pose, one for the side pose or for object recognition you might have multiple models.",
                    "label": 0
                },
                {
                    "sent": "The problem with this, of course, is that you've got a continuous space of post of camera orientation, and you're imposing some quantization over that space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what the common solution is to quantize the rotation and angle and scale?",
                    "label": 1
                },
                {
                    "sent": "I'm going to present this sort of as a general problem rather than just concentrate on pose for the moment.",
                    "label": 0
                },
                {
                    "sent": "So the question is what range to sample if I'm if I'm looking at a scaling my model or how many samples to take in terms of the samples over rotation and if you look at this within the context of tree based deformable part models, this explodes the search space because we have to quantize over these different variables and search over them.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here we have a non tree model and the tree constraints are shown in red.",
                    "label": 0
                },
                {
                    "sent": "And the non tree constraints are shown in green.",
                    "label": 0
                },
                {
                    "sent": "But the non tree edges can encode scale, consistency or rotation consistency between the parts of the model.",
                    "label": 0
                },
                {
                    "sent": "The red constraints can be the kinematic constraints for instance.",
                    "label": 0
                },
                {
                    "sent": "And then we have a hyperedge which can define properties for the whole model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overall.",
                    "label": 0
                },
                {
                    "sent": "So a strategy that we're going to employ is to to have any constraints on the tree part of the model, the red links and then not will have linear constraints on the non tree links in the model.",
                    "label": 0
                },
                {
                    "sent": "This is called a linearly augmented tree.",
                    "label": 1
                },
                {
                    "sent": "And this is described in the publication shown here.",
                    "label": 0
                },
                {
                    "sent": "CPR 2011.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the basic equation, so we've got it.",
                    "label": 0
                },
                {
                    "sent": "We've got an optimization problem and we want to do invariant matching invariant to rotation and scale, for instance.",
                    "label": 1
                },
                {
                    "sent": "And so we've got the unary cost term, which is looking at the matching of a template point.",
                    "label": 0
                },
                {
                    "sent": "To a target point in an image.",
                    "label": 0
                },
                {
                    "sent": "And there's a mapping function F which is describing the mapping from the target to the from the target to the template.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this can be described as a simple linear form using a binary assignment matrix and a local cost matrix.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we have rotation and scale consistency.",
                    "label": 1
                },
                {
                    "sent": "And there are the parameters are rotation and scale of course, and we're looking for consistent mappings of pairs of points from the target.",
                    "label": 0
                },
                {
                    "sent": "It's a template to the target.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here what we can do is we can we can write this term and linearize it as follows.",
                    "label": 0
                },
                {
                    "sent": "We have a pairwise assignment matrix for each pair PQ and in the template in the target.",
                    "label": 1
                },
                {
                    "sent": "We've got these rotation matrices that are written here in terms of the cosine and the sign of the angles.",
                    "label": 0
                },
                {
                    "sent": "And we also linearize in terms of scale.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got this.",
                    "label": 1
                },
                {
                    "sent": "Rotation angle matrix and a scale matrix and some variables as shown here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what this yields is this mixed integer optimization, which is a complex problem to solve in general, but but the special structure is going to help us.",
                    "label": 0
                },
                {
                    "sent": "I want to note that there can also be global terms here and they should be linear or L1 and L1 can be linearized through some auxiliary auxiliary variable techniques, so this subject to some constraints as I'll show in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next slide so we can relax this.",
                    "label": 0
                },
                {
                    "sent": "To solve it and.",
                    "label": 0
                },
                {
                    "sent": "There's our objective function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These are our non tree constraints.",
                    "label": 0
                },
                {
                    "sent": "It looks kind of complex.",
                    "label": 0
                },
                {
                    "sent": "So I'll call.",
                    "label": 0
                },
                {
                    "sent": "Those are hard constraints and as in they are difficult to work with, not in another sense, so that I'll put those in red.",
                    "label": 0
                },
                {
                    "sent": "The remaining constraints and bounds look like tree constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got this problem now where we've got our linearly augmented tree where the augmented constraints are in red.",
                    "label": 0
                },
                {
                    "sent": "And the tree constraints are in green.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem again with the special structure.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "We can actually use the Danzig Wolfe decomposition to solve our problem by using the the solutions for the constraints in the green box as proposals for finding the optimal solution which lives on the combined solution with the hard constraints.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is also known as delayed column generation, where the proposals are being generated using a dynamic programming technique for the tree constraints, which were shown in.",
                    "label": 0
                },
                {
                    "sent": "Green",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are simple, simpler to compute.",
                    "label": 0
                },
                {
                    "sent": "And then what we can do is we use linear combinations of those similar problems to find the solution to the harder problem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm.",
                    "label": 0
                },
                {
                    "sent": "At each step we choose a new proposal that optimizes some game game.",
                    "label": 0
                },
                {
                    "sent": "Sorry, which I'm not going to describe here.",
                    "label": 0
                },
                {
                    "sent": "I just want to give the idea.",
                    "label": 0
                },
                {
                    "sent": "So here again, the key concept is they look at.",
                    "label": 0
                },
                {
                    "sent": "If you choose the right structure for the problem.",
                    "label": 0
                },
                {
                    "sent": "In this case by using linearly augmented trees.",
                    "label": 0
                },
                {
                    "sent": "What you get is initially a problem that looks like it's very difficult to solve, but you recognize this structure.",
                    "label": 0
                },
                {
                    "sent": "In fact, their proposal generation is cheap.",
                    "label": 0
                },
                {
                    "sent": "It can be done via dynamic programming OK.",
                    "label": 1
                },
                {
                    "sent": "So in this paper that I that I cite below.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the example applications of this technique was to use it for matching templates of regions to video sequences where the regions are extracted roughly using a superpixel representation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we've got a template image.",
                    "label": 0
                },
                {
                    "sent": "Here, and this is the original image and the segmentation and the matching note here.",
                    "label": 0
                },
                {
                    "sent": "This is the same template but now later in the video sequence it's rotated OK and here we have another template and a target image in varying scale and rotation.",
                    "label": 0
                },
                {
                    "sent": "OK, so key point is we don't have to quantize.",
                    "label": 0
                },
                {
                    "sent": "Over rotation and scale.",
                    "label": 0
                },
                {
                    "sent": "To find these solutions.",
                    "label": 0
                },
                {
                    "sent": "So here are some example videos and showing the matchings that are obtained and the CPU time in matching per frame in these sequences varies between .02 and .42 seconds per frame on a regular personal computer.",
                    "label": 1
                },
                {
                    "sent": "And the way the experiment was run is so you pick one of the frames as randomly as a template.",
                    "label": 0
                },
                {
                    "sent": "Use the regions from that you select the regions that are relevant to tracking and then you track it forward through the whole sequence OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so some observations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The branch and bound and exploiting the problem structure allowed us to to to do exact post inference with non tree graphs.",
                    "label": 0
                },
                {
                    "sent": "For the majority of cases now I want to emphasize two things, one branch and bound, in practice gave very good performance, but still the worst case complexity is there and occasionally you will get a case which will go toward the worst case of complexity.",
                    "label": 0
                },
                {
                    "sent": "But on average it does much better.",
                    "label": 0
                },
                {
                    "sent": "And in practice, for us, it works quite well.",
                    "label": 0
                },
                {
                    "sent": "The other thing I want to note is that if you're thinking about using a tree based lower bound, you need to think about how well the lower bound cost fits how close the lower bound is, or what how good a lower bound the tree tree cost is for the overall cost function.",
                    "label": 0
                },
                {
                    "sent": "In our problem, we chose the tree based lower bound so that we could take advantage of the distance transform and that further reduce the complexity of computing the lower bound at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So you can think of finding the spanning tree of a.",
                    "label": 0
                },
                {
                    "sent": "Of a non tree graph and using that as the lower bound.",
                    "label": 0
                },
                {
                    "sent": "But you have to be careful in how you choose it, so it's very well so suited for pose estimation.",
                    "label": 0
                },
                {
                    "sent": "Future work will tell us whether it's suited for other applications.",
                    "label": 0
                },
                {
                    "sent": "And linearly augmented trees, scale and rotation invariant matching does not require quantization of the continuous variables and the special structure permits.",
                    "label": 0
                },
                {
                    "sent": "Permits use of the Doncic Wolfe decomposition and dynamic programming for proposal.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rayshon alright, so in in the second set of projects what I'd like to talk about is learning of actions from loose supervision by loose supervision I mean that you're given an image an you're told that there's an action taking place in that image, but you don't know who's doing that action.",
                    "label": 0
                },
                {
                    "sent": "Or where they are, or you're given a video and you're told that there is someone playing basket in that basketball.",
                    "label": 0
                },
                {
                    "sent": "In that video, you don't know who's playing video, where in the video they appear.",
                    "label": 0
                },
                {
                    "sent": "There may be other people present in the video, etc.",
                    "label": 0
                },
                {
                    "sent": "We'll also be interested in learning what scene features are relevant and what object features are relevant in the scene with the action, and we don't know which objects might be relevant, for instance, or maybe a basketball, but there may also be other objects moving around in the scene, and maybe the basketball is relevant, but the others are not.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So really, we're interested in action recognition and uncontrolled videos, and this is becoming more and more popular.",
                    "label": 1
                },
                {
                    "sent": "Moving away from the staged action recognition datasets more towards.",
                    "label": 0
                },
                {
                    "sent": "Uncontrolled video YouTube home movies.",
                    "label": 1
                },
                {
                    "sent": "These these sequences are are typified by.",
                    "label": 1
                },
                {
                    "sent": "They are difficulty moving cameras, cluttered scenes, dynamic backgrounds.",
                    "label": 0
                },
                {
                    "sent": "And there's also great variation in the actions.",
                    "label": 0
                },
                {
                    "sent": "The styles of those actions, the dynamics, the body, proportions of the people performing those actions, the clothing variation, etc.",
                    "label": 0
                },
                {
                    "sent": "And then, on top of that, there are compression artifacts.",
                    "label": 0
                },
                {
                    "sent": "Due to the compression standards used in compressing the video.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work, what we've been doing is we're looking at learning actions from the web, and this has been.",
                    "label": 0
                },
                {
                    "sent": "Internet vision has been quite successfully used in the object recognition communities, so we're taking our inspiration from that.",
                    "label": 1
                },
                {
                    "sent": "And the beauty, of course, is that there are lots of action images on the web.",
                    "label": 1
                },
                {
                    "sent": "We're going to use images of human poses.",
                    "label": 0
                },
                {
                    "sent": "In recognizing actions actions not only in images but also in video.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about these images that they can help capture the diversity of the nature of the actions that were trying to recognize, and they also may reduce the training bias that might be encountered in staged datasets.",
                    "label": 0
                },
                {
                    "sent": "For action recognition, there are uncontrolled poses.",
                    "label": 0
                },
                {
                    "sent": "There are various people, clothing, body proportions, but the same time this data set or if you consider the web, the web imagery presents a challenge due to its diversity.",
                    "label": 1
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "And there are two papers here that I've cited.",
                    "label": 0
                },
                {
                    "sent": "One is from my CV.",
                    "label": 0
                },
                {
                    "sent": "Another is more recently from ACM Multimedia, so here's the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So system overview.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The basic system works as follows.",
                    "label": 0
                },
                {
                    "sent": "We specify a text query.",
                    "label": 0
                },
                {
                    "sent": "To retrieve images from the web.",
                    "label": 0
                },
                {
                    "sent": "Using a standard search engine like Google.",
                    "label": 0
                },
                {
                    "sent": "And we get the first page of the results due to the text processing that's inherent in these search engines and the tuning of the image search engine engine parameters.",
                    "label": 0
                },
                {
                    "sent": "The results have pretty high precision, but still there are images which don't show people running.",
                    "label": 0
                },
                {
                    "sent": "But the first page tends to have pretty high precision, and that many of the images are relevant.",
                    "label": 0
                },
                {
                    "sent": "By relevant, I mean there's at least one person in the picture doing that action.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, the the we find that actually there's about 4040% label noise and some of the retrieval results that we get, and specifying the text keywords that are used in the query turns out to be somewhat critical.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, what we're doing, I'll call it loop loosely supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In that all we do to train the system as we give it some keywords, and then we run some searches on Yahoo and Google another image, search engines and what we get in the first page is a set of images and we run.",
                    "label": 0
                },
                {
                    "sent": "A person detector over those images.",
                    "label": 0
                },
                {
                    "sent": "The felzenszwalb at all person detector will do and for all of the images for which a person is detected.",
                    "label": 0
                },
                {
                    "sent": "We save those images and we get rid of the other images right off the bat.",
                    "label": 0
                },
                {
                    "sent": "And given that first set of images we we do some outlier rejection further by fitting a model to them that I'll describe in a second.",
                    "label": 0
                },
                {
                    "sent": "And then using that model we retrieve the next page of results, which tend to be a bit noisier, and using the posterior of the model that we just estimated we reject images which are below a threshold in terms of the posterior we include the images which are thought to be likely depicting of that action, or consistent with the model we've learned, and we iterate.",
                    "label": 0
                },
                {
                    "sent": "Then, given that clean data set, we we learn a classifier for each of the poses for that action and then use those key poses to label actions in video.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first step is the action image retrieval step.",
                    "label": 1
                },
                {
                    "sent": "And what you see in this slide?",
                    "label": 0
                },
                {
                    "sent": "Are retrieved images for five actions running in the top row.",
                    "label": 1
                },
                {
                    "sent": "Walking sitting.",
                    "label": 0
                },
                {
                    "sent": "Playing golf.",
                    "label": 0
                },
                {
                    "sent": "And dancing.",
                    "label": 0
                },
                {
                    "sent": "The red boxes show outliers.",
                    "label": 1
                },
                {
                    "sent": "So outliers could be a person not engaged in that action.",
                    "label": 0
                },
                {
                    "sent": "It could be not a person.",
                    "label": 0
                },
                {
                    "sent": "'cause sometimes the detector doesn't give correct result.",
                    "label": 0
                },
                {
                    "sent": "And our goal is to remove as many of these as possible.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the image representation that we're using for the windows of the detection we could use what's called a hog descriptor.",
                    "label": 1
                },
                {
                    "sent": "The histogram of oriented oriented gradients.",
                    "label": 0
                },
                {
                    "sent": "And that descriptor is shown here.",
                    "label": 0
                },
                {
                    "sent": "So you compute the gradients and the orientation of the gradients and then a histogram instead.",
                    "label": 0
                },
                {
                    "sent": "We've found that using a probability of boundary image and works better for us, and the reason is that.",
                    "label": 1
                },
                {
                    "sent": "The there is parts of the background lie within the detection window.",
                    "label": 0
                },
                {
                    "sent": "It's a persons here, but there's also a fence in the background, and what we'd like to do is focus on the human in the foreground and less on the background.",
                    "label": 0
                },
                {
                    "sent": "And we found that the probability of boundary measure works somewhat better in reducing the effect of the background within the detection window.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here's the incremental procedure and I'll describe it step by step, just briefly so you run a query.",
                    "label": 0
                },
                {
                    "sent": "You preprocess the retrieved images as I've described.",
                    "label": 0
                },
                {
                    "sent": "We do an alignment of the head positions because the head head positions tend to be somewhat more stable.",
                    "label": 0
                },
                {
                    "sent": "We extract the PB Hog features and then we build an action model and the way we build the action model is we use a logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And one of the insights here is that keeping the model simple and the outlier rejection method simple tends to work better than using a more complicated procedure.",
                    "label": 0
                },
                {
                    "sent": "We've tried many and this seems to work the best.",
                    "label": 0
                },
                {
                    "sent": "And then there's a loop.",
                    "label": 0
                },
                {
                    "sent": "So while more images remain, we compute the posterior.",
                    "label": 0
                },
                {
                    "sent": "We include all of those where the posterior is greater than a threshold.",
                    "label": 0
                },
                {
                    "sent": "We will train the logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And continue.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this can be used to improve the precision of the retrieved results.",
                    "label": 0
                },
                {
                    "sent": "So here we've collected images from Google and Yahoo Image Search and you can see the categories and the number of images that were in the final set.",
                    "label": 1
                },
                {
                    "sent": "The blue bars on this chart showed the precision of just the keyword retrieved result.",
                    "label": 0
                },
                {
                    "sent": "The green bars show the result using our our iterative technique to refine the set, but using hog features and then red shows the result using PB hogs and you can see that over eight of the 11 actions the results are better using PB hogs in improving the precision.",
                    "label": 0
                },
                {
                    "sent": "But there are.",
                    "label": 0
                },
                {
                    "sent": "There are three actions where the.",
                    "label": 0
                },
                {
                    "sent": "The precision is actually slightly.",
                    "label": 0
                },
                {
                    "sent": "Better for hogs, so that would be riding a horse.",
                    "label": 0
                },
                {
                    "sent": "Swinging a Golf Club.",
                    "label": 0
                },
                {
                    "sent": "And jumping.",
                    "label": 0
                },
                {
                    "sent": "And we believe that this might be due to the fact that in those particular images, the background is is a useful feature.",
                    "label": 0
                },
                {
                    "sent": "So after we improve.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The precision of the retrieved data set.",
                    "label": 0
                },
                {
                    "sent": "So we've got one collection of images, let's say for running we apply non negative matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "To find the modes in the data and here are some examples, this is these are five modes for running you could see a side pose three quarter poses front pose, and I think this is roughly a leg up and both legs down.",
                    "label": 1
                },
                {
                    "sent": "So using these, what we're going to do is then train a separate logistic regression classifier for each pose mode, so you can think of these as key poses.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to use these in action recognition in video.",
                    "label": 1
                },
                {
                    "sent": "It's going to be a simple scheme.",
                    "label": 0
                },
                {
                    "sent": "It's tracking by detection.",
                    "label": 0
                },
                {
                    "sent": "So you run a detector over the video sequence that's input.",
                    "label": 0
                },
                {
                    "sent": "And we use simple bounding box tracking where we initialize the tracker on the 1st frame using the detector and then consecutive frames giving given the results of the detection we link them together based on proximity.",
                    "label": 1
                },
                {
                    "sent": "If no track is associated to a new detection, that would create a new track and then on top of that we run our classifier and our classifier works as follows.",
                    "label": 1
                },
                {
                    "sent": "We're going to use a very simple 1st order model where the action is allowed to move from one pose to another pose within the same action with a low cost.",
                    "label": 0
                },
                {
                    "sent": "And you're allowed to move from one action to another action with a higher cost.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple dynamic programming problem and we can solve for the the resulting classification.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an example of a YouTube video from video data set of nebulous at all.",
                    "label": 1
                },
                {
                    "sent": "And the red boxes show incorrect classification.",
                    "label": 0
                },
                {
                    "sent": "And what we find is that some of the poses are shared across actions.",
                    "label": 0
                },
                {
                    "sent": "So if we're dancing, you'll find poses sometimes that look like poses that you would find in walking.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless I this is, uh, I've enlarged this video.",
                    "label": 0
                },
                {
                    "sent": "It looks kind of blurry, and in fact it is.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some other results.",
                    "label": 0
                },
                {
                    "sent": "For this data set and again, the red boxes show some.",
                    "label": 0
                },
                {
                    "sent": "False classifications this is thought to be dancing, I think may because of hands on hip playing golf.",
                    "label": 0
                },
                {
                    "sent": "Walking, dancing.",
                    "label": 0
                },
                {
                    "sent": "Etc So it works pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've extended this, so now let's say I've given you a technique where I've learned poses from images.",
                    "label": 0
                },
                {
                    "sent": "What if I combined web images and web video?",
                    "label": 0
                },
                {
                    "sent": "So let's say I've only got, you know, labeling videos is relatively tedious expensive, so maybe I'll give you a few videos where I tell you, OK, there's a person doing this action, and I. I asked, OK, what happens if I combine the OR I bootstrap?",
                    "label": 0
                },
                {
                    "sent": "I use the images to 1st labeled the first set of videos and then I add more and more videos and as I add videos, how does the performance change?",
                    "label": 0
                },
                {
                    "sent": "So in this experiment what we did was we took the JCF YouTube video data set.",
                    "label": 1
                },
                {
                    "sent": "We grouped the videos into videos that showed similar actions, and we chose at random.",
                    "label": 0
                },
                {
                    "sent": "One example from each one of those eight clusters, and that's where we started, and we kept adding more and more videos.",
                    "label": 0
                },
                {
                    "sent": "So here the green curve is web poses, plus video poses, and So what you see is that when you have only a few videos, you can do quite well by starting with the image based model.",
                    "label": 1
                },
                {
                    "sent": "But as you get more and more videos, eventually the video video based model overtake the hybrid model.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing that we examined was the ordering of the poses.",
                    "label": 0
                },
                {
                    "sent": "The model that I described earlier that was based on the dynamic programming didn't look at the ordering, so we used a simple model that's like a bigram model where.",
                    "label": 0
                },
                {
                    "sent": "You look at the order of 1 pose after another within within the action.",
                    "label": 0
                },
                {
                    "sent": "And as you would expect, this also improves our classification.",
                    "label": 0
                },
                {
                    "sent": "Accuracy, so if you look at the number of training videos in this direction.",
                    "label": 0
                },
                {
                    "sent": "This red curve is web poses plus video poses.",
                    "label": 0
                },
                {
                    "sent": "This blue curve is with the ordered pose pairs so you can see that it improves as you would expect.",
                    "label": 1
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So that was.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One project that looked at using web images and video for training action models in a loosely supervised way and what I'd like to do is briefly describe another approach that we've developed that looks at leveraging scene properties, object properties, and human motion and poses.",
                    "label": 0
                },
                {
                    "sent": "Together to improve the accuracy of action classification.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is a familiar one.",
                    "label": 0
                },
                {
                    "sent": "The context of the video will tell, or of an image will tell us something about what actions we would expect are likely there.",
                    "label": 0
                },
                {
                    "sent": "For instance, we don't expect golf to be played in this image.",
                    "label": 0
                },
                {
                    "sent": "And here we probably wouldn't expect to see swimming.",
                    "label": 0
                },
                {
                    "sent": "And here the presence of a basketball moving in the air might tell us that it's likely that we would observe basketball or it would be unlikely to see swimming.",
                    "label": 0
                },
                {
                    "sent": "Maybe water polo.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the approach that we're going to take is to recognize that single features that we're going to extract in these videos are going to be quite noisy.",
                    "label": 1
                },
                {
                    "sent": "The detections are going to be error prone for object detections.",
                    "label": 0
                },
                {
                    "sent": "We're going to get many moving objects which aren't relevant to an action.",
                    "label": 0
                },
                {
                    "sent": "So given these noisy features, we need an approach which is going to be robust.",
                    "label": 0
                },
                {
                    "sent": "And given many non relevant tracks including other people performing not performing the action of interest, irrelevant objects, parts of the sequence that don't show that action, if you've got a video, that's a clip and part of the clip shows that action, but part of it does not.",
                    "label": 1
                },
                {
                    "sent": "You've got what I'll call a multiple, but can be framed as a multiple instance learning problem.",
                    "label": 0
                },
                {
                    "sent": "So multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "In this case, each video is a bag.",
                    "label": 0
                },
                {
                    "sent": "The instances in the bag include the moving objects, the people in the scene features.",
                    "label": 0
                },
                {
                    "sent": "We don't know which of the instances inside these each of these bags is relevant to the action.",
                    "label": 0
                },
                {
                    "sent": "That appears.",
                    "label": 1
                },
                {
                    "sent": "OK, so I hope people are familiar with multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple concept.",
                    "label": 0
                },
                {
                    "sent": "I'll say very briefly, so let's say I want to teach you how to classify objects and all I do is I give you bags.",
                    "label": 0
                },
                {
                    "sent": "I say this is a positive bag.",
                    "label": 0
                },
                {
                    "sent": "And inside the bag there are some examples of the positive class and also some examples of the negative class.",
                    "label": 0
                },
                {
                    "sent": "But I just say this is a positive bag, one at least one of the items in this bag.",
                    "label": 0
                },
                {
                    "sent": "Is the of the positive class in our case, the bag is a video right?",
                    "label": 0
                },
                {
                    "sent": "Somewhere in this video there's a person performing this action somewhere.",
                    "label": 0
                },
                {
                    "sent": "In this video there may be objects whose motions are relevant to this action, but you don't know which ones there are and the learning process must discover those.",
                    "label": 0
                },
                {
                    "sent": "I give you negative bags and I say in this in this bag there's no instance of this action, so basketball does not occur here.",
                    "label": 0
                },
                {
                    "sent": "So given a collection of these bags, your job is to learn.",
                    "label": 0
                },
                {
                    "sent": "For the positive bags, which instances are relevant?",
                    "label": 0
                },
                {
                    "sent": "Which body poses are relevant?",
                    "label": 0
                },
                {
                    "sent": "Which scene features are relevant?",
                    "label": 0
                },
                {
                    "sent": "Which objects are relevant?",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about this is that some of the detections can be wrong, and those instances we hope will be filtered out as a process as part of the learning process.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an overview with pictures.",
                    "label": 0
                },
                {
                    "sent": "First, we stabilize an input video using a basic technique that stabilizes the video.",
                    "label": 0
                },
                {
                    "sent": "Then we extract the moving object tracks.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We also extract the person tracks by using a person detector, much like I described earlier.",
                    "label": 0
                },
                {
                    "sent": "And then what we've got are these multiple channels.",
                    "label": 0
                },
                {
                    "sent": "We have candidate objects.",
                    "label": 0
                },
                {
                    "sent": "Candidate person detections.",
                    "label": 1
                },
                {
                    "sent": "And we also sample features of the scene at randomly selected sets of frames within the video, OK?",
                    "label": 0
                },
                {
                    "sent": "So these are our bags.",
                    "label": 0
                },
                {
                    "sent": "We have bags across multiple channels.",
                    "label": 0
                },
                {
                    "sent": "And then within those channels we compute multiple features, for instance object hogs.",
                    "label": 0
                },
                {
                    "sent": "Object upflow for the moving objects.",
                    "label": 0
                },
                {
                    "sent": "Person Hogson person optic flow for the person detection's.",
                    "label": 0
                },
                {
                    "sent": "Just features and also color features for the scene.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So using this approach, I'm not going to describe the mathematical formulation, but the mathematical formulation in brief, learns the weight relative weighting of those feature channels for each of the actions, and also can find out or can discover which instances are relevant in the training examples for each of the actions, and most important for discriminating between pairs of action classes.",
                    "label": 0
                },
                {
                    "sent": "So we'll evaluate our system using the UC F YouTube data set with 11 actions as shown and will use leave one out cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a big chart.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is focus your attention.",
                    "label": 0
                },
                {
                    "sent": "So focus your attention on the Gray area.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The combination of features is what we're most interested in.",
                    "label": 0
                },
                {
                    "sent": "So in this table, what you see across the top are the action classes present in the data set, and then these labels are just the individual feature channels.",
                    "label": 0
                },
                {
                    "sent": "And if I were to develop a classifier that just use that particular feature channel, what accuracy would I get in the classification of those actions in that data set?",
                    "label": 0
                },
                {
                    "sent": "And so the best the best feature on average.",
                    "label": 0
                },
                {
                    "sent": "If you use it just alone, is the gist feature, which is a scene.",
                    "label": 0
                },
                {
                    "sent": "Feature for scene context and what this is saying is that for the UC F data set, there's actually seen features.",
                    "label": 0
                },
                {
                    "sent": "A scene features alone can get you pretty good classification of the actions.",
                    "label": 0
                },
                {
                    "sent": "Then if you look at the overall best performance, what we get is by combining the features using the multiple instance learning technique, we do better significantly better on average.",
                    "label": 0
                },
                {
                    "sent": "OK. We compared against a work by Lou at all and we also do better than they did on this data set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some visual examples.",
                    "label": 1
                },
                {
                    "sent": "The interesting thing is to see for the cases that it gets right.",
                    "label": 0
                },
                {
                    "sent": "So T means the test and C means the classification.",
                    "label": 0
                },
                {
                    "sent": "So here the test case was diving and the class that it was assigned was diving and the system found the blue is shows the object that it thought it was relevant and the green is the person detection that it thought was relevant.",
                    "label": 0
                },
                {
                    "sent": "So it's both classifying and saying which objects and elements of the scene are relevant.",
                    "label": 0
                },
                {
                    "sent": "And for other examples tennis, it detects a racket.",
                    "label": 0
                },
                {
                    "sent": "For golf, it detects a person at some part of the Golf Club.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And there are also cases where it doesn't do the right thing, so here's an example where.",
                    "label": 0
                },
                {
                    "sent": "I can't, I can't read that.",
                    "label": 0
                },
                {
                    "sent": "Basketball, I hope you can read it so.",
                    "label": 0
                },
                {
                    "sent": "Basketball shooting.",
                    "label": 0
                },
                {
                    "sent": "So it detected the relevant people, but it classified it as a volleyball spike, which is not not that bad, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about where volleyball and basketball are played and the kinds of moves you might find.",
                    "label": 0
                },
                {
                    "sent": "But there are also others, which, for instance here we have a bike and here we have.",
                    "label": 0
                },
                {
                    "sent": "It's classified as walking and the pose of the person looks a little bit like walking, but still it's biking.",
                    "label": 0
                },
                {
                    "sent": "So it got it wrong.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here are some observations about the work.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've just described.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about learning actions from the web, but is that the action classes are easily extensible, you can just run a text based query and extend your model right with a new action.",
                    "label": 0
                },
                {
                    "sent": "Data is plentiful, plentiful, diverse, but noisy.",
                    "label": 0
                },
                {
                    "sent": "Using the multiple instance learning approach it.",
                    "label": 0
                },
                {
                    "sent": "It was possible to leverage scene objects and people features in the scene despite somewhat noisy tracking and detection.",
                    "label": 0
                },
                {
                    "sent": "And it also selected the relevant features for the actions that we were classifying.",
                    "label": 0
                },
                {
                    "sent": "So finally the third part.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk and I'll be brief here.",
                    "label": 0
                },
                {
                    "sent": "With the sign language stuff I is communication and there's a longstanding collaboration that I've had it be you with Carol Neidle and her group.",
                    "label": 0
                },
                {
                    "sent": "Carol is a linguist at a specialist in American Sign Language.",
                    "label": 1
                },
                {
                    "sent": "This collaboration that's been going on for about 10 years and.",
                    "label": 0
                },
                {
                    "sent": "There's also a graduate student of Mindless Vassilis defeat sauces now an associate professor at University of Texas was a collaborator on this project for a long time, so I wanted knowledge.",
                    "label": 0
                },
                {
                    "sent": "Their work and contributions in this effort.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our current our current effort is focusing on the following problem.",
                    "label": 0
                },
                {
                    "sent": "How to look up a sign, let's say in an ASL dictionary, or on a sign language YouTube channel.",
                    "label": 1
                },
                {
                    "sent": "I buy using.",
                    "label": 0
                },
                {
                    "sent": "Your native language.",
                    "label": 0
                },
                {
                    "sent": "Sign language, let's say or.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're learning sign language and you want to look up a sign.",
                    "label": 0
                },
                {
                    "sent": "You pick the relevant piece of clip and you say the sign occurs.",
                    "label": 0
                },
                {
                    "sent": "Here you submit it to the search tool and then it returns the result which is.",
                    "label": 0
                },
                {
                    "sent": "A ranked set of of retrieved sign.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lines where the most similar appears first.",
                    "label": 0
                },
                {
                    "sent": "I don't have a live demo unfortunately, so this is work in progress.",
                    "label": 0
                },
                {
                    "sent": "So let's say I submit an input sign, and then it retrieves some results based on the similarity to that sign and presents some other information.",
                    "label": 0
                },
                {
                    "sent": "So this in this case these are entries from lexecon of signs, but it could also be examples of videos that are present in some collection of videos that you've indexed.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a major part of this effort has been data collection.",
                    "label": 0
                },
                {
                    "sent": "This is a data driven, an linguistics driven project.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The data collection that we've engaged in for this project involves working with native signers in a video capture lab.",
                    "label": 0
                },
                {
                    "sent": "And collecting over 3000 signs that are present in the Gallaudet Dictionary and also adding additional signs to that collection and the elicitation was led by our linguistics collaborator Carol Neidle.",
                    "label": 1
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this data set, which will soon be made available, is pretty high quality, high resolution both in space and time.",
                    "label": 0
                },
                {
                    "sent": "So it's 60 frames per second for some of these videos, and it also includes bounding boxes for the faces and hands for some of the video clips in the data set.",
                    "label": 0
                },
                {
                    "sent": "The linguistics team has been annotating this data, and this is a quite involved process that, as a result of labeling the start and the end handshapes in each of the signs that are so.",
                    "label": 1
                },
                {
                    "sent": "Imagine going through a video labeling the start and the end point of every sign.",
                    "label": 1
                },
                {
                    "sent": "The start and the end handshape of every sign labeling other relevant features.",
                    "label": 0
                },
                {
                    "sent": "This is a process that actually sometimes exposes interesting questions about the nature of sign language and also raises questions about exactly which hand shape is being used and what you find is that there's variation, just natural variation, just like there's natural variation in pronunciation in spoken speech, there's pronunciation variation in sign language.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of the data set includes labeling the handshapes, and here you see the handshape set classes that are used in the data set.",
                    "label": 0
                },
                {
                    "sent": "And in labeling, this is time consuming.",
                    "label": 0
                },
                {
                    "sent": "And also it needs to be verified before it's released.",
                    "label": 0
                },
                {
                    "sent": "That's why it's taking us awhile beef.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or it's available?",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're exploiting this data set in some of the work related to the retrieval, and I'll tell you how that works, so we're.",
                    "label": 0
                },
                {
                    "sent": "We're looking at ways to constrain the retrieved set that I showed.",
                    "label": 0
                },
                {
                    "sent": "You have relevant signs that you get back based on the start and the end handshapes, and also what we'd like to do is improve our hand shape recognition by leveraging the training, leveraging the data that we've collected as a training set.",
                    "label": 1
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an overview of what I mean.",
                    "label": 0
                },
                {
                    "sent": "So, given the beginning of a sign at the end of the sign and hand detections which are found.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to classify the hand shape and the hand shape.",
                    "label": 0
                },
                {
                    "sent": "Of course is is 1 relevant feature of recognizing assign hand orientation is another in hand motion as another.",
                    "label": 0
                },
                {
                    "sent": "So these are features of signs that we need to consider in order to be able to do accurate sign language retrieval.",
                    "label": 0
                },
                {
                    "sent": "So, given the the input is the start and end frame, we want to find the nearest neighbor handshapes from the training data set that we've got.",
                    "label": 0
                },
                {
                    "sent": "And then what we use is what we call a hand shape inference or handshape Bayesian network HBN.",
                    "label": 1
                },
                {
                    "sent": "And the the HBN is structured as follows, the lowest level it's shown here for one handed signs where you have a start hand shape and an end hand shape, and you have the observed produced hand shape.",
                    "label": 0
                },
                {
                    "sent": "So here are the image features.",
                    "label": 0
                },
                {
                    "sent": "The lowest node, of course is the image features.",
                    "label": 0
                },
                {
                    "sent": "The blue note.",
                    "label": 0
                },
                {
                    "sent": "The middle node is the produced handshapes.",
                    "label": 0
                },
                {
                    "sent": "The this higher node here is an observed and that would be there's some underlying process where their natural variations in the hand shapes that are actually produced.",
                    "label": 0
                },
                {
                    "sent": "These are equivalent to allophones and speech in some ways, so you have just natural variation in the hand shapes that are produced.",
                    "label": 0
                },
                {
                    "sent": "These these actually variations are follow our general and are occur across signs.",
                    "label": 0
                },
                {
                    "sent": "It's not assigned specific phenomenon, there are certain.",
                    "label": 0
                },
                {
                    "sent": "Variations that occur, and there's subsets of of shapes that that can vary.",
                    "label": 0
                },
                {
                    "sent": "Together so we want to exploit that we're going to learn that from the data set and build it into our model.",
                    "label": 0
                },
                {
                    "sent": "Here I show the model for one handed signs.",
                    "label": 0
                },
                {
                    "sent": "There are models for two handed symmetric signs like book and two handed nonsymmetric signs like assigned to give OK, so that's the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've used this model in.",
                    "label": 0
                },
                {
                    "sent": "Hand held shape classification or given a training set which is different a different signer than the training set is different signers than the test set.",
                    "label": 1
                },
                {
                    "sent": "So it's signer independent and.",
                    "label": 1
                },
                {
                    "sent": "We're given signs for model evaluation that are taken from our data set as follows, and as you would expect by using the model for inference, we get better accuracy in classifying the handshapes.",
                    "label": 0
                },
                {
                    "sent": "So this is just one subsystem within the retrieval system that I described earlier.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a completely different project, this is future work.",
                    "label": 0
                },
                {
                    "sent": "I'm almost I'm almost done so.",
                    "label": 0
                },
                {
                    "sent": "In a completely different project, unrelated to the sign Language project that I just described, we're looking at children who are learning sign language in sessions with.",
                    "label": 0
                },
                {
                    "sent": "With an adult where these are children who have have delays or communication problems and they need to learn sign language in order to communicate their what they know.",
                    "label": 0
                },
                {
                    "sent": "For instance that this child wants popcorn.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "A person here who's showing the child.",
                    "label": 0
                },
                {
                    "sent": "That sign as an example and then helping the child.",
                    "label": 0
                },
                {
                    "sent": "And now she's showing the child the sign for cracker.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's around handshake cracker, so I don't speak sign language unfortunately, so the basic idea though is that.",
                    "label": 0
                },
                {
                    "sent": "We've got a session here with a child, and you've got a person touching the hands of the child, showing them how to form the right hand shape.",
                    "label": 0
                },
                {
                    "sent": "So we want to make sure that the hand shape is correct.",
                    "label": 0
                },
                {
                    "sent": "We want to make sure that the shape is correct.",
                    "label": 0
                },
                {
                    "sent": "And as part of this session, what's happening is that the child gets a reward of the popcorn when the sign is produced, and by repeatedly performing this training overtime, the signing will improve, and what we want to be able to do is to measure that improvement overtime automatically.",
                    "label": 0
                },
                {
                    "sent": "'cause this is a very.",
                    "label": 0
                },
                {
                    "sent": "Time consuming process an it happens there sessions multiple times per week and from session to session.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to pick up where you left off and continue what's called the behavior shaping which is providing moving the target so as the child gets closer and closer to the target sign that you withhold the reward until they get a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So this is part.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A larger project.",
                    "label": 0
                },
                {
                    "sent": "The data that I showed you was collected at Georgia Tech, which is the lead in this collaboration.",
                    "label": 0
                },
                {
                    "sent": "It's a multi institutional project that's looking very broadly at computational methods for analyzing face to face communication.",
                    "label": 1
                },
                {
                    "sent": "And it's looking at speech.",
                    "label": 0
                },
                {
                    "sent": "It's looking at autonomic signals.",
                    "label": 0
                },
                {
                    "sent": "First person, computer vision.",
                    "label": 0
                },
                {
                    "sent": "That what I just showed you is just a video that was collected for the sign language part of the project.",
                    "label": 0
                },
                {
                    "sent": "There's much more going on in this project and the target.",
                    "label": 0
                },
                {
                    "sent": "Application area that we're looking at is supporting the diagnosis and treatment of autism and other behavioral developmental disorders, but really, the tools that are being developed will be could be useful in many other many other disciplines, such as education, advertising, customer relations, and what we're looking to do is to develop quantitative tools that enable analysis of human behavior.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some observations about the sign language work, data driven and linguistics driven methods are promising and I want to emphasize that sign language recognition in unconstrained settings is remains challenging.",
                    "label": 0
                },
                {
                    "sent": "I'll leave it at that.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So to recap, I told you about these three projects.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I want to thank my collaborators who without them especially being a Department chair without collaborators, I'd be subbed, but generally speaking, I'm indebted to the collaborators here, particularly for the linguistics work.",
                    "label": 0
                },
                {
                    "sent": "Carol Neidle an for the search engine work of Bacillus thit sauce and nasli for the action recognition work an how and TN for the work on the on the inference of pose.",
                    "label": 0
                },
                {
                    "sent": "And matching.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}