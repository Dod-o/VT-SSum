{
    "id": "agcexm3jxuyey7merfczcajxrbyr6c4r",
    "title": "Domain Adaptation for Ontology Localization",
    "info": {
        "author": [
            "John Philip McCrae, National University of Ireland (NUI) Galway"
        ],
        "published": "Nov. 10, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2016_mccrae_domain_adaptation/",
    "segmentation": [
        [
            "Hi, thank you so this paper originates from a project called Money but was in FP 7 project finished.",
            "There are few years back.",
            "I also will note that the second often is paper.",
            "Mallorcan is also the author of the 2nd paper here and many of the ideas that were in this paper were further developed in the 2nd paper.",
            "So there'll be some overlapping content between these two paper."
        ],
        [
            "So the semantic web.",
            "Is Nova multilingual in general?",
            "Most of the ontologies that we use, like Owl and so forth.",
            "I only really available in English, but this is slowly changing and this study here by Gomez pillars and old shows that it is slowly increasing, but we still a long way off achieving a dream where a sort of everybody is able to access the semantic web in their own language."
        ],
        [
            "So to do this, I think we need to kind of work on this idea of ontology localization.",
            "And so I'm told you localization had sort of two meanings that are kind of a little distinct.",
            "So the first meeting is to do this idea, but different cultures.",
            "I could have very different concepts, but the ontology of a Spanish speaker is not the same as the ontology of an English speaker.",
            "There's been some work where people take different ontologies in different languages and try to align them, so this is actually not what this paper is about.",
            "This paper takes a different view of ontologies.",
            "The idea is that the ontology is defined and it is fixed and it is Inter lingual, and all we really need to do is translate the labels.",
            "So we just take the label speech concept and we bring we somehow lexicalized them in a new language, and then this ontology is translated.",
            "So obviously translation is translation manually is time intensive and costly.",
            "But when we take standard Mt like Google Translate and Microsoft, we find it has quite poor performance.",
            "So in this paper we focused on the translation of domain ontologies.",
            "So Domain ontology, we mean small focused ontology.",
            "When we talk about large general purpose ontology pedia, which covered well everything.",
            "Then to apply the techniques in this paper you have to somehow sub divide them into smaller ontologies.",
            "We talk about small ontologies on, particularly means like finance or bio medicine for example."
        ],
        [
            "So why is ontology translation so hard?",
            "So normally when you try to put on an ontology into machine translation system, you put each label in, so you."
        ],
        [
            "Add a label like vessel so these labels are very short and vessel is very hard to translate.",
            "So if we were to translate to integer."
        ],
        [
            "Chinese, for example, we might want to translate it as funny meaning."
        ],
        [
            "Ship or we might want to translate it as CAD Cam which is the bio medical term of blood vessel.",
            "And if you just saw this label, you would be no better than a machine would have no clue which translation you should prefer.",
            "So to understand which translation you want, you have to understand the context in the ontology."
        ],
        [
            "So example, if vessel was divided into artery and vein, you might choose the second translation."
        ],
        [
            "But if it was cruiser, an aircraft carrier, then you might choose the first translation."
        ],
        [
            "So we call this idea the domain of the ontology.",
            "We have to understand the domain of the ontology in order to understand how to translate these short, isolated terms."
        ],
        [
            "So we developed kind of a free pronged attack.",
            "So first of all, we looked at trying to find knew translation, so trying to find translations that are in the domain which maybe Google in all of its searching hasn't found and we call this the main lexicon approach.",
            "We also tried to change how the translation system works to encourage it to prefer translations, but are in domain which we called the main feature generation.",
            "And then finally we do post selection where we look at the translations that are coming out of the system and try to choose this.",
            "And this is based on a domain language model."
        ],
        [
            "So I'll spend a few minutes few slides here trying to explain how machine translation works.",
            "So when Pinky Lewis is the phrase based machine translation which is being called the workhorse of machine translation, it was developed from the early 2000s.",
            "It is the model that is used by Google Translator, Microsoft Translator.",
            "Up until at least very recently.",
            "I think they're still partly using it.",
            "And the basic idea is that this is translation on the sentence level.",
            "So you take a phone sentence or so in the source language.",
            "If you call F and you want to generate a translation T. And the way to phrase based model works is you start off.",
            "You divide your source sentences into a sequence of phrases.",
            "So phases in the context of machine translation just mean one or more consecutive words don't actually have to follow linguistic phrase boundaries.",
            "So you divide your sentence into sets of phrases and then you look up in a big big database called a phrase table, or the possible translations, and then you permute those into some order so you can switch these phrases around and you joined him together, and then you get your translation."
        ],
        [
            "So this is optimized according to what's called a log linear model.",
            "So this is essentially a combination of linear factors.",
            "So you have a group of linear factors which are coming from each individual phrase translation, and then you have two factors that are on the overall sentence.",
            "So you have the language model translation which only cares about the actual result, so it doesn't care what the input is, and you have a distortion model.",
            "So the features that are used are generally to do with log probabilities and these are calculated from large corpora of existing translation or parallel corpora.",
            "And they include various probabilities, and they also include, if the translation is out of vocabulary.",
            "So if you have a word as you've never seen before, like say, Portopia hotel, then you might want to call a subsystem to transliterate it into your script, and then you mark this.",
            "And a constant value of 1, which encourages a system to try to use this bigger phase as possible.",
            "We then have a feature that is coming from the language model.",
            "This estimates only looking at the translation where the translation is good and this tends to generate translations of a fluent and at least sound good in the target language."
        ],
        [
            "Finally, we have the distortion score and exhaust distortion score embeds this assumption that you have in machine translation, but you shouldn't move the words around a lot so that the words are generally following this roughly the same order in the translation language.",
            "And so when you put this into a decoder, decoder is a big heuristic algorithm, but attempts to find this division into phrases attempts to find the best translation of each of these phrases, and the best permutation, which is of course is a computationally very expensive task.",
            "But the simple idea is you'll have an input translation like blood group antigen and then you'll get a translation, say into Spanish."
        ],
        [
            "So for all of our experiments, we used the Moses system the most system is sort of the the gold standard system.",
            "It still uses the baseline by most papers.",
            "And is an open source system.",
            "It anyone can use?"
        ],
        [
            "So in our architecture, we attempted to modify Moses by introducing this four step pipeline.",
            "We do pre processing and then our first promise will add new sources.",
            "So we add sources from Wikipedia from a site called Linguee an from ya TE which is a European terminology.",
            "We invent other extra domain specific features coming from avanca and then would give the decoder modified language model and then this would then work as ontology translation system where we could put our phone sentence, hear the English Minimum Finance lease payment and get our translation at the bottom into German."
        ],
        [
            "So did the main lexecon realizing this factor?",
            "The parallel text might not have good translations of the words, or maybe even the multi word units that are particularly this domain.",
            "So the idea is we want to try to find additional data that is good translations in the domain.",
            "So the first one used Wikipedia.",
            "So the idea of Wikipedia approach is what we would do is we would look up in ontology.",
            "We try to find all Wikipedia articles that had names that match labels and ontologies.",
            "And then for E for every matching article in Wikipedia we look at which categories it has.",
            "And then we look at the that we look at the categories and we say which categories correspond to many articles and we do a filter on nose, so we'd end up with sort of categories, and then we then expand the translation by taking over the article in that category and adding it's translation from DB pedia.",
            "So this allows us to get some translations that were more partial matches that we wouldn't get otherwise."
        ],
        [
            "So for example, in with a financial ontology, we saw that we got categories like economics terminology during accepted accounting principle of macroeconomics.",
            "So this was very effective at finding which categories, at least in Wikipedia, were relevant and then therefore, which translations from the pedia we could use and could guarantee were in domain."
        ],
        [
            "Secondly, we work with linguee.",
            "Linguee is a Pty crawl of parallel data from the web.",
            "It's searchable but you can't download it all.",
            "And what we did was we just created for the ontology labels that we had and this generates a very small parallel corpus.",
            "So this is about 24,000 sentences.",
            "In comparison, our baseline system is trained on 100 million sentences, so this was a very small.",
            "Corpus, but it was a very accurate corpus.",
            "So for finance 24,000 sentences is quite effective.",
            "And then this would be treated just like a normal parallel corpus will be trained using Moses.",
            "An applied this way.",
            "I finally we use Yayati was a terminology.",
            "It covers many terms.",
            "One of the problems you have is including a terminology like Yachty is that you don't know how to do the features 'cause you don't have the corpus data to back it up.",
            "So we experimented using the explicit semantic analysis system to weigh trees values."
        ],
        [
            "We also use the explicit semantic analysis system, which was introduced originally by Kabila, Vichten Markovich and then was extended to a multilingual setting.",
            "I'm easy as a domain feature, so this means that when the code is actually translating, there are features that are coming in from the ESA.",
            "did I going to estimate in this particular domain how good the translation is?",
            "So the basic idea of the essay is that you take something you take a Wikipedia.",
            "You build an index of it.",
            "So you in your Lucene or whatever.",
            "And then when you want to know how similar two phrases are, so, for example, you might have two elements T&FI from our phase table.",
            "What we'll do is we'll count how many Wikipedia articles contain these terms, and this gives us a vector weighted by how frequently these terms occur in Wikipedia articles.",
            "And then you do a vector comparison and then the result is we get a comparison single value saying how likely these two terms are.",
            "In the domain, and this then is used to.",
            "Neither feature score."
        ],
        [
            "And the domain adoption here is done again by simply selecting the Wikipedia articles that are in domain according to the categories."
        ],
        [
            "Define the approach was based on using a language model, so the idea of a language model is that you estimate the likelihood of a sentence and the idea is the district prefer fluent centers and should prefer in the main translations.",
            "So this is based on Markov assumption, so this is the formal version."
        ],
        [
            "Here's a slightly easier way to understand it that if you want to know how likely the string welcome to Kobe is all you do is you calculate the probability of seeing the world welcome and the probability of the word 2 coming after welcome and affordability of gender word, Kobe coming after 2 words.",
            "Welcome to.",
            "And the only way to calculate the probability of Kobe coming up to welcome to is very simple.",
            "You get big corpus which only has to be monolingual, and then you count how many times do you see welcome to Kobe and you divide that by how many times do you see welcome to anything.",
            "So any word.",
            "So that gives you a count of how likely you are to see Kobe after the two words, welcome to."
        ],
        [
            "We slightly modified this so we took our big court.",
            "I'll be corpus and we use the arnetta method.",
            "I introduced a couple of years back to estimate the similarity of a corpus to an ontology, so this meant for every single document in our corpus we had a score between zero and one that rated whether this document was relevant to the domain of the ontology.",
            "And then we use this to modified account.",
            "So essentially account was weighted by how likely oneta thought it was to be in domain.",
            "So how likely the document was to be in domain and then we the count was appropriately rated?"
        ],
        [
            "So we evaluated this on many metrics, so the blur metric is sort of seen as the standard in machine translation.",
            "It's kind of the only way you can publish a paper in machine translation is to provide blur scores.",
            "On the other hand, you ask nearly everyone of the machine translation conference, and I'll tell you blow is awful, particularly in this case of ontology translation.",
            "We've actually shown to blur is even worse than normal photology translation, and this is to do a particular floor it has.",
            "In dealing with very short labels.",
            "And we introduced a method called Blur 2, which is a modification of blood.",
            "It works better for ontology translation.",
            "But also to show we have general improvement, we use a variety of metrics that have come from different evaluations.",
            "We evaluated then on two domains, so five ontologies across two domains.",
            "Two of these ontologies were from finance and they were in English, Dutch, German and Spanish and they were quite big and these are open and available and then we had three ontologies that came from public services.",
            "This is actually I think, provided by the Dutch Ministry of in of Immigration.",
            "And these resource is unfortunately not open, so we can't provide them for comparison."
        ],
        [
            "So the results show that our baseline translation was getting blur scores that were about .1.",
            "Now if you know machine to machine translation scores of .1 for blur is pretty terrible.",
            "This is in part to do with the fact that blur is actually very poor to valuating ontology translation, so Blur 2 scores are much higher, but there's still only about 25 with the baseline system which is still not exactly a great translation.",
            "So when we included the lexical methods, we see a significant improvement in all metrics.",
            "But actually when we looked at the domain features and the domain language model, so the 2nd two prongs of attack.",
            "But it didn't seem to work.",
            "Actually they didn't really move any of these scores a lot.",
            "The language model improved it a little bit, but only very slightly.",
            "But then what we tried, we actually put the whole system together and we were glad to see if it actually hvizdos still improve.",
            "So we actually get significant improvements by combining the domain features and a domain language model with the domain lexicon construction.",
            "I mean, this is improvements both over election by itself and over the base."
        ],
        [
            "So what this says is it the strongest improvement is really provided by creating a domain lexecon and this is a result that May Larkin has gone to prove in other papers such as the one you saw there today and another paper that was it ACL last year.",
            "You have methods by themselves aren't particularly great, but combine them together does seem to in most cases, improve.",
            "There were a few cases where it didn't, and this was mostly actually translating into Dutch for the public services use case.",
            "But this result was also marked by the fact that the domain adaptation for the financial services was much less effective.",
            "Or sorry for the public services, was much less effective than for the financial services.",
            "And if you look at we're looking at the ontologies is fairly obvious that the pub, the financial ontologies are using very specific terms that are layperson wouldn't understand where is the immigration domain were sort of the same things you would read in newspapers.",
            "So there wasn't as much of a domain bias in the public services case."
        ],
        [
            "So to conclude, domain Lexicon can significantly improve translation.",
            "It is capable of suggesting you translations, but it does also have the issue that you require a lot of parallel text to make it work best.",
            "So particularly in the Linguee case domain selection.",
            "Is weaker, but it can still be useful, and I think there's still some research to do into improving this, so all the code is available on GitHub and he saw again Second dog.",
            "Again, there's this demo here, which I recommend you check out, which is an ontology translation system that came out of this project.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, thank you so this paper originates from a project called Money but was in FP 7 project finished.",
                    "label": 0
                },
                {
                    "sent": "There are few years back.",
                    "label": 0
                },
                {
                    "sent": "I also will note that the second often is paper.",
                    "label": 0
                },
                {
                    "sent": "Mallorcan is also the author of the 2nd paper here and many of the ideas that were in this paper were further developed in the 2nd paper.",
                    "label": 0
                },
                {
                    "sent": "So there'll be some overlapping content between these two paper.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the semantic web.",
                    "label": 0
                },
                {
                    "sent": "Is Nova multilingual in general?",
                    "label": 0
                },
                {
                    "sent": "Most of the ontologies that we use, like Owl and so forth.",
                    "label": 0
                },
                {
                    "sent": "I only really available in English, but this is slowly changing and this study here by Gomez pillars and old shows that it is slowly increasing, but we still a long way off achieving a dream where a sort of everybody is able to access the semantic web in their own language.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do this, I think we need to kind of work on this idea of ontology localization.",
                    "label": 0
                },
                {
                    "sent": "And so I'm told you localization had sort of two meanings that are kind of a little distinct.",
                    "label": 0
                },
                {
                    "sent": "So the first meeting is to do this idea, but different cultures.",
                    "label": 0
                },
                {
                    "sent": "I could have very different concepts, but the ontology of a Spanish speaker is not the same as the ontology of an English speaker.",
                    "label": 0
                },
                {
                    "sent": "There's been some work where people take different ontologies in different languages and try to align them, so this is actually not what this paper is about.",
                    "label": 0
                },
                {
                    "sent": "This paper takes a different view of ontologies.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the ontology is defined and it is fixed and it is Inter lingual, and all we really need to do is translate the labels.",
                    "label": 0
                },
                {
                    "sent": "So we just take the label speech concept and we bring we somehow lexicalized them in a new language, and then this ontology is translated.",
                    "label": 0
                },
                {
                    "sent": "So obviously translation is translation manually is time intensive and costly.",
                    "label": 1
                },
                {
                    "sent": "But when we take standard Mt like Google Translate and Microsoft, we find it has quite poor performance.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we focused on the translation of domain ontologies.",
                    "label": 1
                },
                {
                    "sent": "So Domain ontology, we mean small focused ontology.",
                    "label": 0
                },
                {
                    "sent": "When we talk about large general purpose ontology pedia, which covered well everything.",
                    "label": 0
                },
                {
                    "sent": "Then to apply the techniques in this paper you have to somehow sub divide them into smaller ontologies.",
                    "label": 0
                },
                {
                    "sent": "We talk about small ontologies on, particularly means like finance or bio medicine for example.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why is ontology translation so hard?",
                    "label": 0
                },
                {
                    "sent": "So normally when you try to put on an ontology into machine translation system, you put each label in, so you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add a label like vessel so these labels are very short and vessel is very hard to translate.",
                    "label": 0
                },
                {
                    "sent": "So if we were to translate to integer.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chinese, for example, we might want to translate it as funny meaning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ship or we might want to translate it as CAD Cam which is the bio medical term of blood vessel.",
                    "label": 0
                },
                {
                    "sent": "And if you just saw this label, you would be no better than a machine would have no clue which translation you should prefer.",
                    "label": 0
                },
                {
                    "sent": "So to understand which translation you want, you have to understand the context in the ontology.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So example, if vessel was divided into artery and vein, you might choose the second translation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if it was cruiser, an aircraft carrier, then you might choose the first translation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we call this idea the domain of the ontology.",
                    "label": 0
                },
                {
                    "sent": "We have to understand the domain of the ontology in order to understand how to translate these short, isolated terms.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we developed kind of a free pronged attack.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we looked at trying to find knew translation, so trying to find translations that are in the domain which maybe Google in all of its searching hasn't found and we call this the main lexicon approach.",
                    "label": 0
                },
                {
                    "sent": "We also tried to change how the translation system works to encourage it to prefer translations, but are in domain which we called the main feature generation.",
                    "label": 0
                },
                {
                    "sent": "And then finally we do post selection where we look at the translations that are coming out of the system and try to choose this.",
                    "label": 0
                },
                {
                    "sent": "And this is based on a domain language model.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll spend a few minutes few slides here trying to explain how machine translation works.",
                    "label": 0
                },
                {
                    "sent": "So when Pinky Lewis is the phrase based machine translation which is being called the workhorse of machine translation, it was developed from the early 2000s.",
                    "label": 1
                },
                {
                    "sent": "It is the model that is used by Google Translator, Microsoft Translator.",
                    "label": 0
                },
                {
                    "sent": "Up until at least very recently.",
                    "label": 0
                },
                {
                    "sent": "I think they're still partly using it.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is that this is translation on the sentence level.",
                    "label": 0
                },
                {
                    "sent": "So you take a phone sentence or so in the source language.",
                    "label": 1
                },
                {
                    "sent": "If you call F and you want to generate a translation T. And the way to phrase based model works is you start off.",
                    "label": 1
                },
                {
                    "sent": "You divide your source sentences into a sequence of phrases.",
                    "label": 0
                },
                {
                    "sent": "So phases in the context of machine translation just mean one or more consecutive words don't actually have to follow linguistic phrase boundaries.",
                    "label": 0
                },
                {
                    "sent": "So you divide your sentence into sets of phrases and then you look up in a big big database called a phrase table, or the possible translations, and then you permute those into some order so you can switch these phrases around and you joined him together, and then you get your translation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is optimized according to what's called a log linear model.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially a combination of linear factors.",
                    "label": 0
                },
                {
                    "sent": "So you have a group of linear factors which are coming from each individual phrase translation, and then you have two factors that are on the overall sentence.",
                    "label": 0
                },
                {
                    "sent": "So you have the language model translation which only cares about the actual result, so it doesn't care what the input is, and you have a distortion model.",
                    "label": 0
                },
                {
                    "sent": "So the features that are used are generally to do with log probabilities and these are calculated from large corpora of existing translation or parallel corpora.",
                    "label": 0
                },
                {
                    "sent": "And they include various probabilities, and they also include, if the translation is out of vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So if you have a word as you've never seen before, like say, Portopia hotel, then you might want to call a subsystem to transliterate it into your script, and then you mark this.",
                    "label": 0
                },
                {
                    "sent": "And a constant value of 1, which encourages a system to try to use this bigger phase as possible.",
                    "label": 0
                },
                {
                    "sent": "We then have a feature that is coming from the language model.",
                    "label": 0
                },
                {
                    "sent": "This estimates only looking at the translation where the translation is good and this tends to generate translations of a fluent and at least sound good in the target language.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we have the distortion score and exhaust distortion score embeds this assumption that you have in machine translation, but you shouldn't move the words around a lot so that the words are generally following this roughly the same order in the translation language.",
                    "label": 1
                },
                {
                    "sent": "And so when you put this into a decoder, decoder is a big heuristic algorithm, but attempts to find this division into phrases attempts to find the best translation of each of these phrases, and the best permutation, which is of course is a computationally very expensive task.",
                    "label": 1
                },
                {
                    "sent": "But the simple idea is you'll have an input translation like blood group antigen and then you'll get a translation, say into Spanish.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for all of our experiments, we used the Moses system the most system is sort of the the gold standard system.",
                    "label": 0
                },
                {
                    "sent": "It still uses the baseline by most papers.",
                    "label": 0
                },
                {
                    "sent": "And is an open source system.",
                    "label": 0
                },
                {
                    "sent": "It anyone can use?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our architecture, we attempted to modify Moses by introducing this four step pipeline.",
                    "label": 0
                },
                {
                    "sent": "We do pre processing and then our first promise will add new sources.",
                    "label": 0
                },
                {
                    "sent": "So we add sources from Wikipedia from a site called Linguee an from ya TE which is a European terminology.",
                    "label": 0
                },
                {
                    "sent": "We invent other extra domain specific features coming from avanca and then would give the decoder modified language model and then this would then work as ontology translation system where we could put our phone sentence, hear the English Minimum Finance lease payment and get our translation at the bottom into German.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So did the main lexecon realizing this factor?",
                    "label": 0
                },
                {
                    "sent": "The parallel text might not have good translations of the words, or maybe even the multi word units that are particularly this domain.",
                    "label": 1
                },
                {
                    "sent": "So the idea is we want to try to find additional data that is good translations in the domain.",
                    "label": 0
                },
                {
                    "sent": "So the first one used Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So the idea of Wikipedia approach is what we would do is we would look up in ontology.",
                    "label": 1
                },
                {
                    "sent": "We try to find all Wikipedia articles that had names that match labels and ontologies.",
                    "label": 0
                },
                {
                    "sent": "And then for E for every matching article in Wikipedia we look at which categories it has.",
                    "label": 0
                },
                {
                    "sent": "And then we look at the that we look at the categories and we say which categories correspond to many articles and we do a filter on nose, so we'd end up with sort of categories, and then we then expand the translation by taking over the article in that category and adding it's translation from DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So this allows us to get some translations that were more partial matches that we wouldn't get otherwise.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, in with a financial ontology, we saw that we got categories like economics terminology during accepted accounting principle of macroeconomics.",
                    "label": 0
                },
                {
                    "sent": "So this was very effective at finding which categories, at least in Wikipedia, were relevant and then therefore, which translations from the pedia we could use and could guarantee were in domain.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Secondly, we work with linguee.",
                    "label": 0
                },
                {
                    "sent": "Linguee is a Pty crawl of parallel data from the web.",
                    "label": 1
                },
                {
                    "sent": "It's searchable but you can't download it all.",
                    "label": 0
                },
                {
                    "sent": "And what we did was we just created for the ontology labels that we had and this generates a very small parallel corpus.",
                    "label": 1
                },
                {
                    "sent": "So this is about 24,000 sentences.",
                    "label": 0
                },
                {
                    "sent": "In comparison, our baseline system is trained on 100 million sentences, so this was a very small.",
                    "label": 0
                },
                {
                    "sent": "Corpus, but it was a very accurate corpus.",
                    "label": 0
                },
                {
                    "sent": "So for finance 24,000 sentences is quite effective.",
                    "label": 0
                },
                {
                    "sent": "And then this would be treated just like a normal parallel corpus will be trained using Moses.",
                    "label": 0
                },
                {
                    "sent": "An applied this way.",
                    "label": 0
                },
                {
                    "sent": "I finally we use Yayati was a terminology.",
                    "label": 0
                },
                {
                    "sent": "It covers many terms.",
                    "label": 0
                },
                {
                    "sent": "One of the problems you have is including a terminology like Yachty is that you don't know how to do the features 'cause you don't have the corpus data to back it up.",
                    "label": 1
                },
                {
                    "sent": "So we experimented using the explicit semantic analysis system to weigh trees values.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also use the explicit semantic analysis system, which was introduced originally by Kabila, Vichten Markovich and then was extended to a multilingual setting.",
                    "label": 1
                },
                {
                    "sent": "I'm easy as a domain feature, so this means that when the code is actually translating, there are features that are coming in from the ESA.",
                    "label": 0
                },
                {
                    "sent": "did I going to estimate in this particular domain how good the translation is?",
                    "label": 0
                },
                {
                    "sent": "So the basic idea of the essay is that you take something you take a Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "You build an index of it.",
                    "label": 0
                },
                {
                    "sent": "So you in your Lucene or whatever.",
                    "label": 0
                },
                {
                    "sent": "And then when you want to know how similar two phrases are, so, for example, you might have two elements T&FI from our phase table.",
                    "label": 0
                },
                {
                    "sent": "What we'll do is we'll count how many Wikipedia articles contain these terms, and this gives us a vector weighted by how frequently these terms occur in Wikipedia articles.",
                    "label": 0
                },
                {
                    "sent": "And then you do a vector comparison and then the result is we get a comparison single value saying how likely these two terms are.",
                    "label": 0
                },
                {
                    "sent": "In the domain, and this then is used to.",
                    "label": 0
                },
                {
                    "sent": "Neither feature score.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the domain adoption here is done again by simply selecting the Wikipedia articles that are in domain according to the categories.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Define the approach was based on using a language model, so the idea of a language model is that you estimate the likelihood of a sentence and the idea is the district prefer fluent centers and should prefer in the main translations.",
                    "label": 0
                },
                {
                    "sent": "So this is based on Markov assumption, so this is the formal version.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a slightly easier way to understand it that if you want to know how likely the string welcome to Kobe is all you do is you calculate the probability of seeing the world welcome and the probability of the word 2 coming after welcome and affordability of gender word, Kobe coming after 2 words.",
                    "label": 0
                },
                {
                    "sent": "Welcome to.",
                    "label": 0
                },
                {
                    "sent": "And the only way to calculate the probability of Kobe coming up to welcome to is very simple.",
                    "label": 0
                },
                {
                    "sent": "You get big corpus which only has to be monolingual, and then you count how many times do you see welcome to Kobe and you divide that by how many times do you see welcome to anything.",
                    "label": 0
                },
                {
                    "sent": "So any word.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a count of how likely you are to see Kobe after the two words, welcome to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We slightly modified this so we took our big court.",
                    "label": 0
                },
                {
                    "sent": "I'll be corpus and we use the arnetta method.",
                    "label": 0
                },
                {
                    "sent": "I introduced a couple of years back to estimate the similarity of a corpus to an ontology, so this meant for every single document in our corpus we had a score between zero and one that rated whether this document was relevant to the domain of the ontology.",
                    "label": 1
                },
                {
                    "sent": "And then we use this to modified account.",
                    "label": 1
                },
                {
                    "sent": "So essentially account was weighted by how likely oneta thought it was to be in domain.",
                    "label": 0
                },
                {
                    "sent": "So how likely the document was to be in domain and then we the count was appropriately rated?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we evaluated this on many metrics, so the blur metric is sort of seen as the standard in machine translation.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the only way you can publish a paper in machine translation is to provide blur scores.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you ask nearly everyone of the machine translation conference, and I'll tell you blow is awful, particularly in this case of ontology translation.",
                    "label": 0
                },
                {
                    "sent": "We've actually shown to blur is even worse than normal photology translation, and this is to do a particular floor it has.",
                    "label": 0
                },
                {
                    "sent": "In dealing with very short labels.",
                    "label": 0
                },
                {
                    "sent": "And we introduced a method called Blur 2, which is a modification of blood.",
                    "label": 0
                },
                {
                    "sent": "It works better for ontology translation.",
                    "label": 0
                },
                {
                    "sent": "But also to show we have general improvement, we use a variety of metrics that have come from different evaluations.",
                    "label": 0
                },
                {
                    "sent": "We evaluated then on two domains, so five ontologies across two domains.",
                    "label": 0
                },
                {
                    "sent": "Two of these ontologies were from finance and they were in English, Dutch, German and Spanish and they were quite big and these are open and available and then we had three ontologies that came from public services.",
                    "label": 0
                },
                {
                    "sent": "This is actually I think, provided by the Dutch Ministry of in of Immigration.",
                    "label": 0
                },
                {
                    "sent": "And these resource is unfortunately not open, so we can't provide them for comparison.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the results show that our baseline translation was getting blur scores that were about .1.",
                    "label": 0
                },
                {
                    "sent": "Now if you know machine to machine translation scores of .1 for blur is pretty terrible.",
                    "label": 0
                },
                {
                    "sent": "This is in part to do with the fact that blur is actually very poor to valuating ontology translation, so Blur 2 scores are much higher, but there's still only about 25 with the baseline system which is still not exactly a great translation.",
                    "label": 0
                },
                {
                    "sent": "So when we included the lexical methods, we see a significant improvement in all metrics.",
                    "label": 0
                },
                {
                    "sent": "But actually when we looked at the domain features and the domain language model, so the 2nd two prongs of attack.",
                    "label": 0
                },
                {
                    "sent": "But it didn't seem to work.",
                    "label": 0
                },
                {
                    "sent": "Actually they didn't really move any of these scores a lot.",
                    "label": 0
                },
                {
                    "sent": "The language model improved it a little bit, but only very slightly.",
                    "label": 0
                },
                {
                    "sent": "But then what we tried, we actually put the whole system together and we were glad to see if it actually hvizdos still improve.",
                    "label": 0
                },
                {
                    "sent": "So we actually get significant improvements by combining the domain features and a domain language model with the domain lexicon construction.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is improvements both over election by itself and over the base.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what this says is it the strongest improvement is really provided by creating a domain lexecon and this is a result that May Larkin has gone to prove in other papers such as the one you saw there today and another paper that was it ACL last year.",
                    "label": 1
                },
                {
                    "sent": "You have methods by themselves aren't particularly great, but combine them together does seem to in most cases, improve.",
                    "label": 1
                },
                {
                    "sent": "There were a few cases where it didn't, and this was mostly actually translating into Dutch for the public services use case.",
                    "label": 0
                },
                {
                    "sent": "But this result was also marked by the fact that the domain adaptation for the financial services was much less effective.",
                    "label": 0
                },
                {
                    "sent": "Or sorry for the public services, was much less effective than for the financial services.",
                    "label": 0
                },
                {
                    "sent": "And if you look at we're looking at the ontologies is fairly obvious that the pub, the financial ontologies are using very specific terms that are layperson wouldn't understand where is the immigration domain were sort of the same things you would read in newspapers.",
                    "label": 0
                },
                {
                    "sent": "So there wasn't as much of a domain bias in the public services case.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, domain Lexicon can significantly improve translation.",
                    "label": 1
                },
                {
                    "sent": "It is capable of suggesting you translations, but it does also have the issue that you require a lot of parallel text to make it work best.",
                    "label": 1
                },
                {
                    "sent": "So particularly in the Linguee case domain selection.",
                    "label": 0
                },
                {
                    "sent": "Is weaker, but it can still be useful, and I think there's still some research to do into improving this, so all the code is available on GitHub and he saw again Second dog.",
                    "label": 0
                },
                {
                    "sent": "Again, there's this demo here, which I recommend you check out, which is an ontology translation system that came out of this project.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}