{
    "id": "hrp6gqwxz2jf6djqqne56ngk2vuohvgs",
    "title": "Introduction and General Problem Statement",
    "info": {
        "author": [
            "Matthias W. Seeger, Laboratory for Probabilistic Machine Learning, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_seeger_igps/",
    "segmentation": [
        [
            "Learning things like reliability, stability.",
            "Um, guarantees that we can give about them that they don't breakdown.",
            "So I think, given that it's a different world today, we also have to think about whether we can change the way that we do things in machine learning."
        ],
        [
            "So if you want to build a very complicated system with a with a lot of components that interact, and there's one thing we really learned from computer science, and that is that we should build layered architecture, and I mean whatever the context of this idea of a layered architecture is, that one thing you really have to make sure that your base layer is is robust so that you understand that it's robust and you even can be sure that it's robust.",
            "Not just hope that it's robust and the other thing is that you have to understand what its limitations are and what these limitations mean to you when you're sitting on top and do some.",
            "Very high level application.",
            "So then some people saying machine learning is not really my base."
        ],
        [
            "And as I have the idea, somebody else is going to do the system.",
            "If my idea really works, I think that it's a bit of a limiting view because I really believe that if you are building a system that is entirely based on machine learning than a machine learning person has to design that system and not somebody who's coming from another field.",
            "And so while I think that nobody else is going to do this job for us, we certainly can be held.",
            "This is one of the reasons why we got some experts from American mathematics here where we can discuss how we could be helped."
        ],
        [
            "Because this is really the picture of the workshop, if you do almost anything that has to do with continuous variables in machine learning, then this base layer of your layered architecture is going to be numerical mathematics.",
            "So you can build a fancy method.",
            "You can build a fancier method on top, but the base layer is going to be numerical mathematics."
        ],
        [
            "So, so let me now come to the motivations of this workshop.",
            "So, given that the base layer is numerical mathematics, we should really make sure that we have some very basic understanding about what numerical mathematics is about and with basic I mean not in order to not be able to follow the very intricate proofs that people sometimes do, but.",
            "Have a list of the basic tools and tones that come out of this inside."
        ],
        [
            "So then there's often this talk about black boxes, so numerical mathematics in some parts of numerical mathematics.",
            "This really works.",
            "For example, if you if you talk about dense matrix algebra, this is a black box, so you can put in any metrics you want.",
            "The running time is going to not depend on the matrix, and usually the stability is also not going to depend on the matrix, but my opinion and all sorts of other people in machine learning is that black boxes are really something that that are for soft problems in machine learning is.",
            "Something for problems that are not solved yet.",
            "So usually if you think about machine learning researcher, it's kind of the power user.",
            "If you use a Windows speech, we really want to be at the limit of what is possible.",
            "Right now we want to do things with very large matrices with very huge problems with nonlinear huge problems.",
            "And in order to do that when you want to work at the cutting edge then you really have to understand what the what the properties of the one layer down are that you interface again.",
            "So that would be the numerical mathematics layer.",
            "So for example, if you are, what is my representation?",
            "How am I going to call some robust code and how am I going to not call it?",
            "And I really would like to emphasize that this interaction goes both ways.",
            "So for numerical mathematics research, it's certainly also important.",
            "What the what?",
            "The algorithms that that the researcher develops is actually going to be useful in the real world, because then if you have some tradeoffs, you can decide what the majority will be really interested in.",
            "Or think about topics which we are going to hear about in the workshop, like preconditioning, which are inherently dependent on the structure of the system that sits on top.",
            "You just cannot abstract that away.",
            "So what we what we need and what this workshop is really about is an interchange between these two fields because."
        ],
        [
            "We are sitting at this layer between a machine learning method and the numerical mathematics layer below.",
            "Another thing is that we would like to at least start to raise some awareness of these issues of you today.",
            "If you look at the paper and it's a good paper, then it's a good paper in machine learning because you propose something that's faster that's maybe convex that does better in terms of test error than some other methods on a data set of your choice."
        ],
        [
            "But why don't we add this points to the list?",
            "My method is maybe more numerically stable than another method.",
            "It's more reliable and I can prove that or it's actually reduce ibleto very well solve problems, so I don't have to do intricate programming myself.",
            "Just an awareness that there are these other quality issues which are quite a bit overlooked right now in the machine learning world and following on that.",
            "Of course, if you want to reduce your problems to, well, solve problems.",
            "What is the code that you're going to call and what are the caveats for calling this code?",
            "If you want to do things that machine learning people want to do?"
        ],
        [
            "So I'm just going to give you a few examples which are most certainly going to be addressed during this workshop.",
            "Our speakers, so don't don't worry about the formulas or PM through.",
            "Our speakers will be more careful explaining what this means, but just think about a model that's Gaussian, linear, and in general you would want to compute some means and variances.",
            "Posterior means and variances, and the only thing you should note that this is basically a linear algebra problem, which in many cases we can do exactly so.",
            "Then we need numerical mathematics.",
            "So think about means in a Gaussian model where that's least squares estimation."
        ],
        [
            "So least squares estimation when your matrices get too big, you cannot do this exactly anymore, but you can certainly use the structure in your model in order to run iterative solvers.",
            "Now, if you want to do this in a good way, you need to precondition these solvers and you need to do this for exactly 4 models that we're interested in machine learning.",
            "Or if your matrices are even larger and you cannot even run iterative solvers anymore, than you might consider randomized approaches in order to get a good, fairly close solution in a randomized way."
        ],
        [
            "Then you might be interested in the variances in these Gaussian models, and that is not least squares estimation.",
            "That is harder, but there are some ideas how you could reduce it by low rank approximation to again to least squares estimation.",
            "So you could think about doing a lowering of projections, and that would require you to know something about your model in order to choose good projections.",
            "Or you could use PCA, which is kind of optimal projections, and then you would have to have good algorithms for doing these things.",
            "Again, that's numerical mathematics.",
            "And then you might say, well, this is not really fair."
        ],
        [
            "See I don't care so much about Gaussian linear models, but that is exactly one of the good examples that you can then reduce.",
            "Approximate inference method."
        ],
        [
            "Example over non Gaussian models you can reduce that so that the only computation you have to do on the lower level are Gaussian linear computations and you might be interested in dynamical systems.",
            "Again, you can reduce this to the numerical mathematics primitives that you've learned about and then you can also do nonlinear dynamical systems because you stick both of these things together.",
            "Again, it's a.",
            "It's a clean reduction to a well formulated problem."
        ],
        [
            "So another example would be eigen decomposition.",
            "This comes up in machine learning all over PCA.",
            "Canonical correlation analysis, spectral clustering, manifold regularization, posterior covariance approximation.",
            "I could continue this list.",
            "That's very about maybe maybe more than 2030% of all machine learning papers.",
            "Do some decomposition somewhere?"
        ],
        [
            "Now, given that we put so much effort into developing the model that we then, which in this case would be the a matrix that we then do the eigendecomposition on, there's an obvious question here.",
            "Could we then use the structure in this model in order to do the eigen decomposition faster?",
            "Which usually is the one thing that takes the most running time and one operation that if we do it bad, we have to suffer for it, so there would be ideas, can we precondition it?",
            "Or even more interesting, can we use parallel hardware that is now sitting on every desk in order to make do that faster?"
        ],
        [
            "Another example would be, for example, what people in machine learning called lowering kernel approximations or have a big dense kernel matrix that come out of methods like SVM's and GPS.",
            "No structure in these matrices, there just ends, so it just doesn't scale up and you have to do something about it or people in machine learning.",
            "They call this the Nystrom method.",
            "They do incomplete Cholesky, but the basic problem here is of course how to select the columns that contain that give you the best approximation up there."
        ],
        [
            "And now this has just another name in American mathematics, and so we can certainly profit from work that people do in the neighboring field.",
            "Just by translating the terms."
        ],
        [
            "And then as the 4th and last example, there would be interior point optimization methods.",
            "So the very rough idea is that you have a convex optimization problem and you do it by by Newton method on the objective plus a barrier.",
            "Now there's been a black box packages been released for this, but very quickly in machine learning.",
            "People notice this is certainly not fast enough and it seems if you now look at kind of this conference that we've just seen that people now simply say, well, let's do first order.",
            "Methods instead of interior point, because that's just simpler, right?",
            "I."
        ],
        [
            "Let's say let's not abandon interior point methods up front because we abandoned black box implementations of interior point methods, But let's probably try to see whether people working on very large interior point methods have some idea in."
        ],
        [
            "Order to address the problems that we have in machine learning, because if there's a model structure, how to exploit it that might be harder than in first order methods, but but certainly it's worth it.",
            "How can we introduce ideas like blocking that people do in SVM optimization?",
            "How can we introduce it into interior point methods?",
            "What are the difference between these different algorithms in terms of numerical stability?",
            "And if we for example cannot solve for Newton directions exactly what is going to be the impact for the final problem?",
            "Or how can we precondition these Newton solutions?"
        ],
        [
            "So let me stop here and and give over to our speakers, and I wish you a very great workshop, but just let me list again a few points of demotivation so we would like to have a basic understanding about the do's and don'ts of numerical mathematics now that you can do by talking to numerica mathematicians.",
            "We would like to understand this interface that we use day by day better.",
            "Again, we would like to have two."
        ],
        [
            "Good discussion with numerical mathematicians would like to know what are the relevant properties for us."
        ],
        [
            "Again, we can address this."
        ],
        [
            "And then we would also like to know about good new code and how."
        ],
        [
            "And we call it in order to solve our problems for all these points, there's really a need for an interchange with numerical mathematics.",
            "And with that I give over to our first speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning things like reliability, stability.",
                    "label": 0
                },
                {
                    "sent": "Um, guarantees that we can give about them that they don't breakdown.",
                    "label": 0
                },
                {
                    "sent": "So I think, given that it's a different world today, we also have to think about whether we can change the way that we do things in machine learning.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to build a very complicated system with a with a lot of components that interact, and there's one thing we really learned from computer science, and that is that we should build layered architecture, and I mean whatever the context of this idea of a layered architecture is, that one thing you really have to make sure that your base layer is is robust so that you understand that it's robust and you even can be sure that it's robust.",
                    "label": 1
                },
                {
                    "sent": "Not just hope that it's robust and the other thing is that you have to understand what its limitations are and what these limitations mean to you when you're sitting on top and do some.",
                    "label": 1
                },
                {
                    "sent": "Very high level application.",
                    "label": 0
                },
                {
                    "sent": "So then some people saying machine learning is not really my base.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I have the idea, somebody else is going to do the system.",
                    "label": 0
                },
                {
                    "sent": "If my idea really works, I think that it's a bit of a limiting view because I really believe that if you are building a system that is entirely based on machine learning than a machine learning person has to design that system and not somebody who's coming from another field.",
                    "label": 0
                },
                {
                    "sent": "And so while I think that nobody else is going to do this job for us, we certainly can be held.",
                    "label": 1
                },
                {
                    "sent": "This is one of the reasons why we got some experts from American mathematics here where we can discuss how we could be helped.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because this is really the picture of the workshop, if you do almost anything that has to do with continuous variables in machine learning, then this base layer of your layered architecture is going to be numerical mathematics.",
                    "label": 1
                },
                {
                    "sent": "So you can build a fancy method.",
                    "label": 1
                },
                {
                    "sent": "You can build a fancier method on top, but the base layer is going to be numerical mathematics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so let me now come to the motivations of this workshop.",
                    "label": 0
                },
                {
                    "sent": "So, given that the base layer is numerical mathematics, we should really make sure that we have some very basic understanding about what numerical mathematics is about and with basic I mean not in order to not be able to follow the very intricate proofs that people sometimes do, but.",
                    "label": 1
                },
                {
                    "sent": "Have a list of the basic tools and tones that come out of this inside.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then there's often this talk about black boxes, so numerical mathematics in some parts of numerical mathematics.",
                    "label": 1
                },
                {
                    "sent": "This really works.",
                    "label": 0
                },
                {
                    "sent": "For example, if you if you talk about dense matrix algebra, this is a black box, so you can put in any metrics you want.",
                    "label": 1
                },
                {
                    "sent": "The running time is going to not depend on the matrix, and usually the stability is also not going to depend on the matrix, but my opinion and all sorts of other people in machine learning is that black boxes are really something that that are for soft problems in machine learning is.",
                    "label": 0
                },
                {
                    "sent": "Something for problems that are not solved yet.",
                    "label": 0
                },
                {
                    "sent": "So usually if you think about machine learning researcher, it's kind of the power user.",
                    "label": 0
                },
                {
                    "sent": "If you use a Windows speech, we really want to be at the limit of what is possible.",
                    "label": 0
                },
                {
                    "sent": "Right now we want to do things with very large matrices with very huge problems with nonlinear huge problems.",
                    "label": 1
                },
                {
                    "sent": "And in order to do that when you want to work at the cutting edge then you really have to understand what the what the properties of the one layer down are that you interface again.",
                    "label": 0
                },
                {
                    "sent": "So that would be the numerical mathematics layer.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you are, what is my representation?",
                    "label": 0
                },
                {
                    "sent": "How am I going to call some robust code and how am I going to not call it?",
                    "label": 0
                },
                {
                    "sent": "And I really would like to emphasize that this interaction goes both ways.",
                    "label": 0
                },
                {
                    "sent": "So for numerical mathematics research, it's certainly also important.",
                    "label": 0
                },
                {
                    "sent": "What the what?",
                    "label": 0
                },
                {
                    "sent": "The algorithms that that the researcher develops is actually going to be useful in the real world, because then if you have some tradeoffs, you can decide what the majority will be really interested in.",
                    "label": 0
                },
                {
                    "sent": "Or think about topics which we are going to hear about in the workshop, like preconditioning, which are inherently dependent on the structure of the system that sits on top.",
                    "label": 0
                },
                {
                    "sent": "You just cannot abstract that away.",
                    "label": 0
                },
                {
                    "sent": "So what we what we need and what this workshop is really about is an interchange between these two fields because.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are sitting at this layer between a machine learning method and the numerical mathematics layer below.",
                    "label": 1
                },
                {
                    "sent": "Another thing is that we would like to at least start to raise some awareness of these issues of you today.",
                    "label": 0
                },
                {
                    "sent": "If you look at the paper and it's a good paper, then it's a good paper in machine learning because you propose something that's faster that's maybe convex that does better in terms of test error than some other methods on a data set of your choice.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But why don't we add this points to the list?",
                    "label": 0
                },
                {
                    "sent": "My method is maybe more numerically stable than another method.",
                    "label": 0
                },
                {
                    "sent": "It's more reliable and I can prove that or it's actually reduce ibleto very well solve problems, so I don't have to do intricate programming myself.",
                    "label": 0
                },
                {
                    "sent": "Just an awareness that there are these other quality issues which are quite a bit overlooked right now in the machine learning world and following on that.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you want to reduce your problems to, well, solve problems.",
                    "label": 0
                },
                {
                    "sent": "What is the code that you're going to call and what are the caveats for calling this code?",
                    "label": 0
                },
                {
                    "sent": "If you want to do things that machine learning people want to do?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm just going to give you a few examples which are most certainly going to be addressed during this workshop.",
                    "label": 0
                },
                {
                    "sent": "Our speakers, so don't don't worry about the formulas or PM through.",
                    "label": 0
                },
                {
                    "sent": "Our speakers will be more careful explaining what this means, but just think about a model that's Gaussian, linear, and in general you would want to compute some means and variances.",
                    "label": 0
                },
                {
                    "sent": "Posterior means and variances, and the only thing you should note that this is basically a linear algebra problem, which in many cases we can do exactly so.",
                    "label": 0
                },
                {
                    "sent": "Then we need numerical mathematics.",
                    "label": 1
                },
                {
                    "sent": "So think about means in a Gaussian model where that's least squares estimation.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So least squares estimation when your matrices get too big, you cannot do this exactly anymore, but you can certainly use the structure in your model in order to run iterative solvers.",
                    "label": 1
                },
                {
                    "sent": "Now, if you want to do this in a good way, you need to precondition these solvers and you need to do this for exactly 4 models that we're interested in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Or if your matrices are even larger and you cannot even run iterative solvers anymore, than you might consider randomized approaches in order to get a good, fairly close solution in a randomized way.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then you might be interested in the variances in these Gaussian models, and that is not least squares estimation.",
                    "label": 1
                },
                {
                    "sent": "That is harder, but there are some ideas how you could reduce it by low rank approximation to again to least squares estimation.",
                    "label": 0
                },
                {
                    "sent": "So you could think about doing a lowering of projections, and that would require you to know something about your model in order to choose good projections.",
                    "label": 0
                },
                {
                    "sent": "Or you could use PCA, which is kind of optimal projections, and then you would have to have good algorithms for doing these things.",
                    "label": 0
                },
                {
                    "sent": "Again, that's numerical mathematics.",
                    "label": 0
                },
                {
                    "sent": "And then you might say, well, this is not really fair.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See I don't care so much about Gaussian linear models, but that is exactly one of the good examples that you can then reduce.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference method.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example over non Gaussian models you can reduce that so that the only computation you have to do on the lower level are Gaussian linear computations and you might be interested in dynamical systems.",
                    "label": 0
                },
                {
                    "sent": "Again, you can reduce this to the numerical mathematics primitives that you've learned about and then you can also do nonlinear dynamical systems because you stick both of these things together.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a clean reduction to a well formulated problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another example would be eigen decomposition.",
                    "label": 0
                },
                {
                    "sent": "This comes up in machine learning all over PCA.",
                    "label": 1
                },
                {
                    "sent": "Canonical correlation analysis, spectral clustering, manifold regularization, posterior covariance approximation.",
                    "label": 1
                },
                {
                    "sent": "I could continue this list.",
                    "label": 0
                },
                {
                    "sent": "That's very about maybe maybe more than 2030% of all machine learning papers.",
                    "label": 0
                },
                {
                    "sent": "Do some decomposition somewhere?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, given that we put so much effort into developing the model that we then, which in this case would be the a matrix that we then do the eigendecomposition on, there's an obvious question here.",
                    "label": 0
                },
                {
                    "sent": "Could we then use the structure in this model in order to do the eigen decomposition faster?",
                    "label": 0
                },
                {
                    "sent": "Which usually is the one thing that takes the most running time and one operation that if we do it bad, we have to suffer for it, so there would be ideas, can we precondition it?",
                    "label": 0
                },
                {
                    "sent": "Or even more interesting, can we use parallel hardware that is now sitting on every desk in order to make do that faster?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example would be, for example, what people in machine learning called lowering kernel approximations or have a big dense kernel matrix that come out of methods like SVM's and GPS.",
                    "label": 0
                },
                {
                    "sent": "No structure in these matrices, there just ends, so it just doesn't scale up and you have to do something about it or people in machine learning.",
                    "label": 1
                },
                {
                    "sent": "They call this the Nystrom method.",
                    "label": 1
                },
                {
                    "sent": "They do incomplete Cholesky, but the basic problem here is of course how to select the columns that contain that give you the best approximation up there.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now this has just another name in American mathematics, and so we can certainly profit from work that people do in the neighboring field.",
                    "label": 0
                },
                {
                    "sent": "Just by translating the terms.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then as the 4th and last example, there would be interior point optimization methods.",
                    "label": 0
                },
                {
                    "sent": "So the very rough idea is that you have a convex optimization problem and you do it by by Newton method on the objective plus a barrier.",
                    "label": 0
                },
                {
                    "sent": "Now there's been a black box packages been released for this, but very quickly in machine learning.",
                    "label": 1
                },
                {
                    "sent": "People notice this is certainly not fast enough and it seems if you now look at kind of this conference that we've just seen that people now simply say, well, let's do first order.",
                    "label": 1
                },
                {
                    "sent": "Methods instead of interior point, because that's just simpler, right?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's say let's not abandon interior point methods up front because we abandoned black box implementations of interior point methods, But let's probably try to see whether people working on very large interior point methods have some idea in.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Order to address the problems that we have in machine learning, because if there's a model structure, how to exploit it that might be harder than in first order methods, but but certainly it's worth it.",
                    "label": 1
                },
                {
                    "sent": "How can we introduce ideas like blocking that people do in SVM optimization?",
                    "label": 1
                },
                {
                    "sent": "How can we introduce it into interior point methods?",
                    "label": 1
                },
                {
                    "sent": "What are the difference between these different algorithms in terms of numerical stability?",
                    "label": 0
                },
                {
                    "sent": "And if we for example cannot solve for Newton directions exactly what is going to be the impact for the final problem?",
                    "label": 0
                },
                {
                    "sent": "Or how can we precondition these Newton solutions?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me stop here and and give over to our speakers, and I wish you a very great workshop, but just let me list again a few points of demotivation so we would like to have a basic understanding about the do's and don'ts of numerical mathematics now that you can do by talking to numerica mathematicians.",
                    "label": 1
                },
                {
                    "sent": "We would like to understand this interface that we use day by day better.",
                    "label": 0
                },
                {
                    "sent": "Again, we would like to have two.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good discussion with numerical mathematicians would like to know what are the relevant properties for us.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we can address this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we would also like to know about good new code and how.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we call it in order to solve our problems for all these points, there's really a need for an interchange with numerical mathematics.",
                    "label": 0
                },
                {
                    "sent": "And with that I give over to our first speaker.",
                    "label": 0
                }
            ]
        }
    }
}