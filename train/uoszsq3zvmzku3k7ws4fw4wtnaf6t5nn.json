{
    "id": "uoszsq3zvmzku3k7ws4fw4wtnaf6t5nn",
    "title": "Toward the understanding of partial-monitoring games",
    "info": {
        "author": [
            "Csaba Szepesv\u00e1ri, Department of Computing Science, University of Alberta"
        ],
        "published": "July 25, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Mathematics->Game Theory"
        ]
    },
    "url": "http://videolectures.net/explorationexploitation2011_szepesvari_toward/",
    "segmentation": [
        [
            "So it's a pleasure to be here.",
            "Thanks for inviting me and I'm going to talk about a topic that's.",
            "Kept us occupied for Hawaiian.",
            "It's pretty fascinating and we're fascinated with the topic and.",
            "We haven't completely answered all the questions he wanted to answer says also good opportunity for you guys to just understand where we are right now and then maybe you can help us to resolve some of the questions.",
            "It's really intriguing.",
            "So this is joint work with a bunch of people on bushka.",
            "Gobbler Bartock David Parr, who is Pino at Google and that's me obviously, and.",
            "Most of the work was done by Gower Bartle, who's my PhD student, and then these two guys helped us out in various respects.",
            "It's it was a pleasure to work with them, and I yeah."
        ],
        [
            "Kit.",
            "Alright, so here's the contents.",
            "1st I'm going to define the framework of partial monitoring.",
            "What is it?",
            "Why should we care?",
            "How is it different from bandits?",
            "We all, I guess know about bandit problems by now.",
            "And then I'm going to talk about the results that we have about these problems.",
            "And then conclusion."
        ],
        [
            "And open problems and that's it."
        ],
        [
            "Alright, so a little graphical illustration of.",
            "Prediction games.",
            "The way I call them, so we have three agents, learner and environment, and at every you Jeffrey you don't much see in the classical setting, but I'm going to need the referee for my own purposes for later an so the game goes like the following way.",
            "It goes in rounds.",
            "So at time T, so it works in discrete time steps.",
            "The environment chooses an outcome and the learner chooses an action, and both are communicated to the referee.",
            "OK. And then the referee computes the loss that the learner suffers and there is a function that connects the outcome and action of the learner.",
            "That's capital L, and that function is publicly known.",
            "So actually have to learn it.",
            "And so the learner receives feedback which is in this case the outcome that the environment has chosen OK and the learners call obviously is to minimize the sum of the losses that it suffers.",
            "OK, so he has to figure out the outcomes that the environment likes to choose, so it's like weather prediction.",
            "You know it could be good weather, bad weather that chosen by nature and the learners trying to learn like how to predict to minimize.",
            "Los maybe it's a 01 loss somehow."
        ],
        [
            "OK, so more formally and mathematical framework so you have a discrete time game at time T. Learner chooses an action it it belongs to, set X and then the environment chooses an outcome it belongs to set Y and there is the lowest that's computed by the referee.",
            "Depends on that reason outcome, an action and then the learner receives feedback.",
            "And because the learner gets to see the.",
            "Action or the outcome chosen by the environment.",
            "This is called a full information setting.",
            "OK so I I gotta see what my opponent has chosen, so that's full information.",
            "And as I said, the learners code is to keep the some of the losses as slow as possible and the gamer.",
            "The game is basically governed by by this function capital."
        ],
        [
            "Alright, so how do we measure the performance of the learner?",
            "So probably by now everyone is pretty much familiar with the concept of the regret.",
            "What he tries to measure is like how fast the average loss of learner converges to the best average loss in hindsight.",
            "OK, so because like talking about averages is kind of the same as talking about just the Sam's like you.",
            "Remove the normalizing factor.",
            "We like to talk about the regret, so we talk about like how much the learner loses in competition to choosing the best action in hindsight over the period of the game, and we'd like to keep the regret low.",
            "So we'd like to design A learner whose regret it slow and important concept is hanging consistency.",
            "That just means that we're learning, so eventually we are going to figure out.",
            "Uh, how to act?",
            "What is the optimal action in hindsight?",
            "And the typical result in the literature is that the regrets chaos in a sub linear fashion is like order T to the above, That, is a number between zero and one.",
            "OK, so that means that you are learning at the rate of T to the power of, Or if you normalize it after normalization, that means that the rate of convergence of your average loss to the best.",
            "Average loss is tied to the paraffin, minus one OK.",
            "So it so you converging to the best possible and so the smaller Gama is obviously the faster you learn.",
            "So I'd like to design algorithms that may go as small as Poss."
        ],
        [
            "7.",
            "OK, so that's the standard setting that a lot of people have been working on.",
            "In this talk, I'm going to look into."
        ],
        [
            "Passion monitoring in a passion monitoring setting.",
            "The problem addressed is that, well, it was kind of optimistic to assume that the learner gets to see the outcome chosen by opponent.",
            "Sometimes that's not the case and I will come up with.",
            "I will show you several examples when that's not the case, so we'd like to address this deficiency of this framework moving towards you know more practical settings like in practice.",
            "If this condition is not met, like what do you do like it just give it up.",
            "You want to have a solution.",
            "So this is called a partial information setting and one example is called dynamic pricing.",
            "So in dynamic pricing you are trying to sell a product.",
            "And you said the price for the product.",
            "So maybe it's like between one and $10 OK. And so you are the learner and your actions are like just the numbers between one and 10.",
            "OK, so the environment is like whoever comes to your shop.",
            "It's like a random guy from the street.",
            "Or if you have, you know a web company starting the product.",
            "The guy who enters your web shop.",
            "OK, and the outcome chosen is like he has a certain price in his mind about the product, right?",
            "So you choose a price.",
            "Let's say it's five.",
            "He chooses a price.",
            "Let's say he thinks that, oh, I would buy this for $4 then obviously there will be no deal because he was saying that your price is too high, right?",
            "And so you see that, OK, this customer was coming and then it's gone.",
            "No deal.",
            "But the customer is not going to tie you that.",
            "Like he was thinking about $4, there is no negotiation like that, right?",
            "In the opposite opposing case where the customer sent that.",
            "Oh I, I it would be lovely to buy this product for $6 and you fed the product for $5.",
            "There will be a deal.",
            "You see that oh, there's a deal.",
            "But again, the customer is not going to tell you that I would have voted for $6 as well, right?",
            "So because of this, you don't really see the outcome, so it goes.",
            "Outcome was like the price that the customer had.",
            "In his hat, right, you don't see it.",
            "You just see this binary feedback Sir.",
            "No Sir, alright.",
            "And if there is no sale, then your cost is maybe you have to store the items so that items so there is a fixed cost attached to know says and there is another cost, the opportunity loss if you start too low right?",
            "So when you actually solve the item for $5 and the customer was thinking about $6, you missed the opportunity to actually set it for $6.",
            "So you have a loss of 1.",
            "OK, so we're pretty pessimistic here.",
            "In both cases you just suffer losses, but that's OK. We want to maximize revenue or minimize loss that the same thing.",
            "So the main thing is that you didn't see the outcome 'cause the outcome was the price that the customer has had in its head.",
            "Alright, so that was one example.",
            "Another example is very similar to this bandwidth allocation in a network.",
            "You know you have two computers.",
            "You are the player and you want to send some video feed or what not from one computer to another.",
            "The link is shared between you and many other customers and like the other customers, choose like how much bandwidth that utilized and if you overutilize the bandwidth you suffer packet loss.",
            "You see the packet loss suffered, your throughput goes down.",
            "So that's your loss but you only see that you have packet loss.",
            "If you underutilize the bandits, you see that OK there is no packet loss, but you don't get to see like how much you have underutilized the bandwidth.",
            "So that's very similar again.",
            "So that's bandwidth allocation.",
            "Apple tasting, online advertising, even bandit problems are of this form, and you don't get to see the whole feedback, so these are the examples and I hope that I convince you by know that these are interesting and important examples in many real life problems.",
            "You don't see the opponents choice.",
            "And so so as in the case of bandits, the question is if you want to explore an action or if you want to exploit your knowledge 'cause you didn't get full information, you have to keep exploring the actions in order to get information about everything so that you're not missing opportunity.",
            "So it's kind of obvious.",
            "The question is how how to control this trade off and what are the limits to actually being able to control this tradeoff.",
            "And this is what we'd like to study.",
            "By studying the regret of this setting in this setting.",
            "So at this stage you're probably wondering about.",
            "OK, other guy is just crazy is going to talk about yet another variation of bandit problems, right?",
            "So is there any real difference to bandit problems here?",
            "So that's the first question."
        ],
        [
            "I'd like to to look into, but before that let me just recap the framework with this figure of what we're talking about.",
            "So we have the environment and the learner and the referee, and so the game works as before environment choose an outcome, learner chooses inaction, both communicated to the referee.",
            "And the referee.",
            "Actually in this case computes two things.",
            "It computes the loss and the price.",
            "You know the loss in two hits, a little book.",
            "And it also computes the feedback and the loss is not communicated back to the learner, but only the feedback is communicated back to the learner and there was one more very important thing ahead of time.",
            "Both L&H are publicly known in the other game, it was only added was publicly known.",
            "Right now we have Allen age, these two functions, and then you're asked how to play this game to minimize the regret.",
            "And is this game any different than dangerous bandit games?"
        ],
        [
            "Alright, OK so the mathematical framework it's like the same as before, except that we don't see the outcome chosen by the environment and both feedback and the loss is computed and we only see the feedback and otherwise it's the same.",
            "Except that right now the game is governed by these two functions.",
            "OK. OK. Wow, OK. Alright, so.",
            "Nope, Nope, that's the thing.",
            "That's the thing.",
            "So somehow, implicitly through age, you're learning about the feedback, like in the case of dynamic pricing, you know that if I over priced, the feedback is 1.",
            "If I underprice, the feedback is 0, so you're learning about the unknown outcome through the feedback.",
            "Any other questions?",
            "OK good, please stop me."
        ],
        [
            "First, something is unclear.",
            "I can make this lexer.",
            "Alright, so I'm going to skip some of these examples for the sake of."
        ],
        [
            "Saving some time or just go to let's say this game, so there's a game like a game can be also like if it's finite, you have finitely many choices, then the two functions obviously can be represented with two matrices, and all learner is going to be always the role player, and the environment is going to be the column player.",
            "OK, so here we have three actions and the environment or opponent has two actions, right?",
            "And then it's like it's a.",
            "This is the loss function represented as a matrix, and then this is the feedback represented as a matrix and hear different letters denote different symbols and that's that's it.",
            "So there are symbols, so they're off about.",
            "There's an awful weather.",
            "OK, so the first action.",
            "It's like penny matching, right?",
            "So the environment chooses something and then if I happen to choose the same thing then I suffer a loss, right?",
            "So I want to choose the opposing action, then what the opponent chooses.",
            "But the only way I can learn because well, if I choose the first action.",
            "I only got to see the same symbol a no matter what the opponent chooses, so I received no information, so information right?",
            "The same with the second action.",
            "I only got to see B if I choose the second action independently of opponent choice, whatever the third action in that case.",
            "Unfortunately, it seems that I'm always going to suffer the loss of 1.",
            "The constant no matter what the opponent has chosen, so it's kind of pricey action, but I can make a distinction between the two choices of the opponent.",
            "C&D are unequal, right?",
            "So I have a way of learning about the.",
            "The old comes, but I have to pay a price.",
            "So the question is OK, is this any harder than the usual games like the bandit and other games?",
            "Yes, it's very important.",
            "It's I cannot emphasize it sufficiently.",
            "Many times LNH are known as head of time.",
            "Like if you would observe them, you know like after finitely many tries, like if you are observing them without noise, you would figure them out and then you would play the same way.",
            "So it's actually not a big assumption.",
            "But just think about a case when Alan H. Are known ahead of the time they are given to you and I ask you like how you play this game.",
            "OK."
        ],
        [
            "Alright, there are lots of other examples.",
            "Bandits are an example as far right, so when the feedback is the actual loss.",
            "That we suffer well then in that case L&H these two metrics are just equal, right?",
            "So if I choose something, opponent chooses something.",
            "My feedback is my loss.",
            "So the two matrices are the same, so there is something going on with the structure of these matrices.",
            "Yes.",
            "Right, right?",
            "So we'll get into that like to assume that the environment is an oblivious adversary.",
            "Uh, what we'll see that like later on, when it comes to the results, I will take back from this."
        ],
        [
            "Generate a little bit.",
            "But otherwise we'd like to work about the adversary case, like dynamic pricing, so it is how dynamic pricing can be formalized, but you."
        ],
        [
            "And all this so alright?",
            "So what is known about this games?",
            "And I'm like I'd like to illustrate this with a figure and so this is the the spectrum of the games as far as the hardness of the games is concerned.",
            "So here are the games like on this segment between 01.",
            "I want to put like every game which is encoded by these two matrices.",
            "Like let's say Ellen age.",
            "And this segment somewhere.",
            "And if I put the game here, that means that the regret scales like order one you have constant regret or no regret at all.",
            "So in that case, for those other games when opponents choice doesn't actually matter, that games like that stupid, not very interesting games the other very uninteresting games is that when you don't have information or doesn't matter what you do, you will suffer a linear regret you actually there games like that.",
            "So just imagine that the opponent has these two actions.",
            "It makes a difference of which action the opponent chooses, but you received no information.",
            "Of course you are.",
            "That's going to scale linearly, so we know that we can put some games here, and we can put some games there.",
            "And we also know that full information games actually lie here.",
            "The regret, the way it's chaos with time is square root, T right for full information.",
            "Again, so that those are the games.",
            "When re you receive the choice of the.",
            "Overlap, and there's been a lot of previous work on this, so we pretty well understand that those games belong to that.",
            "So in that case you know each has a special structure.",
            "It's like age and codes like the opponents choices.",
            "Also, bandit games.",
            "We understand them pretty well.",
            "And we know that they should actually be placed at the same spot, right?",
            "So as far as the scaling of the regret is concerned, as a function of time there as hard as full information games.",
            "Alright, and we also know because of some lower bounds.",
            "It's pretty interesting that there are no games in this open interval between 0 and 1/2.",
            "OK, so either game is, you know, trivially easy or it's at least this hard.",
            "There is nothing in between.",
            "It's kind of fascinating.",
            "And then there's been other work on this or previous work, and this passion monitoring problems.",
            "It was actually first picked up by forgive me if I say the name, rank, pico, Bonyan, Schindler Power and they studied this games and later on cheese of Bianchi thoughts and logo.",
            "She actually studied this games and together if you put the two papers together they proved that there are no games.",
            "In this open interval.",
            "Alright, so that's an empty interval as well, so that's the boundary at two surd.",
            "And actually they showed that the revealing action game that we talked about is at 2:30.",
            "It's hard when you have to pay an extra cost for gaining information.",
            "It turns out that that game is actually harder.",
            "Alright, but this leaves open a couple of questions, isn't."
        ],
        [
            "So where is dynamic pricing, for example, so we talked about this little game and you're trying to say something and you have limited information.",
            "So where is that like?",
            "Where should I put this on this figure?",
            "So for revealing action game, we happen to know that it should be there, but there's just one game like there are so many other games.",
            "So what about dynamic pricing?",
            "And like generally, if you have a game which is encoded with these two matrices, where should we put it?",
            "Shouldn't there be a characterization that, like just you look at these two metrics and you say that, oh, that game should be, let's say at 1/2?",
            "Or maybe it's in between exactly 1/2 and two third, or what?",
            "And Lastly, but very importantly, like if I give you a game which is specified by these two matrices, how do you want to play it right?",
            "So it depends on where the game is going to lend on this segment, right?",
            "So you you there are different algorithms like there is there agmc variations.",
            "For games which are here, they are somewhat different than games that are over there.",
            "OK.",
            "So the question mark should be much bigger.",
            "So there is a huge question mark here.",
            "Imagine that that we just don't know if there are any games in between these two points, and where exactly a game should land.",
            "And that's the question we'd like to answer.",
            "OK."
        ],
        [
            "Alright, so our contribution is.",
            "Being in the past that we classified some special keys is for adversary outcomes, but today I'd like to talk about when the adversary is actually stochastic, so we found it very difficult to think about the adversary setting.",
            "We didn't have much ideas, so we said OK, why not simplify?",
            "So we simplify the problem and so we're looking at the stochastic setting when the opponent chooses a fixed distribution over the outcomes, which you don't see.",
            "And you have to learn about that distribution, and in pretty much other cases that we know of the adversary and stochastic settings were kind of the same way.",
            "So if you crack the stochastic setting, then we figured that we will have some idea about how to correct the adversary settings.",
            "So from now on I'm going to talk about the stochastic setting.",
            "It's like you know the difference between stochastic bandit problems and adversary bandit problems.",
            "Alright, and the result that I'd like to present in a not charge says that yes, there are no other cases.",
            "They're just like these four cases that trivia games, hopeless games, easy games, and hard games.",
            "Ankola game easy if it southeast is a banded game, and I call it hard if it's as hard as labor official."
        ],
        [
            "Prediction.",
            "Alright, so how are we going to get here?"
        ],
        [
            "How?",
            "So the results.",
            "So actually let's think a bit about.",
            "How can you actually learn?",
            "So?",
            "How can you actually?",
            "So we are in the stochastic setting, so the opponent has chosen some probability distribution.",
            "We'd like to learn about that probability distribution.",
            "So let's think about a simple case when you when you keep choosing the same action all the time.",
            "So what is an information that we can gain?",
            "And here is a simple illustration for this.",
            "So the outcomes are like, you know, Banana, Apple, banana and strawberry for the four.",
            "Choices of the opponent.",
            "OK for your chosen action, and these are the probabilities.",
            "We switch.",
            "The opponent is going to choose this outcomes.",
            "Alright, so if you think about just for a sack, if you keep choosing this action, but you can learn is the probability of seeing a banner or the probability of seeing an Apple and the probability of seeing at strawberry.",
            "So you can learn another probability distribution.",
            "You are not going to learn exactly.",
            "B but you learn different probability distribution that's derived from P, right?",
            "So it's in the keys, the probability that the feedback is banana.",
            "It's obviously just the sum of 0.1 and 0.2 right?",
            "And the probability that the feedback is Apple is exactly 0.6, and the probability that the feedback is strawberry is what it should be.",
            "0.1 alright and.",
            "You can simply write the probability of the feedback being equal to some outcome as the sum of the individual.",
            "The probabilities of the individual outcomes using these indicator functions right so you like you have to sum up like for which cases are going to see the same outcome.",
            "OK, and you can write this in a matrix form, so you just see kind of a projection of the unknown probability distribution.",
            "And this SI matrix it's like an indicator matrix.",
            "It's going to have as many rows as the different feedback types you have for that action, and it has as many columns as the number of outcomes is.",
            "Alright, so this signal matrix is going to play a crucial role in our characterization of the hardness of these games, right?",
            "So what it does again is that if you keep choosing that action.",
            "The probability distribution that you see over the feedbacks.",
            "Can be obtained from the unknown probability distribution by multiplying that probability distribution by this signal matrix.",
            "That's the ultimately very important property of this matrix, and this matrix is going to play a crucial role.",
            "OK, so."
        ],
        [
            "Or the next simple question, if I give you 2 actions, I and I prime, can we differentiate between the two actions right?",
            "So if their losses underlying the two actions would be different for the property distribution chosen by the opponent, it would be crucial to differentiate between them.",
            "So talking about the simplest case so a little bit more notation.",
            "So if Ally is the last vector that underlies these action I. OK. We want to figure out.",
            "Just the sign.",
            "Of of this quantity here, because that would tell us if action I has a smaller loss or action.",
            "I prime has a smaller loss because Ally transpose P is the expected loss under action.",
            "I'll I prime transpose, P is the expected loss and an action I prime.",
            "So we'd like to figure out this sign.",
            "So it's like a classification problem.",
            "OK.",
            "Uh, and So what is the information that we can receive?",
            "So imagine that you keep using these actions, then your information you receive is, you know, this projection of P underlying side.",
            "And as I prime these two signal matrices, so you can actually stack the two matrices and then you can.",
            "See that the bad case for you, in which case you will not be able to figure out which loss which action is better or worse, is when there exists two probability distributions in the probability simplex such that the feedback underlying the two probability distribution is the same even if you pull the arms like or choose these actions infinitely many times and the signs are different.",
            "OK, like it makes a difference.",
            "If the opponent actually chooses P1 and P2 in terms of your losses, this is the bad case that therefore I call it BC.",
            "Not refering to, you know."
        ],
        [
            "That"
        ],
        [
            "I'm from Alberta.",
            "So the fundamental lemma is this and due to lack of time, we're not going to the details.",
            "But the lemma says that the bad keys happens exactly if and only if the difference between the loss factors.",
            "Underlying these two actions, I and I prime or loss factors doesn't lie in the image space of the concatenated signal matrix transposed OK, and this little linear children I just show 1."
        ],
        [
            "Direction of the proof so you can have a talk with other proof, right?",
            "This is going to be really useful for the upper bound construction, and so the way it works is that.",
            "We want to prove this direction and we prove it by by contradiction.",
            "So we prove actually that if the last difference lies in this image space, then this implies that for any P distribution you can figure out the sign of this thing.",
            "Actually, you can actually figure out that quantity, so you can figure out how big is the gap between the losses of the two actions.",
            "And this is really easy to see, so let's assume that this holds.",
            "If this holds, and that means that there is some vector V such that the last difference can be expressed in this form, right?",
            "It's very simple, and if that's the case, well, you just multiply from right by fee and you are done alright.",
            "So this is going to be the basis for the algorithm that we are going to use, because what it has you is that well, if this condition holds then you can figure out this vector V. And you can estimate loss differences in that awesome, right?",
            "So you don't actually have to see the losses, but you can estimate that you can figure them out by playing actions."
        ],
        [
            "I and I prime.",
            "OK, so the other direction is a little bit more cumbersome and click requires a little bit of linear algebra, but it's it fits a slide, so it's not very common."
        ],
        [
            "Located but I don't have time to go into the details.",
            "I just showed the end result so the other direction says that if the last difference is not in this image space then the bad case holds and that means that you can actually find OK.",
            "So here is the probability simplex for the all the outcomes.",
            "Alright, so we have three outcomes so I can throw this probability simplex in two dimension.",
            "And.",
            "I have two actions action ion action, I prime and so here is the boundary between like when a probability distribution lies in this section of the simplex then action I is the winner.",
            "It gives you the lowest cost.",
            "In the other case action I prime is the winner.",
            "So you can decompose the probability simplex you know in these two halves in this case and what you can show is that if this case doesn't hurt that there exists APO.",
            "Probability distribution that's exactly on the boundary, and you can perturb it by a small quantity.",
            "Where we is in the kernel of this matrix such that this normalized equality holds such that one of these guys.",
            "Is actually an in 1/2 where action I is the winner in the other guy is in the other half an action action I prime is the winner, so you recover the bad case so you figure out that you have two probability distributions indeed.",
            "Are such that the true probability distributions because of this conditions they will look identical to you if you just keep using I and I prime.",
            "You will not be able to distinguish between them.",
            "And they they lie in different parts of the simplex.",
            "So therefore if the opponent keeps playing between these two, you are kind of like very limit like you don't actually have information for figuring out which is the case."
        ],
        [
            "And kind of don't situation.",
            "So let's talk a little bit more about this side of composition so you can keep doing this thing when you introduce more and more actions and then eventually you can decompose the probability simplex into these polygons such that over a Polygon Polygon belongs to an action where the action is actually the winner, keeps it the lowest loss.",
            "If the unknown property distribution lies.",
            "Into that part of the.",
            "The composition and your job is by playing, you know, the actions to figure out where the unknown probability distribution lies, and you can see that.",
            "Sometimes you the actions give you information about certain directions in this space and and so that's how you're going to play.",
            "OK, so one important concept here is the concept of neighboring actions.",
            "So two actions are neighboring.",
            "If in this side of composition their neighbors.",
            "So there is this geometrical concept."
        ],
        [
            "So the lower bound I pretty much explained."
        ],
        [
            "Ready, so if there exists two actions which are neighboring decider composition such that the last difference vector doesn't lie in the image space of this matrix of the composed signal matrix, then you can prove that the regret is going to scale like T to the two 3rd.",
            "And this works in the diverse are cases file of course, because if it works in the stochastic as it works in the adversary case tool and the proof is just follows very simply by the previous arguments.",
            "So you can come up with this P1 NP.",
            "Two answer and so forth.",
            "And if there is 1/3 action if there is no third action that the game is whole class, if there is 1/3 action, it will be costly."
        ],
        [
            "'cause it's further away.",
            "So let's say you have this action there and the opponents playing, playing either this or that.",
            "This is just too costly.",
            "It's not within an optimal region, so you're going to pay a high price for it."
        ],
        [
            "Alright, so."
        ],
        [
            "It's a large one, so for the upper bound, so let's take a look at the regret.",
            "So that's.",
            "We are going to use the notion of expected regret in this case, so you have.",
            "The sum of the losses that you suffer and you subtract the minimum of the expectation of the some of the losses for the meaning was taken over all possible actions, and then there is this classic rather composition where you can write the regret in terms of you know, summing over all actions, how many times you're playing in action and how much does it cost to play that action.",
            "So all four I I prime here is the gap between the.",
            "The losses are faction I and I prime and to lie is a number of times an algorithm would play action.",
            "I and I started would be just an optimal action and the.",
            "Distribution chosen by the opponent.",
            "OK, so that's a classic.",
            "Regret their composition and because of this, you know that all you have to do is to keep the number of times you're playing actions which have a higher gap smaller number of times, right?",
            "So how can we do this?",
            "Well, the very simple idea is just as trying to estimate these gaps.",
            "And when it becomes obvious how big is the gap like, the sign becomes obvious, right?",
            "So it's relatively relative error estimate is good, like over 1/2 or sorry below 1/2.",
            "Then just animate this suboptimal action because done design.",
            "So that's the idea of the R."
        ],
        [
            "Gotham and so how to contract the good estimates we already had partially answer on the slides, so show that for every neighboring action I and I prime we have this condition satisfied.",
            "So we want to exploit this condition to construct an algorithm that actually achieves a good regret.",
            "So we want to show that under this condition the regret is quietly.",
            "So how to exploit this?",
            "Well, if the two actions are actually neighbors.",
            "And this condition is met.",
            "We know that there is this underlying vector such that you can express the loss differences in this form and then you know if we already saw this that like if you multiply from the right from the known probability distribution, you will get an estimate of the gap between the pay offs of the two actions, and so the good thing is that if you do that, you have to transpose the whole thing.",
            "You are going to have.",
            "Actually it's here estimate.",
            "So because this is the signal matrix, and as I said at the beginning, the signal matrix is constructed in such a way that you can actually estimate this quantity if you keep choosing I and I prime, you're going to estimate it quantity, place that product with the estimates and that's your estimate.",
            "OK, so in other words, compute the relative frequencies of outcomes and direction I and I prime that's Q hat and justice use Q hat in place of this product.",
            "OK, that's your estimate of that projection.",
            "And that's going to converge fast.",
            "And then if actions are not neighbors, you still have to estimate the gaps.",
            "Whether you do like you find a chain that connects the actions through through neighboring actions and you can show that the chain always exists, and then you are chaining and it's just a telescoping sum, so it's going to be a good estimate."
        ],
        [
            "Alright, and as I said before, you want to stop using an action when you can decide about the sign of the estimates, and for that you need maybe to use versions inequality.",
            "Actually you have to use bastions inequality or to stop early enough.",
            "And then you can prove that you are going to stop.",
            "In time, that's inversely proportional to the size of the gap between the two actions.",
            "OK, so if there is a positive gap, then you stop this."
        ],
        [
            "OK, this is what the algorithm dies and then if you go back to the regret decomposition.",
            "So when the stopping rule doesn't fair then you can just replaced Olaf.",
            "I buy this upper pond and if you do that that guy just cancel it and then you can decompose the sum into two parts.",
            "The one part where the gaps are big.",
            "The one part that the gaps are small.",
            "Your per bombos terms choose off a zero in the way to optimize the bondan you get this."
        ],
        [
            "Red with the band.",
            "That's how it works.",
            "OK, so once they are, get them so it's called Balaton.",
            "To advertise called that's going to happen very soon.",
            "Very close to Balaton which is a Lake in Hungary, so it's bandit based Lausanne elation and the algorithm works like this.",
            "You repeat the following.",
            "Why there is more than one alive action, so you have you know you start with all actions and then you categorize them.",
            "Being alive with that.",
            "So if.",
            "So you keep trying the alive actions estimate.",
            "Loss difference is sort of a.",
            "It was said before and when you can, when you see that some estimates substantially different from zero, eliminate one of the actions and then once only one actions remains, keep pulling that action until the end."
        ],
        [
            "Type OK and with this you can actually show."
        ],
        [
            "This theorem that was advertised before and so in summary right now, for finite stochastic games we could complete the picture.",
            "There are no games, no games in between 1/2 and two third.",
            "We have a full character characterization.",
            "We just give Me 2 matrices.",
            "I can very cheaply compute valid game belongs.",
            "I can tell you which are get them to use.",
            "In this case you should use a different target and in this case and dynamic pricing is actually at 2 third.",
            "So unfortunately it's hard game.",
            "And."
        ],
        [
            "At.",
            "Alright, so conclusions so we got this full characterization and we got this very interesting concept that we have geometria naughty bra and it's pretty fascinating.",
            "So big question is does this extend to the adversarial setting?",
            "We believe it does, but we haven't proved.",
            "OK, will you like what is missing because the lower bound actually clears half of the problem.",
            "What is missing is a good algorithm that achieves quadrati regret in the easy case when we have this condition map, there are lots of other questions that we'd like to explore and and I think that there is a relationship between a large Shamir stalk.",
            "I don't know if he's here and this work is far.",
            "It's all about controlling the variance, so bottom might not be the.",
            "Only algorithm that that's able to do this.",
            "It just happens that we were able to prove for these algorithms that you can achieve square root of T regret for easy games, but we don't know actually if the same would work for FedEx Pre if you know do the expiration and more clever way let's say."
        ],
        [
            "Alright, so that thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "Thanks for inviting me and I'm going to talk about a topic that's.",
                    "label": 0
                },
                {
                    "sent": "Kept us occupied for Hawaiian.",
                    "label": 0
                },
                {
                    "sent": "It's pretty fascinating and we're fascinated with the topic and.",
                    "label": 0
                },
                {
                    "sent": "We haven't completely answered all the questions he wanted to answer says also good opportunity for you guys to just understand where we are right now and then maybe you can help us to resolve some of the questions.",
                    "label": 0
                },
                {
                    "sent": "It's really intriguing.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with a bunch of people on bushka.",
                    "label": 0
                },
                {
                    "sent": "Gobbler Bartock David Parr, who is Pino at Google and that's me obviously, and.",
                    "label": 0
                },
                {
                    "sent": "Most of the work was done by Gower Bartle, who's my PhD student, and then these two guys helped us out in various respects.",
                    "label": 0
                },
                {
                    "sent": "It's it was a pleasure to work with them, and I yeah.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kit.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's the contents.",
                    "label": 0
                },
                {
                    "sent": "1st I'm going to define the framework of partial monitoring.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "Why should we care?",
                    "label": 0
                },
                {
                    "sent": "How is it different from bandits?",
                    "label": 0
                },
                {
                    "sent": "We all, I guess know about bandit problems by now.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk about the results that we have about these problems.",
                    "label": 0
                },
                {
                    "sent": "And then conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And open problems and that's it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so a little graphical illustration of.",
                    "label": 0
                },
                {
                    "sent": "Prediction games.",
                    "label": 0
                },
                {
                    "sent": "The way I call them, so we have three agents, learner and environment, and at every you Jeffrey you don't much see in the classical setting, but I'm going to need the referee for my own purposes for later an so the game goes like the following way.",
                    "label": 0
                },
                {
                    "sent": "It goes in rounds.",
                    "label": 0
                },
                {
                    "sent": "So at time T, so it works in discrete time steps.",
                    "label": 0
                },
                {
                    "sent": "The environment chooses an outcome and the learner chooses an action, and both are communicated to the referee.",
                    "label": 0
                },
                {
                    "sent": "OK. And then the referee computes the loss that the learner suffers and there is a function that connects the outcome and action of the learner.",
                    "label": 0
                },
                {
                    "sent": "That's capital L, and that function is publicly known.",
                    "label": 0
                },
                {
                    "sent": "So actually have to learn it.",
                    "label": 0
                },
                {
                    "sent": "And so the learner receives feedback which is in this case the outcome that the environment has chosen OK and the learners call obviously is to minimize the sum of the losses that it suffers.",
                    "label": 0
                },
                {
                    "sent": "OK, so he has to figure out the outcomes that the environment likes to choose, so it's like weather prediction.",
                    "label": 0
                },
                {
                    "sent": "You know it could be good weather, bad weather that chosen by nature and the learners trying to learn like how to predict to minimize.",
                    "label": 0
                },
                {
                    "sent": "Los maybe it's a 01 loss somehow.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so more formally and mathematical framework so you have a discrete time game at time T. Learner chooses an action it it belongs to, set X and then the environment chooses an outcome it belongs to set Y and there is the lowest that's computed by the referee.",
                    "label": 1
                },
                {
                    "sent": "Depends on that reason outcome, an action and then the learner receives feedback.",
                    "label": 0
                },
                {
                    "sent": "And because the learner gets to see the.",
                    "label": 1
                },
                {
                    "sent": "Action or the outcome chosen by the environment.",
                    "label": 0
                },
                {
                    "sent": "This is called a full information setting.",
                    "label": 0
                },
                {
                    "sent": "OK so I I gotta see what my opponent has chosen, so that's full information.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the learners code is to keep the some of the losses as slow as possible and the gamer.",
                    "label": 1
                },
                {
                    "sent": "The game is basically governed by by this function capital.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so how do we measure the performance of the learner?",
                    "label": 0
                },
                {
                    "sent": "So probably by now everyone is pretty much familiar with the concept of the regret.",
                    "label": 0
                },
                {
                    "sent": "What he tries to measure is like how fast the average loss of learner converges to the best average loss in hindsight.",
                    "label": 0
                },
                {
                    "sent": "OK, so because like talking about averages is kind of the same as talking about just the Sam's like you.",
                    "label": 0
                },
                {
                    "sent": "Remove the normalizing factor.",
                    "label": 0
                },
                {
                    "sent": "We like to talk about the regret, so we talk about like how much the learner loses in competition to choosing the best action in hindsight over the period of the game, and we'd like to keep the regret low.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to design A learner whose regret it slow and important concept is hanging consistency.",
                    "label": 0
                },
                {
                    "sent": "That just means that we're learning, so eventually we are going to figure out.",
                    "label": 0
                },
                {
                    "sent": "Uh, how to act?",
                    "label": 0
                },
                {
                    "sent": "What is the optimal action in hindsight?",
                    "label": 0
                },
                {
                    "sent": "And the typical result in the literature is that the regrets chaos in a sub linear fashion is like order T to the above, That, is a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means that you are learning at the rate of T to the power of, Or if you normalize it after normalization, that means that the rate of convergence of your average loss to the best.",
                    "label": 0
                },
                {
                    "sent": "Average loss is tied to the paraffin, minus one OK.",
                    "label": 0
                },
                {
                    "sent": "So it so you converging to the best possible and so the smaller Gama is obviously the faster you learn.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to design algorithms that may go as small as Poss.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "7.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the standard setting that a lot of people have been working on.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I'm going to look into.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Passion monitoring in a passion monitoring setting.",
                    "label": 0
                },
                {
                    "sent": "The problem addressed is that, well, it was kind of optimistic to assume that the learner gets to see the outcome chosen by opponent.",
                    "label": 0
                },
                {
                    "sent": "Sometimes that's not the case and I will come up with.",
                    "label": 0
                },
                {
                    "sent": "I will show you several examples when that's not the case, so we'd like to address this deficiency of this framework moving towards you know more practical settings like in practice.",
                    "label": 0
                },
                {
                    "sent": "If this condition is not met, like what do you do like it just give it up.",
                    "label": 0
                },
                {
                    "sent": "You want to have a solution.",
                    "label": 0
                },
                {
                    "sent": "So this is called a partial information setting and one example is called dynamic pricing.",
                    "label": 1
                },
                {
                    "sent": "So in dynamic pricing you are trying to sell a product.",
                    "label": 0
                },
                {
                    "sent": "And you said the price for the product.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's like between one and $10 OK. And so you are the learner and your actions are like just the numbers between one and 10.",
                    "label": 0
                },
                {
                    "sent": "OK, so the environment is like whoever comes to your shop.",
                    "label": 0
                },
                {
                    "sent": "It's like a random guy from the street.",
                    "label": 0
                },
                {
                    "sent": "Or if you have, you know a web company starting the product.",
                    "label": 0
                },
                {
                    "sent": "The guy who enters your web shop.",
                    "label": 0
                },
                {
                    "sent": "OK, and the outcome chosen is like he has a certain price in his mind about the product, right?",
                    "label": 0
                },
                {
                    "sent": "So you choose a price.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's five.",
                    "label": 0
                },
                {
                    "sent": "He chooses a price.",
                    "label": 0
                },
                {
                    "sent": "Let's say he thinks that, oh, I would buy this for $4 then obviously there will be no deal because he was saying that your price is too high, right?",
                    "label": 0
                },
                {
                    "sent": "And so you see that, OK, this customer was coming and then it's gone.",
                    "label": 0
                },
                {
                    "sent": "No deal.",
                    "label": 0
                },
                {
                    "sent": "But the customer is not going to tie you that.",
                    "label": 0
                },
                {
                    "sent": "Like he was thinking about $4, there is no negotiation like that, right?",
                    "label": 0
                },
                {
                    "sent": "In the opposite opposing case where the customer sent that.",
                    "label": 0
                },
                {
                    "sent": "Oh I, I it would be lovely to buy this product for $6 and you fed the product for $5.",
                    "label": 0
                },
                {
                    "sent": "There will be a deal.",
                    "label": 0
                },
                {
                    "sent": "You see that oh, there's a deal.",
                    "label": 0
                },
                {
                    "sent": "But again, the customer is not going to tell you that I would have voted for $6 as well, right?",
                    "label": 0
                },
                {
                    "sent": "So because of this, you don't really see the outcome, so it goes.",
                    "label": 0
                },
                {
                    "sent": "Outcome was like the price that the customer had.",
                    "label": 0
                },
                {
                    "sent": "In his hat, right, you don't see it.",
                    "label": 0
                },
                {
                    "sent": "You just see this binary feedback Sir.",
                    "label": 0
                },
                {
                    "sent": "No Sir, alright.",
                    "label": 0
                },
                {
                    "sent": "And if there is no sale, then your cost is maybe you have to store the items so that items so there is a fixed cost attached to know says and there is another cost, the opportunity loss if you start too low right?",
                    "label": 0
                },
                {
                    "sent": "So when you actually solve the item for $5 and the customer was thinking about $6, you missed the opportunity to actually set it for $6.",
                    "label": 0
                },
                {
                    "sent": "So you have a loss of 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're pretty pessimistic here.",
                    "label": 0
                },
                {
                    "sent": "In both cases you just suffer losses, but that's OK. We want to maximize revenue or minimize loss that the same thing.",
                    "label": 0
                },
                {
                    "sent": "So the main thing is that you didn't see the outcome 'cause the outcome was the price that the customer has had in its head.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that was one example.",
                    "label": 0
                },
                {
                    "sent": "Another example is very similar to this bandwidth allocation in a network.",
                    "label": 0
                },
                {
                    "sent": "You know you have two computers.",
                    "label": 0
                },
                {
                    "sent": "You are the player and you want to send some video feed or what not from one computer to another.",
                    "label": 0
                },
                {
                    "sent": "The link is shared between you and many other customers and like the other customers, choose like how much bandwidth that utilized and if you overutilize the bandwidth you suffer packet loss.",
                    "label": 0
                },
                {
                    "sent": "You see the packet loss suffered, your throughput goes down.",
                    "label": 0
                },
                {
                    "sent": "So that's your loss but you only see that you have packet loss.",
                    "label": 0
                },
                {
                    "sent": "If you underutilize the bandits, you see that OK there is no packet loss, but you don't get to see like how much you have underutilized the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "So that's very similar again.",
                    "label": 0
                },
                {
                    "sent": "So that's bandwidth allocation.",
                    "label": 0
                },
                {
                    "sent": "Apple tasting, online advertising, even bandit problems are of this form, and you don't get to see the whole feedback, so these are the examples and I hope that I convince you by know that these are interesting and important examples in many real life problems.",
                    "label": 1
                },
                {
                    "sent": "You don't see the opponents choice.",
                    "label": 0
                },
                {
                    "sent": "And so so as in the case of bandits, the question is if you want to explore an action or if you want to exploit your knowledge 'cause you didn't get full information, you have to keep exploring the actions in order to get information about everything so that you're not missing opportunity.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of obvious.",
                    "label": 0
                },
                {
                    "sent": "The question is how how to control this trade off and what are the limits to actually being able to control this tradeoff.",
                    "label": 0
                },
                {
                    "sent": "And this is what we'd like to study.",
                    "label": 0
                },
                {
                    "sent": "By studying the regret of this setting in this setting.",
                    "label": 0
                },
                {
                    "sent": "So at this stage you're probably wondering about.",
                    "label": 0
                },
                {
                    "sent": "OK, other guy is just crazy is going to talk about yet another variation of bandit problems, right?",
                    "label": 1
                },
                {
                    "sent": "So is there any real difference to bandit problems here?",
                    "label": 0
                },
                {
                    "sent": "So that's the first question.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to to look into, but before that let me just recap the framework with this figure of what we're talking about.",
                    "label": 0
                },
                {
                    "sent": "So we have the environment and the learner and the referee, and so the game works as before environment choose an outcome, learner chooses inaction, both communicated to the referee.",
                    "label": 0
                },
                {
                    "sent": "And the referee.",
                    "label": 0
                },
                {
                    "sent": "Actually in this case computes two things.",
                    "label": 0
                },
                {
                    "sent": "It computes the loss and the price.",
                    "label": 0
                },
                {
                    "sent": "You know the loss in two hits, a little book.",
                    "label": 0
                },
                {
                    "sent": "And it also computes the feedback and the loss is not communicated back to the learner, but only the feedback is communicated back to the learner and there was one more very important thing ahead of time.",
                    "label": 0
                },
                {
                    "sent": "Both L&H are publicly known in the other game, it was only added was publicly known.",
                    "label": 0
                },
                {
                    "sent": "Right now we have Allen age, these two functions, and then you're asked how to play this game to minimize the regret.",
                    "label": 0
                },
                {
                    "sent": "And is this game any different than dangerous bandit games?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, OK so the mathematical framework it's like the same as before, except that we don't see the outcome chosen by the environment and both feedback and the loss is computed and we only see the feedback and otherwise it's the same.",
                    "label": 0
                },
                {
                    "sent": "Except that right now the game is governed by these two functions.",
                    "label": 1
                },
                {
                    "sent": "OK. OK. Wow, OK. Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Nope, Nope, that's the thing.",
                    "label": 0
                },
                {
                    "sent": "That's the thing.",
                    "label": 0
                },
                {
                    "sent": "So somehow, implicitly through age, you're learning about the feedback, like in the case of dynamic pricing, you know that if I over priced, the feedback is 1.",
                    "label": 0
                },
                {
                    "sent": "If I underprice, the feedback is 0, so you're learning about the unknown outcome through the feedback.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK good, please stop me.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, something is unclear.",
                    "label": 0
                },
                {
                    "sent": "I can make this lexer.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to skip some of these examples for the sake of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Saving some time or just go to let's say this game, so there's a game like a game can be also like if it's finite, you have finitely many choices, then the two functions obviously can be represented with two matrices, and all learner is going to be always the role player, and the environment is going to be the column player.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we have three actions and the environment or opponent has two actions, right?",
                    "label": 0
                },
                {
                    "sent": "And then it's like it's a.",
                    "label": 0
                },
                {
                    "sent": "This is the loss function represented as a matrix, and then this is the feedback represented as a matrix and hear different letters denote different symbols and that's that's it.",
                    "label": 0
                },
                {
                    "sent": "So there are symbols, so they're off about.",
                    "label": 0
                },
                {
                    "sent": "There's an awful weather.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first action.",
                    "label": 0
                },
                {
                    "sent": "It's like penny matching, right?",
                    "label": 0
                },
                {
                    "sent": "So the environment chooses something and then if I happen to choose the same thing then I suffer a loss, right?",
                    "label": 0
                },
                {
                    "sent": "So I want to choose the opposing action, then what the opponent chooses.",
                    "label": 0
                },
                {
                    "sent": "But the only way I can learn because well, if I choose the first action.",
                    "label": 0
                },
                {
                    "sent": "I only got to see the same symbol a no matter what the opponent chooses, so I received no information, so information right?",
                    "label": 0
                },
                {
                    "sent": "The same with the second action.",
                    "label": 0
                },
                {
                    "sent": "I only got to see B if I choose the second action independently of opponent choice, whatever the third action in that case.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it seems that I'm always going to suffer the loss of 1.",
                    "label": 0
                },
                {
                    "sent": "The constant no matter what the opponent has chosen, so it's kind of pricey action, but I can make a distinction between the two choices of the opponent.",
                    "label": 0
                },
                {
                    "sent": "C&D are unequal, right?",
                    "label": 0
                },
                {
                    "sent": "So I have a way of learning about the.",
                    "label": 0
                },
                {
                    "sent": "The old comes, but I have to pay a price.",
                    "label": 0
                },
                {
                    "sent": "So the question is OK, is this any harder than the usual games like the bandit and other games?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's very important.",
                    "label": 0
                },
                {
                    "sent": "It's I cannot emphasize it sufficiently.",
                    "label": 0
                },
                {
                    "sent": "Many times LNH are known as head of time.",
                    "label": 0
                },
                {
                    "sent": "Like if you would observe them, you know like after finitely many tries, like if you are observing them without noise, you would figure them out and then you would play the same way.",
                    "label": 0
                },
                {
                    "sent": "So it's actually not a big assumption.",
                    "label": 0
                },
                {
                    "sent": "But just think about a case when Alan H. Are known ahead of the time they are given to you and I ask you like how you play this game.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, there are lots of other examples.",
                    "label": 0
                },
                {
                    "sent": "Bandits are an example as far right, so when the feedback is the actual loss.",
                    "label": 0
                },
                {
                    "sent": "That we suffer well then in that case L&H these two metrics are just equal, right?",
                    "label": 0
                },
                {
                    "sent": "So if I choose something, opponent chooses something.",
                    "label": 0
                },
                {
                    "sent": "My feedback is my loss.",
                    "label": 0
                },
                {
                    "sent": "So the two matrices are the same, so there is something going on with the structure of these matrices.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So we'll get into that like to assume that the environment is an oblivious adversary.",
                    "label": 0
                },
                {
                    "sent": "Uh, what we'll see that like later on, when it comes to the results, I will take back from this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generate a little bit.",
                    "label": 0
                },
                {
                    "sent": "But otherwise we'd like to work about the adversary case, like dynamic pricing, so it is how dynamic pricing can be formalized, but you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And all this so alright?",
                    "label": 0
                },
                {
                    "sent": "So what is known about this games?",
                    "label": 0
                },
                {
                    "sent": "And I'm like I'd like to illustrate this with a figure and so this is the the spectrum of the games as far as the hardness of the games is concerned.",
                    "label": 0
                },
                {
                    "sent": "So here are the games like on this segment between 01.",
                    "label": 0
                },
                {
                    "sent": "I want to put like every game which is encoded by these two matrices.",
                    "label": 0
                },
                {
                    "sent": "Like let's say Ellen age.",
                    "label": 0
                },
                {
                    "sent": "And this segment somewhere.",
                    "label": 0
                },
                {
                    "sent": "And if I put the game here, that means that the regret scales like order one you have constant regret or no regret at all.",
                    "label": 0
                },
                {
                    "sent": "So in that case, for those other games when opponents choice doesn't actually matter, that games like that stupid, not very interesting games the other very uninteresting games is that when you don't have information or doesn't matter what you do, you will suffer a linear regret you actually there games like that.",
                    "label": 0
                },
                {
                    "sent": "So just imagine that the opponent has these two actions.",
                    "label": 0
                },
                {
                    "sent": "It makes a difference of which action the opponent chooses, but you received no information.",
                    "label": 0
                },
                {
                    "sent": "Of course you are.",
                    "label": 0
                },
                {
                    "sent": "That's going to scale linearly, so we know that we can put some games here, and we can put some games there.",
                    "label": 1
                },
                {
                    "sent": "And we also know that full information games actually lie here.",
                    "label": 0
                },
                {
                    "sent": "The regret, the way it's chaos with time is square root, T right for full information.",
                    "label": 0
                },
                {
                    "sent": "Again, so that those are the games.",
                    "label": 0
                },
                {
                    "sent": "When re you receive the choice of the.",
                    "label": 0
                },
                {
                    "sent": "Overlap, and there's been a lot of previous work on this, so we pretty well understand that those games belong to that.",
                    "label": 0
                },
                {
                    "sent": "So in that case you know each has a special structure.",
                    "label": 0
                },
                {
                    "sent": "It's like age and codes like the opponents choices.",
                    "label": 0
                },
                {
                    "sent": "Also, bandit games.",
                    "label": 0
                },
                {
                    "sent": "We understand them pretty well.",
                    "label": 0
                },
                {
                    "sent": "And we know that they should actually be placed at the same spot, right?",
                    "label": 0
                },
                {
                    "sent": "So as far as the scaling of the regret is concerned, as a function of time there as hard as full information games.",
                    "label": 0
                },
                {
                    "sent": "Alright, and we also know because of some lower bounds.",
                    "label": 0
                },
                {
                    "sent": "It's pretty interesting that there are no games in this open interval between 0 and 1/2.",
                    "label": 1
                },
                {
                    "sent": "OK, so either game is, you know, trivially easy or it's at least this hard.",
                    "label": 0
                },
                {
                    "sent": "There is nothing in between.",
                    "label": 0
                },
                {
                    "sent": "It's kind of fascinating.",
                    "label": 0
                },
                {
                    "sent": "And then there's been other work on this or previous work, and this passion monitoring problems.",
                    "label": 0
                },
                {
                    "sent": "It was actually first picked up by forgive me if I say the name, rank, pico, Bonyan, Schindler Power and they studied this games and later on cheese of Bianchi thoughts and logo.",
                    "label": 1
                },
                {
                    "sent": "She actually studied this games and together if you put the two papers together they proved that there are no games.",
                    "label": 0
                },
                {
                    "sent": "In this open interval.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's an empty interval as well, so that's the boundary at two surd.",
                    "label": 0
                },
                {
                    "sent": "And actually they showed that the revealing action game that we talked about is at 2:30.",
                    "label": 1
                },
                {
                    "sent": "It's hard when you have to pay an extra cost for gaining information.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that game is actually harder.",
                    "label": 0
                },
                {
                    "sent": "Alright, but this leaves open a couple of questions, isn't.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where is dynamic pricing, for example, so we talked about this little game and you're trying to say something and you have limited information.",
                    "label": 1
                },
                {
                    "sent": "So where is that like?",
                    "label": 0
                },
                {
                    "sent": "Where should I put this on this figure?",
                    "label": 1
                },
                {
                    "sent": "So for revealing action game, we happen to know that it should be there, but there's just one game like there are so many other games.",
                    "label": 0
                },
                {
                    "sent": "So what about dynamic pricing?",
                    "label": 0
                },
                {
                    "sent": "And like generally, if you have a game which is encoded with these two matrices, where should we put it?",
                    "label": 1
                },
                {
                    "sent": "Shouldn't there be a characterization that, like just you look at these two metrics and you say that, oh, that game should be, let's say at 1/2?",
                    "label": 0
                },
                {
                    "sent": "Or maybe it's in between exactly 1/2 and two third, or what?",
                    "label": 0
                },
                {
                    "sent": "And Lastly, but very importantly, like if I give you a game which is specified by these two matrices, how do you want to play it right?",
                    "label": 0
                },
                {
                    "sent": "So it depends on where the game is going to lend on this segment, right?",
                    "label": 0
                },
                {
                    "sent": "So you you there are different algorithms like there is there agmc variations.",
                    "label": 0
                },
                {
                    "sent": "For games which are here, they are somewhat different than games that are over there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the question mark should be much bigger.",
                    "label": 0
                },
                {
                    "sent": "So there is a huge question mark here.",
                    "label": 0
                },
                {
                    "sent": "Imagine that that we just don't know if there are any games in between these two points, and where exactly a game should land.",
                    "label": 0
                },
                {
                    "sent": "And that's the question we'd like to answer.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so our contribution is.",
                    "label": 0
                },
                {
                    "sent": "Being in the past that we classified some special keys is for adversary outcomes, but today I'd like to talk about when the adversary is actually stochastic, so we found it very difficult to think about the adversary setting.",
                    "label": 0
                },
                {
                    "sent": "We didn't have much ideas, so we said OK, why not simplify?",
                    "label": 0
                },
                {
                    "sent": "So we simplify the problem and so we're looking at the stochastic setting when the opponent chooses a fixed distribution over the outcomes, which you don't see.",
                    "label": 0
                },
                {
                    "sent": "And you have to learn about that distribution, and in pretty much other cases that we know of the adversary and stochastic settings were kind of the same way.",
                    "label": 0
                },
                {
                    "sent": "So if you crack the stochastic setting, then we figured that we will have some idea about how to correct the adversary settings.",
                    "label": 0
                },
                {
                    "sent": "So from now on I'm going to talk about the stochastic setting.",
                    "label": 0
                },
                {
                    "sent": "It's like you know the difference between stochastic bandit problems and adversary bandit problems.",
                    "label": 0
                },
                {
                    "sent": "Alright, and the result that I'd like to present in a not charge says that yes, there are no other cases.",
                    "label": 0
                },
                {
                    "sent": "They're just like these four cases that trivia games, hopeless games, easy games, and hard games.",
                    "label": 0
                },
                {
                    "sent": "Ankola game easy if it southeast is a banded game, and I call it hard if it's as hard as labor official.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how are we going to get here?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "So the results.",
                    "label": 0
                },
                {
                    "sent": "So actually let's think a bit about.",
                    "label": 0
                },
                {
                    "sent": "How can you actually learn?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "How can you actually?",
                    "label": 0
                },
                {
                    "sent": "So we are in the stochastic setting, so the opponent has chosen some probability distribution.",
                    "label": 0
                },
                {
                    "sent": "We'd like to learn about that probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's think about a simple case when you when you keep choosing the same action all the time.",
                    "label": 0
                },
                {
                    "sent": "So what is an information that we can gain?",
                    "label": 0
                },
                {
                    "sent": "And here is a simple illustration for this.",
                    "label": 0
                },
                {
                    "sent": "So the outcomes are like, you know, Banana, Apple, banana and strawberry for the four.",
                    "label": 0
                },
                {
                    "sent": "Choices of the opponent.",
                    "label": 0
                },
                {
                    "sent": "OK for your chosen action, and these are the probabilities.",
                    "label": 0
                },
                {
                    "sent": "We switch.",
                    "label": 0
                },
                {
                    "sent": "The opponent is going to choose this outcomes.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if you think about just for a sack, if you keep choosing this action, but you can learn is the probability of seeing a banner or the probability of seeing an Apple and the probability of seeing at strawberry.",
                    "label": 1
                },
                {
                    "sent": "So you can learn another probability distribution.",
                    "label": 0
                },
                {
                    "sent": "You are not going to learn exactly.",
                    "label": 0
                },
                {
                    "sent": "B but you learn different probability distribution that's derived from P, right?",
                    "label": 1
                },
                {
                    "sent": "So it's in the keys, the probability that the feedback is banana.",
                    "label": 0
                },
                {
                    "sent": "It's obviously just the sum of 0.1 and 0.2 right?",
                    "label": 0
                },
                {
                    "sent": "And the probability that the feedback is Apple is exactly 0.6, and the probability that the feedback is strawberry is what it should be.",
                    "label": 0
                },
                {
                    "sent": "0.1 alright and.",
                    "label": 0
                },
                {
                    "sent": "You can simply write the probability of the feedback being equal to some outcome as the sum of the individual.",
                    "label": 0
                },
                {
                    "sent": "The probabilities of the individual outcomes using these indicator functions right so you like you have to sum up like for which cases are going to see the same outcome.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can write this in a matrix form, so you just see kind of a projection of the unknown probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And this SI matrix it's like an indicator matrix.",
                    "label": 0
                },
                {
                    "sent": "It's going to have as many rows as the different feedback types you have for that action, and it has as many columns as the number of outcomes is.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this signal matrix is going to play a crucial role in our characterization of the hardness of these games, right?",
                    "label": 1
                },
                {
                    "sent": "So what it does again is that if you keep choosing that action.",
                    "label": 0
                },
                {
                    "sent": "The probability distribution that you see over the feedbacks.",
                    "label": 0
                },
                {
                    "sent": "Can be obtained from the unknown probability distribution by multiplying that probability distribution by this signal matrix.",
                    "label": 0
                },
                {
                    "sent": "That's the ultimately very important property of this matrix, and this matrix is going to play a crucial role.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or the next simple question, if I give you 2 actions, I and I prime, can we differentiate between the two actions right?",
                    "label": 1
                },
                {
                    "sent": "So if their losses underlying the two actions would be different for the property distribution chosen by the opponent, it would be crucial to differentiate between them.",
                    "label": 0
                },
                {
                    "sent": "So talking about the simplest case so a little bit more notation.",
                    "label": 0
                },
                {
                    "sent": "So if Ally is the last vector that underlies these action I. OK. We want to figure out.",
                    "label": 1
                },
                {
                    "sent": "Just the sign.",
                    "label": 0
                },
                {
                    "sent": "Of of this quantity here, because that would tell us if action I has a smaller loss or action.",
                    "label": 1
                },
                {
                    "sent": "I prime has a smaller loss because Ally transpose P is the expected loss under action.",
                    "label": 0
                },
                {
                    "sent": "I'll I prime transpose, P is the expected loss and an action I prime.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to figure out this sign.",
                    "label": 0
                },
                {
                    "sent": "So it's like a classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "Uh, and So what is the information that we can receive?",
                    "label": 0
                },
                {
                    "sent": "So imagine that you keep using these actions, then your information you receive is, you know, this projection of P underlying side.",
                    "label": 0
                },
                {
                    "sent": "And as I prime these two signal matrices, so you can actually stack the two matrices and then you can.",
                    "label": 1
                },
                {
                    "sent": "See that the bad case for you, in which case you will not be able to figure out which loss which action is better or worse, is when there exists two probability distributions in the probability simplex such that the feedback underlying the two probability distribution is the same even if you pull the arms like or choose these actions infinitely many times and the signs are different.",
                    "label": 0
                },
                {
                    "sent": "OK, like it makes a difference.",
                    "label": 0
                },
                {
                    "sent": "If the opponent actually chooses P1 and P2 in terms of your losses, this is the bad case that therefore I call it BC.",
                    "label": 0
                },
                {
                    "sent": "Not refering to, you know.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm from Alberta.",
                    "label": 0
                },
                {
                    "sent": "So the fundamental lemma is this and due to lack of time, we're not going to the details.",
                    "label": 1
                },
                {
                    "sent": "But the lemma says that the bad keys happens exactly if and only if the difference between the loss factors.",
                    "label": 0
                },
                {
                    "sent": "Underlying these two actions, I and I prime or loss factors doesn't lie in the image space of the concatenated signal matrix transposed OK, and this little linear children I just show 1.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Direction of the proof so you can have a talk with other proof, right?",
                    "label": 0
                },
                {
                    "sent": "This is going to be really useful for the upper bound construction, and so the way it works is that.",
                    "label": 0
                },
                {
                    "sent": "We want to prove this direction and we prove it by by contradiction.",
                    "label": 0
                },
                {
                    "sent": "So we prove actually that if the last difference lies in this image space, then this implies that for any P distribution you can figure out the sign of this thing.",
                    "label": 1
                },
                {
                    "sent": "Actually, you can actually figure out that quantity, so you can figure out how big is the gap between the losses of the two actions.",
                    "label": 0
                },
                {
                    "sent": "And this is really easy to see, so let's assume that this holds.",
                    "label": 0
                },
                {
                    "sent": "If this holds, and that means that there is some vector V such that the last difference can be expressed in this form, right?",
                    "label": 0
                },
                {
                    "sent": "It's very simple, and if that's the case, well, you just multiply from right by fee and you are done alright.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be the basis for the algorithm that we are going to use, because what it has you is that well, if this condition holds then you can figure out this vector V. And you can estimate loss differences in that awesome, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't actually have to see the losses, but you can estimate that you can figure them out by playing actions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I and I prime.",
                    "label": 0
                },
                {
                    "sent": "OK, so the other direction is a little bit more cumbersome and click requires a little bit of linear algebra, but it's it fits a slide, so it's not very common.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Located but I don't have time to go into the details.",
                    "label": 0
                },
                {
                    "sent": "I just showed the end result so the other direction says that if the last difference is not in this image space then the bad case holds and that means that you can actually find OK.",
                    "label": 0
                },
                {
                    "sent": "So here is the probability simplex for the all the outcomes.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we have three outcomes so I can throw this probability simplex in two dimension.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I have two actions action ion action, I prime and so here is the boundary between like when a probability distribution lies in this section of the simplex then action I is the winner.",
                    "label": 0
                },
                {
                    "sent": "It gives you the lowest cost.",
                    "label": 0
                },
                {
                    "sent": "In the other case action I prime is the winner.",
                    "label": 0
                },
                {
                    "sent": "So you can decompose the probability simplex you know in these two halves in this case and what you can show is that if this case doesn't hurt that there exists APO.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution that's exactly on the boundary, and you can perturb it by a small quantity.",
                    "label": 0
                },
                {
                    "sent": "Where we is in the kernel of this matrix such that this normalized equality holds such that one of these guys.",
                    "label": 0
                },
                {
                    "sent": "Is actually an in 1/2 where action I is the winner in the other guy is in the other half an action action I prime is the winner, so you recover the bad case so you figure out that you have two probability distributions indeed.",
                    "label": 0
                },
                {
                    "sent": "Are such that the true probability distributions because of this conditions they will look identical to you if you just keep using I and I prime.",
                    "label": 0
                },
                {
                    "sent": "You will not be able to distinguish between them.",
                    "label": 0
                },
                {
                    "sent": "And they they lie in different parts of the simplex.",
                    "label": 0
                },
                {
                    "sent": "So therefore if the opponent keeps playing between these two, you are kind of like very limit like you don't actually have information for figuring out which is the case.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And kind of don't situation.",
                    "label": 0
                },
                {
                    "sent": "So let's talk a little bit more about this side of composition so you can keep doing this thing when you introduce more and more actions and then eventually you can decompose the probability simplex into these polygons such that over a Polygon Polygon belongs to an action where the action is actually the winner, keeps it the lowest loss.",
                    "label": 0
                },
                {
                    "sent": "If the unknown property distribution lies.",
                    "label": 0
                },
                {
                    "sent": "Into that part of the.",
                    "label": 0
                },
                {
                    "sent": "The composition and your job is by playing, you know, the actions to figure out where the unknown probability distribution lies, and you can see that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you the actions give you information about certain directions in this space and and so that's how you're going to play.",
                    "label": 0
                },
                {
                    "sent": "OK, so one important concept here is the concept of neighboring actions.",
                    "label": 1
                },
                {
                    "sent": "So two actions are neighboring.",
                    "label": 1
                },
                {
                    "sent": "If in this side of composition their neighbors.",
                    "label": 0
                },
                {
                    "sent": "So there is this geometrical concept.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the lower bound I pretty much explained.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ready, so if there exists two actions which are neighboring decider composition such that the last difference vector doesn't lie in the image space of this matrix of the composed signal matrix, then you can prove that the regret is going to scale like T to the two 3rd.",
                    "label": 0
                },
                {
                    "sent": "And this works in the diverse are cases file of course, because if it works in the stochastic as it works in the adversary case tool and the proof is just follows very simply by the previous arguments.",
                    "label": 0
                },
                {
                    "sent": "So you can come up with this P1 NP.",
                    "label": 0
                },
                {
                    "sent": "Two answer and so forth.",
                    "label": 0
                },
                {
                    "sent": "And if there is 1/3 action if there is no third action that the game is whole class, if there is 1/3 action, it will be costly.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause it's further away.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have this action there and the opponents playing, playing either this or that.",
                    "label": 0
                },
                {
                    "sent": "This is just too costly.",
                    "label": 0
                },
                {
                    "sent": "It's not within an optimal region, so you're going to pay a high price for it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a large one, so for the upper bound, so let's take a look at the regret.",
                    "label": 1
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "We are going to use the notion of expected regret in this case, so you have.",
                    "label": 0
                },
                {
                    "sent": "The sum of the losses that you suffer and you subtract the minimum of the expectation of the some of the losses for the meaning was taken over all possible actions, and then there is this classic rather composition where you can write the regret in terms of you know, summing over all actions, how many times you're playing in action and how much does it cost to play that action.",
                    "label": 1
                },
                {
                    "sent": "So all four I I prime here is the gap between the.",
                    "label": 1
                },
                {
                    "sent": "The losses are faction I and I prime and to lie is a number of times an algorithm would play action.",
                    "label": 1
                },
                {
                    "sent": "I and I started would be just an optimal action and the.",
                    "label": 0
                },
                {
                    "sent": "Distribution chosen by the opponent.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's a classic.",
                    "label": 0
                },
                {
                    "sent": "Regret their composition and because of this, you know that all you have to do is to keep the number of times you're playing actions which have a higher gap smaller number of times, right?",
                    "label": 1
                },
                {
                    "sent": "So how can we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, the very simple idea is just as trying to estimate these gaps.",
                    "label": 1
                },
                {
                    "sent": "And when it becomes obvious how big is the gap like, the sign becomes obvious, right?",
                    "label": 0
                },
                {
                    "sent": "So it's relatively relative error estimate is good, like over 1/2 or sorry below 1/2.",
                    "label": 0
                },
                {
                    "sent": "Then just animate this suboptimal action because done design.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea of the R.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gotham and so how to contract the good estimates we already had partially answer on the slides, so show that for every neighboring action I and I prime we have this condition satisfied.",
                    "label": 1
                },
                {
                    "sent": "So we want to exploit this condition to construct an algorithm that actually achieves a good regret.",
                    "label": 0
                },
                {
                    "sent": "So we want to show that under this condition the regret is quietly.",
                    "label": 1
                },
                {
                    "sent": "So how to exploit this?",
                    "label": 0
                },
                {
                    "sent": "Well, if the two actions are actually neighbors.",
                    "label": 0
                },
                {
                    "sent": "And this condition is met.",
                    "label": 0
                },
                {
                    "sent": "We know that there is this underlying vector such that you can express the loss differences in this form and then you know if we already saw this that like if you multiply from the right from the known probability distribution, you will get an estimate of the gap between the pay offs of the two actions, and so the good thing is that if you do that, you have to transpose the whole thing.",
                    "label": 0
                },
                {
                    "sent": "You are going to have.",
                    "label": 0
                },
                {
                    "sent": "Actually it's here estimate.",
                    "label": 0
                },
                {
                    "sent": "So because this is the signal matrix, and as I said at the beginning, the signal matrix is constructed in such a way that you can actually estimate this quantity if you keep choosing I and I prime, you're going to estimate it quantity, place that product with the estimates and that's your estimate.",
                    "label": 0
                },
                {
                    "sent": "OK, so in other words, compute the relative frequencies of outcomes and direction I and I prime that's Q hat and justice use Q hat in place of this product.",
                    "label": 0
                },
                {
                    "sent": "OK, that's your estimate of that projection.",
                    "label": 1
                },
                {
                    "sent": "And that's going to converge fast.",
                    "label": 0
                },
                {
                    "sent": "And then if actions are not neighbors, you still have to estimate the gaps.",
                    "label": 0
                },
                {
                    "sent": "Whether you do like you find a chain that connects the actions through through neighboring actions and you can show that the chain always exists, and then you are chaining and it's just a telescoping sum, so it's going to be a good estimate.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and as I said before, you want to stop using an action when you can decide about the sign of the estimates, and for that you need maybe to use versions inequality.",
                    "label": 1
                },
                {
                    "sent": "Actually you have to use bastions inequality or to stop early enough.",
                    "label": 1
                },
                {
                    "sent": "And then you can prove that you are going to stop.",
                    "label": 0
                },
                {
                    "sent": "In time, that's inversely proportional to the size of the gap between the two actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so if there is a positive gap, then you stop this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is what the algorithm dies and then if you go back to the regret decomposition.",
                    "label": 0
                },
                {
                    "sent": "So when the stopping rule doesn't fair then you can just replaced Olaf.",
                    "label": 1
                },
                {
                    "sent": "I buy this upper pond and if you do that that guy just cancel it and then you can decompose the sum into two parts.",
                    "label": 0
                },
                {
                    "sent": "The one part where the gaps are big.",
                    "label": 0
                },
                {
                    "sent": "The one part that the gaps are small.",
                    "label": 0
                },
                {
                    "sent": "Your per bombos terms choose off a zero in the way to optimize the bondan you get this.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Red with the band.",
                    "label": 0
                },
                {
                    "sent": "That's how it works.",
                    "label": 0
                },
                {
                    "sent": "OK, so once they are, get them so it's called Balaton.",
                    "label": 0
                },
                {
                    "sent": "To advertise called that's going to happen very soon.",
                    "label": 0
                },
                {
                    "sent": "Very close to Balaton which is a Lake in Hungary, so it's bandit based Lausanne elation and the algorithm works like this.",
                    "label": 0
                },
                {
                    "sent": "You repeat the following.",
                    "label": 0
                },
                {
                    "sent": "Why there is more than one alive action, so you have you know you start with all actions and then you categorize them.",
                    "label": 1
                },
                {
                    "sent": "Being alive with that.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 1
                },
                {
                    "sent": "So you keep trying the alive actions estimate.",
                    "label": 0
                },
                {
                    "sent": "Loss difference is sort of a.",
                    "label": 1
                },
                {
                    "sent": "It was said before and when you can, when you see that some estimates substantially different from zero, eliminate one of the actions and then once only one actions remains, keep pulling that action until the end.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type OK and with this you can actually show.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This theorem that was advertised before and so in summary right now, for finite stochastic games we could complete the picture.",
                    "label": 1
                },
                {
                    "sent": "There are no games, no games in between 1/2 and two third.",
                    "label": 0
                },
                {
                    "sent": "We have a full character characterization.",
                    "label": 0
                },
                {
                    "sent": "We just give Me 2 matrices.",
                    "label": 0
                },
                {
                    "sent": "I can very cheaply compute valid game belongs.",
                    "label": 0
                },
                {
                    "sent": "I can tell you which are get them to use.",
                    "label": 1
                },
                {
                    "sent": "In this case you should use a different target and in this case and dynamic pricing is actually at 2 third.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately it's hard game.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Alright, so conclusions so we got this full characterization and we got this very interesting concept that we have geometria naughty bra and it's pretty fascinating.",
                    "label": 0
                },
                {
                    "sent": "So big question is does this extend to the adversarial setting?",
                    "label": 1
                },
                {
                    "sent": "We believe it does, but we haven't proved.",
                    "label": 0
                },
                {
                    "sent": "OK, will you like what is missing because the lower bound actually clears half of the problem.",
                    "label": 0
                },
                {
                    "sent": "What is missing is a good algorithm that achieves quadrati regret in the easy case when we have this condition map, there are lots of other questions that we'd like to explore and and I think that there is a relationship between a large Shamir stalk.",
                    "label": 0
                },
                {
                    "sent": "I don't know if he's here and this work is far.",
                    "label": 0
                },
                {
                    "sent": "It's all about controlling the variance, so bottom might not be the.",
                    "label": 0
                },
                {
                    "sent": "Only algorithm that that's able to do this.",
                    "label": 0
                },
                {
                    "sent": "It just happens that we were able to prove for these algorithms that you can achieve square root of T regret for easy games, but we don't know actually if the same would work for FedEx Pre if you know do the expiration and more clever way let's say.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that thank you.",
                    "label": 0
                }
            ]
        }
    }
}