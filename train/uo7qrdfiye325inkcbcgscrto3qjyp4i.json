{
    "id": "uo7qrdfiye325inkcbcgscrto3qjyp4i",
    "title": "Multi-View Dimensionality Reduction via Canonical Correlation Analysis",
    "info": {
        "author": [
            "Sham M. Kakade, Microsoft Research New England, Microsoft Research"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/lms08_kakade_mvdr/",
    "segmentation": [
        [
            "Alright, this is Sean packet as joint work where the Dean Foster and Tong Zhang actually, it really came out of two different lines of work.",
            "Some work that may indeed and I have been doing for awhile and then some work tongue Jang.",
            "An endo had been doing for awhile.",
            "I realize we're very similar so we decided to combine them and."
        ],
        [
            "About the implications.",
            "So I think at this stage it's fair to say we really have a pretty good understanding of supervised learning, but what many of us think is the pressing question is how do we find good features or kernels or subspaces or hypothesis classes of the input?",
            "SpaceX that are predictive of some target variable Y. OK, so we'd like to find predictive representations of X that useful for predicting target.",
            "Why, and I think realistically, we just don't have enough label data to do this, and but we do, however, have orders of magnitude more unlabeled data, an ideally we'd like to learn relevant representations of X with the unlabeled data, and in particular, if you think about what's going on here is we're really interested in this conditional distribution P of Y given X, but we're trying to get this conditional distribution with only the marginal distribution of X.",
            "And why should the marginal distribution of X contain any relationship?"
        ],
        [
            "With this conditional distribution, so I think that's really the underlying question that people are trying to address in semi supervised learning, and the multiview approach we're going to try to exploit the structure between the views to get at this relationship.",
            "So in particular setting we're going to be considering is when we have an input vector X which is broken into viewers, so X is X1 and X2, and these are views that are potentially in.",
            "I mean, it could be pretty abstract.",
            "And we should think about this.",
            "The dimensionalities of these two views being arbitrary large, so we really, you know, like in machine learning we really tossed the kitchen sink into these views.",
            "So we made a very rich representations in these views he put in a lot of features and maybe they correspond to some kernel.",
            "The dimensionality is very large.",
            "We're just in predicting some target variable Y, and I think the intuition is that.",
            "The relationship between these views should reveal some information about the target Y, so in particular we have this marginal distribution between X1 and X2, and we're going to be easy to obtain samples from this motion of distribution and with those samples we'd like to understand the relationship between these two variables and use that relationship to predict or target variable Y, and we saw some examples from Karen why this might be possible, where one view is a picture and the other view is speech.",
            "We can think about an object recognition.",
            "Scenario One View is a picture of the object from 1 camera angle of the other view is a picture of the camera is a picture of the object from a different camera angle.",
            "And intuitively these views these different views are projecting giving us examples of viewpoint invariants and just based on these different views we might like to understand this concept of viewpoint in view."
        ],
        [
            "It's which we realize predicting the the identity of this object shouldn't depend on.",
            "OK, so the question is what structure is useful?",
            "In this joint distribution, such that we can come up with.",
            "So we can really understand when we can exploit the unlabeled data.",
            "And in this talk I'm really going to consider two assumptions and we consider them separately their assumptions that were made in the original code training paper, and I think other people have considered.",
            "Then we're going to weaken them quite a bit and we are going to consider them separately.",
            "So the first assumption is this conditional independence assumption, where we say that the two views are independent given Y and.",
            "This assumption is actually falling into a bit of disrepute because it's a little too strong to assume that X one X2 are going to be independent when you just condition on this very low dimensional target variable.",
            "So I'm going to start with this assumption, but I'm going to relax it and show that it really has a very natural relaxation and then the second assumption is going to be a redundancy based assumption which was something Dean and I considered a while back, which is really that predicting the target variable.",
            "Why would just X one is roughly going to be as good as putting the target variable with both views?",
            "Which is possible in many settings like if I have a this object from two different camera angles, I can predict what it is from either camera angle and the point is with other these two assumptions.",
            "What's going to turn out to be the case is the only features that are going to be useful for predicting the target variable are going to be those features in each view that are correlated with each other so.",
            "So that's the basic idea, and the reason this is very important is we can significantly reduce the label of something complexity because we can basically learn this correlation with unlabeled data and the theorems we're going to have.",
            "The form is that we're not going to lose any predictive information by doing this projection, so the so that's going to be the gist of this talk, and really trying to formalize what these assumptions of."
        ],
        [
            "OK, so again since the outline so quickly go through well, I'll go through the assumptions of conditional independence, independence and I relax it to a notion of independence based on hidden States and I think this idea is really capturing an idea that's less explored in the multi view learning.",
            "So there's been a lot of work on redundancy in core regularization, but the independence assumption hasn't really been studied that much, but I really think it's pretty natural.",
            "And then I'll discuss the redundancy assumption.",
            "And then I'll briefly discuss in pulmonary experiments from Wikipedia on would sense disambiguation."
        ],
        [
            "That I've been doing.",
            "OK, so before we start to just refresh your memory on some basic statistics.",
            "Is it any questions definitely interrupt me and there's more seats in the front.",
            "People want to kind of come up and sit on.",
            "Feel free to just barge in.",
            "OK, so.",
            "Throughout the talk, for simplicity, I'm going to assume that the conditional mean of Y given X is linear.",
            "In X.",
            "We can relax this later, but for simplicity let's just assume that all these conditional means are linear in X and we called the definition of the R-squared between X&Y is just the correlation between this best linear predictor beta dot X&Y squared, so it's just one minus the loss of beta over the variance of Y, and the words are squared is just the.",
            "Fraction of the variance in Y that can be explained if you know X.",
            "So if the R-squared is 1, then you have a perfect prediction of the target variable, an if the R-squared is 0, then using the X has no predictive power of the target variable.",
            "So if it's in, if we scale, variance of would be one in the US.",
            "Squared is just one minus loss, but I think it's a natural quantity to state the assumptions in terms of.",
            "OK, so.",
            "Like"
        ],
        [
            "OK. OK, so now let's look at what I think is the right way to think about conditional independence.",
            "Because you know, like in the original training paper, why was this binary target variable?",
            "And it's kind of completely unreasonable to think that these two high dimensional views are going to be independent condition on this binary target variable.",
            "But we're going to do is we're going to think about going to define H to be a hidden state for X1X2 and Y if we have the property that condition on H. These three are going to be independent.",
            "And hidden Caesar very natural because we have this entire community of graphical models.",
            "Really, really trying to exploit the fact that there are hidden states in the world and would hidden states.",
            "Do they really imply certain conditional independence properties and?",
            "To some degree, if I just assume that we have a hidden state, this is without loss of generality, 'cause we can always find a hidden state such that this property is satisfied for any distribution.",
            "We can definitely find some hidden state that satisfies.",
            "For Kate to be the dimensionality of the hidden state, I mean the many settings are hidden signature also.",
            "In the last example, what would be the hidden state between these two views that made them independent?",
            "Well, it's probably not just the speaker identity, it's probably the speaker identity.",
            "Plus maybe a few phonemes and what they're saying, but it's definitely.",
            "It's definitely still, you might imagine to be much lower dimensional signal than the target variable.",
            "And you know, in time series the natural hidden states or something at a particular time, port, time point which makes the past and future independent.",
            "OK?",
            "So then what's the natural assumption 'cause we haven't lost any?",
            "We really haven't lost anything by thinking about hidden states, 'cause it always exists.",
            "So the assumption is going to be that are hidden views so that our view is X1 and X2 are both going to be nontrivially predictive of the hidden state, and what I mean by this is for every direction WER squared between X1 and WH is greater than 0.",
            "And similarly, for four of you two that the R-squared between X2 and WH is greater than zero.",
            "So in words, what this means is that each view is at least somewhat correlated with every coordinate in our hidden state.",
            "So in this kind of addresses, I think this parity question that if you think about a hidden state where so let's say why was going to be X1 plus X2 and the hidden state now would have to contain X1, X2 in it.",
            "And this is a bad case to deal with because X1 doesn't have might not have any information about X2, so it's not predicted that that coordinates.",
            "The assumption really is going to be that each view is at least somewhat predictive of each part of the hidden state, which is.",
            "I think the natural assumption the multiview setting that we have, this hidden state and somehow it isn't really split between the views, it's at the views.",
            "Both views really contain some correlated information about the hidden state, but it's weak because I'm not saying that there are good predictions of the hidden state, I'm just saying they're correlated with the hidden state."
        ],
        [
            "OK. Now with this we can get the.",
            "The main theorem for conditional independence.",
            "So.",
            "Let's define this projection operator.",
            "PY to take X1 and projected to the correlated subspace.",
            "So what I mean by this is going to project X1 to the CCA subspace and it's going to protect and it's going to project X2 to the CCA subspace.",
            "And if I want to precise to get CCA, you can just look at the cross correlation matrix.",
            "X 1X to make X1X2 mean zero, you can look at the SVD of this cross correlation matrix and you have this you DV.",
            "And we're looking at the thin SVD, so it's only going to contain the correlations which are greater than 0.",
            "And Pi X one is going to be projecting to the column span of yuan.",
            "Pi X2 is going to be projecting to the columns span eviebot in words.",
            "These projection operators are just projecting to the correlated sub spaces between the views.",
            "And now a theorem is going to be.",
            "Let's just assume that the dimension of the hidden state is K and we're assuming this last assumption.",
            "Then we can show that the dimension of this projection operator is K and this basically.",
            "Basically you can argue that this cross correlation matrix is the most rank K because of this hidden state assumption.",
            "And Furthermore, there exists a set of linear weights such that the conditional expectations of Y given X one are just linear in the projection of X1X1X1X2 can start as being very very large dimensional spaces.",
            "But if this assumption holds, it means we know that these conditional expectations are just linear in the projections.",
            "Of.",
            "Of X1, so now we've reduced the learning problem to a K dimensional problem where K is the dimension of the hidden state.",
            "After we've done CCA.",
            "So so again, we need enough unlabeled data to do this CCA, but the point is, we basically completely reduce this potentially infinite dimensional problem to a K dimensional problem if we can.",
            "If we can find 2 views which has some low dimensional hidden state structure.",
            "And one of the nice things about this.",
            "Oh, right, right?",
            "So third statement is we can look at the conditional expectation of Y given both views and the third statement is that the that the conditional expectation of Y given both views is just linear in the projections or both both views.",
            "No, because it's actually a little subtle because so in the redundancy viewpoint.",
            "Something like this is close to true, but it doesn't follow from this because.",
            "Because given both views, I could definitely come up with a distribution where I can have two things linear, but if I have them both together this I'm very complicated relationship to get the target.",
            "But it does follow from.",
            "I mean from the proof.",
            "And actually you could make it linear in a bilinear form as well between X1 and X2.",
            "But the interesting point, this raises the prediction error could be much lower with both views.",
            "So this is something that isn't true in the redundancy viewpoint.",
            "So here's a setting where it's interesting because we can, unlike I think some of the other multiview work that I've been involved in as well, we didn't have this theory where we could do better with both views, whereas here you can put them together and actually do better.",
            "OK, so that's the main theme of the condition independence and this really followed some.",
            "In it was buried in one of tongues earlier papers and it was stated in terms of I think, a weaker conditional independence assumption based on the target was speaking in tongue.",
            "I realized we could relax it and make this a little more palatable assumption with I think, a pretty natural theory.",
            "OK, so are there any more questions on this one, so briefly mentioned?"
        ],
        [
            "The redundancy based assumption.",
            "So this is another natural assumption for.",
            "For learning, it's that now we're going to assume that the best linear predictor from each view is roughly going to be as good as the best linear predictor based on both views.",
            "So more precisely, we will see that the R-squared between one view and Y is greater in the R-squared between both views and why, and there's a little slop in this by epsilon.",
            "So this is the same.",
            "You can say this in terms of losses as well, which is the loss of 1.",
            "View is close to.",
            "The loss the optimal loss had you had both use.",
            "But under this assumption, you won't do better with both views, because they're both about as good each each other, and epsilon is kind of the set up in this assumption, and this isn't really compareable to the condition independence assumption.",
            "Being talking have been thinking about this for awhile.",
            "And it just seems like a different sort of style of assumption and this assumption, I think, has been considered a lot more and things like or regularization where you try to force the predictions from each views to agree with each other.",
            "But under this very so, in the linear setting on this very simple assumption."
        ],
        [
            "Ink.",
            "Again, I think we get one of the cleanest theorems which is.",
            "OK, so now what's the algorithm?",
            "We can again consider one of these projection algorithms.",
            "We run CCA and going to take X1.",
            "We're going to project it to a correlated subspace, except in this setting.",
            "I'm going to project it to the correlated subspace which has correlation greater than .5.",
            "The correlations between zero and one, and I'm just going to pick threshold unprojected.",
            "This subspace where the correlation is greater than .5 and we're going to define the projection for the other view could be similar to find projected subspace with some sufficient correlation.",
            "And the point is, if we do this, we aren't going to lose much then how we worked in this full, potentially infinite dimensional space.",
            "The amount we lose is basically so before the assumption was having both years wasn't much better than epsilon an Now if we just look at our best linear predictor in this projected subspace were worse by a factor of epsilon, so it's a little worse, but it's not too bad and this factor of it can be tweaked by 5.",
            "So it's like a bias variance tradeoff going on here that you could choose the correlations to be.",
            "A little lower and then this constant would be bigger, but then the subspace might be bigger.",
            "And you know, in the tech report you'll see that for this case is actually interesting because we bound the dimensionality of the subspace by the sum of the squares of the correlation coefficient.",
            "So there's kind of a natural notion of the intrinsic dimensionality here by, because basically you can bound the dimension of the subspace by the sum of the squares of the correlation coefficient was the previous theorem.",
            "The dimension of this projection was K, which was the dimension of the hidden state was.",
            "Here we don't really have a hidden state.",
            "OK, so that's the.",
            "Yeah.",
            "Yeah.",
            "No, no, actually the .5 is totally arbitrary.",
            "Basically it's a bias variance tradeoff.",
            "So you could.",
            "It's basically this thing is something like one over Lambda and this thing is that I'm done so.",
            "Or maybe it's like 4 over Lambda, so you can make them would be very very small and this constant will be driven down to one.",
            "But by making Lambda very small you're blowing up the dimensionality of the subspace and it turns out actually you can bound the dimensionality of this by any some of the spectrum to some power with some constant here.",
            "So 'cause you might not expect the spectrum to decay very quickly.",
            "So you might want to use a higher power there and, but basically the constant is really just like a bias variance tradeoff.",
            "So is this actually it's related to some of the earlier work that didn't?",
            "I did, but we were actually able to sharpen it.",
            "So there we actually did a regularization based method.",
            "But here we're showing the dimensionality reduction gives you exactly the same set of results, and again with the same argument that we could do much better in this space because we've thrown away tons of irrelevant features."
        ],
        [
            "OK, so more questions about this.",
            "OK now so maybe.",
            "I'll discuss some of it.",
            "I have some very preliminary experiments, but since the results were interesting, I'll figure I'll talk about them so.",
            "In Wikipedia, there are disambiguation pages, so many, many words which.",
            "Have links to disambiguation pages.",
            "Actually, I think it's something like 100,000 disambiguation pages.",
            "It's a really a very very large number of disintegration pages on Wikipedia.",
            "And you know, and we're just crawling through it grabbing these pages.",
            "And now the task is going to be.",
            "We'd like to learn.",
            "Would like to learn disambiguation.",
            "And.",
            "OK, So what are the natural views now?",
            "Well, what we're going to do is we're going to take our website and we're going to take this time series viewpoint that condition on or would the past and future should be conditioned on a hidden state at a particular word, the past and future should be independent.",
            "And what are the features we're going to use?",
            "Well again, we have these 10 to 6 words, but also this is temporal aspects because we're looking at the entire history, so we do some sort of temporal smoothing of words before and some sort of temporal smoothing of words.",
            "After you can think about word vectors that are like binary and various types of smoothers for past and future.",
            "And right now, I mean, we really just have a couple of runs sending checks in our code and.",
            "We aren't even using all the words were.",
            "I mean, it's extremely simple.",
            "The correlations were looking at.",
            "We really looking at endings before words like things like IONEDING of the season endings before some words, and then endings after the same words and relating them.",
            "And there's also maybe maybe 30 or so common ish words that were also correlating before and after again in this kind of temporal framework.",
            "And we definitely want time because nuns would have a different sort of word before and so on so.",
            "Correlating just these very, very simple things and the test point is really trying to get it is a notion of one shot learning so that so the test is really going to be the same difference paradigm we're going to take one word that wants to be disintegrated, and we're going to present to the learner.",
            "Is this this page with the one word in it and we're going to present him two other pages and one of them is going to be the correct disambiguation page?",
            "And what time is going to be different one?",
            "So if you have, like the word Missouri, that's in some Page 1.",
            "The page will be Missouri.",
            "The state and the other page will be Missouri.",
            "The battleship and the job would be to say which one is correct.",
            "And.",
            "And so this is nice because if we can do this well, we can potentially generalize to two words we've never seen before.",
            "Rather than have a big corpus of disambiguating Missouri many times the data set we only have, maybe one or two examples of Missouri in it, and lots of other words.",
            "And then we like it.",
            "Basically, this is one shot learning error rates in the first example, we actually didn't even use any label data, we just took the CCA representation and did like a cosine error measure to see how well it did.",
            "And then we compared to TF IDF.",
            "The thing is, TF IDF is very limited now because it can really only use these 3 pages to run TF IDF on 'cause there's no big label data set.",
            "So so right now just the vanilla CCA representation unsupervised is compatible with the TF IDF, which is pretty surprising CAS.",
            "CCA isn't given a whole lot here.",
            "I mean, it's basically using these really terrible endings and then a few common words, so we're pretty surprised that we're getting 50% is breaking Even so just a simple representation we're getting.",
            "We're doing a little better and then when we have a few labels we train this thing up and we're getting 77% accuracy now.",
            "But again, once we start actually using more interesting words, I think we're doing much, much better, but that was kind of neat that even with this we're doing well."
        ],
        [
            "OK, so the conclusions so that the tech report on my website out pretty soon.",
            "We're like about this framework is it's really the vanilla linear theory spelled out and the linear theory is very simple.",
            "I think you know all the proofs are very simple.",
            "They're pretty intuitive.",
            "I want to make one point about CC of the algorithm versus the subspace.",
            "So this is kind of a really really nice paper by Andrew and Zhang a couple years ago about future generation.",
            "What they do is they kind of fictitiously makeup prediction tasks in one view and try to predict with the other view, and then they found an if they found it was predicted they'd include this in their feature representation.",
            "So this is, this is really more of a natural algorithm in the subspace, because she might be very, very hard to do if we have these huge.",
            "Dimension of vector is, but what Andrew and Jane were arguing is we could kind of do the poor man's version by just generating fictitious things to try to predict.",
            "And if we can predict them well, we include those.",
            "So there's really a natural feature generation viewpoint, and I also want to stress that this conditional independence viewpoint is very natural and some work I've been doing now is on time series trying to understand this and it's kind of related to some of these subspace ID methods and kind of when we exploit these methods.",
            "One of my recent results are shameless plug plug for a recent result of mine is.",
            "Where is result for learning hidden Markov models in Poly time?",
            "This is the first Poly time result for learning.",
            "Hmm that doesn't revolve around them and it's again.",
            "Based on this fact that the past and future independent and then you can look at particular SVD of a cross correlation matrix, which is going to reveal the hidden state structure, and I think it's again based on this intuition of SVD is revealing hidden state structure that we can come up with a polytime algorithm for Hmm's?",
            "OK, so thanks."
        ],
        [
            "Yep.",
            "So the point is, we're throwing nonlinearities into our future representations X1 and X2.",
            "So this is kind of why, if we really do a lot of non linearity, is finding the subspace starts to become tricky.",
            "What you can do is just take some non linearity, try to predict it with the other view and then use that learn predictor to to learn.",
            "And that's kind of what Anduin Jang do.",
            "Find a good position.",
            "That's basically what's going on when you think about X one is being.",
            "These being some nonlinear representation of your input.",
            "So I think you are learning, so the projection is still linear still in L2 based projection, but the subspace you've defined is now employed by a very.",
            "Do that is fine too.",
            "There's nothing magical.",
            "Yeah, that's right.",
            "You can definitely come out this year.",
            "I mean, that's the nice thing about linear.",
            "If you work it out, you can.",
            "Something like.",
            "Actually OK, so do you think so?",
            "One deception independence doesn't really hold the linear theory.",
            "All you need is a second order correlation, so it's a little weaker than independence, but it's kind of obvious that since we're dealing with a linear prediction, and certainly I've been thinking a lot about the information bottleneck, I think to some degree that was my motivation for this line of work from the information bottleneck is somehow they don't really think about the target in mind, but the same intuition that it's the information shared between the two views that's predicted the target.",
            "So this is I love you very much like the bottleneck, but now we have a target.",
            "In mind, but that ended up with the bottleneck is it's really just a code that you get.",
            "So what do you do with the code?",
            "Whereas this is trying to formalize.",
            "Things that, but there's a nice connection that the bottleneck for Gaussian variables.",
            "The CCA is the is the right answer, so.",
            "Some kind of?",
            "I wish you could argue that somehow the four CCA to really capture all of the stuff that decorrelate things you.",
            "It is a Gaussian assumption, but it doesn't really need that assumption.",
            "I mean, I think there's just a duality between these things between your modeling assumptions and this disco assumptions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, this is Sean packet as joint work where the Dean Foster and Tong Zhang actually, it really came out of two different lines of work.",
                    "label": 1
                },
                {
                    "sent": "Some work that may indeed and I have been doing for awhile and then some work tongue Jang.",
                    "label": 0
                },
                {
                    "sent": "An endo had been doing for awhile.",
                    "label": 0
                },
                {
                    "sent": "I realize we're very similar so we decided to combine them and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the implications.",
                    "label": 0
                },
                {
                    "sent": "So I think at this stage it's fair to say we really have a pretty good understanding of supervised learning, but what many of us think is the pressing question is how do we find good features or kernels or subspaces or hypothesis classes of the input?",
                    "label": 0
                },
                {
                    "sent": "SpaceX that are predictive of some target variable Y. OK, so we'd like to find predictive representations of X that useful for predicting target.",
                    "label": 0
                },
                {
                    "sent": "Why, and I think realistically, we just don't have enough label data to do this, and but we do, however, have orders of magnitude more unlabeled data, an ideally we'd like to learn relevant representations of X with the unlabeled data, and in particular, if you think about what's going on here is we're really interested in this conditional distribution P of Y given X, but we're trying to get this conditional distribution with only the marginal distribution of X.",
                    "label": 1
                },
                {
                    "sent": "And why should the marginal distribution of X contain any relationship?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this conditional distribution, so I think that's really the underlying question that people are trying to address in semi supervised learning, and the multiview approach we're going to try to exploit the structure between the views to get at this relationship.",
                    "label": 0
                },
                {
                    "sent": "So in particular setting we're going to be considering is when we have an input vector X which is broken into viewers, so X is X1 and X2, and these are views that are potentially in.",
                    "label": 0
                },
                {
                    "sent": "I mean, it could be pretty abstract.",
                    "label": 0
                },
                {
                    "sent": "And we should think about this.",
                    "label": 0
                },
                {
                    "sent": "The dimensionalities of these two views being arbitrary large, so we really, you know, like in machine learning we really tossed the kitchen sink into these views.",
                    "label": 1
                },
                {
                    "sent": "So we made a very rich representations in these views he put in a lot of features and maybe they correspond to some kernel.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality is very large.",
                    "label": 0
                },
                {
                    "sent": "We're just in predicting some target variable Y, and I think the intuition is that.",
                    "label": 0
                },
                {
                    "sent": "The relationship between these views should reveal some information about the target Y, so in particular we have this marginal distribution between X1 and X2, and we're going to be easy to obtain samples from this motion of distribution and with those samples we'd like to understand the relationship between these two variables and use that relationship to predict or target variable Y, and we saw some examples from Karen why this might be possible, where one view is a picture and the other view is speech.",
                    "label": 0
                },
                {
                    "sent": "We can think about an object recognition.",
                    "label": 0
                },
                {
                    "sent": "Scenario One View is a picture of the object from 1 camera angle of the other view is a picture of the camera is a picture of the object from a different camera angle.",
                    "label": 0
                },
                {
                    "sent": "And intuitively these views these different views are projecting giving us examples of viewpoint invariants and just based on these different views we might like to understand this concept of viewpoint in view.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's which we realize predicting the the identity of this object shouldn't depend on.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is what structure is useful?",
                    "label": 1
                },
                {
                    "sent": "In this joint distribution, such that we can come up with.",
                    "label": 0
                },
                {
                    "sent": "So we can really understand when we can exploit the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And in this talk I'm really going to consider two assumptions and we consider them separately their assumptions that were made in the original code training paper, and I think other people have considered.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to weaken them quite a bit and we are going to consider them separately.",
                    "label": 0
                },
                {
                    "sent": "So the first assumption is this conditional independence assumption, where we say that the two views are independent given Y and.",
                    "label": 1
                },
                {
                    "sent": "This assumption is actually falling into a bit of disrepute because it's a little too strong to assume that X one X2 are going to be independent when you just condition on this very low dimensional target variable.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start with this assumption, but I'm going to relax it and show that it really has a very natural relaxation and then the second assumption is going to be a redundancy based assumption which was something Dean and I considered a while back, which is really that predicting the target variable.",
                    "label": 1
                },
                {
                    "sent": "Why would just X one is roughly going to be as good as putting the target variable with both views?",
                    "label": 0
                },
                {
                    "sent": "Which is possible in many settings like if I have a this object from two different camera angles, I can predict what it is from either camera angle and the point is with other these two assumptions.",
                    "label": 0
                },
                {
                    "sent": "What's going to turn out to be the case is the only features that are going to be useful for predicting the target variable are going to be those features in each view that are correlated with each other so.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea, and the reason this is very important is we can significantly reduce the label of something complexity because we can basically learn this correlation with unlabeled data and the theorems we're going to have.",
                    "label": 1
                },
                {
                    "sent": "The form is that we're not going to lose any predictive information by doing this projection, so the so that's going to be the gist of this talk, and really trying to formalize what these assumptions of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so again since the outline so quickly go through well, I'll go through the assumptions of conditional independence, independence and I relax it to a notion of independence based on hidden States and I think this idea is really capturing an idea that's less explored in the multi view learning.",
                    "label": 1
                },
                {
                    "sent": "So there's been a lot of work on redundancy in core regularization, but the independence assumption hasn't really been studied that much, but I really think it's pretty natural.",
                    "label": 0
                },
                {
                    "sent": "And then I'll discuss the redundancy assumption.",
                    "label": 0
                },
                {
                    "sent": "And then I'll briefly discuss in pulmonary experiments from Wikipedia on would sense disambiguation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That I've been doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so before we start to just refresh your memory on some basic statistics.",
                    "label": 0
                },
                {
                    "sent": "Is it any questions definitely interrupt me and there's more seats in the front.",
                    "label": 0
                },
                {
                    "sent": "People want to kind of come up and sit on.",
                    "label": 0
                },
                {
                    "sent": "Feel free to just barge in.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Throughout the talk, for simplicity, I'm going to assume that the conditional mean of Y given X is linear.",
                    "label": 0
                },
                {
                    "sent": "In X.",
                    "label": 0
                },
                {
                    "sent": "We can relax this later, but for simplicity let's just assume that all these conditional means are linear in X and we called the definition of the R-squared between X&Y is just the correlation between this best linear predictor beta dot X&Y squared, so it's just one minus the loss of beta over the variance of Y, and the words are squared is just the.",
                    "label": 0
                },
                {
                    "sent": "Fraction of the variance in Y that can be explained if you know X.",
                    "label": 1
                },
                {
                    "sent": "So if the R-squared is 1, then you have a perfect prediction of the target variable, an if the R-squared is 0, then using the X has no predictive power of the target variable.",
                    "label": 0
                },
                {
                    "sent": "So if it's in, if we scale, variance of would be one in the US.",
                    "label": 0
                },
                {
                    "sent": "Squared is just one minus loss, but I think it's a natural quantity to state the assumptions in terms of.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Like",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, so now let's look at what I think is the right way to think about conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Because you know, like in the original training paper, why was this binary target variable?",
                    "label": 0
                },
                {
                    "sent": "And it's kind of completely unreasonable to think that these two high dimensional views are going to be independent condition on this binary target variable.",
                    "label": 0
                },
                {
                    "sent": "But we're going to do is we're going to think about going to define H to be a hidden state for X1X2 and Y if we have the property that condition on H. These three are going to be independent.",
                    "label": 1
                },
                {
                    "sent": "And hidden Caesar very natural because we have this entire community of graphical models.",
                    "label": 0
                },
                {
                    "sent": "Really, really trying to exploit the fact that there are hidden states in the world and would hidden states.",
                    "label": 0
                },
                {
                    "sent": "Do they really imply certain conditional independence properties and?",
                    "label": 0
                },
                {
                    "sent": "To some degree, if I just assume that we have a hidden state, this is without loss of generality, 'cause we can always find a hidden state such that this property is satisfied for any distribution.",
                    "label": 1
                },
                {
                    "sent": "We can definitely find some hidden state that satisfies.",
                    "label": 1
                },
                {
                    "sent": "For Kate to be the dimensionality of the hidden state, I mean the many settings are hidden signature also.",
                    "label": 0
                },
                {
                    "sent": "In the last example, what would be the hidden state between these two views that made them independent?",
                    "label": 0
                },
                {
                    "sent": "Well, it's probably not just the speaker identity, it's probably the speaker identity.",
                    "label": 0
                },
                {
                    "sent": "Plus maybe a few phonemes and what they're saying, but it's definitely.",
                    "label": 0
                },
                {
                    "sent": "It's definitely still, you might imagine to be much lower dimensional signal than the target variable.",
                    "label": 0
                },
                {
                    "sent": "And you know, in time series the natural hidden states or something at a particular time, port, time point which makes the past and future independent.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So then what's the natural assumption 'cause we haven't lost any?",
                    "label": 0
                },
                {
                    "sent": "We really haven't lost anything by thinking about hidden states, 'cause it always exists.",
                    "label": 0
                },
                {
                    "sent": "So the assumption is going to be that are hidden views so that our view is X1 and X2 are both going to be nontrivially predictive of the hidden state, and what I mean by this is for every direction WER squared between X1 and WH is greater than 0.",
                    "label": 0
                },
                {
                    "sent": "And similarly, for four of you two that the R-squared between X2 and WH is greater than zero.",
                    "label": 0
                },
                {
                    "sent": "So in words, what this means is that each view is at least somewhat correlated with every coordinate in our hidden state.",
                    "label": 0
                },
                {
                    "sent": "So in this kind of addresses, I think this parity question that if you think about a hidden state where so let's say why was going to be X1 plus X2 and the hidden state now would have to contain X1, X2 in it.",
                    "label": 0
                },
                {
                    "sent": "And this is a bad case to deal with because X1 doesn't have might not have any information about X2, so it's not predicted that that coordinates.",
                    "label": 0
                },
                {
                    "sent": "The assumption really is going to be that each view is at least somewhat predictive of each part of the hidden state, which is.",
                    "label": 0
                },
                {
                    "sent": "I think the natural assumption the multiview setting that we have, this hidden state and somehow it isn't really split between the views, it's at the views.",
                    "label": 0
                },
                {
                    "sent": "Both views really contain some correlated information about the hidden state, but it's weak because I'm not saying that there are good predictions of the hidden state, I'm just saying they're correlated with the hidden state.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now with this we can get the.",
                    "label": 0
                },
                {
                    "sent": "The main theorem for conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's define this projection operator.",
                    "label": 0
                },
                {
                    "sent": "PY to take X1 and projected to the correlated subspace.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by this is going to project X1 to the CCA subspace and it's going to protect and it's going to project X2 to the CCA subspace.",
                    "label": 0
                },
                {
                    "sent": "And if I want to precise to get CCA, you can just look at the cross correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "X 1X to make X1X2 mean zero, you can look at the SVD of this cross correlation matrix and you have this you DV.",
                    "label": 0
                },
                {
                    "sent": "And we're looking at the thin SVD, so it's only going to contain the correlations which are greater than 0.",
                    "label": 0
                },
                {
                    "sent": "And Pi X one is going to be projecting to the column span of yuan.",
                    "label": 0
                },
                {
                    "sent": "Pi X2 is going to be projecting to the columns span eviebot in words.",
                    "label": 0
                },
                {
                    "sent": "These projection operators are just projecting to the correlated sub spaces between the views.",
                    "label": 0
                },
                {
                    "sent": "And now a theorem is going to be.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume that the dimension of the hidden state is K and we're assuming this last assumption.",
                    "label": 0
                },
                {
                    "sent": "Then we can show that the dimension of this projection operator is K and this basically.",
                    "label": 0
                },
                {
                    "sent": "Basically you can argue that this cross correlation matrix is the most rank K because of this hidden state assumption.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, there exists a set of linear weights such that the conditional expectations of Y given X one are just linear in the projection of X1X1X1X2 can start as being very very large dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "But if this assumption holds, it means we know that these conditional expectations are just linear in the projections.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 1
                },
                {
                    "sent": "Of X1, so now we've reduced the learning problem to a K dimensional problem where K is the dimension of the hidden state.",
                    "label": 0
                },
                {
                    "sent": "After we've done CCA.",
                    "label": 0
                },
                {
                    "sent": "So so again, we need enough unlabeled data to do this CCA, but the point is, we basically completely reduce this potentially infinite dimensional problem to a K dimensional problem if we can.",
                    "label": 0
                },
                {
                    "sent": "If we can find 2 views which has some low dimensional hidden state structure.",
                    "label": 0
                },
                {
                    "sent": "And one of the nice things about this.",
                    "label": 0
                },
                {
                    "sent": "Oh, right, right?",
                    "label": 0
                },
                {
                    "sent": "So third statement is we can look at the conditional expectation of Y given both views and the third statement is that the that the conditional expectation of Y given both views is just linear in the projections or both both views.",
                    "label": 0
                },
                {
                    "sent": "No, because it's actually a little subtle because so in the redundancy viewpoint.",
                    "label": 0
                },
                {
                    "sent": "Something like this is close to true, but it doesn't follow from this because.",
                    "label": 0
                },
                {
                    "sent": "Because given both views, I could definitely come up with a distribution where I can have two things linear, but if I have them both together this I'm very complicated relationship to get the target.",
                    "label": 0
                },
                {
                    "sent": "But it does follow from.",
                    "label": 0
                },
                {
                    "sent": "I mean from the proof.",
                    "label": 0
                },
                {
                    "sent": "And actually you could make it linear in a bilinear form as well between X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "But the interesting point, this raises the prediction error could be much lower with both views.",
                    "label": 1
                },
                {
                    "sent": "So this is something that isn't true in the redundancy viewpoint.",
                    "label": 0
                },
                {
                    "sent": "So here's a setting where it's interesting because we can, unlike I think some of the other multiview work that I've been involved in as well, we didn't have this theory where we could do better with both views, whereas here you can put them together and actually do better.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the main theme of the condition independence and this really followed some.",
                    "label": 0
                },
                {
                    "sent": "In it was buried in one of tongues earlier papers and it was stated in terms of I think, a weaker conditional independence assumption based on the target was speaking in tongue.",
                    "label": 0
                },
                {
                    "sent": "I realized we could relax it and make this a little more palatable assumption with I think, a pretty natural theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so are there any more questions on this one, so briefly mentioned?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The redundancy based assumption.",
                    "label": 0
                },
                {
                    "sent": "So this is another natural assumption for.",
                    "label": 0
                },
                {
                    "sent": "For learning, it's that now we're going to assume that the best linear predictor from each view is roughly going to be as good as the best linear predictor based on both views.",
                    "label": 1
                },
                {
                    "sent": "So more precisely, we will see that the R-squared between one view and Y is greater in the R-squared between both views and why, and there's a little slop in this by epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this is the same.",
                    "label": 0
                },
                {
                    "sent": "You can say this in terms of losses as well, which is the loss of 1.",
                    "label": 0
                },
                {
                    "sent": "View is close to.",
                    "label": 0
                },
                {
                    "sent": "The loss the optimal loss had you had both use.",
                    "label": 0
                },
                {
                    "sent": "But under this assumption, you won't do better with both views, because they're both about as good each each other, and epsilon is kind of the set up in this assumption, and this isn't really compareable to the condition independence assumption.",
                    "label": 0
                },
                {
                    "sent": "Being talking have been thinking about this for awhile.",
                    "label": 0
                },
                {
                    "sent": "And it just seems like a different sort of style of assumption and this assumption, I think, has been considered a lot more and things like or regularization where you try to force the predictions from each views to agree with each other.",
                    "label": 0
                },
                {
                    "sent": "But under this very so, in the linear setting on this very simple assumption.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ink.",
                    "label": 0
                },
                {
                    "sent": "Again, I think we get one of the cleanest theorems which is.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what's the algorithm?",
                    "label": 0
                },
                {
                    "sent": "We can again consider one of these projection algorithms.",
                    "label": 0
                },
                {
                    "sent": "We run CCA and going to take X1.",
                    "label": 0
                },
                {
                    "sent": "We're going to project it to a correlated subspace, except in this setting.",
                    "label": 0
                },
                {
                    "sent": "I'm going to project it to the correlated subspace which has correlation greater than .5.",
                    "label": 1
                },
                {
                    "sent": "The correlations between zero and one, and I'm just going to pick threshold unprojected.",
                    "label": 0
                },
                {
                    "sent": "This subspace where the correlation is greater than .5 and we're going to define the projection for the other view could be similar to find projected subspace with some sufficient correlation.",
                    "label": 0
                },
                {
                    "sent": "And the point is, if we do this, we aren't going to lose much then how we worked in this full, potentially infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The amount we lose is basically so before the assumption was having both years wasn't much better than epsilon an Now if we just look at our best linear predictor in this projected subspace were worse by a factor of epsilon, so it's a little worse, but it's not too bad and this factor of it can be tweaked by 5.",
                    "label": 0
                },
                {
                    "sent": "So it's like a bias variance tradeoff going on here that you could choose the correlations to be.",
                    "label": 0
                },
                {
                    "sent": "A little lower and then this constant would be bigger, but then the subspace might be bigger.",
                    "label": 0
                },
                {
                    "sent": "And you know, in the tech report you'll see that for this case is actually interesting because we bound the dimensionality of the subspace by the sum of the squares of the correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "So there's kind of a natural notion of the intrinsic dimensionality here by, because basically you can bound the dimension of the subspace by the sum of the squares of the correlation coefficient was the previous theorem.",
                    "label": 0
                },
                {
                    "sent": "The dimension of this projection was K, which was the dimension of the hidden state was.",
                    "label": 1
                },
                {
                    "sent": "Here we don't really have a hidden state.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, no, actually the .5 is totally arbitrary.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So you could.",
                    "label": 0
                },
                {
                    "sent": "It's basically this thing is something like one over Lambda and this thing is that I'm done so.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it's like 4 over Lambda, so you can make them would be very very small and this constant will be driven down to one.",
                    "label": 0
                },
                {
                    "sent": "But by making Lambda very small you're blowing up the dimensionality of the subspace and it turns out actually you can bound the dimensionality of this by any some of the spectrum to some power with some constant here.",
                    "label": 0
                },
                {
                    "sent": "So 'cause you might not expect the spectrum to decay very quickly.",
                    "label": 0
                },
                {
                    "sent": "So you might want to use a higher power there and, but basically the constant is really just like a bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So is this actually it's related to some of the earlier work that didn't?",
                    "label": 0
                },
                {
                    "sent": "I did, but we were actually able to sharpen it.",
                    "label": 0
                },
                {
                    "sent": "So there we actually did a regularization based method.",
                    "label": 0
                },
                {
                    "sent": "But here we're showing the dimensionality reduction gives you exactly the same set of results, and again with the same argument that we could do much better in this space because we've thrown away tons of irrelevant features.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so more questions about this.",
                    "label": 0
                },
                {
                    "sent": "OK now so maybe.",
                    "label": 0
                },
                {
                    "sent": "I'll discuss some of it.",
                    "label": 0
                },
                {
                    "sent": "I have some very preliminary experiments, but since the results were interesting, I'll figure I'll talk about them so.",
                    "label": 0
                },
                {
                    "sent": "In Wikipedia, there are disambiguation pages, so many, many words which.",
                    "label": 1
                },
                {
                    "sent": "Have links to disambiguation pages.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think it's something like 100,000 disambiguation pages.",
                    "label": 0
                },
                {
                    "sent": "It's a really a very very large number of disintegration pages on Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And you know, and we're just crawling through it grabbing these pages.",
                    "label": 0
                },
                {
                    "sent": "And now the task is going to be.",
                    "label": 0
                },
                {
                    "sent": "We'd like to learn.",
                    "label": 0
                },
                {
                    "sent": "Would like to learn disambiguation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the natural views now?",
                    "label": 0
                },
                {
                    "sent": "Well, what we're going to do is we're going to take our website and we're going to take this time series viewpoint that condition on or would the past and future should be conditioned on a hidden state at a particular word, the past and future should be independent.",
                    "label": 0
                },
                {
                    "sent": "And what are the features we're going to use?",
                    "label": 0
                },
                {
                    "sent": "Well again, we have these 10 to 6 words, but also this is temporal aspects because we're looking at the entire history, so we do some sort of temporal smoothing of words before and some sort of temporal smoothing of words.",
                    "label": 0
                },
                {
                    "sent": "After you can think about word vectors that are like binary and various types of smoothers for past and future.",
                    "label": 0
                },
                {
                    "sent": "And right now, I mean, we really just have a couple of runs sending checks in our code and.",
                    "label": 0
                },
                {
                    "sent": "We aren't even using all the words were.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's extremely simple.",
                    "label": 0
                },
                {
                    "sent": "The correlations were looking at.",
                    "label": 0
                },
                {
                    "sent": "We really looking at endings before words like things like IONEDING of the season endings before some words, and then endings after the same words and relating them.",
                    "label": 0
                },
                {
                    "sent": "And there's also maybe maybe 30 or so common ish words that were also correlating before and after again in this kind of temporal framework.",
                    "label": 0
                },
                {
                    "sent": "And we definitely want time because nuns would have a different sort of word before and so on so.",
                    "label": 0
                },
                {
                    "sent": "Correlating just these very, very simple things and the test point is really trying to get it is a notion of one shot learning so that so the test is really going to be the same difference paradigm we're going to take one word that wants to be disintegrated, and we're going to present to the learner.",
                    "label": 0
                },
                {
                    "sent": "Is this this page with the one word in it and we're going to present him two other pages and one of them is going to be the correct disambiguation page?",
                    "label": 0
                },
                {
                    "sent": "And what time is going to be different one?",
                    "label": 0
                },
                {
                    "sent": "So if you have, like the word Missouri, that's in some Page 1.",
                    "label": 0
                },
                {
                    "sent": "The page will be Missouri.",
                    "label": 0
                },
                {
                    "sent": "The state and the other page will be Missouri.",
                    "label": 0
                },
                {
                    "sent": "The battleship and the job would be to say which one is correct.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so this is nice because if we can do this well, we can potentially generalize to two words we've never seen before.",
                    "label": 0
                },
                {
                    "sent": "Rather than have a big corpus of disambiguating Missouri many times the data set we only have, maybe one or two examples of Missouri in it, and lots of other words.",
                    "label": 0
                },
                {
                    "sent": "And then we like it.",
                    "label": 1
                },
                {
                    "sent": "Basically, this is one shot learning error rates in the first example, we actually didn't even use any label data, we just took the CCA representation and did like a cosine error measure to see how well it did.",
                    "label": 0
                },
                {
                    "sent": "And then we compared to TF IDF.",
                    "label": 0
                },
                {
                    "sent": "The thing is, TF IDF is very limited now because it can really only use these 3 pages to run TF IDF on 'cause there's no big label data set.",
                    "label": 0
                },
                {
                    "sent": "So so right now just the vanilla CCA representation unsupervised is compatible with the TF IDF, which is pretty surprising CAS.",
                    "label": 0
                },
                {
                    "sent": "CCA isn't given a whole lot here.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's basically using these really terrible endings and then a few common words, so we're pretty surprised that we're getting 50% is breaking Even so just a simple representation we're getting.",
                    "label": 0
                },
                {
                    "sent": "We're doing a little better and then when we have a few labels we train this thing up and we're getting 77% accuracy now.",
                    "label": 0
                },
                {
                    "sent": "But again, once we start actually using more interesting words, I think we're doing much, much better, but that was kind of neat that even with this we're doing well.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the conclusions so that the tech report on my website out pretty soon.",
                    "label": 0
                },
                {
                    "sent": "We're like about this framework is it's really the vanilla linear theory spelled out and the linear theory is very simple.",
                    "label": 1
                },
                {
                    "sent": "I think you know all the proofs are very simple.",
                    "label": 0
                },
                {
                    "sent": "They're pretty intuitive.",
                    "label": 1
                },
                {
                    "sent": "I want to make one point about CC of the algorithm versus the subspace.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a really really nice paper by Andrew and Zhang a couple years ago about future generation.",
                    "label": 0
                },
                {
                    "sent": "What they do is they kind of fictitiously makeup prediction tasks in one view and try to predict with the other view, and then they found an if they found it was predicted they'd include this in their feature representation.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is really more of a natural algorithm in the subspace, because she might be very, very hard to do if we have these huge.",
                    "label": 0
                },
                {
                    "sent": "Dimension of vector is, but what Andrew and Jane were arguing is we could kind of do the poor man's version by just generating fictitious things to try to predict.",
                    "label": 1
                },
                {
                    "sent": "And if we can predict them well, we include those.",
                    "label": 0
                },
                {
                    "sent": "So there's really a natural feature generation viewpoint, and I also want to stress that this conditional independence viewpoint is very natural and some work I've been doing now is on time series trying to understand this and it's kind of related to some of these subspace ID methods and kind of when we exploit these methods.",
                    "label": 0
                },
                {
                    "sent": "One of my recent results are shameless plug plug for a recent result of mine is.",
                    "label": 0
                },
                {
                    "sent": "Where is result for learning hidden Markov models in Poly time?",
                    "label": 1
                },
                {
                    "sent": "This is the first Poly time result for learning.",
                    "label": 0
                },
                {
                    "sent": "Hmm that doesn't revolve around them and it's again.",
                    "label": 0
                },
                {
                    "sent": "Based on this fact that the past and future independent and then you can look at particular SVD of a cross correlation matrix, which is going to reveal the hidden state structure, and I think it's again based on this intuition of SVD is revealing hidden state structure that we can come up with a polytime algorithm for Hmm's?",
                    "label": 0
                },
                {
                    "sent": "OK, so thanks.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So the point is, we're throwing nonlinearities into our future representations X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of why, if we really do a lot of non linearity, is finding the subspace starts to become tricky.",
                    "label": 0
                },
                {
                    "sent": "What you can do is just take some non linearity, try to predict it with the other view and then use that learn predictor to to learn.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of what Anduin Jang do.",
                    "label": 0
                },
                {
                    "sent": "Find a good position.",
                    "label": 0
                },
                {
                    "sent": "That's basically what's going on when you think about X one is being.",
                    "label": 0
                },
                {
                    "sent": "These being some nonlinear representation of your input.",
                    "label": 0
                },
                {
                    "sent": "So I think you are learning, so the projection is still linear still in L2 based projection, but the subspace you've defined is now employed by a very.",
                    "label": 0
                },
                {
                    "sent": "Do that is fine too.",
                    "label": 0
                },
                {
                    "sent": "There's nothing magical.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "You can definitely come out this year.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the nice thing about linear.",
                    "label": 0
                },
                {
                    "sent": "If you work it out, you can.",
                    "label": 0
                },
                {
                    "sent": "Something like.",
                    "label": 0
                },
                {
                    "sent": "Actually OK, so do you think so?",
                    "label": 0
                },
                {
                    "sent": "One deception independence doesn't really hold the linear theory.",
                    "label": 0
                },
                {
                    "sent": "All you need is a second order correlation, so it's a little weaker than independence, but it's kind of obvious that since we're dealing with a linear prediction, and certainly I've been thinking a lot about the information bottleneck, I think to some degree that was my motivation for this line of work from the information bottleneck is somehow they don't really think about the target in mind, but the same intuition that it's the information shared between the two views that's predicted the target.",
                    "label": 0
                },
                {
                    "sent": "So this is I love you very much like the bottleneck, but now we have a target.",
                    "label": 0
                },
                {
                    "sent": "In mind, but that ended up with the bottleneck is it's really just a code that you get.",
                    "label": 0
                },
                {
                    "sent": "So what do you do with the code?",
                    "label": 0
                },
                {
                    "sent": "Whereas this is trying to formalize.",
                    "label": 0
                },
                {
                    "sent": "Things that, but there's a nice connection that the bottleneck for Gaussian variables.",
                    "label": 0
                },
                {
                    "sent": "The CCA is the is the right answer, so.",
                    "label": 0
                },
                {
                    "sent": "Some kind of?",
                    "label": 0
                },
                {
                    "sent": "I wish you could argue that somehow the four CCA to really capture all of the stuff that decorrelate things you.",
                    "label": 0
                },
                {
                    "sent": "It is a Gaussian assumption, but it doesn't really need that assumption.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think there's just a duality between these things between your modeling assumptions and this disco assumptions.",
                    "label": 0
                }
            ]
        }
    }
}