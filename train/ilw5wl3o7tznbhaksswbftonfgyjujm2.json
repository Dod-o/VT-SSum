{
    "id": "ilw5wl3o7tznbhaksswbftonfgyjujm2",
    "title": "Random Matrices",
    "info": {
        "author": [
            "Pierpaolo Vivo, Laboratory of Theoretical Physics and Statistical Models (LPTMS), University of Paris-Sud 11"
        ],
        "published": "Nov. 21, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Physics->Statistical Physics",
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/netadis2013_vivo_random_matrices/",
    "segmentation": [
        [
            "So I was asked to give a few introductory lectures on the topic of random matrix theory and in particular new applications of theory that is quite old.",
            "This is almost a century old, as we will see it is main important application in statistical mechanics and not only there, and so let me start."
        ],
        [
            "So first of all, I just borrowed my slide from talk by Alan Edelman, who is one of the world leading experts in random matrix theory.",
            "So his question is why are random matrix eigenvalues cool and this message is OK?",
            "Take any important mathematics, then randomize and this will have many applications.",
            "So random matrix theory basically is a combination of linear algebra.",
            "And randomness or linear algebra and probability theory.",
            "OK, so matrices are."
        ],
        [
            "Quite important, even though they have bad reputation, and since that usually proves that involves matrices can be shortened by 50% if one throws the matrices out, so we will see that indeed we can develop the theory without making any or very few references to actual matrices.",
            "The good thing about random matrix matrices is that you can simulate them.",
            "You can open your C++, Matlab, Mathematica.",
            "Do produce your matrices, diagonalize them and study the statistics of eigenvalues.",
            "So in the notes that I prepared that have been circulated, I used Matlab to perform numerical simulations.",
            "And of course you are encouraged to take a step yourself and try to reproduce the codes that I put in there and have a touch on the real stuff.",
            "Of course, if you have any questions, please interrupt me now or call.",
            "Call to me come to me later."
        ],
        [
            "So let me introduce the characters of this story.",
            "The fathers of random matrix theory.",
            "First John Wishart in the 20s.",
            "Eugen Vigner in the 50s and Freeman dies on 50s and 60s and so on.",
            "He is the only one in color because he's still alive.",
            "And this is the.",
            "This is the paper that is thought to be starting the field of random matrices.",
            "Distribution of the roots of certain symmetric matrices by Wigner in 1957.",
            "I'm going to show you that actually the history of random matrices is a bit more complicated and longer than that."
        ],
        [
            "So there is a unusual old story that is taught in all the textbooks about random matrices.",
            "According to which the theory was born, because people were trying to understand the Hamiltonian.",
            "So the energy levels of complex nuclei nuclei that are made up of a large number of new clones.",
            "So one would be tempted to try to write down Hamiltonian operator for this this guy to diagonalize it and to study the energy levels.",
            "Actually, due to the large number of nucleons this this task is quite becomes rapidly quite hopeless.",
            "At the same time, there was an idea put forward by the guys I showed you before that actually Hamiltonian in a given basis is just can be thought of as a huge matrix.",
            "Matrix elements are just, you know, like hopping, hopping rates and so on and so forth.",
            "So you might just want to say OK, we don't know anything about the actual structure of this guy, so we take the matrix entries at random with some probability distribution, and then we study the properties of the eigenvalues.",
            "Maybe some regularity will emerge, maybe some statistical universality will emerge.",
            "We don't know.",
            "We can try to do this.",
            "This procedure."
        ],
        [
            "But very soon one realizes that the history of random matrices goes far back in the past.",
            "So the 1st paper that has ever appeared or is thought to have ever appeared on the subject is from 1928.",
            "John Wishart is a statistician.",
            "And he started random matrices in this constant context of covariance estimation for the multivariate normal distribution.",
            "So he basically studied.",
            "Random covariance matrices of normally distributed data.",
            "OK, this is the first ever appearance.",
            "The word the expression random matrix theory did not appear there, but still we have a lot of results that have been later rediscovered many times, and so this is the first ever appearance of random matrix in the literature."
        ],
        [
            "Also for Norman 1947, so 15 years before the actual date of birth of random matrix theory, did some work on some numerical bounds for floating point errors in some lower upper decomposition.",
            "So he was basically interested in finding the upper bound for the norm of a random matrix.",
            "Again, he never mentioned the word random matrix or random matrix theory, but still this is one of the this is the second occurrence if you want.",
            "Girlfriend of matrix theory before."
        ],
        [
            "That slide was to mention that even in 1939 there was another quite important result.",
            "But OK, we will come to that later if it's going to work OK.",
            "So this is the first our first encounter with the random matrix you set N = 5 and you produce a 5 by 5 matrix that is symmetric.",
            "OK, so you just populate the matrix by sampling random Gaussian variables.",
            "OK so you have some positive numbers, some negative numbers, numbers mostly around 0.",
            "So this is a symmetric matrix, so you can diagonalize it and you get the eigenvalues.",
            "What you see is that there are eigenvalues quite close to 0.",
            "There are three negative eigenvalues and two positive, so this is a quite typical thing, so you will get.",
            "An Alpha positive, an Alpha negative eigenvalue, roughly.",
            "And typically we will be interested in taking N to Infinity, so a very large matrix size, but sometimes even finite and results can be can be interesting so."
        ],
        [
            "So the basic goal of random matrix theory can be summarized here.",
            "I give you an object that is the joint probability density of the entries.",
            "So it is a P of H11 H&N.",
            "And from this you try to compute as much as you can about the eigenvalues.",
            "The eigenvalues are random variables because the entries are.",
            "So you may compute, try to compute the average density of eigenvalues.",
            "We will see what this is.",
            "The distribution of spacings between adjacent eigenvalues.",
            "The distribution, for example, of the largest or the smallest eigenvalue correlation between eigenvalues and so on and so forth.",
            "So."
        ],
        [
            "Ideally, if I give you this object.",
            "Need your first thought?",
            "Should be OK. Maybe from this object I can derive a similar object which is the joint density of the N eigenvalues.",
            "After all, we're talking about random variables, so if I can derive something like this, then all the statistical properties of this of the eigenvalues will be encoded in this object.",
            "Note that here we have N square degrees of freedom.",
            "And here we have only N degrees of freedom.",
            "So is this this reduction of degrees of freedom clearly implies that we are wiping out some information.",
            "So is this reduction always possible?",
            "Well, the answer is not.",
            "Not always, not not for some basic principle or some.",
            "It's just that we don't have the technical tools to deal with this reduction in the general case.",
            "And I will show you why this is so.",
            "So this this stuff is for you.",
            "So you are the ones that will maybe help us breaking through this this problem and give us some more weapons to talk to attack it."
        ],
        [
            "OK, let's come back to Laura to our matrix.",
            "We compute the eigenvalues.",
            "So now let's repeat this experiment numerically.",
            "There are some quotes in the notes many times like 10,000 times.",
            "For each sample of your matrix, you compute the eigenvalues.",
            "So you will get at the end 5 * 10,000 eigenvalues 50,000 numbers, and then you just make a histogram of all the eigenvalues.",
            "So what is the shape that you will get?",
            "This is what is called average spectral density or average density of states or density of states or counting function or frequency counting function, and I think there are about 17 different definition for the same for the same object.",
            "This is just the the histogram of all the eigenvalues of all the samples.",
            "OK."
        ],
        [
            "Now if you do it, these figures are taken from page 27 of my notes.",
            "You will get for the N = 5.",
            "A curve that looks like like this.",
            "OK, so the density is very high, close to zero.",
            "We have a lot of eigenvalues that are close to 0.",
            "Very few eigenvalues that are larger than five.",
            "And we have this oscillations.",
            "How many 123455 is equal to N?",
            "Next, you can do the same.",
            "This is a symmetric curve.",
            "So in principle, on average you get same number of eigenvalues that are positive and or negative.",
            "Now you do the same experiment with N = 10.",
            "You see that the spectrum widens a bit.",
            "It invades a bit more of the real axis.",
            "And you get a lot of wiggles.",
            "How many 123456789 ten?",
            "And so on and so forth.",
            "So this this curve like light blue curve is for N = 25.",
            "You see that the oscillations get smoothed out.",
            "There are many more oscillations, but much thinner.",
            "And you can see that there is a limiting shape that starts to appear OK. We will see what what this is notes that for a matrix 25 * 25 You will not find many eigenvalues larger than 10, which is just twice as much as a matrix that is 5 times smaller.",
            "So the rate of growth of the eigenvalues you know is not linear in.",
            "And then it is much lower than.",
            "Here in the second panel we have again the case 123456789 ten 10 by 10 and the dots are numerical simulations.",
            "So that is the histogram and the black curve is the analytical results.",
            "So we do actually have an analytical prediction for the spectral density of a 10 by 10 Gaussian matrix.",
            "Or 20 by 20 four 9 by 9 within finite."
        ],
        [
            "OK, So what?",
            "What is the limiting?",
            "The limiting curve is what is called the weakness semicircle low, so it says that the density of eigenvalues for end very large converges to a scaling form that is 1 / sqrt 2 beta N times a function of Lambda over root of 2 beta.",
            "Where this function F is 2 / \u03c0 root of 1 -- X square.",
            "So actually the curve that I showed you before."
        ],
        [
            "This dashed dotted line here.",
            "Is precisely."
        ],
        [
            "This one so it is one over root of 2 beta times N was 10.",
            "And beta was one and I will show you why.",
            "Times 2 / \u03c0 root of 1 minus Lambda square over 2 beta, so 1 * 10.",
            "So this is the curve that I drew on this.",
            "On this panel.",
            "So this beta is an index, is a discrete index that can take just three values, 1, two, or four.",
            "Which is called the Dyson Index of the ensemble and gives rise to what was called the Dysons threefold way.",
            "OK so beta equals one corresponds to matrices that are real whose entries are real and symmetric.",
            "Beta equals two corresponds to emission matrices, so complex admission.",
            "And we take those four corresponds to matrices whose entries are quaternions.",
            "And they have a symmetry such that the spectrum is real, so there are only three.",
            "Three types of such matrices that are indexed by one.",
            "If the entry is characterized by just one number real.",
            "Two for complex, an four for quaternions.",
            "So question, why is it called?",
            "Same circle Semi circle law.",
            "Previous so this one, this one is the limiting curve.",
            "OK, so you're saying that this one is a semicircle.",
            "OK, but the problem is that the radius here and here are different, right?",
            "We have a 2 / \u03c0 and a one.",
            "So what do?"
        ],
        [
            "Veganism circle is not.",
            "It's not the same thing.",
            "It is a semi ellipse.",
            "Kinda.",
            "I don't know why Bignor didn't call it semi circle.",
            "It was called afterwards."
        ],
        [
            "And if you think that the circle and an ellipse are the same thing, well.",
            "OK."
        ],
        [
            "So some some of the stuff doesn't come out and some does, so I have some 50% of chance to be able to finish the presentation.",
            "OK, so here there was just the title of a paper on nine 1955 where which was the first occurrence.",
            "I hope you can read some.",
            "It's a bit light but I will.",
            "OK, this is just the first occurrence of the same circle law in random matrix theory.",
            "And it was actually derived by vigner, but not for the type of matrix that I showed you before, not for Gaussian matrices.",
            "It was derived for random sign symmetric matrices that are, so the two N + 1 real symmetric matrices diagonal elements are zero.",
            "And the non diagonal elements are plus or minus V. With random signs, so outside the diagonal we have the same number but with random signs plus or minus.",
            "So for this matrix the limiting Loaiza some circle.",
            "This was derived two years before the degauss the case.",
            "The Gaussian case that is usually referred to as the prototype of some circle.",
            "So originally the same circle was, which is not the same circle, was not derived for Gaussian matrices."
        ],
        [
            "So this clearly from some questions, because you see, we've seen two different ensembles with the same limiting density.",
            "So is the same circle law sort of universal like?",
            "Do all matrix model have a semicircle low as the density of eigenvalues?",
            "And if not, can we derive the corresponding spectral density for any matrix matrix model?",
            "And if we can't, why is it so?",
            "So the answers are well symmetrical, low is not very, very universal, but is quite robust.",
            "So it is not true that all all the the ensembles have the same circle low.",
            "But there are many who does.",
            "Anne.",
            "We cannot derive the corresponding special density for any matrix model that we want, we cannot.",
            "So whether we can or we can't depends on some classification that I'm going to present to you, the.",
            "Next slides OK."
        ],
        [
            "First of all, I limit my talk in my investigation.",
            "2.",
            "Matrices with real eigenvalues OK.",
            "So matrices with real eigenvalues means that we have some symmetry in the matrix such that the spectrum is real.",
            "I will not consider the case of complex complex spectrum.",
            "This is a whole whole new theory.",
            "And I will not have time to talk about that, so I give you a few.",
            "Hints about how to classify models with real eigenvalues.",
            "So first big big class is the class of matrices with independent entries.",
            "Well.",
            "Well thank you.",
            "Thanks so independent interest means that.",
            "The joint density of the entries.",
            "Factorizes so we have the entries on the diagonal and the real and imaginary parts of the entries of the off diagonal elements.",
            "OK, so here we are considering complex permission matrices, but of course for real matrices the stuff is very simple.",
            "So these matrices with some properties on the FIS are usually called vigner vigner matrices.",
            "OK, now you need to be careful because in random matrix theory everything is named after Wigner or die soon so.",
            "So one could be tempted to say, OK, the Wigner matrices maybe are those whose spectral density is the Wigner semi circle law and this is false.",
            "This is not true.",
            "It is there called Wigner matrices, because I don't know for historical reasons, so those are just.",
            "Models with independent entries.",
            "Yep.",
            "Always ID or there's like some sort of covariance constant here.",
            "Here it is not necessary.",
            "You see, there is an index for this for the function.",
            "So each entry can have different function, But the point is that you have a factorized form, so they can be independent but not identically distributed.",
            "This is fine, but what is important is that they are independent, so the joint density factorizes.",
            "Random matrix.",
            "So let's assume that everything is Gaussian, yes?",
            "We assume that they are independent from each other.",
            "Always orders covariance as well.",
            "That will be also that case of random matrix.",
            "Well, you can.",
            "You can include.",
            "You can include the covariance in there.",
            "I will give you OK. Of course, if you have a covariance you the you mean that you are introducing correlations between between the entries.",
            "So this this example does not fall into this category.",
            "This theory means it's like a very general yes, yes.",
            "So the covariance function of some sort of intersection of this field.",
            "You can, you can construct random matrix theories for Gaussian random variables with a given covariance.",
            "It does exist, but it is not in this.",
            "In this bit it does exist, yes."
        ],
        [
            "So there is a second big group.",
            "That is called rotational invariant or just invariant models.",
            "So the definition for this group is a bit more complicated, so it is P of H is equal to P of UHU to the minus one OK.",
            "So this means that the concept is quite simple.",
            "It means that if you take 2 matrices in your ensemble and these two matrices are similar to each other, so there is a similarity transformation that brings one into the other.",
            "Then the statistical weight of the two is the same.",
            "For example, for HA real symmetric matrix you can take you as an orthogonal matrix.",
            "If H is your mission, you can take you as a unitary matrix, and so on and so forth.",
            "So all matrices that are obtained one from each other by a similarity transformation have the same statistical weight.",
            "They occur in the ensemble with the same weight with the same probability.",
            "The condition is a bit technical, but in essence it means that the eigenvectors of these models are not that important because whatever rotation you make into your matrix, you send your matrix into another one which has the same probability distribution as the first one.",
            "So you can rotate your matrices as you wish, but they're not very important.",
            "You know the angle vectors are not very important.",
            "This means that we have some hope to integrate out the eigenvectors.",
            "In these models, because they are not important.",
            "So for these models we have some hopes to go from entries to eigenvalues."
        ],
        [
            "OK, as you can see there is an intersection.",
            "That one symbols have an intersection, the intersection.",
            "Well, we we know for sure.",
            "One ensemble that lies in the intersection, the Gaussian ensemble that we just.",
            "We just saw.",
            "So if you think.",
            "D. Entries with this probability law.",
            "OK, it is a factorized form, so it is independent entries.",
            "But provided that you take the variance of the off diagonal elements scale with an extra factor of 2.",
            "This is a very important technical requirement.",
            "You can recast this object into this form E to the minus 1 / 2 Sigma square.",
            "The trace of H square.",
            "So the trace of a matrix square is just this object, but there is a factor of two that is very important.",
            "So if you can write your weight in this form, then clearly the model is rotationally invariant, because trace of H square.",
            "The trace of each square is equal to the trace of UHU minus one all square.",
            "By the cyclical property of that race.",
            "OK, so this this ensemble belongs simultaneously to the two groups.",
            "Provided that you scale the variance of the off diagonal elements with a factor of 2.",
            "The matrix, yes, they are symmetric in this in this case because they are real symmetric.",
            "Schedule, but well, it's because the way to see it.",
            "Sure, because because you need to, you need to sum the.",
            "But what I'm saying is that in principle you could define a random matrix model where you put another another number here.",
            "Because you can scale the variance of the off diagonal elements as you want in that case, technically the ensemble, even if it is morally the same.",
            "Technically it will not belong to the ensemble with rotational invariance because you are not able to write the weight in an invariant form.",
            "So this ensemble is called GOE.",
            "So Gaussian orthogonal ensemble.",
            "Of course, here the names are chosen in a very peculiar way, because the Gaussian orthogonal ensemble does not contain orthogonal matrices.",
            "The Gaussian orthogonal ensemble contains real symmetric matrices that are diagonalized by orthogonal matrix.",
            "OK.",
            "So gaussian.",
            "Orthogonal ensemble for real symmetric.",
            "Now question Gaussian ensemble and here what is there if the matrix is complex?",
            "Permission.",
            "How how you call you ensemble?",
            "Which letter?",
            "You write Gaussian unitary ensemble.",
            "It does not contain unitary matrices, it contains matrices that are diagonalized by unitary matrix.",
            "Gaussian.",
            "Sample.",
            "What union?",
            "Self do matrices.",
            "What is the letter?",
            "You have only 2119 possibilities.",
            "S Gaussian symplectic ensemble contains matrices that are diagonalized by a symplectic transformation.",
            "Good.",
            "So now we have seen that there is one element in the intersection.",
            "I'm sure it will not appear in the next slide, but OK.",
            "So do we have other ensembles that lie in the intersection?",
            "Survey yes or no?",
            "No gosh, so you say we have.",
            "We have other ensembles that like here.",
            "Just ask.",
            "Trace of age before.",
            "Three traits of each to the 4th does not have independent entries.",
            "Nobody, nobody cares.",
            "Yeah."
        ],
        [
            "I was sure here was the first front page.",
            "OK, just imagine that there is a front page of the paper.",
            "There is a theorem by Porter and Rosenzweig, 1960.",
            "So this paper was published in the.",
            "Tunnels.",
            "Off the finish.",
            "Academy of science.",
            "So it is probably the most important theorem in random matrix theory, but it appears in a Journal that.",
            "It's basically impossible to find.",
            "I struggled quite a lot and I I'm quite disappointed.",
            "So the what the theorem says is that the Gaussian ensemble is the only ensemble that is simultaneously rotationally invariant and has independent entries, so these are no go theorem.",
            "You have only one ensemble in the intersection.",
            "So this is clearly a bad news, but piece of news, right?",
            "Why?",
            "Because it has some consequences.",
            "It means that.",
            "If you take one ensemble at random in the space of all ensembles, you may fall in here.",
            "So very nice independent entries.",
            "It is easy to simulate.",
            "You can populate your matrix on the computer very easily, but it does not have rotational invariance, so the eigenvectors are important, so the eigenvalues and the egg and vectors cannot be the coupled.",
            "So we are in deep troubles.",
            "Or you may land here you have rotational invariants, so great so we can dig up all we can have some hope because the angle vectors are not important week and a couple eigenvalues and lagging vectors blah blah blah.",
            "But they don't have independent entries, so they are harder to simulate.",
            "Because you cannot just populate your matrix 1 by 1.",
            "You need to take into account the correlations.",
            "So there's only one.",
            "I will give you, I will send you a copy of this."
        ],
        [
            "So the Gaussian sample.",
            "Is the only one in the intersection we said right?",
            "Here is a paper.",
            "Buy Dyson in which Dyson said, OK, it was shown by Parton rose and like that if you apply the two requirements, independent entries and rotational invariants, this is the only guy that survives OK?",
            "And those on who has in mind applications to nuclear physics says, OK, the requirement of rotational invariances quite natural.",
            "OK, because if you think of it.",
            "Hamiltonian terms you would like to construct the theory that is not really whose energy levels do not depend on which basis you choose.",
            "You might choose another basis.",
            "The energy levels will be the same as they are physical observables.",
            "They don't change.",
            "On the other hand, the requirement one, which is the independence of matrix elements, is artificial and without clear physical motivation and doesn't doesn't then ask the question WHI is this while we are forced.",
            "Too small.",
            "To have some unsatisfactory feature of this this independence stuff, because, well, we cannot define a uniform probability distribution on an infinite range.",
            "So what we would like to have our matrix entries so transition rates that are uniformly distributed because we don't know we don't know anything.",
            "So we would like to have the maximal maximal entropy ensemble something we don't know anything, but we cannot put a flat measure on the whole real line.",
            "So somehow we need to we need to cut it.",
            "We need to find a way to cut the distribution of the matrix entries and the only way to do it in a way that is compatible with this requirement is to choose.",
            "Gaussian Gaussian distribution.",
            "So this is a very artificial artificial thing in this nuclear physics.",
            "An idea?"
        ],
        [
            "OK, so then you might wonder, is there anything else in the universe of random matrices with real entries?",
            "Well, yes, you might be.",
            "If you are outside these two groups.",
            "Well, the situation becomes complicated, so you might as well want to change job or do go abroad or do whatever you want.",
            "But if you have a model that lives here, you don't have many chances.",
            "But still you can be extremely lucky an land into this very small group we have.",
            "I don't know a handful of cases I tried to track down maybe 5 to 10.",
            "Mumbles that I called.",
            "Value models, so these are very strange ensembles, so for this ensembles you are able to write down the joint density of the eigenvalues.",
            "OK.",
            "So you have the P of Lambda one Lambda N, so you have all the statistical control on the eigenvalues.",
            "But on the other hand, it is difficult to find the P of H, so the probability of the entries are is paradoxically more difficult or not interesting to obtain.",
            "I give you an example.",
            "Suppose the example is truncations of matrices.",
            "So suppose that you have a big matrix and you have some P of some POS.",
            "Some probability density on the space of these matrices.",
            "OK, and you know that this matrix has a block block structure like RR Prime D prime.",
            "Say, OK, let's take this.",
            "This sub block and produce the matrix TT dagger.",
            "So the matrix, the dagger is our mission, so it has real eigenvalues.",
            "Let's call it T1T N. So you might want to know something about T1T N. Boeing POS.",
            "But not P of TT dagger.",
            "There is, you know, it's difficult to obtain is not not interesting.",
            "What you have is the probability of the full matrix, but you want to find some something related to a submatrix of it, so there is.",
            "There is a clear application of this in the field of like.",
            "Celtic Transport electronic transport.",
            "So you can have some cavities.",
            "And some electrons that come in scatter and goes out.",
            "OK, so you can describe the system like this by a scattering matrix.",
            "OK, this is a unitary.",
            "User matrix that connects the wave function of the incoming electron to the wave function of the outgoing electron.",
            "So it is unitary because it must conserve the probability the flux.",
            "So it turns out that the scattering matrix for this problem is precisely this structure.",
            "We have reflection blocks.",
            "End transmission blocks.",
            "Because the electron can come here, get scattered and go out from the same side.",
            "Same thing on the left or can be transmitted.",
            "So the scattering matrix has a block structure.",
            "Now it turns out that the singular values of T of the transmission matrix are the one related to the physical properties of the system, not the eigenvalues of S in values.",
            "S are not interesting because they are just pure phases.",
            "The matrix is unitary, but the eigenvalues, the singular values of T are interesting.",
            "So here we are in a case where the joint distribution of the eigenvalues is known, but the joint distribution of the entries is more complicated.",
            "But OK, here there are just a handful of cases, and, well, you must be really lucky to land in here.",
            "Here.",
            "The intersection between the two is also interesting.",
            "These are models with independent entries, but for which you have the eigenvalues.",
            "The joint distribution of eigenvalues.",
            "There is 1 famous example which is this Dimitri Edelman model and will see if I have time to this very very important for numerical simulations.",
            "So essentially the point is that the Gaussian.",
            "The."
        ],
        [
            "The Gaussian structure as the only nice property of being multiplicative in the probabilities, but additive in the independent because what you have here.",
            "Basically, you must.",
            "You must complete a trace of some sort in the exponent, and a trace is a sum of objects, so you need something that.",
            "That is, that has a multiplicative structure.",
            "But in this in the exponent reproduces a trace.",
            "So in some sense, if you think about it, there are not many possibilities because how to convert a product into some?",
            "Well, the log, the logarithm and exponential do it.",
            "Song."
        ],
        [
            "OK, now there is a survey moment.",
            "So if I give you the choice between these two sets.",
            "Which one would you prefer to build your scientific career on?",
            "Which one guarantees the highest number of papers?",
            "Which one guarantees the to have a very successful career?",
            "What?",
            "For which of the two sets?",
            "We have a lot of tools, a lot of techniques, a lot of weapons.",
            "The Independent one who says independent?",
            "What no?",
            "No razor.",
            "Enter 3 who says rotational invariant.",
            "Who says nothing?",
            "OK, so this is.",
            "This was a trap of course.",
            "Well, I mean.",
            "Then the point is that it is a bit deceiving or paradoxical or a bit counter intuitive, because if we have just random variables.",
            "Clearly the case of independent random variables is the simplest.",
            "It is the case you know which is the case that you would start studying, and probably you can finish University without having encountered correlated random variables, right?",
            "But of course you always see independent random variable, so we have a lot of weapons for independent random variables.",
            "But somehow in the case of random matrices the."
        ],
        [
            "The thing is completely different.",
            "So.",
            "We know nothing about these guys.",
            "We know very little about matrices with independent entries.",
            "It is amazing, amazing how little we know.",
            "We don't have weapons we don't have analytical tools.",
            "We have really nothing.",
            "We we we.",
            "We know a lot about matrices with rotational invariance.",
            "We have a lot of weapons.",
            "But very little for independent entries.",
            "Say spectral density density of eigenvalues.",
            "Models with independent entries with incredibly complicated calculation, we may be able to find out the spectral density of one of those models for N to Infinity in the end to Infinity limit.",
            "For finite N, we know nothing.",
            "In general we know nothing.",
            "Here we have explicit formula for finite N for infinite N and it goes very smoothly.",
            "Spacing between eigenvalues, nothing.",
            "We have basically no results here.",
            "We know nothing here.",
            "We have a whole theory of.",
            "Largest eigenvalues, wonderful theory here.",
            "Here we are starting to get something like 2000 Eleven 2010 results like this.",
            "So for 60 years we knew nothing about about that.",
            "So the independent entries case is the by far the most complicated of the two.",
            "Huawei.",
            "Huawei.",
            "In some sense, we should be able to answer this question right now alright?",
            "Because independent entries.",
            "What is the problem here?",
            "The problem is that eigenvectors here are very important.",
            "Because you have one matrix with independent entries in one basis.",
            "If you make a transformation rotation, you mix all the entries together.",
            "So what is independent in one basis is no longer independent in any other.",
            "So the eigenvectors are very important, so it means that eigenvalues and eigenvectors are not easy to the couple.",
            "Here, the eigenvectors are not important, so we can.",
            "Integrate them out and you know reduce the degrees of freedom from North Squared entries to N eigenvalues.",
            "So here we have a lot of weapons.",
            "To use.",
            "This is the.",
            "Roughly speaking, the idea."
        ],
        [
            "So this is.",
            "Basically a summary well.",
            "This.",
            "This transformation between.",
            "Entries and eigenvalues.",
            "Is not always possible, so it is possible here for rotational invariant models.",
            "It is possible here for the eigenvalue models because by definition here we have gang and values from scratch.",
            "It is in general not possible for matrices with independent entries and for theorie it is not possible here, here, here.",
            "So if you if you land here, matrices that have correlated entries, no invariants and no eigenvalues.",
            "There's no chance.",
            "OK. Now I give you a simplified summary.",
            "So the idea of a simplified."
        ],
        [
            "Marie came to me.",
            "From a picture that was sent to my Facebook like a simplified map of London, you have losers here and very rich here.",
            "When I was in London I was living here.",
            "Too bad.",
            "OK, so this is the takehome message.",
            "For this first part.",
            "You should.",
            "You can forget all I said man, you probably did it already, but just just keep this in mind.",
            "This is very important.",
            "Yeah, matrices with independent entrance.",
            "Very bad horrible.",
            "Rotationally invariant models very good ikenberry models very good, but very few.",
            "OK. OK."
        ],
        [
            "So, generalities.",
            "So we'll talk a bit about what can we expect in general.",
            "For the spectrum of a random matrix, OK."
        ],
        [
            "OK, the first thing to notice is that random eigen values do not behave at all like random points, say on a segment.",
            "So if you think if you take a segment and you throw at random with uniform distribution some points.",
            "This is the pattern that you will get.",
            "If you take the eigenvalues of a Gaussian matrix, this is the pattern you will get.",
            "So you will see that here in the case of random points uniformly distributed, you have a lot of kissing.",
            "You have a lot of very close.",
            "Encounters very close meetings, so random points are online.",
            "They tend to cluster.",
            "They like to stay very close to each other.",
            "Eigenvalues of a random matrix they don't like to stay very close to each other.",
            "They repel each other.",
            "You have a very large.",
            "Much larger spacing between adjacent points.",
            "This is a very general feature of random matrix model.",
            "In the case of invariant model, we are able to spot analytically why this is the case.",
            "And we will see it while they repair."
        ],
        [
            "Another concept that is recurrent in papers or books on random matrix.",
            "Theory is the concept of universality.",
            "So universality means that you can put different phenomena under the same or within the same framework.",
            "So for example, here we have.",
            "Energy levels of some resonances of a heavy nucleus.",
            "I don't remember which one from maybe erbium or something.",
            "Which is superimposed.",
            "The theoretical curve is the distribution of spacings between eigenvalues of a goed matrix.",
            "So you see the repulsion is the probability of finding a spacing S. So if the spacing is very low.",
            "The probability is very low.",
            "So you don't find small spacings.",
            "Level repulsion if you add random points on a line you would get a person distribution.",
            "So basically an exponential distribution of spacings, so small spacings are very very likely.",
            "So the behavior of a small spacings is markedly different into 2 cases.",
            "Now here we have the same theoretical curve and two empirical plots.",
            "I think the.",
            "The squares are.",
            "Distances between parked cars.",
            "On a street in London.",
            "OK.",
            "So this is interesting because, well, OK, I know the guy who did this research, so he was sent out in London to measure the spacings between cars that were parked on on street, so clearly.",
            "The cars don't like to stay too close to each other, right?",
            "On the other hand, they don't like to stay too far because it doesn't make sense, right?",
            "So this is the typical profile you get.",
            "Of course, the legend tells that the guy was questioned by the police and almost arrested because.",
            "But the other the crosses are the distances between birds.",
            "That are sitting on a wire on another Christy.",
            "Why so?",
            "The birds don't like to.",
            "To stay too close to each other.",
            "But they also don't like to stay there, and too far from each other.",
            "The interesting thing is that you have some universal universal feature that can be fitted by the same, roughly the same curve.",
            "Here we have another.",
            "These are zeros of Riemann's Zeta function, but I will talk extensively about this later.",
            "So all these phenomena can be fitted by a curve like this.",
            "Each of the minus square at large spacings, and as to the beta for small spacing where this beta in the case of random matrices E is the Dyson index beta.",
            "1, two or four that we.",
            "We saw the beginning, so this is called the Wigner Dyson law.",
            "I told you that everything is named after this."
        ],
        [
            "Oh great, so if you are interested, well, this is clearly the least interesting slide, but it was perfect.",
            "OK, so here are some papers about the distribution of Parking's and gap size distribution of parked cars.",
            "I don't know if someone is interested in."
        ],
        [
            "OK.",
            "So here.",
            "There's a classical picture of possible situations that arise when calculating spacings.",
            "So all these six cases have the same average, the same mean level spacing.",
            "So the mean is the same.",
            "But of course you can see very different patterns here.",
            "So here we have the uniform distribution.",
            "Off of spacings.",
            "Here we have the distribution of zeros of the Riemann Zeta function, and we talk extensively about this later.",
            "So you see level repulsion here.",
            "This is our the energy levels of Essanay billiard.",
            "This is the resonant scattering energies of nucleus of erbium.",
            "This is the distribution of primes.",
            "And this is the person distribution.",
            "So you see both in the case of person distribution, we have a lot of key things.",
            "A lot of a lot of very close encounters between the levels.",
            "And the same happens with the distribution of primes.",
            "Why?",
            "Because we have a lot of twin primes.",
            "Twin primes are the primes that are that differ by two.",
            "OK, like 11 and 13.",
            "And so on and so forth.",
            "So we have a lot of.",
            "Actually about the distribution of primes, well, there's just a digression to relax."
        ],
        [
            "A bit so you know that there is a twin prime.",
            "Twin prime conjecture.",
            "The Twin Prime Conjecture is a very well known unsolved problem in number theory that says that there are an infinite number of pairs of twin primes.",
            "So if you go all the way to Infinity you will will keep finding twin primes.",
            "That means primes that differ by two.",
            "So now apparently this year there was a very nice breakthrough in Prime theory.",
            "There was an unknown mathematician like Mr. Nobody who actually was able not to prove the Twin Prime conjecture, but version that.",
            "Shows that there is some number N smaller than 70 million.",
            "Such that there are infinitely many primes of many pairs of primes that differ by N. So he bounded the gap between consecutive primes.",
            "So now the work of mathematician will be to put this number down from 70 million to two.",
            "Sorry, yeah, now it's about 5000 something, yes.",
            "Yeah I think so.",
            "Yes, this is what they but I just found it quite interesting.",
            "The other thing I checked, I checked this news on on different articles so most of the journalists didn't mention what he did, but all of them mention was young.",
            "He was working at a subway sandwich shop before getting a job as a lecture.",
            "So apparently for the vast large number of journalists, the relative importance of these two bits of information.",
            "Is history."
        ],
        [
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I was asked to give a few introductory lectures on the topic of random matrix theory and in particular new applications of theory that is quite old.",
                    "label": 0
                },
                {
                    "sent": "This is almost a century old, as we will see it is main important application in statistical mechanics and not only there, and so let me start.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, I just borrowed my slide from talk by Alan Edelman, who is one of the world leading experts in random matrix theory.",
                    "label": 1
                },
                {
                    "sent": "So his question is why are random matrix eigenvalues cool and this message is OK?",
                    "label": 0
                },
                {
                    "sent": "Take any important mathematics, then randomize and this will have many applications.",
                    "label": 0
                },
                {
                    "sent": "So random matrix theory basically is a combination of linear algebra.",
                    "label": 0
                },
                {
                    "sent": "And randomness or linear algebra and probability theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so matrices are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite important, even though they have bad reputation, and since that usually proves that involves matrices can be shortened by 50% if one throws the matrices out, so we will see that indeed we can develop the theory without making any or very few references to actual matrices.",
                    "label": 1
                },
                {
                    "sent": "The good thing about random matrix matrices is that you can simulate them.",
                    "label": 0
                },
                {
                    "sent": "You can open your C++, Matlab, Mathematica.",
                    "label": 0
                },
                {
                    "sent": "Do produce your matrices, diagonalize them and study the statistics of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So in the notes that I prepared that have been circulated, I used Matlab to perform numerical simulations.",
                    "label": 0
                },
                {
                    "sent": "And of course you are encouraged to take a step yourself and try to reproduce the codes that I put in there and have a touch on the real stuff.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have any questions, please interrupt me now or call.",
                    "label": 0
                },
                {
                    "sent": "Call to me come to me later.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me introduce the characters of this story.",
                    "label": 0
                },
                {
                    "sent": "The fathers of random matrix theory.",
                    "label": 0
                },
                {
                    "sent": "First John Wishart in the 20s.",
                    "label": 1
                },
                {
                    "sent": "Eugen Vigner in the 50s and Freeman dies on 50s and 60s and so on.",
                    "label": 0
                },
                {
                    "sent": "He is the only one in color because he's still alive.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the paper that is thought to be starting the field of random matrices.",
                    "label": 0
                },
                {
                    "sent": "Distribution of the roots of certain symmetric matrices by Wigner in 1957.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you that actually the history of random matrices is a bit more complicated and longer than that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a unusual old story that is taught in all the textbooks about random matrices.",
                    "label": 0
                },
                {
                    "sent": "According to which the theory was born, because people were trying to understand the Hamiltonian.",
                    "label": 0
                },
                {
                    "sent": "So the energy levels of complex nuclei nuclei that are made up of a large number of new clones.",
                    "label": 0
                },
                {
                    "sent": "So one would be tempted to try to write down Hamiltonian operator for this this guy to diagonalize it and to study the energy levels.",
                    "label": 0
                },
                {
                    "sent": "Actually, due to the large number of nucleons this this task is quite becomes rapidly quite hopeless.",
                    "label": 0
                },
                {
                    "sent": "At the same time, there was an idea put forward by the guys I showed you before that actually Hamiltonian in a given basis is just can be thought of as a huge matrix.",
                    "label": 1
                },
                {
                    "sent": "Matrix elements are just, you know, like hopping, hopping rates and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you might just want to say OK, we don't know anything about the actual structure of this guy, so we take the matrix entries at random with some probability distribution, and then we study the properties of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Maybe some regularity will emerge, maybe some statistical universality will emerge.",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "We can try to do this.",
                    "label": 0
                },
                {
                    "sent": "This procedure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But very soon one realizes that the history of random matrices goes far back in the past.",
                    "label": 0
                },
                {
                    "sent": "So the 1st paper that has ever appeared or is thought to have ever appeared on the subject is from 1928.",
                    "label": 0
                },
                {
                    "sent": "John Wishart is a statistician.",
                    "label": 0
                },
                {
                    "sent": "And he started random matrices in this constant context of covariance estimation for the multivariate normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So he basically studied.",
                    "label": 0
                },
                {
                    "sent": "Random covariance matrices of normally distributed data.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the first ever appearance.",
                    "label": 0
                },
                {
                    "sent": "The word the expression random matrix theory did not appear there, but still we have a lot of results that have been later rediscovered many times, and so this is the first ever appearance of random matrix in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also for Norman 1947, so 15 years before the actual date of birth of random matrix theory, did some work on some numerical bounds for floating point errors in some lower upper decomposition.",
                    "label": 0
                },
                {
                    "sent": "So he was basically interested in finding the upper bound for the norm of a random matrix.",
                    "label": 0
                },
                {
                    "sent": "Again, he never mentioned the word random matrix or random matrix theory, but still this is one of the this is the second occurrence if you want.",
                    "label": 0
                },
                {
                    "sent": "Girlfriend of matrix theory before.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That slide was to mention that even in 1939 there was another quite important result.",
                    "label": 0
                },
                {
                    "sent": "But OK, we will come to that later if it's going to work OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the first our first encounter with the random matrix you set N = 5 and you produce a 5 by 5 matrix that is symmetric.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just populate the matrix by sampling random Gaussian variables.",
                    "label": 0
                },
                {
                    "sent": "OK so you have some positive numbers, some negative numbers, numbers mostly around 0.",
                    "label": 0
                },
                {
                    "sent": "So this is a symmetric matrix, so you can diagonalize it and you get the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "What you see is that there are eigenvalues quite close to 0.",
                    "label": 0
                },
                {
                    "sent": "There are three negative eigenvalues and two positive, so this is a quite typical thing, so you will get.",
                    "label": 0
                },
                {
                    "sent": "An Alpha positive, an Alpha negative eigenvalue, roughly.",
                    "label": 0
                },
                {
                    "sent": "And typically we will be interested in taking N to Infinity, so a very large matrix size, but sometimes even finite and results can be can be interesting so.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic goal of random matrix theory can be summarized here.",
                    "label": 1
                },
                {
                    "sent": "I give you an object that is the joint probability density of the entries.",
                    "label": 1
                },
                {
                    "sent": "So it is a P of H11 H&N.",
                    "label": 0
                },
                {
                    "sent": "And from this you try to compute as much as you can about the eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "The eigenvalues are random variables because the entries are.",
                    "label": 0
                },
                {
                    "sent": "So you may compute, try to compute the average density of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "We will see what this is.",
                    "label": 0
                },
                {
                    "sent": "The distribution of spacings between adjacent eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "The distribution, for example, of the largest or the smallest eigenvalue correlation between eigenvalues and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ideally, if I give you this object.",
                    "label": 0
                },
                {
                    "sent": "Need your first thought?",
                    "label": 0
                },
                {
                    "sent": "Should be OK. Maybe from this object I can derive a similar object which is the joint density of the N eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "After all, we're talking about random variables, so if I can derive something like this, then all the statistical properties of this of the eigenvalues will be encoded in this object.",
                    "label": 0
                },
                {
                    "sent": "Note that here we have N square degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "And here we have only N degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "So is this this reduction of degrees of freedom clearly implies that we are wiping out some information.",
                    "label": 0
                },
                {
                    "sent": "So is this reduction always possible?",
                    "label": 1
                },
                {
                    "sent": "Well, the answer is not.",
                    "label": 1
                },
                {
                    "sent": "Not always, not not for some basic principle or some.",
                    "label": 0
                },
                {
                    "sent": "It's just that we don't have the technical tools to deal with this reduction in the general case.",
                    "label": 0
                },
                {
                    "sent": "And I will show you why this is so.",
                    "label": 0
                },
                {
                    "sent": "So this this stuff is for you.",
                    "label": 0
                },
                {
                    "sent": "So you are the ones that will maybe help us breaking through this this problem and give us some more weapons to talk to attack it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's come back to Laura to our matrix.",
                    "label": 0
                },
                {
                    "sent": "We compute the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So now let's repeat this experiment numerically.",
                    "label": 1
                },
                {
                    "sent": "There are some quotes in the notes many times like 10,000 times.",
                    "label": 0
                },
                {
                    "sent": "For each sample of your matrix, you compute the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So you will get at the end 5 * 10,000 eigenvalues 50,000 numbers, and then you just make a histogram of all the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So what is the shape that you will get?",
                    "label": 0
                },
                {
                    "sent": "This is what is called average spectral density or average density of states or density of states or counting function or frequency counting function, and I think there are about 17 different definition for the same for the same object.",
                    "label": 0
                },
                {
                    "sent": "This is just the the histogram of all the eigenvalues of all the samples.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you do it, these figures are taken from page 27 of my notes.",
                    "label": 0
                },
                {
                    "sent": "You will get for the N = 5.",
                    "label": 0
                },
                {
                    "sent": "A curve that looks like like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so the density is very high, close to zero.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of eigenvalues that are close to 0.",
                    "label": 0
                },
                {
                    "sent": "Very few eigenvalues that are larger than five.",
                    "label": 0
                },
                {
                    "sent": "And we have this oscillations.",
                    "label": 0
                },
                {
                    "sent": "How many 123455 is equal to N?",
                    "label": 0
                },
                {
                    "sent": "Next, you can do the same.",
                    "label": 0
                },
                {
                    "sent": "This is a symmetric curve.",
                    "label": 0
                },
                {
                    "sent": "So in principle, on average you get same number of eigenvalues that are positive and or negative.",
                    "label": 0
                },
                {
                    "sent": "Now you do the same experiment with N = 10.",
                    "label": 0
                },
                {
                    "sent": "You see that the spectrum widens a bit.",
                    "label": 0
                },
                {
                    "sent": "It invades a bit more of the real axis.",
                    "label": 0
                },
                {
                    "sent": "And you get a lot of wiggles.",
                    "label": 0
                },
                {
                    "sent": "How many 123456789 ten?",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this this curve like light blue curve is for N = 25.",
                    "label": 0
                },
                {
                    "sent": "You see that the oscillations get smoothed out.",
                    "label": 0
                },
                {
                    "sent": "There are many more oscillations, but much thinner.",
                    "label": 0
                },
                {
                    "sent": "And you can see that there is a limiting shape that starts to appear OK. We will see what what this is notes that for a matrix 25 * 25 You will not find many eigenvalues larger than 10, which is just twice as much as a matrix that is 5 times smaller.",
                    "label": 0
                },
                {
                    "sent": "So the rate of growth of the eigenvalues you know is not linear in.",
                    "label": 0
                },
                {
                    "sent": "And then it is much lower than.",
                    "label": 0
                },
                {
                    "sent": "Here in the second panel we have again the case 123456789 ten 10 by 10 and the dots are numerical simulations.",
                    "label": 0
                },
                {
                    "sent": "So that is the histogram and the black curve is the analytical results.",
                    "label": 0
                },
                {
                    "sent": "So we do actually have an analytical prediction for the spectral density of a 10 by 10 Gaussian matrix.",
                    "label": 0
                },
                {
                    "sent": "Or 20 by 20 four 9 by 9 within finite.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what?",
                    "label": 0
                },
                {
                    "sent": "What is the limiting?",
                    "label": 0
                },
                {
                    "sent": "The limiting curve is what is called the weakness semicircle low, so it says that the density of eigenvalues for end very large converges to a scaling form that is 1 / sqrt 2 beta N times a function of Lambda over root of 2 beta.",
                    "label": 0
                },
                {
                    "sent": "Where this function F is 2 / \u03c0 root of 1 -- X square.",
                    "label": 0
                },
                {
                    "sent": "So actually the curve that I showed you before.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This dashed dotted line here.",
                    "label": 0
                },
                {
                    "sent": "Is precisely.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This one so it is one over root of 2 beta times N was 10.",
                    "label": 0
                },
                {
                    "sent": "And beta was one and I will show you why.",
                    "label": 0
                },
                {
                    "sent": "Times 2 / \u03c0 root of 1 minus Lambda square over 2 beta, so 1 * 10.",
                    "label": 0
                },
                {
                    "sent": "So this is the curve that I drew on this.",
                    "label": 0
                },
                {
                    "sent": "On this panel.",
                    "label": 0
                },
                {
                    "sent": "So this beta is an index, is a discrete index that can take just three values, 1, two, or four.",
                    "label": 0
                },
                {
                    "sent": "Which is called the Dyson Index of the ensemble and gives rise to what was called the Dysons threefold way.",
                    "label": 1
                },
                {
                    "sent": "OK so beta equals one corresponds to matrices that are real whose entries are real and symmetric.",
                    "label": 0
                },
                {
                    "sent": "Beta equals two corresponds to emission matrices, so complex admission.",
                    "label": 0
                },
                {
                    "sent": "And we take those four corresponds to matrices whose entries are quaternions.",
                    "label": 0
                },
                {
                    "sent": "And they have a symmetry such that the spectrum is real, so there are only three.",
                    "label": 0
                },
                {
                    "sent": "Three types of such matrices that are indexed by one.",
                    "label": 0
                },
                {
                    "sent": "If the entry is characterized by just one number real.",
                    "label": 0
                },
                {
                    "sent": "Two for complex, an four for quaternions.",
                    "label": 0
                },
                {
                    "sent": "So question, why is it called?",
                    "label": 0
                },
                {
                    "sent": "Same circle Semi circle law.",
                    "label": 0
                },
                {
                    "sent": "Previous so this one, this one is the limiting curve.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're saying that this one is a semicircle.",
                    "label": 0
                },
                {
                    "sent": "OK, but the problem is that the radius here and here are different, right?",
                    "label": 0
                },
                {
                    "sent": "We have a 2 / \u03c0 and a one.",
                    "label": 0
                },
                {
                    "sent": "So what do?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Veganism circle is not.",
                    "label": 0
                },
                {
                    "sent": "It's not the same thing.",
                    "label": 0
                },
                {
                    "sent": "It is a semi ellipse.",
                    "label": 0
                },
                {
                    "sent": "Kinda.",
                    "label": 0
                },
                {
                    "sent": "I don't know why Bignor didn't call it semi circle.",
                    "label": 0
                },
                {
                    "sent": "It was called afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you think that the circle and an ellipse are the same thing, well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some some of the stuff doesn't come out and some does, so I have some 50% of chance to be able to finish the presentation.",
                    "label": 0
                },
                {
                    "sent": "OK, so here there was just the title of a paper on nine 1955 where which was the first occurrence.",
                    "label": 0
                },
                {
                    "sent": "I hope you can read some.",
                    "label": 0
                },
                {
                    "sent": "It's a bit light but I will.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just the first occurrence of the same circle law in random matrix theory.",
                    "label": 1
                },
                {
                    "sent": "And it was actually derived by vigner, but not for the type of matrix that I showed you before, not for Gaussian matrices.",
                    "label": 0
                },
                {
                    "sent": "It was derived for random sign symmetric matrices that are, so the two N + 1 real symmetric matrices diagonal elements are zero.",
                    "label": 0
                },
                {
                    "sent": "And the non diagonal elements are plus or minus V. With random signs, so outside the diagonal we have the same number but with random signs plus or minus.",
                    "label": 0
                },
                {
                    "sent": "So for this matrix the limiting Loaiza some circle.",
                    "label": 0
                },
                {
                    "sent": "This was derived two years before the degauss the case.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian case that is usually referred to as the prototype of some circle.",
                    "label": 1
                },
                {
                    "sent": "So originally the same circle was, which is not the same circle, was not derived for Gaussian matrices.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this clearly from some questions, because you see, we've seen two different ensembles with the same limiting density.",
                    "label": 0
                },
                {
                    "sent": "So is the same circle law sort of universal like?",
                    "label": 0
                },
                {
                    "sent": "Do all matrix model have a semicircle low as the density of eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "And if not, can we derive the corresponding spectral density for any matrix matrix model?",
                    "label": 1
                },
                {
                    "sent": "And if we can't, why is it so?",
                    "label": 0
                },
                {
                    "sent": "So the answers are well symmetrical, low is not very, very universal, but is quite robust.",
                    "label": 0
                },
                {
                    "sent": "So it is not true that all all the the ensembles have the same circle low.",
                    "label": 0
                },
                {
                    "sent": "But there are many who does.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We cannot derive the corresponding special density for any matrix model that we want, we cannot.",
                    "label": 0
                },
                {
                    "sent": "So whether we can or we can't depends on some classification that I'm going to present to you, the.",
                    "label": 0
                },
                {
                    "sent": "Next slides OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, I limit my talk in my investigation.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Matrices with real eigenvalues OK.",
                    "label": 1
                },
                {
                    "sent": "So matrices with real eigenvalues means that we have some symmetry in the matrix such that the spectrum is real.",
                    "label": 0
                },
                {
                    "sent": "I will not consider the case of complex complex spectrum.",
                    "label": 0
                },
                {
                    "sent": "This is a whole whole new theory.",
                    "label": 0
                },
                {
                    "sent": "And I will not have time to talk about that, so I give you a few.",
                    "label": 0
                },
                {
                    "sent": "Hints about how to classify models with real eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So first big big class is the class of matrices with independent entries.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Well thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks so independent interest means that.",
                    "label": 0
                },
                {
                    "sent": "The joint density of the entries.",
                    "label": 0
                },
                {
                    "sent": "Factorizes so we have the entries on the diagonal and the real and imaginary parts of the entries of the off diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we are considering complex permission matrices, but of course for real matrices the stuff is very simple.",
                    "label": 0
                },
                {
                    "sent": "So these matrices with some properties on the FIS are usually called vigner vigner matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, now you need to be careful because in random matrix theory everything is named after Wigner or die soon so.",
                    "label": 0
                },
                {
                    "sent": "So one could be tempted to say, OK, the Wigner matrices maybe are those whose spectral density is the Wigner semi circle law and this is false.",
                    "label": 0
                },
                {
                    "sent": "This is not true.",
                    "label": 0
                },
                {
                    "sent": "It is there called Wigner matrices, because I don't know for historical reasons, so those are just.",
                    "label": 0
                },
                {
                    "sent": "Models with independent entries.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Always ID or there's like some sort of covariance constant here.",
                    "label": 0
                },
                {
                    "sent": "Here it is not necessary.",
                    "label": 0
                },
                {
                    "sent": "You see, there is an index for this for the function.",
                    "label": 0
                },
                {
                    "sent": "So each entry can have different function, But the point is that you have a factorized form, so they can be independent but not identically distributed.",
                    "label": 0
                },
                {
                    "sent": "This is fine, but what is important is that they are independent, so the joint density factorizes.",
                    "label": 0
                },
                {
                    "sent": "Random matrix.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that everything is Gaussian, yes?",
                    "label": 0
                },
                {
                    "sent": "We assume that they are independent from each other.",
                    "label": 0
                },
                {
                    "sent": "Always orders covariance as well.",
                    "label": 0
                },
                {
                    "sent": "That will be also that case of random matrix.",
                    "label": 0
                },
                {
                    "sent": "Well, you can.",
                    "label": 0
                },
                {
                    "sent": "You can include.",
                    "label": 0
                },
                {
                    "sent": "You can include the covariance in there.",
                    "label": 0
                },
                {
                    "sent": "I will give you OK. Of course, if you have a covariance you the you mean that you are introducing correlations between between the entries.",
                    "label": 0
                },
                {
                    "sent": "So this this example does not fall into this category.",
                    "label": 0
                },
                {
                    "sent": "This theory means it's like a very general yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So the covariance function of some sort of intersection of this field.",
                    "label": 0
                },
                {
                    "sent": "You can, you can construct random matrix theories for Gaussian random variables with a given covariance.",
                    "label": 0
                },
                {
                    "sent": "It does exist, but it is not in this.",
                    "label": 0
                },
                {
                    "sent": "In this bit it does exist, yes.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a second big group.",
                    "label": 0
                },
                {
                    "sent": "That is called rotational invariant or just invariant models.",
                    "label": 0
                },
                {
                    "sent": "So the definition for this group is a bit more complicated, so it is P of H is equal to P of UHU to the minus one OK.",
                    "label": 0
                },
                {
                    "sent": "So this means that the concept is quite simple.",
                    "label": 1
                },
                {
                    "sent": "It means that if you take 2 matrices in your ensemble and these two matrices are similar to each other, so there is a similarity transformation that brings one into the other.",
                    "label": 0
                },
                {
                    "sent": "Then the statistical weight of the two is the same.",
                    "label": 0
                },
                {
                    "sent": "For example, for HA real symmetric matrix you can take you as an orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "If H is your mission, you can take you as a unitary matrix, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So all matrices that are obtained one from each other by a similarity transformation have the same statistical weight.",
                    "label": 0
                },
                {
                    "sent": "They occur in the ensemble with the same weight with the same probability.",
                    "label": 0
                },
                {
                    "sent": "The condition is a bit technical, but in essence it means that the eigenvectors of these models are not that important because whatever rotation you make into your matrix, you send your matrix into another one which has the same probability distribution as the first one.",
                    "label": 0
                },
                {
                    "sent": "So you can rotate your matrices as you wish, but they're not very important.",
                    "label": 0
                },
                {
                    "sent": "You know the angle vectors are not very important.",
                    "label": 0
                },
                {
                    "sent": "This means that we have some hope to integrate out the eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "In these models, because they are not important.",
                    "label": 0
                },
                {
                    "sent": "So for these models we have some hopes to go from entries to eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, as you can see there is an intersection.",
                    "label": 0
                },
                {
                    "sent": "That one symbols have an intersection, the intersection.",
                    "label": 0
                },
                {
                    "sent": "Well, we we know for sure.",
                    "label": 0
                },
                {
                    "sent": "One ensemble that lies in the intersection, the Gaussian ensemble that we just.",
                    "label": 1
                },
                {
                    "sent": "We just saw.",
                    "label": 0
                },
                {
                    "sent": "So if you think.",
                    "label": 0
                },
                {
                    "sent": "D. Entries with this probability law.",
                    "label": 0
                },
                {
                    "sent": "OK, it is a factorized form, so it is independent entries.",
                    "label": 1
                },
                {
                    "sent": "But provided that you take the variance of the off diagonal elements scale with an extra factor of 2.",
                    "label": 0
                },
                {
                    "sent": "This is a very important technical requirement.",
                    "label": 0
                },
                {
                    "sent": "You can recast this object into this form E to the minus 1 / 2 Sigma square.",
                    "label": 0
                },
                {
                    "sent": "The trace of H square.",
                    "label": 0
                },
                {
                    "sent": "So the trace of a matrix square is just this object, but there is a factor of two that is very important.",
                    "label": 0
                },
                {
                    "sent": "So if you can write your weight in this form, then clearly the model is rotationally invariant, because trace of H square.",
                    "label": 0
                },
                {
                    "sent": "The trace of each square is equal to the trace of UHU minus one all square.",
                    "label": 0
                },
                {
                    "sent": "By the cyclical property of that race.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this ensemble belongs simultaneously to the two groups.",
                    "label": 0
                },
                {
                    "sent": "Provided that you scale the variance of the off diagonal elements with a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "The matrix, yes, they are symmetric in this in this case because they are real symmetric.",
                    "label": 0
                },
                {
                    "sent": "Schedule, but well, it's because the way to see it.",
                    "label": 0
                },
                {
                    "sent": "Sure, because because you need to, you need to sum the.",
                    "label": 0
                },
                {
                    "sent": "But what I'm saying is that in principle you could define a random matrix model where you put another another number here.",
                    "label": 0
                },
                {
                    "sent": "Because you can scale the variance of the off diagonal elements as you want in that case, technically the ensemble, even if it is morally the same.",
                    "label": 0
                },
                {
                    "sent": "Technically it will not belong to the ensemble with rotational invariance because you are not able to write the weight in an invariant form.",
                    "label": 0
                },
                {
                    "sent": "So this ensemble is called GOE.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian orthogonal ensemble.",
                    "label": 0
                },
                {
                    "sent": "Of course, here the names are chosen in a very peculiar way, because the Gaussian orthogonal ensemble does not contain orthogonal matrices.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian orthogonal ensemble contains real symmetric matrices that are diagonalized by orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So gaussian.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal ensemble for real symmetric.",
                    "label": 0
                },
                {
                    "sent": "Now question Gaussian ensemble and here what is there if the matrix is complex?",
                    "label": 0
                },
                {
                    "sent": "Permission.",
                    "label": 0
                },
                {
                    "sent": "How how you call you ensemble?",
                    "label": 0
                },
                {
                    "sent": "Which letter?",
                    "label": 0
                },
                {
                    "sent": "You write Gaussian unitary ensemble.",
                    "label": 0
                },
                {
                    "sent": "It does not contain unitary matrices, it contains matrices that are diagonalized by unitary matrix.",
                    "label": 0
                },
                {
                    "sent": "Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Sample.",
                    "label": 0
                },
                {
                    "sent": "What union?",
                    "label": 0
                },
                {
                    "sent": "Self do matrices.",
                    "label": 0
                },
                {
                    "sent": "What is the letter?",
                    "label": 0
                },
                {
                    "sent": "You have only 2119 possibilities.",
                    "label": 0
                },
                {
                    "sent": "S Gaussian symplectic ensemble contains matrices that are diagonalized by a symplectic transformation.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So now we have seen that there is one element in the intersection.",
                    "label": 0
                },
                {
                    "sent": "I'm sure it will not appear in the next slide, but OK.",
                    "label": 0
                },
                {
                    "sent": "So do we have other ensembles that lie in the intersection?",
                    "label": 0
                },
                {
                    "sent": "Survey yes or no?",
                    "label": 0
                },
                {
                    "sent": "No gosh, so you say we have.",
                    "label": 0
                },
                {
                    "sent": "We have other ensembles that like here.",
                    "label": 0
                },
                {
                    "sent": "Just ask.",
                    "label": 0
                },
                {
                    "sent": "Trace of age before.",
                    "label": 0
                },
                {
                    "sent": "Three traits of each to the 4th does not have independent entries.",
                    "label": 0
                },
                {
                    "sent": "Nobody, nobody cares.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was sure here was the first front page.",
                    "label": 0
                },
                {
                    "sent": "OK, just imagine that there is a front page of the paper.",
                    "label": 0
                },
                {
                    "sent": "There is a theorem by Porter and Rosenzweig, 1960.",
                    "label": 0
                },
                {
                    "sent": "So this paper was published in the.",
                    "label": 0
                },
                {
                    "sent": "Tunnels.",
                    "label": 0
                },
                {
                    "sent": "Off the finish.",
                    "label": 0
                },
                {
                    "sent": "Academy of science.",
                    "label": 0
                },
                {
                    "sent": "So it is probably the most important theorem in random matrix theory, but it appears in a Journal that.",
                    "label": 0
                },
                {
                    "sent": "It's basically impossible to find.",
                    "label": 0
                },
                {
                    "sent": "I struggled quite a lot and I I'm quite disappointed.",
                    "label": 0
                },
                {
                    "sent": "So the what the theorem says is that the Gaussian ensemble is the only ensemble that is simultaneously rotationally invariant and has independent entries, so these are no go theorem.",
                    "label": 0
                },
                {
                    "sent": "You have only one ensemble in the intersection.",
                    "label": 0
                },
                {
                    "sent": "So this is clearly a bad news, but piece of news, right?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because it has some consequences.",
                    "label": 0
                },
                {
                    "sent": "It means that.",
                    "label": 0
                },
                {
                    "sent": "If you take one ensemble at random in the space of all ensembles, you may fall in here.",
                    "label": 0
                },
                {
                    "sent": "So very nice independent entries.",
                    "label": 0
                },
                {
                    "sent": "It is easy to simulate.",
                    "label": 0
                },
                {
                    "sent": "You can populate your matrix on the computer very easily, but it does not have rotational invariance, so the eigenvectors are important, so the eigenvalues and the egg and vectors cannot be the coupled.",
                    "label": 0
                },
                {
                    "sent": "So we are in deep troubles.",
                    "label": 0
                },
                {
                    "sent": "Or you may land here you have rotational invariants, so great so we can dig up all we can have some hope because the angle vectors are not important week and a couple eigenvalues and lagging vectors blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "But they don't have independent entries, so they are harder to simulate.",
                    "label": 0
                },
                {
                    "sent": "Because you cannot just populate your matrix 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "You need to take into account the correlations.",
                    "label": 0
                },
                {
                    "sent": "So there's only one.",
                    "label": 0
                },
                {
                    "sent": "I will give you, I will send you a copy of this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Gaussian sample.",
                    "label": 0
                },
                {
                    "sent": "Is the only one in the intersection we said right?",
                    "label": 0
                },
                {
                    "sent": "Here is a paper.",
                    "label": 0
                },
                {
                    "sent": "Buy Dyson in which Dyson said, OK, it was shown by Parton rose and like that if you apply the two requirements, independent entries and rotational invariants, this is the only guy that survives OK?",
                    "label": 0
                },
                {
                    "sent": "And those on who has in mind applications to nuclear physics says, OK, the requirement of rotational invariances quite natural.",
                    "label": 0
                },
                {
                    "sent": "OK, because if you think of it.",
                    "label": 0
                },
                {
                    "sent": "Hamiltonian terms you would like to construct the theory that is not really whose energy levels do not depend on which basis you choose.",
                    "label": 0
                },
                {
                    "sent": "You might choose another basis.",
                    "label": 0
                },
                {
                    "sent": "The energy levels will be the same as they are physical observables.",
                    "label": 0
                },
                {
                    "sent": "They don't change.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the requirement one, which is the independence of matrix elements, is artificial and without clear physical motivation and doesn't doesn't then ask the question WHI is this while we are forced.",
                    "label": 0
                },
                {
                    "sent": "Too small.",
                    "label": 0
                },
                {
                    "sent": "To have some unsatisfactory feature of this this independence stuff, because, well, we cannot define a uniform probability distribution on an infinite range.",
                    "label": 0
                },
                {
                    "sent": "So what we would like to have our matrix entries so transition rates that are uniformly distributed because we don't know we don't know anything.",
                    "label": 0
                },
                {
                    "sent": "So we would like to have the maximal maximal entropy ensemble something we don't know anything, but we cannot put a flat measure on the whole real line.",
                    "label": 0
                },
                {
                    "sent": "So somehow we need to we need to cut it.",
                    "label": 0
                },
                {
                    "sent": "We need to find a way to cut the distribution of the matrix entries and the only way to do it in a way that is compatible with this requirement is to choose.",
                    "label": 0
                },
                {
                    "sent": "Gaussian Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is a very artificial artificial thing in this nuclear physics.",
                    "label": 0
                },
                {
                    "sent": "An idea?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then you might wonder, is there anything else in the universe of random matrices with real entries?",
                    "label": 0
                },
                {
                    "sent": "Well, yes, you might be.",
                    "label": 0
                },
                {
                    "sent": "If you are outside these two groups.",
                    "label": 0
                },
                {
                    "sent": "Well, the situation becomes complicated, so you might as well want to change job or do go abroad or do whatever you want.",
                    "label": 0
                },
                {
                    "sent": "But if you have a model that lives here, you don't have many chances.",
                    "label": 0
                },
                {
                    "sent": "But still you can be extremely lucky an land into this very small group we have.",
                    "label": 0
                },
                {
                    "sent": "I don't know a handful of cases I tried to track down maybe 5 to 10.",
                    "label": 1
                },
                {
                    "sent": "Mumbles that I called.",
                    "label": 0
                },
                {
                    "sent": "Value models, so these are very strange ensembles, so for this ensembles you are able to write down the joint density of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you have the P of Lambda one Lambda N, so you have all the statistical control on the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, it is difficult to find the P of H, so the probability of the entries are is paradoxically more difficult or not interesting to obtain.",
                    "label": 0
                },
                {
                    "sent": "I give you an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose the example is truncations of matrices.",
                    "label": 0
                },
                {
                    "sent": "So suppose that you have a big matrix and you have some P of some POS.",
                    "label": 0
                },
                {
                    "sent": "Some probability density on the space of these matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, and you know that this matrix has a block block structure like RR Prime D prime.",
                    "label": 0
                },
                {
                    "sent": "Say, OK, let's take this.",
                    "label": 0
                },
                {
                    "sent": "This sub block and produce the matrix TT dagger.",
                    "label": 0
                },
                {
                    "sent": "So the matrix, the dagger is our mission, so it has real eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Let's call it T1T N. So you might want to know something about T1T N. Boeing POS.",
                    "label": 0
                },
                {
                    "sent": "But not P of TT dagger.",
                    "label": 0
                },
                {
                    "sent": "There is, you know, it's difficult to obtain is not not interesting.",
                    "label": 0
                },
                {
                    "sent": "What you have is the probability of the full matrix, but you want to find some something related to a submatrix of it, so there is.",
                    "label": 0
                },
                {
                    "sent": "There is a clear application of this in the field of like.",
                    "label": 0
                },
                {
                    "sent": "Celtic Transport electronic transport.",
                    "label": 0
                },
                {
                    "sent": "So you can have some cavities.",
                    "label": 0
                },
                {
                    "sent": "And some electrons that come in scatter and goes out.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can describe the system like this by a scattering matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a unitary.",
                    "label": 0
                },
                {
                    "sent": "User matrix that connects the wave function of the incoming electron to the wave function of the outgoing electron.",
                    "label": 0
                },
                {
                    "sent": "So it is unitary because it must conserve the probability the flux.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the scattering matrix for this problem is precisely this structure.",
                    "label": 0
                },
                {
                    "sent": "We have reflection blocks.",
                    "label": 0
                },
                {
                    "sent": "End transmission blocks.",
                    "label": 0
                },
                {
                    "sent": "Because the electron can come here, get scattered and go out from the same side.",
                    "label": 0
                },
                {
                    "sent": "Same thing on the left or can be transmitted.",
                    "label": 0
                },
                {
                    "sent": "So the scattering matrix has a block structure.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that the singular values of T of the transmission matrix are the one related to the physical properties of the system, not the eigenvalues of S in values.",
                    "label": 0
                },
                {
                    "sent": "S are not interesting because they are just pure phases.",
                    "label": 0
                },
                {
                    "sent": "The matrix is unitary, but the eigenvalues, the singular values of T are interesting.",
                    "label": 0
                },
                {
                    "sent": "So here we are in a case where the joint distribution of the eigenvalues is known, but the joint distribution of the entries is more complicated.",
                    "label": 0
                },
                {
                    "sent": "But OK, here there are just a handful of cases, and, well, you must be really lucky to land in here.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "The intersection between the two is also interesting.",
                    "label": 0
                },
                {
                    "sent": "These are models with independent entries, but for which you have the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "The joint distribution of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "There is 1 famous example which is this Dimitri Edelman model and will see if I have time to this very very important for numerical simulations.",
                    "label": 0
                },
                {
                    "sent": "So essentially the point is that the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Gaussian structure as the only nice property of being multiplicative in the probabilities, but additive in the independent because what you have here.",
                    "label": 0
                },
                {
                    "sent": "Basically, you must.",
                    "label": 0
                },
                {
                    "sent": "You must complete a trace of some sort in the exponent, and a trace is a sum of objects, so you need something that.",
                    "label": 0
                },
                {
                    "sent": "That is, that has a multiplicative structure.",
                    "label": 0
                },
                {
                    "sent": "But in this in the exponent reproduces a trace.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, if you think about it, there are not many possibilities because how to convert a product into some?",
                    "label": 0
                },
                {
                    "sent": "Well, the log, the logarithm and exponential do it.",
                    "label": 0
                },
                {
                    "sent": "Song.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now there is a survey moment.",
                    "label": 0
                },
                {
                    "sent": "So if I give you the choice between these two sets.",
                    "label": 1
                },
                {
                    "sent": "Which one would you prefer to build your scientific career on?",
                    "label": 1
                },
                {
                    "sent": "Which one guarantees the highest number of papers?",
                    "label": 0
                },
                {
                    "sent": "Which one guarantees the to have a very successful career?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "For which of the two sets?",
                    "label": 0
                },
                {
                    "sent": "We have a lot of tools, a lot of techniques, a lot of weapons.",
                    "label": 0
                },
                {
                    "sent": "The Independent one who says independent?",
                    "label": 0
                },
                {
                    "sent": "What no?",
                    "label": 0
                },
                {
                    "sent": "No razor.",
                    "label": 0
                },
                {
                    "sent": "Enter 3 who says rotational invariant.",
                    "label": 0
                },
                {
                    "sent": "Who says nothing?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This was a trap of course.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean.",
                    "label": 0
                },
                {
                    "sent": "Then the point is that it is a bit deceiving or paradoxical or a bit counter intuitive, because if we have just random variables.",
                    "label": 0
                },
                {
                    "sent": "Clearly the case of independent random variables is the simplest.",
                    "label": 0
                },
                {
                    "sent": "It is the case you know which is the case that you would start studying, and probably you can finish University without having encountered correlated random variables, right?",
                    "label": 0
                },
                {
                    "sent": "But of course you always see independent random variable, so we have a lot of weapons for independent random variables.",
                    "label": 0
                },
                {
                    "sent": "But somehow in the case of random matrices the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The thing is completely different.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We know nothing about these guys.",
                    "label": 0
                },
                {
                    "sent": "We know very little about matrices with independent entries.",
                    "label": 1
                },
                {
                    "sent": "It is amazing, amazing how little we know.",
                    "label": 1
                },
                {
                    "sent": "We don't have weapons we don't have analytical tools.",
                    "label": 0
                },
                {
                    "sent": "We have really nothing.",
                    "label": 1
                },
                {
                    "sent": "We we we.",
                    "label": 0
                },
                {
                    "sent": "We know a lot about matrices with rotational invariance.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of weapons.",
                    "label": 0
                },
                {
                    "sent": "But very little for independent entries.",
                    "label": 0
                },
                {
                    "sent": "Say spectral density density of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Models with independent entries with incredibly complicated calculation, we may be able to find out the spectral density of one of those models for N to Infinity in the end to Infinity limit.",
                    "label": 0
                },
                {
                    "sent": "For finite N, we know nothing.",
                    "label": 0
                },
                {
                    "sent": "In general we know nothing.",
                    "label": 0
                },
                {
                    "sent": "Here we have explicit formula for finite N for infinite N and it goes very smoothly.",
                    "label": 0
                },
                {
                    "sent": "Spacing between eigenvalues, nothing.",
                    "label": 0
                },
                {
                    "sent": "We have basically no results here.",
                    "label": 0
                },
                {
                    "sent": "We know nothing here.",
                    "label": 0
                },
                {
                    "sent": "We have a whole theory of.",
                    "label": 0
                },
                {
                    "sent": "Largest eigenvalues, wonderful theory here.",
                    "label": 0
                },
                {
                    "sent": "Here we are starting to get something like 2000 Eleven 2010 results like this.",
                    "label": 0
                },
                {
                    "sent": "So for 60 years we knew nothing about about that.",
                    "label": 0
                },
                {
                    "sent": "So the independent entries case is the by far the most complicated of the two.",
                    "label": 0
                },
                {
                    "sent": "Huawei.",
                    "label": 0
                },
                {
                    "sent": "Huawei.",
                    "label": 0
                },
                {
                    "sent": "In some sense, we should be able to answer this question right now alright?",
                    "label": 0
                },
                {
                    "sent": "Because independent entries.",
                    "label": 0
                },
                {
                    "sent": "What is the problem here?",
                    "label": 0
                },
                {
                    "sent": "The problem is that eigenvectors here are very important.",
                    "label": 0
                },
                {
                    "sent": "Because you have one matrix with independent entries in one basis.",
                    "label": 0
                },
                {
                    "sent": "If you make a transformation rotation, you mix all the entries together.",
                    "label": 0
                },
                {
                    "sent": "So what is independent in one basis is no longer independent in any other.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvectors are very important, so it means that eigenvalues and eigenvectors are not easy to the couple.",
                    "label": 0
                },
                {
                    "sent": "Here, the eigenvectors are not important, so we can.",
                    "label": 0
                },
                {
                    "sent": "Integrate them out and you know reduce the degrees of freedom from North Squared entries to N eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So here we have a lot of weapons.",
                    "label": 0
                },
                {
                    "sent": "To use.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, the idea.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Basically a summary well.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "This transformation between.",
                    "label": 0
                },
                {
                    "sent": "Entries and eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Is not always possible, so it is possible here for rotational invariant models.",
                    "label": 0
                },
                {
                    "sent": "It is possible here for the eigenvalue models because by definition here we have gang and values from scratch.",
                    "label": 0
                },
                {
                    "sent": "It is in general not possible for matrices with independent entries and for theorie it is not possible here, here, here.",
                    "label": 0
                },
                {
                    "sent": "So if you if you land here, matrices that have correlated entries, no invariants and no eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "There's no chance.",
                    "label": 0
                },
                {
                    "sent": "OK. Now I give you a simplified summary.",
                    "label": 0
                },
                {
                    "sent": "So the idea of a simplified.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marie came to me.",
                    "label": 0
                },
                {
                    "sent": "From a picture that was sent to my Facebook like a simplified map of London, you have losers here and very rich here.",
                    "label": 0
                },
                {
                    "sent": "When I was in London I was living here.",
                    "label": 0
                },
                {
                    "sent": "Too bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the takehome message.",
                    "label": 0
                },
                {
                    "sent": "For this first part.",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "You can forget all I said man, you probably did it already, but just just keep this in mind.",
                    "label": 0
                },
                {
                    "sent": "This is very important.",
                    "label": 0
                },
                {
                    "sent": "Yeah, matrices with independent entrance.",
                    "label": 0
                },
                {
                    "sent": "Very bad horrible.",
                    "label": 0
                },
                {
                    "sent": "Rotationally invariant models very good ikenberry models very good, but very few.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, generalities.",
                    "label": 0
                },
                {
                    "sent": "So we'll talk a bit about what can we expect in general.",
                    "label": 0
                },
                {
                    "sent": "For the spectrum of a random matrix, OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the first thing to notice is that random eigen values do not behave at all like random points, say on a segment.",
                    "label": 1
                },
                {
                    "sent": "So if you think if you take a segment and you throw at random with uniform distribution some points.",
                    "label": 0
                },
                {
                    "sent": "This is the pattern that you will get.",
                    "label": 0
                },
                {
                    "sent": "If you take the eigenvalues of a Gaussian matrix, this is the pattern you will get.",
                    "label": 0
                },
                {
                    "sent": "So you will see that here in the case of random points uniformly distributed, you have a lot of kissing.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of very close.",
                    "label": 0
                },
                {
                    "sent": "Encounters very close meetings, so random points are online.",
                    "label": 0
                },
                {
                    "sent": "They tend to cluster.",
                    "label": 0
                },
                {
                    "sent": "They like to stay very close to each other.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues of a random matrix they don't like to stay very close to each other.",
                    "label": 0
                },
                {
                    "sent": "They repel each other.",
                    "label": 0
                },
                {
                    "sent": "You have a very large.",
                    "label": 0
                },
                {
                    "sent": "Much larger spacing between adjacent points.",
                    "label": 0
                },
                {
                    "sent": "This is a very general feature of random matrix model.",
                    "label": 0
                },
                {
                    "sent": "In the case of invariant model, we are able to spot analytically why this is the case.",
                    "label": 0
                },
                {
                    "sent": "And we will see it while they repair.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another concept that is recurrent in papers or books on random matrix.",
                    "label": 0
                },
                {
                    "sent": "Theory is the concept of universality.",
                    "label": 0
                },
                {
                    "sent": "So universality means that you can put different phenomena under the same or within the same framework.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we have.",
                    "label": 0
                },
                {
                    "sent": "Energy levels of some resonances of a heavy nucleus.",
                    "label": 0
                },
                {
                    "sent": "I don't remember which one from maybe erbium or something.",
                    "label": 0
                },
                {
                    "sent": "Which is superimposed.",
                    "label": 0
                },
                {
                    "sent": "The theoretical curve is the distribution of spacings between eigenvalues of a goed matrix.",
                    "label": 0
                },
                {
                    "sent": "So you see the repulsion is the probability of finding a spacing S. So if the spacing is very low.",
                    "label": 0
                },
                {
                    "sent": "The probability is very low.",
                    "label": 0
                },
                {
                    "sent": "So you don't find small spacings.",
                    "label": 0
                },
                {
                    "sent": "Level repulsion if you add random points on a line you would get a person distribution.",
                    "label": 0
                },
                {
                    "sent": "So basically an exponential distribution of spacings, so small spacings are very very likely.",
                    "label": 0
                },
                {
                    "sent": "So the behavior of a small spacings is markedly different into 2 cases.",
                    "label": 0
                },
                {
                    "sent": "Now here we have the same theoretical curve and two empirical plots.",
                    "label": 0
                },
                {
                    "sent": "I think the.",
                    "label": 0
                },
                {
                    "sent": "The squares are.",
                    "label": 0
                },
                {
                    "sent": "Distances between parked cars.",
                    "label": 0
                },
                {
                    "sent": "On a street in London.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting because, well, OK, I know the guy who did this research, so he was sent out in London to measure the spacings between cars that were parked on on street, so clearly.",
                    "label": 0
                },
                {
                    "sent": "The cars don't like to stay too close to each other, right?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, they don't like to stay too far because it doesn't make sense, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the typical profile you get.",
                    "label": 0
                },
                {
                    "sent": "Of course, the legend tells that the guy was questioned by the police and almost arrested because.",
                    "label": 0
                },
                {
                    "sent": "But the other the crosses are the distances between birds.",
                    "label": 0
                },
                {
                    "sent": "That are sitting on a wire on another Christy.",
                    "label": 0
                },
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "The birds don't like to.",
                    "label": 0
                },
                {
                    "sent": "To stay too close to each other.",
                    "label": 0
                },
                {
                    "sent": "But they also don't like to stay there, and too far from each other.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that you have some universal universal feature that can be fitted by the same, roughly the same curve.",
                    "label": 0
                },
                {
                    "sent": "Here we have another.",
                    "label": 0
                },
                {
                    "sent": "These are zeros of Riemann's Zeta function, but I will talk extensively about this later.",
                    "label": 0
                },
                {
                    "sent": "So all these phenomena can be fitted by a curve like this.",
                    "label": 0
                },
                {
                    "sent": "Each of the minus square at large spacings, and as to the beta for small spacing where this beta in the case of random matrices E is the Dyson index beta.",
                    "label": 0
                },
                {
                    "sent": "1, two or four that we.",
                    "label": 0
                },
                {
                    "sent": "We saw the beginning, so this is called the Wigner Dyson law.",
                    "label": 0
                },
                {
                    "sent": "I told you that everything is named after this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh great, so if you are interested, well, this is clearly the least interesting slide, but it was perfect.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are some papers about the distribution of Parking's and gap size distribution of parked cars.",
                    "label": 0
                },
                {
                    "sent": "I don't know if someone is interested in.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "There's a classical picture of possible situations that arise when calculating spacings.",
                    "label": 0
                },
                {
                    "sent": "So all these six cases have the same average, the same mean level spacing.",
                    "label": 0
                },
                {
                    "sent": "So the mean is the same.",
                    "label": 0
                },
                {
                    "sent": "But of course you can see very different patterns here.",
                    "label": 0
                },
                {
                    "sent": "So here we have the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Off of spacings.",
                    "label": 0
                },
                {
                    "sent": "Here we have the distribution of zeros of the Riemann Zeta function, and we talk extensively about this later.",
                    "label": 0
                },
                {
                    "sent": "So you see level repulsion here.",
                    "label": 0
                },
                {
                    "sent": "This is our the energy levels of Essanay billiard.",
                    "label": 0
                },
                {
                    "sent": "This is the resonant scattering energies of nucleus of erbium.",
                    "label": 0
                },
                {
                    "sent": "This is the distribution of primes.",
                    "label": 0
                },
                {
                    "sent": "And this is the person distribution.",
                    "label": 0
                },
                {
                    "sent": "So you see both in the case of person distribution, we have a lot of key things.",
                    "label": 0
                },
                {
                    "sent": "A lot of a lot of very close encounters between the levels.",
                    "label": 0
                },
                {
                    "sent": "And the same happens with the distribution of primes.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because we have a lot of twin primes.",
                    "label": 0
                },
                {
                    "sent": "Twin primes are the primes that are that differ by two.",
                    "label": 0
                },
                {
                    "sent": "OK, like 11 and 13.",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we have a lot of.",
                    "label": 0
                },
                {
                    "sent": "Actually about the distribution of primes, well, there's just a digression to relax.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit so you know that there is a twin prime.",
                    "label": 0
                },
                {
                    "sent": "Twin prime conjecture.",
                    "label": 0
                },
                {
                    "sent": "The Twin Prime Conjecture is a very well known unsolved problem in number theory that says that there are an infinite number of pairs of twin primes.",
                    "label": 0
                },
                {
                    "sent": "So if you go all the way to Infinity you will will keep finding twin primes.",
                    "label": 0
                },
                {
                    "sent": "That means primes that differ by two.",
                    "label": 0
                },
                {
                    "sent": "So now apparently this year there was a very nice breakthrough in Prime theory.",
                    "label": 0
                },
                {
                    "sent": "There was an unknown mathematician like Mr. Nobody who actually was able not to prove the Twin Prime conjecture, but version that.",
                    "label": 0
                },
                {
                    "sent": "Shows that there is some number N smaller than 70 million.",
                    "label": 0
                },
                {
                    "sent": "Such that there are infinitely many primes of many pairs of primes that differ by N. So he bounded the gap between consecutive primes.",
                    "label": 0
                },
                {
                    "sent": "So now the work of mathematician will be to put this number down from 70 million to two.",
                    "label": 0
                },
                {
                    "sent": "Sorry, yeah, now it's about 5000 something, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah I think so.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is what they but I just found it quite interesting.",
                    "label": 0
                },
                {
                    "sent": "The other thing I checked, I checked this news on on different articles so most of the journalists didn't mention what he did, but all of them mention was young.",
                    "label": 0
                },
                {
                    "sent": "He was working at a subway sandwich shop before getting a job as a lecture.",
                    "label": 0
                },
                {
                    "sent": "So apparently for the vast large number of journalists, the relative importance of these two bits of information.",
                    "label": 0
                },
                {
                    "sent": "Is history.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}