{
    "id": "7mtbufz5k6l2rbywri4mhtvazmnubkhw",
    "title": "Online Learning with Predictable Sequences",
    "info": {
        "author": [
            "Alexander Rakhlin, Statistics Department, Wharton School, University of Pennsylvania"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_rakhlin_learning/",
    "segmentation": [
        [
            "So the purpose of this talk is to satisfy Manfred and there will be no bounds that depend on T. Now we're in the."
        ],
        [
            "Setting of a standard setting of online learning learning pics.",
            "FT from F. Nature chooses XT from X and we have external regret as the performance measure.",
            "So the question is whether we can add prior knowledge about the sequences.",
            "So we're I guess the goal is to move away from completely adversarial situations for online learning and introduce some prior knowledge about the sequence.",
            "Is one way to."
        ],
        [
            "Do this is to say that there is some evolution of the sequence which is captured by this function M which is known to us for now and then there is a adversarial noise Delta on top of that function.",
            "So there is some evolution and we call it predictable process and such a restriction on the way that the sequence involves or constraints on the adversary.",
            "If you wish were analyzed in this paper with Kartik and Ambush.",
            "Where we showed that the learner will indeed pay only for the adversarial part of this process, so only for the magnitude of deltas.",
            "However, for the minimax analysis it's nonconstructive, and these bounds on Delta on the magnitude of the adversarial noise needs to be known to the learner.",
            "So for us the."
        ],
        [
            "Prize in this work was that this knowledge is not required.",
            "In other words, there exist algorithms that, at least for online linear optimization that attain abound, Wichita automatically tunes to the deviations from the trend.",
            "So if the trend is indeed closely following the predictable process empty, then we get something small, and if it doesn't follow, then we still get squared of TI bounds, and it not known that.",
            "The process actually follows.",
            "The sequence follow this process OK, so."
        ],
        [
            "Examples have been some of these have been studied in the literature.",
            "If we think that the previous move of the adversary is a good indication for the next move, and then we might use the predictable process of XT minus one just the previous move, variance bounds have been shown in the literature, but you can imagine other things like phases.",
            "So for instance, if you know that there is a phase in your sequence, such as in in financial markets.",
            "Then you should be subtracting off this phase as is done in statistics.",
            "You shouldn't be paying for this phase.",
            "And more generally, this function can come from external sources, so you might have more information later down the road about something that happened before, and so forth.",
            "No."
        ],
        [
            "So there are two algorithms that are very simple.",
            "One is optimistic, followed the regularize dealer.",
            "This is just optimism followed the regulars, either with self important barrier and the small modification where we just add the function that we can compute the M We just added into the optimization problem and the second one is basically a very straightforward modification of the paper that was a result that was presented last year by Chung Yong Lee, Madhavi, Lou, Jean, Andrew for the case so that.",
            "Under this case, for M T = 2 X T -- 1 The previous guy."
        ],
        [
            "The next step is to ask, can we learn the process?",
            "So if we have a bunch of models and treated treat this as a model selection problem, we have a bunch of possible models for each of them we know what the predictable process should be.",
            "Can we do as well as the best one?",
            "And of course you can think of this as a two layer with experts on top and one can gain get such a result.",
            "One can also get such a result in the partial."
        ],
        [
            "Formation scenario where you only observe the dot product or the out the how much the cost is on that round and only the prediction of 1 process out of a pool of processes.",
            "And so this is 2 bandit problems actually working at the same time at 2 levels, and it crucially relies on the multi Arm bandit algorithm with regret scaling in terms of the loss of the best arm which we didn't see in the literature.",
            "Maybe someone knows how to do it, but.",
            "This might be offer independent interest, so a multi arm bandit where the loss depends only scales not with T, but the scales with the loss of the best arm.",
            "That's it further."
        ],
        [
            "We would like to consider predictable sequences beyond online convex optimization.",
            "The minimax analysis still holds there, so there must be algorithms out there that we can find should be faster raise for problems where regret is used as a black box.",
            "But now we have a bit more structure.",
            "We have applied this to structured optimization and mirror and elucidated the connection to mirror Prox switches.",
            "Quite interesting, it would be nice to couple this idea exactly with Karthick was talking with regret again strategies and that's a work in progress and that's it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the purpose of this talk is to satisfy Manfred and there will be no bounds that depend on T. Now we're in the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting of a standard setting of online learning learning pics.",
                    "label": 0
                },
                {
                    "sent": "FT from F. Nature chooses XT from X and we have external regret as the performance measure.",
                    "label": 1
                },
                {
                    "sent": "So the question is whether we can add prior knowledge about the sequences.",
                    "label": 0
                },
                {
                    "sent": "So we're I guess the goal is to move away from completely adversarial situations for online learning and introduce some prior knowledge about the sequence.",
                    "label": 0
                },
                {
                    "sent": "Is one way to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do this is to say that there is some evolution of the sequence which is captured by this function M which is known to us for now and then there is a adversarial noise Delta on top of that function.",
                    "label": 0
                },
                {
                    "sent": "So there is some evolution and we call it predictable process and such a restriction on the way that the sequence involves or constraints on the adversary.",
                    "label": 0
                },
                {
                    "sent": "If you wish were analyzed in this paper with Kartik and Ambush.",
                    "label": 0
                },
                {
                    "sent": "Where we showed that the learner will indeed pay only for the adversarial part of this process, so only for the magnitude of deltas.",
                    "label": 0
                },
                {
                    "sent": "However, for the minimax analysis it's nonconstructive, and these bounds on Delta on the magnitude of the adversarial noise needs to be known to the learner.",
                    "label": 1
                },
                {
                    "sent": "So for us the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prize in this work was that this knowledge is not required.",
                    "label": 1
                },
                {
                    "sent": "In other words, there exist algorithms that, at least for online linear optimization that attain abound, Wichita automatically tunes to the deviations from the trend.",
                    "label": 0
                },
                {
                    "sent": "So if the trend is indeed closely following the predictable process empty, then we get something small, and if it doesn't follow, then we still get squared of TI bounds, and it not known that.",
                    "label": 0
                },
                {
                    "sent": "The process actually follows.",
                    "label": 0
                },
                {
                    "sent": "The sequence follow this process OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples have been some of these have been studied in the literature.",
                    "label": 0
                },
                {
                    "sent": "If we think that the previous move of the adversary is a good indication for the next move, and then we might use the predictable process of XT minus one just the previous move, variance bounds have been shown in the literature, but you can imagine other things like phases.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you know that there is a phase in your sequence, such as in in financial markets.",
                    "label": 0
                },
                {
                    "sent": "Then you should be subtracting off this phase as is done in statistics.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't be paying for this phase.",
                    "label": 0
                },
                {
                    "sent": "And more generally, this function can come from external sources, so you might have more information later down the road about something that happened before, and so forth.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are two algorithms that are very simple.",
                    "label": 0
                },
                {
                    "sent": "One is optimistic, followed the regularize dealer.",
                    "label": 0
                },
                {
                    "sent": "This is just optimism followed the regulars, either with self important barrier and the small modification where we just add the function that we can compute the M We just added into the optimization problem and the second one is basically a very straightforward modification of the paper that was a result that was presented last year by Chung Yong Lee, Madhavi, Lou, Jean, Andrew for the case so that.",
                    "label": 0
                },
                {
                    "sent": "Under this case, for M T = 2 X T -- 1 The previous guy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next step is to ask, can we learn the process?",
                    "label": 1
                },
                {
                    "sent": "So if we have a bunch of models and treated treat this as a model selection problem, we have a bunch of possible models for each of them we know what the predictable process should be.",
                    "label": 0
                },
                {
                    "sent": "Can we do as well as the best one?",
                    "label": 0
                },
                {
                    "sent": "And of course you can think of this as a two layer with experts on top and one can gain get such a result.",
                    "label": 0
                },
                {
                    "sent": "One can also get such a result in the partial.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formation scenario where you only observe the dot product or the out the how much the cost is on that round and only the prediction of 1 process out of a pool of processes.",
                    "label": 0
                },
                {
                    "sent": "And so this is 2 bandit problems actually working at the same time at 2 levels, and it crucially relies on the multi Arm bandit algorithm with regret scaling in terms of the loss of the best arm which we didn't see in the literature.",
                    "label": 1
                },
                {
                    "sent": "Maybe someone knows how to do it, but.",
                    "label": 0
                },
                {
                    "sent": "This might be offer independent interest, so a multi arm bandit where the loss depends only scales not with T, but the scales with the loss of the best arm.",
                    "label": 0
                },
                {
                    "sent": "That's it further.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We would like to consider predictable sequences beyond online convex optimization.",
                    "label": 1
                },
                {
                    "sent": "The minimax analysis still holds there, so there must be algorithms out there that we can find should be faster raise for problems where regret is used as a black box.",
                    "label": 0
                },
                {
                    "sent": "But now we have a bit more structure.",
                    "label": 1
                },
                {
                    "sent": "We have applied this to structured optimization and mirror and elucidated the connection to mirror Prox switches.",
                    "label": 0
                },
                {
                    "sent": "Quite interesting, it would be nice to couple this idea exactly with Karthick was talking with regret again strategies and that's a work in progress and that's it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}