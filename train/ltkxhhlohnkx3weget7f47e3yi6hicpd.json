{
    "id": "ltkxhhlohnkx3weget7f47e3yi6hicpd",
    "title": "First-order regret bounds for combinatorial semi-bandits",
    "info": {
        "author": [
            "Gergely Neu, SequeL, INRIA Lille - Nord Europe"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2015_neu_online_learning/",
    "segmentation": [
        [
            "So this is about first order regret bounds for combinatorial semi bandit, so this is a this is a generalization of the multi armed bandit problem that many of you might know."
        ],
        [
            "So the most iconic instance of this combinatorial semi bandit problem is the online routing problem, where in every round you decide on a route for a packet for example."
        ],
        [
            "Side on this one and then your package will travel through this path, suffer some losses along the edges and and then you observe these losses along the edges.",
            "So more generally, you pick an action which is represented by binary vector and then the environment or the adversary chooses a loss vector and then you suffer a loss which is just the inner product of the two and then your observations are."
        ],
        [
            "Along the individual edges that you've chosen so, but this applies to do many more problems as well.",
            "So we already know."
        ],
        [
            "Many things about this problem when we look at the regret.",
            "So the regret is defined as usual.",
            "So this is just a gap between the performance of the learner.",
            "And the total loss accumulated by the best fixed decision fixed in hindsight.",
            "So we already know what the minimax regret is that it scales with the square root of the number of rounds, and we have an efficient algorithm that achieves this bound.",
            "But the question is whether."
        ],
        [
            "We can do anything better than this, so in particular month Treadwell is better."
        ],
        [
            "Can improve the dependency and rooty to the roots of the total.",
            "Also the best expert so.",
            "So these sort of improvement is known as a first order bounds by some.",
            "So here Lt start is the loss of the best experts, so this bound may be much better than a minimalist band, which is a fruity in case there is a very good expert.",
            "So we already know many examples in the full information setting where this is achievable.",
            "Pretty much you can modify any algorithm to achieve such a guarantee with a clever tuning of the learning rate, but very little is known.",
            "About about these bonds.",
            "For partial information, any particular bandit.",
            "So so I've listed some results that I've found, but these are very obscured, so the obscurity of this was clearly reflected by this 2013 paper by session Karthik who proved this bound because they were not aware of the others."
        ],
        [
            "So.",
            "And the other problem with these mounts and these are things that they don't really generalize sufficiently to combinatorial settings, so these ones inside.",
            "So these were about the multi unbanded setting, so it is not really known how to go for larger scale.",
            "And there is no no good understanding about what makes a good first order bound in the bandit setting.",
            "So and in."
        ],
        [
            "This work well.",
            "I constructed an algorithm, combined combining follow the perturb leader with some tricks.",
            "So trick number one is using truncated exponential perturbation.",
            "So somehow ensure that the learner is not picking a suboptimal actions too much.",
            "So this is essentially another thing, is is something that I call implicit expiration, which is a clever biasing of of the loss estimates that ensures sort of your loss estimates are staying close to each other.",
            "And this these techniques together make it possible to prove such fun.",
            "So if you're familiar with the analysis of Exp.",
            "Three and nine, FPL and mirror dissent and whatnot, then you might remember that these sort of terms are very important in the analysis of these algorithms.",
            "And usually you can only prove a linear bound and that linear in the number of rounds.",
            "But thank you so, but these techniques, together low abounding technique that enables replacing this defector by the loss of the best expert.",
            "So and then the main result is a first order bound for combinatorial bandits that replaces the two.",
            "If that, and these tricks together also offer some new insights about how to achieve 1st order bonds in general.",
            "So thank you and see it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is about first order regret bounds for combinatorial semi bandit, so this is a this is a generalization of the multi armed bandit problem that many of you might know.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the most iconic instance of this combinatorial semi bandit problem is the online routing problem, where in every round you decide on a route for a packet for example.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Side on this one and then your package will travel through this path, suffer some losses along the edges and and then you observe these losses along the edges.",
                    "label": 0
                },
                {
                    "sent": "So more generally, you pick an action which is represented by binary vector and then the environment or the adversary chooses a loss vector and then you suffer a loss which is just the inner product of the two and then your observations are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along the individual edges that you've chosen so, but this applies to do many more problems as well.",
                    "label": 0
                },
                {
                    "sent": "So we already know.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many things about this problem when we look at the regret.",
                    "label": 0
                },
                {
                    "sent": "So the regret is defined as usual.",
                    "label": 0
                },
                {
                    "sent": "So this is just a gap between the performance of the learner.",
                    "label": 0
                },
                {
                    "sent": "And the total loss accumulated by the best fixed decision fixed in hindsight.",
                    "label": 0
                },
                {
                    "sent": "So we already know what the minimax regret is that it scales with the square root of the number of rounds, and we have an efficient algorithm that achieves this bound.",
                    "label": 1
                },
                {
                    "sent": "But the question is whether.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can do anything better than this, so in particular month Treadwell is better.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can improve the dependency and rooty to the roots of the total.",
                    "label": 0
                },
                {
                    "sent": "Also the best expert so.",
                    "label": 0
                },
                {
                    "sent": "So these sort of improvement is known as a first order bounds by some.",
                    "label": 0
                },
                {
                    "sent": "So here Lt start is the loss of the best experts, so this bound may be much better than a minimalist band, which is a fruity in case there is a very good expert.",
                    "label": 0
                },
                {
                    "sent": "So we already know many examples in the full information setting where this is achievable.",
                    "label": 1
                },
                {
                    "sent": "Pretty much you can modify any algorithm to achieve such a guarantee with a clever tuning of the learning rate, but very little is known.",
                    "label": 0
                },
                {
                    "sent": "About about these bonds.",
                    "label": 0
                },
                {
                    "sent": "For partial information, any particular bandit.",
                    "label": 0
                },
                {
                    "sent": "So so I've listed some results that I've found, but these are very obscured, so the obscurity of this was clearly reflected by this 2013 paper by session Karthik who proved this bound because they were not aware of the others.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the other problem with these mounts and these are things that they don't really generalize sufficiently to combinatorial settings, so these ones inside.",
                    "label": 1
                },
                {
                    "sent": "So these were about the multi unbanded setting, so it is not really known how to go for larger scale.",
                    "label": 0
                },
                {
                    "sent": "And there is no no good understanding about what makes a good first order bound in the bandit setting.",
                    "label": 0
                },
                {
                    "sent": "So and in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work well.",
                    "label": 0
                },
                {
                    "sent": "I constructed an algorithm, combined combining follow the perturb leader with some tricks.",
                    "label": 1
                },
                {
                    "sent": "So trick number one is using truncated exponential perturbation.",
                    "label": 1
                },
                {
                    "sent": "So somehow ensure that the learner is not picking a suboptimal actions too much.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially another thing, is is something that I call implicit expiration, which is a clever biasing of of the loss estimates that ensures sort of your loss estimates are staying close to each other.",
                    "label": 0
                },
                {
                    "sent": "And this these techniques together make it possible to prove such fun.",
                    "label": 1
                },
                {
                    "sent": "So if you're familiar with the analysis of Exp.",
                    "label": 0
                },
                {
                    "sent": "Three and nine, FPL and mirror dissent and whatnot, then you might remember that these sort of terms are very important in the analysis of these algorithms.",
                    "label": 1
                },
                {
                    "sent": "And usually you can only prove a linear bound and that linear in the number of rounds.",
                    "label": 0
                },
                {
                    "sent": "But thank you so, but these techniques, together low abounding technique that enables replacing this defector by the loss of the best expert.",
                    "label": 0
                },
                {
                    "sent": "So and then the main result is a first order bound for combinatorial bandits that replaces the two.",
                    "label": 0
                },
                {
                    "sent": "If that, and these tricks together also offer some new insights about how to achieve 1st order bonds in general.",
                    "label": 0
                },
                {
                    "sent": "So thank you and see it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}