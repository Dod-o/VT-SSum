{
    "id": "j26wwvdlvgmhgdkhx5efqgcpp54binha",
    "title": "Semi-Supervised Learning of Semantic Spatial Concepts for a Mobile Robot",
    "info": {
        "author": [
            "Nicol\u00f2 Cesa-Bianchi, University of Milan"
        ],
        "published": "April 25, 2012",
        "recorded": "March 2012",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/workshops2012_cesa_bianchi_spatial_concepts/",
    "segmentation": [
        [
            "So this is a report about the pump priming, supervised learning, or semantic spatial concepts for a mobile robot.",
            "And I will be reporting about some work I did with Francesca Robona, the Happy Smiley guy who's now in Chicago TTI.",
            "And that's the reason why it's not here doing his job duties and."
        ],
        [
            "OK so I will give you a short program project description and then we'll touch basically 2 themes that this is the stuff we did."
        ],
        [
            "Anne Sophie."
        ],
        [
            "Of all the project details, so this projects I had two participants, Milano Ann Martini.",
            "And the duration was one year and a half and starting date was beginning of 2010.",
            "And we did have a kickoff meeting Imagineered in February.",
            "However, the essentially so far the project went on for about one year and then last year nothing happened 'cause the Matheney site had quite serious hiring problems.",
            "And we think we have solved this actually in this couple of yesterday, and so we think the project could be resumed very soon.",
            "And essentially we the second part of it can be carried out.",
            "So will be.",
            "So this is not not to talk about.",
            "Terminated from priming, but will be reporting some preliminary work that we did with Francesco and hopefully the second part will be done at route to this year, OK?"
        ],
        [
            "So the problem we were looking at is the problem of place recognition in robot navigation and this is kind of a simple intuitive problem you have, for instance the building and the mobile robot roaming in the building and the Roman wants to know wants to categorize the rooms or the environment where it is at any given point of time.",
            "So he wants to know that he's in a corridor or that is in the kitchen or it isn't in office in the.",
            "Maybe some persons office so and so forth and this is of course an SN accessory module in the building of an autonomous robot.",
            "Even I can see that that I'm doing almost anything about Roberts, and it's also kind of clear that online learning can play an important role in this kind of domain, just cause these environments are not.",
            "Stationary in terms of different conditions.",
            "So, for instance that the if you do the training in winter to train the robot recognize an environment, then in summer the lighting will be different than many things will be different and but also throughout the day.",
            "For instance, said things in a room can change because people move things.",
            "Chairs have been moved, tables are being moved, so on so forth.",
            "So it is important that the learning algorithm that is a supervisor that is underlying the robot navigation is able to do continuous learning so.",
            "This is a good case for online learning and we were all happy about this.",
            "Viewing this and it's also kind of clear that you cannot just take online learning off the shelf and apply to the problem, because there Robert all in online learning you're supposed to get the label after each prediction, which is typically very unconvenient if you want to have a robot goes autonomously, it's better.",
            "The form of what we call Semisupervised active learning or selective sampling, which is the.",
            "The declination of online learning where the.",
            "Where the agent, in this case, the robot, asks only occasionally supervised the help.",
            "So in this case the robot will go about the building.",
            "At some point the confidence level in its classification will drop below a certain threshold and he will stop to ask for human supervision.",
            "Where where am I OK?",
            "So this was kind of a very clear and.",
            "And and intuitive basis on which we built a bit of."
        ],
        [
            "Theory, and So what we did in this first part of the project.",
            "So in the year 2010 with Francesco was too.",
            "Essentially, craft all the modules that we needed to build a system that was able to perform this online.",
            "Robot navigation in a super supervised in a semi supervised setting.",
            "So first of all we need to have online algorithms that are able to do.",
            "Multi view learning because typically robots have different sensors and they also you typically the camera.",
            "The camera provides different sets of features and so it is important to be able to integrate these different source of features in such a way to exploit specificities of the data.",
            "For instance sparsity which is typical.",
            "So this is worth one problem.",
            "The other problem was to.",
            "Try to get a clearer understanding of the algorithmic landscape in online active learning or selective sampling, so to sharpen the results and to clean up the analysis and the algorithms in order to have a good set of tools to use.",
            "And then.",
            "And in doing that, we immediately ran into the problem of having these two things talk to each other and the problem is this.",
            "In order to do multi multi kernel learning, for instance to enhance the sparsity, you typically would like to apply regularizers to the problem that are non ingredient like L1 in some in some linear space.",
            "OK and on the other hand especially the most powerful algorithms.",
            "Analysis For Semi supervised learning for selective sampling do rely on Euclidean norms.",
            "So we wanted first also to come up with a way of reconciling these two aspects to get an algorithm to get a selective sampling algorithm that was able to deal with non Euclidean norms but also enjoy the strong performance guarantees that.",
            "He then norm based algorithms have OK and then the we would like what's left to do here is to do an integration and TuneIn tune up of these modules.",
            "You will see that not all results are like totally satisfactory yet.",
            "And testing them on Roberts platform and maybe do a bit of iterations in these two last two steps.",
            "So we need we need really to see how it works on anything.",
            "On the field.",
            "OK."
        ],
        [
            "So publications, so we had two publication publications out of it in 2010 and then one in 2011 Eleven and this publication here was the one which triggered the whole research because we had a very simple and powerful algorithm for doing semi supervised online learning.",
            "And so this paper is about the multikernel extension, and this paper is about the refinement of selective sampling and the non Euclidean norm's business."
        ],
        [
            "Came."
        ],
        [
            "So we start with the online multi kernel learning.",
            "So we we want to do all integrate online integration of data coming from robot sensors and this.",
            "This is not.",
            "This is not.",
            "Basically it was not really hard given the existing technology becauses essentially the online analysis of an online algorithms is general enough that once you come up with the right regularizer you can almost to plug it in into the current analysis and get meaningful bound at a good algorithm.",
            "Here there were, there was a set of additional difficulties because we wanted to get a multiclass extension of it.",
            "And we did want to use the specific norms that were in there, haven't been used before so we didn't know anything about the practical performance of that.",
            "So in OK, so we look at the kernel based multiview learning so we have a fixed set of reproducing kernel Hilbert spaces and we want to simultaneously.",
            "On lunch, online, train or training, the online Model 1 model in each reproducing kernel Hilbert space.",
            "OK, so we get this stream of data.",
            "The data that come from the sensor of the robot and we want to get this data, apply different kernels to each feature set and then to integrate them.",
            "And then we we do online prediction with this set of models that are being trained using a combination where we.",
            "In both the model within the reproducing kernel Hilbert space and the coefficients of linear combination.",
            "So the idea here is that OK, if you believe that for instance there are a few good kernels in your pool, you would like to.",
            "I'd like to.",
            "Have a few big coefficients here that focus on the good kernels and leave the others kind of law and OK at the baseline is the natural baselines here.",
            "The single best training model in a in a best, best kernel essay?",
            "So the model in the training model in the best position, kernel Hilbert space and the simple argument does a average margin prediction with the flat average.",
            "So without playing with these coefficients, so here again both coefficients and the models are trained online."
        ],
        [
            "So the way you do this is by using like it.",
            "This is Francesco the definition and the mother of all all online algorithms.",
            "This is the.",
            "Yes."
        ],
        [
            "In understanding that the FIS will be learned in some sense independently of the learning of the authorizer, there will be interaction.",
            "There will be interaction.",
            "There will be interaction.",
            "I'm not going to explain that the way the interaction goes."
        ],
        [
            "Because they don't have time to do, but there is interaction and the regularizer is providing this interaction.",
            "OK.",
            "So basically this is the bread and butter of any online learner and so it's it's online mirror dissent.",
            "This is the kind of generalized gradient descent, so you have a data stream coming in and you want to.",
            "So we are here.",
            "We are in the in the standard online learning settings, so you get one day at a time.",
            "You predict the label of the data and then you get the true label.",
            "You suffer some loss and then you update your model.",
            "OK, so all enemy they sent you have actually two models, the primal model, which I call G here because we are possibly in some weird space and in some General general general linear space and dual model, which I call F. So the dual model F you use it to predict and the primal Model G you use it to.",
            "Perform the update.",
            "This is the kind of schizoid algorithm and that has these two parameters, and so the regularization function plays the role of talking like.",
            "Does the communication between the primal and dual model, indeed the gradient of the regularization function is called the link function because it acts as a link between primal and dual.",
            "And so the trick is, is that you have regularization function but and the bound is going to depend on the performance bond is going to depend on this regularization function.",
            "However, the update is going to be defined in terms of the gradient of the dual of the regularization function.",
            "What is that OK, if R is the squared norm, which is most often the case, then you have the the.",
            "Our star is the square the dual norm.",
            "OK, for instance, if you have the P norm for vectors, are stars going to be Q norm squared OK and you see here that the update of the primal model is done doing a gradient step with the gradient of the loss.",
            "Oh, this is a mistake.",
            "This should be an F here.",
            "There's an F here, I'm sorry.",
            "Right, right?",
            "OK, so doing the.",
            "With the gradient of the loss of the dual model, so again there's going to be an F here.",
            "OK, sorry.",
            "Where it is, there is no.",
            "There's a OK good sorry, sorry I I changed notation which I shouldn't have done, and so this is a G and this is an F. So you can use any strongly convex function as a regularizer, so it's a pretty liberal setup and the performance bond will be good when the choice of the Rising match matches the data sequence that you're going to predict on.",
            "This is kind of expectable, and F&G can belong to many pair of dollars.",
            "Do a linear spaces so you can run this into an on a metric space, for instance, or a vector space, or any other linear space.",
            "OK.",
            "Right, and if you run it into my automatic space as we do for doing multi kernel learning, then at R will be a typical E squared matrix norm.",
            "OK."
        ],
        [
            "So to do online multi kernel learning is actually kind of simple.",
            "It's a very simple thing to do and you take a matrix.",
            "For instance if you want to like we wanted to see whether we will be able to pick sparsity in the data.",
            "So what we did is to use what is called the 2P matrix group norm which is simple.",
            "You have your metrics of models.",
            "OK so these are these are vectors of coefficients of linear coefficients.",
            "And you take the square the P norm of the vector whose.",
            "Elements whose components are the Euclidean norms of the models.",
            "If you run this into kernel space, these are not vectors.",
            "These are functions and, but you can take the Euclidean norm of that function in the reproducing kernel Hilbert space, so that's well defined it, and by playing with the P coefficient you can.",
            "You can enhance this sparsity in the vector of norms for the models in the different kernel spaces.",
            "So essentially you can focus on a subset of the kernels that looks good for the specific data.",
            "Attend so for P = 2 There is no interaction.",
            "So this when you take the gradient of this you get something that especially doesn't provide an interaction between the models and you get a flat margin average with all coefficients equal to 1 over North with all the coefficients the same, but for P bigger than two you have.",
            "You have this phenomenon of interactions which can be explained as in terms of sparsity of this set of coefficients.",
            "Less than.",
            "OK, in.",
            "You know you need OK.",
            "The bound will depend on the dual norm.",
            "OK, so this is.",
            "Right, the bomb will depend on the dual norm.",
            "Maybe calling this OK, yeah yeah.",
            "So either way I guess you should.",
            "You should use people 2IN.",
            "Probably North Star, so it's true, so he speaks more than one in R. But be cause the bound will depend on the dual norm of the vector of coefficients and you will make your update who's using Echo official bigger than two, but regularizer will be with the dual coefficient, which is less than two, so the dual if BBP gets larger that will coefficient get smaller, so eventually did work.",
            "Official will approximate the one norm.",
            "Oh conclusions good so much short talk.",
            "OK. And."
        ],
        [
            "So we tried this on two datasets.",
            "One is a standard datasets of object categorization, Caltech 101.",
            "We many lots of classes, 3000 images and we used 48 kernels which were obtained by extracting different feature sets from images using different parameterizations.",
            "This is a sort of standard preprocessing of the.",
            "Standard, the multikernel preprocessing of the Caltech datasets."
        ],
        [
            "And we did find this.",
            "We did find this phenomenon of sparsity in this data.",
            "So you see here, this is an average multiclass error rate, which is high because there are lots of classes here.",
            "And so the algorithms are run in the true multiclass set setting.",
            "In the this is the number of examples, so this is the online error rate.",
            "And you see that the best curve here is achieved by this OM two algorithm which is this multi kernel algorithm based on online mirror dissent with a fairly big value of P. Which means that you are essentially looking at a bounded depends on the one plus epsilon norm of your vector of norms of models in the different repetition here, spaces.",
            "And the the if you look at the passive aggressive run on the average cut flat kernel flat corner leverage with passive aggressive, which is a slightly better than.",
            "Then the.",
            "Then what we would have gotten by using mirror dissent with equal 2, so this is actually helping helping the baseline we get something worse and we get also something worse when we run a passive addressing in there with the best kernel.",
            "OK, remember that the prediction is not linear, so it could be that the average kernel gets better than the best kernel because that we're taking you're not making a linear prediction.",
            "OK, then we."
        ],
        [
            "Try the second data set.",
            "This was the data set more focused on our.",
            "No."
        ],
        [
            "You know we tried different values and this this was.",
            "Yeah, I mean at some point it improves and then it then it's there is a. I don't know exactly, but there is a reasonable range of values.",
            "It is not so sensitive up to some point.",
            "Responds to 1 + 1 plus epsilon 1 + 1 / P -- 1.",
            "Something like that if you do it again.",
            "Yes it's which.",
            "Yes, I have another question.",
            "Your framework.",
            "The kernels are fixed ahead of time.",
            "Yes, the kernels not not the not the models the models are being learned right.",
            "The models meaning the DBO.",
            "And how?",
            "Japanese special way of choosing here in the kernels applications.",
            "Sorry the update so you mention some applications here on some datasets.",
            "How do you choose the kernels for the corners now?",
            "In this case it was sort of a standard data set used for multikernel learning, so we had already the kernel matrices.",
            "Because this is out of a standard datasets for doing a multi for playing with multi kernel which is done.",
            "I mean independently of online Robert Analysis, come back to my question yes.",
            "Infinity.",
            "Where is Staples Infinity on this track equals Infinity spicola hundred.",
            "I mean above that it's essentially is to all practical purposes, is like Infinity.",
            "I'm just wondering, yeah, I don't know what actually if it gets worse or not.",
            "If you push it further.",
            "Question that is, does the algorithm get worse?",
            "Equals Infinity.",
            "Is this something that you know 'cause you're making?",
            "Everything you say no no because P goes to Infinity.",
            "Essentially you're using.",
            "You're using an algorithm for with exponential with.",
            "Yeah, no, it's.",
            "OK, if you do vector vector learning and not matrix learning equals Infinity, essentially it gets you a way of putting weights, exponentials using exponentials.",
            "OK, like we know as opposed to perception.",
            "And so you can think of it here, although you don't have a window corresponding a window for this case.",
            "OK, but I don't think it gets any worse.",
            "It's just it doesn't get any better."
        ],
        [
            "So now we we also had this data set, which is Robin navigation data set for place recognition.",
            "This was constructed by Edie up before the project started, and it's like.",
            "Subset because the actual data set was done by having two robots that were roaming in their building and we just took one of them and we have 12 images sequences using a camera on this mobile robot platform and these sequences were taken in different times of the year and the different lighting conditions and rooms.",
            "You can't see much here, but rooms were undergoing a significant variability in there.",
            "Visual conditions.",
            "And then, like we had five different rooms, and so we had five classes, 6K images and four feature sets corresponding to three feature sets from the camera and one feature set from the laser scan.",
            "And so this is a lot less than what we had before, and indeed, that On this date."
        ],
        [
            "That that wasn't done on purpose for this, but was something already there.",
            "The best one turns out to be the essentially the average kernel, and our algorithm with the choice equal 2, essentially matches the performance of the average kernel.",
            "OK, so here we don't see any kind of gain potential gained by betting on sparsity, and the reason could be that you need.",
            "Probably you need a little bit more views to choose from.",
            "So for views here I mean there were no.",
            "No essential advantage of picking a subset of them.",
            "All of them were equally important in terms of predictive capability, so this is just a passive aggressive.",
            "The difference is just the passive aggressive, I bet so yes, yes yes.",
            "Yes it is.",
            "It is absolutely this, or MCL is some other algorithm that was proposed in a different context.",
            "In the so this this we weren't that happy about that and.",
            "OK. How many classes?",
            "These are five classes.",
            "OK. Or"
        ],
        [
            "Right, so how much time do I have?",
            "15 minutes 15 right so?"
        ],
        [
            "The second, the second argument we will look at it is the selective sampling.",
            "Remember, we wanted to sort of a clean a bit.",
            "The landscape of selective sampling algorithms.",
            "So we.",
            "OK, in in this set up in this standard as online learning online active learning setup, you have a family of functions.",
            "For instance, again reproducing kernel Hilbert space or linear or linear predictors as you like, and you have a binary classifier.",
            "So we look at the binary case in this in this presentation and you are comparing the standard online supervised learning where you get one label at each point of time.",
            "You oversee the label.",
            "Of the current example, and so you essentially don't is like if you.",
            "If you think you're bothering the human expert at every point of time, maybe at the beginning of your of your online learning process, and then at some point you decided that you bothered him too much and you freeze your model and you go on with it.",
            "And the alternative is that you know you don't.",
            "You just decide when it's a good time to bother the human expert, because this is a.",
            "A point which is specifically interesting and you have a lot of low confidence in your prediction, so this is of course."
        ],
        [
            "Something that has been studied a lot and we look at the specific setting in which the data process.",
            "So the sequence of instances is the deterministic.",
            "We don't make an assumption.",
            "Is there an?",
            "We have a stochastic label process, so we assume that there is some base optimal.",
            "The Bayes optimal model belongs to our model class, which is OK. For instance, if you are working in a universal kernel space within kernel Hilbert space, and we assume that stochastic.",
            "We assume we make this assumption that the label is a stochastic and expected value conditioned on X is the F star of X, where F star of X is the Bayes optimal classifier for that sequence.",
            "So remember the wise are actually binary values.",
            "OK, and we want to measure the regret we want to measure how many mistakes we make in excess with respect to the mistakes made by the Bayes optimal classifier.",
            "On that specific sequence of X. OK."
        ],
        [
            "So we this problem might be studied a lot in the online context and we focus on the unlike some other groups we focused on River regularised least squares estimates for doing this prediction."
        ],
        [
            "Which makes sense because we are essentially betting on if this is a linear space you remember is sort of this internal space, or is a vector, so you are betting on a linear dependence there.",
            "OK."
        ],
        [
            "And.",
            "So there are regularize these queries, advantage that you get a reasonably efficient algorithm so you can run this algorithm real real real data long sequences and it works.",
            "You don't have a lot of you, you reasonably efficient.",
            "This is unlike other approaches that for instance require you to resolve an empirical risk minimization problem at each time step, which is not feasible in real.",
            "Cases so we in this motivating paper we proposed a query rule, so we're unregularized squares for doing this prediction, and we have a way of deciding whether the confidence of the least squares model is too low for the data at hand, and we use this proxy for the confidence, and this is a statistic.",
            "Actually it depends on a which is the metrics of the past.",
            "The data and by the current depends on the current point X OK and this is large when X is not correlated with any principal component of the past query data.",
            "So it's a kind of a strange guy.",
            "And is an upper bound on both virus bias and variance of F head of your least square estimate with respect to that noise label model.",
            "OK, so you can use this and we compare this against a threshold that varies overtime and depends on a parameter K. So this is a very simple and very easy and also it gets you a deterministic query rule.",
            "You see here it nothing depends on the labels and so you.",
            "Essentially, you know in advance where are you going to make your queries."
        ],
        [
            "And so we have a very complicated theorem with.",
            "The theorem is very complicated because holds for any specific sequence of axis, so I'm not going to comment it, just to scare you a little bit.",
            "But if you assume if you take like one of those, a nice assumptions that statisticians take in this kind of on this kind of problems, which is the standard see back of noise condition, which is a condition on the way the axis are generated.",
            "So now if I assume that the axis are stochastic process actually IID.",
            "So a very specific stochastic process, and I there's a sort of a nice collusion between the density of the axis and the Bayes optimal classifier.",
            "So the axis are not that there is not much to density.",
            "There's no much density in the vicinity of the Biasion decisions surface.",
            "And the way this this parameterization is given by this parameter Alpha, then I can prove.",
            "Basically this can be written that way in if the number of dimensions it's finite.",
            "And so here you measure, measuring the regret how much your how much how fast you are you are converging to base air to base rate to Bayes error rate to base risk.",
            "Per number of queries, issue it OK and this rate is optimal up to log factors in terms of the see back of parameter.",
            "So this is something that tells you that you're doing something reasonable so you're not suboptimal here.",
            "You got the right rate if the data is generated by this process, But the actually the algorithm is more robust because is also good on arbitrary data.",
            "What I mean is, it works for every it work for every sequence of axis, but you need this stochastic noise model on the wise OK?",
            "But ignore here that in the bottom in the bottom, I'm assuming it's the back of, but you need to know how to get that bound.",
            "That's true, which it's not nice.",
            "However you see here.",
            "Well I will come back to this."
        ],
        [
            "After so there are other.",
            "So this barbecue was done in a paper with Francesca Claudio and later on in 2010 Cloud you came up with the offer.",
            "Dacquel and Karthik came up with a very nice other query rule which uses the still uses this proxy but compares it, compares it against the squared margin of the current classifier.",
            "So this is a sort of a nice and intuition.",
            "Shoot if Roof is the squared margin is smaller than something which tells you how strange is the data with respect to the past data of saw.",
            "Then you should make a query.",
            "OK, and so this these are.",
            "This gets regret, regret and query bonds that are incomparable to those of barbecue.",
            "And the under tobacco condition they can.",
            "They can match the optimal rates without need of knowing Alpha, so it's in some sense is stronger algorithm.",
            "Although in practical data the performance is not always better than barbecue, in some cases can be worse.",
            "And so we we one of the merits of this paper I did with Francesco was to sort of understand why in certain cases this other method that doesn't perform so well, and we also tried the other query rules that were proposed before.",
            "One, for instance, is a simplification of DGS that uses a simple the same squared margin, but with a different threshold.",
            "This NT is the number of times the number of queries made in the past.",
            "So basically replace this way that.",
            "And then we have also probabilistic query rule which tells you that you should query a label with probability inversely proportional to the size of the margin.",
            "It makes sense.",
            "OK, so."
        ],
        [
            "Right so.",
            "OK, the last thing we leaving was the fact that these algorithms all these selected something algorithm based on regularised least squares regression.",
            "These queries are Euclidean regularization and.",
            "We would like to have a way of combining online mirror dissent, the mother of all algorithms online algorithms with semi supervised learning and we would like to keep the good properties of the selective sampling algorithm so to have simultaneous bounds on number of queries and not regret like we have here.",
            "And however, we would like the this time, we will let the regret to depend also on the fit between the specific regularizer, not Euclidian using by the algorithm and the data.",
            "So we will like everything.",
            "In the OK."
        ],
        [
            "So we tried to try that.",
            "This is kind of a hard problem and we got some solution which gets you abound that has all the properties we asked for, but it's not.",
            "In practice is not like the thing you would like to use, and so we're still looking for something better.",
            "So basically we are mixing up DJ S, the algorithm of Claudio and the online mirror dissent within arbitrary regularizer and idea, and so you.",
            "Run them in parallel and whenever the selective sampling sampler using Euclidean norm is uncertain, then you go with the other guy you predict with the other guy.",
            "Get the label an up to both of them.",
            "If the Euclidean based the selected sampler is confident, then you go with him.",
            "And you don't get any label.",
            "You don't ask and you and you don't get any label.",
            "You don't query into not getting label.",
            "So again when you look at the selected sampler based on Euclidean norms in order to decide whether to quit or not.",
            "If the selective sampler distance query, then you.",
            "Then you predict with the other one, because this guy is not sure and you use the label to update both.",
            "Otherwise, if the if this algorithm decide not to query, then you just use it to make it a prediction and you gone.",
            "So we can get mistakes that depend on the regularizer.",
            "Here, is there a bound on the number of mistakes of expected number of mistakes?",
            "That depends on the hinge loss, so these bounds now.",
            "Are similar to those that you can prove to the standard online bounds for online mirror dissent, but.",
            "So you have the dependence on the regularizer here, and you also have a bound on a number of queries which is inherited.",
            "With this empty, is the query bound for the GS mode, so we saw the mixed up.",
            "The two guarantee two guarantees.",
            "We have a regret bound which looks like online mirror dissent and the query bound, which looks like the one that we had before, for regularised least squares.",
            "OK."
        ],
        [
            "I'm done these things on real so we tried them on synthetic data, set that synthetic data set generated according to the assumptions we did on the on the labels on the label process, so stochastic labels exactly generated in the way we assume and we see that barbeque.",
            "So this is a standard way of plotting these algorithms.",
            "So here is the fraction of queries examples.",
            "So this is the query rate and this is the performance, the average accuracy.",
            "So you see here you would like this curve to be very very.",
            "To go up very quickly and stay high so the best one here is barbecue and then we have this hybrid rule here, which is also pretty good and this synthetic data were generated according to a sparse target.",
            "So here this I believe that was in the ideal situation and then we have DGS.",
            "Model is kind of a little low but not too much.",
            "OK then we tried on."
        ],
        [
            "The adult data set with Gaussian kernel so typical kernel kernel based data set, and here barbecue is the best and the hybrid there is blown out of the water and so it's not quite satisfying and.",
            "The DGS is not doing well and in the bounds that we prove we can see that in the in kernel space.",
            "DGS has some problem because as a worse dependence on on the kernel space then barbecue."
        ],
        [
            "And then we tried on texture with linear kernel and on text.",
            "There's this other one of the other rules was the best, for which we don't have an analysis.",
            "The hybrid algorithm is still bad and digest now is better, and the barbeque is close to it.",
            "OK, and so we have different.",
            "You see different patterns of behaviors according to the weather.",
            "Data are synthetic kernel based or linear.",
            "And we still don't have, although in the in theory or synthetic data this hybrid algorithm is is working on real world data.",
            "It's really has some serious problems.",
            "We're still trying to understand what's wrong there.",
            "OK conclusions."
        ],
        [
            "So we we saw how to use to make online multiple learning based on metrics, group norms and we also had a better understanding of rigorous discourse based selective sampling algorithms.",
            "And now we have some attempt to the sort of emerge these two worlds that are.",
            "Is this a necessary step in order to have this?",
            "Multi Q Robert Navigation with the semi supervised learning."
        ],
        [
            "In order to answer Josh observation OK, in order to draw these plots you need to parameter.",
            "The parameter is the one that helps you to decide how many.",
            "How big is the created you would like to get.",
            "OK, so if you want to decide on specific query rate, so typically you want to tolerate a certain query rate.",
            "So you set the parameter in order to exhibit that query rate.",
            "I want to I want to bother the human expert one out 100 examples.",
            "That's fine.",
            "OK, so I set my parameter there, so setting that parameter amounts to essentially to choose Alpha."
        ],
        [
            "Over here.",
            "So it's true that in theory it's bad, but actually in practice I have a parameter here which is this Kappa and in order to get this bound I have to tune Kappa in order to know how Alpha.",
            "But in practice I need to.",
            "I need to pick Kappa anyway in order to get the desired query rate, so this is a sort of.",
            "It makes it nicer in for practical use.",
            "OK, thank you very much."
        ],
        [
            "Yes, yes.",
            "If the if the robot is on line right and then the chances are that the true label from one time step to the next time step, yes very good.",
            "So I we I didn't put it.",
            "Yeah I didn't put it, so it's another.",
            "It's in the Insignia proposal.",
            "It's in the proposal.",
            "Clearly there are spatial dependencies here and you clearly would like the online predictor.",
            "Take advantage to take advantage.",
            "On these special dependencies, and this is also and so you would have to have a notion of state, which is something completely another half in this online mirror.",
            "Dissent, kind of.",
            "Analysis and yeah, this is definitely something that is.",
            "We were thinking of looking at it.",
            "Yes, it's in the proposal.",
            "Very good that you.",
            "Tell me back to your hybrid algorithm.",
            "Not doing so well, yes does.",
            "Would you expect if you kind of really pushed to one end of the spectrum or something?",
            "No, I think we need a different algorithm.",
            "This was a sort of a hack to get abound.",
            "In my view, this thing that we used.",
            "It makes sense, but.",
            "We're not basically coming out with a new algorithm, we're just using the things we have a commanding them in a sort of a tricky way in order to get the bound.",
            "Actually, if you have a data set that has these tricky behaviors that bounds going to be a good bound, right?",
            "So in some senses that you haven't yet tested, we we.",
            "We didn't test it, yes, but.",
            "Yes, that's my.",
            "My guess is that this is not going to work.",
            "This is basically to show that it's possible to obtain results of this flavor because before we didn't know and however right we don't, we don't.",
            "I don't think this is the right way to go.",
            "I have another kind of method question which is.",
            "You see these bounds and I'm in my own context, so would probably want to do kind of batch learning, but I don't really have the memory to store the batch, so the online is an excuse.",
            "Absolutely yes, yes, and in that context I really don't care how many mistakes the thing makes early on in the process, unless you are in a kernel space.",
            "Well, because those will become supports for.",
            "Well, if you don't have any space concern, if I'm looking at the method an online method as a method of approximating batch method OK, getting a quick handle on the solution right?",
            "Then I've got a certain number of kernels that the ultimate cost function is going give me right?",
            "So you know, there's a number of mistakes as they reduce aghbal in that sense.",
            "I'm sure the online method might introduce more kernels than that absolutely right.",
            "You know, even the best method introduces more, yeah?",
            "So the point of the question is.",
            "The things that one sees online learning methods is doing well is getting good quick solutions approximate solutions early in the learning process, and then if you run them over batches, they typically have much slower convergence than a batch method.",
            "Are you?",
            "Are you asking whether this is true or this is what we want to see in practice OK?",
            "Well, that says about if you're trying to do bounds for these things is that probably shouldn't care too much about how many mistakes made early in the process.",
            "What you should care about is how much we're reducing the error for later examples with your first few steps, OK, it depends on if you want to get a risk bound by means of an online algorithm.",
            "A you will have.",
            "You will have.",
            "Well that still depends.",
            "It still depends on you have a training set.",
            "You do a pass on the training set, but it still depends on the ratio between mistakes and the sample size.",
            "I mean, there's no way.",
            "I mean, unless you are OK, if you're in a realizable setting is like if I made many mistakes, means that I'm really close to the target, but if I made any mistakes, made it, there's a lot of noise.",
            "How do I know?",
            "I mean, what I'd like to see is something that says this algorithm may be lousy asymptotically, but it's really quick in the beginning.",
            "Yeah, no, I see what you mean.",
            "Yeah, so I think this might be a way of you want to select examples in order to make a lot of lots of mistakes at the beginning and be be OK later on and this makes sense if you can pick.",
            "So if you're in a batch batch setting it makes sense.",
            "So the paper I think the paper by a lot as an on this sub linear perception paper.",
            "It does does something like that.",
            "Learning yes, so they pick those nastiest examples in the pool.",
            "OK, that's fine, but you need to look at your whole pool to do that, right?",
            "But if you're in a batch case, yes.",
            "The other sort of example I have in mind is these boosting methods.",
            "Instead of doing a single step kind of optimize all the past weights.",
            "At the same time, so they have very expensive optimization in each step, but they kind of reusing initial things and re optimizing their weights at each step, so they're kind of using examples more efficiently, and I'm not seeing kind of bound online learning types of bounds for these methods coming out.",
            "No methods are inherently local, and that's their online learning bounds and not inherently locals.",
            "The bus not but the algorithm.",
            "Oh yes, so you know, OK, but you want the algorithm to do something local because you want to sort of go back and to introduce.",
            "Each state, but for example, like the Adaboost, I'm willing to allow to do calculation with the previous examples in each step or something.",
            "Um, I've seen things I've seen things in case.",
            "For instance, if you want to do shifting.",
            "If you want to track if you want to track a moving moving target in the sense of a moving model, then you want to do something.",
            "Basically because we have a sort of maybe a decaying window on the past the support vectors so you are at each time you are.",
            "Removing weight from the old vectors.",
            "For instance.",
            "There are ways of guaranteeing that this can be done in a way.",
            "This is kind of a decomposition of the improvement you can make by adjusting existing weights and the improvement you make by adding a new support vector.",
            "I know, yeah.",
            "These paradoxically quickly batch on the examples.",
            "It's more like what is going to be the next feature that I can.",
            "It's it's kind of online on the addition of the features, which is the next week, but that's boosting by filtering, boosting by filtering.",
            "I think there they have.",
            "The philosophy expensive.",
            "No problem, so sparsely yeah.",
            "I wanted to ask a question about along this line.",
            "I mean if you were using data streaming.",
            "You could have saved set of examples.",
            "OK, and this in the compatible way with the amount of data and then label them at home to the speaker.",
            "Late like a sort of mini batch thing.",
            "Doing data streaming, you would do the robot OK. Would keep a few images which are representative of the trajectory and then those examples those prototypes.",
            "Could be labeled at home.",
            "OK if you introduce delays then it's just you have to take those into account in the bound so the delays will play a role in the bound, because then you're delaying your update.",
            "See what I'm looking over anyway.",
            "Was there a strong idea hypothesis at the beginning or the beginning?",
            "Is this place Robin navigation for place recognition data?",
            "In the norm, it's not.",
            "There's never iid that, it's an arbitrary sequence of data.",
            "I mentioned IID whenever I wanted to.",
            "It's a deterministic process.",
            "I mentioned idea whenever to get whenever I wanted to get this sort of a nice looking bound, but yes question.",
            "This relates to my initial question.",
            "You know this is a stream of this research stream about dictionary learning.",
            "Trying to learn both representation and the dictionary with respect to this representation, and they have the similar.",
            "Problem, but with the addition that they want to learn.",
            "They want to learn the dictionary as well at the same time.",
            "Which it sort of relates your kernel learning problem, but without the kernel space there.",
            "Hitler spaces, which are not fixed it ahead of time.",
            "So this is something that is, yeah.",
            "I, I'm not sure there are online result, so you can you can you can stick in a new feature with the 00 wait and then let it go.",
            "You can always do that for instance when we do statistical machine translation.",
            "You can think that the language model, sorry, the translation table grows with time and you get more and more features get more and more pieces to work on.",
            "This is specific properties over the dictionary, kind of sparsity, some kind of.",
            "Bounded incoherent something like this?",
            "Then why are you talking about the way of?",
            "Seeking new features or you get you get new features from time to time and then you start using them.",
            "Business function right at this point.",
            "Can be anything, can be vectors in some space.",
            "You want to learn.",
            "Let's say K of them and.",
            "Your representation will be with respect to this K vectors.",
            "How is it?",
            "How is this different from support vector learning?",
            "Is because.",
            "Your support that do not come from the data.",
            "Support.com from the data OK for instance.",
            "Yeah.",
            "OK. More questions.",
            "OK, thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a report about the pump priming, supervised learning, or semantic spatial concepts for a mobile robot.",
                    "label": 1
                },
                {
                    "sent": "And I will be reporting about some work I did with Francesca Robona, the Happy Smiley guy who's now in Chicago TTI.",
                    "label": 0
                },
                {
                    "sent": "And that's the reason why it's not here doing his job duties and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I will give you a short program project description and then we'll touch basically 2 themes that this is the stuff we did.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne Sophie.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of all the project details, so this projects I had two participants, Milano Ann Martini.",
                    "label": 0
                },
                {
                    "sent": "And the duration was one year and a half and starting date was beginning of 2010.",
                    "label": 0
                },
                {
                    "sent": "And we did have a kickoff meeting Imagineered in February.",
                    "label": 1
                },
                {
                    "sent": "However, the essentially so far the project went on for about one year and then last year nothing happened 'cause the Matheney site had quite serious hiring problems.",
                    "label": 1
                },
                {
                    "sent": "And we think we have solved this actually in this couple of yesterday, and so we think the project could be resumed very soon.",
                    "label": 1
                },
                {
                    "sent": "And essentially we the second part of it can be carried out.",
                    "label": 0
                },
                {
                    "sent": "So will be.",
                    "label": 0
                },
                {
                    "sent": "So this is not not to talk about.",
                    "label": 0
                },
                {
                    "sent": "Terminated from priming, but will be reporting some preliminary work that we did with Francesco and hopefully the second part will be done at route to this year, OK?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem we were looking at is the problem of place recognition in robot navigation and this is kind of a simple intuitive problem you have, for instance the building and the mobile robot roaming in the building and the Roman wants to know wants to categorize the rooms or the environment where it is at any given point of time.",
                    "label": 0
                },
                {
                    "sent": "So he wants to know that he's in a corridor or that is in the kitchen or it isn't in office in the.",
                    "label": 0
                },
                {
                    "sent": "Maybe some persons office so and so forth and this is of course an SN accessory module in the building of an autonomous robot.",
                    "label": 0
                },
                {
                    "sent": "Even I can see that that I'm doing almost anything about Roberts, and it's also kind of clear that online learning can play an important role in this kind of domain, just cause these environments are not.",
                    "label": 0
                },
                {
                    "sent": "Stationary in terms of different conditions.",
                    "label": 0
                },
                {
                    "sent": "So, for instance that the if you do the training in winter to train the robot recognize an environment, then in summer the lighting will be different than many things will be different and but also throughout the day.",
                    "label": 0
                },
                {
                    "sent": "For instance, said things in a room can change because people move things.",
                    "label": 0
                },
                {
                    "sent": "Chairs have been moved, tables are being moved, so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So it is important that the learning algorithm that is a supervisor that is underlying the robot navigation is able to do continuous learning so.",
                    "label": 1
                },
                {
                    "sent": "This is a good case for online learning and we were all happy about this.",
                    "label": 0
                },
                {
                    "sent": "Viewing this and it's also kind of clear that you cannot just take online learning off the shelf and apply to the problem, because there Robert all in online learning you're supposed to get the label after each prediction, which is typically very unconvenient if you want to have a robot goes autonomously, it's better.",
                    "label": 0
                },
                {
                    "sent": "The form of what we call Semisupervised active learning or selective sampling, which is the.",
                    "label": 1
                },
                {
                    "sent": "The declination of online learning where the.",
                    "label": 0
                },
                {
                    "sent": "Where the agent, in this case, the robot, asks only occasionally supervised the help.",
                    "label": 0
                },
                {
                    "sent": "So in this case the robot will go about the building.",
                    "label": 0
                },
                {
                    "sent": "At some point the confidence level in its classification will drop below a certain threshold and he will stop to ask for human supervision.",
                    "label": 0
                },
                {
                    "sent": "Where where am I OK?",
                    "label": 0
                },
                {
                    "sent": "So this was kind of a very clear and.",
                    "label": 0
                },
                {
                    "sent": "And and intuitive basis on which we built a bit of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theory, and So what we did in this first part of the project.",
                    "label": 0
                },
                {
                    "sent": "So in the year 2010 with Francesco was too.",
                    "label": 0
                },
                {
                    "sent": "Essentially, craft all the modules that we needed to build a system that was able to perform this online.",
                    "label": 0
                },
                {
                    "sent": "Robot navigation in a super supervised in a semi supervised setting.",
                    "label": 0
                },
                {
                    "sent": "So first of all we need to have online algorithms that are able to do.",
                    "label": 1
                },
                {
                    "sent": "Multi view learning because typically robots have different sensors and they also you typically the camera.",
                    "label": 0
                },
                {
                    "sent": "The camera provides different sets of features and so it is important to be able to integrate these different source of features in such a way to exploit specificities of the data.",
                    "label": 0
                },
                {
                    "sent": "For instance sparsity which is typical.",
                    "label": 0
                },
                {
                    "sent": "So this is worth one problem.",
                    "label": 0
                },
                {
                    "sent": "The other problem was to.",
                    "label": 0
                },
                {
                    "sent": "Try to get a clearer understanding of the algorithmic landscape in online active learning or selective sampling, so to sharpen the results and to clean up the analysis and the algorithms in order to have a good set of tools to use.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And in doing that, we immediately ran into the problem of having these two things talk to each other and the problem is this.",
                    "label": 0
                },
                {
                    "sent": "In order to do multi multi kernel learning, for instance to enhance the sparsity, you typically would like to apply regularizers to the problem that are non ingredient like L1 in some in some linear space.",
                    "label": 0
                },
                {
                    "sent": "OK and on the other hand especially the most powerful algorithms.",
                    "label": 1
                },
                {
                    "sent": "Analysis For Semi supervised learning for selective sampling do rely on Euclidean norms.",
                    "label": 0
                },
                {
                    "sent": "So we wanted first also to come up with a way of reconciling these two aspects to get an algorithm to get a selective sampling algorithm that was able to deal with non Euclidean norms but also enjoy the strong performance guarantees that.",
                    "label": 0
                },
                {
                    "sent": "He then norm based algorithms have OK and then the we would like what's left to do here is to do an integration and TuneIn tune up of these modules.",
                    "label": 0
                },
                {
                    "sent": "You will see that not all results are like totally satisfactory yet.",
                    "label": 0
                },
                {
                    "sent": "And testing them on Roberts platform and maybe do a bit of iterations in these two last two steps.",
                    "label": 0
                },
                {
                    "sent": "So we need we need really to see how it works on anything.",
                    "label": 0
                },
                {
                    "sent": "On the field.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So publications, so we had two publication publications out of it in 2010 and then one in 2011 Eleven and this publication here was the one which triggered the whole research because we had a very simple and powerful algorithm for doing semi supervised online learning.",
                    "label": 0
                },
                {
                    "sent": "And so this paper is about the multikernel extension, and this paper is about the refinement of selective sampling and the non Euclidean norm's business.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Came.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we start with the online multi kernel learning.",
                    "label": 0
                },
                {
                    "sent": "So we we want to do all integrate online integration of data coming from robot sensors and this.",
                    "label": 1
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "Basically it was not really hard given the existing technology becauses essentially the online analysis of an online algorithms is general enough that once you come up with the right regularizer you can almost to plug it in into the current analysis and get meaningful bound at a good algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here there were, there was a set of additional difficulties because we wanted to get a multiclass extension of it.",
                    "label": 0
                },
                {
                    "sent": "And we did want to use the specific norms that were in there, haven't been used before so we didn't know anything about the practical performance of that.",
                    "label": 0
                },
                {
                    "sent": "So in OK, so we look at the kernel based multiview learning so we have a fixed set of reproducing kernel Hilbert spaces and we want to simultaneously.",
                    "label": 0
                },
                {
                    "sent": "On lunch, online, train or training, the online Model 1 model in each reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get this stream of data.",
                    "label": 0
                },
                {
                    "sent": "The data that come from the sensor of the robot and we want to get this data, apply different kernels to each feature set and then to integrate them.",
                    "label": 1
                },
                {
                    "sent": "And then we we do online prediction with this set of models that are being trained using a combination where we.",
                    "label": 0
                },
                {
                    "sent": "In both the model within the reproducing kernel Hilbert space and the coefficients of linear combination.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that OK, if you believe that for instance there are a few good kernels in your pool, you would like to.",
                    "label": 0
                },
                {
                    "sent": "I'd like to.",
                    "label": 0
                },
                {
                    "sent": "Have a few big coefficients here that focus on the good kernels and leave the others kind of law and OK at the baseline is the natural baselines here.",
                    "label": 1
                },
                {
                    "sent": "The single best training model in a in a best, best kernel essay?",
                    "label": 0
                },
                {
                    "sent": "So the model in the training model in the best position, kernel Hilbert space and the simple argument does a average margin prediction with the flat average.",
                    "label": 0
                },
                {
                    "sent": "So without playing with these coefficients, so here again both coefficients and the models are trained online.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way you do this is by using like it.",
                    "label": 0
                },
                {
                    "sent": "This is Francesco the definition and the mother of all all online algorithms.",
                    "label": 1
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In understanding that the FIS will be learned in some sense independently of the learning of the authorizer, there will be interaction.",
                    "label": 0
                },
                {
                    "sent": "There will be interaction.",
                    "label": 0
                },
                {
                    "sent": "There will be interaction.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to explain that the way the interaction goes.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because they don't have time to do, but there is interaction and the regularizer is providing this interaction.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the bread and butter of any online learner and so it's it's online mirror dissent.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of generalized gradient descent, so you have a data stream coming in and you want to.",
                    "label": 0
                },
                {
                    "sent": "So we are here.",
                    "label": 0
                },
                {
                    "sent": "We are in the in the standard online learning settings, so you get one day at a time.",
                    "label": 0
                },
                {
                    "sent": "You predict the label of the data and then you get the true label.",
                    "label": 0
                },
                {
                    "sent": "You suffer some loss and then you update your model.",
                    "label": 0
                },
                {
                    "sent": "OK, so all enemy they sent you have actually two models, the primal model, which I call G here because we are possibly in some weird space and in some General general general linear space and dual model, which I call F. So the dual model F you use it to predict and the primal Model G you use it to.",
                    "label": 0
                },
                {
                    "sent": "Perform the update.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of schizoid algorithm and that has these two parameters, and so the regularization function plays the role of talking like.",
                    "label": 0
                },
                {
                    "sent": "Does the communication between the primal and dual model, indeed the gradient of the regularization function is called the link function because it acts as a link between primal and dual.",
                    "label": 0
                },
                {
                    "sent": "And so the trick is, is that you have regularization function but and the bound is going to depend on the performance bond is going to depend on this regularization function.",
                    "label": 0
                },
                {
                    "sent": "However, the update is going to be defined in terms of the gradient of the dual of the regularization function.",
                    "label": 0
                },
                {
                    "sent": "What is that OK, if R is the squared norm, which is most often the case, then you have the the.",
                    "label": 1
                },
                {
                    "sent": "Our star is the square the dual norm.",
                    "label": 0
                },
                {
                    "sent": "OK, for instance, if you have the P norm for vectors, are stars going to be Q norm squared OK and you see here that the update of the primal model is done doing a gradient step with the gradient of the loss.",
                    "label": 0
                },
                {
                    "sent": "Oh, this is a mistake.",
                    "label": 0
                },
                {
                    "sent": "This should be an F here.",
                    "label": 0
                },
                {
                    "sent": "There's an F here, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so doing the.",
                    "label": 0
                },
                {
                    "sent": "With the gradient of the loss of the dual model, so again there's going to be an F here.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "Where it is, there is no.",
                    "label": 0
                },
                {
                    "sent": "There's a OK good sorry, sorry I I changed notation which I shouldn't have done, and so this is a G and this is an F. So you can use any strongly convex function as a regularizer, so it's a pretty liberal setup and the performance bond will be good when the choice of the Rising match matches the data sequence that you're going to predict on.",
                    "label": 1
                },
                {
                    "sent": "This is kind of expectable, and F&G can belong to many pair of dollars.",
                    "label": 0
                },
                {
                    "sent": "Do a linear spaces so you can run this into an on a metric space, for instance, or a vector space, or any other linear space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, and if you run it into my automatic space as we do for doing multi kernel learning, then at R will be a typical E squared matrix norm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do online multi kernel learning is actually kind of simple.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple thing to do and you take a matrix.",
                    "label": 0
                },
                {
                    "sent": "For instance if you want to like we wanted to see whether we will be able to pick sparsity in the data.",
                    "label": 0
                },
                {
                    "sent": "So what we did is to use what is called the 2P matrix group norm which is simple.",
                    "label": 0
                },
                {
                    "sent": "You have your metrics of models.",
                    "label": 0
                },
                {
                    "sent": "OK so these are these are vectors of coefficients of linear coefficients.",
                    "label": 0
                },
                {
                    "sent": "And you take the square the P norm of the vector whose.",
                    "label": 0
                },
                {
                    "sent": "Elements whose components are the Euclidean norms of the models.",
                    "label": 0
                },
                {
                    "sent": "If you run this into kernel space, these are not vectors.",
                    "label": 0
                },
                {
                    "sent": "These are functions and, but you can take the Euclidean norm of that function in the reproducing kernel Hilbert space, so that's well defined it, and by playing with the P coefficient you can.",
                    "label": 0
                },
                {
                    "sent": "You can enhance this sparsity in the vector of norms for the models in the different kernel spaces.",
                    "label": 0
                },
                {
                    "sent": "So essentially you can focus on a subset of the kernels that looks good for the specific data.",
                    "label": 0
                },
                {
                    "sent": "Attend so for P = 2 There is no interaction.",
                    "label": 1
                },
                {
                    "sent": "So this when you take the gradient of this you get something that especially doesn't provide an interaction between the models and you get a flat margin average with all coefficients equal to 1 over North with all the coefficients the same, but for P bigger than two you have.",
                    "label": 0
                },
                {
                    "sent": "You have this phenomenon of interactions which can be explained as in terms of sparsity of this set of coefficients.",
                    "label": 0
                },
                {
                    "sent": "Less than.",
                    "label": 0
                },
                {
                    "sent": "OK, in.",
                    "label": 0
                },
                {
                    "sent": "You know you need OK.",
                    "label": 0
                },
                {
                    "sent": "The bound will depend on the dual norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Right, the bomb will depend on the dual norm.",
                    "label": 0
                },
                {
                    "sent": "Maybe calling this OK, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So either way I guess you should.",
                    "label": 0
                },
                {
                    "sent": "You should use people 2IN.",
                    "label": 0
                },
                {
                    "sent": "Probably North Star, so it's true, so he speaks more than one in R. But be cause the bound will depend on the dual norm of the vector of coefficients and you will make your update who's using Echo official bigger than two, but regularizer will be with the dual coefficient, which is less than two, so the dual if BBP gets larger that will coefficient get smaller, so eventually did work.",
                    "label": 0
                },
                {
                    "sent": "Official will approximate the one norm.",
                    "label": 0
                },
                {
                    "sent": "Oh conclusions good so much short talk.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we tried this on two datasets.",
                    "label": 0
                },
                {
                    "sent": "One is a standard datasets of object categorization, Caltech 101.",
                    "label": 1
                },
                {
                    "sent": "We many lots of classes, 3000 images and we used 48 kernels which were obtained by extracting different feature sets from images using different parameterizations.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of standard preprocessing of the.",
                    "label": 0
                },
                {
                    "sent": "Standard, the multikernel preprocessing of the Caltech datasets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we did find this.",
                    "label": 0
                },
                {
                    "sent": "We did find this phenomenon of sparsity in this data.",
                    "label": 0
                },
                {
                    "sent": "So you see here, this is an average multiclass error rate, which is high because there are lots of classes here.",
                    "label": 0
                },
                {
                    "sent": "And so the algorithms are run in the true multiclass set setting.",
                    "label": 0
                },
                {
                    "sent": "In the this is the number of examples, so this is the online error rate.",
                    "label": 0
                },
                {
                    "sent": "And you see that the best curve here is achieved by this OM two algorithm which is this multi kernel algorithm based on online mirror dissent with a fairly big value of P. Which means that you are essentially looking at a bounded depends on the one plus epsilon norm of your vector of norms of models in the different repetition here, spaces.",
                    "label": 0
                },
                {
                    "sent": "And the the if you look at the passive aggressive run on the average cut flat kernel flat corner leverage with passive aggressive, which is a slightly better than.",
                    "label": 0
                },
                {
                    "sent": "Then the.",
                    "label": 0
                },
                {
                    "sent": "Then what we would have gotten by using mirror dissent with equal 2, so this is actually helping helping the baseline we get something worse and we get also something worse when we run a passive addressing in there with the best kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, remember that the prediction is not linear, so it could be that the average kernel gets better than the best kernel because that we're taking you're not making a linear prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, then we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try the second data set.",
                    "label": 0
                },
                {
                    "sent": "This was the data set more focused on our.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know we tried different values and this this was.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean at some point it improves and then it then it's there is a. I don't know exactly, but there is a reasonable range of values.",
                    "label": 0
                },
                {
                    "sent": "It is not so sensitive up to some point.",
                    "label": 0
                },
                {
                    "sent": "Responds to 1 + 1 plus epsilon 1 + 1 / P -- 1.",
                    "label": 0
                },
                {
                    "sent": "Something like that if you do it again.",
                    "label": 0
                },
                {
                    "sent": "Yes it's which.",
                    "label": 0
                },
                {
                    "sent": "Yes, I have another question.",
                    "label": 0
                },
                {
                    "sent": "Your framework.",
                    "label": 0
                },
                {
                    "sent": "The kernels are fixed ahead of time.",
                    "label": 0
                },
                {
                    "sent": "Yes, the kernels not not the not the models the models are being learned right.",
                    "label": 0
                },
                {
                    "sent": "The models meaning the DBO.",
                    "label": 0
                },
                {
                    "sent": "And how?",
                    "label": 0
                },
                {
                    "sent": "Japanese special way of choosing here in the kernels applications.",
                    "label": 0
                },
                {
                    "sent": "Sorry the update so you mention some applications here on some datasets.",
                    "label": 0
                },
                {
                    "sent": "How do you choose the kernels for the corners now?",
                    "label": 0
                },
                {
                    "sent": "In this case it was sort of a standard data set used for multikernel learning, so we had already the kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "Because this is out of a standard datasets for doing a multi for playing with multi kernel which is done.",
                    "label": 0
                },
                {
                    "sent": "I mean independently of online Robert Analysis, come back to my question yes.",
                    "label": 0
                },
                {
                    "sent": "Infinity.",
                    "label": 0
                },
                {
                    "sent": "Where is Staples Infinity on this track equals Infinity spicola hundred.",
                    "label": 0
                },
                {
                    "sent": "I mean above that it's essentially is to all practical purposes, is like Infinity.",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering, yeah, I don't know what actually if it gets worse or not.",
                    "label": 0
                },
                {
                    "sent": "If you push it further.",
                    "label": 0
                },
                {
                    "sent": "Question that is, does the algorithm get worse?",
                    "label": 0
                },
                {
                    "sent": "Equals Infinity.",
                    "label": 0
                },
                {
                    "sent": "Is this something that you know 'cause you're making?",
                    "label": 0
                },
                {
                    "sent": "Everything you say no no because P goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Essentially you're using.",
                    "label": 0
                },
                {
                    "sent": "You're using an algorithm for with exponential with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, it's.",
                    "label": 0
                },
                {
                    "sent": "OK, if you do vector vector learning and not matrix learning equals Infinity, essentially it gets you a way of putting weights, exponentials using exponentials.",
                    "label": 0
                },
                {
                    "sent": "OK, like we know as opposed to perception.",
                    "label": 0
                },
                {
                    "sent": "And so you can think of it here, although you don't have a window corresponding a window for this case.",
                    "label": 0
                },
                {
                    "sent": "OK, but I don't think it gets any worse.",
                    "label": 0
                },
                {
                    "sent": "It's just it doesn't get any better.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we we also had this data set, which is Robin navigation data set for place recognition.",
                    "label": 0
                },
                {
                    "sent": "This was constructed by Edie up before the project started, and it's like.",
                    "label": 0
                },
                {
                    "sent": "Subset because the actual data set was done by having two robots that were roaming in their building and we just took one of them and we have 12 images sequences using a camera on this mobile robot platform and these sequences were taken in different times of the year and the different lighting conditions and rooms.",
                    "label": 0
                },
                {
                    "sent": "You can't see much here, but rooms were undergoing a significant variability in there.",
                    "label": 0
                },
                {
                    "sent": "Visual conditions.",
                    "label": 0
                },
                {
                    "sent": "And then, like we had five different rooms, and so we had five classes, 6K images and four feature sets corresponding to three feature sets from the camera and one feature set from the laser scan.",
                    "label": 1
                },
                {
                    "sent": "And so this is a lot less than what we had before, and indeed, that On this date.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That that wasn't done on purpose for this, but was something already there.",
                    "label": 0
                },
                {
                    "sent": "The best one turns out to be the essentially the average kernel, and our algorithm with the choice equal 2, essentially matches the performance of the average kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we don't see any kind of gain potential gained by betting on sparsity, and the reason could be that you need.",
                    "label": 0
                },
                {
                    "sent": "Probably you need a little bit more views to choose from.",
                    "label": 0
                },
                {
                    "sent": "So for views here I mean there were no.",
                    "label": 0
                },
                {
                    "sent": "No essential advantage of picking a subset of them.",
                    "label": 0
                },
                {
                    "sent": "All of them were equally important in terms of predictive capability, so this is just a passive aggressive.",
                    "label": 0
                },
                {
                    "sent": "The difference is just the passive aggressive, I bet so yes, yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yes it is.",
                    "label": 0
                },
                {
                    "sent": "It is absolutely this, or MCL is some other algorithm that was proposed in a different context.",
                    "label": 0
                },
                {
                    "sent": "In the so this this we weren't that happy about that and.",
                    "label": 0
                },
                {
                    "sent": "OK. How many classes?",
                    "label": 0
                },
                {
                    "sent": "These are five classes.",
                    "label": 0
                },
                {
                    "sent": "OK. Or",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so how much time do I have?",
                    "label": 0
                },
                {
                    "sent": "15 minutes 15 right so?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second, the second argument we will look at it is the selective sampling.",
                    "label": 0
                },
                {
                    "sent": "Remember, we wanted to sort of a clean a bit.",
                    "label": 0
                },
                {
                    "sent": "The landscape of selective sampling algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "OK, in in this set up in this standard as online learning online active learning setup, you have a family of functions.",
                    "label": 0
                },
                {
                    "sent": "For instance, again reproducing kernel Hilbert space or linear or linear predictors as you like, and you have a binary classifier.",
                    "label": 0
                },
                {
                    "sent": "So we look at the binary case in this in this presentation and you are comparing the standard online supervised learning where you get one label at each point of time.",
                    "label": 0
                },
                {
                    "sent": "You oversee the label.",
                    "label": 0
                },
                {
                    "sent": "Of the current example, and so you essentially don't is like if you.",
                    "label": 0
                },
                {
                    "sent": "If you think you're bothering the human expert at every point of time, maybe at the beginning of your of your online learning process, and then at some point you decided that you bothered him too much and you freeze your model and you go on with it.",
                    "label": 0
                },
                {
                    "sent": "And the alternative is that you know you don't.",
                    "label": 0
                },
                {
                    "sent": "You just decide when it's a good time to bother the human expert, because this is a.",
                    "label": 0
                },
                {
                    "sent": "A point which is specifically interesting and you have a lot of low confidence in your prediction, so this is of course.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something that has been studied a lot and we look at the specific setting in which the data process.",
                    "label": 1
                },
                {
                    "sent": "So the sequence of instances is the deterministic.",
                    "label": 0
                },
                {
                    "sent": "We don't make an assumption.",
                    "label": 0
                },
                {
                    "sent": "Is there an?",
                    "label": 0
                },
                {
                    "sent": "We have a stochastic label process, so we assume that there is some base optimal.",
                    "label": 1
                },
                {
                    "sent": "The Bayes optimal model belongs to our model class, which is OK. For instance, if you are working in a universal kernel space within kernel Hilbert space, and we assume that stochastic.",
                    "label": 0
                },
                {
                    "sent": "We assume we make this assumption that the label is a stochastic and expected value conditioned on X is the F star of X, where F star of X is the Bayes optimal classifier for that sequence.",
                    "label": 0
                },
                {
                    "sent": "So remember the wise are actually binary values.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want to measure the regret we want to measure how many mistakes we make in excess with respect to the mistakes made by the Bayes optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "On that specific sequence of X. OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we this problem might be studied a lot in the online context and we focus on the unlike some other groups we focused on River regularised least squares estimates for doing this prediction.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which makes sense because we are essentially betting on if this is a linear space you remember is sort of this internal space, or is a vector, so you are betting on a linear dependence there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So there are regularize these queries, advantage that you get a reasonably efficient algorithm so you can run this algorithm real real real data long sequences and it works.",
                    "label": 0
                },
                {
                    "sent": "You don't have a lot of you, you reasonably efficient.",
                    "label": 0
                },
                {
                    "sent": "This is unlike other approaches that for instance require you to resolve an empirical risk minimization problem at each time step, which is not feasible in real.",
                    "label": 0
                },
                {
                    "sent": "Cases so we in this motivating paper we proposed a query rule, so we're unregularized squares for doing this prediction, and we have a way of deciding whether the confidence of the least squares model is too low for the data at hand, and we use this proxy for the confidence, and this is a statistic.",
                    "label": 0
                },
                {
                    "sent": "Actually it depends on a which is the metrics of the past.",
                    "label": 0
                },
                {
                    "sent": "The data and by the current depends on the current point X OK and this is large when X is not correlated with any principal component of the past query data.",
                    "label": 1
                },
                {
                    "sent": "So it's a kind of a strange guy.",
                    "label": 1
                },
                {
                    "sent": "And is an upper bound on both virus bias and variance of F head of your least square estimate with respect to that noise label model.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can use this and we compare this against a threshold that varies overtime and depends on a parameter K. So this is a very simple and very easy and also it gets you a deterministic query rule.",
                    "label": 0
                },
                {
                    "sent": "You see here it nothing depends on the labels and so you.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you know in advance where are you going to make your queries.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we have a very complicated theorem with.",
                    "label": 0
                },
                {
                    "sent": "The theorem is very complicated because holds for any specific sequence of axis, so I'm not going to comment it, just to scare you a little bit.",
                    "label": 0
                },
                {
                    "sent": "But if you assume if you take like one of those, a nice assumptions that statisticians take in this kind of on this kind of problems, which is the standard see back of noise condition, which is a condition on the way the axis are generated.",
                    "label": 0
                },
                {
                    "sent": "So now if I assume that the axis are stochastic process actually IID.",
                    "label": 0
                },
                {
                    "sent": "So a very specific stochastic process, and I there's a sort of a nice collusion between the density of the axis and the Bayes optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "So the axis are not that there is not much to density.",
                    "label": 0
                },
                {
                    "sent": "There's no much density in the vicinity of the Biasion decisions surface.",
                    "label": 0
                },
                {
                    "sent": "And the way this this parameterization is given by this parameter Alpha, then I can prove.",
                    "label": 0
                },
                {
                    "sent": "Basically this can be written that way in if the number of dimensions it's finite.",
                    "label": 0
                },
                {
                    "sent": "And so here you measure, measuring the regret how much your how much how fast you are you are converging to base air to base rate to Bayes error rate to base risk.",
                    "label": 0
                },
                {
                    "sent": "Per number of queries, issue it OK and this rate is optimal up to log factors in terms of the see back of parameter.",
                    "label": 1
                },
                {
                    "sent": "So this is something that tells you that you're doing something reasonable so you're not suboptimal here.",
                    "label": 0
                },
                {
                    "sent": "You got the right rate if the data is generated by this process, But the actually the algorithm is more robust because is also good on arbitrary data.",
                    "label": 0
                },
                {
                    "sent": "What I mean is, it works for every it work for every sequence of axis, but you need this stochastic noise model on the wise OK?",
                    "label": 1
                },
                {
                    "sent": "But ignore here that in the bottom in the bottom, I'm assuming it's the back of, but you need to know how to get that bound.",
                    "label": 0
                },
                {
                    "sent": "That's true, which it's not nice.",
                    "label": 0
                },
                {
                    "sent": "However you see here.",
                    "label": 0
                },
                {
                    "sent": "Well I will come back to this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After so there are other.",
                    "label": 0
                },
                {
                    "sent": "So this barbecue was done in a paper with Francesca Claudio and later on in 2010 Cloud you came up with the offer.",
                    "label": 0
                },
                {
                    "sent": "Dacquel and Karthik came up with a very nice other query rule which uses the still uses this proxy but compares it, compares it against the squared margin of the current classifier.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of a nice and intuition.",
                    "label": 0
                },
                {
                    "sent": "Shoot if Roof is the squared margin is smaller than something which tells you how strange is the data with respect to the past data of saw.",
                    "label": 0
                },
                {
                    "sent": "Then you should make a query.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this these are.",
                    "label": 0
                },
                {
                    "sent": "This gets regret, regret and query bonds that are incomparable to those of barbecue.",
                    "label": 1
                },
                {
                    "sent": "And the under tobacco condition they can.",
                    "label": 1
                },
                {
                    "sent": "They can match the optimal rates without need of knowing Alpha, so it's in some sense is stronger algorithm.",
                    "label": 0
                },
                {
                    "sent": "Although in practical data the performance is not always better than barbecue, in some cases can be worse.",
                    "label": 0
                },
                {
                    "sent": "And so we we one of the merits of this paper I did with Francesco was to sort of understand why in certain cases this other method that doesn't perform so well, and we also tried the other query rules that were proposed before.",
                    "label": 0
                },
                {
                    "sent": "One, for instance, is a simplification of DGS that uses a simple the same squared margin, but with a different threshold.",
                    "label": 0
                },
                {
                    "sent": "This NT is the number of times the number of queries made in the past.",
                    "label": 0
                },
                {
                    "sent": "So basically replace this way that.",
                    "label": 0
                },
                {
                    "sent": "And then we have also probabilistic query rule which tells you that you should query a label with probability inversely proportional to the size of the margin.",
                    "label": 0
                },
                {
                    "sent": "It makes sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "OK, the last thing we leaving was the fact that these algorithms all these selected something algorithm based on regularised least squares regression.",
                    "label": 0
                },
                {
                    "sent": "These queries are Euclidean regularization and.",
                    "label": 1
                },
                {
                    "sent": "We would like to have a way of combining online mirror dissent, the mother of all algorithms online algorithms with semi supervised learning and we would like to keep the good properties of the selective sampling algorithm so to have simultaneous bounds on number of queries and not regret like we have here.",
                    "label": 1
                },
                {
                    "sent": "And however, we would like the this time, we will let the regret to depend also on the fit between the specific regularizer, not Euclidian using by the algorithm and the data.",
                    "label": 0
                },
                {
                    "sent": "So we will like everything.",
                    "label": 0
                },
                {
                    "sent": "In the OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we tried to try that.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a hard problem and we got some solution which gets you abound that has all the properties we asked for, but it's not.",
                    "label": 0
                },
                {
                    "sent": "In practice is not like the thing you would like to use, and so we're still looking for something better.",
                    "label": 0
                },
                {
                    "sent": "So basically we are mixing up DJ S, the algorithm of Claudio and the online mirror dissent within arbitrary regularizer and idea, and so you.",
                    "label": 0
                },
                {
                    "sent": "Run them in parallel and whenever the selective sampling sampler using Euclidean norm is uncertain, then you go with the other guy you predict with the other guy.",
                    "label": 0
                },
                {
                    "sent": "Get the label an up to both of them.",
                    "label": 0
                },
                {
                    "sent": "If the Euclidean based the selected sampler is confident, then you go with him.",
                    "label": 0
                },
                {
                    "sent": "And you don't get any label.",
                    "label": 0
                },
                {
                    "sent": "You don't ask and you and you don't get any label.",
                    "label": 0
                },
                {
                    "sent": "You don't query into not getting label.",
                    "label": 0
                },
                {
                    "sent": "So again when you look at the selected sampler based on Euclidean norms in order to decide whether to quit or not.",
                    "label": 0
                },
                {
                    "sent": "If the selective sampler distance query, then you.",
                    "label": 0
                },
                {
                    "sent": "Then you predict with the other one, because this guy is not sure and you use the label to update both.",
                    "label": 1
                },
                {
                    "sent": "Otherwise, if the if this algorithm decide not to query, then you just use it to make it a prediction and you gone.",
                    "label": 0
                },
                {
                    "sent": "So we can get mistakes that depend on the regularizer.",
                    "label": 0
                },
                {
                    "sent": "Here, is there a bound on the number of mistakes of expected number of mistakes?",
                    "label": 0
                },
                {
                    "sent": "That depends on the hinge loss, so these bounds now.",
                    "label": 0
                },
                {
                    "sent": "Are similar to those that you can prove to the standard online bounds for online mirror dissent, but.",
                    "label": 1
                },
                {
                    "sent": "So you have the dependence on the regularizer here, and you also have a bound on a number of queries which is inherited.",
                    "label": 0
                },
                {
                    "sent": "With this empty, is the query bound for the GS mode, so we saw the mixed up.",
                    "label": 0
                },
                {
                    "sent": "The two guarantee two guarantees.",
                    "label": 0
                },
                {
                    "sent": "We have a regret bound which looks like online mirror dissent and the query bound, which looks like the one that we had before, for regularised least squares.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm done these things on real so we tried them on synthetic data, set that synthetic data set generated according to the assumptions we did on the on the labels on the label process, so stochastic labels exactly generated in the way we assume and we see that barbeque.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard way of plotting these algorithms.",
                    "label": 0
                },
                {
                    "sent": "So here is the fraction of queries examples.",
                    "label": 0
                },
                {
                    "sent": "So this is the query rate and this is the performance, the average accuracy.",
                    "label": 0
                },
                {
                    "sent": "So you see here you would like this curve to be very very.",
                    "label": 0
                },
                {
                    "sent": "To go up very quickly and stay high so the best one here is barbecue and then we have this hybrid rule here, which is also pretty good and this synthetic data were generated according to a sparse target.",
                    "label": 0
                },
                {
                    "sent": "So here this I believe that was in the ideal situation and then we have DGS.",
                    "label": 0
                },
                {
                    "sent": "Model is kind of a little low but not too much.",
                    "label": 0
                },
                {
                    "sent": "OK then we tried on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The adult data set with Gaussian kernel so typical kernel kernel based data set, and here barbecue is the best and the hybrid there is blown out of the water and so it's not quite satisfying and.",
                    "label": 0
                },
                {
                    "sent": "The DGS is not doing well and in the bounds that we prove we can see that in the in kernel space.",
                    "label": 0
                },
                {
                    "sent": "DGS has some problem because as a worse dependence on on the kernel space then barbecue.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we tried on texture with linear kernel and on text.",
                    "label": 0
                },
                {
                    "sent": "There's this other one of the other rules was the best, for which we don't have an analysis.",
                    "label": 0
                },
                {
                    "sent": "The hybrid algorithm is still bad and digest now is better, and the barbeque is close to it.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we have different.",
                    "label": 0
                },
                {
                    "sent": "You see different patterns of behaviors according to the weather.",
                    "label": 0
                },
                {
                    "sent": "Data are synthetic kernel based or linear.",
                    "label": 0
                },
                {
                    "sent": "And we still don't have, although in the in theory or synthetic data this hybrid algorithm is is working on real world data.",
                    "label": 0
                },
                {
                    "sent": "It's really has some serious problems.",
                    "label": 0
                },
                {
                    "sent": "We're still trying to understand what's wrong there.",
                    "label": 0
                },
                {
                    "sent": "OK conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we we saw how to use to make online multiple learning based on metrics, group norms and we also had a better understanding of rigorous discourse based selective sampling algorithms.",
                    "label": 1
                },
                {
                    "sent": "And now we have some attempt to the sort of emerge these two worlds that are.",
                    "label": 0
                },
                {
                    "sent": "Is this a necessary step in order to have this?",
                    "label": 0
                },
                {
                    "sent": "Multi Q Robert Navigation with the semi supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to answer Josh observation OK, in order to draw these plots you need to parameter.",
                    "label": 0
                },
                {
                    "sent": "The parameter is the one that helps you to decide how many.",
                    "label": 0
                },
                {
                    "sent": "How big is the created you would like to get.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to decide on specific query rate, so typically you want to tolerate a certain query rate.",
                    "label": 0
                },
                {
                    "sent": "So you set the parameter in order to exhibit that query rate.",
                    "label": 0
                },
                {
                    "sent": "I want to I want to bother the human expert one out 100 examples.",
                    "label": 0
                },
                {
                    "sent": "That's fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so I set my parameter there, so setting that parameter amounts to essentially to choose Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over here.",
                    "label": 0
                },
                {
                    "sent": "So it's true that in theory it's bad, but actually in practice I have a parameter here which is this Kappa and in order to get this bound I have to tune Kappa in order to know how Alpha.",
                    "label": 0
                },
                {
                    "sent": "But in practice I need to.",
                    "label": 0
                },
                {
                    "sent": "I need to pick Kappa anyway in order to get the desired query rate, so this is a sort of.",
                    "label": 0
                },
                {
                    "sent": "It makes it nicer in for practical use.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "If the if the robot is on line right and then the chances are that the true label from one time step to the next time step, yes very good.",
                    "label": 0
                },
                {
                    "sent": "So I we I didn't put it.",
                    "label": 0
                },
                {
                    "sent": "Yeah I didn't put it, so it's another.",
                    "label": 0
                },
                {
                    "sent": "It's in the Insignia proposal.",
                    "label": 0
                },
                {
                    "sent": "It's in the proposal.",
                    "label": 0
                },
                {
                    "sent": "Clearly there are spatial dependencies here and you clearly would like the online predictor.",
                    "label": 0
                },
                {
                    "sent": "Take advantage to take advantage.",
                    "label": 0
                },
                {
                    "sent": "On these special dependencies, and this is also and so you would have to have a notion of state, which is something completely another half in this online mirror.",
                    "label": 0
                },
                {
                    "sent": "Dissent, kind of.",
                    "label": 0
                },
                {
                    "sent": "Analysis and yeah, this is definitely something that is.",
                    "label": 0
                },
                {
                    "sent": "We were thinking of looking at it.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's in the proposal.",
                    "label": 0
                },
                {
                    "sent": "Very good that you.",
                    "label": 0
                },
                {
                    "sent": "Tell me back to your hybrid algorithm.",
                    "label": 0
                },
                {
                    "sent": "Not doing so well, yes does.",
                    "label": 0
                },
                {
                    "sent": "Would you expect if you kind of really pushed to one end of the spectrum or something?",
                    "label": 0
                },
                {
                    "sent": "No, I think we need a different algorithm.",
                    "label": 0
                },
                {
                    "sent": "This was a sort of a hack to get abound.",
                    "label": 0
                },
                {
                    "sent": "In my view, this thing that we used.",
                    "label": 0
                },
                {
                    "sent": "It makes sense, but.",
                    "label": 0
                },
                {
                    "sent": "We're not basically coming out with a new algorithm, we're just using the things we have a commanding them in a sort of a tricky way in order to get the bound.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you have a data set that has these tricky behaviors that bounds going to be a good bound, right?",
                    "label": 0
                },
                {
                    "sent": "So in some senses that you haven't yet tested, we we.",
                    "label": 0
                },
                {
                    "sent": "We didn't test it, yes, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's my.",
                    "label": 0
                },
                {
                    "sent": "My guess is that this is not going to work.",
                    "label": 0
                },
                {
                    "sent": "This is basically to show that it's possible to obtain results of this flavor because before we didn't know and however right we don't, we don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think this is the right way to go.",
                    "label": 0
                },
                {
                    "sent": "I have another kind of method question which is.",
                    "label": 0
                },
                {
                    "sent": "You see these bounds and I'm in my own context, so would probably want to do kind of batch learning, but I don't really have the memory to store the batch, so the online is an excuse.",
                    "label": 0
                },
                {
                    "sent": "Absolutely yes, yes, and in that context I really don't care how many mistakes the thing makes early on in the process, unless you are in a kernel space.",
                    "label": 0
                },
                {
                    "sent": "Well, because those will become supports for.",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't have any space concern, if I'm looking at the method an online method as a method of approximating batch method OK, getting a quick handle on the solution right?",
                    "label": 0
                },
                {
                    "sent": "Then I've got a certain number of kernels that the ultimate cost function is going give me right?",
                    "label": 0
                },
                {
                    "sent": "So you know, there's a number of mistakes as they reduce aghbal in that sense.",
                    "label": 0
                },
                {
                    "sent": "I'm sure the online method might introduce more kernels than that absolutely right.",
                    "label": 0
                },
                {
                    "sent": "You know, even the best method introduces more, yeah?",
                    "label": 0
                },
                {
                    "sent": "So the point of the question is.",
                    "label": 0
                },
                {
                    "sent": "The things that one sees online learning methods is doing well is getting good quick solutions approximate solutions early in the learning process, and then if you run them over batches, they typically have much slower convergence than a batch method.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "Are you asking whether this is true or this is what we want to see in practice OK?",
                    "label": 0
                },
                {
                    "sent": "Well, that says about if you're trying to do bounds for these things is that probably shouldn't care too much about how many mistakes made early in the process.",
                    "label": 0
                },
                {
                    "sent": "What you should care about is how much we're reducing the error for later examples with your first few steps, OK, it depends on if you want to get a risk bound by means of an online algorithm.",
                    "label": 0
                },
                {
                    "sent": "A you will have.",
                    "label": 0
                },
                {
                    "sent": "You will have.",
                    "label": 0
                },
                {
                    "sent": "Well that still depends.",
                    "label": 0
                },
                {
                    "sent": "It still depends on you have a training set.",
                    "label": 0
                },
                {
                    "sent": "You do a pass on the training set, but it still depends on the ratio between mistakes and the sample size.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no way.",
                    "label": 0
                },
                {
                    "sent": "I mean, unless you are OK, if you're in a realizable setting is like if I made many mistakes, means that I'm really close to the target, but if I made any mistakes, made it, there's a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "How do I know?",
                    "label": 0
                },
                {
                    "sent": "I mean, what I'd like to see is something that says this algorithm may be lousy asymptotically, but it's really quick in the beginning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, I see what you mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think this might be a way of you want to select examples in order to make a lot of lots of mistakes at the beginning and be be OK later on and this makes sense if you can pick.",
                    "label": 0
                },
                {
                    "sent": "So if you're in a batch batch setting it makes sense.",
                    "label": 0
                },
                {
                    "sent": "So the paper I think the paper by a lot as an on this sub linear perception paper.",
                    "label": 0
                },
                {
                    "sent": "It does does something like that.",
                    "label": 0
                },
                {
                    "sent": "Learning yes, so they pick those nastiest examples in the pool.",
                    "label": 0
                },
                {
                    "sent": "OK, that's fine, but you need to look at your whole pool to do that, right?",
                    "label": 0
                },
                {
                    "sent": "But if you're in a batch case, yes.",
                    "label": 0
                },
                {
                    "sent": "The other sort of example I have in mind is these boosting methods.",
                    "label": 0
                },
                {
                    "sent": "Instead of doing a single step kind of optimize all the past weights.",
                    "label": 0
                },
                {
                    "sent": "At the same time, so they have very expensive optimization in each step, but they kind of reusing initial things and re optimizing their weights at each step, so they're kind of using examples more efficiently, and I'm not seeing kind of bound online learning types of bounds for these methods coming out.",
                    "label": 0
                },
                {
                    "sent": "No methods are inherently local, and that's their online learning bounds and not inherently locals.",
                    "label": 0
                },
                {
                    "sent": "The bus not but the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, so you know, OK, but you want the algorithm to do something local because you want to sort of go back and to introduce.",
                    "label": 0
                },
                {
                    "sent": "Each state, but for example, like the Adaboost, I'm willing to allow to do calculation with the previous examples in each step or something.",
                    "label": 0
                },
                {
                    "sent": "Um, I've seen things I've seen things in case.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you want to do shifting.",
                    "label": 0
                },
                {
                    "sent": "If you want to track if you want to track a moving moving target in the sense of a moving model, then you want to do something.",
                    "label": 0
                },
                {
                    "sent": "Basically because we have a sort of maybe a decaying window on the past the support vectors so you are at each time you are.",
                    "label": 0
                },
                {
                    "sent": "Removing weight from the old vectors.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "There are ways of guaranteeing that this can be done in a way.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a decomposition of the improvement you can make by adjusting existing weights and the improvement you make by adding a new support vector.",
                    "label": 0
                },
                {
                    "sent": "I know, yeah.",
                    "label": 0
                },
                {
                    "sent": "These paradoxically quickly batch on the examples.",
                    "label": 0
                },
                {
                    "sent": "It's more like what is going to be the next feature that I can.",
                    "label": 0
                },
                {
                    "sent": "It's it's kind of online on the addition of the features, which is the next week, but that's boosting by filtering, boosting by filtering.",
                    "label": 0
                },
                {
                    "sent": "I think there they have.",
                    "label": 0
                },
                {
                    "sent": "The philosophy expensive.",
                    "label": 0
                },
                {
                    "sent": "No problem, so sparsely yeah.",
                    "label": 0
                },
                {
                    "sent": "I wanted to ask a question about along this line.",
                    "label": 0
                },
                {
                    "sent": "I mean if you were using data streaming.",
                    "label": 0
                },
                {
                    "sent": "You could have saved set of examples.",
                    "label": 0
                },
                {
                    "sent": "OK, and this in the compatible way with the amount of data and then label them at home to the speaker.",
                    "label": 0
                },
                {
                    "sent": "Late like a sort of mini batch thing.",
                    "label": 0
                },
                {
                    "sent": "Doing data streaming, you would do the robot OK. Would keep a few images which are representative of the trajectory and then those examples those prototypes.",
                    "label": 0
                },
                {
                    "sent": "Could be labeled at home.",
                    "label": 0
                },
                {
                    "sent": "OK if you introduce delays then it's just you have to take those into account in the bound so the delays will play a role in the bound, because then you're delaying your update.",
                    "label": 0
                },
                {
                    "sent": "See what I'm looking over anyway.",
                    "label": 0
                },
                {
                    "sent": "Was there a strong idea hypothesis at the beginning or the beginning?",
                    "label": 0
                },
                {
                    "sent": "Is this place Robin navigation for place recognition data?",
                    "label": 0
                },
                {
                    "sent": "In the norm, it's not.",
                    "label": 0
                },
                {
                    "sent": "There's never iid that, it's an arbitrary sequence of data.",
                    "label": 0
                },
                {
                    "sent": "I mentioned IID whenever I wanted to.",
                    "label": 0
                },
                {
                    "sent": "It's a deterministic process.",
                    "label": 0
                },
                {
                    "sent": "I mentioned idea whenever to get whenever I wanted to get this sort of a nice looking bound, but yes question.",
                    "label": 0
                },
                {
                    "sent": "This relates to my initial question.",
                    "label": 0
                },
                {
                    "sent": "You know this is a stream of this research stream about dictionary learning.",
                    "label": 0
                },
                {
                    "sent": "Trying to learn both representation and the dictionary with respect to this representation, and they have the similar.",
                    "label": 0
                },
                {
                    "sent": "Problem, but with the addition that they want to learn.",
                    "label": 0
                },
                {
                    "sent": "They want to learn the dictionary as well at the same time.",
                    "label": 0
                },
                {
                    "sent": "Which it sort of relates your kernel learning problem, but without the kernel space there.",
                    "label": 0
                },
                {
                    "sent": "Hitler spaces, which are not fixed it ahead of time.",
                    "label": 0
                },
                {
                    "sent": "So this is something that is, yeah.",
                    "label": 0
                },
                {
                    "sent": "I, I'm not sure there are online result, so you can you can you can stick in a new feature with the 00 wait and then let it go.",
                    "label": 0
                },
                {
                    "sent": "You can always do that for instance when we do statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "You can think that the language model, sorry, the translation table grows with time and you get more and more features get more and more pieces to work on.",
                    "label": 0
                },
                {
                    "sent": "This is specific properties over the dictionary, kind of sparsity, some kind of.",
                    "label": 0
                },
                {
                    "sent": "Bounded incoherent something like this?",
                    "label": 0
                },
                {
                    "sent": "Then why are you talking about the way of?",
                    "label": 0
                },
                {
                    "sent": "Seeking new features or you get you get new features from time to time and then you start using them.",
                    "label": 0
                },
                {
                    "sent": "Business function right at this point.",
                    "label": 0
                },
                {
                    "sent": "Can be anything, can be vectors in some space.",
                    "label": 0
                },
                {
                    "sent": "You want to learn.",
                    "label": 0
                },
                {
                    "sent": "Let's say K of them and.",
                    "label": 0
                },
                {
                    "sent": "Your representation will be with respect to this K vectors.",
                    "label": 0
                },
                {
                    "sent": "How is it?",
                    "label": 0
                },
                {
                    "sent": "How is this different from support vector learning?",
                    "label": 0
                },
                {
                    "sent": "Is because.",
                    "label": 0
                },
                {
                    "sent": "Your support that do not come from the data.",
                    "label": 0
                },
                {
                    "sent": "Support.com from the data OK for instance.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. More questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}