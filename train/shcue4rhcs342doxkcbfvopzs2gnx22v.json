{
    "id": "shcue4rhcs342doxkcbfvopzs2gnx22v",
    "title": "Bayesian methods for data Modelling",
    "info": {
        "author": [
            "Mike Tipping, Microsoft Research"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_tipping_bmd/",
    "segmentation": [
        [
            "Sarah.",
            "Well, we started today I guess is the day of the Bayesians.",
            "We're starting off with my chipping who are very lucky to have.",
            "I've known Mike since I sort of came to the field in 96 where when I started my PhD Aston University where Mike was doing a postdoc with Chris Bishop.",
            "One of the results in that post op was probabilistic interpretation of PCA, which is without that internationally well known and approved foundation for a lot of new research in the areas of dimensionality reduction and mixture models of PCA so and so forth, alot of which Mike has made Bayesian variants on PCA, he showed that he isn't just a one trick pony by after that while he was at Microsoft where he was for six years eight years, eight years.",
            "Coming up with a field, the field of sparse Bayesian learning.",
            "Which is taken off in a big way in the signal processing area and the model of Mike developer that is the relevant vector machine as well as that I also happen to know because he taught me on MFC course while I was at Aston, which I took as part of a PhD program that he's an excellent tutorial speaker.",
            "So he was an obvious choice to come here and give you an introduction to Bayesian methods for data modeling.",
            "Further do over to Mike.",
            "Thank you very much now for excessively kind introduction.",
            "Good morning everyone.",
            "Please have turned up.",
            "My sympathy."
        ],
        [
            "So this is what I want to talk about today.",
            "Let me give you a brief outline.",
            "So first let me say, I don't think we'll get through all this in the first half, so I've split this two hour slot into two halves, hopefully.",
            "But I don't think we'll get through it all, but will go as far as we can.",
            "So I want to start off with a sort of simple prologue if you'd like to set the scene about Bayesian inference, and while we might want to do it.",
            "So consider this.",
            "The sort of the sequence you get.",
            "The opening of a Bond film that comes before the credits, and it won't be quite as exciting.",
            "So I just want to motivate what we're going to do then.",
            "And then I'll go back and start looking at a very simple problem, and then we'll sort of build up a treatment of this sort of linear simple linear regression problem and will start looking about how we might tackle it in a Bayesian way.",
            "And the idea of this talk is that eventually we get start off looking Occam's razor, and we get back to it at the end.",
            "But we may have to push this into the second lecture."
        ],
        [
            "So let's move on to the program.",
            "So um, start off with some minor interaction who's heard of the idea of Occam's razor?",
            "So most but not everyone.",
            "So some were very brief history.",
            "There was a theologian and philosopher William of Ockham, which is a village in Surry you in the 14th century is supposed to have said something to the effect of this.",
            "Which I won't test my Latin pronunciation, but basically he said when looking at solutions for problems that entities should not be multiplied unnecessarily.",
            "The informal translation of that could well just be don't over complicate things.",
            "So his he was thinking very much in a theological and philosophical context at the time.",
            "I'm sure you're familiar with some of the very convoluted explanations dictated by religion and theology for certain physical phenomena such as the earth going around the sun.",
            "And he was basically talking about really, we should be looking for solutions that fit the data, yet which are quite simple and in a modern context this is still very much valid philosophy if we're trying to solve data modeling problems and we have a number of potential solutions, and ideally we believe we should be choosing the one that's the simplest.",
            "We see no reason to overcomplicate.",
            "Gets a little more difficult.",
            "Let me start thinking about degree of solution.",
            "How good is the solution?",
            "It's not necessarily the case that we have, it's either right or it's wrong.",
            "Sometimes we have sort of degrees of accuracy of the model.",
            "So in Monday data modeling what we're interested in doing.",
            "When we want to find good solutions to problems, is somehow trading off the.",
            "Accuracy of a model with its complexity, and we'd like to do that in a well principled and effective practically effective way.",
            "And, well, I hope showing this talk is that the ideas of Bayesian inference effectively embody this.",
            "So let me start."
        ],
        [
            "We're just.",
            "Kind of contrived example.",
            "So the fairly simplistic, but it's intended to show the underlying mechanism at work.",
            "So imagine we have a binary communication system, have two symbols, bits zero and one.",
            "So we have two different bits.",
            "We don't have a way of sending messages.",
            "It's going to start off with taking an arbitrary length string of bits zeros.",
            "And then anywhere in that string at any particular place, we're allowed to place one of these four symbols.",
            "So we have an 8 bit, 4 bit, 2 bit, and a single bit symbol.",
            "And we can construct a message according to the system by placing those anywhere.",
            "Even overlapping, and then we simply or them.",
            "OK, knowledge the fact that in our system.",
            "We may have transmission errors, which are will just say can be independent in version of bits.",
            "So imagine then.",
            "But we received this particular sequence so 10 bit sequence.",
            "And we're interested in knowing what's the best decoding.",
            "I put best in inverted commas, because obviously that's a very qualitative term and we might need to be a little more formal on how we define that.",
            "But for now, we want to ask the question.",
            "What was the original set of symbols that generated the sequence we received?",
            "So I think this.",
            "It's quite an appealing, intuitive one.",
            "Obviously this is generated by a four bit and the two bit symbol question is, is that the most likely or the best decoding in any sense?",
            "Can we show?"
        ],
        [
            "For me.",
            "So let's have a look.",
            "That's impossible decodings.",
            "So I picked four.",
            "Obviously there are.",
            "You can probably cook up hundreds.",
            "If you're that bothered, but I pick four, that's sort of an obvious appeal to them in a certain sense.",
            "So model one is simply we place 6 bits and required positions as we need them.",
            "So if you like that's the most.",
            "Obvious model, least subtle.",
            "Model 2 is that we place 3 two bit sequences.",
            "There where we need them, so that's a perfect decoding of the data.",
            "Model 3, which I hope appeals to us most intuitively, is a four bit similar 2 bit symbol.",
            "So those three models are all perfect decodings in our sequence.",
            "But also we might consider the fact as we said, there could be some noise, so let's just have a look at a model that requires some noise in the decoding or the assumptions.",
            "So this model just the seems a single 8 bit sequence.",
            "That's the most simplistic model in that sense, but it requires us to invert these two bits so assumed 2 bits of noise error.",
            "So we're just looking now.",
            "Some potential choices, three of which.",
            "Decay the sequence perfectly.",
            "An IV.",
            "It's the simplest model, but it doesn't actually fit the sequence precisely.",
            "We have to make an assumption about the noise.",
            "So the question is, which of those is the best?",
            "Well, if we look at it in a probabilistic way, we can actually show that decoding three, but one that appeals to us most intuitively.",
            "This one is actually the most probable.",
            "So let's just work that out."
        ],
        [
            "So what we want to do is calculate the probability.",
            "For any given model produced the observed sequence.",
            "And we do that with reference to all other possible sequences, and this is the key.",
            "All other possible sequences that that model could also produce.",
            "So if we assume.",
            "The but the sequence was a result of Model 1, which is 6 single bit sequences.",
            "And our original knowledge of the system is that bits can be placed anywhere.",
            "Can overlap.",
            "Then we have to consider.",
            "It's a perfectly good decoding of this sequence, but how many other sequences could this model have generated?",
            "And the answer is of course.",
            "We can place each bit in one of 10 positions.",
            "They're allowed to overlap.",
            "So there are 10 to the six possible sequences.",
            "This model could have generated.",
            "And the question is, how many of those sequences?",
            "Look like or match exactly the sequence we observed and that gives us a measure of the probability that this model.",
            "Generates or explains this sequence.",
            "Because we can interchange bits.",
            "We can permute these bit ones without changing our observed sequence.",
            "Hope you can see that there are going to be 6 factorial possible.",
            "Sequence is generated from this model, but will match.",
            "So the probability of this model probability of the data given the model.",
            "Is 6 factorial over 10 to the 6th, which is 720 / 1,000,000.",
            "So the actual probability is point North, North N 7 to 4 decimal places.",
            "Everyone OK with that.",
            "OK.",
            "So if we were to actually.",
            "Run the simulation and sample from that very model I just described to you.",
            "We should find this is the proportion that actually looked like that.",
            "I'll show you an example if you're not convinced.",
            "So just following up for that, then Model 232 bit sequences.",
            "We have to place three symbols.",
            "In one of any nine positions, so there's nine Q possibilities.",
            "We can permute them in three factorial ways, so we have 6 /, 729 point Dublin, or Type 2.",
            "Almost intuitively appealing sequence.",
            "Seven ways of placing this sorry.",
            "Yes, seven ways of placing this.",
            "Nine ways of placing this.",
            "But only one of these generated sequences is the precise hit that we want, so this is a less complex model, because it only there's only one in its set that matches, but the set is only 69.",
            "Sorry 60 three large, so the probability is 163rd, just point North 159.",
            "So this simplest model, because we consider.",
            "All the other possible sequences is more probable, so all three fit the data equally.",
            "We can't separate them in terms of how well they fit the data, but if we take account of all the other possible models, they could have matched.",
            "This one looks more probable.",
            "Now when we do data modeling course.",
            "We don't.",
            "Normally it's not your case that our model either fits the perfect data, fits the data perfectly, or it doesn't.",
            "You lose a degree of accuracy.",
            "Squared error, misclassification rate, something like that.",
            "So that's why I introduced the 4th model, which is where we look at a.",
            "And over simple decoding.",
            "Which assumes some form of error.",
            "So for this decoding, there's only three possible places I could place a symbol, so it's generating set was only has three elements in it.",
            "But it seemed 2 bits of error.",
            "And under I'm just going to seem very simple noise error which says I can place those two bits anywhere I convert any 2 bits, but I won't invert the same bit twice.",
            "So the possibility of different errors.",
            "I could have observed, given that there could be two is 10 choose two or 45.",
            "So the probability of this model is 1 of 3 * 45135.",
            "And that works out as point double nought 7 four.",
            "So.",
            "The oversimple model.",
            "It doesn't quite fit.",
            "The data is less probable than the relatively simple model, but does so you can see how this probability calculation is automatically imposed that radar.",
            "But Interestingly, they have a simple model is actually an order of magnitude more probable than the most complex model we have.",
            "So this probability calculation is automatically.",
            "If you like built in that Occam's razor tradeoff.",
            "Occam's razor trailer.",
            "It's assigned low probability to two complex and model.",
            "Because it had to take into account all the other sequences, all the other data about model could have accurately predicted.",
            "And it's also assigned a lower probability to an over simple model, and it's done that because it had to introduce a noise model in order to appropriately explain the data and that noise model.",
            "Meant that the space of all possible datasets it could have still observed again gets expanded because of that noise assumption.",
            "So I just want to convince in case you know, maybe you don't believe that.",
            "Or maybe don't move my calculations.",
            "I just have a quick simulation I can show.",
            "Well, we just hopefully empirically prove those numbers in the last slide, so I'm just going to do with a few lines of simple Matlab.",
            "You can keep this up yourself if you were so minded.",
            "Going to sample from those four models and we'll just verify these values, so this is just six bits placed at random.",
            "You can almost write the code in your head if you got your laptop, you probably don't know.",
            "This is 3 two bit symbols placed at random before on the two and the eight with two randomly flipped bits.",
            "And every time a symbol actually matches the target sequence, it'll flashing white and will count it up.",
            "So I'm just really simulating.",
            "Exactly what you might do.",
            "If you just wanted to prove that previous.",
            "Mathematics, so we just do 10,000 samples.",
            "And we'll just simply count how many times those generative models they simulated models match the target sequence.",
            "As you can see.",
            "If you ugly little more convinced Model 3 is much more probable out of 10,000 times hits the target sequence, it predicts it and explains it far more than the other models.",
            "The red lines are the theoretical values and actually they match for more than I would have expected.",
            "But Model 4, the noise, the over simple model that doesn't match the data with noise, still predicts the target sequence far more than the complicated Model 1.",
            "So I haven't sort of written down much in the way of probability distributions or really done any probabilistic manipulations, which sort of done everything by counting sequences.",
            "But this example is basically Bayesian inference in disguise, and the reason is is because we've calculated probabilities for our models that take into account.",
            "All the other.",
            "Sense of data that those models were capable of explaining.",
            "This is the essence of the way Bayesian inference implements Occam's razor, penalizes over complex models because they have to explain much many more datasets in this calculation.",
            "At the same time, it penalizes simple models because by the introduction of a noise or error model.",
            "They have to also explain a lot more data.",
            "Same everyone happy with that so far.",
            "So that's just setting the scene.",
            "So what I want to do for the rest of the lecture, which maybe will creep on into the next, is look at a very simple problem in data modeling, just modeling some univariate real value data.",
            "And hopefully at the end will arrive back at this point where we can look at.",
            "Potential models or sequence of models and see this effect at."
        ],
        [
            "Good game.",
            "So here is a simple modeling problem.",
            "So we have a set of supposedly mystery data we don't really know where it's come from.",
            "Imagine as we go along, but really you don't know anything about it, but the truth is for the purposes of this lecture.",
            "An many I've given for years, in fact probably generated this data about eight years ago.",
            "There are actually 15 samples generated from sine wave function.",
            "With semantic Gaussian noise, standard deviation nought .2.",
            "And just in terms of notation, usually called the inputs XR function, I'll call Y&R observations or sometimes will target variables are called T. So I've got 15 of these 15 inputs, 15 corresponding target variable."
        ],
        [
            "Now kind of models we're going to look at again.",
            "So 'cause we want to have some.",
            "Transparent analysis of what we're doing.",
            "We're going to give almost the simplest formal model, and those are models that are linear in the parameters.",
            "So our prediction is simply going to be a weighted sum of some fixed basis functions.",
            "So the basis functions could be linear, but typically will use some nonlinear functions.",
            "And in this example we use some Gaussian data data center basis functions.",
            "So project so will be nonlinear, but of course the key thing is it's called a linear model or linear in the parameters model, because the parameters only appear linearly.",
            "So in optical problem we've got 15 of these Gaussians, each centered on.",
            "In the X axis, on each of the data points.",
            "So our prediction model is a linearly weighted sum of those 15."
        ],
        [
            "So somehow we want to find a good model once, find good values, those underlying there's built-in weights.",
            "Anne.",
            "There's a range of approaches, but perhaps the most simple simply to say, let's try and fit the data by defining an error measure.",
            "I'm sure you're all familiar with this idea of least squares.",
            "So we define the error, measure the least squares error, which is simply the sum squared deviation of the data from our predictions.",
            "This is a popular measure for a number of reasons, but one obvious one is.",
            "Because the weights appear quadratically, we can minimize it analytically.",
            "And if we define the design matrix Phi as the matrix of the output of every basis function for every data point, we can have this analytic matrix vector expression that gives us a solution for the set of weights that minimizes."
        ],
        [
            "Sarah.",
            "Now of course we know in this case I think you're probably expecting we've got 15 basis functions.",
            "15 data points.",
            "We should be able to fit the data perfectly.",
            "And that's what we see.",
            "So that's the least squares fit using an RBF model with some Gaussian basis functions on our noisy data.",
            "In principle, this is the ideal fit because we know that the underlying generator was a sign function, so this is a sine wave.",
            "So.",
            "An obvious question is which of these models is actually better?",
            "So we get to a sort of philosophical point.",
            "It's important to realize.",
            "But without any further assumptions, without knowing where the data came from, we can't judge between these two models.",
            "We don't know.",
            "Whether the data really came from this more complex function with no noise.",
            "Or whether it came from a simple function with noise.",
            "We just can't tell.",
            "Without any further information, we can't make any further progress in modeling this data.",
            "We must accept.",
            "But we're going to have to introduce some prior knowledge or belief or understanding into this problem.",
            "And so we don't give up because we're fairly happy to make the assumption then the real world and the kind of data modeling problems that we tackle.",
            "The underlying functions that we expect to find.",
            "Are probably smooth and probably have some degree of noise, but let's concentrate on the smoothness aspect.",
            "Even if I hadn't told you that, we'd use assign way to generate this data.",
            "I think you probably would have found.",
            "This solution more appealing than this solution.",
            "That's partly it's sort of inherent Occam's razor at work, but it's also a reflection of the fact that we believe when modeling data that underlying.",
            "Functions that we're looking to try and capture in our models tend to have.",
            "Smoother character rather.",
            "Bing complex character.",
            "So fit.",
            "But bottom line is we have to introduce some kind of prior knowledge into the problem.",
            "And so in this kind of least squares.",
            "Regression or linear modeling?",
            "What we need to do is add something to the model such that we.",
            "More likely to dig out or extract functions like this and like this.",
            "So what is typically done?"
        ],
        [
            "Conventionally, is to do something called regularization.",
            "So what we want to do is add a penalty term that penalizes.",
            "There is over complex functions.",
            "A very convenient way to do this is to add a regularization weight penalty which penalizes large weights because.",
            "More rapidly changing functions, more complex functions typically have larger weights, so we simply add to our least squares error this weight penalty, which is a sum of squares of the weights.",
            "And one reason why this penalty is quite nice.",
            "Is that?",
            "It gives us an analytic estimate for what I'll call the penalized least squares value for the weights.",
            "And we notice how this term.",
            "Containing the hyperparameter, Lambda enters into the.",
            "Solution.",
            "So if we set Lambda very high.",
            "Then the white term dominates the overall term and we forced the weights three very small.",
            "So I think you can see how if we set Lambda high here.",
            "This weight vector will be pushed towards there."
        ],
        [
            "So let's just have a look at some potential values of Lambda.",
            "So there's our data.",
            "For set Lambda to a very small value, say roughly 0 here, then it's almost equivalent to not having that regularization term.",
            "And this almost fits the data perfectly.",
            "For set Lambda quite high, I'm imposing a lot of penalty.",
            "And my fit to the data is very poor.",
            "I have effectively over smooth or oversimplified function.",
            "Somewhere in between there's going to be a value of Lambda that gives us an intuitively appealing result.",
            "And that's this value here.",
            "So I'm not saying this is the best value, but this is just a value.",
            "But gives a reasonable answer.",
            "So somewhere out there, there's a decent value of Lambda.",
            "The question is, how can we find it for our particular modeling problem?",
            "So here we have to sort of acknowledge that we're going to require within this framework I'm presenting.",
            "Some extra help.",
            "And that help is going to come by the mechanism of a separate validation set.",
            "So imagine we're given another 15 samples of data from this data set.",
            "And we can use that to validate our choice of Lambda by looking at the."
        ],
        [
            "Error the model would make on this new data, so that's what we do here.",
            "So the green points are new data points.",
            "And all three models are simply checked against the validation set, and sure enough, the oversimple and the overcomplex model give a worse error then.",
            "The market looks about right.",
            "So we sort of now have a mechanism system if you like for fitting our model in that we will use.",
            "The validation set will fit the weights during using least squares and will use the validation set to determine the hyperparameter Lambda.",
            "But typically, of course, won't just pick three values.",
            "What we'll do in a reasonable practical experiment will try a lot of Lambda values, and then we'll plot the results."
        ],
        [
            "So here's the result of that experiment.",
            "So along the bottom I have Lambda or log of Lambda.",
            "It's quite natural actually to take logarithms of variance parameters or scale parameters and Landers.",
            "An example of that.",
            "Errors plotted on the vertical axis, and I'm plotting one obvious thing I'm plotting is the least squares well.",
            "And I should say, yes, that's the least squares error fit to the training data shown in black.",
            "So obviously when Lambda is 0 this end there is no regularization fits the data perfectly as we increase Lambda.",
            "Our model gets more simplified and fit to the data deteriorates.",
            "However, if we measure the error of the model on the validation set, what we hope is this goes down.",
            "Interesting, Lee just kicks up there, which is presumably a numeric effect.",
            "But nevertheless, the validation error has a distinguished minimum.",
            "And what we hope that that minimum is a good fit to the underlying function.",
            "And in this case, because we generated it from sine wave, we can test our solution against the true noise free underlying function.",
            "So this test error is simply the error of the model calculated with respect to the true underlying sign.",
            "So in the real world we type, but this minimum would be close to the minimum of the test error.",
            "In Lambda, Space is a little far little further away, but this is very flat Valley if you like and the error value of that is not too far away from that, so it's not too bad.",
            "It's come up with a reasonable answer.",
            "So we'll come back to this graph, possibly by the end of the lecture.",
            "Um?",
            "When we look at it in a Bayesian from a Bayesian perspective.",
            "But before we get there, let's just rewind to the start of this regression problem again, and let's look at how we might tackle it from."
        ],
        [
            "Asian perspective.",
            "So.",
            "Let me just start by defining the sort of the basic principle of Bayesian inference.",
            "So the idea is that we have to specify.",
            "Probability distributions, normally called priors.",
            "Over every variable in the model.",
            "So that includes things such as the observations themselves, which it seems intuitive to model probabilistically, 'cause you can think of those as being stochastic realizations from some underlying random process, so it can be.",
            "Noise might specify a noise model, or if it's a classification of pattern recognition problem, it would be an expression of the uncertainty of observations, because classes overlap, so that usually makes intuitive sense.",
            "People don't see that as controversial.",
            "Slightly more challenging to the mind is the idea of placing probability distributions over parameters in the model.",
            "So we say before seeing the data we placed prior probability distributions in principle over all the parameters that we have in our model.",
            "And in this case, rather than thinking of probability as summary of the outcomes of a random process, probability is interpreted as.",
            "Expressing our degree of belief in.",
            "The values that parameter should take before we start the modeling process.",
            "One advantage of that seems like maybe.",
            "Slightly complicated notion.",
            "Is there as I show, we can actually sample from these priors and be actually quite informative and tell us something that other paradigms to modeling data don't necessarily do.",
            "It will come to that.",
            "Final elements we might actually want to put probabilities are on the model itself, so in principle.",
            "We can actually put some probability distribution on the type of the model.",
            "The structure is choice of basis set or even parameters within the basis set.",
            "For example, the width of those Gaussians.",
            "So will look at this in principle beginning of the next lecture.",
            "But for now we'll just look at how prize on parameters work.",
            "So what we do is we've seen our data.",
            "We take these prior distributions and we convert them into posterior distributions using Bayes rule.",
            "So we calculate.",
            "Probabilities of these conditioned purely on the observed data.",
            "And the crucial aspect to Bayesian inference, and to get Occam's Razor working will see is that we need to integrate out variables which are not directly of interest.",
            "So in other words, all our parameters.",
            "Unless we actually want to predict parameters, we need to get rid of them.",
            "So this notion of integrating out is a little bit involved and one can spend several slides explaining how this works, and indeed I did produce some, but I've cut those out.",
            "So what I'll do is I'll show this idea this notion by one or two examples as we go through.",
            "But I have to point out at this stage.",
            "But a lot of these integration procedures turn out to be analytically intractable, and that's quite unfortunate.",
            "I think if this wasn't the case.",
            "Everyone in the world will be Bayesian and using Bayesian procedures.",
            "But this actually complicates matters quite considerably.",
            "In most models.",
            "Most work in Bayesian inference is arguably all about using and applying approximation methods for calculating these integrals.",
            "But the reason we put up with this problem is because there are some really nice benefits and features of the Bayesian approach.",
            "The key one I'm going to focus on is this automatic implementation of Occam's razor.",
            "So let's stop talking about philosophy and principles.",
            "And let's go back to look at our data."
        ],
        [
            "So let's consider how we model it.",
            "So we start off with a very simple likelihood model.",
            "This is our model of our observations, so we see observations equal or underlying function plus noise, and we place a probability distribution over that noise.",
            "So this is our modeling assumption over what noise we might think we might see.",
            "And we simply say we expect mean zero noise with some variance as yet unknown Sigma squared.",
            "We assume all the data is seen independently.",
            "The probability of observing the data given the current weights and noise.",
            "Is.",
            "Given by the product of the individual probabilities, and that's simply the probable product of these Gaussians centered on the model.",
            "So if I simply plug back into there.",
            "This is what I'll get.",
            "So this is the likelihood of the common term in modeling for the data.",
            "Now, can you see hopefully?",
            "But if I try maximized this I try and make the data look most probable.",
            "By changing the weights will consider Sigma squared to be fixed I should say.",
            "So if we look at maximizing this, doing maximum likelihood with respect to the parameters W. If I maximize this, I get the same result as least squares.",
            "I can see that because by maximize this is the same as maximizing the logarithm of it.",
            "If I maximize the log of a product, I'll get a sum of logs of this.",
            "An log of this obviously gets rid of that, and that will give we minus.",
            "Some of the minor squared error, so if I minimize that I can take out that - so maximum likelihood is equal to least squares under this assumption that all comes from the assumption of a Gaussian noise.",
            "So we sort of framed it equivalently now to least squares."
        ],
        [
            "But the interesting thing, of course, is the prior distribution.",
            "So as I said before, we have to specify a prior which sort of encapsulates our belief in what values parameters should take before we start modeling.",
            "Now you might see there's a lot of think there's a lot of voodoo in how you specify the prior, and specifying prize is sort of a key part, and it's very important there's a lot of expert knowledge in it.",
            "Well, actually will see another example.",
            "But we can assume a perfectly flat prior in many cases and still get some really interesting and powerful results out.",
            "So.",
            "Occam's Razor actually will still work for you, even in cases where you assume a flat prior ever in this case.",
            "A flat pie wouldn't be appropriate because we're actually looking to find smoother functions.",
            "So conventional choice is 0 mean Gaussian.",
            "So instead of penalizing large weights as we did in the more conventional treatment.",
            "We're actually going to make small weights more probable.",
            "So we specify Gaussian distribution over the weights that makes each weight individual weight, so it's independent for each weight.",
            "But they share this hyperparameter Alpha, which is an inverse variance parameter.",
            "So if I make Alpha very large.",
            "I make that wait prior very tight around 0.",
            "So if Lambda is large, with apriori very certain, the weights are small.",
            "How to set lambdas are very small value and Gaussian becomes very flat.",
            "Yeah, it's almost uniform and uninformative.",
            "So we've sort of now.",
            "Got an analogue for the regularization hyperparameter in this Bayesian treatment, in that we now have this inverse variance hyperparameter, we need to consider.",
            "Well, let's consider."
        ],
        [
            "How we do inference?",
            "So finally Bayes rule appears.",
            "So we take that likelihood function.",
            "We take the prior and we combine it.",
            "In Bayes rule.",
            "To give us the posterior distribution.",
            "Over the parameters.",
            "Conditioned on the data.",
            "So this is a I've written this here because Bayes rule often appears in this form in Bayesian calculations is typically the multiple of a likelihood, which is the probability of the data condition on some parameters times the prior over those parameters over normalizing factor and the key point about the normalizing factor is this term doesn't depend on.",
            "The whites of the parameters W. So this is independent of W. So the posterior here.",
            "Because this is Gaussian and this is Gaussian, so happens that the posterior is also Gaussian.",
            "It has a mean given by.",
            "This term here, but it also has a measure of spread, so it has a measure of uncertainty, the covariance matrix.",
            "Which is kind of an expression of how unsure the model is about the values of the parameters.",
            "Having seen the data.",
            "And the idea is, and what we'll see through practice.",
            "For example, is that the more data we see intuitively as you would expect?",
            "The smaller the narrower this covariance matrix gets.",
            "Everyone OK so far with this.",
            "I've got a little refresher slide on the rules of probability, if anyone.",
            "Daz acknowledged that they would like to see it.",
            "Right, let me just do that then.",
            "So this is maybe a good point."
        ],
        [
            "To remind, remind him refresh people about the basic rules of probability.",
            "This is all you need to know.",
            "Fitting manipulations in Bayesian inference, or indeed a lot of other probability problems, so the product rule of probability.",
            "22 variables with joint distribution of those variables.",
            "Is the condition of 1 on the other?",
            "Times the marginal of the other.",
            "And because this these two terms can be reversed, there's a symmetric.",
            "Form, which is simply the swap of the two variables.",
            "So typically Bayesian inference so will be will be more than two variables and will be keeping some variables on the right hand side.",
            "If you like purely well we.",
            "Will be retaining them as conditioning variables?",
            "We would say so quite often we have something of this form which is a more general form of the product rule.",
            "Well, we have the joint distribution of.",
            "A&B, conditioned on a particular value of C. And we simply all we do in this case is.",
            "This would see retained on the right of the conditioning.",
            "So this is just this, but we condition everything on C. And Bayes rule actually comes simply from rearranging the product rule.",
            "So we could just simply rearrange those, but more generally will just set that equal to that rearrange and we get sort of a general form of Bayes rule, where we convert the probability be given a ANSI into the probability of a given B&C.",
            "So C remains always on the right of the conditioning.",
            "So that's the product rule in Bayes rule, which comes out of it.",
            "Another important rule is just what will be called the sum rule in probability, but in over continuous spaces.",
            "We call it the integral rule.",
            "And that simply says that the marginal probability of B and again we generalize by conditioning on C. Is simply the integral over the joint distribution.",
            "Integrating over variable A, so this we would call.",
            "Integrating out a.",
            "So the probability is the probability of the joint of AMB integrating out a.",
            "So this is the integrating out procedure.",
            "So these are really all the manipulations you need.",
            "A lot of the difficulty comes in performing this calculation.",
            "Oh, I'm just also point out.",
            "But this term, as you can see, it appears as the normalizing constants in Bayes rule, and that's typically what happens.",
            "Will find that the normalizing constant is actually the integral of the.",
            "Numerator.",
            "And that's what will often stop us from calculating these posterior distributions via Bayes rule because we can't calculate this normalizing factor because that requires an integral."
        ],
        [
            "Right?",
            "So that was Bayes rule.",
            "So the expression I just showed on that slide for example.",
            "Sigma squared and Alpha are examples of that C variable that we kept on the right hand side and retained for conditioning.",
            "Now, of course, you're probably thinking we should be Bayesian about those, and we will be later.",
            "But for now we're just going to consider them as fixed quantities to be conditioned on."
        ],
        [
            "So now we've got this posterior distribution.",
            "What do we do with that?",
            "Well, here's what we probably shouldn't do with it, but this gets done a lot, and there is often.",
            "Adding problems there is some value in doing this, but this isn't really being Bayesian and this is the idea of simply taking that posterior distribution.",
            "And sticking with just its most probable value.",
            "So this is the.",
            "Maximum a posteriori estimate.",
            "Which is simply the most probable value of the parameters under the posterior.",
            "Now for Gaussian, as you will know, the maximum is equal to the mean, and we had the mean on the previous slide, just re expressing it here.",
            "And you probably noticed already, but the mean or the mode of this distribution bore a strong resemblance to the penalized least squared solution.",
            "In fact, the two of the same were just a simple reparameterization.",
            "Solandri equals Sigma squared Alpha.",
            "These solutions are effectively the same.",
            "So far we've got through quite a long way in this lecture, and then we've done is recovered.",
            "Exactly what conventional methods do?",
            "But the key to Bayesian methods is in marginalization.",
            "Before I show you that, I just want to show you.",
            "A brief example of.",
            "This inference."
        ],
        [
            "Work.",
            "So over to matter.",
            "Remember when I called it?",
            "Yeah, that's the one.",
            "So.",
            "We're just going to perform.",
            "Slides showing how Bayesian inference works will just do it in a simulation.",
            "Here running under matlab.",
            "So.",
            "You recognize that data.",
            "Very interesting data.",
            "I'm just demonstrating underneath it so you have a feeling for the problem.",
            "What those Gaussian basis functions look like.",
            "So here 15 basis functions.",
            "One sensor on each data point.",
            "I've highlighted to those basis functions which are.",
            "Basic functions 10 and 11 counting from the left.",
            "And I'm looking.",
            "I'm going to look at the prior and subsequently the posterior distribution over those basis functions.",
            "So here I'm plotting the prior distribution.",
            "Overweights 10 and 11.",
            "Obviously it be nice to visualize all 15 weights, but.",
            "Not particularly visualizing 15 dimensional spaces, so I'm just going to focus on these two.",
            "So this is the Gaussian prior distribution consoles of it.",
            "For these two basic functions.",
            "Something I referred to earlier, I think is quite valid point.",
            "Worth making that Bayesian methods is that we can by specifying everything probabilistically in principle.",
            "Depending on the precise where you've specified the distributions, you should be able to sample from them.",
            "So this means if you sample from your model, you should be able to produce samples by your likelihood function of actual data you may see.",
            "And that in principle could be a good guide to telling you whether your model assumptions are good or not.",
            "So here are some samples from the prior distribution.",
            "So by that I mean I'm taking not just these two weights, but all 15 weights, something a Gaussian.",
            "According to that prior plugging into the model function Y and evaluating it will respect those basis functions and these are samples.",
            "In this univariate case it's very simple just to visualize what those look like, so these are noise free.",
            "I should say I could have noise.",
            "But under this model I've kept.",
            "The noise variance fixed in this example.",
            "But I'm just showing the underlying functions.",
            "So these are samples from the prior and this is something I don't think you can get out of.",
            "For example, a regularization approach so you can impose a squared weight penalty, but.",
            "It's hard to see how to get a handle on what that is doing to your model.",
            "It's hard to understand perhaps what kind of functions are likely to come out with low error from that model, whereas the probabilistic expression of.",
            "That weight penalty if you like via prior distribution, is far more intuitively understandable 'cause we can sample from it.",
            "We can actually look at what it produces.",
            "So what I'm going to do now?",
            "I'm going to actually start observing the data and calculating posterior distributions, so I'll start plotting the posterior in this graph.",
            "And because we observe the data independently, we can look at the data one data point at a time.",
            "So we'll look at this data point, but why don't I do that?",
            "And we'll calculate the posterior as we go along.",
            "So I don't have to look at all the data at one time.",
            "I can just click on.",
            "I'll say the first 4.",
            "Click on the 1st four data points and will calculate the posterior distribution over the weights.",
            "Condition purely on those four observations.",
            "And the idea is, if I because of independence, if I look at 15 data points, one at a time, the results I get at the end is exactly the same as if I looked at all 15 data points at once.",
            "So what do we see when we're looking at the posterior distribution here over these two weights?",
            "And it doesn't appear to change from the prior, and hopefully that makes sense to you, because the four data points it's seen all four outside the support outside the region where these basis functions are non 0.",
            "Or nearly zero.",
            "So seeing this for days points tells you next to nothing about the weights coming from these basis functions, because the output these basis functions is pretty much 0.",
            "It does tell you something about the basis functions at this end.",
            "And although we don't observe it here, we can see that in this graph here, which shows samples now from the posterior distribution.",
            "So these samples from the prior haven't changed, but the posterior distribution.",
            "This is samples from all 15 weights of these red lines.",
            "And they now you can see, are now more concentrated around the data at this end, where we've seen examples.",
            "But at this end they're still pretty diffuse, like the prior effectively.",
            "This green line I plotted is the posterior mean or the map also.",
            "Predictor.",
            "So this is plugging in the weights that are the mode of the posterior distribution over all 15 parameters.",
            "So of course, what you would expect now if I start clicking a bit more across.",
            "You would expect that the posterior distribution over weights 1011 should begin to change.",
            "Sure enough, that's what we begin to see.",
            "So the distribution is still quite diffuse.",
            "Let's begin to move, and you can begin to see a correlation.",
            "And of course, you begin to say the samples.",
            "Well, the green mean tends to follow the data and the samples are all more closely packed around the data from the posterior, but they're still diffuse at this end, so I'll just.",
            "Absorb, as we sometimes say, those last few data points.",
            "And there we go.",
            "So that sort of procedure in principle finished.",
            "We've seen all 15 data points.",
            "Here is the posterior distribution.",
            "I've just also marked with across the map estimate, which is the mean of that distribution.",
            "So hopefully now the posterior samples make some sense.",
            "Posterior mean roughly tracks the data in the samples and now.",
            "Roughly in the region of the observed data.",
            "For something else to notice is that this probability distribution posterior is still quite diffuse.",
            "It still seems it's definitely more compact than the prior, but it's still quite a bit of uncertainty there.",
            "Also notice that slightly anticorrelated between those two data points, and that's what you would expect for two basis functions that are close together.",
            "If you think about it.",
            "'cause they're very similar outputs, so the weights coming from them.",
            "Probably sum to roughly a constant value because these numbers are roughly the same.",
            "So one second.",
            "So if 2 numbers add up at the same then one is something minus the other.",
            "So they should be anticorrelated without question.",
            "Yes.",
            "One final thing I can do.",
            "Maybe we'll stop at this point.",
            "Because we've generated this data artificially, I can actually introduce another 100 examples at the click of a button.",
            "So magically we have another 100 examples.",
            "And you see, hopefully what makes a little bit of intuitive sense.",
            "This posterior distribution, now we've seen a lot more data, becomes a lot more compact where a lot more certain about values of the weights, and you should also notice that the samples from the posterior also get tighter, so I'll just do that once more.",
            "We could keep going and this will get closer and closer to a Delta distribution, and these samples will get more more tightly packed around the mean.",
            "So again, yes.",
            "These here.",
            "These yes, these are weighted Gaussians.",
            "Yes please.",
            "Now the red ones.",
            "OK, so the red ones here.",
            "Or samples from this distribution.",
            "If overall 15 weights, so we have a Gaussian distribution.",
            "The posterior overall 15 weights.",
            "We can sample from that, which gives us 15 weight values.",
            "We plug that in to our function Y.",
            "So that effectively weights these basis functions, and these red curves are those functions evaluated with the sampled weights.",
            "So if we sample take multiple samples will get different curves, red curves.",
            "The green curve is simply the value of the weights that corresponds to the mode of the posterior or the mean of the posterior.",
            "So that's the map predictor.",
            "The posterior mean predictor.",
            "We would probably say.",
            "So I think I'm going to probably, yes, definitely pause here before I do.",
            "Does anyone have any questions on what we've seen so far?"
        ],
        [
            "Sorry.",
            "So this is the prior distribution specification.",
            "Anne.",
            "So once we've agreed.",
            "But before we can tackle modeling the data, we have to make an assumption about the kind of functions in the real world we expect to find.",
            "We need to build that in to our Bayesian model.",
            "And in the same way in the.",
            "In the regularization case, we penalize large values of weights because they lead to complex functions.",
            "A way of building that into a Bayesian model is to make small weights or smooth functions more probable.",
            "And so really, we want a distribution that gives more probability to wait near 0 small probability to weights further away.",
            "So first modeling assumption is to place that distribution over the parameters in the model.",
            "Now we could choose arbitrary distributions arguably, but there are.",
            "Perhaps good for this optical, in particular good computational reasons for choosing a distribution that's amenable to some calculations, and the Gaussian distribution is a good example of that.",
            "Spell re specify in advance.",
            "That we believe.",
            "The underlying models that we want to or the underlying functions that we're trying to model.",
            "Will typically have.",
            "Our belief is they will typically have smaller weights and this is how in the Bayesian context.",
            "We would impose that belief upon the problem, and we do that by the mechanism of specifying probability distribution over parameters.",
            "In this case, the weights that says in advance we believe.",
            "The kind of functions we're likely to find, we ones that have smaller weights.",
            "So we make a kind of what seems like an arbitrary choice of a Gaussian distribution, mean zero Gaussian that makes small weights more probable, so that has some computational advantages.",
            "And also, as we saw in this case, it's a good distribution to use in the sense that it's analogous to the squared weight penalty used in regularization.",
            "And as I showed in the example, if you want to have an understanding.",
            "For how what this assumption actually implies to your model, you can sample from it.",
            "And that's what I showed in the bottom right picture.",
            "Another example samples of weights from this distribution plugged into the model, which gives me samples of the function Y.",
            "So that gives you an understanding if you're able to visualize it of what your modeling assumptions actually are.",
            "So if these functions look absolutely nothing like what you're expecting to find, there's a good sign that you might want to rethink your prior distribution.",
            "Yeah.",
            "If you got two weights, then that's a probability distribution over 2 dimensional space.",
            "So if you fish.",
            "Yes.",
            "Well, that's what some people do.",
            "That's not a Bayesian thing to do, as I will show in a few minutes time.",
            "But yes, that distribution, once you've seen the data, it reflects your knowledge about the problem.",
            "In that.",
            "And sure that distribution is a maximum value, but it also has a spread and also has some.",
            "It's an indication of that.",
            "Your knowledge is diffuse.",
            "There are other values of weights that are still probable under that distribution that might give reasonable answers, but nevertheless that distribution is the most probable value and one approach or might say a pragmatic approach in certain modeling scenarios would be to say, oh, I just want one particular model, so let's take the maximum under that distribution.",
            "So in certain problems that may work well.",
            "Show in a few minutes time that there is an advantage in not doing that and actually doing something else with that distribution, but that's looking ahead.",
            "Control.",
            "Call me back.",
            "Not quite sure I see your problem with your question.",
            "Are you worried about not having probabilities over Lambda and Alpha?",
            "Yes, well.",
            "In principle, yes.",
            "Well, I should have shown you.",
            "And I will.",
            "Later show.",
            "But yeah, this this is so far as I specified.",
            "I haven't been properly a Bayesian.",
            "I'm correctly adhered to my religion in that there's no way I should have left this Alpha on the right hand side, so I should also specify the prior distribution over Alpha.",
            "I'm not sure if that's quite what your question was getting out.",
            "And I should have also done the same with that noise variance that I left hanging around that Sigma squared.",
            "So I should have put prize over everything.",
            "Time doing we're only doing it over the weights.",
            "In a few minutes time will.",
            "Think a bit more about what to do with this hyperparameter Alpha.",
            "Yep.",
            "Sign.",
            "And simple.",
            "All these things.",
            "Data points on a sine wave.",
            "Is the Bayesian method that you're going to the moon?",
            "Is it reasonable to assume that, given an arbitrary complex model of that data, it would eventually find?",
            "Something with sharp edges on it.",
            "Yeah, so you're absolutely right.",
            "If one just models with Gaussians.",
            "Data that underlyingly maybe has discontinuity's.",
            "Then that's.",
            "You're basing treatment.",
            "The problem won't find a good answer.",
            "The reason is because your prior is effectively misspecified with respect to the underlying functions you're looking for, but you can see that by sampling from it.",
            "For example, you would say, well, those are all lovely and smooth, but I know there are discontinuities, so I've seen some amazing work in time series analysis, for example by Bill Fitzgerald at Cambridge, where he was looking for nice move time sequences that suddenly change into different states an by specifying inappropriate prior.",
            "You can actually detect those change points by for example, in a very simplistic sense, one could augment this model with perhaps some discontinuous functions and do a Bayesian treatment over a mixture of Gaussians and discontinuous functions.",
            "And there's kind of.",
            "I've got an example at the very very end.",
            "Which I might get time to get to which kind of shows that it's an action, but it all comes down to your prior, more complex problems with more complex underlying functions.",
            "You need to think more carefully about specifying your prior.",
            "And typically, as you make your model more complicated.",
            "Calculations you need to do, particularly integrals, become troublesome.",
            "They're analytically intractable.",
            "You'll have to appeal to some probability, and now there are some numerical methods to solve them.",
            "And that's where your main problem will arise.",
            "But there's quite a lot of problems, including some you described that are amenable to linear models.",
            "Are you saying that?",
            "Separate different functions depending on your knowledge.",
            "In principle, yes, you could throw in a whole lot.",
            "Yeah, and you could use them all together and as long as you're correctly Bayesian.",
            "The model should pick out the right ones, but it depends on the data you observe.",
            "An to an extent, because if you if you don't observe data with high enough resolution, for example to detect some of the sharp features you're looking at the model in the world can help you.",
            "Bayesian.",
            "Graph.",
            "Prior point.",
            "Well, I just have no idea.",
            "No idea.",
            "You have to go through the whole process.",
            "Eventually, yes, I mean.",
            "In this case.",
            "In this case, I mean, you could imagine a model, it just keeps going on and on every prior.",
            "As a associated hyperparameter, specifying the hyper prior has another hyperparameter.",
            "In practice, what happens is you get to a point where you have a flat prior with no hyperparameters, so in fact.",
            "In this case I could specify Alpha from a gamma distribution with its own parameters, but equally I could specify Alpha coming from a log uniform distribution so I can stop there.",
            "I won't go on with this infinite regression.",
            "I can stop by simply specifying an uninformative Alpha, and usually in the model.",
            "You would get to a point.",
            "Maybe two or three layers deep where there is a appropriate distribution, as you say, but you can specify to be flat or noninformative which has no associated parameters.",
            "I think we should take a break there.",
            "5 minutes inappropriate so 5 minute break and reconvene."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sarah.",
                    "label": 0
                },
                {
                    "sent": "Well, we started today I guess is the day of the Bayesians.",
                    "label": 0
                },
                {
                    "sent": "We're starting off with my chipping who are very lucky to have.",
                    "label": 0
                },
                {
                    "sent": "I've known Mike since I sort of came to the field in 96 where when I started my PhD Aston University where Mike was doing a postdoc with Chris Bishop.",
                    "label": 0
                },
                {
                    "sent": "One of the results in that post op was probabilistic interpretation of PCA, which is without that internationally well known and approved foundation for a lot of new research in the areas of dimensionality reduction and mixture models of PCA so and so forth, alot of which Mike has made Bayesian variants on PCA, he showed that he isn't just a one trick pony by after that while he was at Microsoft where he was for six years eight years, eight years.",
                    "label": 0
                },
                {
                    "sent": "Coming up with a field, the field of sparse Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "Which is taken off in a big way in the signal processing area and the model of Mike developer that is the relevant vector machine as well as that I also happen to know because he taught me on MFC course while I was at Aston, which I took as part of a PhD program that he's an excellent tutorial speaker.",
                    "label": 0
                },
                {
                    "sent": "So he was an obvious choice to come here and give you an introduction to Bayesian methods for data modeling.",
                    "label": 1
                },
                {
                    "sent": "Further do over to Mike.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much now for excessively kind introduction.",
                    "label": 0
                },
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "Please have turned up.",
                    "label": 0
                },
                {
                    "sent": "My sympathy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what I want to talk about today.",
                    "label": 0
                },
                {
                    "sent": "Let me give you a brief outline.",
                    "label": 0
                },
                {
                    "sent": "So first let me say, I don't think we'll get through all this in the first half, so I've split this two hour slot into two halves, hopefully.",
                    "label": 0
                },
                {
                    "sent": "But I don't think we'll get through it all, but will go as far as we can.",
                    "label": 0
                },
                {
                    "sent": "So I want to start off with a sort of simple prologue if you'd like to set the scene about Bayesian inference, and while we might want to do it.",
                    "label": 0
                },
                {
                    "sent": "So consider this.",
                    "label": 0
                },
                {
                    "sent": "The sort of the sequence you get.",
                    "label": 0
                },
                {
                    "sent": "The opening of a Bond film that comes before the credits, and it won't be quite as exciting.",
                    "label": 0
                },
                {
                    "sent": "So I just want to motivate what we're going to do then.",
                    "label": 0
                },
                {
                    "sent": "And then I'll go back and start looking at a very simple problem, and then we'll sort of build up a treatment of this sort of linear simple linear regression problem and will start looking about how we might tackle it in a Bayesian way.",
                    "label": 1
                },
                {
                    "sent": "And the idea of this talk is that eventually we get start off looking Occam's razor, and we get back to it at the end.",
                    "label": 0
                },
                {
                    "sent": "But we may have to push this into the second lecture.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's move on to the program.",
                    "label": 0
                },
                {
                    "sent": "So um, start off with some minor interaction who's heard of the idea of Occam's razor?",
                    "label": 0
                },
                {
                    "sent": "So most but not everyone.",
                    "label": 0
                },
                {
                    "sent": "So some were very brief history.",
                    "label": 0
                },
                {
                    "sent": "There was a theologian and philosopher William of Ockham, which is a village in Surry you in the 14th century is supposed to have said something to the effect of this.",
                    "label": 1
                },
                {
                    "sent": "Which I won't test my Latin pronunciation, but basically he said when looking at solutions for problems that entities should not be multiplied unnecessarily.",
                    "label": 1
                },
                {
                    "sent": "The informal translation of that could well just be don't over complicate things.",
                    "label": 0
                },
                {
                    "sent": "So his he was thinking very much in a theological and philosophical context at the time.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you're familiar with some of the very convoluted explanations dictated by religion and theology for certain physical phenomena such as the earth going around the sun.",
                    "label": 0
                },
                {
                    "sent": "And he was basically talking about really, we should be looking for solutions that fit the data, yet which are quite simple and in a modern context this is still very much valid philosophy if we're trying to solve data modeling problems and we have a number of potential solutions, and ideally we believe we should be choosing the one that's the simplest.",
                    "label": 0
                },
                {
                    "sent": "We see no reason to overcomplicate.",
                    "label": 0
                },
                {
                    "sent": "Gets a little more difficult.",
                    "label": 0
                },
                {
                    "sent": "Let me start thinking about degree of solution.",
                    "label": 0
                },
                {
                    "sent": "How good is the solution?",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily the case that we have, it's either right or it's wrong.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we have sort of degrees of accuracy of the model.",
                    "label": 0
                },
                {
                    "sent": "So in Monday data modeling what we're interested in doing.",
                    "label": 0
                },
                {
                    "sent": "When we want to find good solutions to problems, is somehow trading off the.",
                    "label": 0
                },
                {
                    "sent": "Accuracy of a model with its complexity, and we'd like to do that in a well principled and effective practically effective way.",
                    "label": 0
                },
                {
                    "sent": "And, well, I hope showing this talk is that the ideas of Bayesian inference effectively embody this.",
                    "label": 0
                },
                {
                    "sent": "So let me start.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're just.",
                    "label": 0
                },
                {
                    "sent": "Kind of contrived example.",
                    "label": 0
                },
                {
                    "sent": "So the fairly simplistic, but it's intended to show the underlying mechanism at work.",
                    "label": 0
                },
                {
                    "sent": "So imagine we have a binary communication system, have two symbols, bits zero and one.",
                    "label": 1
                },
                {
                    "sent": "So we have two different bits.",
                    "label": 0
                },
                {
                    "sent": "We don't have a way of sending messages.",
                    "label": 1
                },
                {
                    "sent": "It's going to start off with taking an arbitrary length string of bits zeros.",
                    "label": 0
                },
                {
                    "sent": "And then anywhere in that string at any particular place, we're allowed to place one of these four symbols.",
                    "label": 0
                },
                {
                    "sent": "So we have an 8 bit, 4 bit, 2 bit, and a single bit symbol.",
                    "label": 0
                },
                {
                    "sent": "And we can construct a message according to the system by placing those anywhere.",
                    "label": 0
                },
                {
                    "sent": "Even overlapping, and then we simply or them.",
                    "label": 0
                },
                {
                    "sent": "OK, knowledge the fact that in our system.",
                    "label": 0
                },
                {
                    "sent": "We may have transmission errors, which are will just say can be independent in version of bits.",
                    "label": 0
                },
                {
                    "sent": "So imagine then.",
                    "label": 0
                },
                {
                    "sent": "But we received this particular sequence so 10 bit sequence.",
                    "label": 1
                },
                {
                    "sent": "And we're interested in knowing what's the best decoding.",
                    "label": 0
                },
                {
                    "sent": "I put best in inverted commas, because obviously that's a very qualitative term and we might need to be a little more formal on how we define that.",
                    "label": 0
                },
                {
                    "sent": "But for now, we want to ask the question.",
                    "label": 0
                },
                {
                    "sent": "What was the original set of symbols that generated the sequence we received?",
                    "label": 0
                },
                {
                    "sent": "So I think this.",
                    "label": 0
                },
                {
                    "sent": "It's quite an appealing, intuitive one.",
                    "label": 0
                },
                {
                    "sent": "Obviously this is generated by a four bit and the two bit symbol question is, is that the most likely or the best decoding in any sense?",
                    "label": 0
                },
                {
                    "sent": "Can we show?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For me.",
                    "label": 0
                },
                {
                    "sent": "So let's have a look.",
                    "label": 0
                },
                {
                    "sent": "That's impossible decodings.",
                    "label": 0
                },
                {
                    "sent": "So I picked four.",
                    "label": 0
                },
                {
                    "sent": "Obviously there are.",
                    "label": 0
                },
                {
                    "sent": "You can probably cook up hundreds.",
                    "label": 0
                },
                {
                    "sent": "If you're that bothered, but I pick four, that's sort of an obvious appeal to them in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "So model one is simply we place 6 bits and required positions as we need them.",
                    "label": 0
                },
                {
                    "sent": "So if you like that's the most.",
                    "label": 0
                },
                {
                    "sent": "Obvious model, least subtle.",
                    "label": 0
                },
                {
                    "sent": "Model 2 is that we place 3 two bit sequences.",
                    "label": 0
                },
                {
                    "sent": "There where we need them, so that's a perfect decoding of the data.",
                    "label": 0
                },
                {
                    "sent": "Model 3, which I hope appeals to us most intuitively, is a four bit similar 2 bit symbol.",
                    "label": 0
                },
                {
                    "sent": "So those three models are all perfect decodings in our sequence.",
                    "label": 0
                },
                {
                    "sent": "But also we might consider the fact as we said, there could be some noise, so let's just have a look at a model that requires some noise in the decoding or the assumptions.",
                    "label": 0
                },
                {
                    "sent": "So this model just the seems a single 8 bit sequence.",
                    "label": 0
                },
                {
                    "sent": "That's the most simplistic model in that sense, but it requires us to invert these two bits so assumed 2 bits of noise error.",
                    "label": 0
                },
                {
                    "sent": "So we're just looking now.",
                    "label": 0
                },
                {
                    "sent": "Some potential choices, three of which.",
                    "label": 0
                },
                {
                    "sent": "Decay the sequence perfectly.",
                    "label": 0
                },
                {
                    "sent": "An IV.",
                    "label": 0
                },
                {
                    "sent": "It's the simplest model, but it doesn't actually fit the sequence precisely.",
                    "label": 1
                },
                {
                    "sent": "We have to make an assumption about the noise.",
                    "label": 0
                },
                {
                    "sent": "So the question is, which of those is the best?",
                    "label": 0
                },
                {
                    "sent": "Well, if we look at it in a probabilistic way, we can actually show that decoding three, but one that appeals to us most intuitively.",
                    "label": 1
                },
                {
                    "sent": "This one is actually the most probable.",
                    "label": 0
                },
                {
                    "sent": "So let's just work that out.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do is calculate the probability.",
                    "label": 0
                },
                {
                    "sent": "For any given model produced the observed sequence.",
                    "label": 0
                },
                {
                    "sent": "And we do that with reference to all other possible sequences, and this is the key.",
                    "label": 1
                },
                {
                    "sent": "All other possible sequences that that model could also produce.",
                    "label": 0
                },
                {
                    "sent": "So if we assume.",
                    "label": 0
                },
                {
                    "sent": "The but the sequence was a result of Model 1, which is 6 single bit sequences.",
                    "label": 0
                },
                {
                    "sent": "And our original knowledge of the system is that bits can be placed anywhere.",
                    "label": 0
                },
                {
                    "sent": "Can overlap.",
                    "label": 0
                },
                {
                    "sent": "Then we have to consider.",
                    "label": 0
                },
                {
                    "sent": "It's a perfectly good decoding of this sequence, but how many other sequences could this model have generated?",
                    "label": 0
                },
                {
                    "sent": "And the answer is of course.",
                    "label": 0
                },
                {
                    "sent": "We can place each bit in one of 10 positions.",
                    "label": 0
                },
                {
                    "sent": "They're allowed to overlap.",
                    "label": 1
                },
                {
                    "sent": "So there are 10 to the six possible sequences.",
                    "label": 0
                },
                {
                    "sent": "This model could have generated.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how many of those sequences?",
                    "label": 0
                },
                {
                    "sent": "Look like or match exactly the sequence we observed and that gives us a measure of the probability that this model.",
                    "label": 0
                },
                {
                    "sent": "Generates or explains this sequence.",
                    "label": 0
                },
                {
                    "sent": "Because we can interchange bits.",
                    "label": 0
                },
                {
                    "sent": "We can permute these bit ones without changing our observed sequence.",
                    "label": 0
                },
                {
                    "sent": "Hope you can see that there are going to be 6 factorial possible.",
                    "label": 1
                },
                {
                    "sent": "Sequence is generated from this model, but will match.",
                    "label": 0
                },
                {
                    "sent": "So the probability of this model probability of the data given the model.",
                    "label": 0
                },
                {
                    "sent": "Is 6 factorial over 10 to the 6th, which is 720 / 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "So the actual probability is point North, North N 7 to 4 decimal places.",
                    "label": 0
                },
                {
                    "sent": "Everyone OK with that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if we were to actually.",
                    "label": 0
                },
                {
                    "sent": "Run the simulation and sample from that very model I just described to you.",
                    "label": 0
                },
                {
                    "sent": "We should find this is the proportion that actually looked like that.",
                    "label": 0
                },
                {
                    "sent": "I'll show you an example if you're not convinced.",
                    "label": 0
                },
                {
                    "sent": "So just following up for that, then Model 232 bit sequences.",
                    "label": 0
                },
                {
                    "sent": "We have to place three symbols.",
                    "label": 0
                },
                {
                    "sent": "In one of any nine positions, so there's nine Q possibilities.",
                    "label": 0
                },
                {
                    "sent": "We can permute them in three factorial ways, so we have 6 /, 729 point Dublin, or Type 2.",
                    "label": 0
                },
                {
                    "sent": "Almost intuitively appealing sequence.",
                    "label": 0
                },
                {
                    "sent": "Seven ways of placing this sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, seven ways of placing this.",
                    "label": 0
                },
                {
                    "sent": "Nine ways of placing this.",
                    "label": 0
                },
                {
                    "sent": "But only one of these generated sequences is the precise hit that we want, so this is a less complex model, because it only there's only one in its set that matches, but the set is only 69.",
                    "label": 0
                },
                {
                    "sent": "Sorry 60 three large, so the probability is 163rd, just point North 159.",
                    "label": 0
                },
                {
                    "sent": "So this simplest model, because we consider.",
                    "label": 0
                },
                {
                    "sent": "All the other possible sequences is more probable, so all three fit the data equally.",
                    "label": 0
                },
                {
                    "sent": "We can't separate them in terms of how well they fit the data, but if we take account of all the other possible models, they could have matched.",
                    "label": 0
                },
                {
                    "sent": "This one looks more probable.",
                    "label": 0
                },
                {
                    "sent": "Now when we do data modeling course.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "Normally it's not your case that our model either fits the perfect data, fits the data perfectly, or it doesn't.",
                    "label": 0
                },
                {
                    "sent": "You lose a degree of accuracy.",
                    "label": 0
                },
                {
                    "sent": "Squared error, misclassification rate, something like that.",
                    "label": 0
                },
                {
                    "sent": "So that's why I introduced the 4th model, which is where we look at a.",
                    "label": 0
                },
                {
                    "sent": "And over simple decoding.",
                    "label": 0
                },
                {
                    "sent": "Which assumes some form of error.",
                    "label": 0
                },
                {
                    "sent": "So for this decoding, there's only three possible places I could place a symbol, so it's generating set was only has three elements in it.",
                    "label": 0
                },
                {
                    "sent": "But it seemed 2 bits of error.",
                    "label": 0
                },
                {
                    "sent": "And under I'm just going to seem very simple noise error which says I can place those two bits anywhere I convert any 2 bits, but I won't invert the same bit twice.",
                    "label": 0
                },
                {
                    "sent": "So the possibility of different errors.",
                    "label": 0
                },
                {
                    "sent": "I could have observed, given that there could be two is 10 choose two or 45.",
                    "label": 0
                },
                {
                    "sent": "So the probability of this model is 1 of 3 * 45135.",
                    "label": 1
                },
                {
                    "sent": "And that works out as point double nought 7 four.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The oversimple model.",
                    "label": 0
                },
                {
                    "sent": "It doesn't quite fit.",
                    "label": 0
                },
                {
                    "sent": "The data is less probable than the relatively simple model, but does so you can see how this probability calculation is automatically imposed that radar.",
                    "label": 0
                },
                {
                    "sent": "But Interestingly, they have a simple model is actually an order of magnitude more probable than the most complex model we have.",
                    "label": 0
                },
                {
                    "sent": "So this probability calculation is automatically.",
                    "label": 0
                },
                {
                    "sent": "If you like built in that Occam's razor tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Occam's razor trailer.",
                    "label": 0
                },
                {
                    "sent": "It's assigned low probability to two complex and model.",
                    "label": 0
                },
                {
                    "sent": "Because it had to take into account all the other sequences, all the other data about model could have accurately predicted.",
                    "label": 1
                },
                {
                    "sent": "And it's also assigned a lower probability to an over simple model, and it's done that because it had to introduce a noise model in order to appropriately explain the data and that noise model.",
                    "label": 0
                },
                {
                    "sent": "Meant that the space of all possible datasets it could have still observed again gets expanded because of that noise assumption.",
                    "label": 0
                },
                {
                    "sent": "So I just want to convince in case you know, maybe you don't believe that.",
                    "label": 0
                },
                {
                    "sent": "Or maybe don't move my calculations.",
                    "label": 0
                },
                {
                    "sent": "I just have a quick simulation I can show.",
                    "label": 0
                },
                {
                    "sent": "Well, we just hopefully empirically prove those numbers in the last slide, so I'm just going to do with a few lines of simple Matlab.",
                    "label": 0
                },
                {
                    "sent": "You can keep this up yourself if you were so minded.",
                    "label": 0
                },
                {
                    "sent": "Going to sample from those four models and we'll just verify these values, so this is just six bits placed at random.",
                    "label": 0
                },
                {
                    "sent": "You can almost write the code in your head if you got your laptop, you probably don't know.",
                    "label": 0
                },
                {
                    "sent": "This is 3 two bit symbols placed at random before on the two and the eight with two randomly flipped bits.",
                    "label": 0
                },
                {
                    "sent": "And every time a symbol actually matches the target sequence, it'll flashing white and will count it up.",
                    "label": 0
                },
                {
                    "sent": "So I'm just really simulating.",
                    "label": 0
                },
                {
                    "sent": "Exactly what you might do.",
                    "label": 0
                },
                {
                    "sent": "If you just wanted to prove that previous.",
                    "label": 0
                },
                {
                    "sent": "Mathematics, so we just do 10,000 samples.",
                    "label": 0
                },
                {
                    "sent": "And we'll just simply count how many times those generative models they simulated models match the target sequence.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "If you ugly little more convinced Model 3 is much more probable out of 10,000 times hits the target sequence, it predicts it and explains it far more than the other models.",
                    "label": 0
                },
                {
                    "sent": "The red lines are the theoretical values and actually they match for more than I would have expected.",
                    "label": 0
                },
                {
                    "sent": "But Model 4, the noise, the over simple model that doesn't match the data with noise, still predicts the target sequence far more than the complicated Model 1.",
                    "label": 0
                },
                {
                    "sent": "So I haven't sort of written down much in the way of probability distributions or really done any probabilistic manipulations, which sort of done everything by counting sequences.",
                    "label": 0
                },
                {
                    "sent": "But this example is basically Bayesian inference in disguise, and the reason is is because we've calculated probabilities for our models that take into account.",
                    "label": 1
                },
                {
                    "sent": "All the other.",
                    "label": 0
                },
                {
                    "sent": "Sense of data that those models were capable of explaining.",
                    "label": 0
                },
                {
                    "sent": "This is the essence of the way Bayesian inference implements Occam's razor, penalizes over complex models because they have to explain much many more datasets in this calculation.",
                    "label": 0
                },
                {
                    "sent": "At the same time, it penalizes simple models because by the introduction of a noise or error model.",
                    "label": 0
                },
                {
                    "sent": "They have to also explain a lot more data.",
                    "label": 0
                },
                {
                    "sent": "Same everyone happy with that so far.",
                    "label": 0
                },
                {
                    "sent": "So that's just setting the scene.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do for the rest of the lecture, which maybe will creep on into the next, is look at a very simple problem in data modeling, just modeling some univariate real value data.",
                    "label": 0
                },
                {
                    "sent": "And hopefully at the end will arrive back at this point where we can look at.",
                    "label": 0
                },
                {
                    "sent": "Potential models or sequence of models and see this effect at.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good game.",
                    "label": 0
                },
                {
                    "sent": "So here is a simple modeling problem.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of supposedly mystery data we don't really know where it's come from.",
                    "label": 1
                },
                {
                    "sent": "Imagine as we go along, but really you don't know anything about it, but the truth is for the purposes of this lecture.",
                    "label": 0
                },
                {
                    "sent": "An many I've given for years, in fact probably generated this data about eight years ago.",
                    "label": 0
                },
                {
                    "sent": "There are actually 15 samples generated from sine wave function.",
                    "label": 1
                },
                {
                    "sent": "With semantic Gaussian noise, standard deviation nought .2.",
                    "label": 0
                },
                {
                    "sent": "And just in terms of notation, usually called the inputs XR function, I'll call Y&R observations or sometimes will target variables are called T. So I've got 15 of these 15 inputs, 15 corresponding target variable.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now kind of models we're going to look at again.",
                    "label": 0
                },
                {
                    "sent": "So 'cause we want to have some.",
                    "label": 0
                },
                {
                    "sent": "Transparent analysis of what we're doing.",
                    "label": 0
                },
                {
                    "sent": "We're going to give almost the simplest formal model, and those are models that are linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "So our prediction is simply going to be a weighted sum of some fixed basis functions.",
                    "label": 1
                },
                {
                    "sent": "So the basis functions could be linear, but typically will use some nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "And in this example we use some Gaussian data data center basis functions.",
                    "label": 1
                },
                {
                    "sent": "So project so will be nonlinear, but of course the key thing is it's called a linear model or linear in the parameters model, because the parameters only appear linearly.",
                    "label": 0
                },
                {
                    "sent": "So in optical problem we've got 15 of these Gaussians, each centered on.",
                    "label": 0
                },
                {
                    "sent": "In the X axis, on each of the data points.",
                    "label": 0
                },
                {
                    "sent": "So our prediction model is a linearly weighted sum of those 15.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So somehow we want to find a good model once, find good values, those underlying there's built-in weights.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There's a range of approaches, but perhaps the most simple simply to say, let's try and fit the data by defining an error measure.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you're all familiar with this idea of least squares.",
                    "label": 0
                },
                {
                    "sent": "So we define the error, measure the least squares error, which is simply the sum squared deviation of the data from our predictions.",
                    "label": 1
                },
                {
                    "sent": "This is a popular measure for a number of reasons, but one obvious one is.",
                    "label": 0
                },
                {
                    "sent": "Because the weights appear quadratically, we can minimize it analytically.",
                    "label": 1
                },
                {
                    "sent": "And if we define the design matrix Phi as the matrix of the output of every basis function for every data point, we can have this analytic matrix vector expression that gives us a solution for the set of weights that minimizes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sarah.",
                    "label": 0
                },
                {
                    "sent": "Now of course we know in this case I think you're probably expecting we've got 15 basis functions.",
                    "label": 1
                },
                {
                    "sent": "15 data points.",
                    "label": 0
                },
                {
                    "sent": "We should be able to fit the data perfectly.",
                    "label": 1
                },
                {
                    "sent": "And that's what we see.",
                    "label": 0
                },
                {
                    "sent": "So that's the least squares fit using an RBF model with some Gaussian basis functions on our noisy data.",
                    "label": 0
                },
                {
                    "sent": "In principle, this is the ideal fit because we know that the underlying generator was a sign function, so this is a sine wave.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An obvious question is which of these models is actually better?",
                    "label": 0
                },
                {
                    "sent": "So we get to a sort of philosophical point.",
                    "label": 0
                },
                {
                    "sent": "It's important to realize.",
                    "label": 0
                },
                {
                    "sent": "But without any further assumptions, without knowing where the data came from, we can't judge between these two models.",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "Whether the data really came from this more complex function with no noise.",
                    "label": 0
                },
                {
                    "sent": "Or whether it came from a simple function with noise.",
                    "label": 0
                },
                {
                    "sent": "We just can't tell.",
                    "label": 0
                },
                {
                    "sent": "Without any further information, we can't make any further progress in modeling this data.",
                    "label": 0
                },
                {
                    "sent": "We must accept.",
                    "label": 0
                },
                {
                    "sent": "But we're going to have to introduce some prior knowledge or belief or understanding into this problem.",
                    "label": 1
                },
                {
                    "sent": "And so we don't give up because we're fairly happy to make the assumption then the real world and the kind of data modeling problems that we tackle.",
                    "label": 0
                },
                {
                    "sent": "The underlying functions that we expect to find.",
                    "label": 0
                },
                {
                    "sent": "Are probably smooth and probably have some degree of noise, but let's concentrate on the smoothness aspect.",
                    "label": 0
                },
                {
                    "sent": "Even if I hadn't told you that, we'd use assign way to generate this data.",
                    "label": 0
                },
                {
                    "sent": "I think you probably would have found.",
                    "label": 0
                },
                {
                    "sent": "This solution more appealing than this solution.",
                    "label": 0
                },
                {
                    "sent": "That's partly it's sort of inherent Occam's razor at work, but it's also a reflection of the fact that we believe when modeling data that underlying.",
                    "label": 0
                },
                {
                    "sent": "Functions that we're looking to try and capture in our models tend to have.",
                    "label": 0
                },
                {
                    "sent": "Smoother character rather.",
                    "label": 0
                },
                {
                    "sent": "Bing complex character.",
                    "label": 0
                },
                {
                    "sent": "So fit.",
                    "label": 0
                },
                {
                    "sent": "But bottom line is we have to introduce some kind of prior knowledge into the problem.",
                    "label": 0
                },
                {
                    "sent": "And so in this kind of least squares.",
                    "label": 0
                },
                {
                    "sent": "Regression or linear modeling?",
                    "label": 0
                },
                {
                    "sent": "What we need to do is add something to the model such that we.",
                    "label": 0
                },
                {
                    "sent": "More likely to dig out or extract functions like this and like this.",
                    "label": 0
                },
                {
                    "sent": "So what is typically done?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conventionally, is to do something called regularization.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is add a penalty term that penalizes.",
                    "label": 1
                },
                {
                    "sent": "There is over complex functions.",
                    "label": 0
                },
                {
                    "sent": "A very convenient way to do this is to add a regularization weight penalty which penalizes large weights because.",
                    "label": 0
                },
                {
                    "sent": "More rapidly changing functions, more complex functions typically have larger weights, so we simply add to our least squares error this weight penalty, which is a sum of squares of the weights.",
                    "label": 1
                },
                {
                    "sent": "And one reason why this penalty is quite nice.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "It gives us an analytic estimate for what I'll call the penalized least squares value for the weights.",
                    "label": 0
                },
                {
                    "sent": "And we notice how this term.",
                    "label": 0
                },
                {
                    "sent": "Containing the hyperparameter, Lambda enters into the.",
                    "label": 0
                },
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "So if we set Lambda very high.",
                    "label": 0
                },
                {
                    "sent": "Then the white term dominates the overall term and we forced the weights three very small.",
                    "label": 0
                },
                {
                    "sent": "So I think you can see how if we set Lambda high here.",
                    "label": 0
                },
                {
                    "sent": "This weight vector will be pushed towards there.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just have a look at some potential values of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So there's our data.",
                    "label": 0
                },
                {
                    "sent": "For set Lambda to a very small value, say roughly 0 here, then it's almost equivalent to not having that regularization term.",
                    "label": 0
                },
                {
                    "sent": "And this almost fits the data perfectly.",
                    "label": 0
                },
                {
                    "sent": "For set Lambda quite high, I'm imposing a lot of penalty.",
                    "label": 0
                },
                {
                    "sent": "And my fit to the data is very poor.",
                    "label": 0
                },
                {
                    "sent": "I have effectively over smooth or oversimplified function.",
                    "label": 0
                },
                {
                    "sent": "Somewhere in between there's going to be a value of Lambda that gives us an intuitively appealing result.",
                    "label": 0
                },
                {
                    "sent": "And that's this value here.",
                    "label": 0
                },
                {
                    "sent": "So I'm not saying this is the best value, but this is just a value.",
                    "label": 0
                },
                {
                    "sent": "But gives a reasonable answer.",
                    "label": 0
                },
                {
                    "sent": "So somewhere out there, there's a decent value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "The question is, how can we find it for our particular modeling problem?",
                    "label": 0
                },
                {
                    "sent": "So here we have to sort of acknowledge that we're going to require within this framework I'm presenting.",
                    "label": 0
                },
                {
                    "sent": "Some extra help.",
                    "label": 0
                },
                {
                    "sent": "And that help is going to come by the mechanism of a separate validation set.",
                    "label": 0
                },
                {
                    "sent": "So imagine we're given another 15 samples of data from this data set.",
                    "label": 0
                },
                {
                    "sent": "And we can use that to validate our choice of Lambda by looking at the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error the model would make on this new data, so that's what we do here.",
                    "label": 0
                },
                {
                    "sent": "So the green points are new data points.",
                    "label": 0
                },
                {
                    "sent": "And all three models are simply checked against the validation set, and sure enough, the oversimple and the overcomplex model give a worse error then.",
                    "label": 0
                },
                {
                    "sent": "The market looks about right.",
                    "label": 0
                },
                {
                    "sent": "So we sort of now have a mechanism system if you like for fitting our model in that we will use.",
                    "label": 0
                },
                {
                    "sent": "The validation set will fit the weights during using least squares and will use the validation set to determine the hyperparameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "But typically, of course, won't just pick three values.",
                    "label": 0
                },
                {
                    "sent": "What we'll do in a reasonable practical experiment will try a lot of Lambda values, and then we'll plot the results.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the result of that experiment.",
                    "label": 0
                },
                {
                    "sent": "So along the bottom I have Lambda or log of Lambda.",
                    "label": 0
                },
                {
                    "sent": "It's quite natural actually to take logarithms of variance parameters or scale parameters and Landers.",
                    "label": 0
                },
                {
                    "sent": "An example of that.",
                    "label": 0
                },
                {
                    "sent": "Errors plotted on the vertical axis, and I'm plotting one obvious thing I'm plotting is the least squares well.",
                    "label": 0
                },
                {
                    "sent": "And I should say, yes, that's the least squares error fit to the training data shown in black.",
                    "label": 0
                },
                {
                    "sent": "So obviously when Lambda is 0 this end there is no regularization fits the data perfectly as we increase Lambda.",
                    "label": 0
                },
                {
                    "sent": "Our model gets more simplified and fit to the data deteriorates.",
                    "label": 0
                },
                {
                    "sent": "However, if we measure the error of the model on the validation set, what we hope is this goes down.",
                    "label": 0
                },
                {
                    "sent": "Interesting, Lee just kicks up there, which is presumably a numeric effect.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless, the validation error has a distinguished minimum.",
                    "label": 0
                },
                {
                    "sent": "And what we hope that that minimum is a good fit to the underlying function.",
                    "label": 0
                },
                {
                    "sent": "And in this case, because we generated it from sine wave, we can test our solution against the true noise free underlying function.",
                    "label": 0
                },
                {
                    "sent": "So this test error is simply the error of the model calculated with respect to the true underlying sign.",
                    "label": 0
                },
                {
                    "sent": "So in the real world we type, but this minimum would be close to the minimum of the test error.",
                    "label": 0
                },
                {
                    "sent": "In Lambda, Space is a little far little further away, but this is very flat Valley if you like and the error value of that is not too far away from that, so it's not too bad.",
                    "label": 0
                },
                {
                    "sent": "It's come up with a reasonable answer.",
                    "label": 0
                },
                {
                    "sent": "So we'll come back to this graph, possibly by the end of the lecture.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "When we look at it in a Bayesian from a Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "But before we get there, let's just rewind to the start of this regression problem again, and let's look at how we might tackle it from.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian perspective.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me just start by defining the sort of the basic principle of Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we have to specify.",
                    "label": 0
                },
                {
                    "sent": "Probability distributions, normally called priors.",
                    "label": 0
                },
                {
                    "sent": "Over every variable in the model.",
                    "label": 0
                },
                {
                    "sent": "So that includes things such as the observations themselves, which it seems intuitive to model probabilistically, 'cause you can think of those as being stochastic realizations from some underlying random process, so it can be.",
                    "label": 0
                },
                {
                    "sent": "Noise might specify a noise model, or if it's a classification of pattern recognition problem, it would be an expression of the uncertainty of observations, because classes overlap, so that usually makes intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "People don't see that as controversial.",
                    "label": 0
                },
                {
                    "sent": "Slightly more challenging to the mind is the idea of placing probability distributions over parameters in the model.",
                    "label": 0
                },
                {
                    "sent": "So we say before seeing the data we placed prior probability distributions in principle over all the parameters that we have in our model.",
                    "label": 1
                },
                {
                    "sent": "And in this case, rather than thinking of probability as summary of the outcomes of a random process, probability is interpreted as.",
                    "label": 0
                },
                {
                    "sent": "Expressing our degree of belief in.",
                    "label": 0
                },
                {
                    "sent": "The values that parameter should take before we start the modeling process.",
                    "label": 0
                },
                {
                    "sent": "One advantage of that seems like maybe.",
                    "label": 0
                },
                {
                    "sent": "Slightly complicated notion.",
                    "label": 0
                },
                {
                    "sent": "Is there as I show, we can actually sample from these priors and be actually quite informative and tell us something that other paradigms to modeling data don't necessarily do.",
                    "label": 0
                },
                {
                    "sent": "It will come to that.",
                    "label": 0
                },
                {
                    "sent": "Final elements we might actually want to put probabilities are on the model itself, so in principle.",
                    "label": 0
                },
                {
                    "sent": "We can actually put some probability distribution on the type of the model.",
                    "label": 0
                },
                {
                    "sent": "The structure is choice of basis set or even parameters within the basis set.",
                    "label": 0
                },
                {
                    "sent": "For example, the width of those Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So will look at this in principle beginning of the next lecture.",
                    "label": 0
                },
                {
                    "sent": "But for now we'll just look at how prize on parameters work.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we've seen our data.",
                    "label": 0
                },
                {
                    "sent": "We take these prior distributions and we convert them into posterior distributions using Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So we calculate.",
                    "label": 0
                },
                {
                    "sent": "Probabilities of these conditioned purely on the observed data.",
                    "label": 0
                },
                {
                    "sent": "And the crucial aspect to Bayesian inference, and to get Occam's Razor working will see is that we need to integrate out variables which are not directly of interest.",
                    "label": 1
                },
                {
                    "sent": "So in other words, all our parameters.",
                    "label": 0
                },
                {
                    "sent": "Unless we actually want to predict parameters, we need to get rid of them.",
                    "label": 0
                },
                {
                    "sent": "So this notion of integrating out is a little bit involved and one can spend several slides explaining how this works, and indeed I did produce some, but I've cut those out.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do is I'll show this idea this notion by one or two examples as we go through.",
                    "label": 0
                },
                {
                    "sent": "But I have to point out at this stage.",
                    "label": 0
                },
                {
                    "sent": "But a lot of these integration procedures turn out to be analytically intractable, and that's quite unfortunate.",
                    "label": 0
                },
                {
                    "sent": "I think if this wasn't the case.",
                    "label": 0
                },
                {
                    "sent": "Everyone in the world will be Bayesian and using Bayesian procedures.",
                    "label": 0
                },
                {
                    "sent": "But this actually complicates matters quite considerably.",
                    "label": 0
                },
                {
                    "sent": "In most models.",
                    "label": 1
                },
                {
                    "sent": "Most work in Bayesian inference is arguably all about using and applying approximation methods for calculating these integrals.",
                    "label": 0
                },
                {
                    "sent": "But the reason we put up with this problem is because there are some really nice benefits and features of the Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "The key one I'm going to focus on is this automatic implementation of Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "So let's stop talking about philosophy and principles.",
                    "label": 0
                },
                {
                    "sent": "And let's go back to look at our data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's consider how we model it.",
                    "label": 0
                },
                {
                    "sent": "So we start off with a very simple likelihood model.",
                    "label": 1
                },
                {
                    "sent": "This is our model of our observations, so we see observations equal or underlying function plus noise, and we place a probability distribution over that noise.",
                    "label": 0
                },
                {
                    "sent": "So this is our modeling assumption over what noise we might think we might see.",
                    "label": 0
                },
                {
                    "sent": "And we simply say we expect mean zero noise with some variance as yet unknown Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "We assume all the data is seen independently.",
                    "label": 1
                },
                {
                    "sent": "The probability of observing the data given the current weights and noise.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Given by the product of the individual probabilities, and that's simply the probable product of these Gaussians centered on the model.",
                    "label": 0
                },
                {
                    "sent": "So if I simply plug back into there.",
                    "label": 0
                },
                {
                    "sent": "This is what I'll get.",
                    "label": 0
                },
                {
                    "sent": "So this is the likelihood of the common term in modeling for the data.",
                    "label": 1
                },
                {
                    "sent": "Now, can you see hopefully?",
                    "label": 0
                },
                {
                    "sent": "But if I try maximized this I try and make the data look most probable.",
                    "label": 0
                },
                {
                    "sent": "By changing the weights will consider Sigma squared to be fixed I should say.",
                    "label": 0
                },
                {
                    "sent": "So if we look at maximizing this, doing maximum likelihood with respect to the parameters W. If I maximize this, I get the same result as least squares.",
                    "label": 0
                },
                {
                    "sent": "I can see that because by maximize this is the same as maximizing the logarithm of it.",
                    "label": 0
                },
                {
                    "sent": "If I maximize the log of a product, I'll get a sum of logs of this.",
                    "label": 0
                },
                {
                    "sent": "An log of this obviously gets rid of that, and that will give we minus.",
                    "label": 0
                },
                {
                    "sent": "Some of the minor squared error, so if I minimize that I can take out that - so maximum likelihood is equal to least squares under this assumption that all comes from the assumption of a Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So we sort of framed it equivalently now to least squares.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the interesting thing, of course, is the prior distribution.",
                    "label": 1
                },
                {
                    "sent": "So as I said before, we have to specify a prior which sort of encapsulates our belief in what values parameters should take before we start modeling.",
                    "label": 0
                },
                {
                    "sent": "Now you might see there's a lot of think there's a lot of voodoo in how you specify the prior, and specifying prize is sort of a key part, and it's very important there's a lot of expert knowledge in it.",
                    "label": 0
                },
                {
                    "sent": "Well, actually will see another example.",
                    "label": 0
                },
                {
                    "sent": "But we can assume a perfectly flat prior in many cases and still get some really interesting and powerful results out.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Occam's Razor actually will still work for you, even in cases where you assume a flat prior ever in this case.",
                    "label": 0
                },
                {
                    "sent": "A flat pie wouldn't be appropriate because we're actually looking to find smoother functions.",
                    "label": 0
                },
                {
                    "sent": "So conventional choice is 0 mean Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So instead of penalizing large weights as we did in the more conventional treatment.",
                    "label": 1
                },
                {
                    "sent": "We're actually going to make small weights more probable.",
                    "label": 0
                },
                {
                    "sent": "So we specify Gaussian distribution over the weights that makes each weight individual weight, so it's independent for each weight.",
                    "label": 0
                },
                {
                    "sent": "But they share this hyperparameter Alpha, which is an inverse variance parameter.",
                    "label": 0
                },
                {
                    "sent": "So if I make Alpha very large.",
                    "label": 0
                },
                {
                    "sent": "I make that wait prior very tight around 0.",
                    "label": 0
                },
                {
                    "sent": "So if Lambda is large, with apriori very certain, the weights are small.",
                    "label": 0
                },
                {
                    "sent": "How to set lambdas are very small value and Gaussian becomes very flat.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's almost uniform and uninformative.",
                    "label": 0
                },
                {
                    "sent": "So we've sort of now.",
                    "label": 0
                },
                {
                    "sent": "Got an analogue for the regularization hyperparameter in this Bayesian treatment, in that we now have this inverse variance hyperparameter, we need to consider.",
                    "label": 0
                },
                {
                    "sent": "Well, let's consider.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How we do inference?",
                    "label": 0
                },
                {
                    "sent": "So finally Bayes rule appears.",
                    "label": 1
                },
                {
                    "sent": "So we take that likelihood function.",
                    "label": 0
                },
                {
                    "sent": "We take the prior and we combine it.",
                    "label": 0
                },
                {
                    "sent": "In Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "To give us the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "Over the parameters.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on the data.",
                    "label": 0
                },
                {
                    "sent": "So this is a I've written this here because Bayes rule often appears in this form in Bayesian calculations is typically the multiple of a likelihood, which is the probability of the data condition on some parameters times the prior over those parameters over normalizing factor and the key point about the normalizing factor is this term doesn't depend on.",
                    "label": 0
                },
                {
                    "sent": "The whites of the parameters W. So this is independent of W. So the posterior here.",
                    "label": 0
                },
                {
                    "sent": "Because this is Gaussian and this is Gaussian, so happens that the posterior is also Gaussian.",
                    "label": 1
                },
                {
                    "sent": "It has a mean given by.",
                    "label": 0
                },
                {
                    "sent": "This term here, but it also has a measure of spread, so it has a measure of uncertainty, the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of an expression of how unsure the model is about the values of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Having seen the data.",
                    "label": 0
                },
                {
                    "sent": "And the idea is, and what we'll see through practice.",
                    "label": 0
                },
                {
                    "sent": "For example, is that the more data we see intuitively as you would expect?",
                    "label": 0
                },
                {
                    "sent": "The smaller the narrower this covariance matrix gets.",
                    "label": 1
                },
                {
                    "sent": "Everyone OK so far with this.",
                    "label": 0
                },
                {
                    "sent": "I've got a little refresher slide on the rules of probability, if anyone.",
                    "label": 0
                },
                {
                    "sent": "Daz acknowledged that they would like to see it.",
                    "label": 0
                },
                {
                    "sent": "Right, let me just do that then.",
                    "label": 0
                },
                {
                    "sent": "So this is maybe a good point.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To remind, remind him refresh people about the basic rules of probability.",
                    "label": 1
                },
                {
                    "sent": "This is all you need to know.",
                    "label": 0
                },
                {
                    "sent": "Fitting manipulations in Bayesian inference, or indeed a lot of other probability problems, so the product rule of probability.",
                    "label": 0
                },
                {
                    "sent": "22 variables with joint distribution of those variables.",
                    "label": 0
                },
                {
                    "sent": "Is the condition of 1 on the other?",
                    "label": 0
                },
                {
                    "sent": "Times the marginal of the other.",
                    "label": 0
                },
                {
                    "sent": "And because this these two terms can be reversed, there's a symmetric.",
                    "label": 0
                },
                {
                    "sent": "Form, which is simply the swap of the two variables.",
                    "label": 0
                },
                {
                    "sent": "So typically Bayesian inference so will be will be more than two variables and will be keeping some variables on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "If you like purely well we.",
                    "label": 0
                },
                {
                    "sent": "Will be retaining them as conditioning variables?",
                    "label": 0
                },
                {
                    "sent": "We would say so quite often we have something of this form which is a more general form of the product rule.",
                    "label": 0
                },
                {
                    "sent": "Well, we have the joint distribution of.",
                    "label": 0
                },
                {
                    "sent": "A&B, conditioned on a particular value of C. And we simply all we do in this case is.",
                    "label": 0
                },
                {
                    "sent": "This would see retained on the right of the conditioning.",
                    "label": 0
                },
                {
                    "sent": "So this is just this, but we condition everything on C. And Bayes rule actually comes simply from rearranging the product rule.",
                    "label": 0
                },
                {
                    "sent": "So we could just simply rearrange those, but more generally will just set that equal to that rearrange and we get sort of a general form of Bayes rule, where we convert the probability be given a ANSI into the probability of a given B&C.",
                    "label": 0
                },
                {
                    "sent": "So C remains always on the right of the conditioning.",
                    "label": 1
                },
                {
                    "sent": "So that's the product rule in Bayes rule, which comes out of it.",
                    "label": 0
                },
                {
                    "sent": "Another important rule is just what will be called the sum rule in probability, but in over continuous spaces.",
                    "label": 0
                },
                {
                    "sent": "We call it the integral rule.",
                    "label": 0
                },
                {
                    "sent": "And that simply says that the marginal probability of B and again we generalize by conditioning on C. Is simply the integral over the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Integrating over variable A, so this we would call.",
                    "label": 0
                },
                {
                    "sent": "Integrating out a.",
                    "label": 0
                },
                {
                    "sent": "So the probability is the probability of the joint of AMB integrating out a.",
                    "label": 0
                },
                {
                    "sent": "So this is the integrating out procedure.",
                    "label": 0
                },
                {
                    "sent": "So these are really all the manipulations you need.",
                    "label": 0
                },
                {
                    "sent": "A lot of the difficulty comes in performing this calculation.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm just also point out.",
                    "label": 0
                },
                {
                    "sent": "But this term, as you can see, it appears as the normalizing constants in Bayes rule, and that's typically what happens.",
                    "label": 0
                },
                {
                    "sent": "Will find that the normalizing constant is actually the integral of the.",
                    "label": 0
                },
                {
                    "sent": "Numerator.",
                    "label": 0
                },
                {
                    "sent": "And that's what will often stop us from calculating these posterior distributions via Bayes rule because we can't calculate this normalizing factor because that requires an integral.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that was Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So the expression I just showed on that slide for example.",
                    "label": 0
                },
                {
                    "sent": "Sigma squared and Alpha are examples of that C variable that we kept on the right hand side and retained for conditioning.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, you're probably thinking we should be Bayesian about those, and we will be later.",
                    "label": 0
                },
                {
                    "sent": "But for now we're just going to consider them as fixed quantities to be conditioned on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we've got this posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "What do we do with that?",
                    "label": 0
                },
                {
                    "sent": "Well, here's what we probably shouldn't do with it, but this gets done a lot, and there is often.",
                    "label": 0
                },
                {
                    "sent": "Adding problems there is some value in doing this, but this isn't really being Bayesian and this is the idea of simply taking that posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "And sticking with just its most probable value.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Maximum a posteriori estimate.",
                    "label": 0
                },
                {
                    "sent": "Which is simply the most probable value of the parameters under the posterior.",
                    "label": 1
                },
                {
                    "sent": "Now for Gaussian, as you will know, the maximum is equal to the mean, and we had the mean on the previous slide, just re expressing it here.",
                    "label": 1
                },
                {
                    "sent": "And you probably noticed already, but the mean or the mode of this distribution bore a strong resemblance to the penalized least squared solution.",
                    "label": 0
                },
                {
                    "sent": "In fact, the two of the same were just a simple reparameterization.",
                    "label": 0
                },
                {
                    "sent": "Solandri equals Sigma squared Alpha.",
                    "label": 0
                },
                {
                    "sent": "These solutions are effectively the same.",
                    "label": 0
                },
                {
                    "sent": "So far we've got through quite a long way in this lecture, and then we've done is recovered.",
                    "label": 0
                },
                {
                    "sent": "Exactly what conventional methods do?",
                    "label": 0
                },
                {
                    "sent": "But the key to Bayesian methods is in marginalization.",
                    "label": 0
                },
                {
                    "sent": "Before I show you that, I just want to show you.",
                    "label": 0
                },
                {
                    "sent": "A brief example of.",
                    "label": 0
                },
                {
                    "sent": "This inference.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "So over to matter.",
                    "label": 0
                },
                {
                    "sent": "Remember when I called it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're just going to perform.",
                    "label": 0
                },
                {
                    "sent": "Slides showing how Bayesian inference works will just do it in a simulation.",
                    "label": 1
                },
                {
                    "sent": "Here running under matlab.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You recognize that data.",
                    "label": 0
                },
                {
                    "sent": "Very interesting data.",
                    "label": 0
                },
                {
                    "sent": "I'm just demonstrating underneath it so you have a feeling for the problem.",
                    "label": 0
                },
                {
                    "sent": "What those Gaussian basis functions look like.",
                    "label": 0
                },
                {
                    "sent": "So here 15 basis functions.",
                    "label": 0
                },
                {
                    "sent": "One sensor on each data point.",
                    "label": 0
                },
                {
                    "sent": "I've highlighted to those basis functions which are.",
                    "label": 0
                },
                {
                    "sent": "Basic functions 10 and 11 counting from the left.",
                    "label": 0
                },
                {
                    "sent": "And I'm looking.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at the prior and subsequently the posterior distribution over those basis functions.",
                    "label": 0
                },
                {
                    "sent": "So here I'm plotting the prior distribution.",
                    "label": 1
                },
                {
                    "sent": "Overweights 10 and 11.",
                    "label": 0
                },
                {
                    "sent": "Obviously it be nice to visualize all 15 weights, but.",
                    "label": 0
                },
                {
                    "sent": "Not particularly visualizing 15 dimensional spaces, so I'm just going to focus on these two.",
                    "label": 0
                },
                {
                    "sent": "So this is the Gaussian prior distribution consoles of it.",
                    "label": 0
                },
                {
                    "sent": "For these two basic functions.",
                    "label": 0
                },
                {
                    "sent": "Something I referred to earlier, I think is quite valid point.",
                    "label": 0
                },
                {
                    "sent": "Worth making that Bayesian methods is that we can by specifying everything probabilistically in principle.",
                    "label": 0
                },
                {
                    "sent": "Depending on the precise where you've specified the distributions, you should be able to sample from them.",
                    "label": 0
                },
                {
                    "sent": "So this means if you sample from your model, you should be able to produce samples by your likelihood function of actual data you may see.",
                    "label": 0
                },
                {
                    "sent": "And that in principle could be a good guide to telling you whether your model assumptions are good or not.",
                    "label": 0
                },
                {
                    "sent": "So here are some samples from the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So by that I mean I'm taking not just these two weights, but all 15 weights, something a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "According to that prior plugging into the model function Y and evaluating it will respect those basis functions and these are samples.",
                    "label": 0
                },
                {
                    "sent": "In this univariate case it's very simple just to visualize what those look like, so these are noise free.",
                    "label": 0
                },
                {
                    "sent": "I should say I could have noise.",
                    "label": 0
                },
                {
                    "sent": "But under this model I've kept.",
                    "label": 0
                },
                {
                    "sent": "The noise variance fixed in this example.",
                    "label": 0
                },
                {
                    "sent": "But I'm just showing the underlying functions.",
                    "label": 0
                },
                {
                    "sent": "So these are samples from the prior and this is something I don't think you can get out of.",
                    "label": 0
                },
                {
                    "sent": "For example, a regularization approach so you can impose a squared weight penalty, but.",
                    "label": 0
                },
                {
                    "sent": "It's hard to see how to get a handle on what that is doing to your model.",
                    "label": 0
                },
                {
                    "sent": "It's hard to understand perhaps what kind of functions are likely to come out with low error from that model, whereas the probabilistic expression of.",
                    "label": 0
                },
                {
                    "sent": "That weight penalty if you like via prior distribution, is far more intuitively understandable 'cause we can sample from it.",
                    "label": 0
                },
                {
                    "sent": "We can actually look at what it produces.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do now?",
                    "label": 0
                },
                {
                    "sent": "I'm going to actually start observing the data and calculating posterior distributions, so I'll start plotting the posterior in this graph.",
                    "label": 1
                },
                {
                    "sent": "And because we observe the data independently, we can look at the data one data point at a time.",
                    "label": 0
                },
                {
                    "sent": "So we'll look at this data point, but why don't I do that?",
                    "label": 0
                },
                {
                    "sent": "And we'll calculate the posterior as we go along.",
                    "label": 0
                },
                {
                    "sent": "So I don't have to look at all the data at one time.",
                    "label": 0
                },
                {
                    "sent": "I can just click on.",
                    "label": 0
                },
                {
                    "sent": "I'll say the first 4.",
                    "label": 0
                },
                {
                    "sent": "Click on the 1st four data points and will calculate the posterior distribution over the weights.",
                    "label": 0
                },
                {
                    "sent": "Condition purely on those four observations.",
                    "label": 0
                },
                {
                    "sent": "And the idea is, if I because of independence, if I look at 15 data points, one at a time, the results I get at the end is exactly the same as if I looked at all 15 data points at once.",
                    "label": 0
                },
                {
                    "sent": "So what do we see when we're looking at the posterior distribution here over these two weights?",
                    "label": 0
                },
                {
                    "sent": "And it doesn't appear to change from the prior, and hopefully that makes sense to you, because the four data points it's seen all four outside the support outside the region where these basis functions are non 0.",
                    "label": 0
                },
                {
                    "sent": "Or nearly zero.",
                    "label": 0
                },
                {
                    "sent": "So seeing this for days points tells you next to nothing about the weights coming from these basis functions, because the output these basis functions is pretty much 0.",
                    "label": 0
                },
                {
                    "sent": "It does tell you something about the basis functions at this end.",
                    "label": 0
                },
                {
                    "sent": "And although we don't observe it here, we can see that in this graph here, which shows samples now from the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So these samples from the prior haven't changed, but the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "This is samples from all 15 weights of these red lines.",
                    "label": 0
                },
                {
                    "sent": "And they now you can see, are now more concentrated around the data at this end, where we've seen examples.",
                    "label": 0
                },
                {
                    "sent": "But at this end they're still pretty diffuse, like the prior effectively.",
                    "label": 0
                },
                {
                    "sent": "This green line I plotted is the posterior mean or the map also.",
                    "label": 0
                },
                {
                    "sent": "Predictor.",
                    "label": 0
                },
                {
                    "sent": "So this is plugging in the weights that are the mode of the posterior distribution over all 15 parameters.",
                    "label": 0
                },
                {
                    "sent": "So of course, what you would expect now if I start clicking a bit more across.",
                    "label": 0
                },
                {
                    "sent": "You would expect that the posterior distribution over weights 1011 should begin to change.",
                    "label": 0
                },
                {
                    "sent": "Sure enough, that's what we begin to see.",
                    "label": 0
                },
                {
                    "sent": "So the distribution is still quite diffuse.",
                    "label": 0
                },
                {
                    "sent": "Let's begin to move, and you can begin to see a correlation.",
                    "label": 0
                },
                {
                    "sent": "And of course, you begin to say the samples.",
                    "label": 0
                },
                {
                    "sent": "Well, the green mean tends to follow the data and the samples are all more closely packed around the data from the posterior, but they're still diffuse at this end, so I'll just.",
                    "label": 0
                },
                {
                    "sent": "Absorb, as we sometimes say, those last few data points.",
                    "label": 0
                },
                {
                    "sent": "And there we go.",
                    "label": 0
                },
                {
                    "sent": "So that sort of procedure in principle finished.",
                    "label": 0
                },
                {
                    "sent": "We've seen all 15 data points.",
                    "label": 0
                },
                {
                    "sent": "Here is the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "I've just also marked with across the map estimate, which is the mean of that distribution.",
                    "label": 0
                },
                {
                    "sent": "So hopefully now the posterior samples make some sense.",
                    "label": 0
                },
                {
                    "sent": "Posterior mean roughly tracks the data in the samples and now.",
                    "label": 0
                },
                {
                    "sent": "Roughly in the region of the observed data.",
                    "label": 0
                },
                {
                    "sent": "For something else to notice is that this probability distribution posterior is still quite diffuse.",
                    "label": 0
                },
                {
                    "sent": "It still seems it's definitely more compact than the prior, but it's still quite a bit of uncertainty there.",
                    "label": 0
                },
                {
                    "sent": "Also notice that slightly anticorrelated between those two data points, and that's what you would expect for two basis functions that are close together.",
                    "label": 0
                },
                {
                    "sent": "If you think about it.",
                    "label": 0
                },
                {
                    "sent": "'cause they're very similar outputs, so the weights coming from them.",
                    "label": 0
                },
                {
                    "sent": "Probably sum to roughly a constant value because these numbers are roughly the same.",
                    "label": 0
                },
                {
                    "sent": "So one second.",
                    "label": 0
                },
                {
                    "sent": "So if 2 numbers add up at the same then one is something minus the other.",
                    "label": 0
                },
                {
                    "sent": "So they should be anticorrelated without question.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "One final thing I can do.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll stop at this point.",
                    "label": 0
                },
                {
                    "sent": "Because we've generated this data artificially, I can actually introduce another 100 examples at the click of a button.",
                    "label": 0
                },
                {
                    "sent": "So magically we have another 100 examples.",
                    "label": 0
                },
                {
                    "sent": "And you see, hopefully what makes a little bit of intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "This posterior distribution, now we've seen a lot more data, becomes a lot more compact where a lot more certain about values of the weights, and you should also notice that the samples from the posterior also get tighter, so I'll just do that once more.",
                    "label": 0
                },
                {
                    "sent": "We could keep going and this will get closer and closer to a Delta distribution, and these samples will get more more tightly packed around the mean.",
                    "label": 0
                },
                {
                    "sent": "So again, yes.",
                    "label": 0
                },
                {
                    "sent": "These here.",
                    "label": 0
                },
                {
                    "sent": "These yes, these are weighted Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Yes please.",
                    "label": 0
                },
                {
                    "sent": "Now the red ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so the red ones here.",
                    "label": 0
                },
                {
                    "sent": "Or samples from this distribution.",
                    "label": 0
                },
                {
                    "sent": "If overall 15 weights, so we have a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "The posterior overall 15 weights.",
                    "label": 0
                },
                {
                    "sent": "We can sample from that, which gives us 15 weight values.",
                    "label": 0
                },
                {
                    "sent": "We plug that in to our function Y.",
                    "label": 0
                },
                {
                    "sent": "So that effectively weights these basis functions, and these red curves are those functions evaluated with the sampled weights.",
                    "label": 0
                },
                {
                    "sent": "So if we sample take multiple samples will get different curves, red curves.",
                    "label": 0
                },
                {
                    "sent": "The green curve is simply the value of the weights that corresponds to the mode of the posterior or the mean of the posterior.",
                    "label": 0
                },
                {
                    "sent": "So that's the map predictor.",
                    "label": 0
                },
                {
                    "sent": "The posterior mean predictor.",
                    "label": 0
                },
                {
                    "sent": "We would probably say.",
                    "label": 0
                },
                {
                    "sent": "So I think I'm going to probably, yes, definitely pause here before I do.",
                    "label": 0
                },
                {
                    "sent": "Does anyone have any questions on what we've seen so far?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So this is the prior distribution specification.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So once we've agreed.",
                    "label": 0
                },
                {
                    "sent": "But before we can tackle modeling the data, we have to make an assumption about the kind of functions in the real world we expect to find.",
                    "label": 0
                },
                {
                    "sent": "We need to build that in to our Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "And in the same way in the.",
                    "label": 0
                },
                {
                    "sent": "In the regularization case, we penalize large values of weights because they lead to complex functions.",
                    "label": 1
                },
                {
                    "sent": "A way of building that into a Bayesian model is to make small weights or smooth functions more probable.",
                    "label": 0
                },
                {
                    "sent": "And so really, we want a distribution that gives more probability to wait near 0 small probability to weights further away.",
                    "label": 0
                },
                {
                    "sent": "So first modeling assumption is to place that distribution over the parameters in the model.",
                    "label": 0
                },
                {
                    "sent": "Now we could choose arbitrary distributions arguably, but there are.",
                    "label": 0
                },
                {
                    "sent": "Perhaps good for this optical, in particular good computational reasons for choosing a distribution that's amenable to some calculations, and the Gaussian distribution is a good example of that.",
                    "label": 0
                },
                {
                    "sent": "Spell re specify in advance.",
                    "label": 0
                },
                {
                    "sent": "That we believe.",
                    "label": 0
                },
                {
                    "sent": "The underlying models that we want to or the underlying functions that we're trying to model.",
                    "label": 0
                },
                {
                    "sent": "Will typically have.",
                    "label": 0
                },
                {
                    "sent": "Our belief is they will typically have smaller weights and this is how in the Bayesian context.",
                    "label": 0
                },
                {
                    "sent": "We would impose that belief upon the problem, and we do that by the mechanism of specifying probability distribution over parameters.",
                    "label": 0
                },
                {
                    "sent": "In this case, the weights that says in advance we believe.",
                    "label": 1
                },
                {
                    "sent": "The kind of functions we're likely to find, we ones that have smaller weights.",
                    "label": 0
                },
                {
                    "sent": "So we make a kind of what seems like an arbitrary choice of a Gaussian distribution, mean zero Gaussian that makes small weights more probable, so that has some computational advantages.",
                    "label": 0
                },
                {
                    "sent": "And also, as we saw in this case, it's a good distribution to use in the sense that it's analogous to the squared weight penalty used in regularization.",
                    "label": 0
                },
                {
                    "sent": "And as I showed in the example, if you want to have an understanding.",
                    "label": 0
                },
                {
                    "sent": "For how what this assumption actually implies to your model, you can sample from it.",
                    "label": 0
                },
                {
                    "sent": "And that's what I showed in the bottom right picture.",
                    "label": 0
                },
                {
                    "sent": "Another example samples of weights from this distribution plugged into the model, which gives me samples of the function Y.",
                    "label": 0
                },
                {
                    "sent": "So that gives you an understanding if you're able to visualize it of what your modeling assumptions actually are.",
                    "label": 0
                },
                {
                    "sent": "So if these functions look absolutely nothing like what you're expecting to find, there's a good sign that you might want to rethink your prior distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "If you got two weights, then that's a probability distribution over 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So if you fish.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well, that's what some people do.",
                    "label": 1
                },
                {
                    "sent": "That's not a Bayesian thing to do, as I will show in a few minutes time.",
                    "label": 0
                },
                {
                    "sent": "But yes, that distribution, once you've seen the data, it reflects your knowledge about the problem.",
                    "label": 0
                },
                {
                    "sent": "In that.",
                    "label": 0
                },
                {
                    "sent": "And sure that distribution is a maximum value, but it also has a spread and also has some.",
                    "label": 0
                },
                {
                    "sent": "It's an indication of that.",
                    "label": 0
                },
                {
                    "sent": "Your knowledge is diffuse.",
                    "label": 0
                },
                {
                    "sent": "There are other values of weights that are still probable under that distribution that might give reasonable answers, but nevertheless that distribution is the most probable value and one approach or might say a pragmatic approach in certain modeling scenarios would be to say, oh, I just want one particular model, so let's take the maximum under that distribution.",
                    "label": 0
                },
                {
                    "sent": "So in certain problems that may work well.",
                    "label": 0
                },
                {
                    "sent": "Show in a few minutes time that there is an advantage in not doing that and actually doing something else with that distribution, but that's looking ahead.",
                    "label": 0
                },
                {
                    "sent": "Control.",
                    "label": 0
                },
                {
                    "sent": "Call me back.",
                    "label": 0
                },
                {
                    "sent": "Not quite sure I see your problem with your question.",
                    "label": 0
                },
                {
                    "sent": "Are you worried about not having probabilities over Lambda and Alpha?",
                    "label": 0
                },
                {
                    "sent": "Yes, well.",
                    "label": 0
                },
                {
                    "sent": "In principle, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, I should have shown you.",
                    "label": 0
                },
                {
                    "sent": "And I will.",
                    "label": 0
                },
                {
                    "sent": "Later show.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this this is so far as I specified.",
                    "label": 0
                },
                {
                    "sent": "I haven't been properly a Bayesian.",
                    "label": 0
                },
                {
                    "sent": "I'm correctly adhered to my religion in that there's no way I should have left this Alpha on the right hand side, so I should also specify the prior distribution over Alpha.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if that's quite what your question was getting out.",
                    "label": 0
                },
                {
                    "sent": "And I should have also done the same with that noise variance that I left hanging around that Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So I should have put prize over everything.",
                    "label": 0
                },
                {
                    "sent": "Time doing we're only doing it over the weights.",
                    "label": 0
                },
                {
                    "sent": "In a few minutes time will.",
                    "label": 0
                },
                {
                    "sent": "Think a bit more about what to do with this hyperparameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sign.",
                    "label": 0
                },
                {
                    "sent": "And simple.",
                    "label": 0
                },
                {
                    "sent": "All these things.",
                    "label": 0
                },
                {
                    "sent": "Data points on a sine wave.",
                    "label": 0
                },
                {
                    "sent": "Is the Bayesian method that you're going to the moon?",
                    "label": 0
                },
                {
                    "sent": "Is it reasonable to assume that, given an arbitrary complex model of that data, it would eventually find?",
                    "label": 0
                },
                {
                    "sent": "Something with sharp edges on it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you're absolutely right.",
                    "label": 0
                },
                {
                    "sent": "If one just models with Gaussians.",
                    "label": 1
                },
                {
                    "sent": "Data that underlyingly maybe has discontinuity's.",
                    "label": 0
                },
                {
                    "sent": "Then that's.",
                    "label": 0
                },
                {
                    "sent": "You're basing treatment.",
                    "label": 0
                },
                {
                    "sent": "The problem won't find a good answer.",
                    "label": 0
                },
                {
                    "sent": "The reason is because your prior is effectively misspecified with respect to the underlying functions you're looking for, but you can see that by sampling from it.",
                    "label": 0
                },
                {
                    "sent": "For example, you would say, well, those are all lovely and smooth, but I know there are discontinuities, so I've seen some amazing work in time series analysis, for example by Bill Fitzgerald at Cambridge, where he was looking for nice move time sequences that suddenly change into different states an by specifying inappropriate prior.",
                    "label": 0
                },
                {
                    "sent": "You can actually detect those change points by for example, in a very simplistic sense, one could augment this model with perhaps some discontinuous functions and do a Bayesian treatment over a mixture of Gaussians and discontinuous functions.",
                    "label": 0
                },
                {
                    "sent": "And there's kind of.",
                    "label": 0
                },
                {
                    "sent": "I've got an example at the very very end.",
                    "label": 0
                },
                {
                    "sent": "Which I might get time to get to which kind of shows that it's an action, but it all comes down to your prior, more complex problems with more complex underlying functions.",
                    "label": 0
                },
                {
                    "sent": "You need to think more carefully about specifying your prior.",
                    "label": 0
                },
                {
                    "sent": "And typically, as you make your model more complicated.",
                    "label": 0
                },
                {
                    "sent": "Calculations you need to do, particularly integrals, become troublesome.",
                    "label": 0
                },
                {
                    "sent": "They're analytically intractable.",
                    "label": 0
                },
                {
                    "sent": "You'll have to appeal to some probability, and now there are some numerical methods to solve them.",
                    "label": 0
                },
                {
                    "sent": "And that's where your main problem will arise.",
                    "label": 0
                },
                {
                    "sent": "But there's quite a lot of problems, including some you described that are amenable to linear models.",
                    "label": 0
                },
                {
                    "sent": "Are you saying that?",
                    "label": 0
                },
                {
                    "sent": "Separate different functions depending on your knowledge.",
                    "label": 0
                },
                {
                    "sent": "In principle, yes, you could throw in a whole lot.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and you could use them all together and as long as you're correctly Bayesian.",
                    "label": 0
                },
                {
                    "sent": "The model should pick out the right ones, but it depends on the data you observe.",
                    "label": 0
                },
                {
                    "sent": "An to an extent, because if you if you don't observe data with high enough resolution, for example to detect some of the sharp features you're looking at the model in the world can help you.",
                    "label": 0
                },
                {
                    "sent": "Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Graph.",
                    "label": 0
                },
                {
                    "sent": "Prior point.",
                    "label": 0
                },
                {
                    "sent": "Well, I just have no idea.",
                    "label": 0
                },
                {
                    "sent": "No idea.",
                    "label": 0
                },
                {
                    "sent": "You have to go through the whole process.",
                    "label": 0
                },
                {
                    "sent": "Eventually, yes, I mean.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "In this case, I mean, you could imagine a model, it just keeps going on and on every prior.",
                    "label": 0
                },
                {
                    "sent": "As a associated hyperparameter, specifying the hyper prior has another hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "In practice, what happens is you get to a point where you have a flat prior with no hyperparameters, so in fact.",
                    "label": 0
                },
                {
                    "sent": "In this case I could specify Alpha from a gamma distribution with its own parameters, but equally I could specify Alpha coming from a log uniform distribution so I can stop there.",
                    "label": 0
                },
                {
                    "sent": "I won't go on with this infinite regression.",
                    "label": 0
                },
                {
                    "sent": "I can stop by simply specifying an uninformative Alpha, and usually in the model.",
                    "label": 0
                },
                {
                    "sent": "You would get to a point.",
                    "label": 0
                },
                {
                    "sent": "Maybe two or three layers deep where there is a appropriate distribution, as you say, but you can specify to be flat or noninformative which has no associated parameters.",
                    "label": 1
                },
                {
                    "sent": "I think we should take a break there.",
                    "label": 0
                },
                {
                    "sent": "5 minutes inappropriate so 5 minute break and reconvene.",
                    "label": 0
                }
            ]
        }
    }
}