{
    "id": "eanqqojojh6o4vzo7kl7lcfmbekobeyf",
    "title": "On Low Dimensional Random Projections and Similarity Search",
    "info": {
        "author": [
            "Yu-En Lu, Computer Laboratory, University of Cambridge"
        ],
        "published": "Nov. 19, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/cikm08_lu_oldrpass/",
    "segmentation": [
        [
            "Hello everyone."
        ],
        [
            "Welcome to the last talk of this session.",
            "My name is Eric Young Lou.",
            "I'm going to talk about.",
            "On low dimensional embeddings and similarity search today, this is a work done in jointly with Petroleo and Steven Hand in University of Cambridge, so this is part of my."
        ],
        [
            "PhD work, so the problem is really simple.",
            "Endpoints all then leaving the D dimensional space equipped with the L2 norm, which which is slightly fancier way to say Euclidean distance.",
            "So you are given all these poems.",
            "They can be your documents that can be your images.",
            "They can be audio segments, whatever.",
            "And then I'm sorry I'm still in London time and I just got the freshers flu.",
            "So if I'm talking gibberish just to attack me, so then you.",
            "So you have you got all these endpoints and then in D dimensions and then you want to find an image of it in a much lower dimensional space K such that you want their pairwise distances to be largely largely preserved.",
            "And you don't need anything else.",
            "You don't care about the absolute code details what."
        ],
        [
            "However.",
            "So why are we doing this thing in the previous talk, speaker made a really good point about and.",
            "Nearest neighbor queries.",
            "So these guys are notoriously hard question troubling other theoreticians for like 40 years.",
            "So all so far all the older algorithms.",
            "Either you either have to spend complexity on your storage or computational part.",
            "So if you want constant storage, sorry linear storage then you will have to have a polynomial or even exponential query type.",
            "Or vice versa.",
            "So that makes.",
            "Attempts any attempts that can really help us to reduce down the dimensionality really important.",
            "Then on the systems perspective, and it's arguably more important, is that for large for large systems like peer to peer systems or search engines where there are millions of these thousands and thousands and thousands of machines, you don't really want to.",
            "These infrastructures don't really handle multidimensional queries really well, so most attempts would be.",
            "Either a we learn all the fixed points in these high dimensional space and then remember it for like ages that will spread.",
            "Assign the indices according to that.",
            "So that brought us neatly to the to ask question easy possible that whatever dimension there might be 1000 million features, can I derive a nice low dimensional images of those?",
            "That sort of reflects?",
            "The original properties in the D dimer space in such a way that it still as far as similarities are concerned, they are still quite similar."
        ],
        [
            "So how would you do it?",
            "How high is this problem?",
            "So let's consider a 3 dimensional repo and it's very pretty.",
            "By the way, I spent a lot of time making this photo.",
            "And then so we have a Reaper where you have symmetric circles on the X&Y axis, and then you have these big peak in the center, so you will first."
        ],
        [
            "Step at these problems, probably this.",
            "Let's say we just take out one of the symmetric axes and.",
            "It actually looks not too bad.",
            "It's not too good because basically one because what dimension Graph later now.",
            "So all the symmetric distances, so all the points around the X&Y.",
            "But sort of on the same page, you basically have distortion almost to Infinity, but other than that.",
            "It's kind of OK.",
            "So once."
        ],
        [
            "You have that observation.",
            "You thought ha.",
            "What if I just take a straight line across the XY dimension, then looks slightly better.",
            "So you got some point pairs that are sort of absolute Jeep absolute rubbish, but all we know is sort of looks pretty similar to the three dimensional cases."
        ],
        [
            "So before we dig into the related work, I just want to present these symbols light.",
            "So we are going to talk about so.",
            "The idea is that.",
            "Instead of learning the curvatures and gradients in these high dimensional space, random projection basically do as follows.",
            "You pick a random matrix of two servers, the estimation vectors for your high Daniels.",
            "They pay high dimensional space which in this matrix is all these points V you have endpoints here, and all of them are the dimension.",
            "And then you pick estimators abunch of K estimators A.",
            "And then you basically compute the inner product, then put it that you in the end you get a K dimensional representation of these endpoints and that squared.",
            "The over K thing that's just a scaling factor and differs for from each algorithm to each algorithm.",
            "So obviously the trick.",
            "The only thing you can do is to try to devise a smarter way to pick all these K estimator."
        ],
        [
            "Is.",
            "So the written projection story.",
            "So these are investments results like develop in the 80s by Johnson and Lindenstrauss.",
            "So basically what they say is if you have an important and that all you know you case their pairwise details, namely Elcho Dome.",
            "It is possible that no matter what the original dimensions, maybe as long as you have endpoints you lower bounds of K is about, oh, Logan.",
            "Of course you would.",
            "You would obviously subject to a penalty on distortion, but.",
            "The stories you basically can have a reasonably sharp image at all, again scale.",
            "But what they didn't do is they did this without with a nonconstructive proof.",
            "So basically they said they want they prove these.",
            "Actually they exit the probability of such an embedding exists with probability greater than zero, but without saying how that might be done.",
            "So around 98 a step he stem for PhD student.",
            "Now a professor full professor in my tea is a petrol in Dick.",
            "He published a paper is dog saying are actually all the argue.",
            "We can actually pick.",
            "All these estimates are vectors from D dimensional Gaussian distribution and they will give you these oh nice or log in balance.",
            "But the constant is still rather large, but the proof see grievous significantly simplifies the one done by Johnson Lindenstrauss and other for Kumar.",
            "Sorry.",
            "And then so there was that and then in.",
            "Three years after that, there's another guy called.",
            "Dimitri is actually Optus.",
            "He basically say he's he's working for Microsoft at that time and he thought see doing random projections.",
            "On a per a really large on a really large table is really time consuming, so he decided that.",
            "Let's try dropping 2 third of the attributes in my data in my database, and then we'll just do the projection on one third at each at each projection, and actually show that even with the sparse distribution you will still get that old Logan term and then without losing too much at a constant level, actually.",
            "It is kind of surprising and then they are.",
            "Subsequently.",
            "There are lots of work in stocks and folks talking about how smart you can make that.",
            "How smart you can make that distribution be.",
            "I think the most recent result is you can actually drop one over Square D, so you can.",
            "You can only consider one over Square D attributes in these coordinates and that will be it."
        ],
        [
            "So this talk in one slide, just in case you are, you need to use the toilet really quick or something.",
            "So I'm going to say that sparse distributions is not good for you, and last there are lots of things that you can say about that constant term in that big old notation.",
            "Actually, it's up to 40% improvement if you like, and then these scheme actually works for L2 N cosine similarity too.",
            "And then I can show it to you for both image and text data."
        ],
        [
            "Right, so this is item number one, so the Y axis of these graph is the variance is oh sorry.",
            "So basically what I did for this data set is I took, I instantiate about a million points of each of which is 1000 dimensions, and then I make it really sparse.",
            "I only make sure I make sure that all these points has only like 50 dimensions.",
            "But which part which 50 dimensions are random?",
            "And then I instantly so given these datasets, I instantiate it 10,000 instances of the projection of projection images and then randomly sample.",
            "How much distortion you get and then these distortions they are because there are 10,000 images.",
            "Of these.",
            "I measure the quality of these image both in terms of their average distortion and the variance.",
            "Is there distortion so there's partial Y axes?",
            "Are the X axes are so from these figure you can't really tell the difference between better and which by the way, is what I'm all.",
            "Ideally this paper, so you can't really tell the difference from BP and.",
            "Indic projection algorithm in this figure.",
            "But we'll we'll see about it later.",
            "But if you look at sparse random projections somehow, they managed to always pick a angle to look at the data in such a way that the variation in their quality tend to tend to be larger.",
            "Whereas if you consider everything in in these data points.",
            "Then there is a good chance where you would drive down to a much smaller variance and overcome season quality.",
            "So.",
            "Maisie Williams, saying here, is sometimes having a sparse distribution that actually means you neglect."
        ],
        [
            "Another lots of information.",
            "So now the formal bits.",
            "Basically sorry about the formality.",
            "Basically what they say is if you do if you pick your random matrixes out of a unit circular ensemble random matrix.",
            "Then I'm sorry, then here.",
            "Then here is the result will get with probability 1 -- 10 to the power from minus one, if so long is your distortion then here are the bounds of K which improves pre previous."
        ],
        [
            "The results by these 40% and to see that in a more user friendly way is like this.",
            "You give me a if so and then I will tell you that how much K that would be.",
            "Anne.",
            "One with really high with really high probability, you'll get a really sharp image without repeating.",
            "Try or anything."
        ],
        [
            "Proof strategy, so remember we have these metric views of projections where you pick each roll of your random matrix easier as he made, and then you compute the dot product.",
            "So approved strategy easy.",
            "The paper.",
            "I'm just going to.",
            "Briefing here they say each inner product you take is.",
            "Actually a bit odd.",
            "It is a random variable that obeys the beta distribution.",
            "So if you have K different such projections, then.",
            "You can approximate the distortions due to those through when case case too small for when K is too small for central limit theorem will use a beta approximation due to harness and Geary, and if it's large enough that will use central limit theorem.",
            "So actually 1 interesting coming out of this is beta.",
            "Distributions seem to have offer far better advantage to multidimensional Gaussian distribution, especially when the dimension is low.",
            "I think it's probably because the some properties do too.",
            "The beta distribution that leads to that, but is details in the paper."
        ],
        [
            "We will come to talk to me.",
            "So the key thing here is how we get these back."
        ],
        [
            "So it's really so.",
            "The idea is so if I so if each of these dimension vectors is a.",
            "Draws from the unit circular in some over random matrix.",
            "Then you are.",
            "Essentially you have a detachable sphere.",
            "You randomly pick a point and then you ask what would be a hyperplane iew adults.",
            "What's the door product will be and it turns out after some delivery calculations.",
            "These distributions, or base the geometry due to the the dimensional sphere, which after some calculation it becomes somehow you end up with a better distribution of these two para meters."
        ],
        [
            "Right, so this theory, components of the story.",
            "Now we're going to enter."
        ],
        [
            "Text, So what I did so basically I've got my evaluation methods are roughly the same as the random point sets.",
            "And then I basically pull out the trick corpus.",
            "Rounded, use Lema toolkit to extract the TF IDF and using okapi law.",
            "Then we I run it through the FBS in our times.",
            "The age range."
        ],
        [
            "Method I measure the average and variance of the distortion in terms of L2 distance, an cosine distortion.",
            "And actually I took the trouble.",
            "To ask how much agreement we have with latent semantic indexing.",
            "So just to give you an idea about what these high dimensional points look like."
        ],
        [
            "So these are the first 2 singular dimensions of the LA Times.",
            "So.",
            "You'll see that they have some sort of similarity.",
            "On the mostly Y axis and X axis only offer something like shape.",
            "Never mind, this is what the image looks like for as well as."
        ],
        [
            "Singular value decomposition is concerned, and then this is what the L2.",
            "This is a result on L2 distance, so you'll see that for the same distortion you have transis arigho better random projection tend to have quite a lot better instances that over lower distortion."
        ],
        [
            "And the same thing applies to the times."
        ],
        [
            "And then I also took the trouble to compare better rendered project with singular value with the composition and the result is rather interesting.",
            "Well, it's kind of interesting because you will see that after 20 dimensions.",
            "Actually, the results as far as similarity queries are concerned.",
            "You don't really need SVD, or you need is better because they basically offer the same amount of variation."
        ],
        [
            "And then LA Times conference the same thing where, although as far as low in this case for low dimensional cases.",
            "Singular value decomposition does offer slightly variance is still large, but at least it's better than better in the projection.",
            "But it's worthwhile noting that singular value decomposition is a ON to the power of three algorithm.",
            "Where is better in the projection?",
            "Is pseudo linear on the."
        ],
        [
            "Number of points you have.",
            "And then the agreement with LSI.",
            "So they get so close after.",
            "More than 330 after you, your image has more than 30 dimensions.",
            "So that's kind of interesting too, because LSI is basically.",
            "An optimal way to look at.",
            "To basically find your variances, who evaluated through looking at your data through all possible dimensions.",
            "And then what these shows is actually you are better off.",
            "You think you can.",
            "Eve UK is dimensions larger than 30, which by the way is what?",
            "Airways eyes all about the original paper claims that always are is meaningless before 50 dimensions."
        ],
        [
            "So the agreement is quite nice and FPS."
        ],
        [
            "Comfort is the same thing then ideally for images."
        ],
        [
            "Evolution method is the same because we are running out of time, so I'm going to just talk about the flicker results."
        ],
        [
            "So this is the principle component analysis of the images and basically out of these images we have 166 dimensional data saturation, Hue, color, histogram, you name it all in there.",
            "So if you run principle component analysis to look at these images, actually 10 oven temp or loops, 10 of them will suffice.",
            "Explaining most of the differences in your image at least 50 Piper said."
        ],
        [
            "Love them.",
            "And then if you wrong a 2 dimensional random projection out of these images and then start counting the.",
            "The spikes in this image it's roughly 8 or 9, arguably 10 here, so the lesson to take home from this image is actually that event trend in your data set is significant enough for PCA.",
            "It will be significant enough for random projection too, so you are far better off get your PCA on these projection image and you'll."
        ],
        [
            "Get a really large city bub.",
            "And for images that advantage over sparse and an Gaussian distribution boosted quite a bit.",
            "And then this when you have when you project everything onto a line.",
            "And then if you want to go a little bit more."
        ],
        [
            "Or let's say 5 dimensions.",
            "Actually that gets you gets the average distortion to quite a low pause, and yet used you can see parenting projection gives you consistent performance really quick, and then this is perhaps one of."
        ],
        [
            "Most interesting results I have in this paper in my point of view, which is so basically what these people have done here is to say let's decompose these.",
            "These image 166 dimension of yours and then we decompose it using singular value decomposition to find older single vector.",
            "Basically run his algorithm though so.",
            "Then ask as far as similarities are concerned with SVD, be better than would browsing through all possible angles be better than just trusting it to your log?",
            "And it turns out, in one dimension they are basically the same.",
            "And when you increase that by one, let's say when you have two dimensions.",
            "Actually, they says instead of looking through the data through all possible dimensions or sorry possible angles, you are far better than just trying to your log.",
            "I think the reason for this is image data offers a significant more complex geometry than Orwell, or if you like more render geometry compared with text data.",
            "So if you try to look consider all possible angles, chances are you sort of optimized one Blues on the other, so you take a chance.",
            "Actually it's a lot better as far as variation is concerned."
        ],
        [
            "Conclusion I hope that I have convinced that.",
            "Random paper random projection is good for you, and picking securing symbols are good for you too."
        ],
        [
            "Future work we're going to extend these two multi dimensional cases and we would certainly like to look into the streaming databases, cases and what I learned about last month was where actually random projection actually applies to a computational chemistry tool when you have significant much complicated.",
            "Model numerical model if you want what they said in their paper was.",
            "If you have a really complex chemistry model.",
            "And then you want to find the numerical solution to that.",
            "Actually you can all you can.",
            "Instead of solving that hard problem, you can actually project them down too.",
            "A is actually a huge portrait them down to a problem with, let's say 5 states in your PD's and then solve the solution from there and actually the distortion isn't really that large either, so I guess we are.",
            "This is probably some sort of interesting side notes about."
        ],
        [
            "Finally, acknowledgements.",
            "Thanks for Cambridge NLP Group who provide us with track data.",
            "Daniel Brain can in University of Bamberg providers with attractive sorry Flickr data USUKITA for sponsoring this work.",
            "Referees and you of course for showing up."
        ],
        [
            "Yeah.",
            "Thank you, thank you.",
            "So you're comparing the results that you get with your masturbator.",
            "Projections attack using SPD, and you said that SPD is computationally expensive, which it is, but there are randomized algorithms based on random projections that directly for computing the SVD and then give guarantees.",
            "So did you compare against those approximate SVD algorithms?",
            "No.",
            "There's a short answer, but I think the whole point there is the whole idea.",
            "There is, if you can derive a efficient shop, low dimensional presentation of anything, and then you will always get you there.",
            "So I think that's complementary to the work, because this is a random projection algorithm too.",
            "So chances are those schemes were better off by.",
            "The.",
            "OK, all those methods are based on linear projections.",
            "If there any obvious, compassionately, compassionately reasonable way of extending those for nonlinear projections or for care, no methods.",
            "So that's one thing I'm actively trying to do something about it now.",
            "So in theory.",
            "All gnomes, all metric spaces are equivalent to another in one sense or another, so there's basically automatic if you if you have a tree metric or you have L1 or Infinity norm, they are.",
            "They can emulate each other to a large sense.",
            "For kernel methods.",
            "We would have to do something about it because that would mean that all your that would mean that guaranteeing a certain curvature in your projected space, which isn't really, is not really.",
            "Guaranteed in the original theorem.",
            "So we are, you know, we're working on that, but yeah, they will be really nice extension.",
            "OK, thank you and thank you all for all the speakers."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome to the last talk of this session.",
                    "label": 0
                },
                {
                    "sent": "My name is Eric Young Lou.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "On low dimensional embeddings and similarity search today, this is a work done in jointly with Petroleo and Steven Hand in University of Cambridge, so this is part of my.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "PhD work, so the problem is really simple.",
                    "label": 1
                },
                {
                    "sent": "Endpoints all then leaving the D dimensional space equipped with the L2 norm, which which is slightly fancier way to say Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "So you are given all these poems.",
                    "label": 0
                },
                {
                    "sent": "They can be your documents that can be your images.",
                    "label": 0
                },
                {
                    "sent": "They can be audio segments, whatever.",
                    "label": 0
                },
                {
                    "sent": "And then I'm sorry I'm still in London time and I just got the freshers flu.",
                    "label": 0
                },
                {
                    "sent": "So if I'm talking gibberish just to attack me, so then you.",
                    "label": 1
                },
                {
                    "sent": "So you have you got all these endpoints and then in D dimensions and then you want to find an image of it in a much lower dimensional space K such that you want their pairwise distances to be largely largely preserved.",
                    "label": 0
                },
                {
                    "sent": "And you don't need anything else.",
                    "label": 0
                },
                {
                    "sent": "You don't care about the absolute code details what.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "So why are we doing this thing in the previous talk, speaker made a really good point about and.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor queries.",
                    "label": 0
                },
                {
                    "sent": "So these guys are notoriously hard question troubling other theoreticians for like 40 years.",
                    "label": 0
                },
                {
                    "sent": "So all so far all the older algorithms.",
                    "label": 0
                },
                {
                    "sent": "Either you either have to spend complexity on your storage or computational part.",
                    "label": 0
                },
                {
                    "sent": "So if you want constant storage, sorry linear storage then you will have to have a polynomial or even exponential query type.",
                    "label": 0
                },
                {
                    "sent": "Or vice versa.",
                    "label": 0
                },
                {
                    "sent": "So that makes.",
                    "label": 0
                },
                {
                    "sent": "Attempts any attempts that can really help us to reduce down the dimensionality really important.",
                    "label": 0
                },
                {
                    "sent": "Then on the systems perspective, and it's arguably more important, is that for large for large systems like peer to peer systems or search engines where there are millions of these thousands and thousands and thousands of machines, you don't really want to.",
                    "label": 0
                },
                {
                    "sent": "These infrastructures don't really handle multidimensional queries really well, so most attempts would be.",
                    "label": 0
                },
                {
                    "sent": "Either a we learn all the fixed points in these high dimensional space and then remember it for like ages that will spread.",
                    "label": 0
                },
                {
                    "sent": "Assign the indices according to that.",
                    "label": 0
                },
                {
                    "sent": "So that brought us neatly to the to ask question easy possible that whatever dimension there might be 1000 million features, can I derive a nice low dimensional images of those?",
                    "label": 0
                },
                {
                    "sent": "That sort of reflects?",
                    "label": 0
                },
                {
                    "sent": "The original properties in the D dimer space in such a way that it still as far as similarities are concerned, they are still quite similar.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how would you do it?",
                    "label": 0
                },
                {
                    "sent": "How high is this problem?",
                    "label": 0
                },
                {
                    "sent": "So let's consider a 3 dimensional repo and it's very pretty.",
                    "label": 0
                },
                {
                    "sent": "By the way, I spent a lot of time making this photo.",
                    "label": 0
                },
                {
                    "sent": "And then so we have a Reaper where you have symmetric circles on the X&Y axis, and then you have these big peak in the center, so you will first.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step at these problems, probably this.",
                    "label": 0
                },
                {
                    "sent": "Let's say we just take out one of the symmetric axes and.",
                    "label": 0
                },
                {
                    "sent": "It actually looks not too bad.",
                    "label": 0
                },
                {
                    "sent": "It's not too good because basically one because what dimension Graph later now.",
                    "label": 0
                },
                {
                    "sent": "So all the symmetric distances, so all the points around the X&Y.",
                    "label": 0
                },
                {
                    "sent": "But sort of on the same page, you basically have distortion almost to Infinity, but other than that.",
                    "label": 0
                },
                {
                    "sent": "It's kind of OK.",
                    "label": 0
                },
                {
                    "sent": "So once.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have that observation.",
                    "label": 0
                },
                {
                    "sent": "You thought ha.",
                    "label": 0
                },
                {
                    "sent": "What if I just take a straight line across the XY dimension, then looks slightly better.",
                    "label": 0
                },
                {
                    "sent": "So you got some point pairs that are sort of absolute Jeep absolute rubbish, but all we know is sort of looks pretty similar to the three dimensional cases.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before we dig into the related work, I just want to present these symbols light.",
                    "label": 0
                },
                {
                    "sent": "So we are going to talk about so.",
                    "label": 0
                },
                {
                    "sent": "The idea is that.",
                    "label": 0
                },
                {
                    "sent": "Instead of learning the curvatures and gradients in these high dimensional space, random projection basically do as follows.",
                    "label": 0
                },
                {
                    "sent": "You pick a random matrix of two servers, the estimation vectors for your high Daniels.",
                    "label": 1
                },
                {
                    "sent": "They pay high dimensional space which in this matrix is all these points V you have endpoints here, and all of them are the dimension.",
                    "label": 0
                },
                {
                    "sent": "And then you pick estimators abunch of K estimators A.",
                    "label": 0
                },
                {
                    "sent": "And then you basically compute the inner product, then put it that you in the end you get a K dimensional representation of these endpoints and that squared.",
                    "label": 0
                },
                {
                    "sent": "The over K thing that's just a scaling factor and differs for from each algorithm to each algorithm.",
                    "label": 0
                },
                {
                    "sent": "So obviously the trick.",
                    "label": 0
                },
                {
                    "sent": "The only thing you can do is to try to devise a smarter way to pick all these K estimator.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "So the written projection story.",
                    "label": 0
                },
                {
                    "sent": "So these are investments results like develop in the 80s by Johnson and Lindenstrauss.",
                    "label": 0
                },
                {
                    "sent": "So basically what they say is if you have an important and that all you know you case their pairwise details, namely Elcho Dome.",
                    "label": 0
                },
                {
                    "sent": "It is possible that no matter what the original dimensions, maybe as long as you have endpoints you lower bounds of K is about, oh, Logan.",
                    "label": 0
                },
                {
                    "sent": "Of course you would.",
                    "label": 0
                },
                {
                    "sent": "You would obviously subject to a penalty on distortion, but.",
                    "label": 0
                },
                {
                    "sent": "The stories you basically can have a reasonably sharp image at all, again scale.",
                    "label": 0
                },
                {
                    "sent": "But what they didn't do is they did this without with a nonconstructive proof.",
                    "label": 0
                },
                {
                    "sent": "So basically they said they want they prove these.",
                    "label": 0
                },
                {
                    "sent": "Actually they exit the probability of such an embedding exists with probability greater than zero, but without saying how that might be done.",
                    "label": 0
                },
                {
                    "sent": "So around 98 a step he stem for PhD student.",
                    "label": 0
                },
                {
                    "sent": "Now a professor full professor in my tea is a petrol in Dick.",
                    "label": 0
                },
                {
                    "sent": "He published a paper is dog saying are actually all the argue.",
                    "label": 0
                },
                {
                    "sent": "We can actually pick.",
                    "label": 0
                },
                {
                    "sent": "All these estimates are vectors from D dimensional Gaussian distribution and they will give you these oh nice or log in balance.",
                    "label": 0
                },
                {
                    "sent": "But the constant is still rather large, but the proof see grievous significantly simplifies the one done by Johnson Lindenstrauss and other for Kumar.",
                    "label": 1
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And then so there was that and then in.",
                    "label": 0
                },
                {
                    "sent": "Three years after that, there's another guy called.",
                    "label": 0
                },
                {
                    "sent": "Dimitri is actually Optus.",
                    "label": 0
                },
                {
                    "sent": "He basically say he's he's working for Microsoft at that time and he thought see doing random projections.",
                    "label": 0
                },
                {
                    "sent": "On a per a really large on a really large table is really time consuming, so he decided that.",
                    "label": 0
                },
                {
                    "sent": "Let's try dropping 2 third of the attributes in my data in my database, and then we'll just do the projection on one third at each at each projection, and actually show that even with the sparse distribution you will still get that old Logan term and then without losing too much at a constant level, actually.",
                    "label": 0
                },
                {
                    "sent": "It is kind of surprising and then they are.",
                    "label": 0
                },
                {
                    "sent": "Subsequently.",
                    "label": 0
                },
                {
                    "sent": "There are lots of work in stocks and folks talking about how smart you can make that.",
                    "label": 0
                },
                {
                    "sent": "How smart you can make that distribution be.",
                    "label": 0
                },
                {
                    "sent": "I think the most recent result is you can actually drop one over Square D, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can only consider one over Square D attributes in these coordinates and that will be it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this talk in one slide, just in case you are, you need to use the toilet really quick or something.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to say that sparse distributions is not good for you, and last there are lots of things that you can say about that constant term in that big old notation.",
                    "label": 1
                },
                {
                    "sent": "Actually, it's up to 40% improvement if you like, and then these scheme actually works for L2 N cosine similarity too.",
                    "label": 0
                },
                {
                    "sent": "And then I can show it to you for both image and text data.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this is item number one, so the Y axis of these graph is the variance is oh sorry.",
                    "label": 0
                },
                {
                    "sent": "So basically what I did for this data set is I took, I instantiate about a million points of each of which is 1000 dimensions, and then I make it really sparse.",
                    "label": 0
                },
                {
                    "sent": "I only make sure I make sure that all these points has only like 50 dimensions.",
                    "label": 0
                },
                {
                    "sent": "But which part which 50 dimensions are random?",
                    "label": 0
                },
                {
                    "sent": "And then I instantly so given these datasets, I instantiate it 10,000 instances of the projection of projection images and then randomly sample.",
                    "label": 0
                },
                {
                    "sent": "How much distortion you get and then these distortions they are because there are 10,000 images.",
                    "label": 0
                },
                {
                    "sent": "Of these.",
                    "label": 0
                },
                {
                    "sent": "I measure the quality of these image both in terms of their average distortion and the variance.",
                    "label": 0
                },
                {
                    "sent": "Is there distortion so there's partial Y axes?",
                    "label": 0
                },
                {
                    "sent": "Are the X axes are so from these figure you can't really tell the difference between better and which by the way, is what I'm all.",
                    "label": 0
                },
                {
                    "sent": "Ideally this paper, so you can't really tell the difference from BP and.",
                    "label": 0
                },
                {
                    "sent": "Indic projection algorithm in this figure.",
                    "label": 0
                },
                {
                    "sent": "But we'll we'll see about it later.",
                    "label": 0
                },
                {
                    "sent": "But if you look at sparse random projections somehow, they managed to always pick a angle to look at the data in such a way that the variation in their quality tend to tend to be larger.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you consider everything in in these data points.",
                    "label": 0
                },
                {
                    "sent": "Then there is a good chance where you would drive down to a much smaller variance and overcome season quality.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maisie Williams, saying here, is sometimes having a sparse distribution that actually means you neglect.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another lots of information.",
                    "label": 0
                },
                {
                    "sent": "So now the formal bits.",
                    "label": 0
                },
                {
                    "sent": "Basically sorry about the formality.",
                    "label": 0
                },
                {
                    "sent": "Basically what they say is if you do if you pick your random matrixes out of a unit circular ensemble random matrix.",
                    "label": 0
                },
                {
                    "sent": "Then I'm sorry, then here.",
                    "label": 0
                },
                {
                    "sent": "Then here is the result will get with probability 1 -- 10 to the power from minus one, if so long is your distortion then here are the bounds of K which improves pre previous.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results by these 40% and to see that in a more user friendly way is like this.",
                    "label": 0
                },
                {
                    "sent": "You give me a if so and then I will tell you that how much K that would be.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "One with really high with really high probability, you'll get a really sharp image without repeating.",
                    "label": 0
                },
                {
                    "sent": "Try or anything.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proof strategy, so remember we have these metric views of projections where you pick each roll of your random matrix easier as he made, and then you compute the dot product.",
                    "label": 0
                },
                {
                    "sent": "So approved strategy easy.",
                    "label": 0
                },
                {
                    "sent": "The paper.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to.",
                    "label": 0
                },
                {
                    "sent": "Briefing here they say each inner product you take is.",
                    "label": 0
                },
                {
                    "sent": "Actually a bit odd.",
                    "label": 0
                },
                {
                    "sent": "It is a random variable that obeys the beta distribution.",
                    "label": 1
                },
                {
                    "sent": "So if you have K different such projections, then.",
                    "label": 0
                },
                {
                    "sent": "You can approximate the distortions due to those through when case case too small for when K is too small for central limit theorem will use a beta approximation due to harness and Geary, and if it's large enough that will use central limit theorem.",
                    "label": 1
                },
                {
                    "sent": "So actually 1 interesting coming out of this is beta.",
                    "label": 0
                },
                {
                    "sent": "Distributions seem to have offer far better advantage to multidimensional Gaussian distribution, especially when the dimension is low.",
                    "label": 0
                },
                {
                    "sent": "I think it's probably because the some properties do too.",
                    "label": 0
                },
                {
                    "sent": "The beta distribution that leads to that, but is details in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will come to talk to me.",
                    "label": 0
                },
                {
                    "sent": "So the key thing here is how we get these back.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's really so.",
                    "label": 0
                },
                {
                    "sent": "The idea is so if I so if each of these dimension vectors is a.",
                    "label": 0
                },
                {
                    "sent": "Draws from the unit circular in some over random matrix.",
                    "label": 0
                },
                {
                    "sent": "Then you are.",
                    "label": 0
                },
                {
                    "sent": "Essentially you have a detachable sphere.",
                    "label": 0
                },
                {
                    "sent": "You randomly pick a point and then you ask what would be a hyperplane iew adults.",
                    "label": 0
                },
                {
                    "sent": "What's the door product will be and it turns out after some delivery calculations.",
                    "label": 0
                },
                {
                    "sent": "These distributions, or base the geometry due to the the dimensional sphere, which after some calculation it becomes somehow you end up with a better distribution of these two para meters.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this theory, components of the story.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to enter.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text, So what I did so basically I've got my evaluation methods are roughly the same as the random point sets.",
                    "label": 0
                },
                {
                    "sent": "And then I basically pull out the trick corpus.",
                    "label": 0
                },
                {
                    "sent": "Rounded, use Lema toolkit to extract the TF IDF and using okapi law.",
                    "label": 0
                },
                {
                    "sent": "Then we I run it through the FBS in our times.",
                    "label": 0
                },
                {
                    "sent": "The age range.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Method I measure the average and variance of the distortion in terms of L2 distance, an cosine distortion.",
                    "label": 1
                },
                {
                    "sent": "And actually I took the trouble.",
                    "label": 0
                },
                {
                    "sent": "To ask how much agreement we have with latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an idea about what these high dimensional points look like.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the first 2 singular dimensions of the LA Times.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You'll see that they have some sort of similarity.",
                    "label": 0
                },
                {
                    "sent": "On the mostly Y axis and X axis only offer something like shape.",
                    "label": 0
                },
                {
                    "sent": "Never mind, this is what the image looks like for as well as.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Singular value decomposition is concerned, and then this is what the L2.",
                    "label": 0
                },
                {
                    "sent": "This is a result on L2 distance, so you'll see that for the same distortion you have transis arigho better random projection tend to have quite a lot better instances that over lower distortion.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the same thing applies to the times.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I also took the trouble to compare better rendered project with singular value with the composition and the result is rather interesting.",
                    "label": 0
                },
                {
                    "sent": "Well, it's kind of interesting because you will see that after 20 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Actually, the results as far as similarity queries are concerned.",
                    "label": 0
                },
                {
                    "sent": "You don't really need SVD, or you need is better because they basically offer the same amount of variation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then LA Times conference the same thing where, although as far as low in this case for low dimensional cases.",
                    "label": 0
                },
                {
                    "sent": "Singular value decomposition does offer slightly variance is still large, but at least it's better than better in the projection.",
                    "label": 0
                },
                {
                    "sent": "But it's worthwhile noting that singular value decomposition is a ON to the power of three algorithm.",
                    "label": 0
                },
                {
                    "sent": "Where is better in the projection?",
                    "label": 0
                },
                {
                    "sent": "Is pseudo linear on the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Number of points you have.",
                    "label": 0
                },
                {
                    "sent": "And then the agreement with LSI.",
                    "label": 0
                },
                {
                    "sent": "So they get so close after.",
                    "label": 0
                },
                {
                    "sent": "More than 330 after you, your image has more than 30 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of interesting too, because LSI is basically.",
                    "label": 0
                },
                {
                    "sent": "An optimal way to look at.",
                    "label": 0
                },
                {
                    "sent": "To basically find your variances, who evaluated through looking at your data through all possible dimensions.",
                    "label": 0
                },
                {
                    "sent": "And then what these shows is actually you are better off.",
                    "label": 0
                },
                {
                    "sent": "You think you can.",
                    "label": 0
                },
                {
                    "sent": "Eve UK is dimensions larger than 30, which by the way is what?",
                    "label": 0
                },
                {
                    "sent": "Airways eyes all about the original paper claims that always are is meaningless before 50 dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the agreement is quite nice and FPS.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comfort is the same thing then ideally for images.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evolution method is the same because we are running out of time, so I'm going to just talk about the flicker results.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the principle component analysis of the images and basically out of these images we have 166 dimensional data saturation, Hue, color, histogram, you name it all in there.",
                    "label": 0
                },
                {
                    "sent": "So if you run principle component analysis to look at these images, actually 10 oven temp or loops, 10 of them will suffice.",
                    "label": 0
                },
                {
                    "sent": "Explaining most of the differences in your image at least 50 Piper said.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love them.",
                    "label": 0
                },
                {
                    "sent": "And then if you wrong a 2 dimensional random projection out of these images and then start counting the.",
                    "label": 0
                },
                {
                    "sent": "The spikes in this image it's roughly 8 or 9, arguably 10 here, so the lesson to take home from this image is actually that event trend in your data set is significant enough for PCA.",
                    "label": 0
                },
                {
                    "sent": "It will be significant enough for random projection too, so you are far better off get your PCA on these projection image and you'll.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get a really large city bub.",
                    "label": 0
                },
                {
                    "sent": "And for images that advantage over sparse and an Gaussian distribution boosted quite a bit.",
                    "label": 0
                },
                {
                    "sent": "And then this when you have when you project everything onto a line.",
                    "label": 0
                },
                {
                    "sent": "And then if you want to go a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or let's say 5 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Actually that gets you gets the average distortion to quite a low pause, and yet used you can see parenting projection gives you consistent performance really quick, and then this is perhaps one of.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most interesting results I have in this paper in my point of view, which is so basically what these people have done here is to say let's decompose these.",
                    "label": 0
                },
                {
                    "sent": "These image 166 dimension of yours and then we decompose it using singular value decomposition to find older single vector.",
                    "label": 0
                },
                {
                    "sent": "Basically run his algorithm though so.",
                    "label": 0
                },
                {
                    "sent": "Then ask as far as similarities are concerned with SVD, be better than would browsing through all possible angles be better than just trusting it to your log?",
                    "label": 0
                },
                {
                    "sent": "And it turns out, in one dimension they are basically the same.",
                    "label": 0
                },
                {
                    "sent": "And when you increase that by one, let's say when you have two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Actually, they says instead of looking through the data through all possible dimensions or sorry possible angles, you are far better than just trying to your log.",
                    "label": 0
                },
                {
                    "sent": "I think the reason for this is image data offers a significant more complex geometry than Orwell, or if you like more render geometry compared with text data.",
                    "label": 0
                },
                {
                    "sent": "So if you try to look consider all possible angles, chances are you sort of optimized one Blues on the other, so you take a chance.",
                    "label": 0
                },
                {
                    "sent": "Actually it's a lot better as far as variation is concerned.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusion I hope that I have convinced that.",
                    "label": 0
                },
                {
                    "sent": "Random paper random projection is good for you, and picking securing symbols are good for you too.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Future work we're going to extend these two multi dimensional cases and we would certainly like to look into the streaming databases, cases and what I learned about last month was where actually random projection actually applies to a computational chemistry tool when you have significant much complicated.",
                    "label": 1
                },
                {
                    "sent": "Model numerical model if you want what they said in their paper was.",
                    "label": 0
                },
                {
                    "sent": "If you have a really complex chemistry model.",
                    "label": 0
                },
                {
                    "sent": "And then you want to find the numerical solution to that.",
                    "label": 0
                },
                {
                    "sent": "Actually you can all you can.",
                    "label": 0
                },
                {
                    "sent": "Instead of solving that hard problem, you can actually project them down too.",
                    "label": 0
                },
                {
                    "sent": "A is actually a huge portrait them down to a problem with, let's say 5 states in your PD's and then solve the solution from there and actually the distortion isn't really that large either, so I guess we are.",
                    "label": 0
                },
                {
                    "sent": "This is probably some sort of interesting side notes about.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, acknowledgements.",
                    "label": 0
                },
                {
                    "sent": "Thanks for Cambridge NLP Group who provide us with track data.",
                    "label": 0
                },
                {
                    "sent": "Daniel Brain can in University of Bamberg providers with attractive sorry Flickr data USUKITA for sponsoring this work.",
                    "label": 1
                },
                {
                    "sent": "Referees and you of course for showing up.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "So you're comparing the results that you get with your masturbator.",
                    "label": 0
                },
                {
                    "sent": "Projections attack using SPD, and you said that SPD is computationally expensive, which it is, but there are randomized algorithms based on random projections that directly for computing the SVD and then give guarantees.",
                    "label": 0
                },
                {
                    "sent": "So did you compare against those approximate SVD algorithms?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "There's a short answer, but I think the whole point there is the whole idea.",
                    "label": 0
                },
                {
                    "sent": "There is, if you can derive a efficient shop, low dimensional presentation of anything, and then you will always get you there.",
                    "label": 0
                },
                {
                    "sent": "So I think that's complementary to the work, because this is a random projection algorithm too.",
                    "label": 0
                },
                {
                    "sent": "So chances are those schemes were better off by.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "OK, all those methods are based on linear projections.",
                    "label": 0
                },
                {
                    "sent": "If there any obvious, compassionately, compassionately reasonable way of extending those for nonlinear projections or for care, no methods.",
                    "label": 0
                },
                {
                    "sent": "So that's one thing I'm actively trying to do something about it now.",
                    "label": 0
                },
                {
                    "sent": "So in theory.",
                    "label": 0
                },
                {
                    "sent": "All gnomes, all metric spaces are equivalent to another in one sense or another, so there's basically automatic if you if you have a tree metric or you have L1 or Infinity norm, they are.",
                    "label": 0
                },
                {
                    "sent": "They can emulate each other to a large sense.",
                    "label": 0
                },
                {
                    "sent": "For kernel methods.",
                    "label": 0
                },
                {
                    "sent": "We would have to do something about it because that would mean that all your that would mean that guaranteeing a certain curvature in your projected space, which isn't really, is not really.",
                    "label": 0
                },
                {
                    "sent": "Guaranteed in the original theorem.",
                    "label": 0
                },
                {
                    "sent": "So we are, you know, we're working on that, but yeah, they will be really nice extension.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you and thank you all for all the speakers.",
                    "label": 0
                }
            ]
        }
    }
}