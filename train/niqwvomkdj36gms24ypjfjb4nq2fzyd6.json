{
    "id": "niqwvomkdj36gms24ypjfjb4nq2fzyd6",
    "title": "Semi-Supervised Domain Adaptation with Non-Parametric Copulas",
    "info": {
        "author": [
            "David Lopez-Paz, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_lopez_paz_domain_adaptation/",
    "segmentation": [
        [
            "So this is work about the main adaptation, and that means that you have access to a collection of data sets.",
            "That are different but somehow related, right?",
            "So in our particular setup, we assumed to have access to a very large sample for a source task, but then we assume to have a very limited sample for a related but different target task, so of course we're interested in estimating the model for the target task and to do so, we're interested in transferring the most amount of knowledge from the target task to the target from the source task to the target task so."
        ],
        [
            "So to get an idea on how this difference between domains may look like, we can have a look at.",
            "Real world data.",
            "This is the wiring quality problem from the UCI, and these plots depict density functions for each of the features of the data set.",
            "So in the red lines you can see the density functions for the red wine quality problem an in the black lines to the same theme for the white wine quality problem.",
            "So the data sets are clearly related, but differences between the marginal distributions can be quite large.",
            "This is also known as covariate shift, so one can go first."
        ],
        [
            "There and take a look at what happens at interactions among two random variables.",
            "In this case sugar and density for both white wine and red wine, and see that there's also room for variation across domains, so if one eliminates the marginal distribution information before taking a look at these interactions, what you're looking at is the copula of this random variables and one compare from this analysis to see how the interactions of 345 and so on from the variables interact an eventually how they change across domains."
        ],
        [
            "So it turns out that if you specify the source task and the target task learning as density estimation problems, there exists a model which actually does this same analysis that we have done, and this is called by in distributions bank popular distributions and they are nothing but the factorization of a D dimensional density P of X into a collection of D dimensional of the marginal distributions.",
            "An uptade square copula functions that are the interactions that we've seen before.",
            "So or domain adaptation algorithm is a choose the process first, we infer.",
            "Buying couple of distribution of this form from the source task and then using this little data we have available from the target task, we go factor by factor seeing which which of them have changed on which of them have remained the same.",
            "With the two sample test such as MMD.",
            "So if one of the factors have changed, you take the data from the target task raced, emitted with it and plug it into the model.",
            "If the factor has remained the same, you can just leave it there or are estimated using the Union of the source and target task data.",
            "So my favorite feature of this algorithm is that it extends very naturally to semi supervised and unsupervised minor application, and that's because if you have extra unlabeled data from a target task or incomplete or unpaired data, you can still use it to refine the factors for which you have the information for.",
            "And, um.",
            "As a side contribution of this work, we have proposed the first nonparametric regular Bang distribution, which estimates this decomposition in a fully nonparametric away, which is, by the way general purpose, very neat density estimator.",
            "So if you want to get more information about this, we will be at poster 11 at tonight's session.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is work about the main adaptation, and that means that you have access to a collection of data sets.",
                    "label": 0
                },
                {
                    "sent": "That are different but somehow related, right?",
                    "label": 0
                },
                {
                    "sent": "So in our particular setup, we assumed to have access to a very large sample for a source task, but then we assume to have a very limited sample for a related but different target task, so of course we're interested in estimating the model for the target task and to do so, we're interested in transferring the most amount of knowledge from the target task to the target from the source task to the target task so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to get an idea on how this difference between domains may look like, we can have a look at.",
                    "label": 0
                },
                {
                    "sent": "Real world data.",
                    "label": 0
                },
                {
                    "sent": "This is the wiring quality problem from the UCI, and these plots depict density functions for each of the features of the data set.",
                    "label": 0
                },
                {
                    "sent": "So in the red lines you can see the density functions for the red wine quality problem an in the black lines to the same theme for the white wine quality problem.",
                    "label": 0
                },
                {
                    "sent": "So the data sets are clearly related, but differences between the marginal distributions can be quite large.",
                    "label": 0
                },
                {
                    "sent": "This is also known as covariate shift, so one can go first.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There and take a look at what happens at interactions among two random variables.",
                    "label": 0
                },
                {
                    "sent": "In this case sugar and density for both white wine and red wine, and see that there's also room for variation across domains, so if one eliminates the marginal distribution information before taking a look at these interactions, what you're looking at is the copula of this random variables and one compare from this analysis to see how the interactions of 345 and so on from the variables interact an eventually how they change across domains.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that if you specify the source task and the target task learning as density estimation problems, there exists a model which actually does this same analysis that we have done, and this is called by in distributions bank popular distributions and they are nothing but the factorization of a D dimensional density P of X into a collection of D dimensional of the marginal distributions.",
                    "label": 0
                },
                {
                    "sent": "An uptade square copula functions that are the interactions that we've seen before.",
                    "label": 0
                },
                {
                    "sent": "So or domain adaptation algorithm is a choose the process first, we infer.",
                    "label": 1
                },
                {
                    "sent": "Buying couple of distribution of this form from the source task and then using this little data we have available from the target task, we go factor by factor seeing which which of them have changed on which of them have remained the same.",
                    "label": 0
                },
                {
                    "sent": "With the two sample test such as MMD.",
                    "label": 0
                },
                {
                    "sent": "So if one of the factors have changed, you take the data from the target task raced, emitted with it and plug it into the model.",
                    "label": 1
                },
                {
                    "sent": "If the factor has remained the same, you can just leave it there or are estimated using the Union of the source and target task data.",
                    "label": 0
                },
                {
                    "sent": "So my favorite feature of this algorithm is that it extends very naturally to semi supervised and unsupervised minor application, and that's because if you have extra unlabeled data from a target task or incomplete or unpaired data, you can still use it to refine the factors for which you have the information for.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 1
                },
                {
                    "sent": "As a side contribution of this work, we have proposed the first nonparametric regular Bang distribution, which estimates this decomposition in a fully nonparametric away, which is, by the way general purpose, very neat density estimator.",
                    "label": 0
                },
                {
                    "sent": "So if you want to get more information about this, we will be at poster 11 at tonight's session.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}