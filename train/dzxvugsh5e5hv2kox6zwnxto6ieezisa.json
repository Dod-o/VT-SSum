{
    "id": "dzxvugsh5e5hv2kox6zwnxto6ieezisa",
    "title": "Majorization for CRFs and Latent Likelihoods",
    "info": {
        "author": [
            "Anna Choromanska, Department of Electrical Engineering, Columbia University"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/machine_choromanska_majorization/",
    "segmentation": [
        [
            "Majorization is a method for optimizing the cost function.",
            "When there is no closed form solution, Majorization uses a surrogate function Q which closed form to monotonically improve from some initial value of the para meters.",
            "So first, the cost function that needs to be minimized is upper bounded by the surrogate function, and then the parameters are updated by minimizing the bound.",
            "Those two steps are repeated until convergence.",
            "Training conditional random fields and log linear models was traditionally done via iterative scaling and bound majorization methods.",
            "However, in the works of Valoran Andrew Angau, it was shown that classical majorization methods such as IIS or GIS are slower than generic 1st order and 2nd order methods such as Elegs.",
            "And it turns out that the reason for it is because I've loosened complicated bounce that earlier.",
            "Majorization methods were using.",
            "So we decided to revisit majorization and repair it slow convergence.",
            "By proposing a tide."
        ],
        [
            "Sound.",
            "So we will focus on the task of minimizing the partition function.",
            "Since this is a central quantity in many learning problems like conditional random fields or log linear models.",
            "So consider the log linear model partition function as is defined on this slide, we propose a tight quadratic bound on the lock partition function that is parameterized by the additive scholar Zine.",
            "The 2nd order term seek mom and a vector mute.",
            "It is simply a gradient, so it turns out that there is a remarkably simple algorithm for recovering the bound.",
            "Parameters and this algorithm is presented on this slide.",
            "It is quite similar to the computation of the Hessian.",
            "An first derivative, however, it contains a crucial modification such that we are able to guarantee the tight upper bound on the lock partition function noted that this is a very simple bound, yet we couldn't find it anywhere in the literature before."
        ],
        [
            "The bond admits multiple versatile extensions.",
            "These are some examples.",
            "So first of all, the band is easily recovered for graphical models and is computed using efficient message passing.",
            "Adenta bound also admits a low rank representation for where the dimensionality of the data is very high and the storage an inversion of matrix Sigma is simply impractical.",
            "The table compares the performance of the bound with the state of the art.",
            "Methods such as LBF steepest decent and conjugate gradient.",
            "On benchmark datasets, we report the runtime in seconds and the number of iterations till convergence.",
            "The 1st four columns are considering the multiclass logistic regression problem and the last two are considering conditional random fields with linear Markov chain structure.",
            "The objective function is convex.",
            "All methods converge to the same solution.",
            "However, it turns out that the bound is the fastest boat in terms of time and the number of iterations.",
            "We also made experiments with a maximum latent conditional likelihood problem.",
            "So this is the case where they are missing data.",
            "So the objective is non concave.",
            "So we're mostly interested in finding a good local maximum.",
            "The plots are showing the test likelihood versus time for the bound and many other competitor methods.",
            "The red curve depicts the bound which clearly has the fastest convergence, but remarkably it also finds much better local maximum.",
            "So to sum up, we have proposed and you and tight quadratic bound on the log partition function.",
            "That is very very simple.",
            "Yet it is a very powerful and makes majorization methods beat state of the art.",
            "Achieving fastest convergence and.",
            "Better local Optima.",
            "So if you want to learn more, please see the pastor.",
            "It's a W 63 thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Majorization is a method for optimizing the cost function.",
                    "label": 0
                },
                {
                    "sent": "When there is no closed form solution, Majorization uses a surrogate function Q which closed form to monotonically improve from some initial value of the para meters.",
                    "label": 0
                },
                {
                    "sent": "So first, the cost function that needs to be minimized is upper bounded by the surrogate function, and then the parameters are updated by minimizing the bound.",
                    "label": 0
                },
                {
                    "sent": "Those two steps are repeated until convergence.",
                    "label": 0
                },
                {
                    "sent": "Training conditional random fields and log linear models was traditionally done via iterative scaling and bound majorization methods.",
                    "label": 0
                },
                {
                    "sent": "However, in the works of Valoran Andrew Angau, it was shown that classical majorization methods such as IIS or GIS are slower than generic 1st order and 2nd order methods such as Elegs.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the reason for it is because I've loosened complicated bounce that earlier.",
                    "label": 0
                },
                {
                    "sent": "Majorization methods were using.",
                    "label": 0
                },
                {
                    "sent": "So we decided to revisit majorization and repair it slow convergence.",
                    "label": 0
                },
                {
                    "sent": "By proposing a tide.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sound.",
                    "label": 0
                },
                {
                    "sent": "So we will focus on the task of minimizing the partition function.",
                    "label": 0
                },
                {
                    "sent": "Since this is a central quantity in many learning problems like conditional random fields or log linear models.",
                    "label": 0
                },
                {
                    "sent": "So consider the log linear model partition function as is defined on this slide, we propose a tight quadratic bound on the lock partition function that is parameterized by the additive scholar Zine.",
                    "label": 0
                },
                {
                    "sent": "The 2nd order term seek mom and a vector mute.",
                    "label": 0
                },
                {
                    "sent": "It is simply a gradient, so it turns out that there is a remarkably simple algorithm for recovering the bound.",
                    "label": 0
                },
                {
                    "sent": "Parameters and this algorithm is presented on this slide.",
                    "label": 0
                },
                {
                    "sent": "It is quite similar to the computation of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "An first derivative, however, it contains a crucial modification such that we are able to guarantee the tight upper bound on the lock partition function noted that this is a very simple bound, yet we couldn't find it anywhere in the literature before.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bond admits multiple versatile extensions.",
                    "label": 0
                },
                {
                    "sent": "These are some examples.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the band is easily recovered for graphical models and is computed using efficient message passing.",
                    "label": 0
                },
                {
                    "sent": "Adenta bound also admits a low rank representation for where the dimensionality of the data is very high and the storage an inversion of matrix Sigma is simply impractical.",
                    "label": 0
                },
                {
                    "sent": "The table compares the performance of the bound with the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Methods such as LBF steepest decent and conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "On benchmark datasets, we report the runtime in seconds and the number of iterations till convergence.",
                    "label": 0
                },
                {
                    "sent": "The 1st four columns are considering the multiclass logistic regression problem and the last two are considering conditional random fields with linear Markov chain structure.",
                    "label": 0
                },
                {
                    "sent": "The objective function is convex.",
                    "label": 0
                },
                {
                    "sent": "All methods converge to the same solution.",
                    "label": 0
                },
                {
                    "sent": "However, it turns out that the bound is the fastest boat in terms of time and the number of iterations.",
                    "label": 1
                },
                {
                    "sent": "We also made experiments with a maximum latent conditional likelihood problem.",
                    "label": 0
                },
                {
                    "sent": "So this is the case where they are missing data.",
                    "label": 0
                },
                {
                    "sent": "So the objective is non concave.",
                    "label": 0
                },
                {
                    "sent": "So we're mostly interested in finding a good local maximum.",
                    "label": 0
                },
                {
                    "sent": "The plots are showing the test likelihood versus time for the bound and many other competitor methods.",
                    "label": 0
                },
                {
                    "sent": "The red curve depicts the bound which clearly has the fastest convergence, but remarkably it also finds much better local maximum.",
                    "label": 0
                },
                {
                    "sent": "So to sum up, we have proposed and you and tight quadratic bound on the log partition function.",
                    "label": 1
                },
                {
                    "sent": "That is very very simple.",
                    "label": 0
                },
                {
                    "sent": "Yet it is a very powerful and makes majorization methods beat state of the art.",
                    "label": 0
                },
                {
                    "sent": "Achieving fastest convergence and.",
                    "label": 0
                },
                {
                    "sent": "Better local Optima.",
                    "label": 0
                },
                {
                    "sent": "So if you want to learn more, please see the pastor.",
                    "label": 0
                },
                {
                    "sent": "It's a W 63 thank you.",
                    "label": 0
                }
            ]
        }
    }
}