{
    "id": "v72tklueiglwzyqw4gcsp2dbx66hhvph",
    "title": "Closed-form Supervised Dimensionality Reduction with Generalized Linear Models",
    "info": {
        "author": [
            "Irina Rish, IBM Thomas J. Watson Research Center"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/icml08_rish_cfsdr/",
    "segmentation": [
        [
            "Hi so this is joint work with my colleagues from IBM Watson Research burning and you have a chip and also with Francisco Pereira from Princeton University and his former Co advisor Jeff Gordon.",
            "OK."
        ],
        [
            "Skip one so the quicker outline.",
            "Basically I'm going to talk about supervised dimensionality reduction and give some motivating examples why one may want to do supervised rather than unsupervised dimensionality reduction, although I'm pretty sure people quite familiar with those motivations by now and I'm going to describe framework for supervised dimensionality reduction which tries to generalize some of the previous approaches and also.",
            "An algorithm which is very simple, efficient and closed form, and that was kind of the beauty of that.",
            "So say if you're trying to run your experiments before the nips deadline and you have like few hours left using some other algorithms didn't quite work, so I decided that I just need to come up with closed form an actually help to meet the deadline or the paper had to be submitted by simulator.",
            "But anyway, so I'll show empirical results are on different types of problems, and there are definitely multiple.",
            "Open issues, so I think it's very exciting area of supervised dimensionality reduction in general."
        ],
        [
            "So basically our main motivating application was analysis of very high dimensional data that come from brain imaging, particularly a functional magnetic resonance imaging data.",
            "And probably you're aware that more and more machine learning people are actually getting into this exciting area these days.",
            "But the problem with this type of data is that you typically have on the order of 10s to hundreds of thousands of variables.",
            "Or book sells 3 dimensional pixels, but you only have few hundreds of samples which actually the time points.",
            "Say the subject sits in their scanner for 20 minutes playing video game.",
            "You get about 700 times point San.",
            "You get about say 30,000 voxels Ann from those data.",
            "People try to learn multiple things.",
            "For example a classical machine learning problem.",
            "Can you predict the mental state of a person is here looking at face or a building?",
            "Or is he listening to French or Korean sentence?",
            "Or what is his emotional state?",
            "Another type of a problem is more of interest, not to machine learning people, but rather to neuro scientists like can you come up with interpretable model which provides predictive pattern which kind of characterizes certain disease says Kaiser Premium.",
            "There is also various activity just trying to build a model that would reconstruct how the brain image would look like vice versa from stimulus.",
            "And of course with this type of data, you face all the typical issues such as how do you avoid overfitting and how do you come up with model which is interpretable."
        ],
        [
            "Well among other high dimensional applications there is like this, tons of them and just to name a few.",
            "Network management, like say sensor network or peer to peer management problems.",
            "They often provide you with very high dimensional but binary data where they say the value of the feature is connectivity between pair of nodes.",
            "So it's also high dimension above the data type is binary now and your usual linear PCA may not work well.",
            "There is tons of other problems like collaborative prediction where you have basically ranking, you have the discrete valued data you have.",
            "Text data, depending on how you process it could be binary too, so there is a need for some general way of doing dimensionality reduction with multiple data types."
        ],
        [
            "And dimensionality reduction is usually attractive because it can provide visualization in two or three dimensional space.",
            "It hopefully helps you to do noise reduction in your data and hopefully it will give you some interpretable model like this cartoon picture over former I image shares that in the reality what you saw was this activation pattern on the left and actually it was a composition of two independent brain processes, the red one and the green one.",
            "And they could respond to particular time courses.",
            "And here you go.",
            "That would be nice to know that that's what's going on in the brain.",
            "Well, it's a little bit more complicated than that, of course, but on the other hand are dimensionality reduction.",
            "Maybe just a good tool to prevent overfitting?",
            "So if your main goal is still classical prediction, if you don't care about interpretability, you still may want to use dimensionality reduction for the purpose of preventing overfitting.",
            "So examples are of course numerous and I am not going to even list all of them."
        ],
        [
            "But after the question of dimensionality reduction, there is another question well.",
            "Do you want to reduce dimensionality 1st and then learn predictor or you better do those two steps together and the like common sense knowledge and actually lots of experience in this field shows that indeed if your goal is prediction you better do those steps together.",
            "Classical old example is just linear discriminate analysis versus PCA.",
            "So it's again cartoon example where it has the data and the direction of highest variance.",
            "Is this one that PCA gives you but the data belong to three classes and if you wanted to separate them.",
            "And PCA did the worst job, and LDA was orthogonal to that.",
            "So when you want to do supervised component analysis, you might be finding absolutely different components."
        ],
        [
            "OK, so if you.",
            "Think about that in general.",
            "Most of the existing supervised dimensionality reduction approaches can be viewed on this framework that people assume that there is some inherent low dimensional structure in the data and what you see is a noisy high dimensional blown up version of that true data and we also hope that this true all dimensional structure will be predictive about your class.",
            "Say particular case of PCA assumes that how you get your real data from those.",
            "True data through linear Gaussian model.",
            "Let's PCA, logistic, PCA would say well through logistic model and so on.",
            "So in general, you could assume that why won't we try to handle various data types just by using exponential family noise.",
            "So generalized linear models, both for features and class variables, and we will assume that their parameters of those distributions actually share the low dimensional structure and then our goal will be to try and learn.",
            "Both mappings simultaneously with the hidden variables in between and the hope will be there doing that just in general and supervised dimensionality reduction provides you with better results than doing those steps separately.",
            "An empirically that seems to be true indeed."
        ],
        [
            "So more specifically, well, just before we go into the details of the model, just at very high level, you can look at their existing work in supervised dimensionality reduction, and you can actually map it into the picture I showed on the previous slide, because if you think about say, support vector decomposition machine, the work by Francisco Pereira and Jim Gordon from Ice Melt, 2006.",
            "Effectively they do PC combined with this VM.",
            "So effectively they use particular types of loss or particular type of mapping for each part, and they have the only real value data, an only say binary or multivalued class.",
            "Then there is extensive work on distance metric learning and I only mentioned two representative papers, but there is of course many many more.",
            "But again, you can think about that work as finding actually a linear map from the data to hidden variables.",
            "That's effectively what happens when you learn hobbies distance.",
            "You find that linear mapping followed by learning nearest neighbor classifier but not really followed.",
            "It's again happening simultaneously.",
            "Then this work was also extended to handling regression later.",
            "Then there was a work by Sejam Alisky, which comes probably closest to our work because they would handle exponential family data.",
            "But their class variables would be only multivalent.",
            "That was basically the mixture of exponential families.",
            "Also, very relevant work diseases of course.",
            "EPC, exponential, family PC and logistic PCA because we really build on top of that, but it was kind of just a nice put those things into.",
            "That perspective of trying to learn to mappings to class and to data from home and hidden representation."
        ],
        [
            "So, um.",
            "Basically, what we do we come up with this general model that can handle mixed data types.",
            "It can actually also incorporate both linear and nonlinear mappings, because generalized linear models for non Gaussian data provide you with nonlinear mappings.",
            "It can obviously handle multiple prediction problems, classification and regression an it can naturally handle multi task learning because you can stack together multiple prediction problems and just try to learn common.",
            "A low dimensional representation.",
            "It's also again naturally extends to semi supervised learning because you can take into account the data and if it doesn't have a label then particular part of the loss function will be missing.",
            "But that's OK.",
            "So anyway, when you see the model it will be, I guess obvious.",
            "And the second plus was that the way you solve it will be actually alternating minimization, but at every iteration of your algorithm you don't really have to call any of the optimization packages like Salumi, and things like that.",
            "You can have your closed form update rules an etransfer."
        ],
        [
            "So here is a model.",
            "Basically you assume that you have your an samples the dimensions of variables and K prediction tasks.",
            "That's a.",
            "That's why multitask problem, and you assume that the data X and the labels Y are both noisy versions of set us, which are actually their natural parameters of corresponding exponential family distributions.",
            "And those distributions include.",
            "Pretty much all the useful distributions you know, like Gaussian, Bernoulli, multinomial, exponential person, you name it, so whatever you need, and the second assumption is that those natural parameters actually share common low dimensional structure.",
            "So you can think about actually doing PCA on natural parameters and then disturbing those parameters by arbitrary noise.",
            "So that's pretty much it."
        ],
        [
            "Well, another way to look at that equivalent to the previous picture is to say that each variable, the input variable or the output variable is modeled by appropriate generalized linear model regression with appropriate exponential family and say when this function, the inverse link function is identity.",
            "You just get linear regression and you get pissy when it is.",
            "Logit function, you get logistic regression and so on.",
            "Appropriately, function defines uniquely appropriate generalized linear model and corresponds uniquely to a particular type of exponential family noise.",
            "So just think about doing multiple generalized linear model regressions at the same time, assuming that your data are those hidden low dimensional true points U.",
            "So that's where to think about that."
        ],
        [
            "Then, assuming this model, what do you want?",
            "Well, you would like to find the parameters of the model that would, on one hand, minimize that data reconstruction laws.",
            "On the other hand, you would like to minimize the prediction loss, and you want to have weighted combination of both where the parameter on the reconstruction loss well in the sense it kind of works as a regularization parameter like you want to optimize prediction loss while putting.",
            "The constraint on reconstruction laws.",
            "You don't want reconstruction loss suffered too much while optimizing prediction loss.",
            "An losses in this case just simply log likelihoods of particular distributions.",
            "Or in other words, you can just say it's maximization problem where your parameters are the three matrices, the hidden data matrix U, the set of regression parameters V for their input data, and the set of regression parameters W for their class variables.",
            "So how do you solve this?"
        ],
        [
            "Type of problems.",
            "Well when the same boat with everyone who uses hidden variables.",
            "Finding optimal solution may be hard because problem is jointly nonconvex, although if you fix all the variables but one, it is convex in each variable separately.",
            "So again, just like almost everybody and this type of setting, we are using alternating minimization.",
            "You fix two matrices, find the third one and fix another to find the third one iterate.",
            "But The thing is typically what people do, they use optimization at each iteration step to solve that.",
            "Basically, to make that step they're greedy, step an intrusive.",
            "Actually, you don't necessarily always have to do that because you may get away with easier closed form solution.",
            "And that was one of."
        ],
        [
            "Contribuciones here.",
            "So if you remember this notion of auxiliary functions, say you want to minimize maximize your black function and every to do that is to come up with so called auxiliary function which will touch your objective at the current point, and otherwise it will be always below your function.",
            "So it's kind of obvious to see that if you will instead maximizes auxiliary function, you can only go up.",
            "In terms of value of your final objective.",
            "And therefore if you come up with that auxiliary function or that bound that looks.",
            "Simple, like it's quadratic.",
            "Then you take derivatives and that's it.",
            "That's your update rule."
        ],
        [
            "And that's basically what we used.",
            "Well, if you think about the variables which are real valued and you use linear regression for them, or Gaussian log likelihood, you don't have to do anything because the objective is already quadratic.",
            "If you take logistic regression or binary variables, well then you have to build on top of what people have done before.",
            "There is a particular bound that was actually proposed by Yakel and Jordan and later on use by logistic PCA.",
            "And that bond actually will be quadratic in the variable you are optimizing over.",
            "So again, the life is easy, and that's how actually that bound the red curve looks like with respect to logistic function.",
            "And actually recently this bound was extended to multinomial logistic regression.",
            "So now we can also handle multi variant class label.",
            "So."
        ],
        [
            "With those auxiliary functions in mind, the only trick was how to stack the stack them together, and that's again it's very trivial result.",
            "If you have a function or objective that is a mixture mixed combination of two other functions and for both functions they both likelihoods you have.",
            "Auxiliary functions already.",
            "You can stack them together, so basically that linear combination of auxiliary functions will be auxiliary function for linear combination of your objectives, that's it.",
            "So this allows you to pretty much stacked together dimensionality reductions of quite different type.",
            "Well, you could have like put in MF and the picture an whatever mapping for dimensionality reduction you have whatever log likelihood you have for which there is an easy exhilarate function easy.",
            "Easily differentiable bound.",
            "You can stack them together and combine them and mix and match them."
        ],
        [
            "So that's pretty much it.",
            "Well, basically you take those derivatives and that's gives you update rules.",
            "So now how did it work?",
            "So we evaluated basically this supervised dimensionality reduction and several incarnations of that versus unsupervised dimensionality reduction followed by learning predictor versus SVM, Ann versus kind of state award algorithm in this area, the support vector decomposition machines that I mentioned before that roughly combines quadratic loss.",
            "This hinge loss.",
            "Whenever I have Bernoulli STR, that means the data are Bernoulli and therefore logistic regression used for data.",
            "When I have got some corresponding later when I have IDR, that means I did unsupervised dimensionality reduction whether it was PCA or logistic PCA depends on your data and then I just stop there.",
            "I got my reduced representation and I just trained particular classifier on that whether it was logistic regression or SVM.",
            "OK, so how much time I have?",
            "OK."
        ],
        [
            "OK, so.",
            "A sanity check experiment was just to simulate the data in 2 dimensional space which had definitely separable by large margin like this, and then take those data to be your natural parameters and then blow up the data in say 1000 dimensional space by adding appropriate noise.",
            "And."
        ],
        [
            "Can you feed this simulated data to all those algorithms that I mentioned and see what happens?",
            "Well, first of all, you see that all here is classification error as a function of data dimensionality and the terrible guys ontop.",
            "Are there unsupervised dimensionality reduction followed by learning predictors.",
            "So if you do it in unsupervised way they just don't get it.",
            "Well, next to it actually was coming.",
            "The blue light, which was supervised, but it was using wrong data assumption.",
            "It was using Gaussian assumptions, say for binary data for Bernoulli noise.",
            "Next to it comes as VM, which seems to be.",
            "Perfect at low dimensions, but then it quickly loses track of things and when you blow up dimensions it gives you up to 20% error.",
            "While proper data assumption.",
            "With.",
            "Supervised dimensionality reduction stays at 0 error for up to like 700 dimensions, so that was a sanity check which kind of answers questions that you often hear.",
            "Well, that supervised dimensionality reduction do ever better than just a good classifier.",
            "Leaving aside interpret ability, if you just worry about prediction cannot ever do better than, say, swim.",
            "Well, yes, it does sometimes."
        ],
        [
            "Well, with Gaussian noise the picture was quite similar in the sense that unsupervised dimensionality reductions followed by learning classifier didn't get it.",
            "The red line was actually SVM, which surprisingly didn't behave that well.",
            "I hope Francisco won't get offended.",
            "And the best performance.",
            "We're actually SVM together with.",
            "As they are so in this case, unfortunately, as they are also didn't quite get the structure, but at least it matched as VM, so it wasn't pretty much it wasn't worse."
        ],
        [
            "And the interesting point also was how the regularization parameter, how that weight on reconstruction error affects your performance and the bottom line here was again, there is no theoretical results about that.",
            "That would be nice to have, but the rule of some is you use really relatively small regularization parameter an it's just enough.",
            "Don't put too much weight on reconstruction laws.",
            "Put most of the weight on prediction laws, but it should be just enough.",
            "But not less and not more.",
            "So there is some sharp transition I don't have.",
            "Again, any characterization of that, but that's what happened empirically."
        ],
        [
            "And then they had various experiments on real data and I probably for the sake of time have to go quickly over that bottom line is on binary data and classification tasks this was censored network connectivity prediction task.",
            "Using proper assumption that data binary use Bernoulli distribution an using supervised dimensionality reduction was beating everyone, like beating SVM by say 5%.",
            "In general, it was not extremely difficult problem as we've got like 17% error.",
            "But as the Argo 12% doing wrong assumption about the data hurts but not doing surprise.",
            "Dimensionality reduction hurts even more so that."
        ],
        [
            "Pretty much it.",
            "Another example was the fMRI example, indeed, which was indeed high dimensional 14,000, and the task was to predict whether a person is looking at to Laura building an in this case.",
            "We pretty much matched maybe outperformed a little bit SVM and CDM, but the point is that we did it in like 5 dimensions out of 14,000 and it's with the embedded only like 15 dimensions and again not doing supervised was really bad."
        ],
        [
            "And finally, our as I said, you can actually use this framework for both classification regression.",
            "We played with regression problem again.",
            "I don't have too much time to describe it, but it's Pittsburgh brain activity competition data from last year where people are playing video games while in the scanner and they do it like 3 times and after that they rate how angry they were during the game, or annoyed or whatever, whatever or they were listening to instructions and your task is.",
            "Even the former data, can you predict that the person is at this point listening to instructions being very angry, or there is a dog in the picture and person get scared.",
            "So that was a classical regression problem.",
            "Real value to real valued and the competitor was actually stay toward sparse regression.",
            "Elastic net with properly tuned parameters.",
            "And the nice thing was not only the proper SDR regression, kind of was compatible, it was even better than Elastic NET.",
            "In the regime when you don't have very high dimensionality, so it was like working even better with lower dimensional regime.",
            "Well, although it's still a preliminary result, we didn't quite do the full analysis of this data set with comparison between Elastic Net and this method, and that would be actually ideal case for multitask learning testing because you have to predict simultaneously tons of things about the person."
        ],
        [
            "OK.",
            "So basically to conclude, we kind of try to generalize various attempts to do supervised dimensionality reduction in one framework that would handle multiple data types, multiple problems and be sufficiently flexible, and the idea was to use generalized linear models with a commonly shared low dimensional structure on natural parameters, and the algorithm was really like nice and easy, and Matlab code is short and it's not online yet, but I hope to put it there.",
            "The future work is actually still huge because as I mentioned, we have auxiliary functions for several members of exponential family and it's still an open question whether you can derive generic auxiliary function for any GLM.",
            "And also there are multiple other extensions that you can see here.",
            "One of them would include making this thing sparse.",
            "Basically doing sparse GLM so you can truly then compare it to elastic net, another sparse regressions.",
            "OK, I think I'm done.",
            "Weather or the algorithm for optimizing.",
            "Because in your experimental variables you compare.",
            "Run.",
            "But yes, I ran alternating minimization and well, the contribution was in both.",
            "Proposing model that is more general, so it includes different things as particular cases and having their algorithm the simple algorithm for that.",
            "So I would say that the answer is both, yeah.",
            "Yes it was linear, yes.",
            "Yeah, so in a sense it may explain why didn't get the binary data where the true mapping was supposed to be actually nonlinear indeed, so it gives you idea.",
            "But yes, it was linear in this case."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi so this is joint work with my colleagues from IBM Watson Research burning and you have a chip and also with Francisco Pereira from Princeton University and his former Co advisor Jeff Gordon.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Skip one so the quicker outline.",
                    "label": 0
                },
                {
                    "sent": "Basically I'm going to talk about supervised dimensionality reduction and give some motivating examples why one may want to do supervised rather than unsupervised dimensionality reduction, although I'm pretty sure people quite familiar with those motivations by now and I'm going to describe framework for supervised dimensionality reduction which tries to generalize some of the previous approaches and also.",
                    "label": 0
                },
                {
                    "sent": "An algorithm which is very simple, efficient and closed form, and that was kind of the beauty of that.",
                    "label": 0
                },
                {
                    "sent": "So say if you're trying to run your experiments before the nips deadline and you have like few hours left using some other algorithms didn't quite work, so I decided that I just need to come up with closed form an actually help to meet the deadline or the paper had to be submitted by simulator.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so I'll show empirical results are on different types of problems, and there are definitely multiple.",
                    "label": 0
                },
                {
                    "sent": "Open issues, so I think it's very exciting area of supervised dimensionality reduction in general.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically our main motivating application was analysis of very high dimensional data that come from brain imaging, particularly a functional magnetic resonance imaging data.",
                    "label": 0
                },
                {
                    "sent": "And probably you're aware that more and more machine learning people are actually getting into this exciting area these days.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this type of data is that you typically have on the order of 10s to hundreds of thousands of variables.",
                    "label": 0
                },
                {
                    "sent": "Or book sells 3 dimensional pixels, but you only have few hundreds of samples which actually the time points.",
                    "label": 0
                },
                {
                    "sent": "Say the subject sits in their scanner for 20 minutes playing video game.",
                    "label": 0
                },
                {
                    "sent": "You get about 700 times point San.",
                    "label": 0
                },
                {
                    "sent": "You get about say 30,000 voxels Ann from those data.",
                    "label": 0
                },
                {
                    "sent": "People try to learn multiple things.",
                    "label": 0
                },
                {
                    "sent": "For example a classical machine learning problem.",
                    "label": 0
                },
                {
                    "sent": "Can you predict the mental state of a person is here looking at face or a building?",
                    "label": 1
                },
                {
                    "sent": "Or is he listening to French or Korean sentence?",
                    "label": 0
                },
                {
                    "sent": "Or what is his emotional state?",
                    "label": 0
                },
                {
                    "sent": "Another type of a problem is more of interest, not to machine learning people, but rather to neuro scientists like can you come up with interpretable model which provides predictive pattern which kind of characterizes certain disease says Kaiser Premium.",
                    "label": 0
                },
                {
                    "sent": "There is also various activity just trying to build a model that would reconstruct how the brain image would look like vice versa from stimulus.",
                    "label": 0
                },
                {
                    "sent": "And of course with this type of data, you face all the typical issues such as how do you avoid overfitting and how do you come up with model which is interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well among other high dimensional applications there is like this, tons of them and just to name a few.",
                    "label": 0
                },
                {
                    "sent": "Network management, like say sensor network or peer to peer management problems.",
                    "label": 0
                },
                {
                    "sent": "They often provide you with very high dimensional but binary data where they say the value of the feature is connectivity between pair of nodes.",
                    "label": 0
                },
                {
                    "sent": "So it's also high dimension above the data type is binary now and your usual linear PCA may not work well.",
                    "label": 0
                },
                {
                    "sent": "There is tons of other problems like collaborative prediction where you have basically ranking, you have the discrete valued data you have.",
                    "label": 0
                },
                {
                    "sent": "Text data, depending on how you process it could be binary too, so there is a need for some general way of doing dimensionality reduction with multiple data types.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And dimensionality reduction is usually attractive because it can provide visualization in two or three dimensional space.",
                    "label": 1
                },
                {
                    "sent": "It hopefully helps you to do noise reduction in your data and hopefully it will give you some interpretable model like this cartoon picture over former I image shares that in the reality what you saw was this activation pattern on the left and actually it was a composition of two independent brain processes, the red one and the green one.",
                    "label": 0
                },
                {
                    "sent": "And they could respond to particular time courses.",
                    "label": 0
                },
                {
                    "sent": "And here you go.",
                    "label": 0
                },
                {
                    "sent": "That would be nice to know that that's what's going on in the brain.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a little bit more complicated than that, of course, but on the other hand are dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Maybe just a good tool to prevent overfitting?",
                    "label": 1
                },
                {
                    "sent": "So if your main goal is still classical prediction, if you don't care about interpretability, you still may want to use dimensionality reduction for the purpose of preventing overfitting.",
                    "label": 0
                },
                {
                    "sent": "So examples are of course numerous and I am not going to even list all of them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But after the question of dimensionality reduction, there is another question well.",
                    "label": 1
                },
                {
                    "sent": "Do you want to reduce dimensionality 1st and then learn predictor or you better do those two steps together and the like common sense knowledge and actually lots of experience in this field shows that indeed if your goal is prediction you better do those steps together.",
                    "label": 0
                },
                {
                    "sent": "Classical old example is just linear discriminate analysis versus PCA.",
                    "label": 0
                },
                {
                    "sent": "So it's again cartoon example where it has the data and the direction of highest variance.",
                    "label": 0
                },
                {
                    "sent": "Is this one that PCA gives you but the data belong to three classes and if you wanted to separate them.",
                    "label": 1
                },
                {
                    "sent": "And PCA did the worst job, and LDA was orthogonal to that.",
                    "label": 0
                },
                {
                    "sent": "So when you want to do supervised component analysis, you might be finding absolutely different components.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if you.",
                    "label": 0
                },
                {
                    "sent": "Think about that in general.",
                    "label": 0
                },
                {
                    "sent": "Most of the existing supervised dimensionality reduction approaches can be viewed on this framework that people assume that there is some inherent low dimensional structure in the data and what you see is a noisy high dimensional blown up version of that true data and we also hope that this true all dimensional structure will be predictive about your class.",
                    "label": 1
                },
                {
                    "sent": "Say particular case of PCA assumes that how you get your real data from those.",
                    "label": 1
                },
                {
                    "sent": "True data through linear Gaussian model.",
                    "label": 0
                },
                {
                    "sent": "Let's PCA, logistic, PCA would say well through logistic model and so on.",
                    "label": 0
                },
                {
                    "sent": "So in general, you could assume that why won't we try to handle various data types just by using exponential family noise.",
                    "label": 0
                },
                {
                    "sent": "So generalized linear models, both for features and class variables, and we will assume that their parameters of those distributions actually share the low dimensional structure and then our goal will be to try and learn.",
                    "label": 0
                },
                {
                    "sent": "Both mappings simultaneously with the hidden variables in between and the hope will be there doing that just in general and supervised dimensionality reduction provides you with better results than doing those steps separately.",
                    "label": 0
                },
                {
                    "sent": "An empirically that seems to be true indeed.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more specifically, well, just before we go into the details of the model, just at very high level, you can look at their existing work in supervised dimensionality reduction, and you can actually map it into the picture I showed on the previous slide, because if you think about say, support vector decomposition machine, the work by Francisco Pereira and Jim Gordon from Ice Melt, 2006.",
                    "label": 1
                },
                {
                    "sent": "Effectively they do PC combined with this VM.",
                    "label": 0
                },
                {
                    "sent": "So effectively they use particular types of loss or particular type of mapping for each part, and they have the only real value data, an only say binary or multivalued class.",
                    "label": 1
                },
                {
                    "sent": "Then there is extensive work on distance metric learning and I only mentioned two representative papers, but there is of course many many more.",
                    "label": 1
                },
                {
                    "sent": "But again, you can think about that work as finding actually a linear map from the data to hidden variables.",
                    "label": 0
                },
                {
                    "sent": "That's effectively what happens when you learn hobbies distance.",
                    "label": 1
                },
                {
                    "sent": "You find that linear mapping followed by learning nearest neighbor classifier but not really followed.",
                    "label": 0
                },
                {
                    "sent": "It's again happening simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Then this work was also extended to handling regression later.",
                    "label": 1
                },
                {
                    "sent": "Then there was a work by Sejam Alisky, which comes probably closest to our work because they would handle exponential family data.",
                    "label": 0
                },
                {
                    "sent": "But their class variables would be only multivalent.",
                    "label": 0
                },
                {
                    "sent": "That was basically the mixture of exponential families.",
                    "label": 0
                },
                {
                    "sent": "Also, very relevant work diseases of course.",
                    "label": 0
                },
                {
                    "sent": "EPC, exponential, family PC and logistic PCA because we really build on top of that, but it was kind of just a nice put those things into.",
                    "label": 0
                },
                {
                    "sent": "That perspective of trying to learn to mappings to class and to data from home and hidden representation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Basically, what we do we come up with this general model that can handle mixed data types.",
                    "label": 1
                },
                {
                    "sent": "It can actually also incorporate both linear and nonlinear mappings, because generalized linear models for non Gaussian data provide you with nonlinear mappings.",
                    "label": 1
                },
                {
                    "sent": "It can obviously handle multiple prediction problems, classification and regression an it can naturally handle multi task learning because you can stack together multiple prediction problems and just try to learn common.",
                    "label": 0
                },
                {
                    "sent": "A low dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "It's also again naturally extends to semi supervised learning because you can take into account the data and if it doesn't have a label then particular part of the loss function will be missing.",
                    "label": 0
                },
                {
                    "sent": "But that's OK.",
                    "label": 1
                },
                {
                    "sent": "So anyway, when you see the model it will be, I guess obvious.",
                    "label": 0
                },
                {
                    "sent": "And the second plus was that the way you solve it will be actually alternating minimization, but at every iteration of your algorithm you don't really have to call any of the optimization packages like Salumi, and things like that.",
                    "label": 0
                },
                {
                    "sent": "You can have your closed form update rules an etransfer.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a model.",
                    "label": 0
                },
                {
                    "sent": "Basically you assume that you have your an samples the dimensions of variables and K prediction tasks.",
                    "label": 0
                },
                {
                    "sent": "That's a.",
                    "label": 0
                },
                {
                    "sent": "That's why multitask problem, and you assume that the data X and the labels Y are both noisy versions of set us, which are actually their natural parameters of corresponding exponential family distributions.",
                    "label": 0
                },
                {
                    "sent": "And those distributions include.",
                    "label": 0
                },
                {
                    "sent": "Pretty much all the useful distributions you know, like Gaussian, Bernoulli, multinomial, exponential person, you name it, so whatever you need, and the second assumption is that those natural parameters actually share common low dimensional structure.",
                    "label": 0
                },
                {
                    "sent": "So you can think about actually doing PCA on natural parameters and then disturbing those parameters by arbitrary noise.",
                    "label": 1
                },
                {
                    "sent": "So that's pretty much it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, another way to look at that equivalent to the previous picture is to say that each variable, the input variable or the output variable is modeled by appropriate generalized linear model regression with appropriate exponential family and say when this function, the inverse link function is identity.",
                    "label": 0
                },
                {
                    "sent": "You just get linear regression and you get pissy when it is.",
                    "label": 0
                },
                {
                    "sent": "Logit function, you get logistic regression and so on.",
                    "label": 0
                },
                {
                    "sent": "Appropriately, function defines uniquely appropriate generalized linear model and corresponds uniquely to a particular type of exponential family noise.",
                    "label": 0
                },
                {
                    "sent": "So just think about doing multiple generalized linear model regressions at the same time, assuming that your data are those hidden low dimensional true points U.",
                    "label": 0
                },
                {
                    "sent": "So that's where to think about that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then, assuming this model, what do you want?",
                    "label": 0
                },
                {
                    "sent": "Well, you would like to find the parameters of the model that would, on one hand, minimize that data reconstruction laws.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you would like to minimize the prediction loss, and you want to have weighted combination of both where the parameter on the reconstruction loss well in the sense it kind of works as a regularization parameter like you want to optimize prediction loss while putting.",
                    "label": 0
                },
                {
                    "sent": "The constraint on reconstruction laws.",
                    "label": 0
                },
                {
                    "sent": "You don't want reconstruction loss suffered too much while optimizing prediction loss.",
                    "label": 0
                },
                {
                    "sent": "An losses in this case just simply log likelihoods of particular distributions.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, you can just say it's maximization problem where your parameters are the three matrices, the hidden data matrix U, the set of regression parameters V for their input data, and the set of regression parameters W for their class variables.",
                    "label": 0
                },
                {
                    "sent": "So how do you solve this?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type of problems.",
                    "label": 0
                },
                {
                    "sent": "Well when the same boat with everyone who uses hidden variables.",
                    "label": 0
                },
                {
                    "sent": "Finding optimal solution may be hard because problem is jointly nonconvex, although if you fix all the variables but one, it is convex in each variable separately.",
                    "label": 0
                },
                {
                    "sent": "So again, just like almost everybody and this type of setting, we are using alternating minimization.",
                    "label": 0
                },
                {
                    "sent": "You fix two matrices, find the third one and fix another to find the third one iterate.",
                    "label": 0
                },
                {
                    "sent": "But The thing is typically what people do, they use optimization at each iteration step to solve that.",
                    "label": 0
                },
                {
                    "sent": "Basically, to make that step they're greedy, step an intrusive.",
                    "label": 0
                },
                {
                    "sent": "Actually, you don't necessarily always have to do that because you may get away with easier closed form solution.",
                    "label": 0
                },
                {
                    "sent": "And that was one of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Contribuciones here.",
                    "label": 0
                },
                {
                    "sent": "So if you remember this notion of auxiliary functions, say you want to minimize maximize your black function and every to do that is to come up with so called auxiliary function which will touch your objective at the current point, and otherwise it will be always below your function.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of obvious to see that if you will instead maximizes auxiliary function, you can only go up.",
                    "label": 0
                },
                {
                    "sent": "In terms of value of your final objective.",
                    "label": 0
                },
                {
                    "sent": "And therefore if you come up with that auxiliary function or that bound that looks.",
                    "label": 0
                },
                {
                    "sent": "Simple, like it's quadratic.",
                    "label": 0
                },
                {
                    "sent": "Then you take derivatives and that's it.",
                    "label": 0
                },
                {
                    "sent": "That's your update rule.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's basically what we used.",
                    "label": 0
                },
                {
                    "sent": "Well, if you think about the variables which are real valued and you use linear regression for them, or Gaussian log likelihood, you don't have to do anything because the objective is already quadratic.",
                    "label": 0
                },
                {
                    "sent": "If you take logistic regression or binary variables, well then you have to build on top of what people have done before.",
                    "label": 0
                },
                {
                    "sent": "There is a particular bound that was actually proposed by Yakel and Jordan and later on use by logistic PCA.",
                    "label": 0
                },
                {
                    "sent": "And that bond actually will be quadratic in the variable you are optimizing over.",
                    "label": 0
                },
                {
                    "sent": "So again, the life is easy, and that's how actually that bound the red curve looks like with respect to logistic function.",
                    "label": 0
                },
                {
                    "sent": "And actually recently this bound was extended to multinomial logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So now we can also handle multi variant class label.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With those auxiliary functions in mind, the only trick was how to stack the stack them together, and that's again it's very trivial result.",
                    "label": 1
                },
                {
                    "sent": "If you have a function or objective that is a mixture mixed combination of two other functions and for both functions they both likelihoods you have.",
                    "label": 0
                },
                {
                    "sent": "Auxiliary functions already.",
                    "label": 1
                },
                {
                    "sent": "You can stack them together, so basically that linear combination of auxiliary functions will be auxiliary function for linear combination of your objectives, that's it.",
                    "label": 0
                },
                {
                    "sent": "So this allows you to pretty much stacked together dimensionality reductions of quite different type.",
                    "label": 0
                },
                {
                    "sent": "Well, you could have like put in MF and the picture an whatever mapping for dimensionality reduction you have whatever log likelihood you have for which there is an easy exhilarate function easy.",
                    "label": 0
                },
                {
                    "sent": "Easily differentiable bound.",
                    "label": 0
                },
                {
                    "sent": "You can stack them together and combine them and mix and match them.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's pretty much it.",
                    "label": 0
                },
                {
                    "sent": "Well, basically you take those derivatives and that's gives you update rules.",
                    "label": 1
                },
                {
                    "sent": "So now how did it work?",
                    "label": 0
                },
                {
                    "sent": "So we evaluated basically this supervised dimensionality reduction and several incarnations of that versus unsupervised dimensionality reduction followed by learning predictor versus SVM, Ann versus kind of state award algorithm in this area, the support vector decomposition machines that I mentioned before that roughly combines quadratic loss.",
                    "label": 0
                },
                {
                    "sent": "This hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Whenever I have Bernoulli STR, that means the data are Bernoulli and therefore logistic regression used for data.",
                    "label": 0
                },
                {
                    "sent": "When I have got some corresponding later when I have IDR, that means I did unsupervised dimensionality reduction whether it was PCA or logistic PCA depends on your data and then I just stop there.",
                    "label": 0
                },
                {
                    "sent": "I got my reduced representation and I just trained particular classifier on that whether it was logistic regression or SVM.",
                    "label": 0
                },
                {
                    "sent": "OK, so how much time I have?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "A sanity check experiment was just to simulate the data in 2 dimensional space which had definitely separable by large margin like this, and then take those data to be your natural parameters and then blow up the data in say 1000 dimensional space by adding appropriate noise.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can you feed this simulated data to all those algorithms that I mentioned and see what happens?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, you see that all here is classification error as a function of data dimensionality and the terrible guys ontop.",
                    "label": 0
                },
                {
                    "sent": "Are there unsupervised dimensionality reduction followed by learning predictors.",
                    "label": 0
                },
                {
                    "sent": "So if you do it in unsupervised way they just don't get it.",
                    "label": 0
                },
                {
                    "sent": "Well, next to it actually was coming.",
                    "label": 0
                },
                {
                    "sent": "The blue light, which was supervised, but it was using wrong data assumption.",
                    "label": 0
                },
                {
                    "sent": "It was using Gaussian assumptions, say for binary data for Bernoulli noise.",
                    "label": 1
                },
                {
                    "sent": "Next to it comes as VM, which seems to be.",
                    "label": 0
                },
                {
                    "sent": "Perfect at low dimensions, but then it quickly loses track of things and when you blow up dimensions it gives you up to 20% error.",
                    "label": 0
                },
                {
                    "sent": "While proper data assumption.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "Supervised dimensionality reduction stays at 0 error for up to like 700 dimensions, so that was a sanity check which kind of answers questions that you often hear.",
                    "label": 0
                },
                {
                    "sent": "Well, that supervised dimensionality reduction do ever better than just a good classifier.",
                    "label": 0
                },
                {
                    "sent": "Leaving aside interpret ability, if you just worry about prediction cannot ever do better than, say, swim.",
                    "label": 0
                },
                {
                    "sent": "Well, yes, it does sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, with Gaussian noise the picture was quite similar in the sense that unsupervised dimensionality reductions followed by learning classifier didn't get it.",
                    "label": 0
                },
                {
                    "sent": "The red line was actually SVM, which surprisingly didn't behave that well.",
                    "label": 0
                },
                {
                    "sent": "I hope Francisco won't get offended.",
                    "label": 0
                },
                {
                    "sent": "And the best performance.",
                    "label": 0
                },
                {
                    "sent": "We're actually SVM together with.",
                    "label": 0
                },
                {
                    "sent": "As they are so in this case, unfortunately, as they are also didn't quite get the structure, but at least it matched as VM, so it wasn't pretty much it wasn't worse.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the interesting point also was how the regularization parameter, how that weight on reconstruction error affects your performance and the bottom line here was again, there is no theoretical results about that.",
                    "label": 0
                },
                {
                    "sent": "That would be nice to have, but the rule of some is you use really relatively small regularization parameter an it's just enough.",
                    "label": 0
                },
                {
                    "sent": "Don't put too much weight on reconstruction laws.",
                    "label": 1
                },
                {
                    "sent": "Put most of the weight on prediction laws, but it should be just enough.",
                    "label": 0
                },
                {
                    "sent": "But not less and not more.",
                    "label": 0
                },
                {
                    "sent": "So there is some sharp transition I don't have.",
                    "label": 0
                },
                {
                    "sent": "Again, any characterization of that, but that's what happened empirically.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then they had various experiments on real data and I probably for the sake of time have to go quickly over that bottom line is on binary data and classification tasks this was censored network connectivity prediction task.",
                    "label": 0
                },
                {
                    "sent": "Using proper assumption that data binary use Bernoulli distribution an using supervised dimensionality reduction was beating everyone, like beating SVM by say 5%.",
                    "label": 0
                },
                {
                    "sent": "In general, it was not extremely difficult problem as we've got like 17% error.",
                    "label": 0
                },
                {
                    "sent": "But as the Argo 12% doing wrong assumption about the data hurts but not doing surprise.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality reduction hurts even more so that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty much it.",
                    "label": 0
                },
                {
                    "sent": "Another example was the fMRI example, indeed, which was indeed high dimensional 14,000, and the task was to predict whether a person is looking at to Laura building an in this case.",
                    "label": 0
                },
                {
                    "sent": "We pretty much matched maybe outperformed a little bit SVM and CDM, but the point is that we did it in like 5 dimensions out of 14,000 and it's with the embedded only like 15 dimensions and again not doing supervised was really bad.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, our as I said, you can actually use this framework for both classification regression.",
                    "label": 0
                },
                {
                    "sent": "We played with regression problem again.",
                    "label": 0
                },
                {
                    "sent": "I don't have too much time to describe it, but it's Pittsburgh brain activity competition data from last year where people are playing video games while in the scanner and they do it like 3 times and after that they rate how angry they were during the game, or annoyed or whatever, whatever or they were listening to instructions and your task is.",
                    "label": 1
                },
                {
                    "sent": "Even the former data, can you predict that the person is at this point listening to instructions being very angry, or there is a dog in the picture and person get scared.",
                    "label": 0
                },
                {
                    "sent": "So that was a classical regression problem.",
                    "label": 0
                },
                {
                    "sent": "Real value to real valued and the competitor was actually stay toward sparse regression.",
                    "label": 1
                },
                {
                    "sent": "Elastic net with properly tuned parameters.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing was not only the proper SDR regression, kind of was compatible, it was even better than Elastic NET.",
                    "label": 0
                },
                {
                    "sent": "In the regime when you don't have very high dimensionality, so it was like working even better with lower dimensional regime.",
                    "label": 0
                },
                {
                    "sent": "Well, although it's still a preliminary result, we didn't quite do the full analysis of this data set with comparison between Elastic Net and this method, and that would be actually ideal case for multitask learning testing because you have to predict simultaneously tons of things about the person.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically to conclude, we kind of try to generalize various attempts to do supervised dimensionality reduction in one framework that would handle multiple data types, multiple problems and be sufficiently flexible, and the idea was to use generalized linear models with a commonly shared low dimensional structure on natural parameters, and the algorithm was really like nice and easy, and Matlab code is short and it's not online yet, but I hope to put it there.",
                    "label": 1
                },
                {
                    "sent": "The future work is actually still huge because as I mentioned, we have auxiliary functions for several members of exponential family and it's still an open question whether you can derive generic auxiliary function for any GLM.",
                    "label": 0
                },
                {
                    "sent": "And also there are multiple other extensions that you can see here.",
                    "label": 0
                },
                {
                    "sent": "One of them would include making this thing sparse.",
                    "label": 0
                },
                {
                    "sent": "Basically doing sparse GLM so you can truly then compare it to elastic net, another sparse regressions.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm done.",
                    "label": 0
                },
                {
                    "sent": "Weather or the algorithm for optimizing.",
                    "label": 0
                },
                {
                    "sent": "Because in your experimental variables you compare.",
                    "label": 0
                },
                {
                    "sent": "Run.",
                    "label": 0
                },
                {
                    "sent": "But yes, I ran alternating minimization and well, the contribution was in both.",
                    "label": 0
                },
                {
                    "sent": "Proposing model that is more general, so it includes different things as particular cases and having their algorithm the simple algorithm for that.",
                    "label": 0
                },
                {
                    "sent": "So I would say that the answer is both, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes it was linear, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in a sense it may explain why didn't get the binary data where the true mapping was supposed to be actually nonlinear indeed, so it gives you idea.",
                    "label": 0
                },
                {
                    "sent": "But yes, it was linear in this case.",
                    "label": 0
                }
            ]
        }
    }
}