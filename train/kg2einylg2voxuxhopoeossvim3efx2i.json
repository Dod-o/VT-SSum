{
    "id": "kg2einylg2voxuxhopoeossvim3efx2i",
    "title": "Regret Bounds for the Adaptive Control of Linear Quadratic Systems",
    "info": {
        "author": [
            "Csaba Szepesv\u00e1ri, Department of Computing Science, University of Alberta"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Technology->Engineering->Electrical Engineering"
        ]
    },
    "url": "http://videolectures.net/colt2011_szepesvari_regret/",
    "segmentation": [
        [
            "This is joint work with Yasin Abbasi at Korean.",
            "The story here is different, so he's not in London at Literature conference, but he's Iranian poor guy, so he likes to stay in Canada in the winter and science.",
            "Actually, I don't know if I would prefer to be there right at the moment or not.",
            "It's not true.",
            "Anyhow, so he had some visa problems so you can make it.",
            "So the paper is about very simple problem, linear quadratic regulation and we are the most surprised that this problem has kind of over has been kind of overlooked in the literature and no one actually care to prove regret bounds for it.",
            "And then we tried hard to search in the literature and we have indeed identified control papers.",
            "I'm going to talk about those.",
            "But it's kind of neat.",
            "On the other hand, because it's a nice little problem, it has a very cool structure and we can get pretty nice."
        ],
        [
            "Deadlines.",
            "OK, so this is again control, but right now.",
            "We are putting on the other hand, when we are analyzing control in terms of the regret.",
            "And we're looking at a very specific class of systems.",
            "You have lots of structure to exploit, and that's kind of beautiful."
        ],
        [
            "Alright, so this should have been the maybe slide in the previous talk as well.",
            "So what is a control problem?",
            "So you have, you know, an H and an environment and the environment is giving a state to the agent so it gives tribulus its state.",
            "That's pretty generous of the environment in general.",
            "It won't do that and then the agent.",
            "Find certain action and send it back to the environment and the environment somehow response to the action so the state is modified.",
            "Maybe there is some noise process acting on the environment as fast so WT is the noise.",
            "Alright, so and then this loop continues and the agent score is in this talk not to maximize reward, because right now we're putting on or control head.",
            "We're minimizing costs to control.",
            "People are pessimistic.",
            "They like to minimize cost."
        ],
        [
            "Or just tradition?",
            "I don't know like Second World War.",
            "So the problem in learning to control as opposed to just planning for control is that you don't know the system dynamics.",
            "Maybe you know the cost function, but if you don't know it, you could learn it, so that's not a big deal, but if you don't know the system dynamic that tends to be a problem, so you want to learn about the system by controlling it nicely."
        ],
        [
            "And you want to access as well almost as well as if you knew the system dynamics from the beginning of time.",
            "So that's the goal.",
            "And so the questions you might ask like, OK, if I have this controller.",
            "I measure my average cost along the trajectory that my controllers following is the average cost converging to the optimal cost.",
            "If the answer is yes and hopefully it will be yes for the algorithms that we're studying, you can ask the next question, how fast is the convergence rate, and then you can come up with various measures for measuring that one measure is just the regret.",
            "So it's like the difference in terms of the cost that the agent suffers versus the total cost that would have been suffered.",
            "If the agent acted in optimal fashion.",
            "From the beginning of time and you have this concept like consistency ahead and causes tensive and regret per time step is vanishing then the agents learning fast and the typical result would be that you know the regret is growing in a sub linear fashion with some exponent and we'd like to find out what the exponent is."
        ],
        [
            "So in this talk I'm going to talk about the problem of linear quadratic regulation, or when the system dynamic state takes this linear form.",
            "So right now the state and the action are both continues, so that's like a nice thing about this.",
            "So the state is N dimensional countries.",
            "The dimension and you have these matrices and OK, so I'm missing here the plus noise.",
            "The plus WT plus one.",
            "Sorry about that.",
            "And that's the system dynamics.",
            "So it's like, you know, a linear function of the state and the action plus some noise acting and resigned.",
            "Added to the result and the cost.",
            "Is going to be a quality cost, so that's like the standard keys that people like to study in control.",
            "And we assume that the noise sequences sub Goshen Martin get difference noise sequence and for simplicity we assume that the underlying covariance matrix identity matrix.",
            "So the IQR problem itself in control is that given you know all information about the dynamics, find an optimal control area that has been on solutions.",
            "It's been studied since ages.",
            "The accurate learning problem is that you are maybe just given Q&R which determine the cost function, but you don't know the dynamics and you want to learn about the dynamics and still achieve it."
        ],
        [
            "3 grand alright, so why should we care?",
            "So there's the goal, so design controller, which achieves lower regret for reasonably large class of vascular problems.",
            "Well, I care about this because this is simple and I like simple and beautiful nice structures and we have Katia States and controls finance so we have lots of traveling reinforcement, learning proving desires, regret results.",
            "For those cases it's not so easy and it's actually useful.",
            "So in my robotics class the students actually design an accurate based controller to control mini segues and it works.",
            "It works in practice.",
            "And.",
            "Very interesting Lee.",
            "We found that these problems been unsolved, so it was actually."
        ],
        [
            "Surprise for us.",
            "So a little bit about previous works, so there's been a lot of analysis for finite MDP's and a little analysis for infinite NDPS under some leashes assumption in a different framework and.",
            "Fish there Victor was one of the first guys who actually analyzed the linear quadratic regulation problem, but in a different framework, so he didn't look at regret.",
            "And then in the control literature.",
            "Of course you find what is brilliant.",
            "People like Lion, van Chen Guo and Sung and England they proved mostly consistency are actually just consistent results for first exploration, like epsilon, greedy and more recently this.",
            "Campion Kumar and then then later Bitonti and campy proof consistency results.",
            "For an algorithm which is pretty close to the algorithm that I'm going to show you.",
            "And so, or big inspirations coming from Lion Robins, who proposed this principle in the face of uncertainty for bandit problems.",
            "And we are going to recycle this idea.",
            "So that's one main ingredient and the other main ingredient is like a linear estimation theory inequality's for regression problems that we will try to improve a little bit."
        ],
        [
            "Recycle."
        ],
        [
            "So the main ideas for the algorithm, so it's very simple.",
            "We follow the footsteps of previous people.",
            "We estimate the system dynamics and we are optimistic in selecting the controls and we avoid frequent changes to the policy."
        ],
        [
            "Alright, so the estimation problem.",
            "So how do we do these steps?",
            "So the first question is, how do we estimate the system dynamics?",
            "Well, you can immediately see is that like, given that you know the previous state, the previous action, you know the next action.",
            "You can just reply the horsing selliner estimation problem so you have this coverage set of T and you have the noise acting on it, so you observe XD plus one you know exactly and you're trying to estimate that data stars.",
            "That's a linear estimation problem.",
            "Pretty standard, pretty easy.",
            "So that works so far, so you can just use, you know Lee Squares or regularised least squares.",
            "So you have this data and you set up this regression problem and so eventually we used retrogression.",
            "OK, a little bit of regularization doesn't hurt."
        ],
        [
            "OK, so.",
            "The next ingredient is the optimism principle, so it's not enough just to estimate the parameters.",
            "You gotta know like how which parameters are feasible and which are infeasible.",
            "Right, and that will depend on, like in which direction did you see how many covariates and so we set up confidence that it's going to be an ellipsoid, and this parameter that we're talking about is actually a matrix, so it's in a.",
            "It's another signal space of matrices.",
            "What there is that and so this confidence set is like with probability 1 minus data contains the true parameter is going to be constructed like that.",
            "That's where we are going to use stalina qualities and then choose the controller which just gives the best performance.",
            "That's the principle of optimism in the face of uncertainty.",
            "And so how do we do this?",
            "So we need a little bit of notation on too much, so a given parameter, parametrization of Theatre and JFT toilets JLT to denote the optimal average cost, an piasty to be the corresponding optimal policy.",
            "And So what you do is that you first find within the confidence set the best parameter which, like you know, optimistic that it gives you the best average cost in the long run, and then given the identified parameter you just follow the underlying policy and that policy has a very simple form, so that's very well known from control theory.",
            "So there are a few caveats here though, so both GF teeth and by FT talk can be undefined if ttar is not nice, so average cost it can blow up it's bonded state.",
            "So you gotta be careful there.",
            "So we're putting enough technical restrictions so that this doesn't happen, so I don't want to go into the details.",
            "And then the next caveat is that actually finding Tita Theodote might be a difficult problem, so that's kind of unfortunate Willie."
        ],
        [
            "To work more about that, but that's.",
            "We're sweeping this under the rug for the moment.",
            "And so the next ingredient is avoiding frequent changes that kind of unnecessary, and then saving computation is, isn't it?",
            "And?"
        ],
        [
            "And then most importantly, frequent changes seems to be a problem with the proof.",
            "So first we try to analyze the algorithm that allows frequent changes and you will see that we run."
        ],
        [
            "Travis, OK, so one of the cool things that I've learned by doing this thing is from the book of delapenha at R. The method of mixtures which which you can prove so called staff normalized inequalities which are really cool tight.",
            "Tighter than previous inequalities and so in the case of free tradition, when you have this matrix, parameters at inequality would look like this.",
            "So you have your data.",
            "You estimate this Ridge regression parameters, you have your covariance matrix and then you have that this inequality holds with probability 1 minus data and they are the right quantities.",
            "That's my feeling about it, but we can talk about that later in private, but I don't want to get into the details of that at the moment.",
            "So it's at at minimum dimension free, almost except for."
        ],
        [
            "But that day has to be there.",
            "Alright, so the confidence set has this property that OK, you can view it as centered around Tito Standard centered around it ahead and so if you sent it around teeth ahead and you have an empirical reconstructed confidence set with hyperoptic contract contains the true parameter OK?",
            "So that's good.",
            "So we have the confidence."
        ],
        [
            "That's that was one ingredient, and so you just do whatever I said before so it just, you know, calculate the regression and then be optimistic and follow the control and then County."
        ],
        [
            "So that's the algorithm, and so the proof sketches here.",
            "So the most of the work of the proof that I'm not going to talk about at all in the rest of the talk is to show that the state actually stays bounded.",
            "So the state doesn't escape to infinite if it escapes too fast to Infinity, you'll be travel, so you need to work hard to do that.",
            "After you did that, you are on the right track, so you can prove the regret bonds you just do."
        ],
        [
            "That in the composition of the regret.",
            "It's like here, so easy just use the balmenach creations and then a little algebra and then you can have you can end up with this decomposition.",
            "Regret this step.",
            "I'm showing that because this is where we are using the optimism right?",
            "So GF so this inequality holds when the confidence sets do not fair and when the confidence sets do not fail.",
            "Anteater studies obvious in the confidence set and then.",
            "This is always chosen an optimistic fashion that means the cost underlying cost is smaller than at it this time.",
            "So that's why you're using optimistic property and then you have to."
        ],
        [
            "Lies this various terms, so here we go.",
            "So the first term you just regroup and then you notice that is emerging.",
            "Add different sequence or actually this is a martingale.",
            "Almost the martingale and then state doesn't explore.",
            "Then you're done like square root tea.",
            "So."
        ],
        [
            "So the other term, like the third term, we work hard, you reduce it to this other form and then you do more algebra, user confidence and state doesn't exploit.",
            "It's good, OK?"
        ],
        [
            "You gotta trust.",
            "It's kind of lengthy calculate complicated calculations and then there is this lovely term, so in this term you see the state is correlated through like you measure the size of the state with respect to the difference between these two P matrices.",
            "I didn't even define them their solutions to be cut decorations, what not?",
            "I don't care, it somehow depends on the parameters and you want to analyze like how fast this thing changes.",
            "That's what it boils down to.",
            "And like we don't know how to do that.",
            "So what do we do?",
            "Fix the algorithm right so this just talks about this changes.",
            "So this matrix tightly is bonded.",
            "So if you don't change the policy to off and then these parameters stay the same for a long period of time, like you have logarithmic number of changes, you should be fine.",
            "So what you doing?"
        ],
        [
            "You go back to the algorithm and change it such that you just wait until the determinant of the confidence, and if so it doubles and when it doubled then you change to a new policy and then you can show that at least.",
            "So the number of change."
        ],
        [
            "It cannot be more than logarithmic a number of time steps, so that's the algorithm.",
            "So you just modify it in these two steps and then otherwise it's identical.",
            "Alright, so with this that term can be handled."
        ],
        [
            "You answer we have a nice regret bond which reads like this.",
            "So this property one my mind is that or the regret of the algorithm is bonded like square root T log 1 / D one over that."
        ],
        [
            "So we're happy about this paper because there's a 1st result for this nice little problem of controlling linear systems, which this quadratic cost and.",
            "It's.",
            "It's been a pleasure to work on this problem because it has such a nice structure and we learn a lot.",
            "And one problem though is that the algorithm is a little bit too expensive, so it's very very expensive, so it helps a lot that you're not changing the policy that often.",
            "So I was joking about it, but it is actually helpful.",
            "And so the question is, does there exist a cheaper alternative?",
            "Is similar guarantees like?",
            "I suspect that you change the confidence that you make it a little bit more manageable than you will lose a little bit, maybe a constant factor, and everything is going to work fine computational efficiently as well.",
            "So one and the travel that control people would immediately ask, is that OK?",
            "You're making this morning and always assumption, and that's not natural in certain applications, and So what can you say in the other case?",
            "Not much, sorry, I think that.",
            "At least that's out of my reach at the one.",
            "Uh.",
            "So extension tooling are parameterized systems, so that looks like a logical next step, so that's the system dynamics and well in that case the question will be can you ever plan with systems like that?",
            "And if you can plan then value can of course learn an.",
            "Probably you can prove things, but you have to be careful.",
            "So if I is, my feeling is that if I has a bounded growth condition, like maybe Lifshitz Ness.",
            "In terms of States and the the actions, then you'll be fine.",
            "Otherwise the state could just explode, right?",
            "So it could be really sensitive system and it's not uncommon in the control literature either to see results under those conditions, so I wouldn't be surprised if we actually would need to make such an assumption.",
            "But that would be kind of limit.",
            "Sad, but yeah we would survive I guess.",
            "And then under Elizabeth case, so connecting back to the previous talk.",
            "So this talks about the case when you have a system dynamics and then you assume that Foreman, Yuna lies it under the assumption that the system is reacting like that.",
            "So what happens if you have an order dynamics?",
            "Marburg alright thank you."
        ],
        [
            "OK I have a question.",
            "So what kind of algorithms are used for this problem in the control literature an is it true that the you can't prove regret?",
            "Do you have like counterexamples, that you can't prove regret bounds for those kinds of algorithms?",
            "Do they do so?",
            "So people the only people who actually variable to regret bonds are?",
            "Like the ones who listed on this slide or the control people, including the control people that we know of and like verifying to each others, and there are no references out, so that's how you kind of discount this thing right?",
            "And so either papers.",
            "Like Blowe of course proofs.",
            "Lots of regret bonds, but not for this case for some reason.",
            "And be content company.",
            "We're actually analyzing an algorithm very similar to yours.",
            "It's a little bit more complicated than ours.",
            "So you could say that we refine their algorithm and and made the regret analysis work for it.",
            "And other than that, it's kind of.",
            "Fascinating that control people lots of time.",
            "Just assume some system dynamics and then it kind of works and then they are happy with that.",
            "So they're not doing the learning some people do, but they don't prove regret bounds.",
            "Anymore questions.",
            "OK, so let's attack the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Yasin Abbasi at Korean.",
                    "label": 0
                },
                {
                    "sent": "The story here is different, so he's not in London at Literature conference, but he's Iranian poor guy, so he likes to stay in Canada in the winter and science.",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't know if I would prefer to be there right at the moment or not.",
                    "label": 0
                },
                {
                    "sent": "It's not true.",
                    "label": 0
                },
                {
                    "sent": "Anyhow, so he had some visa problems so you can make it.",
                    "label": 0
                },
                {
                    "sent": "So the paper is about very simple problem, linear quadratic regulation and we are the most surprised that this problem has kind of over has been kind of overlooked in the literature and no one actually care to prove regret bounds for it.",
                    "label": 0
                },
                {
                    "sent": "And then we tried hard to search in the literature and we have indeed identified control papers.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about those.",
                    "label": 0
                },
                {
                    "sent": "But it's kind of neat.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, because it's a nice little problem, it has a very cool structure and we can get pretty nice.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deadlines.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is again control, but right now.",
                    "label": 0
                },
                {
                    "sent": "We are putting on the other hand, when we are analyzing control in terms of the regret.",
                    "label": 0
                },
                {
                    "sent": "And we're looking at a very specific class of systems.",
                    "label": 0
                },
                {
                    "sent": "You have lots of structure to exploit, and that's kind of beautiful.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this should have been the maybe slide in the previous talk as well.",
                    "label": 0
                },
                {
                    "sent": "So what is a control problem?",
                    "label": 0
                },
                {
                    "sent": "So you have, you know, an H and an environment and the environment is giving a state to the agent so it gives tribulus its state.",
                    "label": 0
                },
                {
                    "sent": "That's pretty generous of the environment in general.",
                    "label": 0
                },
                {
                    "sent": "It won't do that and then the agent.",
                    "label": 0
                },
                {
                    "sent": "Find certain action and send it back to the environment and the environment somehow response to the action so the state is modified.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is some noise process acting on the environment as fast so WT is the noise.",
                    "label": 0
                },
                {
                    "sent": "Alright, so and then this loop continues and the agent score is in this talk not to maximize reward, because right now we're putting on or control head.",
                    "label": 0
                },
                {
                    "sent": "We're minimizing costs to control.",
                    "label": 0
                },
                {
                    "sent": "People are pessimistic.",
                    "label": 0
                },
                {
                    "sent": "They like to minimize cost.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or just tradition?",
                    "label": 0
                },
                {
                    "sent": "I don't know like Second World War.",
                    "label": 0
                },
                {
                    "sent": "So the problem in learning to control as opposed to just planning for control is that you don't know the system dynamics.",
                    "label": 1
                },
                {
                    "sent": "Maybe you know the cost function, but if you don't know it, you could learn it, so that's not a big deal, but if you don't know the system dynamic that tends to be a problem, so you want to learn about the system by controlling it nicely.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you want to access as well almost as well as if you knew the system dynamics from the beginning of time.",
                    "label": 0
                },
                {
                    "sent": "So that's the goal.",
                    "label": 0
                },
                {
                    "sent": "And so the questions you might ask like, OK, if I have this controller.",
                    "label": 0
                },
                {
                    "sent": "I measure my average cost along the trajectory that my controllers following is the average cost converging to the optimal cost.",
                    "label": 1
                },
                {
                    "sent": "If the answer is yes and hopefully it will be yes for the algorithms that we're studying, you can ask the next question, how fast is the convergence rate, and then you can come up with various measures for measuring that one measure is just the regret.",
                    "label": 1
                },
                {
                    "sent": "So it's like the difference in terms of the cost that the agent suffers versus the total cost that would have been suffered.",
                    "label": 0
                },
                {
                    "sent": "If the agent acted in optimal fashion.",
                    "label": 0
                },
                {
                    "sent": "From the beginning of time and you have this concept like consistency ahead and causes tensive and regret per time step is vanishing then the agents learning fast and the typical result would be that you know the regret is growing in a sub linear fashion with some exponent and we'd like to find out what the exponent is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk I'm going to talk about the problem of linear quadratic regulation, or when the system dynamic state takes this linear form.",
                    "label": 1
                },
                {
                    "sent": "So right now the state and the action are both continues, so that's like a nice thing about this.",
                    "label": 0
                },
                {
                    "sent": "So the state is N dimensional countries.",
                    "label": 0
                },
                {
                    "sent": "The dimension and you have these matrices and OK, so I'm missing here the plus noise.",
                    "label": 0
                },
                {
                    "sent": "The plus WT plus one.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "And that's the system dynamics.",
                    "label": 0
                },
                {
                    "sent": "So it's like, you know, a linear function of the state and the action plus some noise acting and resigned.",
                    "label": 0
                },
                {
                    "sent": "Added to the result and the cost.",
                    "label": 0
                },
                {
                    "sent": "Is going to be a quality cost, so that's like the standard keys that people like to study in control.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the noise sequences sub Goshen Martin get difference noise sequence and for simplicity we assume that the underlying covariance matrix identity matrix.",
                    "label": 1
                },
                {
                    "sent": "So the IQR problem itself in control is that given you know all information about the dynamics, find an optimal control area that has been on solutions.",
                    "label": 0
                },
                {
                    "sent": "It's been studied since ages.",
                    "label": 0
                },
                {
                    "sent": "The accurate learning problem is that you are maybe just given Q&R which determine the cost function, but you don't know the dynamics and you want to learn about the dynamics and still achieve it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "3 grand alright, so why should we care?",
                    "label": 1
                },
                {
                    "sent": "So there's the goal, so design controller, which achieves lower regret for reasonably large class of vascular problems.",
                    "label": 1
                },
                {
                    "sent": "Well, I care about this because this is simple and I like simple and beautiful nice structures and we have Katia States and controls finance so we have lots of traveling reinforcement, learning proving desires, regret results.",
                    "label": 0
                },
                {
                    "sent": "For those cases it's not so easy and it's actually useful.",
                    "label": 0
                },
                {
                    "sent": "So in my robotics class the students actually design an accurate based controller to control mini segues and it works.",
                    "label": 0
                },
                {
                    "sent": "It works in practice.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Very interesting Lee.",
                    "label": 0
                },
                {
                    "sent": "We found that these problems been unsolved, so it was actually.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Surprise for us.",
                    "label": 0
                },
                {
                    "sent": "So a little bit about previous works, so there's been a lot of analysis for finite MDP's and a little analysis for infinite NDPS under some leashes assumption in a different framework and.",
                    "label": 1
                },
                {
                    "sent": "Fish there Victor was one of the first guys who actually analyzed the linear quadratic regulation problem, but in a different framework, so he didn't look at regret.",
                    "label": 0
                },
                {
                    "sent": "And then in the control literature.",
                    "label": 0
                },
                {
                    "sent": "Of course you find what is brilliant.",
                    "label": 0
                },
                {
                    "sent": "People like Lion, van Chen Guo and Sung and England they proved mostly consistency are actually just consistent results for first exploration, like epsilon, greedy and more recently this.",
                    "label": 0
                },
                {
                    "sent": "Campion Kumar and then then later Bitonti and campy proof consistency results.",
                    "label": 0
                },
                {
                    "sent": "For an algorithm which is pretty close to the algorithm that I'm going to show you.",
                    "label": 0
                },
                {
                    "sent": "And so, or big inspirations coming from Lion Robins, who proposed this principle in the face of uncertainty for bandit problems.",
                    "label": 1
                },
                {
                    "sent": "And we are going to recycle this idea.",
                    "label": 0
                },
                {
                    "sent": "So that's one main ingredient and the other main ingredient is like a linear estimation theory inequality's for regression problems that we will try to improve a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recycle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main ideas for the algorithm, so it's very simple.",
                    "label": 1
                },
                {
                    "sent": "We follow the footsteps of previous people.",
                    "label": 0
                },
                {
                    "sent": "We estimate the system dynamics and we are optimistic in selecting the controls and we avoid frequent changes to the policy.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the estimation problem.",
                    "label": 0
                },
                {
                    "sent": "So how do we do these steps?",
                    "label": 0
                },
                {
                    "sent": "So the first question is, how do we estimate the system dynamics?",
                    "label": 0
                },
                {
                    "sent": "Well, you can immediately see is that like, given that you know the previous state, the previous action, you know the next action.",
                    "label": 0
                },
                {
                    "sent": "You can just reply the horsing selliner estimation problem so you have this coverage set of T and you have the noise acting on it, so you observe XD plus one you know exactly and you're trying to estimate that data stars.",
                    "label": 0
                },
                {
                    "sent": "That's a linear estimation problem.",
                    "label": 0
                },
                {
                    "sent": "Pretty standard, pretty easy.",
                    "label": 0
                },
                {
                    "sent": "So that works so far, so you can just use, you know Lee Squares or regularised least squares.",
                    "label": 0
                },
                {
                    "sent": "So you have this data and you set up this regression problem and so eventually we used retrogression.",
                    "label": 0
                },
                {
                    "sent": "OK, a little bit of regularization doesn't hurt.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The next ingredient is the optimism principle, so it's not enough just to estimate the parameters.",
                    "label": 0
                },
                {
                    "sent": "You gotta know like how which parameters are feasible and which are infeasible.",
                    "label": 0
                },
                {
                    "sent": "Right, and that will depend on, like in which direction did you see how many covariates and so we set up confidence that it's going to be an ellipsoid, and this parameter that we're talking about is actually a matrix, so it's in a.",
                    "label": 0
                },
                {
                    "sent": "It's another signal space of matrices.",
                    "label": 0
                },
                {
                    "sent": "What there is that and so this confidence set is like with probability 1 minus data contains the true parameter is going to be constructed like that.",
                    "label": 0
                },
                {
                    "sent": "That's where we are going to use stalina qualities and then choose the controller which just gives the best performance.",
                    "label": 1
                },
                {
                    "sent": "That's the principle of optimism in the face of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "And so how do we do this?",
                    "label": 0
                },
                {
                    "sent": "So we need a little bit of notation on too much, so a given parameter, parametrization of Theatre and JFT toilets JLT to denote the optimal average cost, an piasty to be the corresponding optimal policy.",
                    "label": 1
                },
                {
                    "sent": "And So what you do is that you first find within the confidence set the best parameter which, like you know, optimistic that it gives you the best average cost in the long run, and then given the identified parameter you just follow the underlying policy and that policy has a very simple form, so that's very well known from control theory.",
                    "label": 0
                },
                {
                    "sent": "So there are a few caveats here though, so both GF teeth and by FT talk can be undefined if ttar is not nice, so average cost it can blow up it's bonded state.",
                    "label": 0
                },
                {
                    "sent": "So you gotta be careful there.",
                    "label": 0
                },
                {
                    "sent": "So we're putting enough technical restrictions so that this doesn't happen, so I don't want to go into the details.",
                    "label": 0
                },
                {
                    "sent": "And then the next caveat is that actually finding Tita Theodote might be a difficult problem, so that's kind of unfortunate Willie.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To work more about that, but that's.",
                    "label": 0
                },
                {
                    "sent": "We're sweeping this under the rug for the moment.",
                    "label": 0
                },
                {
                    "sent": "And so the next ingredient is avoiding frequent changes that kind of unnecessary, and then saving computation is, isn't it?",
                    "label": 1
                },
                {
                    "sent": "And?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then most importantly, frequent changes seems to be a problem with the proof.",
                    "label": 0
                },
                {
                    "sent": "So first we try to analyze the algorithm that allows frequent changes and you will see that we run.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Travis, OK, so one of the cool things that I've learned by doing this thing is from the book of delapenha at R. The method of mixtures which which you can prove so called staff normalized inequalities which are really cool tight.",
                    "label": 1
                },
                {
                    "sent": "Tighter than previous inequalities and so in the case of free tradition, when you have this matrix, parameters at inequality would look like this.",
                    "label": 0
                },
                {
                    "sent": "So you have your data.",
                    "label": 0
                },
                {
                    "sent": "You estimate this Ridge regression parameters, you have your covariance matrix and then you have that this inequality holds with probability 1 minus data and they are the right quantities.",
                    "label": 1
                },
                {
                    "sent": "That's my feeling about it, but we can talk about that later in private, but I don't want to get into the details of that at the moment.",
                    "label": 0
                },
                {
                    "sent": "So it's at at minimum dimension free, almost except for.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that day has to be there.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the confidence set has this property that OK, you can view it as centered around Tito Standard centered around it ahead and so if you sent it around teeth ahead and you have an empirical reconstructed confidence set with hyperoptic contract contains the true parameter OK?",
                    "label": 0
                },
                {
                    "sent": "So that's good.",
                    "label": 0
                },
                {
                    "sent": "So we have the confidence.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's that was one ingredient, and so you just do whatever I said before so it just, you know, calculate the regression and then be optimistic and follow the control and then County.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the algorithm, and so the proof sketches here.",
                    "label": 0
                },
                {
                    "sent": "So the most of the work of the proof that I'm not going to talk about at all in the rest of the talk is to show that the state actually stays bounded.",
                    "label": 1
                },
                {
                    "sent": "So the state doesn't escape to infinite if it escapes too fast to Infinity, you'll be travel, so you need to work hard to do that.",
                    "label": 0
                },
                {
                    "sent": "After you did that, you are on the right track, so you can prove the regret bonds you just do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That in the composition of the regret.",
                    "label": 0
                },
                {
                    "sent": "It's like here, so easy just use the balmenach creations and then a little algebra and then you can have you can end up with this decomposition.",
                    "label": 0
                },
                {
                    "sent": "Regret this step.",
                    "label": 0
                },
                {
                    "sent": "I'm showing that because this is where we are using the optimism right?",
                    "label": 0
                },
                {
                    "sent": "So GF so this inequality holds when the confidence sets do not fair and when the confidence sets do not fail.",
                    "label": 0
                },
                {
                    "sent": "Anteater studies obvious in the confidence set and then.",
                    "label": 0
                },
                {
                    "sent": "This is always chosen an optimistic fashion that means the cost underlying cost is smaller than at it this time.",
                    "label": 0
                },
                {
                    "sent": "So that's why you're using optimistic property and then you have to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lies this various terms, so here we go.",
                    "label": 0
                },
                {
                    "sent": "So the first term you just regroup and then you notice that is emerging.",
                    "label": 0
                },
                {
                    "sent": "Add different sequence or actually this is a martingale.",
                    "label": 0
                },
                {
                    "sent": "Almost the martingale and then state doesn't explore.",
                    "label": 0
                },
                {
                    "sent": "Then you're done like square root tea.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other term, like the third term, we work hard, you reduce it to this other form and then you do more algebra, user confidence and state doesn't exploit.",
                    "label": 0
                },
                {
                    "sent": "It's good, OK?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You gotta trust.",
                    "label": 0
                },
                {
                    "sent": "It's kind of lengthy calculate complicated calculations and then there is this lovely term, so in this term you see the state is correlated through like you measure the size of the state with respect to the difference between these two P matrices.",
                    "label": 0
                },
                {
                    "sent": "I didn't even define them their solutions to be cut decorations, what not?",
                    "label": 0
                },
                {
                    "sent": "I don't care, it somehow depends on the parameters and you want to analyze like how fast this thing changes.",
                    "label": 0
                },
                {
                    "sent": "That's what it boils down to.",
                    "label": 0
                },
                {
                    "sent": "And like we don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "Fix the algorithm right so this just talks about this changes.",
                    "label": 0
                },
                {
                    "sent": "So this matrix tightly is bonded.",
                    "label": 0
                },
                {
                    "sent": "So if you don't change the policy to off and then these parameters stay the same for a long period of time, like you have logarithmic number of changes, you should be fine.",
                    "label": 0
                },
                {
                    "sent": "So what you doing?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go back to the algorithm and change it such that you just wait until the determinant of the confidence, and if so it doubles and when it doubled then you change to a new policy and then you can show that at least.",
                    "label": 0
                },
                {
                    "sent": "So the number of change.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It cannot be more than logarithmic a number of time steps, so that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you just modify it in these two steps and then otherwise it's identical.",
                    "label": 0
                },
                {
                    "sent": "Alright, so with this that term can be handled.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You answer we have a nice regret bond which reads like this.",
                    "label": 0
                },
                {
                    "sent": "So this property one my mind is that or the regret of the algorithm is bonded like square root T log 1 / D one over that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're happy about this paper because there's a 1st result for this nice little problem of controlling linear systems, which this quadratic cost and.",
                    "label": 1
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "It's been a pleasure to work on this problem because it has such a nice structure and we learn a lot.",
                    "label": 0
                },
                {
                    "sent": "And one problem though is that the algorithm is a little bit too expensive, so it's very very expensive, so it helps a lot that you're not changing the policy that often.",
                    "label": 0
                },
                {
                    "sent": "So I was joking about it, but it is actually helpful.",
                    "label": 0
                },
                {
                    "sent": "And so the question is, does there exist a cheaper alternative?",
                    "label": 1
                },
                {
                    "sent": "Is similar guarantees like?",
                    "label": 0
                },
                {
                    "sent": "I suspect that you change the confidence that you make it a little bit more manageable than you will lose a little bit, maybe a constant factor, and everything is going to work fine computational efficiently as well.",
                    "label": 0
                },
                {
                    "sent": "So one and the travel that control people would immediately ask, is that OK?",
                    "label": 0
                },
                {
                    "sent": "You're making this morning and always assumption, and that's not natural in certain applications, and So what can you say in the other case?",
                    "label": 0
                },
                {
                    "sent": "Not much, sorry, I think that.",
                    "label": 0
                },
                {
                    "sent": "At least that's out of my reach at the one.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So extension tooling are parameterized systems, so that looks like a logical next step, so that's the system dynamics and well in that case the question will be can you ever plan with systems like that?",
                    "label": 0
                },
                {
                    "sent": "And if you can plan then value can of course learn an.",
                    "label": 0
                },
                {
                    "sent": "Probably you can prove things, but you have to be careful.",
                    "label": 0
                },
                {
                    "sent": "So if I is, my feeling is that if I has a bounded growth condition, like maybe Lifshitz Ness.",
                    "label": 0
                },
                {
                    "sent": "In terms of States and the the actions, then you'll be fine.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the state could just explode, right?",
                    "label": 0
                },
                {
                    "sent": "So it could be really sensitive system and it's not uncommon in the control literature either to see results under those conditions, so I wouldn't be surprised if we actually would need to make such an assumption.",
                    "label": 0
                },
                {
                    "sent": "But that would be kind of limit.",
                    "label": 0
                },
                {
                    "sent": "Sad, but yeah we would survive I guess.",
                    "label": 0
                },
                {
                    "sent": "And then under Elizabeth case, so connecting back to the previous talk.",
                    "label": 0
                },
                {
                    "sent": "So this talks about the case when you have a system dynamics and then you assume that Foreman, Yuna lies it under the assumption that the system is reacting like that.",
                    "label": 0
                },
                {
                    "sent": "So what happens if you have an order dynamics?",
                    "label": 0
                },
                {
                    "sent": "Marburg alright thank you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK I have a question.",
                    "label": 0
                },
                {
                    "sent": "So what kind of algorithms are used for this problem in the control literature an is it true that the you can't prove regret?",
                    "label": 0
                },
                {
                    "sent": "Do you have like counterexamples, that you can't prove regret bounds for those kinds of algorithms?",
                    "label": 0
                },
                {
                    "sent": "Do they do so?",
                    "label": 0
                },
                {
                    "sent": "So people the only people who actually variable to regret bonds are?",
                    "label": 0
                },
                {
                    "sent": "Like the ones who listed on this slide or the control people, including the control people that we know of and like verifying to each others, and there are no references out, so that's how you kind of discount this thing right?",
                    "label": 0
                },
                {
                    "sent": "And so either papers.",
                    "label": 0
                },
                {
                    "sent": "Like Blowe of course proofs.",
                    "label": 0
                },
                {
                    "sent": "Lots of regret bonds, but not for this case for some reason.",
                    "label": 0
                },
                {
                    "sent": "And be content company.",
                    "label": 0
                },
                {
                    "sent": "We're actually analyzing an algorithm very similar to yours.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more complicated than ours.",
                    "label": 0
                },
                {
                    "sent": "So you could say that we refine their algorithm and and made the regret analysis work for it.",
                    "label": 0
                },
                {
                    "sent": "And other than that, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Fascinating that control people lots of time.",
                    "label": 0
                },
                {
                    "sent": "Just assume some system dynamics and then it kind of works and then they are happy with that.",
                    "label": 0
                },
                {
                    "sent": "So they're not doing the learning some people do, but they don't prove regret bounds.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's attack the speaker.",
                    "label": 0
                }
            ]
        }
    }
}