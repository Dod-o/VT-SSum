{
    "id": "osigpct42ykzgwzlesmo7ztb4sm4355s",
    "title": "Instance-based ontological knowledge acquisition",
    "info": {
        "introducer": [
            "Laura Hollink, Centrum Wiskunde & Informatica (CWI)"
        ],
        "author": [
            "Lihua Zhao, National Institute of Informatics"
        ],
        "published": "July 8, 2013",
        "recorded": "May 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2013_zhao_knowledge/",
    "segmentation": [
        [
            "Our next speaker is Lihua Zhao.",
            "Hope I pronounced that the more or less correct an instance based instance based on to logical knowledge acquisition and today I will present the instance based on to logical knowledge acquisition."
        ],
        [
            "And this is an outline of my presentation today.",
            "An.",
            "At first I will briefly introduce the motivation of my research and some discuss related work and then I will discuss about the semi automatic Ontology integration framework of our research and this framework is mainly consists of three main components like graph based, ontology integration and machine learning based on trilogy schema extraction and then ontology merger.",
            "Then I will present some experiments an conclude our work and propose some future work."
        ],
        [
            "As we all know that the linked open data sets it contains a lot of datasets, but and they're a lot about 504 million SMS links.",
            "But in order to assess these linked datasets, we have to know the ontology schemas of the datasets."
        ],
        [
            "So our motivation is to solve the following two examples to the following two problems.",
            "The first one is the energy heterogeneity problem.",
            "As we can see from this figure that the France is interlinked from DB pedia, geonames, New York Times and link them to be.",
            "But the same concept is represented differently in different datasets and the same property has different values.",
            "So in order to reduce the heterogeneity.",
            "We want to map the related classes and properties from different datasets.",
            "And we apply the ontology similar to match method on the SMS growth patterns.",
            "And the second problem is it's difficult to identify which ontology schemas are important in each data set.",
            "So in order to retrieve the core important schemas, we apply the machine learning methods to retrieve the call onto the schemas."
        ],
        [
            "And there are some related work about on touch integration on torch matching, but mostly they they are domain specific and they only limited to expressive lightweight ontologies.",
            "An only work on they only require two inputs.",
            "So our researcher overcome these limitations.",
            "And this is the."
        ],
        [
            "We work first is the graph based on touch integration, which was our previous work and then we extended this work using the machine learning approach an.",
            "We use the energy merger.",
            "And the first is a graph based on touch integration.",
            "We grouped the related classes and properties."
        ],
        [
            "And this is the process of five main processes and 1st we extracted a graph patterns, then we for each graph pattern we collect all the predicted object pairs and then we classify the pure pairs in two different types like class, string, date number and UI.",
            "Then we perform similarity matching methods on the contents to retrieve the related classes and properties.",
            "Then we integrate.",
            "Integrate all the result together an in the last step.",
            "We need some manual revision."
        ],
        [
            "And the first, is it a graph pattern extraction an we define SMS growth pattern as we can see in the on the left side it's the France interlinked from different datasets.",
            "So we retrieve the growth pattern as the right side, an vertice is represents the labels of the datasets and the edges between the nodes.",
            "And I including the UI's of the instances for further data collection."
        ],
        [
            "And the next step is the PEO collection predicate object collection for the SMS graph of France, and we classify the pure parity into five different types like clustering and for further similarity matching methods.",
            "And."
        ],
        [
            "First step is to retrieve the related classes and properties and for the related classes we performed will check the subscription relations and we treat the node leaf nodes as their related classes.",
            "So in this example the DPD, a country and geonames Geo ontology, a dot PCLIN linked MDB, the country and the New York Times Geo is.",
            "They all represent the country information as a class.",
            "So we retrieve all the notes as and group them together."
        ],
        [
            "An for the related properties 1st Way perform the exact matching on the pier pairs.",
            "Then we perform similarity metric methods on the numbers in the string for your eyes and the data by date, we only use the exact matching, so the similarity is simply category acted as the average of the object similarity on the PEO peers and the predicate similarity.",
            "And the object similarity is calculated as according to the formula.",
            "If it's number, it's the first formula An if it's string will perform well, use the General Winkler Levingston and Engram for an calculated average and for the predicate similarity, we use the word net similarity measures and the last step is we.",
            "Refine the sets of pure pairs according to the different domains if they belong to different domains, we separate the groups."
        ],
        [
            "And the next step is simply aggregate all these results and we automatically automatically select the terms for each group and also for class and properties.",
            "And also we constructed a relations like his member classes and has data types in order to link the groups of related classes and properties to the integrated ontology.",
            "And in this step we construct a preliminary ontology integrated ontology."
        ],
        [
            "And the last step is a minor manual revision on the selected terms and the grouping.",
            "And this is a."
        ],
        [
            "Introduction of our previous work an.",
            "In this work we used a machine learning approach to find out the core ontology schemas because the users want to know what kind of schemas are important in the datasets."
        ],
        [
            "And we retrieve the top level classes and call properties using division table and operatory for division table.",
            "It belongs to the.",
            "And rule based machine learning method and it can use simple hypothesis to use the selected sets of properties to classify the different types of instances and for opera or it belongs to the Association rule mining and we can find a set subset of properties which is important for describing the instances in specific classes.",
            "So."
        ],
        [
            "In order to apply the division table, we first collected top level classes and we also filters filter out some infrequent properties.",
            "Then we convert each instance for the addition table algorithm.",
            "We use the weighting method like TF IDF.",
            "We use the.",
            "Property frequency product of the property frequency and the inverse instance frequency.",
            "And we use the waiting with of the properties in each instance to perform the addition table."
        ],
        [
            "And for the opera, Ori will also do the same process like collect the top level classes and filled out infrequent properties.",
            "Then for each samples of the instances in a specific Class C, We collect on a list of properties which are used in the instances and we manually define the minimum support and confident metrics for opera or algorithm."
        ],
        [
            "And the third component is the ontology measure, which merges the ontology schema is retrieved from previous two components."
        ],
        [
            "And as we already constructed at a preliminary integrated ontology in the first component, then we just added a new entry schema retrieval using the machine learning.",
            "If it's a new class, we just added the cheaper at define it as the new class and for the properties retrieved from the operatory we can define the property it belongs to the the class.",
            "So we can add some missing domain informations in this step."
        ],
        [
            "And next I will introduce some experimental results about the datasets and some other result of each component and I also I will also introduce some use cases using our."
        ],
        [
            "Tributed ontology.",
            "And we used this for datasets, DV Pedia, link MTP, New York Times and geonames.",
            "And this this thing linked datasets are used for graph based on search integration."
        ],
        [
            "Ann for machine learning, where you samples of the datasets.",
            "For DB pedia, we choose 5000 instances per class and for June.",
            "Names in the linked MDB, which was 3000 an we used all the New York Times because it's very small datasets.",
            "And in order to retrieve the top level classes, if the datasets is based on the ontology we use the subsumption relations and if it's not like New York Times or Geo names, we use the classic categories as the top level classes and the number of selected properties in.",
            "Using, we used to frequent stress or the Theta, which is calculated as root N we run is the total number of instances in each data set.",
            "So these datasets are used for our machine learning methods.",
            "And."
        ],
        [
            "This is brief results of graph based integration.",
            "We retrieve the 13 graph patterns and we analyze the contents to retrieve the related classes and properties and you can find the detailed experimental results in the previous paper."
        ],
        [
            "And we were merely evaluate the results of machine learning and for the decision table we we try to evaluate if there are achieved properties are important for describing the instances an by testing the performance of the classification in the datasets.",
            "And for Operatory we analyze the examples of the properties in the retrieved.",
            "A subset of properties.",
            "And."
        ],
        [
            "This is the precision required of measure of using the addition table and the last column is the number of properties we retrieved using addition table.",
            "And we use the time for the classification an as we can see that it could empty.",
            "It can perform well.",
            "It can predict all the instances correctly, and we as we can see from this example linked empty because it retrieves some right director ideas and write ideas which can distinguish the instances.",
            "And for the DPD and New York Times, it at performs quite well and it also retrieves some important properties which are normally used for describing the instances in each data set.",
            "But for June it's because it unit is geographically data and all the properties are commonly used in each data set, so it's not significantly can distinguish the instances.",
            "Ah."
        ],
        [
            "For opera Ori we list some examples of the data set properties retrieved from each class.",
            "As we can see from the first one TV pedia event where we achieved some proper place date and related jewel which is important for describing the event instances and also for the link to MP will for the film we retrieve the Director performance and actors and dates about the to describe that.",
            "Film and as we can see that the pedia in the linked MTP, they performs quite well.",
            "But Geonames and the New York Times because most of the properties are commonly used in each data set in each instance.",
            "So it's not that.",
            "Order retrieved properties are very common and in this step we automatically added a missing domains like in the first one we added.",
            "Place belongs to the domain of events, so we can know for describing the instance of the event.",
            "We can use.",
            "What kind of properties?"
        ],
        [
            "And here's the comparison of integrated ontology.",
            "Between our previous work and current work an using the machine learning way, you achieved more classes and properties.",
            "An opera or it can reach more properties than division table and there are 33 new properties are found in both methods.",
            "And so the final integrated ontology consists of 135 classes which are grouped.",
            "Grouped into 38 groups, an 453 properties which are grouped into 90 seven groups."
        ],
        [
            "And here are some case studies.",
            "The first example is find out the missing links of the islands in our integrated ontology.",
            "The Geo Ontology tied at T dot IS R&D pedia.",
            "Ontology islands are integrated together, so we use the sparkle query to find out the islands with the same labels.",
            "As a using this one way to achieve the 509 links, anrich includes 218 existing SMS links and hear their 97 are from TPA to June and 211 from Tunis to DB Pedia.",
            "An 90 bidirectional links.",
            "So as a result we found 291 missing links which which has the same levels of islands an they belong to June insanity Pedia."
        ],
        [
            "And the second case studies is about we can recommend some standard properties as these properties are about the birth of date, birth date, an we integrated all these properties in the.",
            "In one group, and as we can see, the first one is the most frequently used in the datasets and only the first property is defined as using the domain information as the person.",
            "So we can assume the other properties are mistakenly used in the real datasets when they publish the data."
        ],
        [
            "And the third case studies is about this example is 1 example from the QRD Open Challenge and the question is.",
            "Find all the cities with more than 10 million inhabitants and the left one.",
            "The Curie is to the standard query about find out the using the DB pedia property or population total and the right one is using the integrated ontology of ours.",
            "Because we have already integrated several population properties in the example ontology population, we can create with each properties an using the standard will.",
            "Only rich with 9 cities, but using our integrated ontology we found out 20 cities which has rich can answer more answers and using the integrated on Thursday we can help the QA system for searching more related answers."
        ],
        [
            "And here are the conclusion of our work, and we introduced the semi automatic Integration framework which consists of three main components.",
            "We use the graph based on touch integration to retrieve related classes and properties and we applied the addition table and a priority of the machine learning methods to find out the core onto the schemers and then the other one is the energy merger.",
            "And using the integrated ontology we can find some missing links.",
            "And also we can detect some mistakes in the ontology used in the data set.",
            "And we can also assess different datasets using integrated ontology in the sparkle queries.",
            "And as a future work, we want to automatically detect the detect and revise the mistakes on the in the merger process.",
            "And we also want to automatically detect the range information and domain informations in the data because there are still many missing information in the real data sets and we also plan to test our framework with more linked open data sets."
        ],
        [
            "Thank you very much.",
            "On one side we can see what you do as ontology matching because you find the corresponding property and classes.",
            "So what is the benefit of generating an integrated ontologies with respect to keeping the alignment and try to reason with this?",
            "And I had another question, which is you say you say that you are putting the ontology into the query, but what do you do query expansion with Union or what kind of for using Spell Curie?",
            "Yes, for using the ontology into the queries, do you rewrite the query with a set of union or what do you use for some?",
            "Yes, for integrated well I use the unions of the curious and then we or for this one because we defined that has member data types so it can automatically use the properties already integrated in the ontology and query the data sets.",
            "I have a couple of questions actually, so I think you said something about that.",
            "You found out some of the properties were incorrectly used.",
            "Can you say something about how and whether you could use your approach to detect these incorrect use of, or even misspellings of properties?",
            "Misuses property, so I think so there was one slide where you could see there was about the birth birthday.",
            "Think yes this."
        ],
        [
            "Actually, these properties are all used in the DB pedia datasets.",
            "And I guess they when they publish the data, they mystically use the different capitalize of the properties and.",
            "For example, for this data property it's quite frequently used and.",
            "So our with our method we found out this or mentioning about the birth of date.",
            "So by analyzing the frequency of the usage and the definition of domain or something we can detect, maybe the others are not correctly used in the real datasets.",
            "OK so I was wondering your definition of core properties and then in the evaluation you said something about important properties being the ones that you can use to distinguish between instances.",
            "Yes, So what is the relation between the two?",
            "Actually it's the same, I just used the different terms an because if it can distinguish different instances then it is useful to identify what kind of properties should be used for specific class or specific.",
            "Instances, so that's what I call is core properties.",
            "Right OK OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next speaker is Lihua Zhao.",
                    "label": 0
                },
                {
                    "sent": "Hope I pronounced that the more or less correct an instance based instance based on to logical knowledge acquisition and today I will present the instance based on to logical knowledge acquisition.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is an outline of my presentation today.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "At first I will briefly introduce the motivation of my research and some discuss related work and then I will discuss about the semi automatic Ontology integration framework of our research and this framework is mainly consists of three main components like graph based, ontology integration and machine learning based on trilogy schema extraction and then ontology merger.",
                    "label": 1
                },
                {
                    "sent": "Then I will present some experiments an conclude our work and propose some future work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we all know that the linked open data sets it contains a lot of datasets, but and they're a lot about 504 million SMS links.",
                    "label": 0
                },
                {
                    "sent": "But in order to assess these linked datasets, we have to know the ontology schemas of the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our motivation is to solve the following two examples to the following two problems.",
                    "label": 0
                },
                {
                    "sent": "The first one is the energy heterogeneity problem.",
                    "label": 1
                },
                {
                    "sent": "As we can see from this figure that the France is interlinked from DB pedia, geonames, New York Times and link them to be.",
                    "label": 0
                },
                {
                    "sent": "But the same concept is represented differently in different datasets and the same property has different values.",
                    "label": 0
                },
                {
                    "sent": "So in order to reduce the heterogeneity.",
                    "label": 1
                },
                {
                    "sent": "We want to map the related classes and properties from different datasets.",
                    "label": 1
                },
                {
                    "sent": "And we apply the ontology similar to match method on the SMS growth patterns.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is it's difficult to identify which ontology schemas are important in each data set.",
                    "label": 0
                },
                {
                    "sent": "So in order to retrieve the core important schemas, we apply the machine learning methods to retrieve the call onto the schemas.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are some related work about on touch integration on torch matching, but mostly they they are domain specific and they only limited to expressive lightweight ontologies.",
                    "label": 1
                },
                {
                    "sent": "An only work on they only require two inputs.",
                    "label": 0
                },
                {
                    "sent": "So our researcher overcome these limitations.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We work first is the graph based on touch integration, which was our previous work and then we extended this work using the machine learning approach an.",
                    "label": 0
                },
                {
                    "sent": "We use the energy merger.",
                    "label": 0
                },
                {
                    "sent": "And the first is a graph based on touch integration.",
                    "label": 0
                },
                {
                    "sent": "We grouped the related classes and properties.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the process of five main processes and 1st we extracted a graph patterns, then we for each graph pattern we collect all the predicted object pairs and then we classify the pure pairs in two different types like class, string, date number and UI.",
                    "label": 0
                },
                {
                    "sent": "Then we perform similarity matching methods on the contents to retrieve the related classes and properties.",
                    "label": 1
                },
                {
                    "sent": "Then we integrate.",
                    "label": 0
                },
                {
                    "sent": "Integrate all the result together an in the last step.",
                    "label": 0
                },
                {
                    "sent": "We need some manual revision.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the first, is it a graph pattern extraction an we define SMS growth pattern as we can see in the on the left side it's the France interlinked from different datasets.",
                    "label": 1
                },
                {
                    "sent": "So we retrieve the growth pattern as the right side, an vertice is represents the labels of the datasets and the edges between the nodes.",
                    "label": 1
                },
                {
                    "sent": "And I including the UI's of the instances for further data collection.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the next step is the PEO collection predicate object collection for the SMS graph of France, and we classify the pure parity into five different types like clustering and for further similarity matching methods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First step is to retrieve the related classes and properties and for the related classes we performed will check the subscription relations and we treat the node leaf nodes as their related classes.",
                    "label": 1
                },
                {
                    "sent": "So in this example the DPD, a country and geonames Geo ontology, a dot PCLIN linked MDB, the country and the New York Times Geo is.",
                    "label": 0
                },
                {
                    "sent": "They all represent the country information as a class.",
                    "label": 0
                },
                {
                    "sent": "So we retrieve all the notes as and group them together.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An for the related properties 1st Way perform the exact matching on the pier pairs.",
                    "label": 1
                },
                {
                    "sent": "Then we perform similarity metric methods on the numbers in the string for your eyes and the data by date, we only use the exact matching, so the similarity is simply category acted as the average of the object similarity on the PEO peers and the predicate similarity.",
                    "label": 0
                },
                {
                    "sent": "And the object similarity is calculated as according to the formula.",
                    "label": 0
                },
                {
                    "sent": "If it's number, it's the first formula An if it's string will perform well, use the General Winkler Levingston and Engram for an calculated average and for the predicate similarity, we use the word net similarity measures and the last step is we.",
                    "label": 1
                },
                {
                    "sent": "Refine the sets of pure pairs according to the different domains if they belong to different domains, we separate the groups.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the next step is simply aggregate all these results and we automatically automatically select the terms for each group and also for class and properties.",
                    "label": 0
                },
                {
                    "sent": "And also we constructed a relations like his member classes and has data types in order to link the groups of related classes and properties to the integrated ontology.",
                    "label": 1
                },
                {
                    "sent": "And in this step we construct a preliminary ontology integrated ontology.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last step is a minor manual revision on the selected terms and the grouping.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduction of our previous work an.",
                    "label": 0
                },
                {
                    "sent": "In this work we used a machine learning approach to find out the core ontology schemas because the users want to know what kind of schemas are important in the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we retrieve the top level classes and call properties using division table and operatory for division table.",
                    "label": 0
                },
                {
                    "sent": "It belongs to the.",
                    "label": 0
                },
                {
                    "sent": "And rule based machine learning method and it can use simple hypothesis to use the selected sets of properties to classify the different types of instances and for opera or it belongs to the Association rule mining and we can find a set subset of properties which is important for describing the instances in specific classes.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to apply the division table, we first collected top level classes and we also filters filter out some infrequent properties.",
                    "label": 1
                },
                {
                    "sent": "Then we convert each instance for the addition table algorithm.",
                    "label": 1
                },
                {
                    "sent": "We use the weighting method like TF IDF.",
                    "label": 0
                },
                {
                    "sent": "We use the.",
                    "label": 1
                },
                {
                    "sent": "Property frequency product of the property frequency and the inverse instance frequency.",
                    "label": 0
                },
                {
                    "sent": "And we use the waiting with of the properties in each instance to perform the addition table.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the opera, Ori will also do the same process like collect the top level classes and filled out infrequent properties.",
                    "label": 0
                },
                {
                    "sent": "Then for each samples of the instances in a specific Class C, We collect on a list of properties which are used in the instances and we manually define the minimum support and confident metrics for opera or algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the third component is the ontology measure, which merges the ontology schema is retrieved from previous two components.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as we already constructed at a preliminary integrated ontology in the first component, then we just added a new entry schema retrieval using the machine learning.",
                    "label": 1
                },
                {
                    "sent": "If it's a new class, we just added the cheaper at define it as the new class and for the properties retrieved from the operatory we can define the property it belongs to the the class.",
                    "label": 0
                },
                {
                    "sent": "So we can add some missing domain informations in this step.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And next I will introduce some experimental results about the datasets and some other result of each component and I also I will also introduce some use cases using our.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tributed ontology.",
                    "label": 0
                },
                {
                    "sent": "And we used this for datasets, DV Pedia, link MTP, New York Times and geonames.",
                    "label": 0
                },
                {
                    "sent": "And this this thing linked datasets are used for graph based on search integration.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ann for machine learning, where you samples of the datasets.",
                    "label": 1
                },
                {
                    "sent": "For DB pedia, we choose 5000 instances per class and for June.",
                    "label": 1
                },
                {
                    "sent": "Names in the linked MDB, which was 3000 an we used all the New York Times because it's very small datasets.",
                    "label": 0
                },
                {
                    "sent": "And in order to retrieve the top level classes, if the datasets is based on the ontology we use the subsumption relations and if it's not like New York Times or Geo names, we use the classic categories as the top level classes and the number of selected properties in.",
                    "label": 0
                },
                {
                    "sent": "Using, we used to frequent stress or the Theta, which is calculated as root N we run is the total number of instances in each data set.",
                    "label": 1
                },
                {
                    "sent": "So these datasets are used for our machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is brief results of graph based integration.",
                    "label": 0
                },
                {
                    "sent": "We retrieve the 13 graph patterns and we analyze the contents to retrieve the related classes and properties and you can find the detailed experimental results in the previous paper.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we were merely evaluate the results of machine learning and for the decision table we we try to evaluate if there are achieved properties are important for describing the instances an by testing the performance of the classification in the datasets.",
                    "label": 1
                },
                {
                    "sent": "And for Operatory we analyze the examples of the properties in the retrieved.",
                    "label": 0
                },
                {
                    "sent": "A subset of properties.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the precision required of measure of using the addition table and the last column is the number of properties we retrieved using addition table.",
                    "label": 0
                },
                {
                    "sent": "And we use the time for the classification an as we can see that it could empty.",
                    "label": 0
                },
                {
                    "sent": "It can perform well.",
                    "label": 0
                },
                {
                    "sent": "It can predict all the instances correctly, and we as we can see from this example linked empty because it retrieves some right director ideas and write ideas which can distinguish the instances.",
                    "label": 0
                },
                {
                    "sent": "And for the DPD and New York Times, it at performs quite well and it also retrieves some important properties which are normally used for describing the instances in each data set.",
                    "label": 1
                },
                {
                    "sent": "But for June it's because it unit is geographically data and all the properties are commonly used in each data set, so it's not significantly can distinguish the instances.",
                    "label": 1
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For opera Ori we list some examples of the data set properties retrieved from each class.",
                    "label": 0
                },
                {
                    "sent": "As we can see from the first one TV pedia event where we achieved some proper place date and related jewel which is important for describing the event instances and also for the link to MP will for the film we retrieve the Director performance and actors and dates about the to describe that.",
                    "label": 0
                },
                {
                    "sent": "Film and as we can see that the pedia in the linked MTP, they performs quite well.",
                    "label": 0
                },
                {
                    "sent": "But Geonames and the New York Times because most of the properties are commonly used in each data set in each instance.",
                    "label": 1
                },
                {
                    "sent": "So it's not that.",
                    "label": 0
                },
                {
                    "sent": "Order retrieved properties are very common and in this step we automatically added a missing domains like in the first one we added.",
                    "label": 0
                },
                {
                    "sent": "Place belongs to the domain of events, so we can know for describing the instance of the event.",
                    "label": 0
                },
                {
                    "sent": "We can use.",
                    "label": 0
                },
                {
                    "sent": "What kind of properties?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's the comparison of integrated ontology.",
                    "label": 1
                },
                {
                    "sent": "Between our previous work and current work an using the machine learning way, you achieved more classes and properties.",
                    "label": 1
                },
                {
                    "sent": "An opera or it can reach more properties than division table and there are 33 new properties are found in both methods.",
                    "label": 1
                },
                {
                    "sent": "And so the final integrated ontology consists of 135 classes which are grouped.",
                    "label": 0
                },
                {
                    "sent": "Grouped into 38 groups, an 453 properties which are grouped into 90 seven groups.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are some case studies.",
                    "label": 1
                },
                {
                    "sent": "The first example is find out the missing links of the islands in our integrated ontology.",
                    "label": 1
                },
                {
                    "sent": "The Geo Ontology tied at T dot IS R&D pedia.",
                    "label": 1
                },
                {
                    "sent": "Ontology islands are integrated together, so we use the sparkle query to find out the islands with the same labels.",
                    "label": 0
                },
                {
                    "sent": "As a using this one way to achieve the 509 links, anrich includes 218 existing SMS links and hear their 97 are from TPA to June and 211 from Tunis to DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "An 90 bidirectional links.",
                    "label": 1
                },
                {
                    "sent": "So as a result we found 291 missing links which which has the same levels of islands an they belong to June insanity Pedia.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second case studies is about we can recommend some standard properties as these properties are about the birth of date, birth date, an we integrated all these properties in the.",
                    "label": 0
                },
                {
                    "sent": "In one group, and as we can see, the first one is the most frequently used in the datasets and only the first property is defined as using the domain information as the person.",
                    "label": 0
                },
                {
                    "sent": "So we can assume the other properties are mistakenly used in the real datasets when they publish the data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the third case studies is about this example is 1 example from the QRD Open Challenge and the question is.",
                    "label": 1
                },
                {
                    "sent": "Find all the cities with more than 10 million inhabitants and the left one.",
                    "label": 1
                },
                {
                    "sent": "The Curie is to the standard query about find out the using the DB pedia property or population total and the right one is using the integrated ontology of ours.",
                    "label": 0
                },
                {
                    "sent": "Because we have already integrated several population properties in the example ontology population, we can create with each properties an using the standard will.",
                    "label": 1
                },
                {
                    "sent": "Only rich with 9 cities, but using our integrated ontology we found out 20 cities which has rich can answer more answers and using the integrated on Thursday we can help the QA system for searching more related answers.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are the conclusion of our work, and we introduced the semi automatic Integration framework which consists of three main components.",
                    "label": 0
                },
                {
                    "sent": "We use the graph based on touch integration to retrieve related classes and properties and we applied the addition table and a priority of the machine learning methods to find out the core onto the schemers and then the other one is the energy merger.",
                    "label": 1
                },
                {
                    "sent": "And using the integrated ontology we can find some missing links.",
                    "label": 1
                },
                {
                    "sent": "And also we can detect some mistakes in the ontology used in the data set.",
                    "label": 0
                },
                {
                    "sent": "And we can also assess different datasets using integrated ontology in the sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "And as a future work, we want to automatically detect the detect and revise the mistakes on the in the merger process.",
                    "label": 1
                },
                {
                    "sent": "And we also want to automatically detect the range information and domain informations in the data because there are still many missing information in the real data sets and we also plan to test our framework with more linked open data sets.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "On one side we can see what you do as ontology matching because you find the corresponding property and classes.",
                    "label": 0
                },
                {
                    "sent": "So what is the benefit of generating an integrated ontologies with respect to keeping the alignment and try to reason with this?",
                    "label": 0
                },
                {
                    "sent": "And I had another question, which is you say you say that you are putting the ontology into the query, but what do you do query expansion with Union or what kind of for using Spell Curie?",
                    "label": 0
                },
                {
                    "sent": "Yes, for using the ontology into the queries, do you rewrite the query with a set of union or what do you use for some?",
                    "label": 0
                },
                {
                    "sent": "Yes, for integrated well I use the unions of the curious and then we or for this one because we defined that has member data types so it can automatically use the properties already integrated in the ontology and query the data sets.",
                    "label": 0
                },
                {
                    "sent": "I have a couple of questions actually, so I think you said something about that.",
                    "label": 0
                },
                {
                    "sent": "You found out some of the properties were incorrectly used.",
                    "label": 0
                },
                {
                    "sent": "Can you say something about how and whether you could use your approach to detect these incorrect use of, or even misspellings of properties?",
                    "label": 0
                },
                {
                    "sent": "Misuses property, so I think so there was one slide where you could see there was about the birth birthday.",
                    "label": 0
                },
                {
                    "sent": "Think yes this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, these properties are all used in the DB pedia datasets.",
                    "label": 0
                },
                {
                    "sent": "And I guess they when they publish the data, they mystically use the different capitalize of the properties and.",
                    "label": 0
                },
                {
                    "sent": "For example, for this data property it's quite frequently used and.",
                    "label": 0
                },
                {
                    "sent": "So our with our method we found out this or mentioning about the birth of date.",
                    "label": 0
                },
                {
                    "sent": "So by analyzing the frequency of the usage and the definition of domain or something we can detect, maybe the others are not correctly used in the real datasets.",
                    "label": 0
                },
                {
                    "sent": "OK so I was wondering your definition of core properties and then in the evaluation you said something about important properties being the ones that you can use to distinguish between instances.",
                    "label": 0
                },
                {
                    "sent": "Yes, So what is the relation between the two?",
                    "label": 0
                },
                {
                    "sent": "Actually it's the same, I just used the different terms an because if it can distinguish different instances then it is useful to identify what kind of properties should be used for specific class or specific.",
                    "label": 0
                },
                {
                    "sent": "Instances, so that's what I call is core properties.",
                    "label": 0
                },
                {
                    "sent": "Right OK OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}