{
    "id": "ff4vsvyr6ffwlp7p7ve67cvkpeknxwkw",
    "title": "PLSI: The True Fisher Kernel and Beyond",
    "info": {
        "author": [
            "Emmanuel Eckard, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_eckard_plsitfkb/",
    "segmentation": [
        [
            "So welcome everyone and so the 1st paper is on the other side.",
            "The true future kernel and beyond, and it's going to be presented by in a way like this.",
            "Ladies and gentlemen, good morning.",
            "I am in America and I will be presenting PSR the true Fisher Kernel and beyond by my advisor.",
            "Just extrapolate and myself."
        ],
        [
            "So the framework of our work is the problem of representing documentation representing documents for classification of IR.",
            "That instance lighting a document in human readable form into a mathematical object that the computer can understand and use for comparison.",
            "We use the model pealess I, which is a metal based on Latin categories.",
            "That is, it postulates that you don't have any documents and words in the universe, but also topics and that topics are chosen and then generate documents and words.",
            "Our problem is knowing.",
            "Pierre Desir, representation of documents how do we?",
            "Compared two documents or a document in a query.",
            "In order to give a score and make clustering or give information retrieval."
        ],
        [
            "Answers.",
            "So a quick review of the previous I model.",
            "Polisci probabilistic Latin semantic indexing was proposed by Hoffman in 1990, nine 98.",
            "It models pairs of.",
            "In dices of documents and terms?",
            "A document is supposed to be a set of pairs which share the same document in dice.",
            "You have a set of topics which are we have the zed in dices.",
            "And the formula for the probability of a given DW pair is given by this formula.",
            "The parameters of the model are the probabilities of.",
            "One of the different categories.",
            "Then the probabilities of all of the documents known in the corpus given a certain category, and similarly for the words.",
            "That is, if you see a world like ship in.",
            "In a in a document, it's probable that you're talking about the sea rather than the desert.",
            "These parameters here are inferred from the observed data via expectation maximization.",
            "Sofia is a generative model, but go backwards using expectation Max."
        ],
        [
            "Station.",
            "Now the problem of.",
            "The similarities between the documents that we have represented using pearsei arises.",
            "Originally, Huffman used a variant of the cosine similarity.",
            "On the parameters he learned and the results were not very impressive.",
            "A few years later, the Fisher Kernels were introduced, which provide proximities between instances of sophistical processes, which is exactly what Peter is about.",
            "So.",
            "The idea of using Fisher kernels for comparison in parasite is rather straightforward and were used by Hoffman again in 2001."
        ],
        [
            "The kernel that Huffman used.",
            "Is this one so I'm just showing the equation to you get a taste of the idea.",
            "You see that you have two components.",
            "One is a sum over the categories.",
            "On the top and the second one is a sum over the the terms.",
            "You see, if you study the equation a little bit, it's not quite apparent here, but the Fisher information."
        ],
        [
            "Sorry, the official kernel here contains.",
            "A Fisher information matrix which encodes the topology of the universe."
        ],
        [
            "So this.",
            "Fisher information matrix.",
            "Here's here.",
            "Approximal are approximated by the identity matrix.",
            "Furthermore this Fisher kernel has been developed by largely ignoring the generative process of pearsei.",
            "It postulates the documents are entities rather than actually.",
            "Describing what happens when you generate one pair of indicies after another?",
            "And to finish.",
            "You have here and here at normalization by size of document size of query.",
            "And it's also present in the term zed for it's not quite so evident, but I'll come back to that later.",
            "But it's also here, and these normalizations are not justified properly.",
            "In half months paper.",
            "So we wanted to know more about."
        ],
        [
            "These issues.",
            "And we decided to re derive the kernel.",
            "By accounting for the process.",
            "So.",
            "We come back to the fundamental.",
            "Peerless Age is an IID process of generating pairs.",
            "Furthermore, we know that for an ID process, the future cannot will be.",
            "I continuation of elementary Fisher kernels.",
            "Each coding one of the particular elements of the ID process.",
            "With this formula, the demonstration in the paper.",
            "So.",
            "Sorry.",
            "Oh yes, yes one turn.",
            "Sorry.",
            "So by using this same for new kernel.",
            "We"
        ],
        [
            "We developed the atomic so called atomic kernel 4142 pairs of indexes.",
            "Which is like that.",
            "And then we can assemble them into a logical ological for the entire process.",
            "You know the difference, sorry.",
            "It comes from the definition of the future kernel.",
            "So yes, so yeah, good question.",
            "So these are the future information Fisher Information matrix which is also separated into components specify specifically for the categories and here for the terms and we explicitly know that because we are willing to challenge the notion that this is simply the identity matrix.",
            "So to come back to.",
            "It's just different parts of the matrix actually.",
            "The Matrix will look like something like this.",
            "With the part here for a for the categories and apart here for.",
            "Sorry this is just this and here for the terms.",
            "Knowing that so.",
            "Yes, we that's a I didn't mention it, but yes we do.",
            "We do assume that it's going to be diagonal.",
            "Why?",
            "Because you have in the."
        ],
        [
            "Fisher formula you have to invert this matrix and given the size of matrix inverting it is simply not realistic.",
            "So we assume that is diagonal."
        ],
        [
            "Then the comparison between our neways Colonel Ann."
        ],
        [
            "Months cannot.",
            "The kernel has four support, the set of two 2 words because you don't always compare the the contribution of one word with its own other comparisons, also with its.",
            "Own other contributions.",
            "So that makes the idea kernel very costly.",
            "Compute much more costly than Huffman's.",
            "On the other hand, we have a similarity between the."
        ],
        [
            "The kernel, which arises from here simply, it means that we."
        ],
        [
            "You can see the normalization by document and query length coming quite naturally.",
            "So we have."
        ],
        [
            "Just for this part.",
            "Now the problem with.",
            "Hoffman is that is still different from Idi is different from Hoffman, so we want to take a closer look at the assumptions between behind the Huffman scanner.",
            "We see that health Max makes the assumption that overall words.",
            "The observed probability the.",
            "Stocks in well there.",
            "Yes, the observed data.",
            "The probability is a good estimator of the probability as it's going to be inferred by expectation maximization.",
            "It's not true for any particular word, but overall the world is going to work very well.",
            "That's the assumption.",
            "Then you have the second assumption that GD Fisher Information matrix is similar to the identity.",
            "And Furthermore, we introduce an observation is that the probability of a document is commensurable to its length divided by the length of all the corpus.",
            "This is.",
            "Yes, this is an observation we make.",
            "So if we introduce those relations those assumptions and is a perversion into the formula of the kernel, you.",
            "You put this, you there relation, you find that you.",
            "You end up with half months Colonel, but it means that at the price of these three.",
            "Relations.",
            "Huffman's collection be seen as an approximation of the idea, Colonel.",
            "And that could explain why it works."
        ],
        [
            "Because we've also tried removing the normalization by documents and query length and works rather worse than Huffman's kernel.",
            "So, having identified the airport, is this, we won't know to challenge their ability.",
            "The the assumption that the observed probability is going to be consistent with the.",
            "Probability as inferred by expectation maximization, is a big simplification particuli for K for the category part of the kernel.",
            "On the other hand.",
            "The that the probability of the document is commensurable to its length divided by the length of the corpus is.",
            "Both theoretically and experimentally justified.",
            "But seeing the Fisher information matrix as identity is neither theoretically nor experimentally verified.",
            "So it's a big simplification for the equation."
        ],
        [
            "So.",
            "WHI did Huffman portrayed that this matrix is the identity?",
            "He doesn't say so, but we can infer that he does that by analogy with multi human models.",
            "The problem is that pizza is not a material.",
            "It's not even in the expansion family.",
            "So we actually derivate.",
            "The information matrix and we add it to our formulas.",
            "An we test it.",
            "So four tests.",
            "For the experiment framework we used an overlay a software overlay that we coded.",
            "Over exception we use caption for indication for the access, storage and data copy and has a good stemming tool, good documentation etc.",
            "And we evaluated our methods on the data of smart, which is the traditional set on which Pearsei was attempted.",
            "And we also try it on a subset of the AP 85 corpus from track.",
            "This allows is to build much larger corpus than the smart ones, and to test up to where we can you spare design.",
            "These corpora are rather small by true destiny in IR, but they are quite big for PS I."
        ],
        [
            "88.9 did you use or how many queries?",
            "We used 50 or 100, I don't.",
            "Yes, it's a bit of documents, yes."
        ],
        [
            "So the results give.",
            "That you can see clearly two families this here looking like a smile is the family of the kernels which suppose that the fish information matrix looks like the identity.",
            "Oh sorry, excuse me, this is the mean average precision and here we have the number of categories that we give for model number of topics.",
            "So the models that postulate that the information matrix is the identity have a decreasing performance up to some point.",
            "At which point they start increasing again slowly.",
            "But the models which have a more sophisticated representation of the information matrix.",
            "Are mostly keeping to the to the top of the of the chart.",
            "So we have a good validation that the efficient formation metric is indeed not that close to the identity, and that computing it more properly is a valued contribution.",
            "Furthermore, you see that.",
            "Here you have the ID Kerner, and here you have the half mask error.",
            "So you see that they are indeed as we predicted.",
            "Fairly faithful to each other.",
            "And that's also for slightly less true 2 for the.",
            "For the kernels that take the information metrics into account, you see that here Huffmans cannot dominate a bit.",
            "the ID cannot, because in the case of.",
            "Taking the information matrix into account.",
            "The equality between the two kernels is not that that much verify mean, it's a, it's a it's a loser approximation of the kernel and it happens to work pretty well."
        ],
        [
            "So now I'm going to give a second contribution of our paper.",
            "Which is about trying to find alternative proximities than diffusion kernels because of the Fisher kernels.",
            "Need first they are costing compute and then you have big problems with the queries you need to compute the parameters for the queries, But these you can't compute either.",
            "You have to possibly that you know the queries before hand which is contradictory with the premises of information retrieval.",
            "Or you have to use a hack which is called folding in an which entails theoretical problems more specifically with the estimation of likelihood."
        ],
        [
            "So I want to.",
            "Just to offer my completely different point of view and say the queries are not new documents of their own right which are entitled to having their own models.",
            "But there are new occurrence of already known models.",
            "And that means that we simply have to do an modification.",
            "And for modification of that sort we have tools which are mainly the Kublai Divergents an log likelihood.",
            "So we run."
        ],
        [
            "Our formulas for the PS I particular model and we have a formula for library and for log likelihood you see that they are fairly similar except for this term.",
            "So how is going?"
        ],
        [
            "Work.",
            "You see here.",
            "OK, in the map and the number of categories in here BM 25.",
            "Here, very Orientals and better than to be in 25 you have the future kernels.",
            "The various sorts of them, and then you have come back logic division, so you see that the couple of divisions has a very small school at only one category, which is rather normal.",
            "Starts improving a little bit a little bit more than he does overtaking BM.",
            "25 and then it's reaching the level off.",
            "The future cannot.",
            "And then it overtakes the Fisher kernels and it ended by over over pouring the Fisher kernels.",
            "On this particular example.",
            "So this is our other favorable example for these new similarities, sorry.",
            "H is Huffman's kernel.",
            "This define.",
            "WO it's the world part of her fans, Colonel.",
            "It's a yes, so the point here is to show that.",
            "Anne.",
            "Well, I I come back later.",
            "If you if you don't mind."
        ],
        [
            "So overall we have two contributions, one if Huffman versus ID and we see that ID is the correct duration and explains the normalization by the numbers of the size of the documents and queries.",
            "And it explains how fast the revision more properly are.",
            "The price of three hypothesis.",
            "One of which is rather wrong and which we have challenged.",
            "Furthermore, we have those alternative similarities.",
            "We show that the similarity in cubelock light blur is better than the log likelihood which was known in the literature we have besides.",
            "So we confirm this result.",
            "We show that smoothing.",
            "The data in this case does not improve the results.",
            "We show that these sort of of similarities have a straight, fully increasing performance with a number of categories.",
            "And they have a major strange is that they need no further computation on the queries.",
            "You can just issue a query and bites surface form.",
            "You can compare it to the documents you know in Pearsei.",
            "And so my conclusion."
        ],
        [
            "Is that if you want to use piaci for information retrieval, you have the choice of using either.",
            "The define that is.",
            "The the kernel of Huffman.",
            "It's word part I would say, and the variant that takes the information Fisher matrix into account.",
            "Or you could use the similarity in cubelock David Black Library Avengers.",
            "An.",
            "That's about it.",
            "Thank you very much.",
            "Or should I assume that all the questions were already?",
            "Please.",
            ", quote.",
            "I I thought that the code backlog that I purchase was just a normalized log likelihood function well.",
            "Uh."
        ],
        [
            "Likelihood that you can get to a lower smallest number.",
            "Well, it's normalized in comparable to make sure that zero by doing that subtraction.",
            "Well in here you see that.",
            "You you remember the the relation I gave about?",
            "The probability of a document the professor documents commensurable to its length divided by the size of the corpus.",
            "So you see here that the difference between log likelihood and quickly visions.",
            "It's mainly that it takes into account the length of the query and the document.",
            "So that make sure that if if they think you are equal, their callback libel, divergent will be 0.",
            "But the log likelihood will never be 0.",
            "Yeah.",
            "Well, you are obviously right in saying that it's normalized.",
            "Yes, it's normalized and it takes a while.",
            "Intuitively it gives more information about the nature of the data you're comparing and so the results end up being better.",
            "Yes, yeah, just comment on that, basically.",
            "Ranking so basically the second term, the difference is in the second term paper documents into account.",
            "Any other questions?",
            "Then I would like to thank.",
            "What?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So welcome everyone and so the 1st paper is on the other side.",
                    "label": 0
                },
                {
                    "sent": "The true future kernel and beyond, and it's going to be presented by in a way like this.",
                    "label": 0
                },
                {
                    "sent": "Ladies and gentlemen, good morning.",
                    "label": 0
                },
                {
                    "sent": "I am in America and I will be presenting PSR the true Fisher Kernel and beyond by my advisor.",
                    "label": 1
                },
                {
                    "sent": "Just extrapolate and myself.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the framework of our work is the problem of representing documentation representing documents for classification of IR.",
                    "label": 0
                },
                {
                    "sent": "That instance lighting a document in human readable form into a mathematical object that the computer can understand and use for comparison.",
                    "label": 0
                },
                {
                    "sent": "We use the model pealess I, which is a metal based on Latin categories.",
                    "label": 0
                },
                {
                    "sent": "That is, it postulates that you don't have any documents and words in the universe, but also topics and that topics are chosen and then generate documents and words.",
                    "label": 0
                },
                {
                    "sent": "Our problem is knowing.",
                    "label": 0
                },
                {
                    "sent": "Pierre Desir, representation of documents how do we?",
                    "label": 0
                },
                {
                    "sent": "Compared two documents or a document in a query.",
                    "label": 0
                },
                {
                    "sent": "In order to give a score and make clustering or give information retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answers.",
                    "label": 0
                },
                {
                    "sent": "So a quick review of the previous I model.",
                    "label": 0
                },
                {
                    "sent": "Polisci probabilistic Latin semantic indexing was proposed by Hoffman in 1990, nine 98.",
                    "label": 0
                },
                {
                    "sent": "It models pairs of.",
                    "label": 0
                },
                {
                    "sent": "In dices of documents and terms?",
                    "label": 0
                },
                {
                    "sent": "A document is supposed to be a set of pairs which share the same document in dice.",
                    "label": 0
                },
                {
                    "sent": "You have a set of topics which are we have the zed in dices.",
                    "label": 0
                },
                {
                    "sent": "And the formula for the probability of a given DW pair is given by this formula.",
                    "label": 0
                },
                {
                    "sent": "The parameters of the model are the probabilities of.",
                    "label": 0
                },
                {
                    "sent": "One of the different categories.",
                    "label": 0
                },
                {
                    "sent": "Then the probabilities of all of the documents known in the corpus given a certain category, and similarly for the words.",
                    "label": 0
                },
                {
                    "sent": "That is, if you see a world like ship in.",
                    "label": 0
                },
                {
                    "sent": "In a in a document, it's probable that you're talking about the sea rather than the desert.",
                    "label": 0
                },
                {
                    "sent": "These parameters here are inferred from the observed data via expectation maximization.",
                    "label": 0
                },
                {
                    "sent": "Sofia is a generative model, but go backwards using expectation Max.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station.",
                    "label": 0
                },
                {
                    "sent": "Now the problem of.",
                    "label": 0
                },
                {
                    "sent": "The similarities between the documents that we have represented using pearsei arises.",
                    "label": 0
                },
                {
                    "sent": "Originally, Huffman used a variant of the cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "On the parameters he learned and the results were not very impressive.",
                    "label": 0
                },
                {
                    "sent": "A few years later, the Fisher Kernels were introduced, which provide proximities between instances of sophistical processes, which is exactly what Peter is about.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "The idea of using Fisher kernels for comparison in parasite is rather straightforward and were used by Hoffman again in 2001.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The kernel that Huffman used.",
                    "label": 0
                },
                {
                    "sent": "Is this one so I'm just showing the equation to you get a taste of the idea.",
                    "label": 0
                },
                {
                    "sent": "You see that you have two components.",
                    "label": 0
                },
                {
                    "sent": "One is a sum over the categories.",
                    "label": 0
                },
                {
                    "sent": "On the top and the second one is a sum over the the terms.",
                    "label": 0
                },
                {
                    "sent": "You see, if you study the equation a little bit, it's not quite apparent here, but the Fisher information.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, the official kernel here contains.",
                    "label": 0
                },
                {
                    "sent": "A Fisher information matrix which encodes the topology of the universe.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Fisher information matrix.",
                    "label": 0
                },
                {
                    "sent": "Here's here.",
                    "label": 0
                },
                {
                    "sent": "Approximal are approximated by the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Furthermore this Fisher kernel has been developed by largely ignoring the generative process of pearsei.",
                    "label": 1
                },
                {
                    "sent": "It postulates the documents are entities rather than actually.",
                    "label": 0
                },
                {
                    "sent": "Describing what happens when you generate one pair of indicies after another?",
                    "label": 0
                },
                {
                    "sent": "And to finish.",
                    "label": 0
                },
                {
                    "sent": "You have here and here at normalization by size of document size of query.",
                    "label": 0
                },
                {
                    "sent": "And it's also present in the term zed for it's not quite so evident, but I'll come back to that later.",
                    "label": 0
                },
                {
                    "sent": "But it's also here, and these normalizations are not justified properly.",
                    "label": 0
                },
                {
                    "sent": "In half months paper.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to know more about.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These issues.",
                    "label": 0
                },
                {
                    "sent": "And we decided to re derive the kernel.",
                    "label": 0
                },
                {
                    "sent": "By accounting for the process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We come back to the fundamental.",
                    "label": 0
                },
                {
                    "sent": "Peerless Age is an IID process of generating pairs.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, we know that for an ID process, the future cannot will be.",
                    "label": 0
                },
                {
                    "sent": "I continuation of elementary Fisher kernels.",
                    "label": 0
                },
                {
                    "sent": "Each coding one of the particular elements of the ID process.",
                    "label": 0
                },
                {
                    "sent": "With this formula, the demonstration in the paper.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, yes one turn.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So by using this same for new kernel.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We developed the atomic so called atomic kernel 4142 pairs of indexes.",
                    "label": 0
                },
                {
                    "sent": "Which is like that.",
                    "label": 0
                },
                {
                    "sent": "And then we can assemble them into a logical ological for the entire process.",
                    "label": 0
                },
                {
                    "sent": "You know the difference, sorry.",
                    "label": 0
                },
                {
                    "sent": "It comes from the definition of the future kernel.",
                    "label": 0
                },
                {
                    "sent": "So yes, so yeah, good question.",
                    "label": 0
                },
                {
                    "sent": "So these are the future information Fisher Information matrix which is also separated into components specify specifically for the categories and here for the terms and we explicitly know that because we are willing to challenge the notion that this is simply the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So to come back to.",
                    "label": 0
                },
                {
                    "sent": "It's just different parts of the matrix actually.",
                    "label": 0
                },
                {
                    "sent": "The Matrix will look like something like this.",
                    "label": 0
                },
                {
                    "sent": "With the part here for a for the categories and apart here for.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is just this and here for the terms.",
                    "label": 0
                },
                {
                    "sent": "Knowing that so.",
                    "label": 0
                },
                {
                    "sent": "Yes, we that's a I didn't mention it, but yes we do.",
                    "label": 0
                },
                {
                    "sent": "We do assume that it's going to be diagonal.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because you have in the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fisher formula you have to invert this matrix and given the size of matrix inverting it is simply not realistic.",
                    "label": 0
                },
                {
                    "sent": "So we assume that is diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the comparison between our neways Colonel Ann.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Months cannot.",
                    "label": 0
                },
                {
                    "sent": "The kernel has four support, the set of two 2 words because you don't always compare the the contribution of one word with its own other comparisons, also with its.",
                    "label": 0
                },
                {
                    "sent": "Own other contributions.",
                    "label": 0
                },
                {
                    "sent": "So that makes the idea kernel very costly.",
                    "label": 0
                },
                {
                    "sent": "Compute much more costly than Huffman's.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have a similarity between the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The kernel, which arises from here simply, it means that we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see the normalization by document and query length coming quite naturally.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just for this part.",
                    "label": 0
                },
                {
                    "sent": "Now the problem with.",
                    "label": 0
                },
                {
                    "sent": "Hoffman is that is still different from Idi is different from Hoffman, so we want to take a closer look at the assumptions between behind the Huffman scanner.",
                    "label": 0
                },
                {
                    "sent": "We see that health Max makes the assumption that overall words.",
                    "label": 0
                },
                {
                    "sent": "The observed probability the.",
                    "label": 0
                },
                {
                    "sent": "Stocks in well there.",
                    "label": 0
                },
                {
                    "sent": "Yes, the observed data.",
                    "label": 0
                },
                {
                    "sent": "The probability is a good estimator of the probability as it's going to be inferred by expectation maximization.",
                    "label": 1
                },
                {
                    "sent": "It's not true for any particular word, but overall the world is going to work very well.",
                    "label": 0
                },
                {
                    "sent": "That's the assumption.",
                    "label": 0
                },
                {
                    "sent": "Then you have the second assumption that GD Fisher Information matrix is similar to the identity.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, we introduce an observation is that the probability of a document is commensurable to its length divided by the length of all the corpus.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 1
                },
                {
                    "sent": "Yes, this is an observation we make.",
                    "label": 0
                },
                {
                    "sent": "So if we introduce those relations those assumptions and is a perversion into the formula of the kernel, you.",
                    "label": 0
                },
                {
                    "sent": "You put this, you there relation, you find that you.",
                    "label": 0
                },
                {
                    "sent": "You end up with half months Colonel, but it means that at the price of these three.",
                    "label": 0
                },
                {
                    "sent": "Relations.",
                    "label": 1
                },
                {
                    "sent": "Huffman's collection be seen as an approximation of the idea, Colonel.",
                    "label": 0
                },
                {
                    "sent": "And that could explain why it works.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we've also tried removing the normalization by documents and query length and works rather worse than Huffman's kernel.",
                    "label": 0
                },
                {
                    "sent": "So, having identified the airport, is this, we won't know to challenge their ability.",
                    "label": 0
                },
                {
                    "sent": "The the assumption that the observed probability is going to be consistent with the.",
                    "label": 0
                },
                {
                    "sent": "Probability as inferred by expectation maximization, is a big simplification particuli for K for the category part of the kernel.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "The that the probability of the document is commensurable to its length divided by the length of the corpus is.",
                    "label": 0
                },
                {
                    "sent": "Both theoretically and experimentally justified.",
                    "label": 0
                },
                {
                    "sent": "But seeing the Fisher information matrix as identity is neither theoretically nor experimentally verified.",
                    "label": 0
                },
                {
                    "sent": "So it's a big simplification for the equation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "WHI did Huffman portrayed that this matrix is the identity?",
                    "label": 0
                },
                {
                    "sent": "He doesn't say so, but we can infer that he does that by analogy with multi human models.",
                    "label": 1
                },
                {
                    "sent": "The problem is that pizza is not a material.",
                    "label": 0
                },
                {
                    "sent": "It's not even in the expansion family.",
                    "label": 0
                },
                {
                    "sent": "So we actually derivate.",
                    "label": 1
                },
                {
                    "sent": "The information matrix and we add it to our formulas.",
                    "label": 0
                },
                {
                    "sent": "An we test it.",
                    "label": 1
                },
                {
                    "sent": "So four tests.",
                    "label": 0
                },
                {
                    "sent": "For the experiment framework we used an overlay a software overlay that we coded.",
                    "label": 0
                },
                {
                    "sent": "Over exception we use caption for indication for the access, storage and data copy and has a good stemming tool, good documentation etc.",
                    "label": 0
                },
                {
                    "sent": "And we evaluated our methods on the data of smart, which is the traditional set on which Pearsei was attempted.",
                    "label": 0
                },
                {
                    "sent": "And we also try it on a subset of the AP 85 corpus from track.",
                    "label": 0
                },
                {
                    "sent": "This allows is to build much larger corpus than the smart ones, and to test up to where we can you spare design.",
                    "label": 0
                },
                {
                    "sent": "These corpora are rather small by true destiny in IR, but they are quite big for PS I.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "88.9 did you use or how many queries?",
                    "label": 0
                },
                {
                    "sent": "We used 50 or 100, I don't.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a bit of documents, yes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the results give.",
                    "label": 0
                },
                {
                    "sent": "That you can see clearly two families this here looking like a smile is the family of the kernels which suppose that the fish information matrix looks like the identity.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, excuse me, this is the mean average precision and here we have the number of categories that we give for model number of topics.",
                    "label": 0
                },
                {
                    "sent": "So the models that postulate that the information matrix is the identity have a decreasing performance up to some point.",
                    "label": 0
                },
                {
                    "sent": "At which point they start increasing again slowly.",
                    "label": 0
                },
                {
                    "sent": "But the models which have a more sophisticated representation of the information matrix.",
                    "label": 0
                },
                {
                    "sent": "Are mostly keeping to the to the top of the of the chart.",
                    "label": 0
                },
                {
                    "sent": "So we have a good validation that the efficient formation metric is indeed not that close to the identity, and that computing it more properly is a valued contribution.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, you see that.",
                    "label": 0
                },
                {
                    "sent": "Here you have the ID Kerner, and here you have the half mask error.",
                    "label": 0
                },
                {
                    "sent": "So you see that they are indeed as we predicted.",
                    "label": 0
                },
                {
                    "sent": "Fairly faithful to each other.",
                    "label": 0
                },
                {
                    "sent": "And that's also for slightly less true 2 for the.",
                    "label": 0
                },
                {
                    "sent": "For the kernels that take the information metrics into account, you see that here Huffmans cannot dominate a bit.",
                    "label": 0
                },
                {
                    "sent": "the ID cannot, because in the case of.",
                    "label": 0
                },
                {
                    "sent": "Taking the information matrix into account.",
                    "label": 0
                },
                {
                    "sent": "The equality between the two kernels is not that that much verify mean, it's a, it's a it's a loser approximation of the kernel and it happens to work pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to give a second contribution of our paper.",
                    "label": 0
                },
                {
                    "sent": "Which is about trying to find alternative proximities than diffusion kernels because of the Fisher kernels.",
                    "label": 1
                },
                {
                    "sent": "Need first they are costing compute and then you have big problems with the queries you need to compute the parameters for the queries, But these you can't compute either.",
                    "label": 0
                },
                {
                    "sent": "You have to possibly that you know the queries before hand which is contradictory with the premises of information retrieval.",
                    "label": 0
                },
                {
                    "sent": "Or you have to use a hack which is called folding in an which entails theoretical problems more specifically with the estimation of likelihood.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to.",
                    "label": 0
                },
                {
                    "sent": "Just to offer my completely different point of view and say the queries are not new documents of their own right which are entitled to having their own models.",
                    "label": 0
                },
                {
                    "sent": "But there are new occurrence of already known models.",
                    "label": 0
                },
                {
                    "sent": "And that means that we simply have to do an modification.",
                    "label": 0
                },
                {
                    "sent": "And for modification of that sort we have tools which are mainly the Kublai Divergents an log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we run.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our formulas for the PS I particular model and we have a formula for library and for log likelihood you see that they are fairly similar except for this term.",
                    "label": 0
                },
                {
                    "sent": "So how is going?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "You see here.",
                    "label": 0
                },
                {
                    "sent": "OK, in the map and the number of categories in here BM 25.",
                    "label": 0
                },
                {
                    "sent": "Here, very Orientals and better than to be in 25 you have the future kernels.",
                    "label": 0
                },
                {
                    "sent": "The various sorts of them, and then you have come back logic division, so you see that the couple of divisions has a very small school at only one category, which is rather normal.",
                    "label": 0
                },
                {
                    "sent": "Starts improving a little bit a little bit more than he does overtaking BM.",
                    "label": 0
                },
                {
                    "sent": "25 and then it's reaching the level off.",
                    "label": 0
                },
                {
                    "sent": "The future cannot.",
                    "label": 0
                },
                {
                    "sent": "And then it overtakes the Fisher kernels and it ended by over over pouring the Fisher kernels.",
                    "label": 0
                },
                {
                    "sent": "On this particular example.",
                    "label": 0
                },
                {
                    "sent": "So this is our other favorable example for these new similarities, sorry.",
                    "label": 0
                },
                {
                    "sent": "H is Huffman's kernel.",
                    "label": 0
                },
                {
                    "sent": "This define.",
                    "label": 0
                },
                {
                    "sent": "WO it's the world part of her fans, Colonel.",
                    "label": 0
                },
                {
                    "sent": "It's a yes, so the point here is to show that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, I I come back later.",
                    "label": 0
                },
                {
                    "sent": "If you if you don't mind.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So overall we have two contributions, one if Huffman versus ID and we see that ID is the correct duration and explains the normalization by the numbers of the size of the documents and queries.",
                    "label": 0
                },
                {
                    "sent": "And it explains how fast the revision more properly are.",
                    "label": 0
                },
                {
                    "sent": "The price of three hypothesis.",
                    "label": 1
                },
                {
                    "sent": "One of which is rather wrong and which we have challenged.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, we have those alternative similarities.",
                    "label": 1
                },
                {
                    "sent": "We show that the similarity in cubelock light blur is better than the log likelihood which was known in the literature we have besides.",
                    "label": 0
                },
                {
                    "sent": "So we confirm this result.",
                    "label": 0
                },
                {
                    "sent": "We show that smoothing.",
                    "label": 1
                },
                {
                    "sent": "The data in this case does not improve the results.",
                    "label": 0
                },
                {
                    "sent": "We show that these sort of of similarities have a straight, fully increasing performance with a number of categories.",
                    "label": 0
                },
                {
                    "sent": "And they have a major strange is that they need no further computation on the queries.",
                    "label": 0
                },
                {
                    "sent": "You can just issue a query and bites surface form.",
                    "label": 0
                },
                {
                    "sent": "You can compare it to the documents you know in Pearsei.",
                    "label": 0
                },
                {
                    "sent": "And so my conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that if you want to use piaci for information retrieval, you have the choice of using either.",
                    "label": 0
                },
                {
                    "sent": "The define that is.",
                    "label": 0
                },
                {
                    "sent": "The the kernel of Huffman.",
                    "label": 0
                },
                {
                    "sent": "It's word part I would say, and the variant that takes the information Fisher matrix into account.",
                    "label": 0
                },
                {
                    "sent": "Or you could use the similarity in cubelock David Black Library Avengers.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "That's about it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Or should I assume that all the questions were already?",
                    "label": 0
                },
                {
                    "sent": "Please.",
                    "label": 0
                },
                {
                    "sent": ", quote.",
                    "label": 0
                },
                {
                    "sent": "I I thought that the code backlog that I purchase was just a normalized log likelihood function well.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Likelihood that you can get to a lower smallest number.",
                    "label": 0
                },
                {
                    "sent": "Well, it's normalized in comparable to make sure that zero by doing that subtraction.",
                    "label": 0
                },
                {
                    "sent": "Well in here you see that.",
                    "label": 0
                },
                {
                    "sent": "You you remember the the relation I gave about?",
                    "label": 0
                },
                {
                    "sent": "The probability of a document the professor documents commensurable to its length divided by the size of the corpus.",
                    "label": 0
                },
                {
                    "sent": "So you see here that the difference between log likelihood and quickly visions.",
                    "label": 0
                },
                {
                    "sent": "It's mainly that it takes into account the length of the query and the document.",
                    "label": 0
                },
                {
                    "sent": "So that make sure that if if they think you are equal, their callback libel, divergent will be 0.",
                    "label": 0
                },
                {
                    "sent": "But the log likelihood will never be 0.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, you are obviously right in saying that it's normalized.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's normalized and it takes a while.",
                    "label": 0
                },
                {
                    "sent": "Intuitively it gives more information about the nature of the data you're comparing and so the results end up being better.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah, just comment on that, basically.",
                    "label": 0
                },
                {
                    "sent": "Ranking so basically the second term, the difference is in the second term paper documents into account.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Then I would like to thank.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                }
            ]
        }
    }
}