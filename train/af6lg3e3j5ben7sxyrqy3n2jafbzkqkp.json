{
    "id": "af6lg3e3j5ben7sxyrqy3n2jafbzkqkp",
    "title": "Bayesian Gaussian process latent variable model",
    "info": {
        "author": [
            "Michalis K. Titsias, School of Mathematics, University of Manchester"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2010_titsias_bgp/",
    "segmentation": [
        [
            "Social everybody I'm mcalister's yes from University of Manchester.",
            "And I'm going to present you a joint work with near Lawrence about the Gaussian process, the."
        ],
        [
            "Nevada model so what's most people in the audience know?",
            "Gaussian processes are usually used for supervised learning problems.",
            "This means that the input in these models are assumed to be observed deterministic.",
            "One way of doing unsupervised learning using the Gaussian using Gaussian processes is the Gaussian process latent viral model.",
            "That VM, which however is trained by optimizing and not marginalizing out the latent variables.",
            "So in this talk we wish to address the following questions.",
            "How can we train Gaussian process models?",
            "One needs will inputs are random variables.",
            "For example, have uncertain inputs or we have missing values in this image.",
            "And also we would like to answer the question how can marginalized out the latent viral latent value sensitive VM and do more full Bayesian inference the way that we approach this problem is by introducing a variational based framework that will give us approximate Bayesian solutions.",
            "So this is."
        ],
        [
            "The outline of my talk.",
            "I will first describe this variational methods for Gaussian process with random inputs, assuming a very basic model, gross and gross regression.",
            "I will explain the role of auxiliary parameters we have to add some parameters in order to be able to apply the variational methods.",
            "I will show you the form of the variational bound.",
            "Then I will apply this method to GP LVM and I will show you how to perform automatic selection of the of the nonlinear latent dimensionality in simple VM using this query exponential error Journal.",
            "I will show you some experiments and finally I will summarize."
        ],
        [
            "So go somewhere else this I go.",
            "Some processes users are not parametric by of prior over, some function that we would like to estimate for exams as a function generation function or it can be the decision boundary some classification problem.",
            "In any zippy more than any dipping model, looks like this.",
            "You have some data, outputs and inputs.",
            "The ultimately will be denoted by Y and they put by X and you have.",
            "The joint model was given by a lucky good times that the prior as you can observe, this model is actually a conditional discriminating model.",
            "Basically you condition on X.",
            "You assume that this is deterministic.",
            "You don't actually model any distribution of X, so these are the questions.",
            "The question how can we deal with inputs with Arduino?"
        ],
        [
            "So let's generalize the model to deal with that.",
            "So now the model would be like as exactly as before, but Additionally we're going to place a prior distribution, for example Gaussian.",
            "Or the inputs?",
            "This can be useful when the inputs can be as a uncertain noisy metric.",
            "Measurements have missing values or can be latent virus when within nonlinear dimensionality reduction.",
            "However, Despite that graphically, this extension seems very simple.",
            "Computational is actually much more difficult, while because the posterior now over the function.",
            "The function value serve and the inputs given the data becomes analytical, intractable, and of course we cannot compute those are the marginal likelihood.",
            "So the first thing that comes to our mind is how can we apply some approximate inference methods?",
            "For example, a standard variational Bayes method.",
            "It turns out that this problem actually is quite difficult.",
            "It's not trivial.",
            "We cannot actually apply the standard methodology.",
            "You have to add something.",
            "So I'm going down with the next slide is slides to explain the difficulties and then I will actually show you how to deal with this difficulties.",
            "So."
        ],
        [
            "We have the model.",
            "And I just have explicitly write down the Gaussian distribution.",
            "Who's basically the Gaussian process prior, right?",
            "So imagine now that we're going to apply mean field approximation, so I'm going to assume additional distribution that factorize over the function in the inputs and we would like to optimize over this distribution, right?",
            "So as you can see from this from the second equation, then puts appear nonlinearly inside the inverse of the kernel matrix inside the determinant.",
            "It seems to be impossible actually to try to compute operational lower bounding the great out of this inputs."
        ],
        [
            "Let's see actually another simple example.",
            "This is the simplest possible model, right Bayesian linear regression.",
            "You have to use for this model.",
            "You can you explain this using standard program parameters.",
            "The weight, the weight view.",
            "If you're familiar with Gaussian process terminology where you have this WS.",
            "And you have a Gaussian prior over this, the W's and also you have random inputs.",
            "Now if you are familiar with the restaurant, inference is extremely straightforward to apply operational approximation, right?",
            "I mean field.",
            "However, when you generalize the model, would you make it no parametric?",
            "Then?",
            "Variational inference becomes very difficult.",
            "This actually is really weird, but at the same time is really interesting.",
            "So let's try to draw a more general picture for the."
        ],
        [
            "So I mean, what is the cost of that Gaussian process?",
            "Or maybe general methods error are somehow marginalized collapse models?",
            "We have a deep is an extensible model, so the underlying parameter has been degraded out.",
            "If you like this definitely representation.",
            "So in order to.",
            "Send out the input.",
            "We have to place back some parameters at least approximate parameters and this is what we're going to do basically, and the parameters we are going to use it will be extra functions, auxiliary function values basically."
        ],
        [
            "So here is the idea.",
            "This is the original model, the kernel model.",
            "Let's say that the model in the space of the data, the function F and the inputs, and as we explained by this.",
            "Is quite difficult.",
            "It's very difficult as they know Lena signed the killing.",
            "So what we're going to do is to automate this model with extra.",
            "Function values.",
            "Call it you consistent way.",
            "And then I'm going to apply various reference in this augmented model.",
            "So before actually continue explaining this I will basically try to visualize what we're going to be.",
            "This extra function values."
        ],
        [
            "So let's see a picture.",
            "Here we see a function with a red.",
            "It would be this extra function points with.",
            "Sorry with a blue with red, it would be the inputs of this extra function points with the green curve is the function by the conditional prior.",
            "I will explain actually what means this condition prior and they say the area is basically uncertain T when we draw from this condition prior.",
            "So."
        ],
        [
            "Is your parameter that specifies the function.",
            "Board actually will turn out to be a Delta function and also this depends on the kitchen light.",
            "If the kennel is linear, then in at 1 two dimensional space then I need just two points to specify the function exactly.",
            "So in that case the condition prior actually would be a Delta function.",
            "So now you are going to use this text."
        ],
        [
            "But I'm at the stoplight by this run inference.",
            "As we said before, we have the initial model we augmented and what we're going to do is to apply a standard variational Bayes method.",
            "So we assume that."
        ],
        [
            "So the solution that factorizes for news, the one piece of the violation distribution, it will be the conditional prior itself.",
            "And this is actually the trick that makes everything tractable and the other party would be admin field part.",
            "You will have a mean field approximation with respect to their parameters that we already introduced the use and the inputs.",
            "So the distribution of arrangements will be Gaussian there the file distribution would be under strict, but it will turn out to be Gaussian.",
            "And as I said, the trick is that one part of the variational distribution will be the conditional priority.",
            "So now imagine that we are going to compute the lower bound and try to maximize it, right?",
            "So the first bit of the equation in the first line is the exact measure likelihood, and once you apply this inequality and put the variational distribution to that equation.",
            "As you see the day prior, because appears both in the model and version distribution, they will cancel out and after this cancellation method becomes tractable.",
            "A gentle explain."
        ],
        [
            "Full detail this because you have to go through the mass to see this, but what they're going to say is that this bound now is tractable, and it's actually general because I can compute this for many kernels.",
            "For linear kernels, for square, exponential, exponential polynomials, and probably many others, so this is general framework actually, so the way that this is maximized is buys only.",
            "Applying a gradient based optimization of variation of parameters and model hyperparameters.",
            "So let's now."
        ],
        [
            "Apply this to Gaussian process latent variable model.",
            "So gossip related environment is a latent variable model.",
            "What's the latent variable model you have?",
            "You have a latent SpaceX that fits into our latent mapping and related.",
            "Maybe gives an object variable, for example eight in the later mapping can be linear and then you get factor analysis of or PCA in that VM.",
            "The later Magic is given Gaussian process priors.",
            "However the GP LV.",
            "Strained by optimizing and not marginalizing out the latent variables.",
            "And this basically means that there is no proper density in the latent space.",
            "The latent space, that density is basically a mixture of Delta functions.",
            "Somehow, we cannot select the dimensionality becauses not fully Bayesian."
        ],
        [
            "The training and we might also overfit.",
            "So we would like to apply some more full Bayesian approach.",
            "And of course this is and basically treat both the latent space and the latent mapping in in a basic way.",
            "So trying to marginalise them out.",
            "Of course this is intractable, So what we're going to do is to apply our variational approximation and this is done exactly as explained in the same standard regression model.",
            "Not very useful aspects of this approach is that you allow us to perform automatic selection of the lady dimensionality, for example."
        ],
        [
            "If I choose, the tenant will be the square, exponential, errored eternal error demeans automatic relevance determination, which has the equation shows puts a different different inverse link scale for this latent dimension.",
            "This will allow me, but my maximizing the variational lower bound over these parameters to remove redundant redundant latent dimensions so I can start with a large number of latent domain source and let the lower bound decide how many they are going to be."
        ],
        [
            "Boost.",
            "So let's apply this to some exams.",
            "I will first explore and examine visualization in order flow data and I'm going to compare the basin, similar them with standard zippy LVM and probabilistic PCA.",
            "So this is the solution of Bayesian TB LVM, the pic."
        ],
        [
            "Sure, on the on the on the on the left my side on the right of your side, sorry.",
            "We saw the bus, so the inverse length scales, so we start assuming tent tent, tent latent dimensions.",
            "So as you can see out of 10 only three survive.",
            "The seven are strong automatical to 0.",
            "So basically the model selects that dimensionality must be must be free and also the visualization actually is really high quality.",
            "As compared for example."
        ],
        [
            "Through through other methods to the standard deviation, mean PCA.",
            "For example, you see that the invasions, LVM, the two different classes because in this data, although we treated us in a supervised way, there are actually three classes.",
            "Different stages of their own flow.",
            "We can see that visualization is much much superior for the business development.",
            "Also, these classes are well separated from each other.",
            "A second exam."
        ],
        [
            "Is to show how can how can process partially observed data.",
            "So what I'm going to do in this experiment is to.",
            "To choose a data set, I will use a fray.",
            "The fray faces and I will take one part to train the model and then the other part of the data set.",
            "It would be for testing, but when I'm going to test the model, I'm not going to give the email This, I will just give partial images.",
            "So the first row, the first row in the picture shows the exact which is sorry, which is the true test team us the 2nd row which is solved the partial observed image which is basically processed by the model.",
            "And the third row shows the reconstructed image from the model from the DB LVM.",
            "From the business in Belgium.",
            "So synthetic construction makes error in this example is 7.4 any superior to when you apply the standard ZP LVM using different settings of the latent dimensionality, the dimensionality for the program was set to 30.",
            "And a further example."
        ],
        [
            "What did you use this model is to to apply for that generative classification.",
            "So, so, one way to to apply this vision to prevent?",
            "Basically, if you apply for example in this case, we applied for the in the disease that are set, so for this did visit, we run a separate basin, tibial them and run and get a class conditional estimate based on class conditional estimator.",
            "Then we can use this not in generative classifier and get an error there.",
            "We got in this other service 40 points.",
            "4.73 which is quite close I think, to discriminating methods.",
            "I mean, fingers support vector machines gives around.",
            "3%.",
            "And finally, I'm going to summarize."
        ],
        [
            "He presented with reduced variation framework to approximately the great out inputs in Gaussian process.",
            "This is general, allows for for.",
            "For full Bayesian it training of OLVM, and some interesting topics for future work is how to speed up the optimization and then actually spoke to Matt about this, but we're using cause again gradients, but probably the other way.",
            "Other ways of doing this for exam discovered and fixed point equations and another is to apply this method.",
            "For learning new parametric nonlinear dynamical systems using Gaussian processes and variational Bayes.",
            "Friend you."
        ],
        [
            "Because I'm faster questions.",
            "When you're doing optimization, there's the variational lower bound.",
            "What you doing optimization is it's only unique optimum.",
            "No, that is their local Maxima.",
            "Creative then I'm not so much situation.",
            "If you get trapped on false maximum.",
            "Yeah yeah.",
            "Sure that problems with.",
            "Yeah this is not linear optimization.",
            "So that a local maximum we don't get active to find the best solution.",
            "So did you.",
            "In your experiments you decide something like this with families.",
            "Well, he's basically find out one run without authorization.",
            "For all these runs, basically, yeah, I mean idea for the overflow.",
            "For example, I just do one.",
            "Although I fixed the, you know the random seed so.",
            "Because I want to produce the experience.",
            "But yeah, it's possible to find different solutions, although I mean always the Bayesian Sylvia Levine.",
            "Actually, I think the minimum actually is not so severe becausw.",
            "Because you have additional approximation, so you have variance in the latent space, right?",
            "And this kind of smooth objective function so you think you are able to find better, better solutions.",
            "You don't actually find the map estimate over there.",
            "You know for the latent positions also you have the variance.",
            "So if you like the balance to be slightly bigger, then basically you kind of smooth the objective function and that kind of helps.",
            "But the optimization is 'cause I mean that's.",
            "That's for sure.",
            "Honey, you points to use or how you choose.",
            "Yes, that's a very good question.",
            "OK, so I mean, the more you use it, the better.",
            "I mean in the variational approximation.",
            "Next, let's say this is the first equation right this time in field so."
        ],
        [
            "But the first part is the conditions in prior.",
            "The best hope, the best we can do with the condition of priority is to become a Delta function.",
            "So basically reducing points to exactly the producer function.",
            "But in practice you cannot use too many becausw.",
            "We actually optimize over there using inputs, and this is also our variational parameters.",
            "So you want to be numerically, you know like OK, you don't have to increase to match the parameters.",
            "But also this depends on the kernel mean.",
            "For the linear Callan this becomes Delta function.",
            "If you just use you know D + 1 reducing points.",
            "You want these basically there.",
            "The dimensionality of the.",
            "Vex but this is, yeah, I mean it's there.",
            "Would you know the model using Teradata better?",
            "But in practice you should not use two months?",
            "Probably.",
            "So distribution where X is Gaussian as we see here.",
            "Neutralize it to allocate this and can you.",
            "Earth.",
            "No, I mean I haven't tried but.",
            "Probably I mean some situation, maybe actually is possible.",
            "I mean, but a natural choice is the Gaussian, because this imposes you know they are real numbers.",
            "They're not constantly positive.",
            "I guess if you're constrained in some way, maybe it makes sense.",
            "You know to have a different distribution.",
            "Maybe our grammar.",
            "Of course, if you have are you have a mixture of history and mixture of Gaussians, then you have the user problems with very strong inference.",
            "I mean you cannot optimize usually very easily, you know.",
            "A mixture of you know various on main fields approximations, so this is a hard so.",
            "But possibly probably is."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Social everybody I'm mcalister's yes from University of Manchester.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to present you a joint work with near Lawrence about the Gaussian process, the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nevada model so what's most people in the audience know?",
                    "label": 0
                },
                {
                    "sent": "Gaussian processes are usually used for supervised learning problems.",
                    "label": 0
                },
                {
                    "sent": "This means that the input in these models are assumed to be observed deterministic.",
                    "label": 0
                },
                {
                    "sent": "One way of doing unsupervised learning using the Gaussian using Gaussian processes is the Gaussian process latent viral model.",
                    "label": 0
                },
                {
                    "sent": "That VM, which however is trained by optimizing and not marginalizing out the latent variables.",
                    "label": 1
                },
                {
                    "sent": "So in this talk we wish to address the following questions.",
                    "label": 1
                },
                {
                    "sent": "How can we train Gaussian process models?",
                    "label": 0
                },
                {
                    "sent": "One needs will inputs are random variables.",
                    "label": 0
                },
                {
                    "sent": "For example, have uncertain inputs or we have missing values in this image.",
                    "label": 0
                },
                {
                    "sent": "And also we would like to answer the question how can marginalized out the latent viral latent value sensitive VM and do more full Bayesian inference the way that we approach this problem is by introducing a variational based framework that will give us approximate Bayesian solutions.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will first describe this variational methods for Gaussian process with random inputs, assuming a very basic model, gross and gross regression.",
                    "label": 0
                },
                {
                    "sent": "I will explain the role of auxiliary parameters we have to add some parameters in order to be able to apply the variational methods.",
                    "label": 1
                },
                {
                    "sent": "I will show you the form of the variational bound.",
                    "label": 1
                },
                {
                    "sent": "Then I will apply this method to GP LVM and I will show you how to perform automatic selection of the of the nonlinear latent dimensionality in simple VM using this query exponential error Journal.",
                    "label": 0
                },
                {
                    "sent": "I will show you some experiments and finally I will summarize.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So go somewhere else this I go.",
                    "label": 0
                },
                {
                    "sent": "Some processes users are not parametric by of prior over, some function that we would like to estimate for exams as a function generation function or it can be the decision boundary some classification problem.",
                    "label": 1
                },
                {
                    "sent": "In any zippy more than any dipping model, looks like this.",
                    "label": 0
                },
                {
                    "sent": "You have some data, outputs and inputs.",
                    "label": 0
                },
                {
                    "sent": "The ultimately will be denoted by Y and they put by X and you have.",
                    "label": 0
                },
                {
                    "sent": "The joint model was given by a lucky good times that the prior as you can observe, this model is actually a conditional discriminating model.",
                    "label": 1
                },
                {
                    "sent": "Basically you condition on X.",
                    "label": 0
                },
                {
                    "sent": "You assume that this is deterministic.",
                    "label": 0
                },
                {
                    "sent": "You don't actually model any distribution of X, so these are the questions.",
                    "label": 0
                },
                {
                    "sent": "The question how can we deal with inputs with Arduino?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's generalize the model to deal with that.",
                    "label": 0
                },
                {
                    "sent": "So now the model would be like as exactly as before, but Additionally we're going to place a prior distribution, for example Gaussian.",
                    "label": 1
                },
                {
                    "sent": "Or the inputs?",
                    "label": 0
                },
                {
                    "sent": "This can be useful when the inputs can be as a uncertain noisy metric.",
                    "label": 0
                },
                {
                    "sent": "Measurements have missing values or can be latent virus when within nonlinear dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "However, Despite that graphically, this extension seems very simple.",
                    "label": 0
                },
                {
                    "sent": "Computational is actually much more difficult, while because the posterior now over the function.",
                    "label": 1
                },
                {
                    "sent": "The function value serve and the inputs given the data becomes analytical, intractable, and of course we cannot compute those are the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that comes to our mind is how can we apply some approximate inference methods?",
                    "label": 1
                },
                {
                    "sent": "For example, a standard variational Bayes method.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this problem actually is quite difficult.",
                    "label": 0
                },
                {
                    "sent": "It's not trivial.",
                    "label": 0
                },
                {
                    "sent": "We cannot actually apply the standard methodology.",
                    "label": 0
                },
                {
                    "sent": "You have to add something.",
                    "label": 0
                },
                {
                    "sent": "So I'm going down with the next slide is slides to explain the difficulties and then I will actually show you how to deal with this difficulties.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have the model.",
                    "label": 0
                },
                {
                    "sent": "And I just have explicitly write down the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Who's basically the Gaussian process prior, right?",
                    "label": 0
                },
                {
                    "sent": "So imagine now that we're going to apply mean field approximation, so I'm going to assume additional distribution that factorize over the function in the inputs and we would like to optimize over this distribution, right?",
                    "label": 1
                },
                {
                    "sent": "So as you can see from this from the second equation, then puts appear nonlinearly inside the inverse of the kernel matrix inside the determinant.",
                    "label": 1
                },
                {
                    "sent": "It seems to be impossible actually to try to compute operational lower bounding the great out of this inputs.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see actually another simple example.",
                    "label": 0
                },
                {
                    "sent": "This is the simplest possible model, right Bayesian linear regression.",
                    "label": 1
                },
                {
                    "sent": "You have to use for this model.",
                    "label": 0
                },
                {
                    "sent": "You can you explain this using standard program parameters.",
                    "label": 0
                },
                {
                    "sent": "The weight, the weight view.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with Gaussian process terminology where you have this WS.",
                    "label": 0
                },
                {
                    "sent": "And you have a Gaussian prior over this, the W's and also you have random inputs.",
                    "label": 1
                },
                {
                    "sent": "Now if you are familiar with the restaurant, inference is extremely straightforward to apply operational approximation, right?",
                    "label": 0
                },
                {
                    "sent": "I mean field.",
                    "label": 0
                },
                {
                    "sent": "However, when you generalize the model, would you make it no parametric?",
                    "label": 1
                },
                {
                    "sent": "Then?",
                    "label": 0
                },
                {
                    "sent": "Variational inference becomes very difficult.",
                    "label": 0
                },
                {
                    "sent": "This actually is really weird, but at the same time is really interesting.",
                    "label": 0
                },
                {
                    "sent": "So let's try to draw a more general picture for the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I mean, what is the cost of that Gaussian process?",
                    "label": 0
                },
                {
                    "sent": "Or maybe general methods error are somehow marginalized collapse models?",
                    "label": 1
                },
                {
                    "sent": "We have a deep is an extensible model, so the underlying parameter has been degraded out.",
                    "label": 1
                },
                {
                    "sent": "If you like this definitely representation.",
                    "label": 0
                },
                {
                    "sent": "So in order to.",
                    "label": 0
                },
                {
                    "sent": "Send out the input.",
                    "label": 0
                },
                {
                    "sent": "We have to place back some parameters at least approximate parameters and this is what we're going to do basically, and the parameters we are going to use it will be extra functions, auxiliary function values basically.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the idea.",
                    "label": 1
                },
                {
                    "sent": "This is the original model, the kernel model.",
                    "label": 0
                },
                {
                    "sent": "Let's say that the model in the space of the data, the function F and the inputs, and as we explained by this.",
                    "label": 0
                },
                {
                    "sent": "Is quite difficult.",
                    "label": 0
                },
                {
                    "sent": "It's very difficult as they know Lena signed the killing.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is to automate this model with extra.",
                    "label": 0
                },
                {
                    "sent": "Function values.",
                    "label": 0
                },
                {
                    "sent": "Call it you consistent way.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to apply various reference in this augmented model.",
                    "label": 0
                },
                {
                    "sent": "So before actually continue explaining this I will basically try to visualize what we're going to be.",
                    "label": 0
                },
                {
                    "sent": "This extra function values.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see a picture.",
                    "label": 0
                },
                {
                    "sent": "Here we see a function with a red.",
                    "label": 0
                },
                {
                    "sent": "It would be this extra function points with.",
                    "label": 1
                },
                {
                    "sent": "Sorry with a blue with red, it would be the inputs of this extra function points with the green curve is the function by the conditional prior.",
                    "label": 1
                },
                {
                    "sent": "I will explain actually what means this condition prior and they say the area is basically uncertain T when we draw from this condition prior.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is your parameter that specifies the function.",
                    "label": 1
                },
                {
                    "sent": "Board actually will turn out to be a Delta function and also this depends on the kitchen light.",
                    "label": 1
                },
                {
                    "sent": "If the kennel is linear, then in at 1 two dimensional space then I need just two points to specify the function exactly.",
                    "label": 0
                },
                {
                    "sent": "So in that case the condition prior actually would be a Delta function.",
                    "label": 0
                },
                {
                    "sent": "So now you are going to use this text.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm at the stoplight by this run inference.",
                    "label": 0
                },
                {
                    "sent": "As we said before, we have the initial model we augmented and what we're going to do is to apply a standard variational Bayes method.",
                    "label": 0
                },
                {
                    "sent": "So we assume that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the solution that factorizes for news, the one piece of the violation distribution, it will be the conditional prior itself.",
                    "label": 0
                },
                {
                    "sent": "And this is actually the trick that makes everything tractable and the other party would be admin field part.",
                    "label": 1
                },
                {
                    "sent": "You will have a mean field approximation with respect to their parameters that we already introduced the use and the inputs.",
                    "label": 1
                },
                {
                    "sent": "So the distribution of arrangements will be Gaussian there the file distribution would be under strict, but it will turn out to be Gaussian.",
                    "label": 1
                },
                {
                    "sent": "And as I said, the trick is that one part of the variational distribution will be the conditional priority.",
                    "label": 1
                },
                {
                    "sent": "So now imagine that we are going to compute the lower bound and try to maximize it, right?",
                    "label": 0
                },
                {
                    "sent": "So the first bit of the equation in the first line is the exact measure likelihood, and once you apply this inequality and put the variational distribution to that equation.",
                    "label": 0
                },
                {
                    "sent": "As you see the day prior, because appears both in the model and version distribution, they will cancel out and after this cancellation method becomes tractable.",
                    "label": 0
                },
                {
                    "sent": "A gentle explain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full detail this because you have to go through the mass to see this, but what they're going to say is that this bound now is tractable, and it's actually general because I can compute this for many kernels.",
                    "label": 0
                },
                {
                    "sent": "For linear kernels, for square, exponential, exponential polynomials, and probably many others, so this is general framework actually, so the way that this is maximized is buys only.",
                    "label": 1
                },
                {
                    "sent": "Applying a gradient based optimization of variation of parameters and model hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "So let's now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apply this to Gaussian process latent variable model.",
                    "label": 1
                },
                {
                    "sent": "So gossip related environment is a latent variable model.",
                    "label": 0
                },
                {
                    "sent": "What's the latent variable model you have?",
                    "label": 0
                },
                {
                    "sent": "You have a latent SpaceX that fits into our latent mapping and related.",
                    "label": 0
                },
                {
                    "sent": "Maybe gives an object variable, for example eight in the later mapping can be linear and then you get factor analysis of or PCA in that VM.",
                    "label": 0
                },
                {
                    "sent": "The later Magic is given Gaussian process priors.",
                    "label": 0
                },
                {
                    "sent": "However the GP LV.",
                    "label": 0
                },
                {
                    "sent": "Strained by optimizing and not marginalizing out the latent variables.",
                    "label": 1
                },
                {
                    "sent": "And this basically means that there is no proper density in the latent space.",
                    "label": 0
                },
                {
                    "sent": "The latent space, that density is basically a mixture of Delta functions.",
                    "label": 0
                },
                {
                    "sent": "Somehow, we cannot select the dimensionality becauses not fully Bayesian.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The training and we might also overfit.",
                    "label": 0
                },
                {
                    "sent": "So we would like to apply some more full Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "And of course this is and basically treat both the latent space and the latent mapping in in a basic way.",
                    "label": 1
                },
                {
                    "sent": "So trying to marginalise them out.",
                    "label": 0
                },
                {
                    "sent": "Of course this is intractable, So what we're going to do is to apply our variational approximation and this is done exactly as explained in the same standard regression model.",
                    "label": 0
                },
                {
                    "sent": "Not very useful aspects of this approach is that you allow us to perform automatic selection of the lady dimensionality, for example.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I choose, the tenant will be the square, exponential, errored eternal error demeans automatic relevance determination, which has the equation shows puts a different different inverse link scale for this latent dimension.",
                    "label": 0
                },
                {
                    "sent": "This will allow me, but my maximizing the variational lower bound over these parameters to remove redundant redundant latent dimensions so I can start with a large number of latent domain source and let the lower bound decide how many they are going to be.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Boost.",
                    "label": 0
                },
                {
                    "sent": "So let's apply this to some exams.",
                    "label": 0
                },
                {
                    "sent": "I will first explore and examine visualization in order flow data and I'm going to compare the basin, similar them with standard zippy LVM and probabilistic PCA.",
                    "label": 1
                },
                {
                    "sent": "So this is the solution of Bayesian TB LVM, the pic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, on the on the on the on the left my side on the right of your side, sorry.",
                    "label": 0
                },
                {
                    "sent": "We saw the bus, so the inverse length scales, so we start assuming tent tent, tent latent dimensions.",
                    "label": 0
                },
                {
                    "sent": "So as you can see out of 10 only three survive.",
                    "label": 0
                },
                {
                    "sent": "The seven are strong automatical to 0.",
                    "label": 0
                },
                {
                    "sent": "So basically the model selects that dimensionality must be must be free and also the visualization actually is really high quality.",
                    "label": 0
                },
                {
                    "sent": "As compared for example.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through through other methods to the standard deviation, mean PCA.",
                    "label": 0
                },
                {
                    "sent": "For example, you see that the invasions, LVM, the two different classes because in this data, although we treated us in a supervised way, there are actually three classes.",
                    "label": 0
                },
                {
                    "sent": "Different stages of their own flow.",
                    "label": 0
                },
                {
                    "sent": "We can see that visualization is much much superior for the business development.",
                    "label": 0
                },
                {
                    "sent": "Also, these classes are well separated from each other.",
                    "label": 0
                },
                {
                    "sent": "A second exam.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to show how can how can process partially observed data.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do in this experiment is to.",
                    "label": 0
                },
                {
                    "sent": "To choose a data set, I will use a fray.",
                    "label": 0
                },
                {
                    "sent": "The fray faces and I will take one part to train the model and then the other part of the data set.",
                    "label": 0
                },
                {
                    "sent": "It would be for testing, but when I'm going to test the model, I'm not going to give the email This, I will just give partial images.",
                    "label": 0
                },
                {
                    "sent": "So the first row, the first row in the picture shows the exact which is sorry, which is the true test team us the 2nd row which is solved the partial observed image which is basically processed by the model.",
                    "label": 0
                },
                {
                    "sent": "And the third row shows the reconstructed image from the model from the DB LVM.",
                    "label": 0
                },
                {
                    "sent": "From the business in Belgium.",
                    "label": 0
                },
                {
                    "sent": "So synthetic construction makes error in this example is 7.4 any superior to when you apply the standard ZP LVM using different settings of the latent dimensionality, the dimensionality for the program was set to 30.",
                    "label": 0
                },
                {
                    "sent": "And a further example.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What did you use this model is to to apply for that generative classification.",
                    "label": 1
                },
                {
                    "sent": "So, so, one way to to apply this vision to prevent?",
                    "label": 0
                },
                {
                    "sent": "Basically, if you apply for example in this case, we applied for the in the disease that are set, so for this did visit, we run a separate basin, tibial them and run and get a class conditional estimate based on class conditional estimator.",
                    "label": 1
                },
                {
                    "sent": "Then we can use this not in generative classifier and get an error there.",
                    "label": 0
                },
                {
                    "sent": "We got in this other service 40 points.",
                    "label": 0
                },
                {
                    "sent": "4.73 which is quite close I think, to discriminating methods.",
                    "label": 0
                },
                {
                    "sent": "I mean, fingers support vector machines gives around.",
                    "label": 0
                },
                {
                    "sent": "3%.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'm going to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He presented with reduced variation framework to approximately the great out inputs in Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "This is general, allows for for.",
                    "label": 0
                },
                {
                    "sent": "For full Bayesian it training of OLVM, and some interesting topics for future work is how to speed up the optimization and then actually spoke to Matt about this, but we're using cause again gradients, but probably the other way.",
                    "label": 0
                },
                {
                    "sent": "Other ways of doing this for exam discovered and fixed point equations and another is to apply this method.",
                    "label": 0
                },
                {
                    "sent": "For learning new parametric nonlinear dynamical systems using Gaussian processes and variational Bayes.",
                    "label": 1
                },
                {
                    "sent": "Friend you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because I'm faster questions.",
                    "label": 0
                },
                {
                    "sent": "When you're doing optimization, there's the variational lower bound.",
                    "label": 0
                },
                {
                    "sent": "What you doing optimization is it's only unique optimum.",
                    "label": 0
                },
                {
                    "sent": "No, that is their local Maxima.",
                    "label": 0
                },
                {
                    "sent": "Creative then I'm not so much situation.",
                    "label": 0
                },
                {
                    "sent": "If you get trapped on false maximum.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Sure that problems with.",
                    "label": 0
                },
                {
                    "sent": "Yeah this is not linear optimization.",
                    "label": 0
                },
                {
                    "sent": "So that a local maximum we don't get active to find the best solution.",
                    "label": 0
                },
                {
                    "sent": "So did you.",
                    "label": 0
                },
                {
                    "sent": "In your experiments you decide something like this with families.",
                    "label": 0
                },
                {
                    "sent": "Well, he's basically find out one run without authorization.",
                    "label": 0
                },
                {
                    "sent": "For all these runs, basically, yeah, I mean idea for the overflow.",
                    "label": 0
                },
                {
                    "sent": "For example, I just do one.",
                    "label": 0
                },
                {
                    "sent": "Although I fixed the, you know the random seed so.",
                    "label": 0
                },
                {
                    "sent": "Because I want to produce the experience.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's possible to find different solutions, although I mean always the Bayesian Sylvia Levine.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think the minimum actually is not so severe becausw.",
                    "label": 0
                },
                {
                    "sent": "Because you have additional approximation, so you have variance in the latent space, right?",
                    "label": 0
                },
                {
                    "sent": "And this kind of smooth objective function so you think you are able to find better, better solutions.",
                    "label": 0
                },
                {
                    "sent": "You don't actually find the map estimate over there.",
                    "label": 0
                },
                {
                    "sent": "You know for the latent positions also you have the variance.",
                    "label": 0
                },
                {
                    "sent": "So if you like the balance to be slightly bigger, then basically you kind of smooth the objective function and that kind of helps.",
                    "label": 0
                },
                {
                    "sent": "But the optimization is 'cause I mean that's.",
                    "label": 0
                },
                {
                    "sent": "That's for sure.",
                    "label": 0
                },
                {
                    "sent": "Honey, you points to use or how you choose.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean, the more you use it, the better.",
                    "label": 0
                },
                {
                    "sent": "I mean in the variational approximation.",
                    "label": 0
                },
                {
                    "sent": "Next, let's say this is the first equation right this time in field so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the first part is the conditions in prior.",
                    "label": 0
                },
                {
                    "sent": "The best hope, the best we can do with the condition of priority is to become a Delta function.",
                    "label": 0
                },
                {
                    "sent": "So basically reducing points to exactly the producer function.",
                    "label": 0
                },
                {
                    "sent": "But in practice you cannot use too many becausw.",
                    "label": 0
                },
                {
                    "sent": "We actually optimize over there using inputs, and this is also our variational parameters.",
                    "label": 0
                },
                {
                    "sent": "So you want to be numerically, you know like OK, you don't have to increase to match the parameters.",
                    "label": 0
                },
                {
                    "sent": "But also this depends on the kernel mean.",
                    "label": 0
                },
                {
                    "sent": "For the linear Callan this becomes Delta function.",
                    "label": 0
                },
                {
                    "sent": "If you just use you know D + 1 reducing points.",
                    "label": 0
                },
                {
                    "sent": "You want these basically there.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality of the.",
                    "label": 0
                },
                {
                    "sent": "Vex but this is, yeah, I mean it's there.",
                    "label": 0
                },
                {
                    "sent": "Would you know the model using Teradata better?",
                    "label": 0
                },
                {
                    "sent": "But in practice you should not use two months?",
                    "label": 0
                },
                {
                    "sent": "Probably.",
                    "label": 0
                },
                {
                    "sent": "So distribution where X is Gaussian as we see here.",
                    "label": 0
                },
                {
                    "sent": "Neutralize it to allocate this and can you.",
                    "label": 0
                },
                {
                    "sent": "Earth.",
                    "label": 0
                },
                {
                    "sent": "No, I mean I haven't tried but.",
                    "label": 0
                },
                {
                    "sent": "Probably I mean some situation, maybe actually is possible.",
                    "label": 0
                },
                {
                    "sent": "I mean, but a natural choice is the Gaussian, because this imposes you know they are real numbers.",
                    "label": 0
                },
                {
                    "sent": "They're not constantly positive.",
                    "label": 0
                },
                {
                    "sent": "I guess if you're constrained in some way, maybe it makes sense.",
                    "label": 0
                },
                {
                    "sent": "You know to have a different distribution.",
                    "label": 0
                },
                {
                    "sent": "Maybe our grammar.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have are you have a mixture of history and mixture of Gaussians, then you have the user problems with very strong inference.",
                    "label": 0
                },
                {
                    "sent": "I mean you cannot optimize usually very easily, you know.",
                    "label": 0
                },
                {
                    "sent": "A mixture of you know various on main fields approximations, so this is a hard so.",
                    "label": 0
                },
                {
                    "sent": "But possibly probably is.",
                    "label": 0
                }
            ]
        }
    }
}