{
    "id": "gyyfawayusltqbt2qthbxvfo5736j5jz",
    "title": "The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling, incl. discussion by Frank Wood",
    "info": {
        "author": [
            "Frank Wood, Gatsby Computational Neuroscience Unit, University College London",
            "John Paisley, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_paisley_discrete/",
    "segmentation": [
        [
            "So thank you very much.",
            "I'm John Paisley.",
            "This is joint work with Chung Wang and Dave Bly at Princeton University.",
            "So the motivation behind our paper, we develop a mixed membership model that combines the advantages of the hierarchical dearsley process and the correlated topic model we call the resulting prior the discrete infinite logistic normal, and we use it in this paper as a topic model and derive a variational inference algorithm.",
            "So to give a brief review, a quick review of mixed membership models."
        ],
        [
            "For topic modeling, this is just a little cartoon, so you have a certain set of topics and each one of these is a distribution on words, and so for each topic, say one regarding childhood or politics, the highly probable words in that topic would be those that are associated with such a topic.",
            "And those are shared by all documents that are being modeled and so."
        ],
        [
            "One document comes along, for example Charles Dickens and picks childhood in economics."
        ],
        [
            "Another comes along an George Orwell, and he heavily emphasizes politics, an rationality authority, and then maybe Bertrand Russell comes along."
        ],
        [
            "And he picks the same as George Orwell, but also picks logic mathematics."
        ],
        [
            "So yeah, so I'll give a review of two mixed membership models that basically we're going to combine into the model that we propose.",
            "So the first is the hierarchical dearest they process so that HTTP is commonly used for a prior for group data.",
            "And it extends the mixed membership model to the nonparametric setting, for example LDA.",
            "Sorry so OK, So what I have here is 10 groups of data drawn from an HTP where the HTP, each group is a distribution on a mean of a Gaussian, and what you can see is that the feature, the characteristic feature of the HTP for group data, is that the.",
            "Keep the parameters that are shared by all data by all groups, but the probabilities of those parameters are different for each group.",
            "So for example, each group shares say this mean, but some uses it more more than others.",
            "This group tends to use this mean, whereas it's basically absent from this group.",
            "So a drawback of the HTP is."
        ],
        [
            "That it does not explicitly model correlations between the mixing weights of any of the group level distributions.",
            "So for example in the topic model setting, if you know that one topic is highly expressed, if it has a large probability, the HTP then doesn't take any two other topics and say that one is more likely, therefore to occur than another.",
            "So I showed this with this picture here where normalize.",
            "So an HTP is like a normalized gamma process.",
            "Which is a completely random measure, and so if you have some space and you draw a random measure on it, if it's completely random, then the measure on any disjoint sets is going to be independent."
        ],
        [
            "So to address this issue, the CTM the correlated topic model was introduced in the finite setting.",
            "So now we're back to a fixed number of topics.",
            "And it's what it does is it replaces the deer, say prior with the logistic normal prior.",
            "So the logistic normal prior now which is the prior on the mixing weights of the topics is drawn by first drawing a Gaussian vector, exponentiating, and then normalizing, and so the correlations between groups.",
            "Or I'm sorry the correlations among the among the components is built into the Gaussian vector.",
            "The covariance matrix of the Gaussian vector.",
            "So here are 10 new groups of data that have the correlated correlated topic model prior.",
            "They each they have four top four components.",
            "And what I show what this what you see here is that, for example, the lower two clusters are possibly positively correlated and they are negatively correlated with the top two clusters.",
            "So if a bottom cluster appears, then the other ones more likely to appear while the top two or more likely to be absent.",
            "But not always the case."
        ],
        [
            "So one idea would be to then do exponentiation, normalize the Gaussian process on the parameter space, so.",
            "Basically do an infinite extension of the correlated topic model.",
            "So for example, if this were the parameter space and this were Gaussian process, exponentiate this and normalize it.",
            "But the problem with that is that it's not a discrete distribution, it's actually a density.",
            "And Link in his paper in 1988 investigated this.",
            "So an infinite correlated topic model is not by basically taking the CTM, extending it to an infinite number of topics.",
            "That's not a practical."
        ],
        [
            "Practical solution to the problem.",
            "So our goal is to define a prior for the infinite correlated topic model and we have two objectives in this.",
            "The first objective is that we want the prior to be discreet so that atoms that."
        ],
        [
            "The data data share the same set of atoms.",
            "But then the second objective is that we want to explicitly explicitly model the correlations between the mixing weights."
        ],
        [
            "Of each group.",
            "So this first objective just to give.",
            "I'm getting a phone call the 1st first group of the first objective just for Shadow is what the HTTP gives you an.",
            "The second objective is what the?",
            "Normalized exponentiated Gaussian process would give you."
        ],
        [
            "So we define the discrete infinite logistic normal distribution to achieve these two goals."
        ],
        [
            "And we pronounce it Dylan.",
            "Because Dylan doesn't sound as cool.",
            "OK, so just."
        ],
        [
            "Give a review and to kind of give some notation for what's going to come.",
            "So the deer say process is very commonly used for nonparametric mixture modeling.",
            "And so we call Gia dear sleep process.",
            "And it's an infinite collection of atoms Ada 8 one through Infinity, and they are drawn IID from a base distribution.",
            "And here we assume an as is the case with the with the topic model, Gina is a continuous."
        ],
        [
            "Base distribution.",
            "And then Pi are the probability weights on the atoms and those depend on some scaling parameter Alpha which is greater than zero and it's written in this way.",
            "So a problem with this is that if we drew G multiple times, you get a new set of atoms every single time.",
            "So if you had one G and you had an Atom and it had high probability if you drew another G, then with probability one the probability on that Adam would be 0.",
            "So the HTP takes care of this by discretizing the base distribution of the multiple DPS that you want to draw.",
            "So.",
            "Basically, it does that by taking each each of these group level DPS that you want to draw.",
            "And making it states based distribution some DP that's shared by all the groups.",
            "So this is just the DP like up here and then for each group level distribution it uses this DP as a base probability distribution for each of the each of the groups that are being drawn and so here M would be indexing say a document.",
            "And so therefore, if you had G sub M&G sub M prime and you picked an Atom, you picked an Atom in both of them.",
            "They both have non zero probabilities, but those probabilities will be allowed to change between groups."
        ],
        [
            "So we formulate the Dillon as a scaled HTP, where the scaling is performed by exponentiated Gaussian process.",
            "And."
        ],
        [
            "To give the run through that, let Alpha and beta be 2 scaling parameters and let G not cross will not be a product based distribution.",
            "Where Gina covers some parameter space of interest and L not."
        ],
        [
            "Some arbitrary location, some abstract location space.",
            "So the we to go from the top to drawing a group level distribution.",
            "It occurs in three steps.",
            "The first step is we draw a top level DP with this product based distribution.",
            "So this is just drawing a DP like in the HTP.",
            "We then draw a group level DP."
        ],
        [
            "Um?",
            "As in so this is an HTP right here.",
            "We draw a group level DP and then we draw a Gaussian process using the locations of the atoms that.",
            "Were drawn from L not.",
            "And then finally."
        ],
        [
            "Get the group level distribution.",
            "We take the group level DP and we scale it by the exponentiated Gaussian process.",
            "And then normalize.",
            "And so this occurs once, and this these two things occur for each document, and then its scaling is just."
        ],
        [
            "How's that?",
            "So here's an intuitive example.",
            "It's not exactly what I described before, because basically what I'm saying is that in this picture, the location space, the location is the parameter.",
            "So what I have up here is I have the top level DP on say parameter space between zero and one, and then I have a Gaussian process covariance on that same space.",
            "So each column here is a group and first second level DP is drawn with.",
            "This is the base distribution.",
            "A Gaussian process is then drawn that's then exponentiated dot multiplied with the DP and normalized to give the group level distribution.",
            "And so in this one example, you can see that.",
            "Because nearby points are highly correlated, things that are the highly probable atoms tend to come in groups.",
            "But this isn't what we're.",
            "This isn't the objective for the topic model that we're going to present."
        ],
        [
            "OK, so how do we construct draws from dealing with the goal of performing inference?",
            "So we give a representation of the three, the three steps that I described before we give now a constructive representation that can be used to perform inference.",
            "In our case variational inference."
        ],
        [
            "So first we draw a top level DP.",
            "This is just the stick breaking construction of the DP, so we draw proportions from beta 1A and then we draw the locations and parameters from the base distribution.",
            "Then we perform the stick breaking construction and that gives the top level DP.",
            "And even though the Atom is actually just contains both the parameter and the location, we think of it as a distribution on a parameter with a location L."
        ],
        [
            "Then for each second level.",
            "Distribution for each group level distribution we use gamma random variables to construct the distributions.",
            "So we draw a Gaussian process."
        ],
        [
            "Using the locations of the atoms.",
            "And then we draw gamma distributed random variables with this parameterisation, where PK here is the stick breaking weight from the top level.",
            "So this is shared for all documents.",
            "And this second parameter now becomes a exponentiated negative Gaussian process and this is document specific and then normally normalized to give the distribution on the atoms.",
            "And also point out that if this Gaussian process is set to 0, then this parameter is one.",
            "We have an HTP, this is a generative process for an HTP.",
            "So how does?",
            "How does this come about?",
            "So just to review, if you had a finite dearsley distribution and you wanted to draw from it, what you could do is you could draw for each each component.",
            "You could draw gamma distributed random variables where the first parameter is the parameter in the deer, say distribution.",
            "And the second parameter is some constant, like one, and then you normalize that and you get draw from that distribution.",
            "So for the dealers they process, it becomes problematic because as K goes to Infinity in the number of components becomes infinite.",
            "That first parameter is shrinking to zero and so every probability one each.",
            "See that you draw is 0, but with the HTP discretizing the base distribution in advance.",
            "Now all of a sudden it's no problem again.",
            "Now you have an infinite, you have an infinite collection of gamma distributed random variables, but because the stick breaking construction picks out the most probable atoms that are going to be used by all groups.",
            "You can basically you go for the most probable and draw those from the gammas and then you can work your way down using the stick breaking construction.",
            "So when you then scale, this is just the basic property of gamma random variables.",
            "If you had one.",
            "So if this were one here and you scale this by some value that's equal to distribution is in distribution as if this were the inverse.",
            "Using our parameters using our functional form of gamma distribution, that scaling parameter or that scaling factor can become absorbed can be absorbed in this scaling parameter.",
            "And that would be equal in distribution, so that's where the scaling comes in.",
            "So we what we do is we have a gamma representation of the HTP of the second level DPS.",
            "We scale it by the Gaussian process and then that's equal in distribution to this representation.",
            "If we just observe."
        ],
        [
            "That Gaussian process in this parameter.",
            "So we use Dillon as a topic model as mentioned, so the additional hierarchical structure to that model is that for each document or group level distribution for the NTH word in the NTH document, it first picks its topic according to the distribution on topics and then draws the word from the multinomial distribution with that topic parameter.",
            "And for inference, we do the standard procedure of introducing the late indicator that says which topic was selected by which word."
        ],
        [
            "So we perform variational inference to learn the approximate posterior of the Dillon model and to review mean field.",
            "Variational inference uses a factorized Q distribution to approximate the true posterior of a model's parameters.",
            "It searches for the parameters of Q that minimize the KL divergences between Q and the true posterior.",
            "So basically an objective function is set up and then you do coordinate ascent on those parameters to maximize that objective function."
        ],
        [
            "And in the Dylan topic model of the hidden variables are at the document level.",
            "We have the proportion Z.",
            "We have the Gaussian process."
        ],
        [
            "And then we have the latent indicators C. And at the corpus level we have the topics, the top level deep proportion stick breaking proportions.",
            "The Gaussian process parameters.",
            "And then scaling parameters and so we note that this is the one part where we cheat and we learn K directly rather than learning locations through the."
        ],
        [
            "Leighton location.",
            "So instead of defining like a Gaussian kernel, say and then trying to optimize the locations to that kernel, we just learn the we.",
            "Just since we're doing variational inference and we're truncating to a finite number of components, we just learned the values of the covariance matrix directly, 'cause it leads to fast and close from updates.",
            "So this is just just and not to get into the details but to just to give a compare with HTP.",
            "So in the variational."
        ],
        [
            "Objective there's this slow.",
            "There's this intractable expectation, and so we use this first order Taylor expansion lower bound, which introduces this auxiliary parameter.",
            "And what that ends up giving you is analytical updates of these group level proportions on the topics.",
            "And So what you see is the first parameter of this gamma distribution.",
            "This gamma Q distribution, which is the approximate posterior, is just the top of the top level prior and the.",
            "And the expected counts.",
            "So this is just the normal normal thing and then the second parameter of the gamma distribution is this constant and this is constant.",
            "For one, I've suppressed the group level index, but it's constant for all components in within a group.",
            "And then there's this expectation of the of the exponentiated Gaussian process which.",
            "Builds in the covariance.",
            "So when this is just fixed to one or it's ignored, then what we end up having with our algorithm is variational inference algorithm for HTTPS using the normalized gamma representation.",
            "So that's like a side benefit."
        ],
        [
            "OK, so for experiments we test on four text corpora, Huffington Post, New York Times Science, and Wikipedia.",
            "We compare with the HTP."
        ],
        [
            "In the correlated topic model and for the HTP, we used algorithm that I just mentioned."
        ],
        [
            "Or the correlated topic model.",
            "We vary the number of topics an for the HTP and Dillon we truncated to 200 topics, which was much larger than what the corporate that we looked at ended up needing."
        ],
        [
            "And for the base distribution we use symmetric dear slay with the parameter gamma, and then we very gamma."
        ],
        [
            "And then for testing we take a test document.",
            "We partition it into two halves.",
            "We learned documents."
        ],
        [
            "Vic parameters on one half and predict the other half an as a measure of performance, we look at the purple."
        ],
        [
            "City of the words that we held out in the second half.",
            "So here are the proof."
        ],
        [
            "Flexity results and the X axis is the X axis is a function of the base distribution parameter gamma.",
            "And the Y axis is the perplexity.",
            "And so here lower perplexity, lower is better and you can see that in general the Dillon Dillon performs better than HTP an CTM for several different truncation levels, of the CTM."
        ],
        [
            "So then we went back to the New York Times and we looked at some of the topics and we these are the three most probable topics for the New York Times.",
            "On one of the runs and the 10 most probable words in those topics.",
            "And so this is the correlation.",
            "Some of the covariance structure that we see.",
            "So we see that, for instance, the topic on elections is very positively correlated with a topic on more general government.",
            "And this both of these are actually very negatively correlated with a topic on entertainment and arts.",
            "So then, using the kernel we can also project into a lower dimensional space."
        ],
        [
            "To get some sort of a visualization of the topics and so using the kernel, we projected the topics into 2 dimensional space.",
            "Using multidimensional scaling and we represent each topic here with the three most probable words and the size of the words is proportional to the probability of the topic and so.",
            "So a lot of information is missing 'cause it's just two dimensions, but you can see that.",
            "Inference Sports has its region entertainment as its region.",
            "Politics has its region.",
            "Military has its region, so this gives a sense of what kind of a covariance structure the model is learning."
        ],
        [
            "So in conclusion, we presented Dillon nonparametric mixed membership model that models correlations across mixing weights at the group level.",
            "We highlight in the paper.",
            "We highlight the close relationship between Dillon and HTP, and we show how simple modification of our variational inference algorithm gives a new inference algorithm for HTTPS as well, and we demonstrated the performance of Dillon in a topic modeling setting."
        ],
        [
            "So for future directions.",
            "There are many great rock legends who are waiting to lend their names to Beijing nonparametric priors and I hope that this is just the beginning of a very fruitful interdisciplinary collaboration."
        ],
        [
            "So yeah, my name is Frank, William B.",
            "University and."
        ],
        [
            "And I'm I'll be the discussion for this paper.",
            "Undoubtedly, it has happened to all of us, and that we walked into a room at ICL or NIPS or AI stats and said, Oh my God."
        ],
        [
            "It is another room full of topic models and mixed membership models.",
            "How can there be an entire room full of this stuff, right?",
            "It's inarguable that."
        ],
        [
            "Membership models or topic models are popular, and there's good reason for it I think."
        ],
        [
            "They're simple in many respects that easy to describe there."
        ],
        [
            "Intuitively accessible to a large community, both lay practical."
        ],
        [
            "The audience and expert practitioners and they often produce visibly fascinating."
        ],
        [
            "Put there obviously promising for browsing and search applications.",
            "The point here though, is that there's a very, very large community working on these things, and when you make progress, substantial progress like the paper that was just presented did impact a very large number of communities in a very large number of researchers almost immediately.",
            "The problem of course with traditional mixed membership model."
        ],
        [
            "Thing is that the simplicity which is a virtue is in fact also advice.",
            "In other words, mixed membership models, positive latent features, topics in the case of text, text modeling or object objects.",
            "In the case of modeling the real world through, say for instance visual.",
            "Scene analysis or something like that or image models?",
            "Of course, in the real world."
        ],
        [
            "Latent features, their appearance or or their appearance or occurrences correlated in the real world.",
            "So, for instance, that probably the most intuitive way of thinking about this comes from a paper by finale and zoom in on objects appearing in visual scenes where they describe doing mixed membership modeling of images and say, well, you know if we have a correspondence between objects in the world and these latent features, then it's very likely that you will see scenes with chairs and tables together more often than you'll see.",
            "Scenes with chairs and elephants.",
            "So we would like our models to be able to express these kinds of correlations."
        ],
        [
            "Between the latent features, not just not just at the surface level, so most mixed membership models posit uncorrelated features they have.",
            "There has been some work in this direction, obviously some of it was cited before, but there's some way."
        ],
        [
            "Can the Indian buffet process literature as well that looks at building correlated future models?"
        ],
        [
            "We've gone through Dylan so I don't have to really go through this, but in case you were not."
        ],
        [
            "Paying attention or were out drinking coffee instead.",
            "Billing associates every every future with some latent vector.",
            "We use distances between these latent vectors to encode the feature Co occurrence."
        ],
        [
            "Tendencies so close features tend to occur together more often and then we smooth this feature space using a Gaussian process so that the smoothness of the Gaussian process makes close."
        ],
        [
            "Locations have similar prevalences.",
            "We've already had this expert practitioners, HDP, LDA plus gamma process representation, which is really nice.",
            "Gaussian process latent space prior variational in."
        ],
        [
            "It's a whole bucketload of complicated stuff.",
            "What's really nice about the paper if you read it, it works out to be mathematically elegant.",
            "It's very, very nice."
        ],
        [
            "That's a very nice membership model with correlated features.",
            "Now some of the questions Jason raised, he stole some of my Thunder by asking some of the questions that I'll I'll pose here myself.",
            "If you look at the covariance or the correlation, if you normalize this, the correlations at the covariance implied by the model between.",
            "Topics."
        ],
        [
            "You you start to ask questions about what kinds of correlations can be captured by Dylan, it's pretty obvious that you can express strongly positive correlations, but strongly negative correlations are difficult to express.",
            "An individual latent features if they are really highly prevalent will tend to covary more with all."
        ],
        [
            "Other latent features.",
            "It's a detail of the model, but something that somebody will probably work on and fix in future versions.",
            "Jason immediately identified probably the biggest."
        ],
        [
            "Issue or curiosity about the model, which is how scalable is it, particularly with respect to the number?"
        ],
        [
            "Topics in the in the VB truncation, so they're learning.",
            "It's interesting to just learn the kernel matrix directly, but may have may lead to some some issues in terms of scalability.",
            "This is related to exactly how you represent latent topics and how you do a sampling or incremental inference."
        ],
        [
            "The big picture also alluded to in the second half of Jason's question is what about interpretability?",
            "So I, sort of."
        ],
        [
            "About this work as being the first step towards sort of the a first step towards the deep."
        ],
        [
            "Hierarchical Bayesian models in the way that we have deep neural networks and so on and so forth.",
            "Where we have features, but I kind of like these features to have features themselves and those features have features themselves and for us to be able to learn really complicated correlations between the features.",
            "Here we just have distance, right?",
            "So there's no sort of generation of the feature themselves, so chair obviously has features and a desk obviously has features in this topic.",
            "Has features in this topic has features I'd like to know more about the individual?",
            "Constitution of each topic and how those those parts of themselves are related?",
            "So why not further hierarchy?",
            "Is this a punt or is or is there a way that we can actually move forward through learning learning a kernel function, for instance or?",
            "Of course there's some question about whether or not we're robbing from Peter to pay Paul to a certain extent.",
            "So we've introduced a large number of additional parameters.",
            "The question is, how much more data do we really need to get the topics right?",
            "And these locations right?",
            "And I don't think you guys look very closely at that, so that's it, great."
        ],
        [
            "Great work, I highly recommend you all take a look at it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I'm John Paisley.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Chung Wang and Dave Bly at Princeton University.",
                    "label": 1
                },
                {
                    "sent": "So the motivation behind our paper, we develop a mixed membership model that combines the advantages of the hierarchical dearsley process and the correlated topic model we call the resulting prior the discrete infinite logistic normal, and we use it in this paper as a topic model and derive a variational inference algorithm.",
                    "label": 1
                },
                {
                    "sent": "So to give a brief review, a quick review of mixed membership models.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For topic modeling, this is just a little cartoon, so you have a certain set of topics and each one of these is a distribution on words, and so for each topic, say one regarding childhood or politics, the highly probable words in that topic would be those that are associated with such a topic.",
                    "label": 0
                },
                {
                    "sent": "And those are shared by all documents that are being modeled and so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One document comes along, for example Charles Dickens and picks childhood in economics.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another comes along an George Orwell, and he heavily emphasizes politics, an rationality authority, and then maybe Bertrand Russell comes along.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And he picks the same as George Orwell, but also picks logic mathematics.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so I'll give a review of two mixed membership models that basically we're going to combine into the model that we propose.",
                    "label": 0
                },
                {
                    "sent": "So the first is the hierarchical dearest they process so that HTTP is commonly used for a prior for group data.",
                    "label": 1
                },
                {
                    "sent": "And it extends the mixed membership model to the nonparametric setting, for example LDA.",
                    "label": 1
                },
                {
                    "sent": "Sorry so OK, So what I have here is 10 groups of data drawn from an HTP where the HTP, each group is a distribution on a mean of a Gaussian, and what you can see is that the feature, the characteristic feature of the HTP for group data, is that the.",
                    "label": 0
                },
                {
                    "sent": "Keep the parameters that are shared by all data by all groups, but the probabilities of those parameters are different for each group.",
                    "label": 0
                },
                {
                    "sent": "So for example, each group shares say this mean, but some uses it more more than others.",
                    "label": 0
                },
                {
                    "sent": "This group tends to use this mean, whereas it's basically absent from this group.",
                    "label": 0
                },
                {
                    "sent": "So a drawback of the HTP is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That it does not explicitly model correlations between the mixing weights of any of the group level distributions.",
                    "label": 1
                },
                {
                    "sent": "So for example in the topic model setting, if you know that one topic is highly expressed, if it has a large probability, the HTP then doesn't take any two other topics and say that one is more likely, therefore to occur than another.",
                    "label": 0
                },
                {
                    "sent": "So I showed this with this picture here where normalize.",
                    "label": 0
                },
                {
                    "sent": "So an HTP is like a normalized gamma process.",
                    "label": 0
                },
                {
                    "sent": "Which is a completely random measure, and so if you have some space and you draw a random measure on it, if it's completely random, then the measure on any disjoint sets is going to be independent.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to address this issue, the CTM the correlated topic model was introduced in the finite setting.",
                    "label": 1
                },
                {
                    "sent": "So now we're back to a fixed number of topics.",
                    "label": 1
                },
                {
                    "sent": "And it's what it does is it replaces the deer, say prior with the logistic normal prior.",
                    "label": 0
                },
                {
                    "sent": "So the logistic normal prior now which is the prior on the mixing weights of the topics is drawn by first drawing a Gaussian vector, exponentiating, and then normalizing, and so the correlations between groups.",
                    "label": 1
                },
                {
                    "sent": "Or I'm sorry the correlations among the among the components is built into the Gaussian vector.",
                    "label": 0
                },
                {
                    "sent": "The covariance matrix of the Gaussian vector.",
                    "label": 0
                },
                {
                    "sent": "So here are 10 new groups of data that have the correlated correlated topic model prior.",
                    "label": 0
                },
                {
                    "sent": "They each they have four top four components.",
                    "label": 0
                },
                {
                    "sent": "And what I show what this what you see here is that, for example, the lower two clusters are possibly positively correlated and they are negatively correlated with the top two clusters.",
                    "label": 0
                },
                {
                    "sent": "So if a bottom cluster appears, then the other ones more likely to appear while the top two or more likely to be absent.",
                    "label": 0
                },
                {
                    "sent": "But not always the case.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one idea would be to then do exponentiation, normalize the Gaussian process on the parameter space, so.",
                    "label": 1
                },
                {
                    "sent": "Basically do an infinite extension of the correlated topic model.",
                    "label": 0
                },
                {
                    "sent": "So for example, if this were the parameter space and this were Gaussian process, exponentiate this and normalize it.",
                    "label": 0
                },
                {
                    "sent": "But the problem with that is that it's not a discrete distribution, it's actually a density.",
                    "label": 0
                },
                {
                    "sent": "And Link in his paper in 1988 investigated this.",
                    "label": 0
                },
                {
                    "sent": "So an infinite correlated topic model is not by basically taking the CTM, extending it to an infinite number of topics.",
                    "label": 0
                },
                {
                    "sent": "That's not a practical.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practical solution to the problem.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to define a prior for the infinite correlated topic model and we have two objectives in this.",
                    "label": 0
                },
                {
                    "sent": "The first objective is that we want the prior to be discreet so that atoms that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data data share the same set of atoms.",
                    "label": 0
                },
                {
                    "sent": "But then the second objective is that we want to explicitly explicitly model the correlations between the mixing weights.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of each group.",
                    "label": 0
                },
                {
                    "sent": "So this first objective just to give.",
                    "label": 0
                },
                {
                    "sent": "I'm getting a phone call the 1st first group of the first objective just for Shadow is what the HTTP gives you an.",
                    "label": 0
                },
                {
                    "sent": "The second objective is what the?",
                    "label": 0
                },
                {
                    "sent": "Normalized exponentiated Gaussian process would give you.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we define the discrete infinite logistic normal distribution to achieve these two goals.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we pronounce it Dylan.",
                    "label": 1
                },
                {
                    "sent": "Because Dylan doesn't sound as cool.",
                    "label": 0
                },
                {
                    "sent": "OK, so just.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give a review and to kind of give some notation for what's going to come.",
                    "label": 0
                },
                {
                    "sent": "So the deer say process is very commonly used for nonparametric mixture modeling.",
                    "label": 0
                },
                {
                    "sent": "And so we call Gia dear sleep process.",
                    "label": 0
                },
                {
                    "sent": "And it's an infinite collection of atoms Ada 8 one through Infinity, and they are drawn IID from a base distribution.",
                    "label": 0
                },
                {
                    "sent": "And here we assume an as is the case with the with the topic model, Gina is a continuous.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Base distribution.",
                    "label": 0
                },
                {
                    "sent": "And then Pi are the probability weights on the atoms and those depend on some scaling parameter Alpha which is greater than zero and it's written in this way.",
                    "label": 0
                },
                {
                    "sent": "So a problem with this is that if we drew G multiple times, you get a new set of atoms every single time.",
                    "label": 0
                },
                {
                    "sent": "So if you had one G and you had an Atom and it had high probability if you drew another G, then with probability one the probability on that Adam would be 0.",
                    "label": 0
                },
                {
                    "sent": "So the HTP takes care of this by discretizing the base distribution of the multiple DPS that you want to draw.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically, it does that by taking each each of these group level DPS that you want to draw.",
                    "label": 0
                },
                {
                    "sent": "And making it states based distribution some DP that's shared by all the groups.",
                    "label": 0
                },
                {
                    "sent": "So this is just the DP like up here and then for each group level distribution it uses this DP as a base probability distribution for each of the each of the groups that are being drawn and so here M would be indexing say a document.",
                    "label": 0
                },
                {
                    "sent": "And so therefore, if you had G sub M&G sub M prime and you picked an Atom, you picked an Atom in both of them.",
                    "label": 0
                },
                {
                    "sent": "They both have non zero probabilities, but those probabilities will be allowed to change between groups.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we formulate the Dillon as a scaled HTP, where the scaling is performed by exponentiated Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give the run through that, let Alpha and beta be 2 scaling parameters and let G not cross will not be a product based distribution.",
                    "label": 0
                },
                {
                    "sent": "Where Gina covers some parameter space of interest and L not.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some arbitrary location, some abstract location space.",
                    "label": 0
                },
                {
                    "sent": "So the we to go from the top to drawing a group level distribution.",
                    "label": 0
                },
                {
                    "sent": "It occurs in three steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is we draw a top level DP with this product based distribution.",
                    "label": 1
                },
                {
                    "sent": "So this is just drawing a DP like in the HTP.",
                    "label": 0
                },
                {
                    "sent": "We then draw a group level DP.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "As in so this is an HTP right here.",
                    "label": 0
                },
                {
                    "sent": "We draw a group level DP and then we draw a Gaussian process using the locations of the atoms that.",
                    "label": 1
                },
                {
                    "sent": "Were drawn from L not.",
                    "label": 0
                },
                {
                    "sent": "And then finally.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get the group level distribution.",
                    "label": 0
                },
                {
                    "sent": "We take the group level DP and we scale it by the exponentiated Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "And then normalize.",
                    "label": 0
                },
                {
                    "sent": "And so this occurs once, and this these two things occur for each document, and then its scaling is just.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How's that?",
                    "label": 0
                },
                {
                    "sent": "So here's an intuitive example.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly what I described before, because basically what I'm saying is that in this picture, the location space, the location is the parameter.",
                    "label": 0
                },
                {
                    "sent": "So what I have up here is I have the top level DP on say parameter space between zero and one, and then I have a Gaussian process covariance on that same space.",
                    "label": 1
                },
                {
                    "sent": "So each column here is a group and first second level DP is drawn with.",
                    "label": 0
                },
                {
                    "sent": "This is the base distribution.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian process is then drawn that's then exponentiated dot multiplied with the DP and normalized to give the group level distribution.",
                    "label": 0
                },
                {
                    "sent": "And so in this one example, you can see that.",
                    "label": 0
                },
                {
                    "sent": "Because nearby points are highly correlated, things that are the highly probable atoms tend to come in groups.",
                    "label": 0
                },
                {
                    "sent": "But this isn't what we're.",
                    "label": 0
                },
                {
                    "sent": "This isn't the objective for the topic model that we're going to present.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we construct draws from dealing with the goal of performing inference?",
                    "label": 0
                },
                {
                    "sent": "So we give a representation of the three, the three steps that I described before we give now a constructive representation that can be used to perform inference.",
                    "label": 0
                },
                {
                    "sent": "In our case variational inference.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we draw a top level DP.",
                    "label": 0
                },
                {
                    "sent": "This is just the stick breaking construction of the DP, so we draw proportions from beta 1A and then we draw the locations and parameters from the base distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we perform the stick breaking construction and that gives the top level DP.",
                    "label": 0
                },
                {
                    "sent": "And even though the Atom is actually just contains both the parameter and the location, we think of it as a distribution on a parameter with a location L.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then for each second level.",
                    "label": 0
                },
                {
                    "sent": "Distribution for each group level distribution we use gamma random variables to construct the distributions.",
                    "label": 0
                },
                {
                    "sent": "So we draw a Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using the locations of the atoms.",
                    "label": 0
                },
                {
                    "sent": "And then we draw gamma distributed random variables with this parameterisation, where PK here is the stick breaking weight from the top level.",
                    "label": 0
                },
                {
                    "sent": "So this is shared for all documents.",
                    "label": 0
                },
                {
                    "sent": "And this second parameter now becomes a exponentiated negative Gaussian process and this is document specific and then normally normalized to give the distribution on the atoms.",
                    "label": 0
                },
                {
                    "sent": "And also point out that if this Gaussian process is set to 0, then this parameter is one.",
                    "label": 1
                },
                {
                    "sent": "We have an HTP, this is a generative process for an HTP.",
                    "label": 1
                },
                {
                    "sent": "So how does?",
                    "label": 0
                },
                {
                    "sent": "How does this come about?",
                    "label": 0
                },
                {
                    "sent": "So just to review, if you had a finite dearsley distribution and you wanted to draw from it, what you could do is you could draw for each each component.",
                    "label": 0
                },
                {
                    "sent": "You could draw gamma distributed random variables where the first parameter is the parameter in the deer, say distribution.",
                    "label": 0
                },
                {
                    "sent": "And the second parameter is some constant, like one, and then you normalize that and you get draw from that distribution.",
                    "label": 0
                },
                {
                    "sent": "So for the dealers they process, it becomes problematic because as K goes to Infinity in the number of components becomes infinite.",
                    "label": 0
                },
                {
                    "sent": "That first parameter is shrinking to zero and so every probability one each.",
                    "label": 0
                },
                {
                    "sent": "See that you draw is 0, but with the HTP discretizing the base distribution in advance.",
                    "label": 0
                },
                {
                    "sent": "Now all of a sudden it's no problem again.",
                    "label": 0
                },
                {
                    "sent": "Now you have an infinite, you have an infinite collection of gamma distributed random variables, but because the stick breaking construction picks out the most probable atoms that are going to be used by all groups.",
                    "label": 0
                },
                {
                    "sent": "You can basically you go for the most probable and draw those from the gammas and then you can work your way down using the stick breaking construction.",
                    "label": 0
                },
                {
                    "sent": "So when you then scale, this is just the basic property of gamma random variables.",
                    "label": 0
                },
                {
                    "sent": "If you had one.",
                    "label": 0
                },
                {
                    "sent": "So if this were one here and you scale this by some value that's equal to distribution is in distribution as if this were the inverse.",
                    "label": 0
                },
                {
                    "sent": "Using our parameters using our functional form of gamma distribution, that scaling parameter or that scaling factor can become absorbed can be absorbed in this scaling parameter.",
                    "label": 1
                },
                {
                    "sent": "And that would be equal in distribution, so that's where the scaling comes in.",
                    "label": 0
                },
                {
                    "sent": "So we what we do is we have a gamma representation of the HTP of the second level DPS.",
                    "label": 0
                },
                {
                    "sent": "We scale it by the Gaussian process and then that's equal in distribution to this representation.",
                    "label": 0
                },
                {
                    "sent": "If we just observe.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That Gaussian process in this parameter.",
                    "label": 0
                },
                {
                    "sent": "So we use Dillon as a topic model as mentioned, so the additional hierarchical structure to that model is that for each document or group level distribution for the NTH word in the NTH document, it first picks its topic according to the distribution on topics and then draws the word from the multinomial distribution with that topic parameter.",
                    "label": 1
                },
                {
                    "sent": "And for inference, we do the standard procedure of introducing the late indicator that says which topic was selected by which word.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we perform variational inference to learn the approximate posterior of the Dillon model and to review mean field.",
                    "label": 1
                },
                {
                    "sent": "Variational inference uses a factorized Q distribution to approximate the true posterior of a model's parameters.",
                    "label": 0
                },
                {
                    "sent": "It searches for the parameters of Q that minimize the KL divergences between Q and the true posterior.",
                    "label": 0
                },
                {
                    "sent": "So basically an objective function is set up and then you do coordinate ascent on those parameters to maximize that objective function.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the Dylan topic model of the hidden variables are at the document level.",
                    "label": 0
                },
                {
                    "sent": "We have the proportion Z.",
                    "label": 0
                },
                {
                    "sent": "We have the Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have the latent indicators C. And at the corpus level we have the topics, the top level deep proportion stick breaking proportions.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian process parameters.",
                    "label": 0
                },
                {
                    "sent": "And then scaling parameters and so we note that this is the one part where we cheat and we learn K directly rather than learning locations through the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leighton location.",
                    "label": 0
                },
                {
                    "sent": "So instead of defining like a Gaussian kernel, say and then trying to optimize the locations to that kernel, we just learn the we.",
                    "label": 1
                },
                {
                    "sent": "Just since we're doing variational inference and we're truncating to a finite number of components, we just learned the values of the covariance matrix directly, 'cause it leads to fast and close from updates.",
                    "label": 1
                },
                {
                    "sent": "So this is just just and not to get into the details but to just to give a compare with HTP.",
                    "label": 0
                },
                {
                    "sent": "So in the variational.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Objective there's this slow.",
                    "label": 0
                },
                {
                    "sent": "There's this intractable expectation, and so we use this first order Taylor expansion lower bound, which introduces this auxiliary parameter.",
                    "label": 1
                },
                {
                    "sent": "And what that ends up giving you is analytical updates of these group level proportions on the topics.",
                    "label": 0
                },
                {
                    "sent": "And So what you see is the first parameter of this gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "This gamma Q distribution, which is the approximate posterior, is just the top of the top level prior and the.",
                    "label": 0
                },
                {
                    "sent": "And the expected counts.",
                    "label": 0
                },
                {
                    "sent": "So this is just the normal normal thing and then the second parameter of the gamma distribution is this constant and this is constant.",
                    "label": 0
                },
                {
                    "sent": "For one, I've suppressed the group level index, but it's constant for all components in within a group.",
                    "label": 0
                },
                {
                    "sent": "And then there's this expectation of the of the exponentiated Gaussian process which.",
                    "label": 0
                },
                {
                    "sent": "Builds in the covariance.",
                    "label": 0
                },
                {
                    "sent": "So when this is just fixed to one or it's ignored, then what we end up having with our algorithm is variational inference algorithm for HTTPS using the normalized gamma representation.",
                    "label": 1
                },
                {
                    "sent": "So that's like a side benefit.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for experiments we test on four text corpora, Huffington Post, New York Times Science, and Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "We compare with the HTP.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the correlated topic model and for the HTP, we used algorithm that I just mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the correlated topic model.",
                    "label": 0
                },
                {
                    "sent": "We vary the number of topics an for the HTP and Dillon we truncated to 200 topics, which was much larger than what the corporate that we looked at ended up needing.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the base distribution we use symmetric dear slay with the parameter gamma, and then we very gamma.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for testing we take a test document.",
                    "label": 0
                },
                {
                    "sent": "We partition it into two halves.",
                    "label": 0
                },
                {
                    "sent": "We learned documents.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vic parameters on one half and predict the other half an as a measure of performance, we look at the purple.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City of the words that we held out in the second half.",
                    "label": 0
                },
                {
                    "sent": "So here are the proof.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flexity results and the X axis is the X axis is a function of the base distribution parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "And the Y axis is the perplexity.",
                    "label": 0
                },
                {
                    "sent": "And so here lower perplexity, lower is better and you can see that in general the Dillon Dillon performs better than HTP an CTM for several different truncation levels, of the CTM.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we went back to the New York Times and we looked at some of the topics and we these are the three most probable topics for the New York Times.",
                    "label": 1
                },
                {
                    "sent": "On one of the runs and the 10 most probable words in those topics.",
                    "label": 0
                },
                {
                    "sent": "And so this is the correlation.",
                    "label": 0
                },
                {
                    "sent": "Some of the covariance structure that we see.",
                    "label": 0
                },
                {
                    "sent": "So we see that, for instance, the topic on elections is very positively correlated with a topic on more general government.",
                    "label": 0
                },
                {
                    "sent": "And this both of these are actually very negatively correlated with a topic on entertainment and arts.",
                    "label": 0
                },
                {
                    "sent": "So then, using the kernel we can also project into a lower dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get some sort of a visualization of the topics and so using the kernel, we projected the topics into 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Using multidimensional scaling and we represent each topic here with the three most probable words and the size of the words is proportional to the probability of the topic and so.",
                    "label": 0
                },
                {
                    "sent": "So a lot of information is missing 'cause it's just two dimensions, but you can see that.",
                    "label": 0
                },
                {
                    "sent": "Inference Sports has its region entertainment as its region.",
                    "label": 0
                },
                {
                    "sent": "Politics has its region.",
                    "label": 0
                },
                {
                    "sent": "Military has its region, so this gives a sense of what kind of a covariance structure the model is learning.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we presented Dillon nonparametric mixed membership model that models correlations across mixing weights at the group level.",
                    "label": 1
                },
                {
                    "sent": "We highlight in the paper.",
                    "label": 1
                },
                {
                    "sent": "We highlight the close relationship between Dillon and HTP, and we show how simple modification of our variational inference algorithm gives a new inference algorithm for HTTPS as well, and we demonstrated the performance of Dillon in a topic modeling setting.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for future directions.",
                    "label": 0
                },
                {
                    "sent": "There are many great rock legends who are waiting to lend their names to Beijing nonparametric priors and I hope that this is just the beginning of a very fruitful interdisciplinary collaboration.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, my name is Frank, William B.",
                    "label": 0
                },
                {
                    "sent": "University and.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm I'll be the discussion for this paper.",
                    "label": 0
                },
                {
                    "sent": "Undoubtedly, it has happened to all of us, and that we walked into a room at ICL or NIPS or AI stats and said, Oh my God.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is another room full of topic models and mixed membership models.",
                    "label": 1
                },
                {
                    "sent": "How can there be an entire room full of this stuff, right?",
                    "label": 0
                },
                {
                    "sent": "It's inarguable that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Membership models or topic models are popular, and there's good reason for it I think.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're simple in many respects that easy to describe there.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intuitively accessible to a large community, both lay practical.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The audience and expert practitioners and they often produce visibly fascinating.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put there obviously promising for browsing and search applications.",
                    "label": 1
                },
                {
                    "sent": "The point here though, is that there's a very, very large community working on these things, and when you make progress, substantial progress like the paper that was just presented did impact a very large number of communities in a very large number of researchers almost immediately.",
                    "label": 0
                },
                {
                    "sent": "The problem of course with traditional mixed membership model.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing is that the simplicity which is a virtue is in fact also advice.",
                    "label": 0
                },
                {
                    "sent": "In other words, mixed membership models, positive latent features, topics in the case of text, text modeling or object objects.",
                    "label": 1
                },
                {
                    "sent": "In the case of modeling the real world through, say for instance visual.",
                    "label": 0
                },
                {
                    "sent": "Scene analysis or something like that or image models?",
                    "label": 0
                },
                {
                    "sent": "Of course, in the real world.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Latent features, their appearance or or their appearance or occurrences correlated in the real world.",
                    "label": 1
                },
                {
                    "sent": "So, for instance, that probably the most intuitive way of thinking about this comes from a paper by finale and zoom in on objects appearing in visual scenes where they describe doing mixed membership modeling of images and say, well, you know if we have a correspondence between objects in the world and these latent features, then it's very likely that you will see scenes with chairs and tables together more often than you'll see.",
                    "label": 0
                },
                {
                    "sent": "Scenes with chairs and elephants.",
                    "label": 0
                },
                {
                    "sent": "So we would like our models to be able to express these kinds of correlations.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between the latent features, not just not just at the surface level, so most mixed membership models posit uncorrelated features they have.",
                    "label": 0
                },
                {
                    "sent": "There has been some work in this direction, obviously some of it was cited before, but there's some way.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can the Indian buffet process literature as well that looks at building correlated future models?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've gone through Dylan so I don't have to really go through this, but in case you were not.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paying attention or were out drinking coffee instead.",
                    "label": 0
                },
                {
                    "sent": "Billing associates every every future with some latent vector.",
                    "label": 1
                },
                {
                    "sent": "We use distances between these latent vectors to encode the feature Co occurrence.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tendencies so close features tend to occur together more often and then we smooth this feature space using a Gaussian process so that the smoothness of the Gaussian process makes close.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Locations have similar prevalences.",
                    "label": 0
                },
                {
                    "sent": "We've already had this expert practitioners, HDP, LDA plus gamma process representation, which is really nice.",
                    "label": 1
                },
                {
                    "sent": "Gaussian process latent space prior variational in.",
                    "label": 1
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a whole bucketload of complicated stuff.",
                    "label": 0
                },
                {
                    "sent": "What's really nice about the paper if you read it, it works out to be mathematically elegant.",
                    "label": 0
                },
                {
                    "sent": "It's very, very nice.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a very nice membership model with correlated features.",
                    "label": 0
                },
                {
                    "sent": "Now some of the questions Jason raised, he stole some of my Thunder by asking some of the questions that I'll I'll pose here myself.",
                    "label": 0
                },
                {
                    "sent": "If you look at the covariance or the correlation, if you normalize this, the correlations at the covariance implied by the model between.",
                    "label": 0
                },
                {
                    "sent": "Topics.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You you start to ask questions about what kinds of correlations can be captured by Dylan, it's pretty obvious that you can express strongly positive correlations, but strongly negative correlations are difficult to express.",
                    "label": 0
                },
                {
                    "sent": "An individual latent features if they are really highly prevalent will tend to covary more with all.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other latent features.",
                    "label": 0
                },
                {
                    "sent": "It's a detail of the model, but something that somebody will probably work on and fix in future versions.",
                    "label": 0
                },
                {
                    "sent": "Jason immediately identified probably the biggest.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issue or curiosity about the model, which is how scalable is it, particularly with respect to the number?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topics in the in the VB truncation, so they're learning.",
                    "label": 1
                },
                {
                    "sent": "It's interesting to just learn the kernel matrix directly, but may have may lead to some some issues in terms of scalability.",
                    "label": 0
                },
                {
                    "sent": "This is related to exactly how you represent latent topics and how you do a sampling or incremental inference.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The big picture also alluded to in the second half of Jason's question is what about interpretability?",
                    "label": 0
                },
                {
                    "sent": "So I, sort of.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About this work as being the first step towards sort of the a first step towards the deep.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hierarchical Bayesian models in the way that we have deep neural networks and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "Where we have features, but I kind of like these features to have features themselves and those features have features themselves and for us to be able to learn really complicated correlations between the features.",
                    "label": 0
                },
                {
                    "sent": "Here we just have distance, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no sort of generation of the feature themselves, so chair obviously has features and a desk obviously has features in this topic.",
                    "label": 0
                },
                {
                    "sent": "Has features in this topic has features I'd like to know more about the individual?",
                    "label": 0
                },
                {
                    "sent": "Constitution of each topic and how those those parts of themselves are related?",
                    "label": 0
                },
                {
                    "sent": "So why not further hierarchy?",
                    "label": 1
                },
                {
                    "sent": "Is this a punt or is or is there a way that we can actually move forward through learning learning a kernel function, for instance or?",
                    "label": 0
                },
                {
                    "sent": "Of course there's some question about whether or not we're robbing from Peter to pay Paul to a certain extent.",
                    "label": 0
                },
                {
                    "sent": "So we've introduced a large number of additional parameters.",
                    "label": 0
                },
                {
                    "sent": "The question is, how much more data do we really need to get the topics right?",
                    "label": 0
                },
                {
                    "sent": "And these locations right?",
                    "label": 0
                },
                {
                    "sent": "And I don't think you guys look very closely at that, so that's it, great.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great work, I highly recommend you all take a look at it.",
                    "label": 0
                }
            ]
        }
    }
}