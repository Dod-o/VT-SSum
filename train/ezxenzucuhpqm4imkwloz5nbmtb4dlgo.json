{
    "id": "ezxenzucuhpqm4imkwloz5nbmtb4dlgo",
    "title": "Deep Support Vector Machines",
    "info": {
        "author": [
            "Marco Wiering, Faculty of Mathematics and Natural Sciences, University of Groningen"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_wiering_vector/",
    "segmentation": [
        [
            "I am from a hernia from the Institute of Artificial Intelligence and Cognitive Engineering, and well, we already heard talk of her Professor Taylor about deeper kernels.",
            "There's some work on deeper kernels and I studied for the problem and try to come up with support vector machines with multiple layers which can be trained in quite a simple way and can be extended to as many ways as you want.",
            "So the contents of that awkward.",
            "Follows first I."
        ],
        [
            "The capital it's a little bit about support vector machines, especially support vector machine Re question because that will be the tool of extracting features from the input patterns.",
            "Then we will continue with how the deep support vector machine looks like, how it is trained and we will study some experimental results both on 10 requesting problems and eight classification problems."
        ],
        [
            "So in general, machinery in English, you know it's a lot of applications both in classification and regression, but also in the adaptive control.",
            "Like my background is reinforcement learning and I met a lot of like game playing programs like for commercial claims, SLS for like chairs and became and also clustering.",
            "Outlier detection is all part of the people who do like data, different methods it for machine learning.",
            "So in general, you said data, you see there's a little data stored and abundant data, but from this date you want to catch knowledge through the form of a model.",
            "So some example applications which I've worked on, it's like seeing classification, face recognition, emotion recognition and also object recognition.",
            "So I have also worked on this golf datasets.",
            "Um well about support sex."
        ],
        [
            "Machines, of course.",
            "They perform use you much better, especially when the many able features and very few input are factual examples.",
            "Then the yeah, the best choice, however.",
            "Like shallow models, so Professor Boyd said, well, the deep architectures can perform better.",
            "We have to find another nasty word for it, but I kind of like this method is alternative way of going to deep way and therefore I started this problem to do this with support vector machines.",
            "So in general deep architectures have had the best results.",
            "Now many image recognition datasets and then just do like millions of parameters and because it's don't use kernels that don't have to do things in the dual space.",
            "It's still very fast that they look for examples because they don't have to compute the similarity matrix, which is kind of complicated like for big data we use kernel methods and other fingers like support vector machines that resolves often depends a lot on the chosen kernel, but Colonel so not so flexible.",
            "I mean most people just use an RBF kernel, maybe some other colors, or they compare it, but most people like if you look from by infomatics or chemistry we just see support vector machines as a tool.",
            "And your shoes are radial basis function.",
            "Kernel and June like the Sigma parameter.",
            "So the advantages of like going combining kernels of multiple kernel learning at also off the deep support vector machine is that a kernel becomes much more flexible.",
            "So the idea of the deep support vector machine is that basically you can go by many layers of support vector machines instead of multiple kernel learning way of weighting coefficients.",
            "Basically the whole method consists of Lexapro effectiveness ease.",
            "Which are like the support factor coefficients and the bias numbers.",
            "There's nothing else for test to be tuned their system."
        ],
        [
            "So support vector machines are re question because yeah I will use them to extract features because they should be continuous features.",
            "So there are basically based on the theory of structural risk minimisation developed by flooding the fabric in the 60s already, and the idea is to have a function.",
            "But if you use like in hinge loss function to never happen Ellis larger than epsilon, and to generalize well so have prefer small norm on your way factor.",
            "This use convex optimization problem which is perfect because he would just like going to like the club optimum.",
            "And yeah you can return the solution.",
            "You will see quite fast.",
            "More formally, you can see like here."
        ],
        [
            "This is like a picture.",
            "It shows like the boundaries that allows allowed in regression problems here today take samples here, some outliers which are not in the excellent bound.",
            "So you have to introduce some slack for arrival succeed and they were added to the course.",
            "Let's say yourself hyper or may the power meter which should be tuned to get off more forms.",
            "Yeah, these are the constraints in regression problem, so the function should never be much bigger than the output function and the date example should also applied maths below at the learn function.",
            "So."
        ],
        [
            "Check out to the dual space.",
            "It makes it not serve yeah easier and fell for necessary thing to do.",
            "Sir Colonel formulations.",
            "Basically the idea is to to use La Concha multipliers which other super factor coefficients Alpha and then you come up with the following formulation where here somebody could say some value.",
            "It's because you want to maximize this W objective to do effective.",
            "To the Alpha, both offer standoff a.",
            "Then here it means that large number should be penalized.",
            "It means shoot correlate well with outputs.",
            "And here it means that you have based on the similarity of inputs effects similar.",
            "Then basically you want to set some of these offers to sell because otherwise you would minimize it instead of maximizing the function.",
            "And there's some known constants.",
            "This is Tobias constraint which we will also introduce later.",
            "So the finally if you.",
            "Then estimate that Alpha values.",
            "You can have a function which is based in the linear case.",
            "You can transform this just threw away thing to normal weight, but in the door linear functions, each situated here in this way and this is like this perfect expansion where you only have to keep like values which are the differences between Alpha standoff.",
            "It's more cereal."
        ],
        [
            "Self optimization algorithms, not many.",
            "Like many people use the SCM light lip SVM based on Smosh at the programming.",
            "But to make it much simpler for us to develop this deep support vector machine and also because we don't want to minimize the objective at all time steps as we will see later we differed develop like a creative descent method.",
            "Basically you can just have if you have to do or you can just compute the defective parts relative to each Alpha coefficient.",
            "And you get like learning way, so this so this is very simple to program, although of course it's not optimal in like a time complexity and more work is to the be down to make this much faster, but it's a very simple way to just develop your own support vector machine tool kit instead of relying on all the known toolkits in case you want to change them."
        ],
        [
            "So some relations, either with multiple kernel learning so but also the previous talk of Francis, continues to be so like multiple kernel learning an even like multiple layer kernel learning that's related, but there are some differences in multiple kernel learning like for example in kernel learning.",
            "You want maybe to wait some of the dimensions to have an RBF kernel in this way, so some domains can be weighted more heavily than others, which is normal thing to do, although you almost normalized today.",
            "If it still makes sense.",
            "And then you get some kind of a min Max problem in the door collective.",
            "If you want to optimize if you exit.",
            "So."
        ],
        [
            "The DSM is kind of different in this way because basically yeah, if you want to choose secured or you can just like use a perfected machine, it catch the inputs and it computes one feature so you stick like kind of all those as protected machines.",
            "The more you can typically feature feature presentation and then there's main outputs classifier or may request support vector machine which gave the final output of the model.",
            "So basically we use credit or central decent.",
            "Escape Data center decent because now it's a min Max problem an basically there's a big relation with multiple multilayer perceptrons like in the past or like perceptions and over easy and they couldn't solve like X or problems that means to copper for shows and by having multiple off those perceptions put together you have to motivate perception and you can easily make the deep whatever you want.",
            "Support vector machine.",
            "The deep support vector machine.",
            "Basically we do the same, but each perceptron is replaced by support vector machine.",
            "So you can understand that the representation takes much more space.",
            "Go to capital, really big datasets in the dual space like want to solve for.",
            "Also in the talk of Professor Mark you have probably 2 to do some kind of sparsification like adding only kernels which are necessary on input points which are far away of other points.",
            "But there is not yet the case here because we have problems with at most like 1050 examples.",
            "So then still fast enough but the architecture."
        ],
        [
            "Looks as follows.",
            "Do you have inputs that care for tool?",
            "In this case we support vector machines, but in general we optimize them with some apartment search algorithm which is called particle swarm optimization.",
            "And sometimes we get like even like 100 support vector machine here.",
            "Then there these are Kiffin extracted each on one feature.",
            "Bigger should protect machine.",
            "Can you say handle one feature at the same time and for multiple classes you need to train both of us for effective scenes.",
            "Then there's the main support vector machine and basically takes all this feature indicates the final output.",
            "So the workings of this system as follows.",
            "You want to countersue."
        ],
        [
            "Presentation, but just for normal re question you can catch it some kernel.",
            "Here we will use feel radial basis function kernels in both layers.",
            "So this is really the basis function kernel and then we kept on feature if it trade for each support vector machine at 123 in this case or a is 1, two and three we could like support vector machines as protector coefficients Mount for free and for one we care for each data.",
            "Example a lot of support vector coefficients and then we can compute features.",
            "For all the data examples also for the test examples and then the main support vector machine is just the normal way, but now it uses a kernel over the extracted features S inputs.",
            "So."
        ],
        [
            "What we do to train the system or first we have to output function which is usually like the function with GX is now over the feet suspension first expected for the more directive function W is replaced by an Max problem by RW wants to minimize for the features and wants to maximize the overall patients.",
            "So this is still funny.",
            "How this is going with the min Max optimization because the same function goes up and down.",
            "And also if you look at how it learns.",
            "Do Oak active coach up and down all the time?",
            "And it's like something like game theory where you have like openings and it's going up and down and there's no one and end with this.",
            "It's could confirm some saddle point, but this doesn't happen very often.",
            "So.",
            "The."
        ],
        [
            "Training procedure with created a sentence as follows.",
            "Basically when we have like the the main support vector machine, the problem is kind of simple because we first optimize for the main support vector machine.",
            "So we adopt all this Alpha coefficients.",
            "Acambis creative are sent and now it is based on this kernel here.",
            "So if this is the function that this is the creative percent update step, both our friend of a star.",
            "For the."
        ],
        [
            "Featureless, it's a little bit more complicated, but idea of like training multilayer perceptrons is also on backpropagation.",
            "Through.",
            "It applies the same way of using backpropagation to place like datasets for each feature, extracting Asia.",
            "So in this way we have already basis function even here and then we have.",
            "This is the error basically now to do effective to the features then this can be written as follows and because we know the kernel we can compute exactly like how we want to but kind of features how this would be changed an inst."
        ],
        [
            "For changing them immediately, basically will be.",
            "We construct whole data set by by adding this this to the previous feature.",
            "So we have the feature and you subtract this because we want to minimize it.",
            "That's the credit decent perspective.",
            "Then we create for each point we get a new feature vector, which should be the target feature, and then we after getting this training data set with, we can trade each independence, independence, supertech machine.",
            "I want to know two things.",
            "First of all, you could do this with multilayer perceptrons as well by first failing the output.",
            "An note and then giving the errors to all the other nodes would be a little bit similar to like a better approach offers perfecto mobile epicenters, but in this way it makes little sense.",
            "Furthermore, visit support vector machines to initialize them.",
            "We need some kind of symmetry breaking, so otherwise they would all expect the same features, which is not what we want, because then it will turn out that the only.",
            "Learn one feature.",
            "It's always the same.",
            "So what we do we?",
            "We trained superfecta machines the first time or like perturbed outputs off that packet data set.",
            "So in that way if it's ready like in our sample method to little bit like we just throw a lot of noise under the side outputs that can be bold on for classification with output minus one or one is like you had some random number between minus 0.5 zero point 5.",
            "Then you change perspective machines.",
            "Definition Point, which works better.",
            "We also looked at initializing older Alpha coefficients completely randomly.",
            "The also works, I could, but it takes more time to train and the final solution is a little bit worse."
        ],
        [
            "So in kernel, writing down some relations like yeah like I told Freddie also the Franciscan UTSA has like a two layer multi kernel learning framework there.",
            "Others that also deep kernels lack of South enjoy an the other relations like yourself and set already like this things.",
            "Logistics functions to train like a kernel and in that way but use completely different way of optimizing Hughes.",
            "Like if you see dimension to optimize the.",
            "And finish with a bang.",
            "Your proposed newer super effective networks, also known Netflix learn to extract features, but in that made you some kind of a ristic and some boosting methods to train those methods.",
            "So our method is more elegant base that the theories of space."
        ],
        [
            "So, like backpropagation to do this.",
            "So we will call to some experimental results.",
            "First we look at this house and family question problems, but we use 90% twenty days of 10% testing data.",
            "We perform 1000 times cross validations and lot of number of hyperparameters.",
            "This is a problem because you can imagine that the RBF kernel entry question module you have the epsilon you have to see you have the Sigma so that the two layers are going to keep 6 paramaters.",
            "Then the other learning coefficients.",
            "The learning parameters, which are two more so it's 8.",
            "The number of support vector machines which you want to use, so that's nine and we have something like a bias constraint with the didn't show but the bicycle state.",
            "We put also in there and how much it should be respected.",
            "This another major power meter.",
            "So we use the very smart method faces part for Swarm optimization, known form like a biological are you coded optimization an we train this and it was like working or like a 52 CPU.",
            "Station and they just sent parameter factors to to them in parallel, catch them back and then tries to find the best animators so it's all done automatically.",
            "But it still takes for some long experiments like 2 days of of running it, almost 32 core machine.",
            "So I think the problem is that our credit percent metric is not fast enough for this.",
            "We also did buy me to search optimization for like just gradient Ascent Training Officer protection machine.",
            "And then of course, much simpler to find optimal parameters.",
            "On this request and problems we find we show the mean squared errors and their standard errors.",
            "So here are 10."
        ],
        [
            "Datasets but we can see is here are the results of support vector machine.",
            "Here are results of like a paper from which we use eight datasets.",
            "But they only use 10 fold cross validation.",
            "So there is also not very trusted trustworthy because the datasets are sometimes quite small.",
            "So then yeah it's small datasets.",
            "It can basically get over optimistic results.",
            "We can see that in six out of 10 cases, the D SVM outperforms the SM significantly.",
            "Like maybe it's true that the test is not the best test because they're not normally distributed errors, but it keeps like a difference of 10 to the power mean 916.",
            "So that's the P value, so it's highly significant and one data set it before was out performed.",
            "The first abortion housing housing of the Oceania possibility, but don't know why.",
            "Why just happened."
        ],
        [
            "Also, we experimented on eight well known datasets on the sea repository.",
            "We used the same flavor PSO, 90% training day for 10% test data, and now we show effort."
        ],
        [
            "Curses and it's telling that anglers and on this data set back on four of the datasets, our method performs significantly better than support vector machine.",
            "Also then a multilayer perceptron so the effort scores also like 1.4% better, better support vector machine and supports accuracy scores only like half percent better than most of their perception.",
            "So these results are significant, especially for some datasets because some other datasets like for example Pima Indians.",
            "Yeah, all the results are very similar and there's not much like breast cancer Wisconsin you cannot really call much better than that, probably due to the noise in those datasets."
        ],
        [
            "To have showed they show like how you can go from one support vector machine tool like a deep support vector machine in a very simple way, but using basic backpropagation to create datasets and using Creative Arts Center Sterling method.",
            "There's a really complex interaction, so if you look at the curves of like the training out of testing error in the do objective, they related their correlate, but it's still quite complicated.",
            "Like for example, we also have to optimize the number of reputations through the data set, the number of airports recall through it.",
            "You said that was like 30.",
            "So on some datasets, if you train it on ours it only takes 1 second.",
            "But finding the parameters still problem, we want to look which permits really influenced themselves.",
            "I wish we can just set a constant values to get rid of that problem.",
            "Maybe also want to study some other real code apartment optimization problem algorithms.",
            "Yeah he sells which we have for those small datasets are significantly better than state of the art.",
            "Procedural matters.",
            "So we want to."
        ],
        [
            "I didn't tell it, but for the classification problem we had multiple support vector machines in Albert Lea sharing the same hidden layer an all there like do objectives.",
            "Their gradients are back propagated to estimate the feature vectors.",
            "So basically we can also out encoding with this deep support vector machine, which is nice because yeah we already did some experiments and these look very promising.",
            "We also want to get two more challenging supervised learning problems like this colorful datasets, which is much nicer than the toy examples we use here.",
            "But for that we want to speed up the algorithm, so we want to device a GPU implementation which shouldn't be so hard because we have like a created descent methods and could be easily done in parallel.",
            "Every want to study more about the theory, like what influences which influences like maybe convergence properties."
        ],
        [
            "So I want to thank a lot of people, but I don't want to mention all the names here an by for any questions."
        ],
        [
            "Be happy to.",
            "And so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am from a hernia from the Institute of Artificial Intelligence and Cognitive Engineering, and well, we already heard talk of her Professor Taylor about deeper kernels.",
                    "label": 1
                },
                {
                    "sent": "There's some work on deeper kernels and I studied for the problem and try to come up with support vector machines with multiple layers which can be trained in quite a simple way and can be extended to as many ways as you want.",
                    "label": 0
                },
                {
                    "sent": "So the contents of that awkward.",
                    "label": 0
                },
                {
                    "sent": "Follows first I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The capital it's a little bit about support vector machines, especially support vector machine Re question because that will be the tool of extracting features from the input patterns.",
                    "label": 0
                },
                {
                    "sent": "Then we will continue with how the deep support vector machine looks like, how it is trained and we will study some experimental results both on 10 requesting problems and eight classification problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, machinery in English, you know it's a lot of applications both in classification and regression, but also in the adaptive control.",
                    "label": 1
                },
                {
                    "sent": "Like my background is reinforcement learning and I met a lot of like game playing programs like for commercial claims, SLS for like chairs and became and also clustering.",
                    "label": 0
                },
                {
                    "sent": "Outlier detection is all part of the people who do like data, different methods it for machine learning.",
                    "label": 0
                },
                {
                    "sent": "So in general, you said data, you see there's a little data stored and abundant data, but from this date you want to catch knowledge through the form of a model.",
                    "label": 0
                },
                {
                    "sent": "So some example applications which I've worked on, it's like seeing classification, face recognition, emotion recognition and also object recognition.",
                    "label": 1
                },
                {
                    "sent": "So I have also worked on this golf datasets.",
                    "label": 0
                },
                {
                    "sent": "Um well about support sex.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machines, of course.",
                    "label": 0
                },
                {
                    "sent": "They perform use you much better, especially when the many able features and very few input are factual examples.",
                    "label": 0
                },
                {
                    "sent": "Then the yeah, the best choice, however.",
                    "label": 0
                },
                {
                    "sent": "Like shallow models, so Professor Boyd said, well, the deep architectures can perform better.",
                    "label": 1
                },
                {
                    "sent": "We have to find another nasty word for it, but I kind of like this method is alternative way of going to deep way and therefore I started this problem to do this with support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So in general deep architectures have had the best results.",
                    "label": 0
                },
                {
                    "sent": "Now many image recognition datasets and then just do like millions of parameters and because it's don't use kernels that don't have to do things in the dual space.",
                    "label": 0
                },
                {
                    "sent": "It's still very fast that they look for examples because they don't have to compute the similarity matrix, which is kind of complicated like for big data we use kernel methods and other fingers like support vector machines that resolves often depends a lot on the chosen kernel, but Colonel so not so flexible.",
                    "label": 0
                },
                {
                    "sent": "I mean most people just use an RBF kernel, maybe some other colors, or they compare it, but most people like if you look from by infomatics or chemistry we just see support vector machines as a tool.",
                    "label": 0
                },
                {
                    "sent": "And your shoes are radial basis function.",
                    "label": 0
                },
                {
                    "sent": "Kernel and June like the Sigma parameter.",
                    "label": 0
                },
                {
                    "sent": "So the advantages of like going combining kernels of multiple kernel learning at also off the deep support vector machine is that a kernel becomes much more flexible.",
                    "label": 0
                },
                {
                    "sent": "So the idea of the deep support vector machine is that basically you can go by many layers of support vector machines instead of multiple kernel learning way of weighting coefficients.",
                    "label": 1
                },
                {
                    "sent": "Basically the whole method consists of Lexapro effectiveness ease.",
                    "label": 0
                },
                {
                    "sent": "Which are like the support factor coefficients and the bias numbers.",
                    "label": 0
                },
                {
                    "sent": "There's nothing else for test to be tuned their system.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So support vector machines are re question because yeah I will use them to extract features because they should be continuous features.",
                    "label": 0
                },
                {
                    "sent": "So there are basically based on the theory of structural risk minimisation developed by flooding the fabric in the 60s already, and the idea is to have a function.",
                    "label": 1
                },
                {
                    "sent": "But if you use like in hinge loss function to never happen Ellis larger than epsilon, and to generalize well so have prefer small norm on your way factor.",
                    "label": 0
                },
                {
                    "sent": "This use convex optimization problem which is perfect because he would just like going to like the club optimum.",
                    "label": 0
                },
                {
                    "sent": "And yeah you can return the solution.",
                    "label": 0
                },
                {
                    "sent": "You will see quite fast.",
                    "label": 0
                },
                {
                    "sent": "More formally, you can see like here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is like a picture.",
                    "label": 0
                },
                {
                    "sent": "It shows like the boundaries that allows allowed in regression problems here today take samples here, some outliers which are not in the excellent bound.",
                    "label": 0
                },
                {
                    "sent": "So you have to introduce some slack for arrival succeed and they were added to the course.",
                    "label": 0
                },
                {
                    "sent": "Let's say yourself hyper or may the power meter which should be tuned to get off more forms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, these are the constraints in regression problem, so the function should never be much bigger than the output function and the date example should also applied maths below at the learn function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check out to the dual space.",
                    "label": 0
                },
                {
                    "sent": "It makes it not serve yeah easier and fell for necessary thing to do.",
                    "label": 0
                },
                {
                    "sent": "Sir Colonel formulations.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea is to to use La Concha multipliers which other super factor coefficients Alpha and then you come up with the following formulation where here somebody could say some value.",
                    "label": 0
                },
                {
                    "sent": "It's because you want to maximize this W objective to do effective.",
                    "label": 0
                },
                {
                    "sent": "To the Alpha, both offer standoff a.",
                    "label": 0
                },
                {
                    "sent": "Then here it means that large number should be penalized.",
                    "label": 0
                },
                {
                    "sent": "It means shoot correlate well with outputs.",
                    "label": 0
                },
                {
                    "sent": "And here it means that you have based on the similarity of inputs effects similar.",
                    "label": 0
                },
                {
                    "sent": "Then basically you want to set some of these offers to sell because otherwise you would minimize it instead of maximizing the function.",
                    "label": 0
                },
                {
                    "sent": "And there's some known constants.",
                    "label": 0
                },
                {
                    "sent": "This is Tobias constraint which we will also introduce later.",
                    "label": 0
                },
                {
                    "sent": "So the finally if you.",
                    "label": 0
                },
                {
                    "sent": "Then estimate that Alpha values.",
                    "label": 0
                },
                {
                    "sent": "You can have a function which is based in the linear case.",
                    "label": 0
                },
                {
                    "sent": "You can transform this just threw away thing to normal weight, but in the door linear functions, each situated here in this way and this is like this perfect expansion where you only have to keep like values which are the differences between Alpha standoff.",
                    "label": 0
                },
                {
                    "sent": "It's more cereal.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Self optimization algorithms, not many.",
                    "label": 0
                },
                {
                    "sent": "Like many people use the SCM light lip SVM based on Smosh at the programming.",
                    "label": 0
                },
                {
                    "sent": "But to make it much simpler for us to develop this deep support vector machine and also because we don't want to minimize the objective at all time steps as we will see later we differed develop like a creative descent method.",
                    "label": 0
                },
                {
                    "sent": "Basically you can just have if you have to do or you can just compute the defective parts relative to each Alpha coefficient.",
                    "label": 0
                },
                {
                    "sent": "And you get like learning way, so this so this is very simple to program, although of course it's not optimal in like a time complexity and more work is to the be down to make this much faster, but it's a very simple way to just develop your own support vector machine tool kit instead of relying on all the known toolkits in case you want to change them.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some relations, either with multiple kernel learning so but also the previous talk of Francis, continues to be so like multiple kernel learning an even like multiple layer kernel learning that's related, but there are some differences in multiple kernel learning like for example in kernel learning.",
                    "label": 1
                },
                {
                    "sent": "You want maybe to wait some of the dimensions to have an RBF kernel in this way, so some domains can be weighted more heavily than others, which is normal thing to do, although you almost normalized today.",
                    "label": 1
                },
                {
                    "sent": "If it still makes sense.",
                    "label": 0
                },
                {
                    "sent": "And then you get some kind of a min Max problem in the door collective.",
                    "label": 0
                },
                {
                    "sent": "If you want to optimize if you exit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The DSM is kind of different in this way because basically yeah, if you want to choose secured or you can just like use a perfected machine, it catch the inputs and it computes one feature so you stick like kind of all those as protected machines.",
                    "label": 1
                },
                {
                    "sent": "The more you can typically feature feature presentation and then there's main outputs classifier or may request support vector machine which gave the final output of the model.",
                    "label": 0
                },
                {
                    "sent": "So basically we use credit or central decent.",
                    "label": 1
                },
                {
                    "sent": "Escape Data center decent because now it's a min Max problem an basically there's a big relation with multiple multilayer perceptrons like in the past or like perceptions and over easy and they couldn't solve like X or problems that means to copper for shows and by having multiple off those perceptions put together you have to motivate perception and you can easily make the deep whatever you want.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine.",
                    "label": 0
                },
                {
                    "sent": "The deep support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Basically we do the same, but each perceptron is replaced by support vector machine.",
                    "label": 1
                },
                {
                    "sent": "So you can understand that the representation takes much more space.",
                    "label": 0
                },
                {
                    "sent": "Go to capital, really big datasets in the dual space like want to solve for.",
                    "label": 0
                },
                {
                    "sent": "Also in the talk of Professor Mark you have probably 2 to do some kind of sparsification like adding only kernels which are necessary on input points which are far away of other points.",
                    "label": 0
                },
                {
                    "sent": "But there is not yet the case here because we have problems with at most like 1050 examples.",
                    "label": 0
                },
                {
                    "sent": "So then still fast enough but the architecture.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks as follows.",
                    "label": 0
                },
                {
                    "sent": "Do you have inputs that care for tool?",
                    "label": 0
                },
                {
                    "sent": "In this case we support vector machines, but in general we optimize them with some apartment search algorithm which is called particle swarm optimization.",
                    "label": 0
                },
                {
                    "sent": "And sometimes we get like even like 100 support vector machine here.",
                    "label": 0
                },
                {
                    "sent": "Then there these are Kiffin extracted each on one feature.",
                    "label": 1
                },
                {
                    "sent": "Bigger should protect machine.",
                    "label": 0
                },
                {
                    "sent": "Can you say handle one feature at the same time and for multiple classes you need to train both of us for effective scenes.",
                    "label": 0
                },
                {
                    "sent": "Then there's the main support vector machine and basically takes all this feature indicates the final output.",
                    "label": 1
                },
                {
                    "sent": "So the workings of this system as follows.",
                    "label": 0
                },
                {
                    "sent": "You want to countersue.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presentation, but just for normal re question you can catch it some kernel.",
                    "label": 0
                },
                {
                    "sent": "Here we will use feel radial basis function kernels in both layers.",
                    "label": 0
                },
                {
                    "sent": "So this is really the basis function kernel and then we kept on feature if it trade for each support vector machine at 123 in this case or a is 1, two and three we could like support vector machines as protector coefficients Mount for free and for one we care for each data.",
                    "label": 0
                },
                {
                    "sent": "Example a lot of support vector coefficients and then we can compute features.",
                    "label": 0
                },
                {
                    "sent": "For all the data examples also for the test examples and then the main support vector machine is just the normal way, but now it uses a kernel over the extracted features S inputs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do to train the system or first we have to output function which is usually like the function with GX is now over the feet suspension first expected for the more directive function W is replaced by an Max problem by RW wants to minimize for the features and wants to maximize the overall patients.",
                    "label": 0
                },
                {
                    "sent": "So this is still funny.",
                    "label": 0
                },
                {
                    "sent": "How this is going with the min Max optimization because the same function goes up and down.",
                    "label": 0
                },
                {
                    "sent": "And also if you look at how it learns.",
                    "label": 0
                },
                {
                    "sent": "Do Oak active coach up and down all the time?",
                    "label": 0
                },
                {
                    "sent": "And it's like something like game theory where you have like openings and it's going up and down and there's no one and end with this.",
                    "label": 0
                },
                {
                    "sent": "It's could confirm some saddle point, but this doesn't happen very often.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training procedure with created a sentence as follows.",
                    "label": 0
                },
                {
                    "sent": "Basically when we have like the the main support vector machine, the problem is kind of simple because we first optimize for the main support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So we adopt all this Alpha coefficients.",
                    "label": 0
                },
                {
                    "sent": "Acambis creative are sent and now it is based on this kernel here.",
                    "label": 0
                },
                {
                    "sent": "So if this is the function that this is the creative percent update step, both our friend of a star.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Featureless, it's a little bit more complicated, but idea of like training multilayer perceptrons is also on backpropagation.",
                    "label": 0
                },
                {
                    "sent": "Through.",
                    "label": 0
                },
                {
                    "sent": "It applies the same way of using backpropagation to place like datasets for each feature, extracting Asia.",
                    "label": 0
                },
                {
                    "sent": "So in this way we have already basis function even here and then we have.",
                    "label": 0
                },
                {
                    "sent": "This is the error basically now to do effective to the features then this can be written as follows and because we know the kernel we can compute exactly like how we want to but kind of features how this would be changed an inst.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For changing them immediately, basically will be.",
                    "label": 0
                },
                {
                    "sent": "We construct whole data set by by adding this this to the previous feature.",
                    "label": 0
                },
                {
                    "sent": "So we have the feature and you subtract this because we want to minimize it.",
                    "label": 0
                },
                {
                    "sent": "That's the credit decent perspective.",
                    "label": 0
                },
                {
                    "sent": "Then we create for each point we get a new feature vector, which should be the target feature, and then we after getting this training data set with, we can trade each independence, independence, supertech machine.",
                    "label": 0
                },
                {
                    "sent": "I want to know two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, you could do this with multilayer perceptrons as well by first failing the output.",
                    "label": 0
                },
                {
                    "sent": "An note and then giving the errors to all the other nodes would be a little bit similar to like a better approach offers perfecto mobile epicenters, but in this way it makes little sense.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, visit support vector machines to initialize them.",
                    "label": 0
                },
                {
                    "sent": "We need some kind of symmetry breaking, so otherwise they would all expect the same features, which is not what we want, because then it will turn out that the only.",
                    "label": 0
                },
                {
                    "sent": "Learn one feature.",
                    "label": 0
                },
                {
                    "sent": "It's always the same.",
                    "label": 0
                },
                {
                    "sent": "So what we do we?",
                    "label": 0
                },
                {
                    "sent": "We trained superfecta machines the first time or like perturbed outputs off that packet data set.",
                    "label": 0
                },
                {
                    "sent": "So in that way if it's ready like in our sample method to little bit like we just throw a lot of noise under the side outputs that can be bold on for classification with output minus one or one is like you had some random number between minus 0.5 zero point 5.",
                    "label": 0
                },
                {
                    "sent": "Then you change perspective machines.",
                    "label": 0
                },
                {
                    "sent": "Definition Point, which works better.",
                    "label": 0
                },
                {
                    "sent": "We also looked at initializing older Alpha coefficients completely randomly.",
                    "label": 0
                },
                {
                    "sent": "The also works, I could, but it takes more time to train and the final solution is a little bit worse.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in kernel, writing down some relations like yeah like I told Freddie also the Franciscan UTSA has like a two layer multi kernel learning framework there.",
                    "label": 0
                },
                {
                    "sent": "Others that also deep kernels lack of South enjoy an the other relations like yourself and set already like this things.",
                    "label": 0
                },
                {
                    "sent": "Logistics functions to train like a kernel and in that way but use completely different way of optimizing Hughes.",
                    "label": 0
                },
                {
                    "sent": "Like if you see dimension to optimize the.",
                    "label": 0
                },
                {
                    "sent": "And finish with a bang.",
                    "label": 0
                },
                {
                    "sent": "Your proposed newer super effective networks, also known Netflix learn to extract features, but in that made you some kind of a ristic and some boosting methods to train those methods.",
                    "label": 0
                },
                {
                    "sent": "So our method is more elegant base that the theories of space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, like backpropagation to do this.",
                    "label": 0
                },
                {
                    "sent": "So we will call to some experimental results.",
                    "label": 1
                },
                {
                    "sent": "First we look at this house and family question problems, but we use 90% twenty days of 10% testing data.",
                    "label": 1
                },
                {
                    "sent": "We perform 1000 times cross validations and lot of number of hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "This is a problem because you can imagine that the RBF kernel entry question module you have the epsilon you have to see you have the Sigma so that the two layers are going to keep 6 paramaters.",
                    "label": 0
                },
                {
                    "sent": "Then the other learning coefficients.",
                    "label": 1
                },
                {
                    "sent": "The learning parameters, which are two more so it's 8.",
                    "label": 0
                },
                {
                    "sent": "The number of support vector machines which you want to use, so that's nine and we have something like a bias constraint with the didn't show but the bicycle state.",
                    "label": 0
                },
                {
                    "sent": "We put also in there and how much it should be respected.",
                    "label": 0
                },
                {
                    "sent": "This another major power meter.",
                    "label": 0
                },
                {
                    "sent": "So we use the very smart method faces part for Swarm optimization, known form like a biological are you coded optimization an we train this and it was like working or like a 52 CPU.",
                    "label": 0
                },
                {
                    "sent": "Station and they just sent parameter factors to to them in parallel, catch them back and then tries to find the best animators so it's all done automatically.",
                    "label": 0
                },
                {
                    "sent": "But it still takes for some long experiments like 2 days of of running it, almost 32 core machine.",
                    "label": 1
                },
                {
                    "sent": "So I think the problem is that our credit percent metric is not fast enough for this.",
                    "label": 0
                },
                {
                    "sent": "We also did buy me to search optimization for like just gradient Ascent Training Officer protection machine.",
                    "label": 1
                },
                {
                    "sent": "And then of course, much simpler to find optimal parameters.",
                    "label": 0
                },
                {
                    "sent": "On this request and problems we find we show the mean squared errors and their standard errors.",
                    "label": 0
                },
                {
                    "sent": "So here are 10.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datasets but we can see is here are the results of support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Here are results of like a paper from which we use eight datasets.",
                    "label": 0
                },
                {
                    "sent": "But they only use 10 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "So there is also not very trusted trustworthy because the datasets are sometimes quite small.",
                    "label": 0
                },
                {
                    "sent": "So then yeah it's small datasets.",
                    "label": 0
                },
                {
                    "sent": "It can basically get over optimistic results.",
                    "label": 0
                },
                {
                    "sent": "We can see that in six out of 10 cases, the D SVM outperforms the SM significantly.",
                    "label": 0
                },
                {
                    "sent": "Like maybe it's true that the test is not the best test because they're not normally distributed errors, but it keeps like a difference of 10 to the power mean 916.",
                    "label": 0
                },
                {
                    "sent": "So that's the P value, so it's highly significant and one data set it before was out performed.",
                    "label": 0
                },
                {
                    "sent": "The first abortion housing housing of the Oceania possibility, but don't know why.",
                    "label": 0
                },
                {
                    "sent": "Why just happened.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we experimented on eight well known datasets on the sea repository.",
                    "label": 0
                },
                {
                    "sent": "We used the same flavor PSO, 90% training day for 10% test data, and now we show effort.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Curses and it's telling that anglers and on this data set back on four of the datasets, our method performs significantly better than support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Also then a multilayer perceptron so the effort scores also like 1.4% better, better support vector machine and supports accuracy scores only like half percent better than most of their perception.",
                    "label": 0
                },
                {
                    "sent": "So these results are significant, especially for some datasets because some other datasets like for example Pima Indians.",
                    "label": 0
                },
                {
                    "sent": "Yeah, all the results are very similar and there's not much like breast cancer Wisconsin you cannot really call much better than that, probably due to the noise in those datasets.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To have showed they show like how you can go from one support vector machine tool like a deep support vector machine in a very simple way, but using basic backpropagation to create datasets and using Creative Arts Center Sterling method.",
                    "label": 0
                },
                {
                    "sent": "There's a really complex interaction, so if you look at the curves of like the training out of testing error in the do objective, they related their correlate, but it's still quite complicated.",
                    "label": 0
                },
                {
                    "sent": "Like for example, we also have to optimize the number of reputations through the data set, the number of airports recall through it.",
                    "label": 0
                },
                {
                    "sent": "You said that was like 30.",
                    "label": 0
                },
                {
                    "sent": "So on some datasets, if you train it on ours it only takes 1 second.",
                    "label": 0
                },
                {
                    "sent": "But finding the parameters still problem, we want to look which permits really influenced themselves.",
                    "label": 0
                },
                {
                    "sent": "I wish we can just set a constant values to get rid of that problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe also want to study some other real code apartment optimization problem algorithms.",
                    "label": 1
                },
                {
                    "sent": "Yeah he sells which we have for those small datasets are significantly better than state of the art.",
                    "label": 1
                },
                {
                    "sent": "Procedural matters.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I didn't tell it, but for the classification problem we had multiple support vector machines in Albert Lea sharing the same hidden layer an all there like do objectives.",
                    "label": 0
                },
                {
                    "sent": "Their gradients are back propagated to estimate the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So basically we can also out encoding with this deep support vector machine, which is nice because yeah we already did some experiments and these look very promising.",
                    "label": 0
                },
                {
                    "sent": "We also want to get two more challenging supervised learning problems like this colorful datasets, which is much nicer than the toy examples we use here.",
                    "label": 1
                },
                {
                    "sent": "But for that we want to speed up the algorithm, so we want to device a GPU implementation which shouldn't be so hard because we have like a created descent methods and could be easily done in parallel.",
                    "label": 1
                },
                {
                    "sent": "Every want to study more about the theory, like what influences which influences like maybe convergence properties.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to thank a lot of people, but I don't want to mention all the names here an by for any questions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be happy to.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        }
    }
}