{
    "id": "rf6mw6goig2biyoam66g53i4knmny47h",
    "title": "Unsupervised Bayesian Learning of Natural Language Semantics",
    "info": {
        "author": [
            "Ivan Titov, Cluster of Excellence Multimodal Computing and Interaction, Saarland University"
        ],
        "published": "Aug. 6, 2013",
        "recorded": "April 2013",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/machine_titov_bayesian_learning/",
    "segmentation": [
        [
            "So I'm from University of Saarland and this is joint work.",
            "With that experiment if my name is wanted to so I'm going to talk about the project on joining meaning representations for natural language.",
            "So why do we care about meaning representations?",
            "Why do we care about semantic representation?",
            "So consider question answering problems, so you're given a question about the knowledge contained in a collection and you want to find an answer.",
            "But the problem is that the answer can be worked in very different way, from the way it's worded equation.",
            "So you won't understand for example, in the first case that blog and suppress mean essentially the same thing in the biological context.",
            "And to do this we want to induce a form of semantic abstractions, which is always you from this language variability.",
            "Yes."
        ],
        [
            "OK, and so more formally, what we want to do.",
            "We want given input, which is basically a text we want to predict semantic graph which defines what kind of situation we have.",
            "So in this example we have a winning price situation and what kind of participants and properties of the situations we have formalized and in general these are very very simple examples in general graphs and much more complicated.",
            "They don't don't don't necessarily need to be trees and can be very large and.",
            "Recursive.",
            "So we won't know.",
            "So basically you can think of this task as a type of clustering and decompositions because as input you are given sequence of words.",
            "But he also given a syntactic dependency tree.",
            "So basically black part of this graph is provided as input and the colored part is what we want to predict.",
            "And we approach this problem by."
        ],
        [
            "Finding the joint model of syntax, semantic and lexical information so lexical information is basically words and perform inference within this model.",
            "So we approach this task by unsupervised learning.",
            "So why unsupervised learning?",
            "Because basically the two as alternatives, supervised learning or basically defining rules don't provide enough coverage, so resulting models and end up being very domain dependent or don't cover many, many words.",
            "Many lexical phenomena, because semantics is very tight.",
            "Two lexical information.",
            "And another point we approach this by a buyer and learning by nonparametric buyers and learning so the couple of reasons why we choose this method methodology.",
            "So one reason is we want to handle accurately the uncertainty and uncertainty and ambiguity is something which is inherent in natural language.",
            "So we really want to account for uncertainty quite well and another reason we want to have a principled way to encode some prime information in the model and nonparametric techniques allow us to.",
            "I avoid.",
            "Having a model selection every sticks providing more elegant solution.",
            "I would say to this problem.",
            "OK."
        ],
        [
            "So have you ever let our model in the two different setups first showing that this model semantics helping the question answering setup?",
            "So actually the example I had in the very beginning of my talk is actual answers by the system, and in the second case we evaluate how well our result compares to what linguists think about semantics of the languages.",
            "In both cases we get quite favorable result, generally outperforming alternatives.",
            "And we also look into a couple of extensions.",
            "One is induction of cross lingual semantics and not only to induce crossing grow semantics, but also to improve models for every language because language is have a different problems and you can expect that inducing semantics simultaneously across languages might might help.",
            "And another point is to encode some of the supervision.",
            "Use a little bit of supervision to improve learning.",
            "Obviously unsupervised learning has its own problems.",
            "Some of this.",
            "I mean, I've had to learn unsupervised setting, so if you want to know more details, come to my poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm from University of Saarland and this is joint work.",
                    "label": 0
                },
                {
                    "sent": "With that experiment if my name is wanted to so I'm going to talk about the project on joining meaning representations for natural language.",
                    "label": 0
                },
                {
                    "sent": "So why do we care about meaning representations?",
                    "label": 0
                },
                {
                    "sent": "Why do we care about semantic representation?",
                    "label": 0
                },
                {
                    "sent": "So consider question answering problems, so you're given a question about the knowledge contained in a collection and you want to find an answer.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that the answer can be worked in very different way, from the way it's worded equation.",
                    "label": 0
                },
                {
                    "sent": "So you won't understand for example, in the first case that blog and suppress mean essentially the same thing in the biological context.",
                    "label": 0
                },
                {
                    "sent": "And to do this we want to induce a form of semantic abstractions, which is always you from this language variability.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so more formally, what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want given input, which is basically a text we want to predict semantic graph which defines what kind of situation we have.",
                    "label": 1
                },
                {
                    "sent": "So in this example we have a winning price situation and what kind of participants and properties of the situations we have formalized and in general these are very very simple examples in general graphs and much more complicated.",
                    "label": 0
                },
                {
                    "sent": "They don't don't don't necessarily need to be trees and can be very large and.",
                    "label": 0
                },
                {
                    "sent": "Recursive.",
                    "label": 0
                },
                {
                    "sent": "So we won't know.",
                    "label": 0
                },
                {
                    "sent": "So basically you can think of this task as a type of clustering and decompositions because as input you are given sequence of words.",
                    "label": 0
                },
                {
                    "sent": "But he also given a syntactic dependency tree.",
                    "label": 1
                },
                {
                    "sent": "So basically black part of this graph is provided as input and the colored part is what we want to predict.",
                    "label": 0
                },
                {
                    "sent": "And we approach this problem by.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finding the joint model of syntax, semantic and lexical information so lexical information is basically words and perform inference within this model.",
                    "label": 1
                },
                {
                    "sent": "So we approach this task by unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So why unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "Because basically the two as alternatives, supervised learning or basically defining rules don't provide enough coverage, so resulting models and end up being very domain dependent or don't cover many, many words.",
                    "label": 0
                },
                {
                    "sent": "Many lexical phenomena, because semantics is very tight.",
                    "label": 0
                },
                {
                    "sent": "Two lexical information.",
                    "label": 0
                },
                {
                    "sent": "And another point we approach this by a buyer and learning by nonparametric buyers and learning so the couple of reasons why we choose this method methodology.",
                    "label": 0
                },
                {
                    "sent": "So one reason is we want to handle accurately the uncertainty and uncertainty and ambiguity is something which is inherent in natural language.",
                    "label": 1
                },
                {
                    "sent": "So we really want to account for uncertainty quite well and another reason we want to have a principled way to encode some prime information in the model and nonparametric techniques allow us to.",
                    "label": 0
                },
                {
                    "sent": "I avoid.",
                    "label": 0
                },
                {
                    "sent": "Having a model selection every sticks providing more elegant solution.",
                    "label": 0
                },
                {
                    "sent": "I would say to this problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So have you ever let our model in the two different setups first showing that this model semantics helping the question answering setup?",
                    "label": 1
                },
                {
                    "sent": "So actually the example I had in the very beginning of my talk is actual answers by the system, and in the second case we evaluate how well our result compares to what linguists think about semantics of the languages.",
                    "label": 0
                },
                {
                    "sent": "In both cases we get quite favorable result, generally outperforming alternatives.",
                    "label": 0
                },
                {
                    "sent": "And we also look into a couple of extensions.",
                    "label": 1
                },
                {
                    "sent": "One is induction of cross lingual semantics and not only to induce crossing grow semantics, but also to improve models for every language because language is have a different problems and you can expect that inducing semantics simultaneously across languages might might help.",
                    "label": 0
                },
                {
                    "sent": "And another point is to encode some of the supervision.",
                    "label": 1
                },
                {
                    "sent": "Use a little bit of supervision to improve learning.",
                    "label": 0
                },
                {
                    "sent": "Obviously unsupervised learning has its own problems.",
                    "label": 0
                },
                {
                    "sent": "Some of this.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've had to learn unsupervised setting, so if you want to know more details, come to my poster.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}