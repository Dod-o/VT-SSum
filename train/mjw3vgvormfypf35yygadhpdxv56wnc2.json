{
    "id": "mjw3vgvormfypf35yygadhpdxv56wnc2",
    "title": "Learning using Many Examples",
    "info": {
        "author": [
            "L\u00e9on Bottou, NEC Laboratories America, Inc."
        ],
        "published": "Nov. 26, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mmdss07_bottou_lume/",
    "segmentation": [
        [
            "Should I start?",
            "OK, so welcome to the Digest if session.",
            "Oh so the talk is about learning with large datasets and.",
            "The initial observation is that over the last 10 years.",
            "The disk size has increased by about a factor of 1000.",
            "And the CPU speed by a factor of 100.",
            "That means that we have less time for each data item.",
            "So basically we have a sort of trouble and I'm going to try to work out with other consequences of this and."
        ],
        [
            "We can do.",
            "So why should we use large scale data sets to start with?",
            "And there are basically 2.",
            "Broad applications for this.",
            "So the first one is data mining and has to do with money.",
            "The idea is that by analyzing the data that describes the life of our society.",
            "We can gain competitive advantages and make more money.",
            "The other one is about artificial intelligence, and the idea is that if we want to emulate the cognitive abilities of humans, we have to do the same thing they do, which is process huge amounts of data.",
            "Everything we see, everything we hear, everything we feel.",
            "And it does that sometimes very complicated, like language."
        ],
        [
            "So let's go back to the data mining thing and start with a little metaphor of a computerized society.",
            "And suppose you just have two kinds of computers.",
            "We have the makers, they do business, they make revenue, they make money.",
            "And they also produce data that's in proportion with the activity.",
            "And we have the thinkers and they analyze the data and they increase the revenue by finding ways to get competitive advantages.",
            "And of course, when the population grows, the proportion of thinkers cannot grow to one.",
            "It just doesn't seem to work that way, but the data grows like the number of makers.",
            "That means that the number of thinkers doesn't grow faster than data."
        ],
        [
            "So in other words.",
            "The computing resources available for learning do not grow faster than the volume of data.",
            "And the cost of determining cannot exceed the revenue.",
            "No way.",
            "And if you look at the artificial intelligence problem, intelligent animals learn from streaming data.",
            "The amount of processing power they have or scales with the time they can put in it and scales with the volume of the data.",
            "But yet most machine learning algo even demand resource that grow faster than the volume of data.",
            "Like if you do anything with mattress, is that meaningful?",
            "Is going to be Ncube while the size of the matrix is N ^2.",
            "And if you have sparse matrix.",
            "Sparse matrix is actually worse.",
            "So in fact we have a problem ahead.",
            "So even if we want to use a simple tool like linear algebra, we're not going to be able to do it and hope this is going to work for a long time."
        ],
        [
            "So the talk has four parts I'm reasonably happy with the 1st three and on the 4th one.",
            "I'm a little bit less happy, but there are still interesting things I believe.",
            "So the first one is trying to understand the relation between statistical efficiency and computational costs.",
            "On the on, the conceptual at the conceptual level.",
            "The second one is a consequence of the first one is discussed stochastic algorithm.",
            "The third one is to know how well we can do if we see each data item only once.",
            "And the last one is that, well, you know when you want to learn something, you don't go in the library and read random books until you know what you want, you choose.",
            "We should choose our examples and what can we do in that direct?"
        ],
        [
            "So I go with the first part.",
            "I'm not going to say much about data quality issues, which are important when you have large datasets.",
            "I'm not going to say much about data mining specifics.",
            "I'll go even storage and I think this has been done before.",
            "And I will be very fast about implementation.",
            "They'll give some."
        ],
        [
            "Pointers.",
            "So the first part.",
            "Status is called efficiency versus computational costs."
        ],
        [
            "Go back here.",
            "So a quick summary.",
            "If you listen to people who just look at the statistical side, they're going to tell you.",
            "Well, you have to find an algorithm or start a principle that is consistent with high convergence speed, making that you make a good statistical estimation with as few example as possible, and the rest is second order.",
            "If you listen to someone in the optimization domain, they're going to tell you, or you need to use super linear algorithms, otherwise you're doomed.",
            "Happens they're both wrong.",
            "I'm going to try to."
        ],
        [
            "Explain why.",
            "First of all, we have to consider baseline large scale algorithm.",
            "The simplest thing you can do when you have lots of data is to discard data randomly.",
            "You just choose a subset and work with that.",
            "And that that works.",
            "It's not so good, but it works.",
            "And the questions are, what are the statistical benefits of processing more data than that?",
            "And what is the computational cost of processing more data?",
            "And which one wins?",
            "And what is the balance?",
            "In the way there is an interesting remark, as sometimes you see papers that promise to make a learning algorithm was cost doesn't grow faster than the volume of data or the number of examples.",
            "So so called sub linear learning algorithms.",
            "And the thing that's important to understand is that if you have independent data, this.",
            "Is no better than the baseline algorithm.",
            "Here is why.",
            "So if you have a sub linear algorithm, it means that it doesn't even spend the cycle per data item, so it has to discuss some data.",
            "And because you assume that are independent, it means that.",
            "You can you know nothing about the next data item other than the distribution that comes from the previous one.",
            "It means that this discarding has to be random.",
            "So there is a sub linear learning algorithm is no better than the baseline.",
            "So that tells you the space in which we have to work.",
            "We have to work with.",
            "Algorithms are going to be at least linear in the number of examples.",
            "And the question is, what do we gain from it?",
            "From vertical POV, how much do we pay?"
        ],
        [
            "Joint.",
            "So I'm going to use the standard framework.",
            "So we're going to assume examples as Roman and dependently from unknown probability distribution that represents the rule of nature.",
            "We're going to define the expected risk.",
            "We tested all which is the integral of the loss function, so some measure of the quality.",
            "Of a function applied to X&Y.",
            "So for instance, if you have a classification problem, X is apart, then why is the class F of X?",
            "That thing here is the class that we guess.",
            "And the L function measures the cost of making a mistake or not, and we average that over our distribution.",
            "Because we don't know the distribution, we consider the empirical risk also called training error, which is the average over the training example of that loss function.",
            "We will place the integral by an average.",
            "We would like to find the function.",
            "That minimized the test error over all functions.",
            "But in general, these functions do not belong to the class of functions you considering.",
            "And you have to consider a class of function is quite limited.",
            "The best we can have.",
            "Is the best function in the class reconsidering?",
            "And because the probability is unknown, by definition, the only thing you can do.",
            "Actually not the only thing.",
            "The thing you usually do is that you compute the function that minimizes the training error.",
            "And the VC theory tell us when this can work.",
            "So well.",
            "Maybe it's a lot in one slide, but it's going to hopefully to get simpler."
        ],
        [
            "So computing the minimum of the training error is often costly.",
            "And even if you have a linear system, you need to invert the matrix and again you get this cubic behavior that.",
            "But we already make a lot of approximation.",
            "We minimizing the training error instead of the test error, because we cannot.",
            "And in the test error, we minimizing the tester over certain familiar function becausw, but in fact we would like the best function among all function that does not exist.",
            "So why should we compute this exactly?",
            "This is costly.",
            "What should we compute that thing exactly, given that we already make a lot of approximations?",
            "So let's assume that we have an optimizer that returns some F~ that is within an accuracy roll of the best one.",
            "For instance, we could stop some iterative algorithm before it reaches converge."
        ],
        [
            "And if you decompose the error, the error, the test error of UF~ N minus the best one you could reach as three terms.",
            "The first term is approximation error.",
            "So basically the difference between the error of the best function in all functions.",
            "And the best function within the family or considering.",
            "The second term is the estimation error.",
            "The error of the best function.",
            "The difference between the error of the best function within your set of function minus the one you compute by minimizing the training error.",
            "And the third term, which is a new one, is the optimization error, which is the difference between the error of the.",
            "Best training the function that gives the best training error.",
            "And the testing of the one we actually computed approximate.",
            "And your problem is to choose the family of function.",
            "The number of examples and the accuracy to make the sum of these three terms.",
            "As small as possible, subject to budget constraints, and there are two kinds of budget constraints.",
            "You can have a maximum number of example or maximal computing time.",
            "Mobile.",
            "You know your boss."
        ],
        [
            "So if you look at how you three times move.",
            "The approximation error.",
            "It decreases when the family of function gets larger.",
            "The more you family function is powerful, the model approximation error is not.",
            "The estimation error decreases when N gets larger and increases when F gets larger.",
            "And that's the rapid travel in his theory.",
            "If you play a little tricks on that picture in case theory, you quickly see that the optimization error bounds increases with accuracy.",
            "And the computing time decreases with the accuracy increases in above examples increase with the family of function.",
            "So it's quite a complicated problem to fight."
        ],
        [
            "The best thing.",
            "And you can formally define two kind of problems.",
            "You can define the small school small scale learning problem when the active budget constraints is in above examples, you constrained by the number of examples.",
            "And the large scale learning problem is when the active budget constraint is the computing time.",
            "So that gives a formal definition of the two systems, and you'll see that they have clearly two different regimes."
        ],
        [
            "So let's start by the easy one small scale.",
            "To reduce the estimation error, we can take the number of examples as large as the budget allows.",
            "Because we have plenty of time, we can make the accuracy.",
            "As high as we want.",
            "So the.",
            "The row is 0.",
            "So the only thing we left with is the size of F and we have this typical.",
            "Structural stimulation graph.",
            "Depending on the size of the family or function.",
            "What increases you reduce the approximation error?",
            "You increase the estimation error and somewhere there is an optimal.",
            "This has been studied in literature to death, I would say."
        ],
        [
            "Now when you lost scale case, things are bit different.",
            "The active budget constraint is a computing time.",
            "You have a more complicated tradeoff because the three variables play a role.",
            "For instance, if you choose the accuracy.",
            "Small, you decrease the optimization error.",
            "But you must also decrease the size of the family function and or the number of examples.",
            "With adverse effect on the estimation approximation errors because you need to be able to process it within the time that was given.",
            "And so the exact tradeoff depends on the optimization algorithm.",
            "Meaning that how much time you need to process a certain size of familiar function a certain number of examples in which certain accuracy is going to have an impact on the choices you're going to make.",
            "So what we did now is to connect.",
            "The statistical optimization.",
            "And the computational cost of learning algorithms.",
            "If you look at the large scale problem saying I'm constrained by time.",
            "Things are a lot more complicated than the sample case or that discussed insecurities minimization."
        ],
        [
            "So here is an executive summary.",
            "If you take a mediocre optimization algorithm.",
            "The one that are discussed normally at the beginning of optimization books.",
            "The role decreased like exponential minus T. And in the least log log graph, look time and log roll.",
            "This is this purple curve.",
            "And all optimization books are going to say you should use good optimization algorithm where Rd decreases faster than exponential minus T and these are called super linear algorithms.",
            "And if you want to very high accuracy, it's going to be much faster.",
            "But it happens that the best role, the best accuracy you want for certain learning problem.",
            "Because you make all these other approximations is quite.",
            "Course.",
            "And so a very poor optimization algorithm where Rd decrease is like 1 / T this red line.",
            "Might be better in this regime.",
            "And I'm going to show some examples of this."
        ],
        [
            "So first of all we need to do some some premium work.",
            "Let's speak about the estimation error.",
            "So we have this uniform convergence bounds that basically derived from the VC theory and assume that the capacity that what the thing that was edged in blood in this talk is Now D + 1.",
            "And you have results that tell you that the estimation error is less than some coefficient times D / N N is the number of examples and over D to the power Alpha with Alpha between 1/2 and one.",
            "And in fact, if you look at the details, you have several types of bounds to consider, and there are quite a business into finding Alpha that are better than 1/2 the initial bounds by blood in me at the Alpha equals 1/2 and blooding at some special case where the band was Alpha equal 1, but in some cases you can actually find various kind of Alpha depending on the assumptions you do on the distributions, and it's a big business in the statistical theory."
        ],
        [
            "Right now.",
            "If you add the optimization error using the same kind of arguments, you can insert the accuracy term and you can say that estimation error plus optimization error is less than this term.",
            "And this is true for all three cases of bounds.",
            "In fact, the square root, the 1 / N, and the Alpha bands.",
            "With that modern theorists are making.",
            "And that means that if F is fixed, the family of function.",
            "There is no need to choose role much smaller than this.",
            "Because it would be a waste of time.",
            "And it's not advisable to optimize with an accuracy that's much larger than this becausw then that would be the limit on how well you."
        ],
        [
            "Doing so, if you introduce the other term approximation error in it.",
            "So if the family is a function, is chosen with regularization constant or regularization term where there are some uniform convergence theory results for bounds for simple cases and the computing time you could work it out.",
            "When the family function is really realistically complicated, like for instance you have more features or can use richer models, then the bonds are not really good enough for that.",
            "So it's really difficult.",
            "Luckily, even where the family of functions fixed, we can see quite a lot of interesting."
        ],
        [
            "Things.",
            "I'm going to give an example.",
            "They could fix familiar function that is parameterized, actually linearly parameterized.",
            "I'm going to say by parameter in air to the dimensional space and I'm going to compare 4 iterative optimization algorithms.",
            "The gradient descent.",
            "Newton kind of methods.",
            "The stochastic gradient dissent and Stochastic 2nd order gradient decent, which is a mixture of that."
        ],
        [
            "While there are some quantity of interest, I'm sorry for the math thing.",
            "The empirical station, which is the 2nd derivative of the cost that measures the curvature.",
            "The Fisher Information Matrix, which is basically a measure of the variance of the loss function in that case of optimal.",
            "And we're going to assume that you have this constant.",
            "So basically that the trace of GH minus one is basically some new term.",
            "And that the spectrum of division is between Lambda mean, Lambda Max and the condition number is going to be the ratio of Lambda, Maxwell and any.",
            "This is necessary for."
        ],
        [
            "My derivation.",
            "So let's stick with the.",
            "Let's start with the gradient descent.",
            "You iterate an algorithm and WT becomes.",
            "Basically you remove.",
            "At each iteration you subtract from WT ITA, which is the kind of game times today relatives of the cost.",
            "And you get through this kind of confidence.",
            "You hear the derivatives goes this way and use exactly the optimal.",
            "And this has been studied forever.",
            "The best speed is achieved with a fixed learning rate guitar, which is one over Lambda Max.",
            "The cost of each iteration is proportional to ND, where N is the number of examples data dimension.",
            "The number of iterations to reach a certain accuracy role is going to be condition numbers time log one over all the time to reach a certain accuracy role is going to be the product of both.",
            "And the last column is the most interesting one.",
            "The time to reach the point where you test error rate with an examples minus.",
            "The best one you could get is less than epsilon is something that where you have to take this and basically replace the right value of raw.",
            "So in the last column.",
            "Both the number of examples and accuracy are chosen to reach a certain testable as fast as possible.",
            "That means that if we solved if we say this equal TT Max, the maximum time you have and you solve for epsilon.",
            "You get the best.",
            "Hero, you can achieve in a certain time with this algorithm.",
            "And I would say that that makes some abuse of the big O notation, and I can tell what's hidden in the Deagle here in the bigger you sit on the cost of basically doing basic multiplier.",
            "Here in the bigger you have something that's based on the initial conditions.",
            "Depending whether the good or not, you can lose or constant or not.",
            "That's the product of both and the same thing here is not always going to be the same thing."
        ],
        [
            "Now, if you replace this gain factor so the.",
            "But ITA here.",
            "Bye.",
            "The inverse of the Hessian matrix and I'm going to be very optimistic.",
            "I'm going to assume that we know this inverse of the Asian metrics in advance.",
            "We don't have to compute it.",
            "Someone gave it to us.",
            "You obtain super linear optimization speed.",
            "This is supposed to be the ground of optimization theory.",
            "The cost per iteration is higher because you have to multiply biometrics at this square.",
            "The number of iterations to each row is no longer log of one of our role.",
            "That is log log.",
            "This is not less.",
            "And the time to reach a certain accuracy is a lot less becausw.",
            "This.",
            "Decrease much faster.",
            "But when you replaced by replacing robot, what is supposed to be an end but was supposed to be to get the best error rate, you get something like this and the dominant term is no longer the log of the log log is the one over epsilon to the one over Alpha.",
            "If you."
        ],
        [
            "Compare with previous one is the same dominant term, you just have the constant."
        ],
        [
            "OK, you're going to constant.",
            "Is good to take, but it's not as spectacular as what you would expect from optimization."
        ],
        [
            "Theory.",
            "Now, if you take the stochastic algorithm so this time, at each iteration you draw a random example randomly from your data set, and you make an update by just subtracting to WT it over T so that this decreasing learning rate in fact times derivative of the loss.",
            "For that example only.",
            "So basically, at each point you have some gradients and if you are averaging them you would get the full gradient, but you just pick one of them.",
            "So if you run the math, which are quite complicated, in fact.",
            "The best decreasing and schedule is achieved when ITA is one over Lambda.",
            "Mix one over Lambda mean instead of one over Lambda.",
            "Max is a bit surprising.",
            "Now the cost for each iteration is just order D. The number of examples gone because you just consider one example.",
            "The number of iterations to reach a certain accuracy while the locks are gone.",
            "It's new.",
            "This new factor that we had before times a certain constant that's between one and Kappa Square.",
            "The condition number squared divided by row.",
            "So that's very slow, and if you look there is no big ol anymore.",
            "It's so slow that the condition and the initial conditions don't matter so much.",
            "What matters is that when you close to the optimum, you have all these random noise that comes from taking gradients.",
            "And because you have a factor in front that decrease like one over team is what you see, you decrease like one overall.",
            "No.",
            "This is a synthetic interiors.",
            "This is what I was.",
            "Small oil.",
            "The time to reach a certain accuracy is the product of both.",
            "And the time to reach a certain test error rate.",
            "I just replaced, you know.",
            "And if you look.",
            "The optimizations speak speed.",
            "How fast you optimize that catastrophic.",
            "That's pretty much the worst optimizer you can think of.",
            "But the learning speed is quite OK. First of all, it no longer depends on the statistical estimation right Alpha.",
            "Because what's happening is that if the statistical estimation rate is bad.",
            "Listen, the Alpha is 1/2, but you don't need to optimize optimize so much.",
            "Because you don't need to optimize so much, you have more time because you have more time.",
            "You can process more examples.",
            "So that's in complete contradiction with people who say the statistical estimation right is the dominant factor in the learning speed of algorithm.",
            "In this case it's gone.",
            "And it depends on the condition number, but the scaling is really."
        ],
        [
            "Good, in fact, you know if you compare even with the 2nd order gradient while this lock terms are gone and epsilon is pretty much the most favorable one."
        ],
        [
            "The polynomial time.",
            "That's surprising.",
            "Now, if you replace this heater."
        ],
        [
            "By the inverse of the Hessian again.",
            "Well, you get deleted better you save a constant.",
            "You save the K constant.",
            "That depends on the condition number, but you pay something else that each iteration becomes the square in cross instead of some of the only.",
            "And in fact you."
        ],
        [
            "Say something more is that if you look at this, this D here.",
            "If you have sparse data to start with.",
            "This D is actually the number of non zero terms."
        ],
        [
            "But this this square is the size of H -- 1.",
            "And in fact, is D times the number of non zero terms.",
            "So in fact, when you have sparse data, this might not be so good.",
            "This is interesting for other reasons that I'm going to discuss in the third part of the presentation.",
            "So basically using a single model stochastic gradient also changes only the constants.",
            "So what did we see so far?",
            "That using classical optimization is quite bad.",
            "That second order optimal super linear optimization algorithm are slightly better by some constant.",
            "That stochastic algorithm I better buy some orders of magnitude a little bit.",
            "And that second order stochastic are slightly better than the regular stochastic one, but it's all depends on the ratio between sparsity and condition numbers.",
            "And condition numbers.",
            "I think you can pre condition pretty easily.",
            "I'm using finite dimensional cases, finite dimension.",
            "It's a very simple, very simple setup.",
            "What I did.",
            "Let me try to summarize this part.",
            "I said we make lots of approximation in learning.",
            "What should we optimize?",
            "The training error exactly?",
            "Let's it optimize to some accuracy roll.",
            "By optimizing to some accuracy role now, instead of two terms approximation error estimation error are three terms.",
            "Approximation error estimation error optimization error.",
            "Now and I must select the family of function.",
            "Remember of examples and accuracy to give to get the best test error.",
            "Within the budget.",
            "The budget can be.",
            "Number of examples, maximal computing time, or both.",
            "I can give a formal definition of small scale versus large scale.",
            "Small scale is when the active constraint is the number of examples.",
            "Large scale is when the activity constraint is the maximum computing time.",
            "In the first case you just rent is very simple.",
            "You retrieve the user or statistical risk minimization.",
            "The thing that everybody does in the second case, it's a lot more complicated.",
            "It involves the computational complexity of algorithms.",
            "When you work in the simple case, you observe that, well, it's not the way people say that the statistical estimation rate is not that important.",
            "That super linear optimization is not that important, and that everything is about the balance between them.",
            "And that very bad optimization algorithm like stochastic very decent can be quite good.",
            "That's the scenario of this first part."
        ],
        [
            "So in the second part, I'm going to start to discuss learning with."
        ],
        [
            "Testing gradient distant.",
            "So that theory suggests to Casa Grande St is very competitive.",
            "Now if you ask people, they're going to tell you well, stochastic Grandison is trouble.",
            "It's historically associated with back propagation.",
            "And multiply your network.",
            "They have problem the nonlinear nonconvex.",
            "So what is difficult stochastic gradient descent or multilayer networks?",
            "So simple thing is to try plans stochastic gradient descent on simple learning problems.",
            "Simple, but quite big.",
            "Large support vector machines or large conditional random field.",
            "So before writing that talk ideas, spend one or two weeks writing some programs just to give numbers.",
            "And in fact that was quite striking.",
            "So that is your.",
            "You can even get them."
        ],
        [
            "You want to.",
            "So let's start with a simple benchmark.",
            "The text categorization with SVM.",
            "And this actually there is this paper that in ICM 2007 is quite close.",
            "It's not a pure stochastic Grandison, but it's quite close.",
            "Our take the Reuters, MCV.",
            "One document copies about 800,000 training examples.",
            "47,000 dimensions, so it's not really a small problem.",
            "The idea is to.",
            "The problem is to recognize document of a certain category.",
            "And the error has this regularization term here and the loss function.",
            "I put them together so they can treat them by a single stochastic optimization.",
            "So basically that I'm going to do W becomes W minus Y 30, which is again at 20 times the gradient of that thing.",
            "That's two times the gradient that comes from the regularization term and the gradient of the."
        ],
        [
            "Benchmark.",
            "Linear SVM hinge loss.",
            "So take SVM light.",
            "Typical SVM program.",
            "You go to about 23,000 cygames at 6, six hours, 7 hours.",
            "And you get some.",
            "I measured the cost and tested all the SVM pair, which is an optimization guide for the newer Kims but very optimized for linear problems.",
            "It's actually a very sophisticated algorithm.",
            "He's got something like 66 seconds.",
            "Get the primary cost, but still.",
            "Lucas Aggrandisement 1.4 seconds.",
            "Same result.",
            "Let's that was quite striking.",
            "So I want to take other things like log loss classifier.",
            "So this time the loss is no longer the hinge loss, but this log last term and we stochastic Grandison.",
            "That's easy, you just change directive in order to have anything complicated to do.",
            "And I change your Lambda to something else and I compare with what is considered to be one of the best for this problem, which is something by children leaned out.",
            "Taiwanese friend liblinear?",
            "It's supposed to be a trust region Newton method.",
            "It's sophisticated optimization.",
            "And actually even better than this, in fact.",
            "You get something on the order of 30 seconds depending on the accuracy you get.",
            "Stochastic gradient descent 2.3 seconds.",
            "And again you get the same kind of test errors and what you see?",
            "That's I'm using that.",
            "Depending on the accuracy you set here, you get the primal cost.",
            "That's either slightly worse or slightly better than the stochastic grand distant.",
            "For how long I run it?",
            "But in fact the test errors do not reflect that.",
            "You know, getting a better training error do not give you a better test error.",
            "So that's an illustration of this compromise between accuracy and testable.",
            "The number of tests or steps I do.",
            "You're speaking about stopping Criterion.",
            "All of this.",
            "Yes.",
            "Yeah, that's why I compare the primary cost pretty accurately to see exactly where I'm going.",
            "And even this crazy slow optimization algorithm, stochastic grand distant.",
            "The thing is that the regime in which this problem is working is not exploiting the favorable optimization capabilities, because you don't need to optimize that much.",
            "So they actually in the regime where they don't work so well.",
            "If you were to optimize with an accuracy of 10 to the minus six, that would go a lot faster than stochastic gradient distant, but you have no need to wait that long and you see it here by making it work a little bit harder.",
            "It it it reaches a new role that sometimes smaller by a small increment in time but doesn't do any good.",
            "Yes.",
            "These are exactly the same problems.",
            "All this is the first step in the second tab is exactly the same problem.",
            "We optimize the same function on the same examples.",
            "The state.",
            "While probably I'm not, I don't know for sure, but we really optimize the same function, you know.",
            "We're not computing something that different.",
            "If you were pushing this to the end, stochastic Grandison would be slower, but you get the same weight vector at the end.",
            "It's convex, there is just one optimum.",
            "There is just we are approaching the optimum in some way and.",
            "I would say that the tester or you get so on that particular problem.",
            "I've been going basically to 5% by changing a little bit the parameters and this high person means that you have 5% examples in the test set that are some kind of outliers.",
            "So you could be fundamentally difficult to recognize, right?",
            "Yes.",
            "What?",
            "No, it's not that big a this is my version.",
            "The biggest source of social Schwartz.",
            "There is a stochastic gradient plus the projection step.",
            "They do some things.",
            "In addition, in order to make some aspect easier, and many people do.",
            "The problem is that.",
            "Stochastic gradient is an old stuff.",
            "Nobody can publish a paper about stochastic gradient.",
            "Dissent is not new, so everybody wants to say why stochastic parent is quite good.",
            "They have to insist on the little thing they add.",
            "So what I wanted to do is take plans, stochastic gradient, no tricks, no addition.",
            "I can show you the code.",
            "It's absolutely planned.",
            "OK. OK, I'll tell you what I did I used some cross validation methods in that case and at the end of the third part of the talk I'm going to give you a better method to stop it.",
            "Is going to come, but I need to get an interesting stopping criterion.",
            "I need to go to some extra information.",
            "But in that case I just use cross validation.",
            "Yes.",
            "Just to class problem it says the N. Two class problem with quite big.",
            "800,000 examples 47 thousand dimension.",
            "Accuracy.",
            "I chose this particular category because it's quite balanced.",
            "If it's in balance, you have to do some things, but it's true for SVM, syspro for low gloss.",
            "Whatever things you use, when it's completely imbalanced, you have to do some things they're not different here.",
            "This comparison is interesting because you optimize the same cost you compute basically the same learning system.",
            "With the same regularization parameters.",
            "Secret secret versus the rest?",
            "Is one of the global classes you know.",
            "In the SEV one is LC-1 is not the small, Reuters is the big Reuters and they have a tree of classes and at the top of the trees you have a cecati cut and something else.",
            "Seacat is about 42% of the total.",
            "Do.",
            "So the question is if you try to reduce the accuracy in SVM light.",
            "When I took the default one in that case, but you can see what's going on in the log loss case you know here I played with the does it work with the default accuracy here and I thought that it was still shocked to see that the default accuracy for linear was actually slightly worse than what I was easily obtaining with stochastic gradient distant.",
            "So I tried to use slightly higher accuracy.",
            "So basically the setup cost of this sophisticated algorithm is too high.",
            "At the beginning of the convergence they are slow.",
            "And this one at the beginning of the conference is very fast.",
            "But it's fast when it matters."
        ],
        [
            "So so basically, two weeks ago I put this program from the web and I sent an email to Young and Joshua because there was our families quite fun and the weather spread a little bit and at some point I got an email from Patrick Haffner, another of the band of the French guys.",
            "And he tried on some of the data sets is using within a TNT so different setups, so it probably has a probably gets much better error rates because.",
            "So this is this Reuters set, the same one, but with much better choice of hyperparameters.",
            "And he's using the Liberals VM with a specialized implementation of the of the kernel that that user inverted index to do sparse computation.",
            "That's really quite smart.",
            "Yes, his own program that he called Yama does SVM accent, which is another way is not the same cost, but it's something that people lose a lot in this domain and the stochastic gradient.",
            "Here's the machine translation that I said that's quite big with about 1 million training example and quarter million features.",
            "That's another translation stuff, and this is something for dialogue, so these are real that I base that they use.",
            "And well, you can see that.",
            "The gun is big.",
            "And come on, this is not a new algorithm.",
            "This is something from the 50s.",
            "So what have we been doing?",
            "OK.",
            "There is something important there.",
            "This problem is the text problem.",
            "They're all very sparse.",
            "And that's something that I mentioned.",
            "Whether discussed the asymptotics, stochastic gradient dissent, if you sparse.",
            "The defect or the cost of each iteration gains fully from the sparsity.",
            "If you use Newton mattress is well.",
            "When you compute the Hessian, they're not sparse anymore, so you dealing with mattress of ties.",
            "This square and the disk ways really.",
            "This quarter, 1,000,000 ^2.",
            "So well.",
            "Simplicity's value in that case."
        ],
        [
            "So then I went to let's do another one.",
            "So conditional random fields.",
            "They quite an interesting algorithm in text processing, and the task was text chunking.",
            "So basically is the computational neural linguistic 2000 chunking tasks.",
            "You have sentences and you need to segment them.",
            "Incentives syntactically correlated chunks like funded non phrases, verb phrases you want to segment the sentence.",
            "The training set is about 100,000 training segments from 9000 sentences and we have a testing set that's a bit smaller, just enough to make measurements.",
            "So model is a conditional random field or linear.",
            "Log loss.",
            "The features are just N grams of words and parts of speech tags that were actually given.",
            "And this is quite a big model.",
            "You have 1.6 or 1.7 million parameters.",
            "And there is another stochastic gradient by Vishwanathan and showed off and again well because they cannot publish a paper on simple stochastic gradient.",
            "They have all these SMD adaptation and they show it works.",
            "But let's look at plans stochastic."
        ],
        [
            "Brilliant.",
            "So if you use the limited storage BFG S which is considered to be one of the good ones.",
            "So initially people doing CRF, they use iterative scaling, which is bad.",
            "Then they use Newton method in full be of GS, which is slow and eventually they found unlimited storage VGS which is one of the most sophisticated optimization method is quite good.",
            "And you get something like one hour and a half.",
            "And this does it in 10 minutes.",
            "Now there are some things that are interesting thinking about the bias of the community.",
            "The CRF is something that comes from the hidden Markov model literature, and traditionally one use the forward backward algorithm to compute the gradients.",
            "But by making this experiment so upset that if you use the chain rule, you know, like all backdrop is actually 30% faster than using forward backward.",
            "But when I looked at the CRF codes around and there was a CRF plus plus code which is considered considered to be a good one, but they still use forward backward.",
            "And the second one is, well, I'm actually going to discuss this short slide about graph Transformer network later.",
            "Well.",
            "I'm going to argue later.",
            "That I consider that these are simple problems.",
            "I'm going to argue that later and show what real life real life problem looks like.",
            "But I was even surprised myself by making these experiments, because I knew that stochastic gradient is very one.",
            "But I didn't expect such such a beating.",
            "So I should discuss some details, because there are some little detail."
        ],
        [
            "This is stochastic gradient descent.",
            "The first one is choosing this game schedule.",
            "And in that case I choose something on the scalar divide by T + T zero.",
            "So little piece of theory.",
            "The key factor is 2 times Italian Deming If it is less than one stochastic gradient is excruciatingly slow.",
            "With the speed T -- S. So basically less than 1 / T. If it's greater than one, then you get the T -- 1 speed with this thing and you can see easily that S equal 2 is the right, the right value.",
            "So basically the ether here.",
            "Should be one over the smallest eigenvalue of the Asian.",
            "In that case, we have the regularization term.",
            "So the Lambda of the relaxation term is a good.",
            "Lower bound of London.",
            "So I used it to equal 1 over Lambda.",
            "No.",
            "If you look at this data over T, forget about the T0 for now.",
            "At the beginning, tea is very small.",
            "When T is one.",
            "Since Lambda is quite small in there was 10 to the minus 410 to minus five.",
            "In my experiments you get a very large again here.",
            "And your weight goes all over the place and just goes very far away and you cooked.",
            "What people do usually saying, oh, this is the gain is too high.",
            "Let's reduce Inter as a big mistake, you go into the slow, right?",
            "The proper thing to do is to add a shift below.",
            "And what I did was choosing TO to make sure that the expected initial updates the kind of movement you get at the beginning are comparable to the size of the weights I'm expecting at the end that I can compute by duality theory.",
            "But in the CRF, Kase IUC, to equal 1 over Lambda again.",
            "But this was too complicated to calculate, so I used the secret recipe.",
            "Are you secretly CP?",
            "There is a secret with CP.",
            "If you remember them, the math actually I can show them again.",
            "Then maybe on the previous slides.",
            "If you look at this, you observe that the red doesn't depend on N, the number of training examples.",
            "Spread them using, well, you draw one example at the time.",
            "How many you have total weed?"
        ],
        [
            "OK.",
            "So at any moment during training we can pick a small subsample, a few thousand example, 10,000, maybe 100,000.",
            "If you want to be sure.",
            "And we can save those starting weights.",
            "And try values either on the sub sample and see after one go over the subsample where you get.",
            "And you can pick the game that most reduces the cost.",
            "When you have this game, you use it for the next 100,000 interactions.",
            "Stop again, do it again.",
            "It's very simple.",
            "Algorithm is very simple idea.",
            "So in the CF benchmark I did that to choose the initial T 0, but in fact you can do that much simpler.",
            "You can just well, every so often you make this kind of chip measurements to see what kind of games are going to work well, and because of the independent of nature of the asymptotic speedup stochastic gram distant, you have the same one is going to work on a larger data set."
        ],
        [
            "Another thing that's important is to get the engineering right.",
            "I told you that sparsity is important in that case.",
            "And the update is something like this W -- Y to times Lambda W minus grant of the loss, so you can do in 2 steps.",
            "First, remove the grant of the loss, and that's cheap because the loss involves very few terms, and that's that's a very sparse operation.",
            "The second one applying this term is doing something like this, multiplying the weight by 1 minus beta Lambda.",
            "Less expensive because you need to touch all the coefficients in this huge wave vector with 1 million parameters.",
            "So there are simple solutions, just little engineering trick.",
            "Solution one.",
            "You represent the vector W as the product of a scholar and a vector.",
            "You perform one by changing the vector and two by changing its color.",
            "When this color becomes too small, you make the real thing and you put it back to one.",
            "Well, now it's cheap.",
            "Another solution is to do only step one for each example and perform Step 2 every so often, not all the time just so often.",
            "With the higher again, so basically you increase the gain a little bit foldable to compensate, and you can calculate how to do it pretty easily.",
            "And the two work about the same.",
            "In fact, I reward both programs.",
            "I compare them."
        ],
        [
            "They don't really behave the same.",
            "No, I told you this are toy problems.",
            "Let's look like.",
            "Look at the real one so it's all thing is the system for reading checks.",
            "It was industry diploid in 96 run billion of checks over 10 years and there is an estimate that if you live in the US you have 40% chance that some of your checks have been processed by that particular code.",
            "So that was done in a TNT just before it broke up in my in various plastic pieces.",
            "So what you have the examples are pairs.",
            "The pattern is a complete image of a check the scanner, the label is an amount.",
            "And you have a very strong structure in this problem.",
            "You know that you need to find fields in the fields.",
            "You need to find characters you need to recognize characters.",
            "Then you need to do some syntactical interpretation.",
            "You know these things.",
            "You there is no need to actually do the full things, so you make differentiable modules that do everything.",
            "And you can even switch when each module with some hand label data.",
            "Then you put it together and you define the global cost function.",
            "In that case.",
            "In that case it was actually a CRF, so you have them.",
            "That's the numerator and the denominator in log terms, and that's the ratio.",
            "And you put that in the stochastic gradient descent for a few weeks and you get a working system.",
            "Now there is a huge quantity of engineering there.",
            "It took quite a lot of good people and close to one year to get this thing to run.",
            "But that's probably closer to what real life look like.",
            "Looks like so."
        ],
        [
            "OK.",
            "I'm my voice.",
            "I'm losing my voice.",
            "What do you think of five minutes break.",
            "Yes, I need to drink something.",
            "Yeah, 5 minutes break.",
            "Let's let's come back at three.",
            "Let's say 3 something.",
            "Nick yeah yeah yeah I see."
        ],
        [
            "So let's go again.",
            "The part three is about.",
            "Guess we're not going to see examples again.",
            "So sometimes there's just too much data to store.",
            "We can't afford to store that much data, or sometimes it's stored on some is archived in a way that's too expensive to retrieve.",
            "It seems that when you have a very high volume storage, they're very sequential in nature.",
            "So so when you want to go again over your data, you have some expensive operations to do, like changing tapes or whatever.",
            "This is of course related to the topic of streaming data.",
            "This is related to tracking nonstationarity's and novelty detection, but I'm going to remain in a very simple IDE framework.",
            "So I'm going to bring the topic relatively slowly."
        ],
        [
            "1st I'm going to see what's happening when we add one example to the training set.",
            "So.",
            "Look at this.",
            "When you have an example.",
            "The optimum on the training set is FN is the argument of the training error.",
            "When you add another example, you want the argument on N + 1 examples.",
            "Which is the same as the argument of N + 1 / N of 20 or 1 + 1 examples that you can write this way.",
            "Admin of the error on an example plus a small term.",
            "So that's a small perturbation.",
            "And so you have something very close.",
            "So you can do very simple things like first order or second order analysis to try to estimate how FN plus one is related to."
        ],
        [
            "And when you do it just the first of our calculation.",
            "You get that FN plus one is F N -- 1 / N inverse of the empirical Asian on N + 1 example.",
            "Times the directive of the loss for the new examples plus some similar terms.",
            "If you compare this expression with second order stochastic gradient descent.",
            "Well, someone's the same.",
            "You know you don't have the high order terms.",
            "And the Hessian, while you have the real 1 instead of the empirical one, in practice you wouldn't have the real one would have some estimate.",
            "So could these two empirical processes converge with the same speed?",
            "And what would it mean?"
        ],
        [
            "And you can make a theorem, and in fact I did that with you in 2003.",
            "But then I found that Murata in Amery had done something similar in 98.",
            "And the adequate conditions.",
            "End times the distance between F Infinity, which is the limit way where you would go with an infinite number of examples minus F N ^2.",
            "Is equal to the limit 20 goes to Infinity of W Infinity minus double T * T equals some constant you can compute.",
            "OK. Looks like a nice result, but what's interesting is to try to understand what it means.",
            "So let's start from W0F Zero and we can make one path of signaled a stochastic gradient descent.",
            "So we do this stochastic signals to Casiguran distant just once in every example, and you reach some WN.",
            "We can also compute the empirical optimum on an example here.",
            "And what this says is that the average distance between WN and the optimum, the best test error.",
            "And the average distance between the empirical optimum and the best one there about the same.",
            "The old decreased like some constant over N. So this is as good as this.",
            "In terms of training error, this is the optimum.",
            "This has better training error.",
            "But in terms of testing or are close, you are to the best sterile.",
            "This is the same."
        ],
        [
            "So let me handle that.",
            "Given the large new string large enough training set.",
            "A single path of 2nd order stochastic gradient generalize as well as the empirical timem.",
            "That's a very shocking result, I think.",
            "And so we can make some experiments on tentative data, so that's the 19 dimensional purely synthetic data.",
            "This is the number of examples.",
            "This is mean squared error plus some time explaining loss in these quadraplex .1 zero, .01 and so on.",
            "And I don't know which one is the blue and which one is the red.",
            "Yes, the red is the empirical optimum, the blue is the signature stochastic gradient.",
            "But there are some.",
            "That's a pure mean squared error or least square problem.",
            "I think it's one sigmoid loss.",
            "WX is very simple stuff and artificial later.",
            "If you look in terms of computing time.",
            "Well, the signal stochastic gradient descent is a bit faster to compute the empirical optimum I use the Newton method.",
            "So the Newton method, if you think about it, it's really much looks like doing 2nd order stochastic gradient slightly 2nd order gradient again and again and again.",
            "So if you do it well, you can be quite good, but the signal signal stochastic will be faster."
        ],
        [
            "There are some unfortunate practical issues.",
            "The first one is that signaled a stochastic written descent is not that fast.",
            "Compared to stochastic gradient distance, it's just against the constant and loses a constant.",
            "You get a constant which is related to the condition number, and you lose a constant becausw each iteration cause disquiet instead of D and it's actually worse when you have sparse data.",
            "So you must estimate install D by D matrix H -- 1.",
            "You must do a multiply the gradient for each example by that matrix.",
            "And the sparsity tricks do not work so well, because H -- 1 typically is not sparse.",
            "So there are some workarounds.",
            "There are faster ways to compute H -- 1, like using good boys formula Online.",
            "DFCS is quite an interesting way.",
            "You can make limited storage approximation of edge minus one, like diagonal approximations, which is pretty simple.",
            "Low rank approximations, limited storage, VGS like approximations.",
            "But I must say it's not completely satisfactory.",
            "So for a long time I wondered.",
            "Is it possible to do that faster?",
            "At this point I'm going to make a slide.",
            "This question and answer the question a question from last time about stop."
        ],
        [
            "Quiteria let's summarize.",
            "Time to reach a certain accuracy roll.",
            "On the 20 year old.",
            "2nd order stochastic gradient is new overall plus some.",
            "Negligible terminal, one of the role.",
            "Plans to cost a gram distances can you overrule?",
            "Plus this all of them.",
            "If he does remember iteration so it's not time.",
            "Is number of iterations to reach a certain accuracy.",
            "Number of epochs passes over the string data to reach the same test error as the full optimization.",
            "2nd order stochastic gradient one that was what I just discussed.",
            "Here you see.",
            "Is Kate, I'm small, so stochastic gradient descent is K this constant that's between one and Kappa Square, where Kappa is the Commission member.",
            "It's problem dependent.",
            "So there are many ways to make this content smaller.",
            "Like you do, could do an exact signatures to Kathy Griffin distant.",
            "You can approximate.",
            "For one, you can use preconditioning tricks, normalizing the inputs or a lot of these tricks that you heard about multilayer networks they have to do with making case smaller."
        ],
        [
            "But that gives you a stopping criteria.",
            "Good day young sorry.",
            "What I used in the experiment before was early stopping with cross validation.",
            "You create a validation set by setting some training examples apart.",
            "You monitor the cost function of validation set and you stop when it stops decreasing.",
            "But in fact you could do it a priori.",
            "You extract 2 disjoint subsamples of training data.",
            "You train on the 1st substantial you stop by acting on the 2nd.",
            "The number of epochs you have to do is an estimate of K. So now you know, OK?",
            "So while you trend by performing.",
            "KA pox on the full set and you stop your finished you there.",
            "This is asymptotically correct, so you need to have sets that are large enough, of course, and in practice, so you're in the right ballpark.",
            "So if you want to be safe with it 10 times more iterations are still going to be faster than using.",
            "Classical algorithm non stochastic algorithms.",
            "So, so I was awfully fast on my third part, so I'm going to."
        ],
        [
            "Summarize again."
        ],
        [
            "So I do it again because it's something that I think is very important and I'm sorry to do it that way, but I can go fast."
        ],
        [
            "So if I add one example to the cost, I make a small perturbation of the error."
        ],
        [
            "And I can make a first order calculation and I found something that's very close to signal stochastic gradient descent."
        ],
        [
            "I can run the math and see they converted the same speed and what it means."
        ],
        [
            "Is that by making a single path of 2nd order stochastic gradient?",
            "I generalize as well as the empirical optimum.",
            "This can be verified in practice."
        ],
        [
            "There are some nasty problems that second order stochastic gradient descent is painful.",
            "You have this big matrix to store.",
            "Well is not more painful than the classical programs, but compared to plain stochastic gradient, dissent is a bit painful.",
            "You have this big matrix to store.",
            "You lose opportunity to use the sparsity.",
            "There are some partial complicated work arounds.",
            "I wouldn't say this is finished."
        ],
        [
            "But what you get out of this is, and it's stopping criterion for stochastic Grandison 'cause you know that stochastic William Distantes Kate Times slower than signatures to custom gradient decent.",
            "With case some problem dependent constant that's between one and the condition number squared."
        ],
        [
            "And basically you can measure K on subsamples and you know how many times you have to go over your training data in the plans to Cassa Grande.",
            "That was my football."
        ],
        [
            "My 4th spot.",
            "I'm a bit annoyed about that one because I'm less satisfied with this problem with the previous three and I'm going to explain why at which moment I think I took it the wrong way, but the results I have this way so maybe."
        ],
        [
            "And we'd better another time.",
            "So let's go back to this incremental training we add.",
            "Refresh the training example.",
            "The current training set.",
            "So we take an example that we haven't seen yet.",
            "We added to the training set and we reach 1 until reaching sufficient accuracy.",
            "So if we use a loss function with flat segments, something like the hinge loss so.",
            "Although losses like something like a sigmoid is close to that.",
            "You observe two things.",
            "But very few examples code change when you add them to the set.",
            "A lot of them they're going to be in the flat part of the last zero gradient.",
            "They changed nothing.",
            "And when you do the retraining.",
            "Very few example, of course change during the training.",
            "So to return, it's a bit annoying you would have to remember all the examples, but in fact most of them they do nothing.",
            "Can you select the one you actually need to store in memory?",
            "And can you still pick the one you're going to add to your accumulated experience?"
        ],
        [
            "So let me make a tool of the methods that people have been using to select examples.",
            "The first category is things that are coming from the nearest neighbor literature and there is this old book by Deviren Kichler that's very interesting about the topic that the multi edit comments methods.",
            "And you can adapt it to other classifier, but the cost is greater than the square of the number of examples.",
            "Basically, the typical contents method is you pick one example and you look how well this example is recognized by making nearest neighbor with the others or basically by building a classifier with the other examples and you decide whether to discard it or not.",
            "So if it's correctly recognized you say it's redundant.",
            "So I remove it if it's not correctly recognize.",
            "You keep it.",
            "But that's in square in building such such a thing.",
            "You can try to use gradient methods.",
            "This is what I should have been using in fact, but you can select examples when the loss as a sizable gradient.",
            "So that if you have some gradient, now is likely to have gradients in the future.",
            "If the examples, no gradients meanings correctly recognize or is completely off.",
            "Maybe you don't need it.",
            "Now, many examples can carry the same information, so when you store examples for future retraining.",
            "Um?",
            "You can't be one example.",
            "The correction that one example makes could be redundant with that.",
            "That other example makes.",
            "Mick and if you run the thing that leads to organization schemes in the scale, like the square, the number of examples and that was used, for instance, it's optimal experiment design the the federal booking 71 about optimal experiment design.",
            "That's a big part of statistics is entirely about this kind of organization schemes.",
            "Now an approach that I thought was interesting and actually it's interesting because it illustrates a certain points, but I'm not sure what we should really use in practice.",
            "Is to use duality?",
            "So if you take convex common machine, let's support vector machines, there is an expression where the weight parameter is expressed as a linear combination of some of the features associated with some examples.",
            "And this combination is sparse.",
            "In fact, if you think about support vectors.",
            "The set of support vectors you can define it as the set of example that would be sufficient to find the same classification boundary if you have only the support vector, you're going to find the same boundary.",
            "Now the problem is that this involves the bulky kernel matrix, which has all again or of order N squared values.",
            "But the thing that gives some hope is that sparsity makes most of the kernel matrix values irrelevant.",
            "So you don't need to have them all.",
            "So let's see how we can use duality."
        ],
        [
            "So large scale support vector machines?",
            "That's a contradiction in terms you know.",
            "Support vector machines.",
            "They scale like the square of the number of examples I told you at the beginning.",
            "I want to scale like the volume of the data I'm completely off.",
            "But it's also instructive, because if you can get support vector machines to go faster, you probably have understood something.",
            "And I'm going to go through 2 steps.",
            "The first one is to define stochastic and incremental support vector machines.",
            "That's going to work by iteratively constructing the support vector expansion.",
            "And that's going to give an answer about which candidate support vectors to store and discard and how to manage the memory required to store this kernel values.",
            "And the second part is use active learning and support vector machines.",
            "Choose which example to process next and discover that convexity is no longer our friend."
        ],
        [
            "So let's run in the dual, so forget about this for now.",
            "That's the primal problem.",
            "You have one class, the other class you want to find the separation with the largest margin.",
            "So things about two plates with a Big Spring in between.",
            "That's it.",
            "Now the dual problem is well now you take the two convex hulls.",
            "The convex Hull of blue and the convex Hull of the right ones.",
            "And you take a moving point that can move anywhere within the convex Hull.",
            "So at this point they can move anywhere within the blue convex Hull.",
            "This point B can move anywhere between the right convex Hull you put a spring on elastic in between and you find the point where the closest the minimum distance between the health.",
            "And if you take.",
            "The middle.",
            "You get the optimal hyperplane.",
            "So that's a duality.",
            "So if you see what's been written in the literature, you observe that the memory it requires in the best case is something like number of examples, times, number of support vectors.",
            "The time is it requires this end to sum power between one and two times, number of support vectors.",
            "And there are some results that show that in the best case then multiple vectors are syntactically is 12 twice the base zero times the number of Super Bowl number of examples.",
            "So basically we have something N square and worse than square here.",
            "Now, there is evidence that the number of support vectors could be much smaller, like Chris Burges at this reduced supervector machines.",
            "And the Pascal Benson Ushua banjo made the pursuit schemes that can show that you can get good accuracy with much less support vectors.",
            "And the question is how to do it fast?",
            "And how small this can be?"
        ],
        [
            "Or small discount.",
            "So let's start with very inefficiently optimizer.",
            "So I have my blue point that's constrained to stay within the blue color, the blue hole.",
            "And the red point constraint to say in the red hole.",
            "I pick a red example here and I consider that segment between the current red point and the new one.",
            "I project the blue point in that segment and that's my new red point.",
            "Then I pick one example here at the same thing here and here and here and here.",
            "At each step, the distance between the Blue Square in the Red Square decreases.",
            "That's an optimizer.",
            "Now both B&N the linear combination of examples with positive coefficients them into one is what it means to say that the constraint to stay in the Hull.",
            "The projection is quite a simple thing to do.",
            "It's a linear operation.",
            "And the projection time is proportional to the number of support vectors.",
            "But that's still awfully slow for."
        ],
        [
            "Reasons.",
            "The first reason is that suppose it doesn't eliminate unwanted support vectors very easily.",
            "Suppose you have a pattern X that already has some non zero coefficient.",
            "Well.",
            "If you project on the segment between the right point and the new point, and you select this pattern X, what's going to happen is that you say I don't want more of this pattern.",
            "So we're going to state a gamma equals 0.",
            "But not going to remove anything.",
            "The only way to remove to reduce the Alpha from this pattern would be to choose other patterns."
        ],
        [
            "And let them the.",
            "What's going on here?",
            "And let this thing the multiplication by one month."
        ],
        [
            "Eroded, that's very slow.",
            "So there is a simple solution is to let gamma be slightly negative.",
            "You can expand the segment a little bit beyond and show that you remain in the convex Hull that condition and you can allow it to be negative to the point where you actually make Alpha, exactly 0.",
            "That's a very simple solution.",
            "Now the second problem is a few more fundamental is that we don't process super vectors often enough.",
            "The basic idea for support vector machines when it's interesting, is when you have few support vectors.",
            "Now you're going to spend your time considering new points that have no vocation to become super vectors, and decide that you don't need to add them to your recipe to your formula.",
            "Well, in fact the one that are super vectors for which you would need to adjust the alphas, we're going to see just so often."
        ],
        [
            "So there was a very primitive algorithm that we called the color as 2 steps.",
            "First we pick a random fresh example and we do the projection.",
            "So again, we pick a random support vector and we do the projection.",
            "Buffalo football.",
            "You could compare this with the idea of incremental learning, and we're training that at the beginning this is adding one example.",
            "This is returning, but to the simplest level.",
            "Instead of iterating over the example we already had, we just pick one random support vectors one and support vector and we project just one.",
            "This first operation, picking a fresh example and projecting, potentially can add the support vector.",
            "This operation pick a random support vector and projecting potentially can remove one.",
            "So we have the adding operation and the removing operation.",
            "And we have some derivatives of this, so that's been done by Antoine Board, another between the last two or three."
        ],
        [
            "Heels what's interesting is this.",
            "So.",
            "Now I should explain the graph.",
            "This is the honest handwritten digits again.",
            "And each of the group of her bath is recognizing one class versus the right.",
            "So this is plus eight versus the rest.",
            "The yellow thing here.",
            "Is what you get with the guys VN.",
            "And the important thing of the blue thing, and the purple thing the blue thing is running one of these algorithm.",
            "One go over the data.",
            "The purple thing is to go over the letter.",
            "This is the error and what you observe is that one goal over the data 2 passes over the data or full SVM.",
            "You get the same kind of errors.",
            "My daughter wants the just perceptrons for comparison, we can ignore that for now.",
            "But if you look at the training time.",
            "Well, it's much faster you go about once over the data and once about the data by using this simple stupid algorithm where you pick a new example, you make an update.",
            "You pick a random supervector you make an update, you pick a new example in Macon.",
            "Well is much faster and performs as well in one go over the data.",
            "Now, this story of retraining, but you don't need to between that much, you don't return that much work."
        ],
        [
            "Would be correct.",
            "So if you look at how much memory you need.",
            "With that algorithm, the time is still like a plan as VM.",
            "The memory is not a simple vector squared.",
            "So if you think about it, this is where.",
            "The limitation of this kind of algorithm is showing.",
            "Number of super vector squared.",
            "Admitted if you really reduce number of super vectors to the minimum you want.",
            "You still going to have a number of support vectors that's on the order of the effective dimension of the parameter.",
            "So you get a memory that still on the order of this square, similar to SIGMOD or stochastic gradient.",
            "I'm not going to get any better.",
            "That's where it's actually annoying.",
            "No.",
            "If you look at this, these are comparisons of time using memory.",
            "The memory in the Super vector machines are cash.",
            "You can adjust it, and it's going to use more or less memory.",
            "Using you have less memory, you have to make more computation to make it point, so this is from 1 gigabyte to 2 megabytes.",
            "The yellow things are using the Lib SVM program with values accuracy and the blue thing is using one of these online processes.",
            "And what you see is that, well, we can work with a lot less memory than regular SVM thing.",
            "But still it's not going to be enough to approach the stochastic gradient descent."
        ],
        [
            "No, not that I have explained or relatively reasonable method for selecting which examples to keep to do the retraining thing.",
            "My next question is to find how to select the next example.",
            "I'm going to add in my incremental retraining process.",
            "And when I'm doing this process step where I pick a fresh example, I can pick a fresh example of randomly.",
            "I can speak it according to the gradient.",
            "Let's speak the example that has the strongest coefficient in the greater the dual, the one that has increased the deal with the higher slope.",
            "And that means selecting the example.",
            "In my training exam, that's the most incorrect.",
            "I think the most incorrect example and I tend to machine.",
            "That's just that one.",
            "And I can do something a little bit more refined, because if I select the most incorrect example, I'm going to select outliers.",
            "I'm not sure I want to learn on just outliers.",
            "So we can use the minimax approach and select the example that has the biggest gradient regardless of the class I'm assuming.",
            "And that ends up to selecting the exam that closes to the decision boundary."
        ],
        [
            "So let's see how this works.",
            "First of all, you have, sorry I'm."
        ],
        [
            "No.",
            "To select the strongest gradient or the closest to the decision boundary you would think I have to go over all my examples and pick the one."
        ],
        [
            "You don't need to do that, you can just do a sampling.",
            "If you sample, let's say, 59 random examples, you have 95% chance to find something that among the 5% best ones.",
            "So that's good enough.",
            "Regardless of the size of your full training set, you can sample fixed amounts of examples.",
            "And actually be very close to picking the best one, and you can make some heuristic to adapt M as to be slow, small at the beginning, largest at D and while there are some refinements."
        ],
        [
            "So let's take this small example.",
            "This is the UCI adult data set, finding which people have an income greater than $50,000.",
            "There are 32,000 examples.",
            "This Gray line is the accuracy of the regular SVM.",
            "With this particular hyperparameters.",
            "And this red thing is how you progress when you pick examples randomly.",
            "Now this green thing is how you progress when you pick 15.",
            "An example pick the closest to the boundary.",
            "And learning that one and do that again and again and again.",
            "And you see, even though you you turn a lot of examples at the beginning without even doing anything with them, just to 11 every 59 examples, it's quite fast and you get a slightly better error.",
            "And this is actually significant.",
            "And if you make this simple you Ristic to adjust this sampling rate, you can actually go a lot faster, so you reach the SVM accuracy lot faster than this, which is already faster than SVM.",
            "So it's a bit bizarre.",
            "We have a situation where picking examples and not picking the you don't try to pick the example that the most incorrect for the machine you pick.",
            "Examples are closed.",
            "The most ambiguous for the machine, and you give that for training and on this noisy data set.",
            "This is working extra."
        ],
        [
            "Anyway.",
            "If you look here number of support vectors versus test error.",
            "The SVM is here as quite the high number of support vectors because all the outliers become support vectors.",
            "You know SVM outliers have negative margin and they become support vectors.",
            "If you pick example randomly, you go to the SVM point, you find the SVM solution after after basically one path.",
            "But if you pick with this active setups where you pick close to the boundary, well, you go to solution that actually better than SVM and this parser.",
            "And if you keep training eventually you're going to pick up these outliers.",
            "And so you go something that better and then slowly you go back to the SVM result.",
            "So basically we found the trajectory to compute the SVM result.",
            "And some transitory steps actually much better.",
            "This parcel and SVM.",
            "And they are the devil.",
            "Slightly better error rate, even though it's a small effect.",
            "So if you think about it.",
            "This is the fact because this curves this active curves by selecting close to the boundary are not going to select outliers.",
            "Only at the end when there are just outliers left in the data.",
            "And in fact, you can replicate this result.",
            "You can try to define the loss that takes you directly here.",
            "This is what color Bear was done in order did in 2006 that defined the loss, which is not a hinge loss.",
            "But the sigmoid.",
            "So basically outliers have no gradient.",
            "When they do far, and by optimizing that loss they can make an algorithm that fast.",
            "And that basically stops here.",
            "So what do you think about convexity?",
            "Convexity is nice, you have good guarantees about the optimum and everything, but you pay for it.",
            "You pay by bless Pacitti and a little bit more error.",
            "It's bizzare people gas, often shocked by this, but this is true.",
            "Yes."
        ],
        [
            "OK. Let's go back.",
            "Suppose.",
            "On suppose you select by the gradient.",
            "So basically think about an incremental system.",
            "You take one example, one fresh example.",
            "You add it and you optimize on that.",
            "And you take a new example.",
            "You add it and you optimize on all the examples you have.",
            "The question is which example to add?",
            "If you choose the example that has the strongest coefficient in the gradient, it also means in that case, if you do the math, choosing the example for which the output of the system is the most incorrect.",
            "Now, if you're unlucky enough that you data are some outliers.",
            "Well, for outliers, the output of the system would be very incorrect because the the actual level you have in your training set is well.",
            "So you're going to pick this one, so you're going to build a system by adding outliers, and you need to train your system by first showing the outliers.",
            "This is not good pedagogy.",
            "So a way to alleviate this is to do this minimax argument, you say.",
            "I assume when I see an example that I don't know it's class and I say I would like an example that provides a good gradient, even if it's in the worst possible class.",
            "And that leads you to select the example that closes to the decision boundary.",
            "So in this setup, instead of teaching your system by adding new examples that are very incorrect, you teach your system by adding new examples that are ambiguous.",
            "So if you have a pool of examples, fixed pool with some outliers from other examples and everything.",
            "And you first add the ambiguous one.",
            "You're going to leave the outliers because they're not close to the boundary.",
            "They completely fell off."
        ],
        [
            "And this is what's happening here.",
            "Here you start training by taking example the ambiguous, leaving the outliers away."
        ],
        [
            "And when you keep dreaming when you keep going.",
            "Eventually, you in the examples you haven't seen yet, you just have the outliers, so you're going to pick outliers, and when you do that, you go back to the SVM solution.",
            "Close to the boundary.",
            "Close to the decision boundary that the system currently implements."
        ],
        [
            "So.",
            "Sorry.",
            "So we try to do we try to do that on a large scale problem.",
            "So there was with Guile loosely and take handwritten digits again, so it's a good database, but this time we do apply on the fly distortions, so we're going to generate about 8.1 million digits by applying distortion to the training set of analysts.",
            "Are we going to use the RBF kernels and there are some argument this is very difficult problem for local kernels.",
            "So this is really a tough problem to solve.",
            "This is not this is the wrong model for this problem basically.",
            "Potentially you're going to have many support vectors and I would say that this kind of study is more challenging a solution because there is a better solution.",
            "But basically we have 10 binary classifiers, one class versus the rest.",
            "We use about 60 gigabytes of memory for this.",
            "Each classifiers are 8 million examples, so we've been sending 18,000,000 examples in the same SVM.",
            "The training time is about 8 days and the result is 0 zero point, 6% error on the test error which is.",
            "Quite please in the good ones for this kind of problem.",
            "So I think this is probably the largest as VM training on a single processor that was on a single processor.",
            "No parallel system and basically 80,000,000 examples propels them to the thing.",
            "And each example gets only one chance to be selected in securely one pass system.",
            "When we not happy, but one example is not close enough to the boundary, we just keep it and we never come back."
        ],
        [
            "Is it good enough?",
            "Well no.",
            "If you take one of these old convolution networks, we just took a cigarette and distant.",
            "This one was done by Patrice Simard in 2003 with about four million examples.",
            "It trends in two or three hours, two or three hours, and you get a better tester.",
            "So you're not completely there yet.",
            "I must say for the defense of the other systems that the other system we use by using RBF kernels and we know they do not like invariances.",
            "So we know it's the wrong model.",
            "And they need a lot of memory to cache channel values.",
            "Yet I said it's more challenging the solution, and the challenge was met that mentor by using techniques to select examples for training, we were able to train SVM of absolutely absurd scale."
        ],
        [
            "So let's summarize.",
            "Suppose we do that in large scale.",
            "Suppose you have plenty of computers.",
            "They product data.",
            "And we could have a filtering module that's going to select some examples.",
            "Send it to another computer that's going to do more filtering.",
            "Select some example and then to the training machine.",
            "And there is a slow feedback loop that's going to send back the models with the low frequency to all these machines to do better filtering.",
            "If you do this.",
            "Well, all these guys are filtering example close to the decision boundary at a certain time.",
            "Not so long ago, but it's going to work.",
            "It's a parallel implementation of, let's say a very, very large VM.",
            "It can work on the scale or Google scale.",
            "It said you can have 1000 machines there.",
            "That's going to select one.",
            "Thousands of the examples to go for the actual training.",
            "And you don't have this complicated feedback of usual parallel algorithm.",
            "Typically what kills parallel algorithms that you have a lot of feedback between the machines that are involved in the computation.",
            "In that case, the feedback is very loose.",
            "It's very very light.",
            "And this actually has."
        ],
        [
            "I sent a petition.",
            "These are the makers.",
            "These are the thinkers.",
            "What we did was push.",
            "Some of the thinking.",
            "Into the onto the makers.",
            "Now the makers do part of the thinking.",
            "And that way we can solve our initial problems with the makers and the thinkers.",
            "Under that's my conclusion.",
            "Questions.",
            "Yes.",
            "The non convex and that was very very short slide.",
            "There actually a lot more data on this but.",
            "What the fuck do not even finding the real global minima is convex?",
            "When is all these programs for doing convex optimization likely based VMS VM Lite and so on?",
            "The default tuning and tuning people use their with an accuracy that so calls that you find crap in terms of optimization.",
            "So well, if I would choose between an algorithm that's going to find something that's close.",
            "Close to a global minimum and something that's going to find the local minimum, but that's going to perform better.",
            "Which one do you take?",
            "It's a practical question.",
            "I know theoretically is annoying.",
            "And it makes the math a lot more difficult.",
            "But the fact is that if in the same time you can find a local minimum in the nonconvex system, that's actually better than the global minimum.",
            "You could process in the same time, well, you should go with the nonconvex one.",
            "Well.",
            "I'd say that's a limitation of my analysis.",
            "Because in order to do math, I'm very happy to have a single optimum.",
            "I can compare with.",
            "But in practice is not necessary like that, and that's a striking example.",
            "Well, The thing is that I know it doesn't mean it just means that all theoretical tools are not appropriate.",
            "There might be ways to approach this theoretically, but we don't know how to do it.",
            "Think about it in machine learning with the beginning.",
            "No trivial things.",
            "We don't know how to do.",
            "Like I when I was giving blood in his talk, I gave the example of.",
            "Students are always slightly worse than teachers.",
            "Well.",
            "Look at all the machine learning things we always have.",
            "Learning systems that are slightly worse than the thing you feed.",
            "Give for filling them than the teacher teaching signal the supervised supervision signal.",
            "Yeah, unless it really is wrong sometimes, but.",
            "Yeah, what you say is right, but do we have a clean theoretical framework to address this in the general level?",
            "Not that much.",
            "I'm not aware of that much.",
            "You have some, well, good.",
            "Yeah, you can do it some some extra some level.",
            "But the basic.",
            "Things do the basic thing that we see.",
            "Theory has nothing about it.",
            "There's a scandal.",
            "When you want to learn something, you go into a library.",
            "Now all our theories about independent examples.",
            "So you pick an independent book randomly.",
            "You read it.",
            "Do I know what I want?",
            "Not yet.",
            "So you pick a random book, you read it.",
            "Do I know what I want?",
            "Not yet.",
            "It's not the way you do.",
            "You start by looking at the networks of books.",
            "You see how they relate to each other.",
            "You have help from librarians, and you learn much faster that way.",
            "What do we know if this almost nothing?",
            "There are some beginnings, no.",
            "So people have been thinking about that.",
            "I don't see that people have not been thinking about this, but do we have a very constructive theory of this?",
            "No.",
            "So.",
            "OK, so we try to do a little piece.",
            "In that case, the little piece was trying to connect the computational cost and the statistical efficiency and get some some results, but that's only a little piece.",
            "I'm sorry and just I easily get.",
            "This.",
            "Not as I know.",
            "Maybe has some things here in the back of his mind, but he didn't tell me.",
            "No.",
            "Not in that case.",
            "Maybe there is a connection, but I'm not aware of it.",
            "I don't know.",
            "We used this VM because it was convenient.",
            "We have the code at hand, but you know, in the end it was just an additional term in the cost function.",
            "I guess you could modify anything to have this.",
            "Not that I know, but it's not impossible.",
            "I don't know everything.",
            "I know that I don't know everything people have done.",
            "It's not a complicated thing to implement in the end.",
            "It's just the idea that very bizzare to use.",
            "OK, well, we do need another category of example, let's dream them.",
            "Well that's it.",
            "Thank you very much and."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Should I start?",
                    "label": 0
                },
                {
                    "sent": "OK, so welcome to the Digest if session.",
                    "label": 0
                },
                {
                    "sent": "Oh so the talk is about learning with large datasets and.",
                    "label": 1
                },
                {
                    "sent": "The initial observation is that over the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "The disk size has increased by about a factor of 1000.",
                    "label": 0
                },
                {
                    "sent": "And the CPU speed by a factor of 100.",
                    "label": 0
                },
                {
                    "sent": "That means that we have less time for each data item.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a sort of trouble and I'm going to try to work out with other consequences of this and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do.",
                    "label": 0
                },
                {
                    "sent": "So why should we use large scale data sets to start with?",
                    "label": 0
                },
                {
                    "sent": "And there are basically 2.",
                    "label": 0
                },
                {
                    "sent": "Broad applications for this.",
                    "label": 0
                },
                {
                    "sent": "So the first one is data mining and has to do with money.",
                    "label": 0
                },
                {
                    "sent": "The idea is that by analyzing the data that describes the life of our society.",
                    "label": 1
                },
                {
                    "sent": "We can gain competitive advantages and make more money.",
                    "label": 0
                },
                {
                    "sent": "The other one is about artificial intelligence, and the idea is that if we want to emulate the cognitive abilities of humans, we have to do the same thing they do, which is process huge amounts of data.",
                    "label": 0
                },
                {
                    "sent": "Everything we see, everything we hear, everything we feel.",
                    "label": 0
                },
                {
                    "sent": "And it does that sometimes very complicated, like language.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's go back to the data mining thing and start with a little metaphor of a computerized society.",
                    "label": 0
                },
                {
                    "sent": "And suppose you just have two kinds of computers.",
                    "label": 1
                },
                {
                    "sent": "We have the makers, they do business, they make revenue, they make money.",
                    "label": 1
                },
                {
                    "sent": "And they also produce data that's in proportion with the activity.",
                    "label": 1
                },
                {
                    "sent": "And we have the thinkers and they analyze the data and they increase the revenue by finding ways to get competitive advantages.",
                    "label": 0
                },
                {
                    "sent": "And of course, when the population grows, the proportion of thinkers cannot grow to one.",
                    "label": 1
                },
                {
                    "sent": "It just doesn't seem to work that way, but the data grows like the number of makers.",
                    "label": 1
                },
                {
                    "sent": "That means that the number of thinkers doesn't grow faster than data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "The computing resources available for learning do not grow faster than the volume of data.",
                    "label": 1
                },
                {
                    "sent": "And the cost of determining cannot exceed the revenue.",
                    "label": 1
                },
                {
                    "sent": "No way.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the artificial intelligence problem, intelligent animals learn from streaming data.",
                    "label": 0
                },
                {
                    "sent": "The amount of processing power they have or scales with the time they can put in it and scales with the volume of the data.",
                    "label": 1
                },
                {
                    "sent": "But yet most machine learning algo even demand resource that grow faster than the volume of data.",
                    "label": 0
                },
                {
                    "sent": "Like if you do anything with mattress, is that meaningful?",
                    "label": 0
                },
                {
                    "sent": "Is going to be Ncube while the size of the matrix is N ^2.",
                    "label": 0
                },
                {
                    "sent": "And if you have sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "Sparse matrix is actually worse.",
                    "label": 0
                },
                {
                    "sent": "So in fact we have a problem ahead.",
                    "label": 0
                },
                {
                    "sent": "So even if we want to use a simple tool like linear algebra, we're not going to be able to do it and hope this is going to work for a long time.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the talk has four parts I'm reasonably happy with the 1st three and on the 4th one.",
                    "label": 0
                },
                {
                    "sent": "I'm a little bit less happy, but there are still interesting things I believe.",
                    "label": 0
                },
                {
                    "sent": "So the first one is trying to understand the relation between statistical efficiency and computational costs.",
                    "label": 1
                },
                {
                    "sent": "On the on, the conceptual at the conceptual level.",
                    "label": 0
                },
                {
                    "sent": "The second one is a consequence of the first one is discussed stochastic algorithm.",
                    "label": 0
                },
                {
                    "sent": "The third one is to know how well we can do if we see each data item only once.",
                    "label": 0
                },
                {
                    "sent": "And the last one is that, well, you know when you want to learn something, you don't go in the library and read random books until you know what you want, you choose.",
                    "label": 0
                },
                {
                    "sent": "We should choose our examples and what can we do in that direct?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I go with the first part.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say much about data quality issues, which are important when you have large datasets.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to say much about data mining specifics.",
                    "label": 1
                },
                {
                    "sent": "I'll go even storage and I think this has been done before.",
                    "label": 0
                },
                {
                    "sent": "And I will be very fast about implementation.",
                    "label": 0
                },
                {
                    "sent": "They'll give some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pointers.",
                    "label": 0
                },
                {
                    "sent": "So the first part.",
                    "label": 0
                },
                {
                    "sent": "Status is called efficiency versus computational costs.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back here.",
                    "label": 0
                },
                {
                    "sent": "So a quick summary.",
                    "label": 0
                },
                {
                    "sent": "If you listen to people who just look at the statistical side, they're going to tell you.",
                    "label": 0
                },
                {
                    "sent": "Well, you have to find an algorithm or start a principle that is consistent with high convergence speed, making that you make a good statistical estimation with as few example as possible, and the rest is second order.",
                    "label": 0
                },
                {
                    "sent": "If you listen to someone in the optimization domain, they're going to tell you, or you need to use super linear algorithms, otherwise you're doomed.",
                    "label": 0
                },
                {
                    "sent": "Happens they're both wrong.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Explain why.",
                    "label": 0
                },
                {
                    "sent": "First of all, we have to consider baseline large scale algorithm.",
                    "label": 0
                },
                {
                    "sent": "The simplest thing you can do when you have lots of data is to discard data randomly.",
                    "label": 0
                },
                {
                    "sent": "You just choose a subset and work with that.",
                    "label": 0
                },
                {
                    "sent": "And that that works.",
                    "label": 0
                },
                {
                    "sent": "It's not so good, but it works.",
                    "label": 0
                },
                {
                    "sent": "And the questions are, what are the statistical benefits of processing more data than that?",
                    "label": 1
                },
                {
                    "sent": "And what is the computational cost of processing more data?",
                    "label": 1
                },
                {
                    "sent": "And which one wins?",
                    "label": 0
                },
                {
                    "sent": "And what is the balance?",
                    "label": 0
                },
                {
                    "sent": "In the way there is an interesting remark, as sometimes you see papers that promise to make a learning algorithm was cost doesn't grow faster than the volume of data or the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So so called sub linear learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "And the thing that's important to understand is that if you have independent data, this.",
                    "label": 0
                },
                {
                    "sent": "Is no better than the baseline algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here is why.",
                    "label": 0
                },
                {
                    "sent": "So if you have a sub linear algorithm, it means that it doesn't even spend the cycle per data item, so it has to discuss some data.",
                    "label": 0
                },
                {
                    "sent": "And because you assume that are independent, it means that.",
                    "label": 0
                },
                {
                    "sent": "You can you know nothing about the next data item other than the distribution that comes from the previous one.",
                    "label": 0
                },
                {
                    "sent": "It means that this discarding has to be random.",
                    "label": 0
                },
                {
                    "sent": "So there is a sub linear learning algorithm is no better than the baseline.",
                    "label": 0
                },
                {
                    "sent": "So that tells you the space in which we have to work.",
                    "label": 0
                },
                {
                    "sent": "We have to work with.",
                    "label": 0
                },
                {
                    "sent": "Algorithms are going to be at least linear in the number of examples.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what do we gain from it?",
                    "label": 0
                },
                {
                    "sent": "From vertical POV, how much do we pay?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use the standard framework.",
                    "label": 0
                },
                {
                    "sent": "So we're going to assume examples as Roman and dependently from unknown probability distribution that represents the rule of nature.",
                    "label": 1
                },
                {
                    "sent": "We're going to define the expected risk.",
                    "label": 0
                },
                {
                    "sent": "We tested all which is the integral of the loss function, so some measure of the quality.",
                    "label": 0
                },
                {
                    "sent": "Of a function applied to X&Y.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have a classification problem, X is apart, then why is the class F of X?",
                    "label": 0
                },
                {
                    "sent": "That thing here is the class that we guess.",
                    "label": 0
                },
                {
                    "sent": "And the L function measures the cost of making a mistake or not, and we average that over our distribution.",
                    "label": 0
                },
                {
                    "sent": "Because we don't know the distribution, we consider the empirical risk also called training error, which is the average over the training example of that loss function.",
                    "label": 0
                },
                {
                    "sent": "We will place the integral by an average.",
                    "label": 1
                },
                {
                    "sent": "We would like to find the function.",
                    "label": 0
                },
                {
                    "sent": "That minimized the test error over all functions.",
                    "label": 0
                },
                {
                    "sent": "But in general, these functions do not belong to the class of functions you considering.",
                    "label": 0
                },
                {
                    "sent": "And you have to consider a class of function is quite limited.",
                    "label": 1
                },
                {
                    "sent": "The best we can have.",
                    "label": 1
                },
                {
                    "sent": "Is the best function in the class reconsidering?",
                    "label": 0
                },
                {
                    "sent": "And because the probability is unknown, by definition, the only thing you can do.",
                    "label": 0
                },
                {
                    "sent": "Actually not the only thing.",
                    "label": 1
                },
                {
                    "sent": "The thing you usually do is that you compute the function that minimizes the training error.",
                    "label": 0
                },
                {
                    "sent": "And the VC theory tell us when this can work.",
                    "label": 0
                },
                {
                    "sent": "So well.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a lot in one slide, but it's going to hopefully to get simpler.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So computing the minimum of the training error is often costly.",
                    "label": 1
                },
                {
                    "sent": "And even if you have a linear system, you need to invert the matrix and again you get this cubic behavior that.",
                    "label": 0
                },
                {
                    "sent": "But we already make a lot of approximation.",
                    "label": 0
                },
                {
                    "sent": "We minimizing the training error instead of the test error, because we cannot.",
                    "label": 0
                },
                {
                    "sent": "And in the test error, we minimizing the tester over certain familiar function becausw, but in fact we would like the best function among all function that does not exist.",
                    "label": 0
                },
                {
                    "sent": "So why should we compute this exactly?",
                    "label": 0
                },
                {
                    "sent": "This is costly.",
                    "label": 0
                },
                {
                    "sent": "What should we compute that thing exactly, given that we already make a lot of approximations?",
                    "label": 1
                },
                {
                    "sent": "So let's assume that we have an optimizer that returns some F~ that is within an accuracy roll of the best one.",
                    "label": 0
                },
                {
                    "sent": "For instance, we could stop some iterative algorithm before it reaches converge.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you decompose the error, the error, the test error of UF~ N minus the best one you could reach as three terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is approximation error.",
                    "label": 0
                },
                {
                    "sent": "So basically the difference between the error of the best function in all functions.",
                    "label": 0
                },
                {
                    "sent": "And the best function within the family or considering.",
                    "label": 0
                },
                {
                    "sent": "The second term is the estimation error.",
                    "label": 0
                },
                {
                    "sent": "The error of the best function.",
                    "label": 1
                },
                {
                    "sent": "The difference between the error of the best function within your set of function minus the one you compute by minimizing the training error.",
                    "label": 0
                },
                {
                    "sent": "And the third term, which is a new one, is the optimization error, which is the difference between the error of the.",
                    "label": 0
                },
                {
                    "sent": "Best training the function that gives the best training error.",
                    "label": 0
                },
                {
                    "sent": "And the testing of the one we actually computed approximate.",
                    "label": 0
                },
                {
                    "sent": "And your problem is to choose the family of function.",
                    "label": 0
                },
                {
                    "sent": "The number of examples and the accuracy to make the sum of these three terms.",
                    "label": 0
                },
                {
                    "sent": "As small as possible, subject to budget constraints, and there are two kinds of budget constraints.",
                    "label": 1
                },
                {
                    "sent": "You can have a maximum number of example or maximal computing time.",
                    "label": 0
                },
                {
                    "sent": "Mobile.",
                    "label": 0
                },
                {
                    "sent": "You know your boss.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you look at how you three times move.",
                    "label": 0
                },
                {
                    "sent": "The approximation error.",
                    "label": 0
                },
                {
                    "sent": "It decreases when the family of function gets larger.",
                    "label": 0
                },
                {
                    "sent": "The more you family function is powerful, the model approximation error is not.",
                    "label": 0
                },
                {
                    "sent": "The estimation error decreases when N gets larger and increases when F gets larger.",
                    "label": 1
                },
                {
                    "sent": "And that's the rapid travel in his theory.",
                    "label": 0
                },
                {
                    "sent": "If you play a little tricks on that picture in case theory, you quickly see that the optimization error bounds increases with accuracy.",
                    "label": 0
                },
                {
                    "sent": "And the computing time decreases with the accuracy increases in above examples increase with the family of function.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a complicated problem to fight.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The best thing.",
                    "label": 0
                },
                {
                    "sent": "And you can formally define two kind of problems.",
                    "label": 0
                },
                {
                    "sent": "You can define the small school small scale learning problem when the active budget constraints is in above examples, you constrained by the number of examples.",
                    "label": 1
                },
                {
                    "sent": "And the large scale learning problem is when the active budget constraint is the computing time.",
                    "label": 1
                },
                {
                    "sent": "So that gives a formal definition of the two systems, and you'll see that they have clearly two different regimes.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start by the easy one small scale.",
                    "label": 0
                },
                {
                    "sent": "To reduce the estimation error, we can take the number of examples as large as the budget allows.",
                    "label": 1
                },
                {
                    "sent": "Because we have plenty of time, we can make the accuracy.",
                    "label": 0
                },
                {
                    "sent": "As high as we want.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The row is 0.",
                    "label": 1
                },
                {
                    "sent": "So the only thing we left with is the size of F and we have this typical.",
                    "label": 0
                },
                {
                    "sent": "Structural stimulation graph.",
                    "label": 0
                },
                {
                    "sent": "Depending on the size of the family or function.",
                    "label": 0
                },
                {
                    "sent": "What increases you reduce the approximation error?",
                    "label": 0
                },
                {
                    "sent": "You increase the estimation error and somewhere there is an optimal.",
                    "label": 0
                },
                {
                    "sent": "This has been studied in literature to death, I would say.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now when you lost scale case, things are bit different.",
                    "label": 0
                },
                {
                    "sent": "The active budget constraint is a computing time.",
                    "label": 1
                },
                {
                    "sent": "You have a more complicated tradeoff because the three variables play a role.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you choose the accuracy.",
                    "label": 1
                },
                {
                    "sent": "Small, you decrease the optimization error.",
                    "label": 0
                },
                {
                    "sent": "But you must also decrease the size of the family function and or the number of examples.",
                    "label": 0
                },
                {
                    "sent": "With adverse effect on the estimation approximation errors because you need to be able to process it within the time that was given.",
                    "label": 0
                },
                {
                    "sent": "And so the exact tradeoff depends on the optimization algorithm.",
                    "label": 1
                },
                {
                    "sent": "Meaning that how much time you need to process a certain size of familiar function a certain number of examples in which certain accuracy is going to have an impact on the choices you're going to make.",
                    "label": 0
                },
                {
                    "sent": "So what we did now is to connect.",
                    "label": 0
                },
                {
                    "sent": "The statistical optimization.",
                    "label": 0
                },
                {
                    "sent": "And the computational cost of learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "If you look at the large scale problem saying I'm constrained by time.",
                    "label": 0
                },
                {
                    "sent": "Things are a lot more complicated than the sample case or that discussed insecurities minimization.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an executive summary.",
                    "label": 0
                },
                {
                    "sent": "If you take a mediocre optimization algorithm.",
                    "label": 1
                },
                {
                    "sent": "The one that are discussed normally at the beginning of optimization books.",
                    "label": 0
                },
                {
                    "sent": "The role decreased like exponential minus T. And in the least log log graph, look time and log roll.",
                    "label": 0
                },
                {
                    "sent": "This is this purple curve.",
                    "label": 0
                },
                {
                    "sent": "And all optimization books are going to say you should use good optimization algorithm where Rd decreases faster than exponential minus T and these are called super linear algorithms.",
                    "label": 1
                },
                {
                    "sent": "And if you want to very high accuracy, it's going to be much faster.",
                    "label": 0
                },
                {
                    "sent": "But it happens that the best role, the best accuracy you want for certain learning problem.",
                    "label": 0
                },
                {
                    "sent": "Because you make all these other approximations is quite.",
                    "label": 1
                },
                {
                    "sent": "Course.",
                    "label": 0
                },
                {
                    "sent": "And so a very poor optimization algorithm where Rd decrease is like 1 / T this red line.",
                    "label": 0
                },
                {
                    "sent": "Might be better in this regime.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to show some examples of this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all we need to do some some premium work.",
                    "label": 0
                },
                {
                    "sent": "Let's speak about the estimation error.",
                    "label": 0
                },
                {
                    "sent": "So we have this uniform convergence bounds that basically derived from the VC theory and assume that the capacity that what the thing that was edged in blood in this talk is Now D + 1.",
                    "label": 1
                },
                {
                    "sent": "And you have results that tell you that the estimation error is less than some coefficient times D / N N is the number of examples and over D to the power Alpha with Alpha between 1/2 and one.",
                    "label": 0
                },
                {
                    "sent": "And in fact, if you look at the details, you have several types of bounds to consider, and there are quite a business into finding Alpha that are better than 1/2 the initial bounds by blood in me at the Alpha equals 1/2 and blooding at some special case where the band was Alpha equal 1, but in some cases you can actually find various kind of Alpha depending on the assumptions you do on the distributions, and it's a big business in the statistical theory.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "If you add the optimization error using the same kind of arguments, you can insert the accuracy term and you can say that estimation error plus optimization error is less than this term.",
                    "label": 0
                },
                {
                    "sent": "And this is true for all three cases of bounds.",
                    "label": 1
                },
                {
                    "sent": "In fact, the square root, the 1 / N, and the Alpha bands.",
                    "label": 0
                },
                {
                    "sent": "With that modern theorists are making.",
                    "label": 1
                },
                {
                    "sent": "And that means that if F is fixed, the family of function.",
                    "label": 1
                },
                {
                    "sent": "There is no need to choose role much smaller than this.",
                    "label": 0
                },
                {
                    "sent": "Because it would be a waste of time.",
                    "label": 0
                },
                {
                    "sent": "And it's not advisable to optimize with an accuracy that's much larger than this becausw then that would be the limit on how well you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing so, if you introduce the other term approximation error in it.",
                    "label": 0
                },
                {
                    "sent": "So if the family is a function, is chosen with regularization constant or regularization term where there are some uniform convergence theory results for bounds for simple cases and the computing time you could work it out.",
                    "label": 1
                },
                {
                    "sent": "When the family function is really realistically complicated, like for instance you have more features or can use richer models, then the bonds are not really good enough for that.",
                    "label": 0
                },
                {
                    "sent": "So it's really difficult.",
                    "label": 0
                },
                {
                    "sent": "Luckily, even where the family of functions fixed, we can see quite a lot of interesting.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give an example.",
                    "label": 0
                },
                {
                    "sent": "They could fix familiar function that is parameterized, actually linearly parameterized.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say by parameter in air to the dimensional space and I'm going to compare 4 iterative optimization algorithms.",
                    "label": 1
                },
                {
                    "sent": "The gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Newton kind of methods.",
                    "label": 0
                },
                {
                    "sent": "The stochastic gradient dissent and Stochastic 2nd order gradient decent, which is a mixture of that.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "While there are some quantity of interest, I'm sorry for the math thing.",
                    "label": 1
                },
                {
                    "sent": "The empirical station, which is the 2nd derivative of the cost that measures the curvature.",
                    "label": 1
                },
                {
                    "sent": "The Fisher Information Matrix, which is basically a measure of the variance of the loss function in that case of optimal.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume that you have this constant.",
                    "label": 0
                },
                {
                    "sent": "So basically that the trace of GH minus one is basically some new term.",
                    "label": 0
                },
                {
                    "sent": "And that the spectrum of division is between Lambda mean, Lambda Max and the condition number is going to be the ratio of Lambda, Maxwell and any.",
                    "label": 0
                },
                {
                    "sent": "This is necessary for.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My derivation.",
                    "label": 0
                },
                {
                    "sent": "So let's stick with the.",
                    "label": 0
                },
                {
                    "sent": "Let's start with the gradient descent.",
                    "label": 1
                },
                {
                    "sent": "You iterate an algorithm and WT becomes.",
                    "label": 0
                },
                {
                    "sent": "Basically you remove.",
                    "label": 0
                },
                {
                    "sent": "At each iteration you subtract from WT ITA, which is the kind of game times today relatives of the cost.",
                    "label": 0
                },
                {
                    "sent": "And you get through this kind of confidence.",
                    "label": 0
                },
                {
                    "sent": "You hear the derivatives goes this way and use exactly the optimal.",
                    "label": 0
                },
                {
                    "sent": "And this has been studied forever.",
                    "label": 0
                },
                {
                    "sent": "The best speed is achieved with a fixed learning rate guitar, which is one over Lambda Max.",
                    "label": 1
                },
                {
                    "sent": "The cost of each iteration is proportional to ND, where N is the number of examples data dimension.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations to reach a certain accuracy role is going to be condition numbers time log one over all the time to reach a certain accuracy role is going to be the product of both.",
                    "label": 0
                },
                {
                    "sent": "And the last column is the most interesting one.",
                    "label": 1
                },
                {
                    "sent": "The time to reach the point where you test error rate with an examples minus.",
                    "label": 0
                },
                {
                    "sent": "The best one you could get is less than epsilon is something that where you have to take this and basically replace the right value of raw.",
                    "label": 1
                },
                {
                    "sent": "So in the last column.",
                    "label": 0
                },
                {
                    "sent": "Both the number of examples and accuracy are chosen to reach a certain testable as fast as possible.",
                    "label": 1
                },
                {
                    "sent": "That means that if we solved if we say this equal TT Max, the maximum time you have and you solve for epsilon.",
                    "label": 0
                },
                {
                    "sent": "You get the best.",
                    "label": 0
                },
                {
                    "sent": "Hero, you can achieve in a certain time with this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I would say that that makes some abuse of the big O notation, and I can tell what's hidden in the Deagle here in the bigger you sit on the cost of basically doing basic multiplier.",
                    "label": 0
                },
                {
                    "sent": "Here in the bigger you have something that's based on the initial conditions.",
                    "label": 0
                },
                {
                    "sent": "Depending whether the good or not, you can lose or constant or not.",
                    "label": 0
                },
                {
                    "sent": "That's the product of both and the same thing here is not always going to be the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if you replace this gain factor so the.",
                    "label": 0
                },
                {
                    "sent": "But ITA here.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "The inverse of the Hessian matrix and I'm going to be very optimistic.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume that we know this inverse of the Asian metrics in advance.",
                    "label": 1
                },
                {
                    "sent": "We don't have to compute it.",
                    "label": 0
                },
                {
                    "sent": "Someone gave it to us.",
                    "label": 0
                },
                {
                    "sent": "You obtain super linear optimization speed.",
                    "label": 1
                },
                {
                    "sent": "This is supposed to be the ground of optimization theory.",
                    "label": 0
                },
                {
                    "sent": "The cost per iteration is higher because you have to multiply biometrics at this square.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations to each row is no longer log of one of our role.",
                    "label": 0
                },
                {
                    "sent": "That is log log.",
                    "label": 0
                },
                {
                    "sent": "This is not less.",
                    "label": 0
                },
                {
                    "sent": "And the time to reach a certain accuracy is a lot less becausw.",
                    "label": 1
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Decrease much faster.",
                    "label": 0
                },
                {
                    "sent": "But when you replaced by replacing robot, what is supposed to be an end but was supposed to be to get the best error rate, you get something like this and the dominant term is no longer the log of the log log is the one over epsilon to the one over Alpha.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare with previous one is the same dominant term, you just have the constant.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, you're going to constant.",
                    "label": 0
                },
                {
                    "sent": "Is good to take, but it's not as spectacular as what you would expect from optimization.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theory.",
                    "label": 0
                },
                {
                    "sent": "Now, if you take the stochastic algorithm so this time, at each iteration you draw a random example randomly from your data set, and you make an update by just subtracting to WT it over T so that this decreasing learning rate in fact times derivative of the loss.",
                    "label": 0
                },
                {
                    "sent": "For that example only.",
                    "label": 0
                },
                {
                    "sent": "So basically, at each point you have some gradients and if you are averaging them you would get the full gradient, but you just pick one of them.",
                    "label": 0
                },
                {
                    "sent": "So if you run the math, which are quite complicated, in fact.",
                    "label": 0
                },
                {
                    "sent": "The best decreasing and schedule is achieved when ITA is one over Lambda.",
                    "label": 1
                },
                {
                    "sent": "Mix one over Lambda mean instead of one over Lambda.",
                    "label": 0
                },
                {
                    "sent": "Max is a bit surprising.",
                    "label": 0
                },
                {
                    "sent": "Now the cost for each iteration is just order D. The number of examples gone because you just consider one example.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations to reach a certain accuracy while the locks are gone.",
                    "label": 0
                },
                {
                    "sent": "It's new.",
                    "label": 0
                },
                {
                    "sent": "This new factor that we had before times a certain constant that's between one and Kappa Square.",
                    "label": 0
                },
                {
                    "sent": "The condition number squared divided by row.",
                    "label": 1
                },
                {
                    "sent": "So that's very slow, and if you look there is no big ol anymore.",
                    "label": 0
                },
                {
                    "sent": "It's so slow that the condition and the initial conditions don't matter so much.",
                    "label": 0
                },
                {
                    "sent": "What matters is that when you close to the optimum, you have all these random noise that comes from taking gradients.",
                    "label": 0
                },
                {
                    "sent": "And because you have a factor in front that decrease like one over team is what you see, you decrease like one overall.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "This is a synthetic interiors.",
                    "label": 0
                },
                {
                    "sent": "This is what I was.",
                    "label": 0
                },
                {
                    "sent": "Small oil.",
                    "label": 0
                },
                {
                    "sent": "The time to reach a certain accuracy is the product of both.",
                    "label": 1
                },
                {
                    "sent": "And the time to reach a certain test error rate.",
                    "label": 1
                },
                {
                    "sent": "I just replaced, you know.",
                    "label": 0
                },
                {
                    "sent": "And if you look.",
                    "label": 0
                },
                {
                    "sent": "The optimizations speak speed.",
                    "label": 1
                },
                {
                    "sent": "How fast you optimize that catastrophic.",
                    "label": 0
                },
                {
                    "sent": "That's pretty much the worst optimizer you can think of.",
                    "label": 0
                },
                {
                    "sent": "But the learning speed is quite OK. First of all, it no longer depends on the statistical estimation right Alpha.",
                    "label": 1
                },
                {
                    "sent": "Because what's happening is that if the statistical estimation rate is bad.",
                    "label": 0
                },
                {
                    "sent": "Listen, the Alpha is 1/2, but you don't need to optimize optimize so much.",
                    "label": 0
                },
                {
                    "sent": "Because you don't need to optimize so much, you have more time because you have more time.",
                    "label": 0
                },
                {
                    "sent": "You can process more examples.",
                    "label": 0
                },
                {
                    "sent": "So that's in complete contradiction with people who say the statistical estimation right is the dominant factor in the learning speed of algorithm.",
                    "label": 0
                },
                {
                    "sent": "In this case it's gone.",
                    "label": 0
                },
                {
                    "sent": "And it depends on the condition number, but the scaling is really.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, in fact, you know if you compare even with the 2nd order gradient while this lock terms are gone and epsilon is pretty much the most favorable one.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The polynomial time.",
                    "label": 0
                },
                {
                    "sent": "That's surprising.",
                    "label": 0
                },
                {
                    "sent": "Now, if you replace this heater.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the inverse of the Hessian again.",
                    "label": 0
                },
                {
                    "sent": "Well, you get deleted better you save a constant.",
                    "label": 0
                },
                {
                    "sent": "You save the K constant.",
                    "label": 0
                },
                {
                    "sent": "That depends on the condition number, but you pay something else that each iteration becomes the square in cross instead of some of the only.",
                    "label": 0
                },
                {
                    "sent": "And in fact you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say something more is that if you look at this, this D here.",
                    "label": 0
                },
                {
                    "sent": "If you have sparse data to start with.",
                    "label": 0
                },
                {
                    "sent": "This D is actually the number of non zero terms.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this this square is the size of H -- 1.",
                    "label": 0
                },
                {
                    "sent": "And in fact, is D times the number of non zero terms.",
                    "label": 1
                },
                {
                    "sent": "So in fact, when you have sparse data, this might not be so good.",
                    "label": 0
                },
                {
                    "sent": "This is interesting for other reasons that I'm going to discuss in the third part of the presentation.",
                    "label": 0
                },
                {
                    "sent": "So basically using a single model stochastic gradient also changes only the constants.",
                    "label": 0
                },
                {
                    "sent": "So what did we see so far?",
                    "label": 0
                },
                {
                    "sent": "That using classical optimization is quite bad.",
                    "label": 0
                },
                {
                    "sent": "That second order optimal super linear optimization algorithm are slightly better by some constant.",
                    "label": 0
                },
                {
                    "sent": "That stochastic algorithm I better buy some orders of magnitude a little bit.",
                    "label": 0
                },
                {
                    "sent": "And that second order stochastic are slightly better than the regular stochastic one, but it's all depends on the ratio between sparsity and condition numbers.",
                    "label": 0
                },
                {
                    "sent": "And condition numbers.",
                    "label": 0
                },
                {
                    "sent": "I think you can pre condition pretty easily.",
                    "label": 0
                },
                {
                    "sent": "I'm using finite dimensional cases, finite dimension.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple, very simple setup.",
                    "label": 0
                },
                {
                    "sent": "What I did.",
                    "label": 0
                },
                {
                    "sent": "Let me try to summarize this part.",
                    "label": 0
                },
                {
                    "sent": "I said we make lots of approximation in learning.",
                    "label": 0
                },
                {
                    "sent": "What should we optimize?",
                    "label": 0
                },
                {
                    "sent": "The training error exactly?",
                    "label": 0
                },
                {
                    "sent": "Let's it optimize to some accuracy roll.",
                    "label": 0
                },
                {
                    "sent": "By optimizing to some accuracy role now, instead of two terms approximation error estimation error are three terms.",
                    "label": 0
                },
                {
                    "sent": "Approximation error estimation error optimization error.",
                    "label": 0
                },
                {
                    "sent": "Now and I must select the family of function.",
                    "label": 0
                },
                {
                    "sent": "Remember of examples and accuracy to give to get the best test error.",
                    "label": 0
                },
                {
                    "sent": "Within the budget.",
                    "label": 0
                },
                {
                    "sent": "The budget can be.",
                    "label": 0
                },
                {
                    "sent": "Number of examples, maximal computing time, or both.",
                    "label": 0
                },
                {
                    "sent": "I can give a formal definition of small scale versus large scale.",
                    "label": 0
                },
                {
                    "sent": "Small scale is when the active constraint is the number of examples.",
                    "label": 0
                },
                {
                    "sent": "Large scale is when the activity constraint is the maximum computing time.",
                    "label": 0
                },
                {
                    "sent": "In the first case you just rent is very simple.",
                    "label": 0
                },
                {
                    "sent": "You retrieve the user or statistical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "The thing that everybody does in the second case, it's a lot more complicated.",
                    "label": 0
                },
                {
                    "sent": "It involves the computational complexity of algorithms.",
                    "label": 0
                },
                {
                    "sent": "When you work in the simple case, you observe that, well, it's not the way people say that the statistical estimation rate is not that important.",
                    "label": 0
                },
                {
                    "sent": "That super linear optimization is not that important, and that everything is about the balance between them.",
                    "label": 0
                },
                {
                    "sent": "And that very bad optimization algorithm like stochastic very decent can be quite good.",
                    "label": 0
                },
                {
                    "sent": "That's the scenario of this first part.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the second part, I'm going to start to discuss learning with.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Testing gradient distant.",
                    "label": 0
                },
                {
                    "sent": "So that theory suggests to Casa Grande St is very competitive.",
                    "label": 1
                },
                {
                    "sent": "Now if you ask people, they're going to tell you well, stochastic Grandison is trouble.",
                    "label": 1
                },
                {
                    "sent": "It's historically associated with back propagation.",
                    "label": 1
                },
                {
                    "sent": "And multiply your network.",
                    "label": 0
                },
                {
                    "sent": "They have problem the nonlinear nonconvex.",
                    "label": 1
                },
                {
                    "sent": "So what is difficult stochastic gradient descent or multilayer networks?",
                    "label": 1
                },
                {
                    "sent": "So simple thing is to try plans stochastic gradient descent on simple learning problems.",
                    "label": 0
                },
                {
                    "sent": "Simple, but quite big.",
                    "label": 0
                },
                {
                    "sent": "Large support vector machines or large conditional random field.",
                    "label": 1
                },
                {
                    "sent": "So before writing that talk ideas, spend one or two weeks writing some programs just to give numbers.",
                    "label": 0
                },
                {
                    "sent": "And in fact that was quite striking.",
                    "label": 0
                },
                {
                    "sent": "So that is your.",
                    "label": 0
                },
                {
                    "sent": "You can even get them.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to.",
                    "label": 0
                },
                {
                    "sent": "So let's start with a simple benchmark.",
                    "label": 0
                },
                {
                    "sent": "The text categorization with SVM.",
                    "label": 1
                },
                {
                    "sent": "And this actually there is this paper that in ICM 2007 is quite close.",
                    "label": 0
                },
                {
                    "sent": "It's not a pure stochastic Grandison, but it's quite close.",
                    "label": 0
                },
                {
                    "sent": "Our take the Reuters, MCV.",
                    "label": 1
                },
                {
                    "sent": "One document copies about 800,000 training examples.",
                    "label": 0
                },
                {
                    "sent": "47,000 dimensions, so it's not really a small problem.",
                    "label": 0
                },
                {
                    "sent": "The idea is to.",
                    "label": 0
                },
                {
                    "sent": "The problem is to recognize document of a certain category.",
                    "label": 0
                },
                {
                    "sent": "And the error has this regularization term here and the loss function.",
                    "label": 0
                },
                {
                    "sent": "I put them together so they can treat them by a single stochastic optimization.",
                    "label": 0
                },
                {
                    "sent": "So basically that I'm going to do W becomes W minus Y 30, which is again at 20 times the gradient of that thing.",
                    "label": 0
                },
                {
                    "sent": "That's two times the gradient that comes from the regularization term and the gradient of the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Benchmark.",
                    "label": 0
                },
                {
                    "sent": "Linear SVM hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So take SVM light.",
                    "label": 0
                },
                {
                    "sent": "Typical SVM program.",
                    "label": 0
                },
                {
                    "sent": "You go to about 23,000 cygames at 6, six hours, 7 hours.",
                    "label": 0
                },
                {
                    "sent": "And you get some.",
                    "label": 0
                },
                {
                    "sent": "I measured the cost and tested all the SVM pair, which is an optimization guide for the newer Kims but very optimized for linear problems.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very sophisticated algorithm.",
                    "label": 0
                },
                {
                    "sent": "He's got something like 66 seconds.",
                    "label": 0
                },
                {
                    "sent": "Get the primary cost, but still.",
                    "label": 0
                },
                {
                    "sent": "Lucas Aggrandisement 1.4 seconds.",
                    "label": 0
                },
                {
                    "sent": "Same result.",
                    "label": 0
                },
                {
                    "sent": "Let's that was quite striking.",
                    "label": 0
                },
                {
                    "sent": "So I want to take other things like log loss classifier.",
                    "label": 0
                },
                {
                    "sent": "So this time the loss is no longer the hinge loss, but this log last term and we stochastic Grandison.",
                    "label": 0
                },
                {
                    "sent": "That's easy, you just change directive in order to have anything complicated to do.",
                    "label": 0
                },
                {
                    "sent": "And I change your Lambda to something else and I compare with what is considered to be one of the best for this problem, which is something by children leaned out.",
                    "label": 0
                },
                {
                    "sent": "Taiwanese friend liblinear?",
                    "label": 0
                },
                {
                    "sent": "It's supposed to be a trust region Newton method.",
                    "label": 0
                },
                {
                    "sent": "It's sophisticated optimization.",
                    "label": 0
                },
                {
                    "sent": "And actually even better than this, in fact.",
                    "label": 0
                },
                {
                    "sent": "You get something on the order of 30 seconds depending on the accuracy you get.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient descent 2.3 seconds.",
                    "label": 0
                },
                {
                    "sent": "And again you get the same kind of test errors and what you see?",
                    "label": 0
                },
                {
                    "sent": "That's I'm using that.",
                    "label": 0
                },
                {
                    "sent": "Depending on the accuracy you set here, you get the primal cost.",
                    "label": 0
                },
                {
                    "sent": "That's either slightly worse or slightly better than the stochastic grand distant.",
                    "label": 0
                },
                {
                    "sent": "For how long I run it?",
                    "label": 0
                },
                {
                    "sent": "But in fact the test errors do not reflect that.",
                    "label": 0
                },
                {
                    "sent": "You know, getting a better training error do not give you a better test error.",
                    "label": 0
                },
                {
                    "sent": "So that's an illustration of this compromise between accuracy and testable.",
                    "label": 0
                },
                {
                    "sent": "The number of tests or steps I do.",
                    "label": 0
                },
                {
                    "sent": "You're speaking about stopping Criterion.",
                    "label": 0
                },
                {
                    "sent": "All of this.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's why I compare the primary cost pretty accurately to see exactly where I'm going.",
                    "label": 0
                },
                {
                    "sent": "And even this crazy slow optimization algorithm, stochastic grand distant.",
                    "label": 0
                },
                {
                    "sent": "The thing is that the regime in which this problem is working is not exploiting the favorable optimization capabilities, because you don't need to optimize that much.",
                    "label": 0
                },
                {
                    "sent": "So they actually in the regime where they don't work so well.",
                    "label": 0
                },
                {
                    "sent": "If you were to optimize with an accuracy of 10 to the minus six, that would go a lot faster than stochastic gradient distant, but you have no need to wait that long and you see it here by making it work a little bit harder.",
                    "label": 0
                },
                {
                    "sent": "It it it reaches a new role that sometimes smaller by a small increment in time but doesn't do any good.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "These are exactly the same problems.",
                    "label": 0
                },
                {
                    "sent": "All this is the first step in the second tab is exactly the same problem.",
                    "label": 0
                },
                {
                    "sent": "We optimize the same function on the same examples.",
                    "label": 0
                },
                {
                    "sent": "The state.",
                    "label": 0
                },
                {
                    "sent": "While probably I'm not, I don't know for sure, but we really optimize the same function, you know.",
                    "label": 0
                },
                {
                    "sent": "We're not computing something that different.",
                    "label": 0
                },
                {
                    "sent": "If you were pushing this to the end, stochastic Grandison would be slower, but you get the same weight vector at the end.",
                    "label": 0
                },
                {
                    "sent": "It's convex, there is just one optimum.",
                    "label": 0
                },
                {
                    "sent": "There is just we are approaching the optimum in some way and.",
                    "label": 0
                },
                {
                    "sent": "I would say that the tester or you get so on that particular problem.",
                    "label": 0
                },
                {
                    "sent": "I've been going basically to 5% by changing a little bit the parameters and this high person means that you have 5% examples in the test set that are some kind of outliers.",
                    "label": 0
                },
                {
                    "sent": "So you could be fundamentally difficult to recognize, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "No, it's not that big a this is my version.",
                    "label": 0
                },
                {
                    "sent": "The biggest source of social Schwartz.",
                    "label": 0
                },
                {
                    "sent": "There is a stochastic gradient plus the projection step.",
                    "label": 0
                },
                {
                    "sent": "They do some things.",
                    "label": 0
                },
                {
                    "sent": "In addition, in order to make some aspect easier, and many people do.",
                    "label": 0
                },
                {
                    "sent": "The problem is that.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient is an old stuff.",
                    "label": 0
                },
                {
                    "sent": "Nobody can publish a paper about stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "Dissent is not new, so everybody wants to say why stochastic parent is quite good.",
                    "label": 0
                },
                {
                    "sent": "They have to insist on the little thing they add.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to do is take plans, stochastic gradient, no tricks, no addition.",
                    "label": 0
                },
                {
                    "sent": "I can show you the code.",
                    "label": 0
                },
                {
                    "sent": "It's absolutely planned.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, I'll tell you what I did I used some cross validation methods in that case and at the end of the third part of the talk I'm going to give you a better method to stop it.",
                    "label": 0
                },
                {
                    "sent": "Is going to come, but I need to get an interesting stopping criterion.",
                    "label": 0
                },
                {
                    "sent": "I need to go to some extra information.",
                    "label": 0
                },
                {
                    "sent": "But in that case I just use cross validation.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Just to class problem it says the N. Two class problem with quite big.",
                    "label": 0
                },
                {
                    "sent": "800,000 examples 47 thousand dimension.",
                    "label": 0
                },
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "I chose this particular category because it's quite balanced.",
                    "label": 0
                },
                {
                    "sent": "If it's in balance, you have to do some things, but it's true for SVM, syspro for low gloss.",
                    "label": 0
                },
                {
                    "sent": "Whatever things you use, when it's completely imbalanced, you have to do some things they're not different here.",
                    "label": 0
                },
                {
                    "sent": "This comparison is interesting because you optimize the same cost you compute basically the same learning system.",
                    "label": 0
                },
                {
                    "sent": "With the same regularization parameters.",
                    "label": 0
                },
                {
                    "sent": "Secret secret versus the rest?",
                    "label": 0
                },
                {
                    "sent": "Is one of the global classes you know.",
                    "label": 0
                },
                {
                    "sent": "In the SEV one is LC-1 is not the small, Reuters is the big Reuters and they have a tree of classes and at the top of the trees you have a cecati cut and something else.",
                    "label": 0
                },
                {
                    "sent": "Seacat is about 42% of the total.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "So the question is if you try to reduce the accuracy in SVM light.",
                    "label": 0
                },
                {
                    "sent": "When I took the default one in that case, but you can see what's going on in the log loss case you know here I played with the does it work with the default accuracy here and I thought that it was still shocked to see that the default accuracy for linear was actually slightly worse than what I was easily obtaining with stochastic gradient distant.",
                    "label": 0
                },
                {
                    "sent": "So I tried to use slightly higher accuracy.",
                    "label": 0
                },
                {
                    "sent": "So basically the setup cost of this sophisticated algorithm is too high.",
                    "label": 0
                },
                {
                    "sent": "At the beginning of the convergence they are slow.",
                    "label": 0
                },
                {
                    "sent": "And this one at the beginning of the conference is very fast.",
                    "label": 0
                },
                {
                    "sent": "But it's fast when it matters.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so basically, two weeks ago I put this program from the web and I sent an email to Young and Joshua because there was our families quite fun and the weather spread a little bit and at some point I got an email from Patrick Haffner, another of the band of the French guys.",
                    "label": 1
                },
                {
                    "sent": "And he tried on some of the data sets is using within a TNT so different setups, so it probably has a probably gets much better error rates because.",
                    "label": 1
                },
                {
                    "sent": "So this is this Reuters set, the same one, but with much better choice of hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And he's using the Liberals VM with a specialized implementation of the of the kernel that that user inverted index to do sparse computation.",
                    "label": 0
                },
                {
                    "sent": "That's really quite smart.",
                    "label": 0
                },
                {
                    "sent": "Yes, his own program that he called Yama does SVM accent, which is another way is not the same cost, but it's something that people lose a lot in this domain and the stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "Here's the machine translation that I said that's quite big with about 1 million training example and quarter million features.",
                    "label": 0
                },
                {
                    "sent": "That's another translation stuff, and this is something for dialogue, so these are real that I base that they use.",
                    "label": 0
                },
                {
                    "sent": "And well, you can see that.",
                    "label": 0
                },
                {
                    "sent": "The gun is big.",
                    "label": 0
                },
                {
                    "sent": "And come on, this is not a new algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is something from the 50s.",
                    "label": 0
                },
                {
                    "sent": "So what have we been doing?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There is something important there.",
                    "label": 0
                },
                {
                    "sent": "This problem is the text problem.",
                    "label": 0
                },
                {
                    "sent": "They're all very sparse.",
                    "label": 0
                },
                {
                    "sent": "And that's something that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Whether discussed the asymptotics, stochastic gradient dissent, if you sparse.",
                    "label": 0
                },
                {
                    "sent": "The defect or the cost of each iteration gains fully from the sparsity.",
                    "label": 0
                },
                {
                    "sent": "If you use Newton mattress is well.",
                    "label": 0
                },
                {
                    "sent": "When you compute the Hessian, they're not sparse anymore, so you dealing with mattress of ties.",
                    "label": 0
                },
                {
                    "sent": "This square and the disk ways really.",
                    "label": 0
                },
                {
                    "sent": "This quarter, 1,000,000 ^2.",
                    "label": 0
                },
                {
                    "sent": "So well.",
                    "label": 0
                },
                {
                    "sent": "Simplicity's value in that case.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then I went to let's do another one.",
                    "label": 0
                },
                {
                    "sent": "So conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "They quite an interesting algorithm in text processing, and the task was text chunking.",
                    "label": 0
                },
                {
                    "sent": "So basically is the computational neural linguistic 2000 chunking tasks.",
                    "label": 1
                },
                {
                    "sent": "You have sentences and you need to segment them.",
                    "label": 0
                },
                {
                    "sent": "Incentives syntactically correlated chunks like funded non phrases, verb phrases you want to segment the sentence.",
                    "label": 1
                },
                {
                    "sent": "The training set is about 100,000 training segments from 9000 sentences and we have a testing set that's a bit smaller, just enough to make measurements.",
                    "label": 1
                },
                {
                    "sent": "So model is a conditional random field or linear.",
                    "label": 0
                },
                {
                    "sent": "Log loss.",
                    "label": 1
                },
                {
                    "sent": "The features are just N grams of words and parts of speech tags that were actually given.",
                    "label": 0
                },
                {
                    "sent": "And this is quite a big model.",
                    "label": 0
                },
                {
                    "sent": "You have 1.6 or 1.7 million parameters.",
                    "label": 0
                },
                {
                    "sent": "And there is another stochastic gradient by Vishwanathan and showed off and again well because they cannot publish a paper on simple stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "They have all these SMD adaptation and they show it works.",
                    "label": 0
                },
                {
                    "sent": "But let's look at plans stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brilliant.",
                    "label": 0
                },
                {
                    "sent": "So if you use the limited storage BFG S which is considered to be one of the good ones.",
                    "label": 0
                },
                {
                    "sent": "So initially people doing CRF, they use iterative scaling, which is bad.",
                    "label": 0
                },
                {
                    "sent": "Then they use Newton method in full be of GS, which is slow and eventually they found unlimited storage VGS which is one of the most sophisticated optimization method is quite good.",
                    "label": 0
                },
                {
                    "sent": "And you get something like one hour and a half.",
                    "label": 0
                },
                {
                    "sent": "And this does it in 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "Now there are some things that are interesting thinking about the bias of the community.",
                    "label": 0
                },
                {
                    "sent": "The CRF is something that comes from the hidden Markov model literature, and traditionally one use the forward backward algorithm to compute the gradients.",
                    "label": 0
                },
                {
                    "sent": "But by making this experiment so upset that if you use the chain rule, you know, like all backdrop is actually 30% faster than using forward backward.",
                    "label": 1
                },
                {
                    "sent": "But when I looked at the CRF codes around and there was a CRF plus plus code which is considered considered to be a good one, but they still use forward backward.",
                    "label": 1
                },
                {
                    "sent": "And the second one is, well, I'm actually going to discuss this short slide about graph Transformer network later.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I'm going to argue later.",
                    "label": 0
                },
                {
                    "sent": "That I consider that these are simple problems.",
                    "label": 0
                },
                {
                    "sent": "I'm going to argue that later and show what real life real life problem looks like.",
                    "label": 0
                },
                {
                    "sent": "But I was even surprised myself by making these experiments, because I knew that stochastic gradient is very one.",
                    "label": 0
                },
                {
                    "sent": "But I didn't expect such such a beating.",
                    "label": 0
                },
                {
                    "sent": "So I should discuss some details, because there are some little detail.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "The first one is choosing this game schedule.",
                    "label": 0
                },
                {
                    "sent": "And in that case I choose something on the scalar divide by T + T zero.",
                    "label": 0
                },
                {
                    "sent": "So little piece of theory.",
                    "label": 0
                },
                {
                    "sent": "The key factor is 2 times Italian Deming If it is less than one stochastic gradient is excruciatingly slow.",
                    "label": 0
                },
                {
                    "sent": "With the speed T -- S. So basically less than 1 / T. If it's greater than one, then you get the T -- 1 speed with this thing and you can see easily that S equal 2 is the right, the right value.",
                    "label": 0
                },
                {
                    "sent": "So basically the ether here.",
                    "label": 0
                },
                {
                    "sent": "Should be one over the smallest eigenvalue of the Asian.",
                    "label": 0
                },
                {
                    "sent": "In that case, we have the regularization term.",
                    "label": 0
                },
                {
                    "sent": "So the Lambda of the relaxation term is a good.",
                    "label": 0
                },
                {
                    "sent": "Lower bound of London.",
                    "label": 0
                },
                {
                    "sent": "So I used it to equal 1 over Lambda.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "If you look at this data over T, forget about the T0 for now.",
                    "label": 0
                },
                {
                    "sent": "At the beginning, tea is very small.",
                    "label": 0
                },
                {
                    "sent": "When T is one.",
                    "label": 0
                },
                {
                    "sent": "Since Lambda is quite small in there was 10 to the minus 410 to minus five.",
                    "label": 0
                },
                {
                    "sent": "In my experiments you get a very large again here.",
                    "label": 0
                },
                {
                    "sent": "And your weight goes all over the place and just goes very far away and you cooked.",
                    "label": 0
                },
                {
                    "sent": "What people do usually saying, oh, this is the gain is too high.",
                    "label": 0
                },
                {
                    "sent": "Let's reduce Inter as a big mistake, you go into the slow, right?",
                    "label": 0
                },
                {
                    "sent": "The proper thing to do is to add a shift below.",
                    "label": 0
                },
                {
                    "sent": "And what I did was choosing TO to make sure that the expected initial updates the kind of movement you get at the beginning are comparable to the size of the weights I'm expecting at the end that I can compute by duality theory.",
                    "label": 1
                },
                {
                    "sent": "But in the CRF, Kase IUC, to equal 1 over Lambda again.",
                    "label": 0
                },
                {
                    "sent": "But this was too complicated to calculate, so I used the secret recipe.",
                    "label": 0
                },
                {
                    "sent": "Are you secretly CP?",
                    "label": 0
                },
                {
                    "sent": "There is a secret with CP.",
                    "label": 0
                },
                {
                    "sent": "If you remember them, the math actually I can show them again.",
                    "label": 0
                },
                {
                    "sent": "Then maybe on the previous slides.",
                    "label": 0
                },
                {
                    "sent": "If you look at this, you observe that the red doesn't depend on N, the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "Spread them using, well, you draw one example at the time.",
                    "label": 0
                },
                {
                    "sent": "How many you have total weed?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So at any moment during training we can pick a small subsample, a few thousand example, 10,000, maybe 100,000.",
                    "label": 1
                },
                {
                    "sent": "If you want to be sure.",
                    "label": 0
                },
                {
                    "sent": "And we can save those starting weights.",
                    "label": 0
                },
                {
                    "sent": "And try values either on the sub sample and see after one go over the subsample where you get.",
                    "label": 1
                },
                {
                    "sent": "And you can pick the game that most reduces the cost.",
                    "label": 0
                },
                {
                    "sent": "When you have this game, you use it for the next 100,000 interactions.",
                    "label": 0
                },
                {
                    "sent": "Stop again, do it again.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "Algorithm is very simple idea.",
                    "label": 0
                },
                {
                    "sent": "So in the CF benchmark I did that to choose the initial T 0, but in fact you can do that much simpler.",
                    "label": 0
                },
                {
                    "sent": "You can just well, every so often you make this kind of chip measurements to see what kind of games are going to work well, and because of the independent of nature of the asymptotic speedup stochastic gram distant, you have the same one is going to work on a larger data set.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing that's important is to get the engineering right.",
                    "label": 0
                },
                {
                    "sent": "I told you that sparsity is important in that case.",
                    "label": 0
                },
                {
                    "sent": "And the update is something like this W -- Y to times Lambda W minus grant of the loss, so you can do in 2 steps.",
                    "label": 0
                },
                {
                    "sent": "First, remove the grant of the loss, and that's cheap because the loss involves very few terms, and that's that's a very sparse operation.",
                    "label": 0
                },
                {
                    "sent": "The second one applying this term is doing something like this, multiplying the weight by 1 minus beta Lambda.",
                    "label": 0
                },
                {
                    "sent": "Less expensive because you need to touch all the coefficients in this huge wave vector with 1 million parameters.",
                    "label": 0
                },
                {
                    "sent": "So there are simple solutions, just little engineering trick.",
                    "label": 0
                },
                {
                    "sent": "Solution one.",
                    "label": 0
                },
                {
                    "sent": "You represent the vector W as the product of a scholar and a vector.",
                    "label": 1
                },
                {
                    "sent": "You perform one by changing the vector and two by changing its color.",
                    "label": 0
                },
                {
                    "sent": "When this color becomes too small, you make the real thing and you put it back to one.",
                    "label": 0
                },
                {
                    "sent": "Well, now it's cheap.",
                    "label": 0
                },
                {
                    "sent": "Another solution is to do only step one for each example and perform Step 2 every so often, not all the time just so often.",
                    "label": 0
                },
                {
                    "sent": "With the higher again, so basically you increase the gain a little bit foldable to compensate, and you can calculate how to do it pretty easily.",
                    "label": 0
                },
                {
                    "sent": "And the two work about the same.",
                    "label": 0
                },
                {
                    "sent": "In fact, I reward both programs.",
                    "label": 0
                },
                {
                    "sent": "I compare them.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They don't really behave the same.",
                    "label": 0
                },
                {
                    "sent": "No, I told you this are toy problems.",
                    "label": 0
                },
                {
                    "sent": "Let's look like.",
                    "label": 0
                },
                {
                    "sent": "Look at the real one so it's all thing is the system for reading checks.",
                    "label": 0
                },
                {
                    "sent": "It was industry diploid in 96 run billion of checks over 10 years and there is an estimate that if you live in the US you have 40% chance that some of your checks have been processed by that particular code.",
                    "label": 1
                },
                {
                    "sent": "So that was done in a TNT just before it broke up in my in various plastic pieces.",
                    "label": 1
                },
                {
                    "sent": "So what you have the examples are pairs.",
                    "label": 1
                },
                {
                    "sent": "The pattern is a complete image of a check the scanner, the label is an amount.",
                    "label": 0
                },
                {
                    "sent": "And you have a very strong structure in this problem.",
                    "label": 0
                },
                {
                    "sent": "You know that you need to find fields in the fields.",
                    "label": 1
                },
                {
                    "sent": "You need to find characters you need to recognize characters.",
                    "label": 0
                },
                {
                    "sent": "Then you need to do some syntactical interpretation.",
                    "label": 0
                },
                {
                    "sent": "You know these things.",
                    "label": 0
                },
                {
                    "sent": "You there is no need to actually do the full things, so you make differentiable modules that do everything.",
                    "label": 1
                },
                {
                    "sent": "And you can even switch when each module with some hand label data.",
                    "label": 0
                },
                {
                    "sent": "Then you put it together and you define the global cost function.",
                    "label": 0
                },
                {
                    "sent": "In that case.",
                    "label": 1
                },
                {
                    "sent": "In that case it was actually a CRF, so you have them.",
                    "label": 0
                },
                {
                    "sent": "That's the numerator and the denominator in log terms, and that's the ratio.",
                    "label": 1
                },
                {
                    "sent": "And you put that in the stochastic gradient descent for a few weeks and you get a working system.",
                    "label": 0
                },
                {
                    "sent": "Now there is a huge quantity of engineering there.",
                    "label": 0
                },
                {
                    "sent": "It took quite a lot of good people and close to one year to get this thing to run.",
                    "label": 0
                },
                {
                    "sent": "But that's probably closer to what real life look like.",
                    "label": 0
                },
                {
                    "sent": "Looks like so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm my voice.",
                    "label": 0
                },
                {
                    "sent": "I'm losing my voice.",
                    "label": 0
                },
                {
                    "sent": "What do you think of five minutes break.",
                    "label": 0
                },
                {
                    "sent": "Yes, I need to drink something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 5 minutes break.",
                    "label": 0
                },
                {
                    "sent": "Let's let's come back at three.",
                    "label": 0
                },
                {
                    "sent": "Let's say 3 something.",
                    "label": 0
                },
                {
                    "sent": "Nick yeah yeah yeah I see.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's go again.",
                    "label": 0
                },
                {
                    "sent": "The part three is about.",
                    "label": 0
                },
                {
                    "sent": "Guess we're not going to see examples again.",
                    "label": 0
                },
                {
                    "sent": "So sometimes there's just too much data to store.",
                    "label": 1
                },
                {
                    "sent": "We can't afford to store that much data, or sometimes it's stored on some is archived in a way that's too expensive to retrieve.",
                    "label": 0
                },
                {
                    "sent": "It seems that when you have a very high volume storage, they're very sequential in nature.",
                    "label": 0
                },
                {
                    "sent": "So so when you want to go again over your data, you have some expensive operations to do, like changing tapes or whatever.",
                    "label": 0
                },
                {
                    "sent": "This is of course related to the topic of streaming data.",
                    "label": 0
                },
                {
                    "sent": "This is related to tracking nonstationarity's and novelty detection, but I'm going to remain in a very simple IDE framework.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to bring the topic relatively slowly.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st I'm going to see what's happening when we add one example to the training set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Look at this.",
                    "label": 0
                },
                {
                    "sent": "When you have an example.",
                    "label": 0
                },
                {
                    "sent": "The optimum on the training set is FN is the argument of the training error.",
                    "label": 0
                },
                {
                    "sent": "When you add another example, you want the argument on N + 1 examples.",
                    "label": 0
                },
                {
                    "sent": "Which is the same as the argument of N + 1 / N of 20 or 1 + 1 examples that you can write this way.",
                    "label": 0
                },
                {
                    "sent": "Admin of the error on an example plus a small term.",
                    "label": 0
                },
                {
                    "sent": "So that's a small perturbation.",
                    "label": 0
                },
                {
                    "sent": "And so you have something very close.",
                    "label": 0
                },
                {
                    "sent": "So you can do very simple things like first order or second order analysis to try to estimate how FN plus one is related to.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when you do it just the first of our calculation.",
                    "label": 0
                },
                {
                    "sent": "You get that FN plus one is F N -- 1 / N inverse of the empirical Asian on N + 1 example.",
                    "label": 1
                },
                {
                    "sent": "Times the directive of the loss for the new examples plus some similar terms.",
                    "label": 0
                },
                {
                    "sent": "If you compare this expression with second order stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "Well, someone's the same.",
                    "label": 0
                },
                {
                    "sent": "You know you don't have the high order terms.",
                    "label": 0
                },
                {
                    "sent": "And the Hessian, while you have the real 1 instead of the empirical one, in practice you wouldn't have the real one would have some estimate.",
                    "label": 1
                },
                {
                    "sent": "So could these two empirical processes converge with the same speed?",
                    "label": 0
                },
                {
                    "sent": "And what would it mean?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can make a theorem, and in fact I did that with you in 2003.",
                    "label": 0
                },
                {
                    "sent": "But then I found that Murata in Amery had done something similar in 98.",
                    "label": 0
                },
                {
                    "sent": "And the adequate conditions.",
                    "label": 0
                },
                {
                    "sent": "End times the distance between F Infinity, which is the limit way where you would go with an infinite number of examples minus F N ^2.",
                    "label": 0
                },
                {
                    "sent": "Is equal to the limit 20 goes to Infinity of W Infinity minus double T * T equals some constant you can compute.",
                    "label": 0
                },
                {
                    "sent": "OK. Looks like a nice result, but what's interesting is to try to understand what it means.",
                    "label": 0
                },
                {
                    "sent": "So let's start from W0F Zero and we can make one path of signaled a stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So we do this stochastic signals to Casiguran distant just once in every example, and you reach some WN.",
                    "label": 0
                },
                {
                    "sent": "We can also compute the empirical optimum on an example here.",
                    "label": 0
                },
                {
                    "sent": "And what this says is that the average distance between WN and the optimum, the best test error.",
                    "label": 0
                },
                {
                    "sent": "And the average distance between the empirical optimum and the best one there about the same.",
                    "label": 0
                },
                {
                    "sent": "The old decreased like some constant over N. So this is as good as this.",
                    "label": 0
                },
                {
                    "sent": "In terms of training error, this is the optimum.",
                    "label": 0
                },
                {
                    "sent": "This has better training error.",
                    "label": 0
                },
                {
                    "sent": "But in terms of testing or are close, you are to the best sterile.",
                    "label": 0
                },
                {
                    "sent": "This is the same.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me handle that.",
                    "label": 0
                },
                {
                    "sent": "Given the large new string large enough training set.",
                    "label": 1
                },
                {
                    "sent": "A single path of 2nd order stochastic gradient generalize as well as the empirical timem.",
                    "label": 1
                },
                {
                    "sent": "That's a very shocking result, I think.",
                    "label": 1
                },
                {
                    "sent": "And so we can make some experiments on tentative data, so that's the 19 dimensional purely synthetic data.",
                    "label": 0
                },
                {
                    "sent": "This is the number of examples.",
                    "label": 0
                },
                {
                    "sent": "This is mean squared error plus some time explaining loss in these quadraplex .1 zero, .01 and so on.",
                    "label": 0
                },
                {
                    "sent": "And I don't know which one is the blue and which one is the red.",
                    "label": 0
                },
                {
                    "sent": "Yes, the red is the empirical optimum, the blue is the signature stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "But there are some.",
                    "label": 0
                },
                {
                    "sent": "That's a pure mean squared error or least square problem.",
                    "label": 0
                },
                {
                    "sent": "I think it's one sigmoid loss.",
                    "label": 0
                },
                {
                    "sent": "WX is very simple stuff and artificial later.",
                    "label": 0
                },
                {
                    "sent": "If you look in terms of computing time.",
                    "label": 0
                },
                {
                    "sent": "Well, the signal stochastic gradient descent is a bit faster to compute the empirical optimum I use the Newton method.",
                    "label": 0
                },
                {
                    "sent": "So the Newton method, if you think about it, it's really much looks like doing 2nd order stochastic gradient slightly 2nd order gradient again and again and again.",
                    "label": 0
                },
                {
                    "sent": "So if you do it well, you can be quite good, but the signal signal stochastic will be faster.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are some unfortunate practical issues.",
                    "label": 1
                },
                {
                    "sent": "The first one is that signaled a stochastic written descent is not that fast.",
                    "label": 1
                },
                {
                    "sent": "Compared to stochastic gradient distance, it's just against the constant and loses a constant.",
                    "label": 0
                },
                {
                    "sent": "You get a constant which is related to the condition number, and you lose a constant becausw each iteration cause disquiet instead of D and it's actually worse when you have sparse data.",
                    "label": 1
                },
                {
                    "sent": "So you must estimate install D by D matrix H -- 1.",
                    "label": 0
                },
                {
                    "sent": "You must do a multiply the gradient for each example by that matrix.",
                    "label": 1
                },
                {
                    "sent": "And the sparsity tricks do not work so well, because H -- 1 typically is not sparse.",
                    "label": 1
                },
                {
                    "sent": "So there are some workarounds.",
                    "label": 0
                },
                {
                    "sent": "There are faster ways to compute H -- 1, like using good boys formula Online.",
                    "label": 0
                },
                {
                    "sent": "DFCS is quite an interesting way.",
                    "label": 0
                },
                {
                    "sent": "You can make limited storage approximation of edge minus one, like diagonal approximations, which is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "Low rank approximations, limited storage, VGS like approximations.",
                    "label": 0
                },
                {
                    "sent": "But I must say it's not completely satisfactory.",
                    "label": 0
                },
                {
                    "sent": "So for a long time I wondered.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to do that faster?",
                    "label": 0
                },
                {
                    "sent": "At this point I'm going to make a slide.",
                    "label": 0
                },
                {
                    "sent": "This question and answer the question a question from last time about stop.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quiteria let's summarize.",
                    "label": 0
                },
                {
                    "sent": "Time to reach a certain accuracy roll.",
                    "label": 1
                },
                {
                    "sent": "On the 20 year old.",
                    "label": 0
                },
                {
                    "sent": "2nd order stochastic gradient is new overall plus some.",
                    "label": 0
                },
                {
                    "sent": "Negligible terminal, one of the role.",
                    "label": 0
                },
                {
                    "sent": "Plans to cost a gram distances can you overrule?",
                    "label": 0
                },
                {
                    "sent": "Plus this all of them.",
                    "label": 0
                },
                {
                    "sent": "If he does remember iteration so it's not time.",
                    "label": 0
                },
                {
                    "sent": "Is number of iterations to reach a certain accuracy.",
                    "label": 0
                },
                {
                    "sent": "Number of epochs passes over the string data to reach the same test error as the full optimization.",
                    "label": 1
                },
                {
                    "sent": "2nd order stochastic gradient one that was what I just discussed.",
                    "label": 0
                },
                {
                    "sent": "Here you see.",
                    "label": 0
                },
                {
                    "sent": "Is Kate, I'm small, so stochastic gradient descent is K this constant that's between one and Kappa Square, where Kappa is the Commission member.",
                    "label": 0
                },
                {
                    "sent": "It's problem dependent.",
                    "label": 1
                },
                {
                    "sent": "So there are many ways to make this content smaller.",
                    "label": 0
                },
                {
                    "sent": "Like you do, could do an exact signatures to Kathy Griffin distant.",
                    "label": 0
                },
                {
                    "sent": "You can approximate.",
                    "label": 0
                },
                {
                    "sent": "For one, you can use preconditioning tricks, normalizing the inputs or a lot of these tricks that you heard about multilayer networks they have to do with making case smaller.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But that gives you a stopping criteria.",
                    "label": 0
                },
                {
                    "sent": "Good day young sorry.",
                    "label": 0
                },
                {
                    "sent": "What I used in the experiment before was early stopping with cross validation.",
                    "label": 1
                },
                {
                    "sent": "You create a validation set by setting some training examples apart.",
                    "label": 1
                },
                {
                    "sent": "You monitor the cost function of validation set and you stop when it stops decreasing.",
                    "label": 1
                },
                {
                    "sent": "But in fact you could do it a priori.",
                    "label": 0
                },
                {
                    "sent": "You extract 2 disjoint subsamples of training data.",
                    "label": 0
                },
                {
                    "sent": "You train on the 1st substantial you stop by acting on the 2nd.",
                    "label": 1
                },
                {
                    "sent": "The number of epochs you have to do is an estimate of K. So now you know, OK?",
                    "label": 0
                },
                {
                    "sent": "So while you trend by performing.",
                    "label": 0
                },
                {
                    "sent": "KA pox on the full set and you stop your finished you there.",
                    "label": 0
                },
                {
                    "sent": "This is asymptotically correct, so you need to have sets that are large enough, of course, and in practice, so you're in the right ballpark.",
                    "label": 0
                },
                {
                    "sent": "So if you want to be safe with it 10 times more iterations are still going to be faster than using.",
                    "label": 0
                },
                {
                    "sent": "Classical algorithm non stochastic algorithms.",
                    "label": 0
                },
                {
                    "sent": "So, so I was awfully fast on my third part, so I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summarize again.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I do it again because it's something that I think is very important and I'm sorry to do it that way, but I can go fast.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I add one example to the cost, I make a small perturbation of the error.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can make a first order calculation and I found something that's very close to signal stochastic gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can run the math and see they converted the same speed and what it means.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that by making a single path of 2nd order stochastic gradient?",
                    "label": 1
                },
                {
                    "sent": "I generalize as well as the empirical optimum.",
                    "label": 1
                },
                {
                    "sent": "This can be verified in practice.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are some nasty problems that second order stochastic gradient descent is painful.",
                    "label": 0
                },
                {
                    "sent": "You have this big matrix to store.",
                    "label": 0
                },
                {
                    "sent": "Well is not more painful than the classical programs, but compared to plain stochastic gradient, dissent is a bit painful.",
                    "label": 0
                },
                {
                    "sent": "You have this big matrix to store.",
                    "label": 0
                },
                {
                    "sent": "You lose opportunity to use the sparsity.",
                    "label": 0
                },
                {
                    "sent": "There are some partial complicated work arounds.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say this is finished.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what you get out of this is, and it's stopping criterion for stochastic Grandison 'cause you know that stochastic William Distantes Kate Times slower than signatures to custom gradient decent.",
                    "label": 0
                },
                {
                    "sent": "With case some problem dependent constant that's between one and the condition number squared.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically you can measure K on subsamples and you know how many times you have to go over your training data in the plans to Cassa Grande.",
                    "label": 0
                },
                {
                    "sent": "That was my football.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My 4th spot.",
                    "label": 0
                },
                {
                    "sent": "I'm a bit annoyed about that one because I'm less satisfied with this problem with the previous three and I'm going to explain why at which moment I think I took it the wrong way, but the results I have this way so maybe.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we'd better another time.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to this incremental training we add.",
                    "label": 0
                },
                {
                    "sent": "Refresh the training example.",
                    "label": 0
                },
                {
                    "sent": "The current training set.",
                    "label": 0
                },
                {
                    "sent": "So we take an example that we haven't seen yet.",
                    "label": 0
                },
                {
                    "sent": "We added to the training set and we reach 1 until reaching sufficient accuracy.",
                    "label": 1
                },
                {
                    "sent": "So if we use a loss function with flat segments, something like the hinge loss so.",
                    "label": 0
                },
                {
                    "sent": "Although losses like something like a sigmoid is close to that.",
                    "label": 1
                },
                {
                    "sent": "You observe two things.",
                    "label": 0
                },
                {
                    "sent": "But very few examples code change when you add them to the set.",
                    "label": 0
                },
                {
                    "sent": "A lot of them they're going to be in the flat part of the last zero gradient.",
                    "label": 1
                },
                {
                    "sent": "They changed nothing.",
                    "label": 0
                },
                {
                    "sent": "And when you do the retraining.",
                    "label": 0
                },
                {
                    "sent": "Very few example, of course change during the training.",
                    "label": 0
                },
                {
                    "sent": "So to return, it's a bit annoying you would have to remember all the examples, but in fact most of them they do nothing.",
                    "label": 0
                },
                {
                    "sent": "Can you select the one you actually need to store in memory?",
                    "label": 0
                },
                {
                    "sent": "And can you still pick the one you're going to add to your accumulated experience?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me make a tool of the methods that people have been using to select examples.",
                    "label": 0
                },
                {
                    "sent": "The first category is things that are coming from the nearest neighbor literature and there is this old book by Deviren Kichler that's very interesting about the topic that the multi edit comments methods.",
                    "label": 0
                },
                {
                    "sent": "And you can adapt it to other classifier, but the cost is greater than the square of the number of examples.",
                    "label": 0
                },
                {
                    "sent": "Basically, the typical contents method is you pick one example and you look how well this example is recognized by making nearest neighbor with the others or basically by building a classifier with the other examples and you decide whether to discard it or not.",
                    "label": 0
                },
                {
                    "sent": "So if it's correctly recognized you say it's redundant.",
                    "label": 0
                },
                {
                    "sent": "So I remove it if it's not correctly recognize.",
                    "label": 0
                },
                {
                    "sent": "You keep it.",
                    "label": 0
                },
                {
                    "sent": "But that's in square in building such such a thing.",
                    "label": 0
                },
                {
                    "sent": "You can try to use gradient methods.",
                    "label": 1
                },
                {
                    "sent": "This is what I should have been using in fact, but you can select examples when the loss as a sizable gradient.",
                    "label": 1
                },
                {
                    "sent": "So that if you have some gradient, now is likely to have gradients in the future.",
                    "label": 0
                },
                {
                    "sent": "If the examples, no gradients meanings correctly recognize or is completely off.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't need it.",
                    "label": 0
                },
                {
                    "sent": "Now, many examples can carry the same information, so when you store examples for future retraining.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can't be one example.",
                    "label": 0
                },
                {
                    "sent": "The correction that one example makes could be redundant with that.",
                    "label": 0
                },
                {
                    "sent": "That other example makes.",
                    "label": 0
                },
                {
                    "sent": "Mick and if you run the thing that leads to organization schemes in the scale, like the square, the number of examples and that was used, for instance, it's optimal experiment design the the federal booking 71 about optimal experiment design.",
                    "label": 0
                },
                {
                    "sent": "That's a big part of statistics is entirely about this kind of organization schemes.",
                    "label": 0
                },
                {
                    "sent": "Now an approach that I thought was interesting and actually it's interesting because it illustrates a certain points, but I'm not sure what we should really use in practice.",
                    "label": 0
                },
                {
                    "sent": "Is to use duality?",
                    "label": 1
                },
                {
                    "sent": "So if you take convex common machine, let's support vector machines, there is an expression where the weight parameter is expressed as a linear combination of some of the features associated with some examples.",
                    "label": 0
                },
                {
                    "sent": "And this combination is sparse.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you think about support vectors.",
                    "label": 1
                },
                {
                    "sent": "The set of support vectors you can define it as the set of example that would be sufficient to find the same classification boundary if you have only the support vector, you're going to find the same boundary.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that this involves the bulky kernel matrix, which has all again or of order N squared values.",
                    "label": 1
                },
                {
                    "sent": "But the thing that gives some hope is that sparsity makes most of the kernel matrix values irrelevant.",
                    "label": 0
                },
                {
                    "sent": "So you don't need to have them all.",
                    "label": 0
                },
                {
                    "sent": "So let's see how we can use duality.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So large scale support vector machines?",
                    "label": 0
                },
                {
                    "sent": "That's a contradiction in terms you know.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines.",
                    "label": 0
                },
                {
                    "sent": "They scale like the square of the number of examples I told you at the beginning.",
                    "label": 0
                },
                {
                    "sent": "I want to scale like the volume of the data I'm completely off.",
                    "label": 0
                },
                {
                    "sent": "But it's also instructive, because if you can get support vector machines to go faster, you probably have understood something.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to go through 2 steps.",
                    "label": 0
                },
                {
                    "sent": "The first one is to define stochastic and incremental support vector machines.",
                    "label": 1
                },
                {
                    "sent": "That's going to work by iteratively constructing the support vector expansion.",
                    "label": 0
                },
                {
                    "sent": "And that's going to give an answer about which candidate support vectors to store and discard and how to manage the memory required to store this kernel values.",
                    "label": 1
                },
                {
                    "sent": "And the second part is use active learning and support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Choose which example to process next and discover that convexity is no longer our friend.",
                    "label": 1
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's run in the dual, so forget about this for now.",
                    "label": 1
                },
                {
                    "sent": "That's the primal problem.",
                    "label": 0
                },
                {
                    "sent": "You have one class, the other class you want to find the separation with the largest margin.",
                    "label": 0
                },
                {
                    "sent": "So things about two plates with a Big Spring in between.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Now the dual problem is well now you take the two convex hulls.",
                    "label": 0
                },
                {
                    "sent": "The convex Hull of blue and the convex Hull of the right ones.",
                    "label": 0
                },
                {
                    "sent": "And you take a moving point that can move anywhere within the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "So at this point they can move anywhere within the blue convex Hull.",
                    "label": 0
                },
                {
                    "sent": "This point B can move anywhere between the right convex Hull you put a spring on elastic in between and you find the point where the closest the minimum distance between the health.",
                    "label": 0
                },
                {
                    "sent": "And if you take.",
                    "label": 0
                },
                {
                    "sent": "The middle.",
                    "label": 0
                },
                {
                    "sent": "You get the optimal hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So that's a duality.",
                    "label": 0
                },
                {
                    "sent": "So if you see what's been written in the literature, you observe that the memory it requires in the best case is something like number of examples, times, number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "The time is it requires this end to sum power between one and two times, number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "And there are some results that show that in the best case then multiple vectors are syntactically is 12 twice the base zero times the number of Super Bowl number of examples.",
                    "label": 0
                },
                {
                    "sent": "So basically we have something N square and worse than square here.",
                    "label": 0
                },
                {
                    "sent": "Now, there is evidence that the number of support vectors could be much smaller, like Chris Burges at this reduced supervector machines.",
                    "label": 1
                },
                {
                    "sent": "And the Pascal Benson Ushua banjo made the pursuit schemes that can show that you can get good accuracy with much less support vectors.",
                    "label": 0
                },
                {
                    "sent": "And the question is how to do it fast?",
                    "label": 1
                },
                {
                    "sent": "And how small this can be?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or small discount.",
                    "label": 0
                },
                {
                    "sent": "So let's start with very inefficiently optimizer.",
                    "label": 0
                },
                {
                    "sent": "So I have my blue point that's constrained to stay within the blue color, the blue hole.",
                    "label": 0
                },
                {
                    "sent": "And the red point constraint to say in the red hole.",
                    "label": 0
                },
                {
                    "sent": "I pick a red example here and I consider that segment between the current red point and the new one.",
                    "label": 0
                },
                {
                    "sent": "I project the blue point in that segment and that's my new red point.",
                    "label": 0
                },
                {
                    "sent": "Then I pick one example here at the same thing here and here and here and here.",
                    "label": 0
                },
                {
                    "sent": "At each step, the distance between the Blue Square in the Red Square decreases.",
                    "label": 0
                },
                {
                    "sent": "That's an optimizer.",
                    "label": 0
                },
                {
                    "sent": "Now both B&N the linear combination of examples with positive coefficients them into one is what it means to say that the constraint to stay in the Hull.",
                    "label": 1
                },
                {
                    "sent": "The projection is quite a simple thing to do.",
                    "label": 0
                },
                {
                    "sent": "It's a linear operation.",
                    "label": 1
                },
                {
                    "sent": "And the projection time is proportional to the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "But that's still awfully slow for.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reasons.",
                    "label": 0
                },
                {
                    "sent": "The first reason is that suppose it doesn't eliminate unwanted support vectors very easily.",
                    "label": 1
                },
                {
                    "sent": "Suppose you have a pattern X that already has some non zero coefficient.",
                    "label": 1
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If you project on the segment between the right point and the new point, and you select this pattern X, what's going to happen is that you say I don't want more of this pattern.",
                    "label": 0
                },
                {
                    "sent": "So we're going to state a gamma equals 0.",
                    "label": 0
                },
                {
                    "sent": "But not going to remove anything.",
                    "label": 0
                },
                {
                    "sent": "The only way to remove to reduce the Alpha from this pattern would be to choose other patterns.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let them the.",
                    "label": 0
                },
                {
                    "sent": "What's going on here?",
                    "label": 0
                },
                {
                    "sent": "And let this thing the multiplication by one month.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eroded, that's very slow.",
                    "label": 0
                },
                {
                    "sent": "So there is a simple solution is to let gamma be slightly negative.",
                    "label": 0
                },
                {
                    "sent": "You can expand the segment a little bit beyond and show that you remain in the convex Hull that condition and you can allow it to be negative to the point where you actually make Alpha, exactly 0.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple solution.",
                    "label": 0
                },
                {
                    "sent": "Now the second problem is a few more fundamental is that we don't process super vectors often enough.",
                    "label": 1
                },
                {
                    "sent": "The basic idea for support vector machines when it's interesting, is when you have few support vectors.",
                    "label": 0
                },
                {
                    "sent": "Now you're going to spend your time considering new points that have no vocation to become super vectors, and decide that you don't need to add them to your recipe to your formula.",
                    "label": 0
                },
                {
                    "sent": "Well, in fact the one that are super vectors for which you would need to adjust the alphas, we're going to see just so often.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there was a very primitive algorithm that we called the color as 2 steps.",
                    "label": 0
                },
                {
                    "sent": "First we pick a random fresh example and we do the projection.",
                    "label": 1
                },
                {
                    "sent": "So again, we pick a random support vector and we do the projection.",
                    "label": 0
                },
                {
                    "sent": "Buffalo football.",
                    "label": 0
                },
                {
                    "sent": "You could compare this with the idea of incremental learning, and we're training that at the beginning this is adding one example.",
                    "label": 0
                },
                {
                    "sent": "This is returning, but to the simplest level.",
                    "label": 0
                },
                {
                    "sent": "Instead of iterating over the example we already had, we just pick one random support vectors one and support vector and we project just one.",
                    "label": 0
                },
                {
                    "sent": "This first operation, picking a fresh example and projecting, potentially can add the support vector.",
                    "label": 0
                },
                {
                    "sent": "This operation pick a random support vector and projecting potentially can remove one.",
                    "label": 0
                },
                {
                    "sent": "So we have the adding operation and the removing operation.",
                    "label": 0
                },
                {
                    "sent": "And we have some derivatives of this, so that's been done by Antoine Board, another between the last two or three.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Heels what's interesting is this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now I should explain the graph.",
                    "label": 0
                },
                {
                    "sent": "This is the honest handwritten digits again.",
                    "label": 0
                },
                {
                    "sent": "And each of the group of her bath is recognizing one class versus the right.",
                    "label": 0
                },
                {
                    "sent": "So this is plus eight versus the rest.",
                    "label": 0
                },
                {
                    "sent": "The yellow thing here.",
                    "label": 0
                },
                {
                    "sent": "Is what you get with the guys VN.",
                    "label": 0
                },
                {
                    "sent": "And the important thing of the blue thing, and the purple thing the blue thing is running one of these algorithm.",
                    "label": 0
                },
                {
                    "sent": "One go over the data.",
                    "label": 0
                },
                {
                    "sent": "The purple thing is to go over the letter.",
                    "label": 0
                },
                {
                    "sent": "This is the error and what you observe is that one goal over the data 2 passes over the data or full SVM.",
                    "label": 0
                },
                {
                    "sent": "You get the same kind of errors.",
                    "label": 0
                },
                {
                    "sent": "My daughter wants the just perceptrons for comparison, we can ignore that for now.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the training time.",
                    "label": 0
                },
                {
                    "sent": "Well, it's much faster you go about once over the data and once about the data by using this simple stupid algorithm where you pick a new example, you make an update.",
                    "label": 0
                },
                {
                    "sent": "You pick a random supervector you make an update, you pick a new example in Macon.",
                    "label": 0
                },
                {
                    "sent": "Well is much faster and performs as well in one go over the data.",
                    "label": 0
                },
                {
                    "sent": "Now, this story of retraining, but you don't need to between that much, you don't return that much work.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be correct.",
                    "label": 0
                },
                {
                    "sent": "So if you look at how much memory you need.",
                    "label": 0
                },
                {
                    "sent": "With that algorithm, the time is still like a plan as VM.",
                    "label": 0
                },
                {
                    "sent": "The memory is not a simple vector squared.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it, this is where.",
                    "label": 0
                },
                {
                    "sent": "The limitation of this kind of algorithm is showing.",
                    "label": 0
                },
                {
                    "sent": "Number of super vector squared.",
                    "label": 0
                },
                {
                    "sent": "Admitted if you really reduce number of super vectors to the minimum you want.",
                    "label": 0
                },
                {
                    "sent": "You still going to have a number of support vectors that's on the order of the effective dimension of the parameter.",
                    "label": 0
                },
                {
                    "sent": "So you get a memory that still on the order of this square, similar to SIGMOD or stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get any better.",
                    "label": 0
                },
                {
                    "sent": "That's where it's actually annoying.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "If you look at this, these are comparisons of time using memory.",
                    "label": 0
                },
                {
                    "sent": "The memory in the Super vector machines are cash.",
                    "label": 0
                },
                {
                    "sent": "You can adjust it, and it's going to use more or less memory.",
                    "label": 0
                },
                {
                    "sent": "Using you have less memory, you have to make more computation to make it point, so this is from 1 gigabyte to 2 megabytes.",
                    "label": 0
                },
                {
                    "sent": "The yellow things are using the Lib SVM program with values accuracy and the blue thing is using one of these online processes.",
                    "label": 0
                },
                {
                    "sent": "And what you see is that, well, we can work with a lot less memory than regular SVM thing.",
                    "label": 0
                },
                {
                    "sent": "But still it's not going to be enough to approach the stochastic gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, not that I have explained or relatively reasonable method for selecting which examples to keep to do the retraining thing.",
                    "label": 0
                },
                {
                    "sent": "My next question is to find how to select the next example.",
                    "label": 0
                },
                {
                    "sent": "I'm going to add in my incremental retraining process.",
                    "label": 0
                },
                {
                    "sent": "And when I'm doing this process step where I pick a fresh example, I can pick a fresh example of randomly.",
                    "label": 0
                },
                {
                    "sent": "I can speak it according to the gradient.",
                    "label": 1
                },
                {
                    "sent": "Let's speak the example that has the strongest coefficient in the greater the dual, the one that has increased the deal with the higher slope.",
                    "label": 1
                },
                {
                    "sent": "And that means selecting the example.",
                    "label": 0
                },
                {
                    "sent": "In my training exam, that's the most incorrect.",
                    "label": 1
                },
                {
                    "sent": "I think the most incorrect example and I tend to machine.",
                    "label": 0
                },
                {
                    "sent": "That's just that one.",
                    "label": 0
                },
                {
                    "sent": "And I can do something a little bit more refined, because if I select the most incorrect example, I'm going to select outliers.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I want to learn on just outliers.",
                    "label": 0
                },
                {
                    "sent": "So we can use the minimax approach and select the example that has the biggest gradient regardless of the class I'm assuming.",
                    "label": 0
                },
                {
                    "sent": "And that ends up to selecting the exam that closes to the decision boundary.",
                    "label": 1
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how this works.",
                    "label": 0
                },
                {
                    "sent": "First of all, you have, sorry I'm.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "To select the strongest gradient or the closest to the decision boundary you would think I have to go over all my examples and pick the one.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You don't need to do that, you can just do a sampling.",
                    "label": 0
                },
                {
                    "sent": "If you sample, let's say, 59 random examples, you have 95% chance to find something that among the 5% best ones.",
                    "label": 1
                },
                {
                    "sent": "So that's good enough.",
                    "label": 1
                },
                {
                    "sent": "Regardless of the size of your full training set, you can sample fixed amounts of examples.",
                    "label": 0
                },
                {
                    "sent": "And actually be very close to picking the best one, and you can make some heuristic to adapt M as to be slow, small at the beginning, largest at D and while there are some refinements.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take this small example.",
                    "label": 0
                },
                {
                    "sent": "This is the UCI adult data set, finding which people have an income greater than $50,000.",
                    "label": 1
                },
                {
                    "sent": "There are 32,000 examples.",
                    "label": 0
                },
                {
                    "sent": "This Gray line is the accuracy of the regular SVM.",
                    "label": 0
                },
                {
                    "sent": "With this particular hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And this red thing is how you progress when you pick examples randomly.",
                    "label": 0
                },
                {
                    "sent": "Now this green thing is how you progress when you pick 15.",
                    "label": 0
                },
                {
                    "sent": "An example pick the closest to the boundary.",
                    "label": 0
                },
                {
                    "sent": "And learning that one and do that again and again and again.",
                    "label": 0
                },
                {
                    "sent": "And you see, even though you you turn a lot of examples at the beginning without even doing anything with them, just to 11 every 59 examples, it's quite fast and you get a slightly better error.",
                    "label": 0
                },
                {
                    "sent": "And this is actually significant.",
                    "label": 0
                },
                {
                    "sent": "And if you make this simple you Ristic to adjust this sampling rate, you can actually go a lot faster, so you reach the SVM accuracy lot faster than this, which is already faster than SVM.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit bizarre.",
                    "label": 0
                },
                {
                    "sent": "We have a situation where picking examples and not picking the you don't try to pick the example that the most incorrect for the machine you pick.",
                    "label": 0
                },
                {
                    "sent": "Examples are closed.",
                    "label": 0
                },
                {
                    "sent": "The most ambiguous for the machine, and you give that for training and on this noisy data set.",
                    "label": 0
                },
                {
                    "sent": "This is working extra.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "If you look here number of support vectors versus test error.",
                    "label": 1
                },
                {
                    "sent": "The SVM is here as quite the high number of support vectors because all the outliers become support vectors.",
                    "label": 0
                },
                {
                    "sent": "You know SVM outliers have negative margin and they become support vectors.",
                    "label": 0
                },
                {
                    "sent": "If you pick example randomly, you go to the SVM point, you find the SVM solution after after basically one path.",
                    "label": 0
                },
                {
                    "sent": "But if you pick with this active setups where you pick close to the boundary, well, you go to solution that actually better than SVM and this parser.",
                    "label": 0
                },
                {
                    "sent": "And if you keep training eventually you're going to pick up these outliers.",
                    "label": 0
                },
                {
                    "sent": "And so you go something that better and then slowly you go back to the SVM result.",
                    "label": 0
                },
                {
                    "sent": "So basically we found the trajectory to compute the SVM result.",
                    "label": 0
                },
                {
                    "sent": "And some transitory steps actually much better.",
                    "label": 0
                },
                {
                    "sent": "This parcel and SVM.",
                    "label": 0
                },
                {
                    "sent": "And they are the devil.",
                    "label": 0
                },
                {
                    "sent": "Slightly better error rate, even though it's a small effect.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it.",
                    "label": 0
                },
                {
                    "sent": "This is the fact because this curves this active curves by selecting close to the boundary are not going to select outliers.",
                    "label": 0
                },
                {
                    "sent": "Only at the end when there are just outliers left in the data.",
                    "label": 0
                },
                {
                    "sent": "And in fact, you can replicate this result.",
                    "label": 0
                },
                {
                    "sent": "You can try to define the loss that takes you directly here.",
                    "label": 0
                },
                {
                    "sent": "This is what color Bear was done in order did in 2006 that defined the loss, which is not a hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "So basically outliers have no gradient.",
                    "label": 0
                },
                {
                    "sent": "When they do far, and by optimizing that loss they can make an algorithm that fast.",
                    "label": 0
                },
                {
                    "sent": "And that basically stops here.",
                    "label": 0
                },
                {
                    "sent": "So what do you think about convexity?",
                    "label": 0
                },
                {
                    "sent": "Convexity is nice, you have good guarantees about the optimum and everything, but you pay for it.",
                    "label": 0
                },
                {
                    "sent": "You pay by bless Pacitti and a little bit more error.",
                    "label": 0
                },
                {
                    "sent": "It's bizzare people gas, often shocked by this, but this is true.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Let's go back.",
                    "label": 0
                },
                {
                    "sent": "Suppose.",
                    "label": 0
                },
                {
                    "sent": "On suppose you select by the gradient.",
                    "label": 0
                },
                {
                    "sent": "So basically think about an incremental system.",
                    "label": 0
                },
                {
                    "sent": "You take one example, one fresh example.",
                    "label": 0
                },
                {
                    "sent": "You add it and you optimize on that.",
                    "label": 0
                },
                {
                    "sent": "And you take a new example.",
                    "label": 0
                },
                {
                    "sent": "You add it and you optimize on all the examples you have.",
                    "label": 0
                },
                {
                    "sent": "The question is which example to add?",
                    "label": 1
                },
                {
                    "sent": "If you choose the example that has the strongest coefficient in the gradient, it also means in that case, if you do the math, choosing the example for which the output of the system is the most incorrect.",
                    "label": 1
                },
                {
                    "sent": "Now, if you're unlucky enough that you data are some outliers.",
                    "label": 0
                },
                {
                    "sent": "Well, for outliers, the output of the system would be very incorrect because the the actual level you have in your training set is well.",
                    "label": 0
                },
                {
                    "sent": "So you're going to pick this one, so you're going to build a system by adding outliers, and you need to train your system by first showing the outliers.",
                    "label": 0
                },
                {
                    "sent": "This is not good pedagogy.",
                    "label": 0
                },
                {
                    "sent": "So a way to alleviate this is to do this minimax argument, you say.",
                    "label": 0
                },
                {
                    "sent": "I assume when I see an example that I don't know it's class and I say I would like an example that provides a good gradient, even if it's in the worst possible class.",
                    "label": 1
                },
                {
                    "sent": "And that leads you to select the example that closes to the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So in this setup, instead of teaching your system by adding new examples that are very incorrect, you teach your system by adding new examples that are ambiguous.",
                    "label": 0
                },
                {
                    "sent": "So if you have a pool of examples, fixed pool with some outliers from other examples and everything.",
                    "label": 0
                },
                {
                    "sent": "And you first add the ambiguous one.",
                    "label": 0
                },
                {
                    "sent": "You're going to leave the outliers because they're not close to the boundary.",
                    "label": 0
                },
                {
                    "sent": "They completely fell off.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what's happening here.",
                    "label": 0
                },
                {
                    "sent": "Here you start training by taking example the ambiguous, leaving the outliers away.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you keep dreaming when you keep going.",
                    "label": 0
                },
                {
                    "sent": "Eventually, you in the examples you haven't seen yet, you just have the outliers, so you're going to pick outliers, and when you do that, you go back to the SVM solution.",
                    "label": 0
                },
                {
                    "sent": "Close to the boundary.",
                    "label": 0
                },
                {
                    "sent": "Close to the decision boundary that the system currently implements.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So we try to do we try to do that on a large scale problem.",
                    "label": 0
                },
                {
                    "sent": "So there was with Guile loosely and take handwritten digits again, so it's a good database, but this time we do apply on the fly distortions, so we're going to generate about 8.1 million digits by applying distortion to the training set of analysts.",
                    "label": 0
                },
                {
                    "sent": "Are we going to use the RBF kernels and there are some argument this is very difficult problem for local kernels.",
                    "label": 1
                },
                {
                    "sent": "So this is really a tough problem to solve.",
                    "label": 0
                },
                {
                    "sent": "This is not this is the wrong model for this problem basically.",
                    "label": 0
                },
                {
                    "sent": "Potentially you're going to have many support vectors and I would say that this kind of study is more challenging a solution because there is a better solution.",
                    "label": 0
                },
                {
                    "sent": "But basically we have 10 binary classifiers, one class versus the rest.",
                    "label": 0
                },
                {
                    "sent": "We use about 60 gigabytes of memory for this.",
                    "label": 0
                },
                {
                    "sent": "Each classifiers are 8 million examples, so we've been sending 18,000,000 examples in the same SVM.",
                    "label": 0
                },
                {
                    "sent": "The training time is about 8 days and the result is 0 zero point, 6% error on the test error which is.",
                    "label": 0
                },
                {
                    "sent": "Quite please in the good ones for this kind of problem.",
                    "label": 1
                },
                {
                    "sent": "So I think this is probably the largest as VM training on a single processor that was on a single processor.",
                    "label": 0
                },
                {
                    "sent": "No parallel system and basically 80,000,000 examples propels them to the thing.",
                    "label": 0
                },
                {
                    "sent": "And each example gets only one chance to be selected in securely one pass system.",
                    "label": 1
                },
                {
                    "sent": "When we not happy, but one example is not close enough to the boundary, we just keep it and we never come back.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is it good enough?",
                    "label": 0
                },
                {
                    "sent": "Well no.",
                    "label": 0
                },
                {
                    "sent": "If you take one of these old convolution networks, we just took a cigarette and distant.",
                    "label": 0
                },
                {
                    "sent": "This one was done by Patrice Simard in 2003 with about four million examples.",
                    "label": 0
                },
                {
                    "sent": "It trends in two or three hours, two or three hours, and you get a better tester.",
                    "label": 0
                },
                {
                    "sent": "So you're not completely there yet.",
                    "label": 1
                },
                {
                    "sent": "I must say for the defense of the other systems that the other system we use by using RBF kernels and we know they do not like invariances.",
                    "label": 1
                },
                {
                    "sent": "So we know it's the wrong model.",
                    "label": 1
                },
                {
                    "sent": "And they need a lot of memory to cache channel values.",
                    "label": 0
                },
                {
                    "sent": "Yet I said it's more challenging the solution, and the challenge was met that mentor by using techniques to select examples for training, we were able to train SVM of absolutely absurd scale.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's summarize.",
                    "label": 0
                },
                {
                    "sent": "Suppose we do that in large scale.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have plenty of computers.",
                    "label": 0
                },
                {
                    "sent": "They product data.",
                    "label": 0
                },
                {
                    "sent": "And we could have a filtering module that's going to select some examples.",
                    "label": 0
                },
                {
                    "sent": "Send it to another computer that's going to do more filtering.",
                    "label": 0
                },
                {
                    "sent": "Select some example and then to the training machine.",
                    "label": 0
                },
                {
                    "sent": "And there is a slow feedback loop that's going to send back the models with the low frequency to all these machines to do better filtering.",
                    "label": 0
                },
                {
                    "sent": "If you do this.",
                    "label": 0
                },
                {
                    "sent": "Well, all these guys are filtering example close to the decision boundary at a certain time.",
                    "label": 0
                },
                {
                    "sent": "Not so long ago, but it's going to work.",
                    "label": 0
                },
                {
                    "sent": "It's a parallel implementation of, let's say a very, very large VM.",
                    "label": 0
                },
                {
                    "sent": "It can work on the scale or Google scale.",
                    "label": 0
                },
                {
                    "sent": "It said you can have 1000 machines there.",
                    "label": 0
                },
                {
                    "sent": "That's going to select one.",
                    "label": 0
                },
                {
                    "sent": "Thousands of the examples to go for the actual training.",
                    "label": 0
                },
                {
                    "sent": "And you don't have this complicated feedback of usual parallel algorithm.",
                    "label": 0
                },
                {
                    "sent": "Typically what kills parallel algorithms that you have a lot of feedback between the machines that are involved in the computation.",
                    "label": 0
                },
                {
                    "sent": "In that case, the feedback is very loose.",
                    "label": 0
                },
                {
                    "sent": "It's very very light.",
                    "label": 0
                },
                {
                    "sent": "And this actually has.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I sent a petition.",
                    "label": 0
                },
                {
                    "sent": "These are the makers.",
                    "label": 0
                },
                {
                    "sent": "These are the thinkers.",
                    "label": 0
                },
                {
                    "sent": "What we did was push.",
                    "label": 0
                },
                {
                    "sent": "Some of the thinking.",
                    "label": 0
                },
                {
                    "sent": "Into the onto the makers.",
                    "label": 0
                },
                {
                    "sent": "Now the makers do part of the thinking.",
                    "label": 1
                },
                {
                    "sent": "And that way we can solve our initial problems with the makers and the thinkers.",
                    "label": 0
                },
                {
                    "sent": "Under that's my conclusion.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The non convex and that was very very short slide.",
                    "label": 0
                },
                {
                    "sent": "There actually a lot more data on this but.",
                    "label": 0
                },
                {
                    "sent": "What the fuck do not even finding the real global minima is convex?",
                    "label": 0
                },
                {
                    "sent": "When is all these programs for doing convex optimization likely based VMS VM Lite and so on?",
                    "label": 0
                },
                {
                    "sent": "The default tuning and tuning people use their with an accuracy that so calls that you find crap in terms of optimization.",
                    "label": 0
                },
                {
                    "sent": "So well, if I would choose between an algorithm that's going to find something that's close.",
                    "label": 0
                },
                {
                    "sent": "Close to a global minimum and something that's going to find the local minimum, but that's going to perform better.",
                    "label": 0
                },
                {
                    "sent": "Which one do you take?",
                    "label": 0
                },
                {
                    "sent": "It's a practical question.",
                    "label": 0
                },
                {
                    "sent": "I know theoretically is annoying.",
                    "label": 0
                },
                {
                    "sent": "And it makes the math a lot more difficult.",
                    "label": 0
                },
                {
                    "sent": "But the fact is that if in the same time you can find a local minimum in the nonconvex system, that's actually better than the global minimum.",
                    "label": 0
                },
                {
                    "sent": "You could process in the same time, well, you should go with the nonconvex one.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I'd say that's a limitation of my analysis.",
                    "label": 0
                },
                {
                    "sent": "Because in order to do math, I'm very happy to have a single optimum.",
                    "label": 0
                },
                {
                    "sent": "I can compare with.",
                    "label": 0
                },
                {
                    "sent": "But in practice is not necessary like that, and that's a striking example.",
                    "label": 0
                },
                {
                    "sent": "Well, The thing is that I know it doesn't mean it just means that all theoretical tools are not appropriate.",
                    "label": 0
                },
                {
                    "sent": "There might be ways to approach this theoretically, but we don't know how to do it.",
                    "label": 0
                },
                {
                    "sent": "Think about it in machine learning with the beginning.",
                    "label": 0
                },
                {
                    "sent": "No trivial things.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to do.",
                    "label": 0
                },
                {
                    "sent": "Like I when I was giving blood in his talk, I gave the example of.",
                    "label": 0
                },
                {
                    "sent": "Students are always slightly worse than teachers.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Look at all the machine learning things we always have.",
                    "label": 0
                },
                {
                    "sent": "Learning systems that are slightly worse than the thing you feed.",
                    "label": 0
                },
                {
                    "sent": "Give for filling them than the teacher teaching signal the supervised supervision signal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, unless it really is wrong sometimes, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what you say is right, but do we have a clean theoretical framework to address this in the general level?",
                    "label": 0
                },
                {
                    "sent": "Not that much.",
                    "label": 0
                },
                {
                    "sent": "I'm not aware of that much.",
                    "label": 0
                },
                {
                    "sent": "You have some, well, good.",
                    "label": 1
                },
                {
                    "sent": "Yeah, you can do it some some extra some level.",
                    "label": 0
                },
                {
                    "sent": "But the basic.",
                    "label": 0
                },
                {
                    "sent": "Things do the basic thing that we see.",
                    "label": 0
                },
                {
                    "sent": "Theory has nothing about it.",
                    "label": 0
                },
                {
                    "sent": "There's a scandal.",
                    "label": 0
                },
                {
                    "sent": "When you want to learn something, you go into a library.",
                    "label": 0
                },
                {
                    "sent": "Now all our theories about independent examples.",
                    "label": 0
                },
                {
                    "sent": "So you pick an independent book randomly.",
                    "label": 0
                },
                {
                    "sent": "You read it.",
                    "label": 0
                },
                {
                    "sent": "Do I know what I want?",
                    "label": 0
                },
                {
                    "sent": "Not yet.",
                    "label": 0
                },
                {
                    "sent": "So you pick a random book, you read it.",
                    "label": 0
                },
                {
                    "sent": "Do I know what I want?",
                    "label": 0
                },
                {
                    "sent": "Not yet.",
                    "label": 0
                },
                {
                    "sent": "It's not the way you do.",
                    "label": 0
                },
                {
                    "sent": "You start by looking at the networks of books.",
                    "label": 0
                },
                {
                    "sent": "You see how they relate to each other.",
                    "label": 0
                },
                {
                    "sent": "You have help from librarians, and you learn much faster that way.",
                    "label": 0
                },
                {
                    "sent": "What do we know if this almost nothing?",
                    "label": 0
                },
                {
                    "sent": "There are some beginnings, no.",
                    "label": 0
                },
                {
                    "sent": "So people have been thinking about that.",
                    "label": 0
                },
                {
                    "sent": "I don't see that people have not been thinking about this, but do we have a very constructive theory of this?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so we try to do a little piece.",
                    "label": 0
                },
                {
                    "sent": "In that case, the little piece was trying to connect the computational cost and the statistical efficiency and get some some results, but that's only a little piece.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry and just I easily get.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Not as I know.",
                    "label": 0
                },
                {
                    "sent": "Maybe has some things here in the back of his mind, but he didn't tell me.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Not in that case.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a connection, but I'm not aware of it.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "We used this VM because it was convenient.",
                    "label": 0
                },
                {
                    "sent": "We have the code at hand, but you know, in the end it was just an additional term in the cost function.",
                    "label": 0
                },
                {
                    "sent": "I guess you could modify anything to have this.",
                    "label": 0
                },
                {
                    "sent": "Not that I know, but it's not impossible.",
                    "label": 0
                },
                {
                    "sent": "I don't know everything.",
                    "label": 0
                },
                {
                    "sent": "I know that I don't know everything people have done.",
                    "label": 0
                },
                {
                    "sent": "It's not a complicated thing to implement in the end.",
                    "label": 0
                },
                {
                    "sent": "It's just the idea that very bizzare to use.",
                    "label": 0
                },
                {
                    "sent": "OK, well, we do need another category of example, let's dream them.",
                    "label": 0
                },
                {
                    "sent": "Well that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much and.",
                    "label": 0
                }
            ]
        }
    }
}