{
    "id": "qhrphxcgyhq3hocjz5jb2jcwfwiz6cp2",
    "title": "Hierarchical Cost-Sensitive Algorithms For Genome-Wide Gene Function Prediction",
    "info": {
        "author": [
            "Nicol\u00f2 Cesa-Bianchi, University of Milan"
        ],
        "published": "Oct. 5, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlsb09_cesa_bianchi_hcsa/",
    "segmentation": [
        [
            "OK. Hi so the the story of this work is the following few years ago I developed some methods for hierarchical classification in text.",
            "Where you have a large taxonomies of categories for for instance new items and George and Mike Auto read these papers and said, well, well that's sounds interesting why don't we apply them to do function prediction?",
            "So we started to look at.",
            "The problem of.",
            "Uh, of seeing how this method could improve on the current state of the art for hierarchy classification in the domain of gene function."
        ],
        [
            "Action.",
            "So I don't think I have to spend much time on motivating this.",
            "The approach of.",
            "Automatic classification for gene function prediction.",
            "There are by now huge amounts of genomic data that need some good accurate techniques for automatic classification in order to drive the biological validation of that of that functions.",
            "And so from the point of view of.",
            "And the machine learning problem of hierarchical classification.",
            "The gene function prediction has a certain number of distinctive features that are important.",
            "First of all, there are quite a big number of functional classes that you need to classify your genes into and for instance, we worked on the funk at taxonomy that has hundreds of these classes and further."
        ],
        [
            "Or D. Each gene has a multiple annotation, so that corresponds to different functions implemented by the gene, and so you have a problem of multiple classification into the taxonomy.",
            "And Furthermore, the dis annotations are usually sparse, so each gene has maybe a few like 34567 annotations in the taxonomy and you have to find those in a set of hundreds of classes.",
            "So these are the three main features that we tried to address in this work, although there are more aspects that are certainly interesting.",
            "Interesting in this application.",
            "For instance, the fact that annotations don't have the same degree of accuracy of a certainty, and typically annotations that go deep in the taxonomy become less and less certain.",
            "So you might try to incorporate this confidence information.",
            "Confidence information into your classifier, which we didn't yet, and the second extreme interesting aspect is the fact that you might have for each gene and multiple sources of data, so different feature sets, and you might want to capital a mechanism for hierarchical classification with the mechanisms for data integration.",
            "So in order to improve your overall accuracy.",
            "And this is also work that we plan to address in the future.",
            "OK, so this is to give you OK you can see it here."
        ],
        [
            "So this is the excerpt of the.",
            "Fun cat taxonomy for the class is relevant.",
            "Relevant to the East.",
            "Species so we look at East classification of yeast genes and there are about 200 functional classes that are spread across 5 erotical levels and.",
            "In the within the data set with two code, there were about 6000 genes.",
            "Each of them corresponded to a different set of annotations into this hierarchy.",
            "So now the problem is to come up with a method for associating the correct set of annotations to each gene in this data set OK.",
            "So a way to sort of a.",
            "And come up with a.",
            "A nice description of the problem is to take the transitive closure of the annotation, so we."
        ],
        [
            "Now we think about we talk about a multi label for a gene, which is the set of annotations which are the ones at the bottom.",
            "This one and that one plus their transitive closure up to the root, which is a fake domino that closes everything.",
            "So now you can talk about multi label associated to a gene which is a set of paths in this taxonomy and these paths are maybe several for each gene and they need not.",
            "Terminate at the leaf of the taxonomy.",
            "They might end up before because the annotation was done.",
            "The deepest annotation just ended at that level, so this is the kind of objects these are prunings of.",
            "The whole taxonomy, and these are the things kind of things that we'd like to.",
            "Anne.",
            "Output for with our mechanism.",
            "OK, so."
        ],
        [
            "The there are several approaches in machine learning that can tackle the problem of Iraqi classification in taxonomy, and we consider it in this work.",
            "Ensemble methods.",
            "These are methods kind of intuitive, in which you Simply put a binary classifier in each node of the taxonomy, and we in particular we're interested in probabilistic binary classifiers that are able to output on each.",
            "Input gene X probability P."
        ],
        [
            "That is the associated to the probability that that gene is an annotated with that class.",
            "Thank you into account, the transitive closure up to the root OK, and so the idea here is that we wanted to compare different.",
            "An methods based on this.",
            "Underlying a set of classifiers and for the underlying set of classifiers, we chose a sort of standard baseline solution, which is we train at each node using a calibrated SVM's with Gaussian kernels.",
            "OK, so now the problem.",
            "The basic problem we face with we faced with is the following.",
            "So we are given a data set.",
            "We train our ensemble method.",
            "That is we train each classifier for each node and we come up with a set of probabilities given H given each gene X and we want a method to derive the correct multi label.",
            "Given these numbers, Pi OK, so this is the kind of decoding or reconstruction that we want to use and which is the basic philosophy of all these ensemble methods.",
            "So now it's easy to think of a baseline here.",
            "So what's the simplest thing you can do here?",
            "Well, it's it's that's really easy.",
            "The simplest thing is the following.",
            "You just, you know, you get a gene.",
            "You ask each node that you get a Pi."
        ],
        [
            "That gene, and then you just say 1 four?",
            "An if the, let's say if the Pi of an order is above a certain threshold and 0 otherwise, and then since you know that these taxonomies respected the true path rule, meaning that the final multi label must be.",
            "Must be must must respect the the taxonomy in the sense that must be the multi label produced by transitive closure of a certain annotations up to the root.",
            "So you shut down.",
            "For instance this node that node because there if you assign label one to those nodes that are above the threshold, you won't be respecting the true patrol.",
            "OK, so this is a very simple method, so you.",
            "In order to address the problem that these multi labels are sparse.",
            "You might want to introduce a con sensitive parameter which is a tunable threshold which is common for simplicity.",
            "Just a single parameter threshold which is common to all nodes and then you cross validate on those on that threshold in order to find the best accuracy.",
            "This is what we did, and this is sort of we call the cost sensitive hierarchical top down method.",
            "So again, you must you must be."
        ],
        [
            "Where here that in all this in I will I will describe two methods.",
            "This one is another one.",
            "And all these methods are based on the same peas and the same.",
            "These peas are based on independent training on the nodes.",
            "OK, so the notes training independently.",
            "They're not really independent independent because we train them on the same data set.",
            "But the peas are produced just by independent training of those nodes.",
            "Apart from this, common data set issue.",
            "OK.",
            "So.",
            "This is the same sense, a method that tries to."
        ],
        [
            "Exploit this Pi and uses the least taxonomical information so the only taxonomic information I want to use is the fact that my final labeling must respect the true patrol.",
            "So I'm shutting off the notes that like this one and that one that won't respect one producer.",
            "A valid labeling.",
            "OK, so this is really simple and so you might think OK, what's the next here?",
            "What's the next bit more sophisticated method than this one?",
            "How can I go beyond this very simple approach?",
            "So one thing we do in a usually machine learning, if you have a structural problem like this one when you want to classify data into a taxonomy, you come up with the finish."
        ],
        [
            "Of loss which is suitable for your problem and then you derive a sort of optimal algorithm for that loss.",
            "OK so.",
            "An there are several notion of losses that can be defined over the over taxonomies and the previous method is not based on any.",
            "Especially on any notion of loss is just a simple baseline.",
            "Here.",
            "I want to come up with the notion of loss, which is good for taxonomies and also allows me to derive an efficient algorithm for reconstructing the multi label.",
            "So this is a notion of loss which is called H loss, which used for text classification, and the idea is this you have on the left guess Multi Label, a predicted multi label for a certain gene and that's the true multi label for the same gene.",
            "And now you can compare this the blue and yellow and look at the differences you see there are and the way you look at differences the following you take every path, let's say of the true Multi label.",
            "And you see whether that path is matched by the path of the guest multilabel.",
            "And whenever you see a mismatch which is in that for the first path on the left hand side over there.",
            "This one here.",
            "There's a mismatch here.",
            "Sorry here and so you count 1 mistake here and then there is another path here and there's a miss.",
            "This mismatch here and here, and so you come to mistakes and for every path whenever you find a mistake, you don't count the mistakes made in the subtree rooted at the first mistake.",
            "So in particular you want to count the mistake over here.",
            "So this is another difference.",
            "But this is not.",
            "Counted that by the each loss because it's already accounted for by the mistake made today at the higher spot on the same path.",
            "So basically, if you make them, if you make a mistake at some point, you discount the mistakes made on the subtree.",
            "OK, so this is a reasonable notion of loss for classification, and in this case the H loss for between this and that is 3 because of this reason.",
            "So now you might wonder whether.",
            "If you can come up with an algorithm that minimizes this loss based on a set of probability assignments, pipipi APN which we had before, and that's actually kind of easy to come up with, so let's use this to denote the H loss between against multi."
        ],
        [
            "Evil why and some true multi Label V and we have these note predictions.",
            "Probabilistic predictions for each individual node which got by training our SVM's.",
            "And that we predict now the label yhat, which minimizes the suspected H loss with respect to a random multi label W which is generated based on these pies.",
            "So we interpret those peas as the true probability of some underlying probabilistic model for generation of multi labels and we minimize the H lot with respect to this.",
            "Underlying probabilistic model and the underlying pricing model is utterly simple.",
            "Basically, the probability that label in node gets assigned the label one conditioned on the on his parent is \u03c0 if its parent is 1.",
            "Or zero if his parent is zero.",
            "Clearly fits pretty zero by the true patrol.",
            "The label must be 0 for the any children.",
            "OK, and you can compute this in time linear in the number of nodes by using a simple message passing algorithm.",
            "And it turns out that this is the beige optimal assignment given these probabilities in this model."
        ],
        [
            "So this is nice and the way you do it is by a bottom up procedure in which essentially.",
            "The idea is is the following.",
            "If you start from the leaf of your taxonomy and the leaf is assigned one simply by looking at the.",
            "By comparing apply with the with the half, so no no threshold.",
            "This is the optimal assignment for the leaves.",
            "And then each node I sensor is parented.",
            "Expected that lodge each loss of his subtree.",
            "So essentially for any node up here, the decision whether to get enable one or label zero is based is based on comparing the expected.",
            "A loss.",
            "Which the node would incur if it sets labeled 0, which is this one.",
            "So if I this node sets labeled zero and he has probability P of.",
            "The underlying probabilistic model assigning him label one.",
            "That's the expected loss of having labels 0.",
            "If it's for despective loss, for one, is the loss of the node DH lost or the note for one plus the potential expected H loss that the node would have in the subtree.",
            "So you remembered each loss.",
            "If you make a mistake here, then.",
            "In the.",
            "Sorry if if this notice decides on zero then this will be 0 by the true patrols and won't be any loss here.",
            "If they know this one then some of these by get labeled one.",
            "So there might be some further loss over here.",
            "OK, so this is a rule that.",
            "Actually implements this.",
            "This argument over here and it's bottom up linear, so you can do it very quickly and it's you end up with an assignment which is valid."
        ],
        [
            "So if there's a very simple way to make this rule cost sensitive, you simply give a coefficient Alpha to the cost of a false false negative, as opposed to a false positive.",
            "OK, so in this case in the previous example I gave you, there were two false negative mistakes cause these were positive and the predicted labeling was negative and the false positive, and so I instead of counting tree.",
            "As each loss I count twice Alpha, plus one where Alpha is a cost of.",
            "Air Force positing as a false positive mistake.",
            "Sorry, false negative mistake.",
            "OK, so in this case again I can introduce a good parameter in order to essentially tradeoff between precision.",
            "Recall.",
            "OK, so and the whole thing won't change match, so this will be change in by introducing the Alpha coefficient will appear here and there, but there won't be much difference.",
            "OK so.",
            "Now, unfortunately, what you?"
        ],
        [
            "This this is interesting because it is a simple way to derive an algorithm that gets you minimizes a meaningful notion of loss, although is not exactly the notion of loss, which is the most relevant for taxonomical classification with sparse labelings.",
            "So one if you have a taxonomy with sparse multi labels then something that is actually able to.",
            "Measure in a in a in a reasonable, accurate way that the true performance of your classifier is.",
            "What is called hierarchical precision recall, which is used as being used?",
            "Previously in in the biomedical literature, and this is.",
            "Is this sort of a simple way of generalizing precision or recall?",
            "OK, so this is the actual measure of performance that we used for in our experiments.",
            "And the problem here is that it is hard to minimize directly this one on a taxonomy, so it's already harder to do it in the binary case.",
            "In the hierarchical case it's even harder, so we used different losses like like the H loss as a proxy for this one.",
            "OK, so this one is as simple as this, so the again the blue is the predicted and the yellow is the true multi label and the precision.",
            "Is you look at each?",
            "Predicted path and you look at the true path, the extent by which the true path covers the predicted path.",
            "So this is 1 this is covered.",
            "This blue path is covered 100% by the yellow path and this blue path is covered 50% by the yellow path.",
            "So you get one place plus 1/2 which are the coverings.",
            "And then the order by two because we had two paths and you do the same thing for recall.",
            "So look at the stand in which the.",
            "Yellow path are covered by the blue path, so here you have a yellow path which is covered the 1/3 here by the blue path and then here you have a yellow path which is covered a half by the blue path.",
            "So 1/3 + 1/2 / 2 because we have to pass again and then you can compute that measures here.",
            "So this is pretty natural.",
            "You can do it.",
            "It's hard to optimize directly though.",
            "OK."
        ],
        [
            "So we look at different feature sets for the East data set of 600 genes and these are descriptions of the feature sets and, but I won't spend much time on that, and these are the."
        ],
        [
            "Resulting number of genes, so these are subsets of these.",
            "Those 6000 genes yeast genes that we were started on we started from and these are the number of features that are associated to each data sets and these are the number of classes number class is about 200, but they're not all the same becausw we selected all the genes that had at least 20 annotations for each data set.",
            "So that's why you have we have a varying number of classes.",
            "OK. Now."
        ],
        [
            "So it's before giving you the results.",
            "It's interesting to look at the sensitivity of this of the trade of the parameters for the algorithms I just showing I'm just showing you the.",
            "A sensitivity of the cost factor that Alpha which tells you how much does a false negative cost on the hierarchical basian algorithm.",
            "OK, so this is more complicated algorithm and you see here.",
            "So this is the cost factor.",
            "One is neutral, so false positive, false negative cost the same.",
            "And this is done on a specific data set.",
            "I don't remember which one, and so this is the precision which goes up and then goes down.",
            "And this is the recall which goes monotonically up and the F measure is over here.",
            "So you see, there's a region in which they pretty much there, pretty much balanced, and this is the steady performance of the hierarchical top down without cost sensitive measures.",
            "Just to give you a baseline without cost sensitive correction.",
            "OK, so.",
            "See there is there are some issue issue here, so it's some extent if you can tradeoff between precision recall, but it's it's not clear.",
            "For instance why you have this specific, why you have this behavior for precision and different behavior from them for the recall.",
            "OK.",
            "So recall goes up monotonically.",
            "Precision has a as a.",
            "Best shape.",
            "OK."
        ],
        [
            "Now let's let's look at results.",
            "So results are the problem is OK.",
            "The bottom line is this.",
            "So basically we don't.",
            "We didn't find that on average a specific advantage of using the more sophisticated by Asian method, although there are differences, so there are differences in behavior OK, and these differences in behavior an indication that something is something different is going on the the two methods are not doing the same thing there optimizing different that there coming up with different ways of computing a multi label, and so we're hoping to.",
            "Understand better and capitalize on those differences so.",
            "But the bottom line, sadly, is that you have exactly the same average performance overall data sets between the two methods, so this is hierarchical hierarchical top down without cost sensitive correction, and this is with consecutive correction, and this is each base.",
            "So both of these algorithms have a single parameter.",
            "This is the Tao for CS for hierarchical top down, and you have the Alpha, the.",
            "Trade off for the hierarchical base.",
            "And if you look at the significance of these differences, you see there's a OK. Hierarchical base with twice and there are four ties, but of course this is kind of disappointing because pretty much they behave the same.",
            "They do behave better than top down without cost sensitive correction."
        ],
        [
            "So now you you get some more insights if you look at the behavior across the levels of the taxonomy.",
            "So now we look at the five levels of the taxonomy.",
            "OK, the five depths, and we compute the F measure standard F measure macro averaged among the classes of that level and first of all you see a common trend, which is the the F measure decreases as you go down in the in the hierarchy, meaning that the problems become harder and harder.",
            "As the notes are deeper in the taxonomy and this is probably an indication, the fact of the fact that the annotations are less and less dependable and another phenomenon which is not very clear from this thing, but which I tell you, is that OK if you look at the simple flat classification.",
            "So what you do here is just that you do a thresholding of your.",
            "Let's go back here.",
            "OK, so you do a simple thresholding at the."
        ],
        [
            "After we have some probabilistic thresholding, very simple and then you don't do any correction here, you don't account for the true patrol, but you leave just the labels.",
            "OK, and what you get here is a recent very."
        ],
        [
            "The recall and the bad precision.",
            "Now the mode.",
            "Now if you move to more and more informed methods, so this is a method that uses somehow the Sonic the taxonomical information but not so much.",
            "This is a method that uses a lot that tries to come up with a probabilistic hierarchical model for the generation and optimizing that model.",
            "OK, as you move from flat to more sophisticated methods the recall goes down and precision goes up.",
            "So your essentially the algorithm becomes more conservative becausw the taxonomical information is.",
            "Tends to produce shorter annotations in the taxonomy, so the multi labels are sparser.",
            "The algorithm becomes in it becomes very conservative in a way.",
            "The this conservativeness we try to address it by introducing this this can sensitive parameters for instance in each base.",
            "It turns out that the Gen in general a good value of this cost sensitive parameter is 5 which means that you pay five times less a false negative."
        ],
        [
            "False positive, so we really want to make the algorithm or aggressive even though it must take into account taxonomical information.",
            "However, it's somehow the simpler method achieves a better trade off than a more complicated."
        ],
        [
            "OK, although we see that we have a high on average, we have a higher precision at all levels.",
            "Then each base has a higher precision than HDD, so there is some room for maneuvering here, although it's not clear what kind of permit written parameterization of this will get better results.",
            "So to conclude.",
            "We definitely see that.",
            "Unscented these methods."
        ],
        [
            "Are extremely important in order to get good performance on such sparse annotations and hierarchical precision recall are definitely a match.",
            "In assessable measure to user which provides a lot of information on the performance of the algorithm and unfortunately we didn't, we didn't need to succeed in having complex method perform better as a similar ones, although they have different notions they have different.",
            "They perform different classifications.",
            "And one one possibility is that division method gets overfit by the noise in the annotation of the lower levels.",
            "OK, and no method.",
            "None of the two methods we use, the yeah is actually designed to optimize the true performance measure, which is the hierarchical precision, recall and what we're trying to do now is to use a symmetric kernels that actually are able to compute our radical or closely computer radical precision recall, and then you can enclose this symmetric kernels in a tensor in tensor product support vector machine, which is.",
            "Is not an example and I thought this is completely different approach, but at least you should be able to come up with an algorithm that is much more connected to the actual loss that you are trying to minimize.",
            "And another thing is of course which are working on which is very tempting, is to combine these assembled hierarchical classification methods with data integration methods in order to.",
            "Really use up all the information you have available.",
            "OK.",
            "Thank you, that's it.",
            "Biologist I would value much more highly predictions at the bottom.",
            "Value.",
            "That's why I think it's a 2 performance measure.",
            "Is that what you're doing?",
            "No.",
            "Great for utility projectiles at the bottom.",
            "Useless.",
            "Well, help me.",
            "Experiments I know I know.",
            "OK, the idea of these measures is to give you.",
            "Flexibility in sense that you might not get it, but you might get close to it.",
            "You might.",
            "You might not get the exact annotation, but maybe you are in the right sub subtree.",
            "And so these measures will give you will reward predictions that maybe are in the right subtree at the certain level, although they don't really get to the right annotation.",
            "So.",
            "Alternative is to say OK.",
            "Either you get me, I just count.",
            "I just take the symmetric difference between the correct annotation and the wrong annotation.",
            "OK, so the predicted annotation and that rotations I take the symmetric difference is two sets that would be at the bottom level.",
            "OK, that will be sort of a hard.",
            "Hard measure of.",
            "Yeah, so the problem is that as you go down the problems becomes much much harder.",
            "So yeah, yeah so.",
            "But you have to add the algorithm somehow.",
            "So if you insist on putting a lot of weight on the harder problems.",
            "Be the right way to really get.",
            "I agree that it's it's more interesting.",
            "The fact that it's harder makes it probably.",
            "Makes it unclear whether you should really.",
            "Put all of your cost down there.",
            "That's my point.",
            "OK, now I totally agree with you that that's the most interesting information I'm trying to get that we get some kind of information that might be not at the level of not down at the bottom level, but still may be useful.",
            "Yes.",
            "Price.",
            "Protein sequences and then derive some features from it.",
            "Yeah, there were feature sets over here.",
            "Yeah, this is.",
            "Yeah.",
            "The main features."
        ],
        [
            "OK, I'm you should ask my Co author who's not there any any detailed information on those or you can just check the paper and.",
            "These are the yeah."
        ],
        [
            "So.",
            "Yeah, that's right.",
            "System properties.",
            "Right so yeah.",
            "Picture was about was that about trying to overcome that issue, so the.",
            "Did the dealership, the underlying binary classifiers are fixed, so the idea is that you have these probabilities and then what you do with this probabilities.",
            "So we had to translate these probabilities into a multi label which respects the true patrol.",
            "So that's the basic.",
            "Problem and then you have some notion of loss that you compare this and the notion of loss we chose is this hierarchical precision recall, which is good for such sparse annotations, but unfortunately we don't have any direct method to optimize that one.",
            "So we are using proxies like this H loss.",
            "So.",
            "OK OK OK it does that in does that in a way becausw for instance all the negative examples that are used to train each node.",
            "Are those examples which are positive for the father for the parent in negative for the node itself?",
            "So this is a way which is this way of training is consistent with the probabilistic model I am assuming for this age last thing, so there's a by I was trying to decouple the problem of coming up a good estimates of nodes and then use those estimates to construct a multi label.",
            "So this is the way the coupling the problem.",
            "But you're definitely right in saying that.",
            "I am sort of an organic compound way of looking at the problem at once.",
            "Might be much more effective, although I have no idea, it's more complicated.",
            "Yeah, we use this calibrated SVM's and we try to do the calibration as best as possible.",
            "But you're right.",
            "And yes, yes, this is totally true.",
            "I admit I was not so much.",
            "Interested in in.",
            "Well, it no, you're right.",
            "I cannot say you're right.",
            "That would be definitely a better thing to do.",
            "We use SVM just out of convenience.",
            "There he was.",
            "He was.",
            "Yes, I've been very much, yeah yeah yeah.",
            "So this is a quite a large number of systems that try to do such hierarchical function prediction.",
            "Was wondering how this relates because some of these systems purposes they give projected first predict nodes on a higher level and then given that something gets a positive prediction for the higher level, they would make predictions on the lower level.",
            "This is exactly what this thing does.",
            "This is exactly that.",
            "So how does this compare them too?"
        ],
        [
            "Well, things like Bayesian.",
            "There's still approaches that have been used.",
            "Yeah, we right?",
            "So this exactly top down is because you can view this as a top down.",
            "As long as you get a 0 prediction then you stop looking at the subtree.",
            "So that's the way it does it.",
            "And yeah, we are in the process of establishing a benchmark of data set that allows us to make comparison because.",
            "It's it's hard.",
            "I mean, we found it hard to come up with the with the to make a comparison without reimplementing the algorithm of the paper.",
            "We want to compare 2, which is a practice that is not very good.",
            "So yeah, yeah yeah, this is.",
            "This is true.",
            "We definitely need to relate better to the other methods.",
            "Yes.",
            "Dependence on the depth of the tree.",
            "No, you shouldn't be surprised because I did it, but I didn't tell you, and the way I did it is in the.",
            "OK, in the evaluation of the label the costs go down OK.",
            "The costs at the lower levels go down a certain rate that depends on the fan out of the tree.",
            "However, we didn't take.",
            "We didn't use this costs in the evaluation in the in the performance measure.",
            "So to build the label views this cost.",
            "Otherwise the algorithm would be really, really conservative, so this decreasing costs are way too encouraging.",
            "The argument to go down and then we use the game.",
            "This cost sensitive parameter Alpha to tell the algorithm look first negative is going only to cost you a fifth of the false positive.",
            "So go down.",
            "Yet it's it's.",
            "It's really hard to strike the break even point between precision recall.",
            "I mean, maybe you need more parameters.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Hi so the the story of this work is the following few years ago I developed some methods for hierarchical classification in text.",
                    "label": 0
                },
                {
                    "sent": "Where you have a large taxonomies of categories for for instance new items and George and Mike Auto read these papers and said, well, well that's sounds interesting why don't we apply them to do function prediction?",
                    "label": 0
                },
                {
                    "sent": "So we started to look at.",
                    "label": 0
                },
                {
                    "sent": "The problem of.",
                    "label": 0
                },
                {
                    "sent": "Uh, of seeing how this method could improve on the current state of the art for hierarchy classification in the domain of gene function.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action.",
                    "label": 0
                },
                {
                    "sent": "So I don't think I have to spend much time on motivating this.",
                    "label": 0
                },
                {
                    "sent": "The approach of.",
                    "label": 0
                },
                {
                    "sent": "Automatic classification for gene function prediction.",
                    "label": 1
                },
                {
                    "sent": "There are by now huge amounts of genomic data that need some good accurate techniques for automatic classification in order to drive the biological validation of that of that functions.",
                    "label": 1
                },
                {
                    "sent": "And so from the point of view of.",
                    "label": 1
                },
                {
                    "sent": "And the machine learning problem of hierarchical classification.",
                    "label": 0
                },
                {
                    "sent": "The gene function prediction has a certain number of distinctive features that are important.",
                    "label": 0
                },
                {
                    "sent": "First of all, there are quite a big number of functional classes that you need to classify your genes into and for instance, we worked on the funk at taxonomy that has hundreds of these classes and further.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or D. Each gene has a multiple annotation, so that corresponds to different functions implemented by the gene, and so you have a problem of multiple classification into the taxonomy.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, the dis annotations are usually sparse, so each gene has maybe a few like 34567 annotations in the taxonomy and you have to find those in a set of hundreds of classes.",
                    "label": 0
                },
                {
                    "sent": "So these are the three main features that we tried to address in this work, although there are more aspects that are certainly interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting in this application.",
                    "label": 0
                },
                {
                    "sent": "For instance, the fact that annotations don't have the same degree of accuracy of a certainty, and typically annotations that go deep in the taxonomy become less and less certain.",
                    "label": 0
                },
                {
                    "sent": "So you might try to incorporate this confidence information.",
                    "label": 0
                },
                {
                    "sent": "Confidence information into your classifier, which we didn't yet, and the second extreme interesting aspect is the fact that you might have for each gene and multiple sources of data, so different feature sets, and you might want to capital a mechanism for hierarchical classification with the mechanisms for data integration.",
                    "label": 1
                },
                {
                    "sent": "So in order to improve your overall accuracy.",
                    "label": 1
                },
                {
                    "sent": "And this is also work that we plan to address in the future.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is to give you OK you can see it here.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the excerpt of the.",
                    "label": 0
                },
                {
                    "sent": "Fun cat taxonomy for the class is relevant.",
                    "label": 0
                },
                {
                    "sent": "Relevant to the East.",
                    "label": 0
                },
                {
                    "sent": "Species so we look at East classification of yeast genes and there are about 200 functional classes that are spread across 5 erotical levels and.",
                    "label": 0
                },
                {
                    "sent": "In the within the data set with two code, there were about 6000 genes.",
                    "label": 1
                },
                {
                    "sent": "Each of them corresponded to a different set of annotations into this hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So now the problem is to come up with a method for associating the correct set of annotations to each gene in this data set OK.",
                    "label": 0
                },
                {
                    "sent": "So a way to sort of a.",
                    "label": 0
                },
                {
                    "sent": "And come up with a.",
                    "label": 0
                },
                {
                    "sent": "A nice description of the problem is to take the transitive closure of the annotation, so we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we think about we talk about a multi label for a gene, which is the set of annotations which are the ones at the bottom.",
                    "label": 0
                },
                {
                    "sent": "This one and that one plus their transitive closure up to the root, which is a fake domino that closes everything.",
                    "label": 0
                },
                {
                    "sent": "So now you can talk about multi label associated to a gene which is a set of paths in this taxonomy and these paths are maybe several for each gene and they need not.",
                    "label": 0
                },
                {
                    "sent": "Terminate at the leaf of the taxonomy.",
                    "label": 0
                },
                {
                    "sent": "They might end up before because the annotation was done.",
                    "label": 0
                },
                {
                    "sent": "The deepest annotation just ended at that level, so this is the kind of objects these are prunings of.",
                    "label": 0
                },
                {
                    "sent": "The whole taxonomy, and these are the things kind of things that we'd like to.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Output for with our mechanism.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The there are several approaches in machine learning that can tackle the problem of Iraqi classification in taxonomy, and we consider it in this work.",
                    "label": 0
                },
                {
                    "sent": "Ensemble methods.",
                    "label": 0
                },
                {
                    "sent": "These are methods kind of intuitive, in which you Simply put a binary classifier in each node of the taxonomy, and we in particular we're interested in probabilistic binary classifiers that are able to output on each.",
                    "label": 1
                },
                {
                    "sent": "Input gene X probability P.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is the associated to the probability that that gene is an annotated with that class.",
                    "label": 0
                },
                {
                    "sent": "Thank you into account, the transitive closure up to the root OK, and so the idea here is that we wanted to compare different.",
                    "label": 0
                },
                {
                    "sent": "An methods based on this.",
                    "label": 0
                },
                {
                    "sent": "Underlying a set of classifiers and for the underlying set of classifiers, we chose a sort of standard baseline solution, which is we train at each node using a calibrated SVM's with Gaussian kernels.",
                    "label": 1
                },
                {
                    "sent": "OK, so now the problem.",
                    "label": 0
                },
                {
                    "sent": "The basic problem we face with we faced with is the following.",
                    "label": 0
                },
                {
                    "sent": "So we are given a data set.",
                    "label": 0
                },
                {
                    "sent": "We train our ensemble method.",
                    "label": 0
                },
                {
                    "sent": "That is we train each classifier for each node and we come up with a set of probabilities given H given each gene X and we want a method to derive the correct multi label.",
                    "label": 1
                },
                {
                    "sent": "Given these numbers, Pi OK, so this is the kind of decoding or reconstruction that we want to use and which is the basic philosophy of all these ensemble methods.",
                    "label": 0
                },
                {
                    "sent": "So now it's easy to think of a baseline here.",
                    "label": 0
                },
                {
                    "sent": "So what's the simplest thing you can do here?",
                    "label": 0
                },
                {
                    "sent": "Well, it's it's that's really easy.",
                    "label": 0
                },
                {
                    "sent": "The simplest thing is the following.",
                    "label": 0
                },
                {
                    "sent": "You just, you know, you get a gene.",
                    "label": 0
                },
                {
                    "sent": "You ask each node that you get a Pi.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That gene, and then you just say 1 four?",
                    "label": 0
                },
                {
                    "sent": "An if the, let's say if the Pi of an order is above a certain threshold and 0 otherwise, and then since you know that these taxonomies respected the true path rule, meaning that the final multi label must be.",
                    "label": 0
                },
                {
                    "sent": "Must be must must respect the the taxonomy in the sense that must be the multi label produced by transitive closure of a certain annotations up to the root.",
                    "label": 0
                },
                {
                    "sent": "So you shut down.",
                    "label": 0
                },
                {
                    "sent": "For instance this node that node because there if you assign label one to those nodes that are above the threshold, you won't be respecting the true patrol.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very simple method, so you.",
                    "label": 0
                },
                {
                    "sent": "In order to address the problem that these multi labels are sparse.",
                    "label": 0
                },
                {
                    "sent": "You might want to introduce a con sensitive parameter which is a tunable threshold which is common for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Just a single parameter threshold which is common to all nodes and then you cross validate on those on that threshold in order to find the best accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is what we did, and this is sort of we call the cost sensitive hierarchical top down method.",
                    "label": 0
                },
                {
                    "sent": "So again, you must you must be.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where here that in all this in I will I will describe two methods.",
                    "label": 0
                },
                {
                    "sent": "This one is another one.",
                    "label": 0
                },
                {
                    "sent": "And all these methods are based on the same peas and the same.",
                    "label": 0
                },
                {
                    "sent": "These peas are based on independent training on the nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the notes training independently.",
                    "label": 0
                },
                {
                    "sent": "They're not really independent independent because we train them on the same data set.",
                    "label": 0
                },
                {
                    "sent": "But the peas are produced just by independent training of those nodes.",
                    "label": 0
                },
                {
                    "sent": "Apart from this, common data set issue.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the same sense, a method that tries to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exploit this Pi and uses the least taxonomical information so the only taxonomic information I want to use is the fact that my final labeling must respect the true patrol.",
                    "label": 0
                },
                {
                    "sent": "So I'm shutting off the notes that like this one and that one that won't respect one producer.",
                    "label": 0
                },
                {
                    "sent": "A valid labeling.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really simple and so you might think OK, what's the next here?",
                    "label": 0
                },
                {
                    "sent": "What's the next bit more sophisticated method than this one?",
                    "label": 0
                },
                {
                    "sent": "How can I go beyond this very simple approach?",
                    "label": 0
                },
                {
                    "sent": "So one thing we do in a usually machine learning, if you have a structural problem like this one when you want to classify data into a taxonomy, you come up with the finish.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of loss which is suitable for your problem and then you derive a sort of optimal algorithm for that loss.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "An there are several notion of losses that can be defined over the over taxonomies and the previous method is not based on any.",
                    "label": 0
                },
                {
                    "sent": "Especially on any notion of loss is just a simple baseline.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "I want to come up with the notion of loss, which is good for taxonomies and also allows me to derive an efficient algorithm for reconstructing the multi label.",
                    "label": 0
                },
                {
                    "sent": "So this is a notion of loss which is called H loss, which used for text classification, and the idea is this you have on the left guess Multi Label, a predicted multi label for a certain gene and that's the true multi label for the same gene.",
                    "label": 0
                },
                {
                    "sent": "And now you can compare this the blue and yellow and look at the differences you see there are and the way you look at differences the following you take every path, let's say of the true Multi label.",
                    "label": 0
                },
                {
                    "sent": "And you see whether that path is matched by the path of the guest multilabel.",
                    "label": 0
                },
                {
                    "sent": "And whenever you see a mismatch which is in that for the first path on the left hand side over there.",
                    "label": 0
                },
                {
                    "sent": "This one here.",
                    "label": 0
                },
                {
                    "sent": "There's a mismatch here.",
                    "label": 0
                },
                {
                    "sent": "Sorry here and so you count 1 mistake here and then there is another path here and there's a miss.",
                    "label": 0
                },
                {
                    "sent": "This mismatch here and here, and so you come to mistakes and for every path whenever you find a mistake, you don't count the mistakes made in the subtree rooted at the first mistake.",
                    "label": 0
                },
                {
                    "sent": "So in particular you want to count the mistake over here.",
                    "label": 0
                },
                {
                    "sent": "So this is another difference.",
                    "label": 0
                },
                {
                    "sent": "But this is not.",
                    "label": 0
                },
                {
                    "sent": "Counted that by the each loss because it's already accounted for by the mistake made today at the higher spot on the same path.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you make them, if you make a mistake at some point, you discount the mistakes made on the subtree.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a reasonable notion of loss for classification, and in this case the H loss for between this and that is 3 because of this reason.",
                    "label": 0
                },
                {
                    "sent": "So now you might wonder whether.",
                    "label": 0
                },
                {
                    "sent": "If you can come up with an algorithm that minimizes this loss based on a set of probability assignments, pipipi APN which we had before, and that's actually kind of easy to come up with, so let's use this to denote the H loss between against multi.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evil why and some true multi Label V and we have these note predictions.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic predictions for each individual node which got by training our SVM's.",
                    "label": 0
                },
                {
                    "sent": "And that we predict now the label yhat, which minimizes the suspected H loss with respect to a random multi label W which is generated based on these pies.",
                    "label": 0
                },
                {
                    "sent": "So we interpret those peas as the true probability of some underlying probabilistic model for generation of multi labels and we minimize the H lot with respect to this.",
                    "label": 0
                },
                {
                    "sent": "Underlying probabilistic model and the underlying pricing model is utterly simple.",
                    "label": 0
                },
                {
                    "sent": "Basically, the probability that label in node gets assigned the label one conditioned on the on his parent is \u03c0 if its parent is 1.",
                    "label": 0
                },
                {
                    "sent": "Or zero if his parent is zero.",
                    "label": 0
                },
                {
                    "sent": "Clearly fits pretty zero by the true patrol.",
                    "label": 0
                },
                {
                    "sent": "The label must be 0 for the any children.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can compute this in time linear in the number of nodes by using a simple message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this is the beige optimal assignment given these probabilities in this model.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is nice and the way you do it is by a bottom up procedure in which essentially.",
                    "label": 0
                },
                {
                    "sent": "The idea is is the following.",
                    "label": 0
                },
                {
                    "sent": "If you start from the leaf of your taxonomy and the leaf is assigned one simply by looking at the.",
                    "label": 0
                },
                {
                    "sent": "By comparing apply with the with the half, so no no threshold.",
                    "label": 0
                },
                {
                    "sent": "This is the optimal assignment for the leaves.",
                    "label": 0
                },
                {
                    "sent": "And then each node I sensor is parented.",
                    "label": 0
                },
                {
                    "sent": "Expected that lodge each loss of his subtree.",
                    "label": 0
                },
                {
                    "sent": "So essentially for any node up here, the decision whether to get enable one or label zero is based is based on comparing the expected.",
                    "label": 0
                },
                {
                    "sent": "A loss.",
                    "label": 0
                },
                {
                    "sent": "Which the node would incur if it sets labeled 0, which is this one.",
                    "label": 0
                },
                {
                    "sent": "So if I this node sets labeled zero and he has probability P of.",
                    "label": 0
                },
                {
                    "sent": "The underlying probabilistic model assigning him label one.",
                    "label": 0
                },
                {
                    "sent": "That's the expected loss of having labels 0.",
                    "label": 0
                },
                {
                    "sent": "If it's for despective loss, for one, is the loss of the node DH lost or the note for one plus the potential expected H loss that the node would have in the subtree.",
                    "label": 0
                },
                {
                    "sent": "So you remembered each loss.",
                    "label": 0
                },
                {
                    "sent": "If you make a mistake here, then.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "Sorry if if this notice decides on zero then this will be 0 by the true patrols and won't be any loss here.",
                    "label": 0
                },
                {
                    "sent": "If they know this one then some of these by get labeled one.",
                    "label": 0
                },
                {
                    "sent": "So there might be some further loss over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a rule that.",
                    "label": 0
                },
                {
                    "sent": "Actually implements this.",
                    "label": 0
                },
                {
                    "sent": "This argument over here and it's bottom up linear, so you can do it very quickly and it's you end up with an assignment which is valid.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if there's a very simple way to make this rule cost sensitive, you simply give a coefficient Alpha to the cost of a false false negative, as opposed to a false positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case in the previous example I gave you, there were two false negative mistakes cause these were positive and the predicted labeling was negative and the false positive, and so I instead of counting tree.",
                    "label": 0
                },
                {
                    "sent": "As each loss I count twice Alpha, plus one where Alpha is a cost of.",
                    "label": 0
                },
                {
                    "sent": "Air Force positing as a false positive mistake.",
                    "label": 0
                },
                {
                    "sent": "Sorry, false negative mistake.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case again I can introduce a good parameter in order to essentially tradeoff between precision.",
                    "label": 0
                },
                {
                    "sent": "Recall.",
                    "label": 0
                },
                {
                    "sent": "OK, so and the whole thing won't change match, so this will be change in by introducing the Alpha coefficient will appear here and there, but there won't be much difference.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately, what you?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This this is interesting because it is a simple way to derive an algorithm that gets you minimizes a meaningful notion of loss, although is not exactly the notion of loss, which is the most relevant for taxonomical classification with sparse labelings.",
                    "label": 0
                },
                {
                    "sent": "So one if you have a taxonomy with sparse multi labels then something that is actually able to.",
                    "label": 0
                },
                {
                    "sent": "Measure in a in a in a reasonable, accurate way that the true performance of your classifier is.",
                    "label": 0
                },
                {
                    "sent": "What is called hierarchical precision recall, which is used as being used?",
                    "label": 1
                },
                {
                    "sent": "Previously in in the biomedical literature, and this is.",
                    "label": 0
                },
                {
                    "sent": "Is this sort of a simple way of generalizing precision or recall?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the actual measure of performance that we used for in our experiments.",
                    "label": 0
                },
                {
                    "sent": "And the problem here is that it is hard to minimize directly this one on a taxonomy, so it's already harder to do it in the binary case.",
                    "label": 0
                },
                {
                    "sent": "In the hierarchical case it's even harder, so we used different losses like like the H loss as a proxy for this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one is as simple as this, so the again the blue is the predicted and the yellow is the true multi label and the precision.",
                    "label": 0
                },
                {
                    "sent": "Is you look at each?",
                    "label": 0
                },
                {
                    "sent": "Predicted path and you look at the true path, the extent by which the true path covers the predicted path.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 this is covered.",
                    "label": 0
                },
                {
                    "sent": "This blue path is covered 100% by the yellow path and this blue path is covered 50% by the yellow path.",
                    "label": 0
                },
                {
                    "sent": "So you get one place plus 1/2 which are the coverings.",
                    "label": 0
                },
                {
                    "sent": "And then the order by two because we had two paths and you do the same thing for recall.",
                    "label": 0
                },
                {
                    "sent": "So look at the stand in which the.",
                    "label": 0
                },
                {
                    "sent": "Yellow path are covered by the blue path, so here you have a yellow path which is covered the 1/3 here by the blue path and then here you have a yellow path which is covered a half by the blue path.",
                    "label": 0
                },
                {
                    "sent": "So 1/3 + 1/2 / 2 because we have to pass again and then you can compute that measures here.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty natural.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "It's hard to optimize directly though.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we look at different feature sets for the East data set of 600 genes and these are descriptions of the feature sets and, but I won't spend much time on that, and these are the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Resulting number of genes, so these are subsets of these.",
                    "label": 0
                },
                {
                    "sent": "Those 6000 genes yeast genes that we were started on we started from and these are the number of features that are associated to each data sets and these are the number of classes number class is about 200, but they're not all the same becausw we selected all the genes that had at least 20 annotations for each data set.",
                    "label": 0
                },
                {
                    "sent": "So that's why you have we have a varying number of classes.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's before giving you the results.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to look at the sensitivity of this of the trade of the parameters for the algorithms I just showing I'm just showing you the.",
                    "label": 0
                },
                {
                    "sent": "A sensitivity of the cost factor that Alpha which tells you how much does a false negative cost on the hierarchical basian algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is more complicated algorithm and you see here.",
                    "label": 0
                },
                {
                    "sent": "So this is the cost factor.",
                    "label": 1
                },
                {
                    "sent": "One is neutral, so false positive, false negative cost the same.",
                    "label": 0
                },
                {
                    "sent": "And this is done on a specific data set.",
                    "label": 0
                },
                {
                    "sent": "I don't remember which one, and so this is the precision which goes up and then goes down.",
                    "label": 0
                },
                {
                    "sent": "And this is the recall which goes monotonically up and the F measure is over here.",
                    "label": 0
                },
                {
                    "sent": "So you see, there's a region in which they pretty much there, pretty much balanced, and this is the steady performance of the hierarchical top down without cost sensitive measures.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a baseline without cost sensitive correction.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "See there is there are some issue issue here, so it's some extent if you can tradeoff between precision recall, but it's it's not clear.",
                    "label": 0
                },
                {
                    "sent": "For instance why you have this specific, why you have this behavior for precision and different behavior from them for the recall.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So recall goes up monotonically.",
                    "label": 0
                },
                {
                    "sent": "Precision has a as a.",
                    "label": 0
                },
                {
                    "sent": "Best shape.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's let's look at results.",
                    "label": 0
                },
                {
                    "sent": "So results are the problem is OK.",
                    "label": 0
                },
                {
                    "sent": "The bottom line is this.",
                    "label": 0
                },
                {
                    "sent": "So basically we don't.",
                    "label": 0
                },
                {
                    "sent": "We didn't find that on average a specific advantage of using the more sophisticated by Asian method, although there are differences, so there are differences in behavior OK, and these differences in behavior an indication that something is something different is going on the the two methods are not doing the same thing there optimizing different that there coming up with different ways of computing a multi label, and so we're hoping to.",
                    "label": 0
                },
                {
                    "sent": "Understand better and capitalize on those differences so.",
                    "label": 0
                },
                {
                    "sent": "But the bottom line, sadly, is that you have exactly the same average performance overall data sets between the two methods, so this is hierarchical hierarchical top down without cost sensitive correction, and this is with consecutive correction, and this is each base.",
                    "label": 0
                },
                {
                    "sent": "So both of these algorithms have a single parameter.",
                    "label": 0
                },
                {
                    "sent": "This is the Tao for CS for hierarchical top down, and you have the Alpha, the.",
                    "label": 0
                },
                {
                    "sent": "Trade off for the hierarchical base.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the significance of these differences, you see there's a OK. Hierarchical base with twice and there are four ties, but of course this is kind of disappointing because pretty much they behave the same.",
                    "label": 0
                },
                {
                    "sent": "They do behave better than top down without cost sensitive correction.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now you you get some more insights if you look at the behavior across the levels of the taxonomy.",
                    "label": 0
                },
                {
                    "sent": "So now we look at the five levels of the taxonomy.",
                    "label": 0
                },
                {
                    "sent": "OK, the five depths, and we compute the F measure standard F measure macro averaged among the classes of that level and first of all you see a common trend, which is the the F measure decreases as you go down in the in the hierarchy, meaning that the problems become harder and harder.",
                    "label": 0
                },
                {
                    "sent": "As the notes are deeper in the taxonomy and this is probably an indication, the fact of the fact that the annotations are less and less dependable and another phenomenon which is not very clear from this thing, but which I tell you, is that OK if you look at the simple flat classification.",
                    "label": 0
                },
                {
                    "sent": "So what you do here is just that you do a thresholding of your.",
                    "label": 0
                },
                {
                    "sent": "Let's go back here.",
                    "label": 0
                },
                {
                    "sent": "OK, so you do a simple thresholding at the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After we have some probabilistic thresholding, very simple and then you don't do any correction here, you don't account for the true patrol, but you leave just the labels.",
                    "label": 0
                },
                {
                    "sent": "OK, and what you get here is a recent very.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The recall and the bad precision.",
                    "label": 0
                },
                {
                    "sent": "Now the mode.",
                    "label": 0
                },
                {
                    "sent": "Now if you move to more and more informed methods, so this is a method that uses somehow the Sonic the taxonomical information but not so much.",
                    "label": 0
                },
                {
                    "sent": "This is a method that uses a lot that tries to come up with a probabilistic hierarchical model for the generation and optimizing that model.",
                    "label": 0
                },
                {
                    "sent": "OK, as you move from flat to more sophisticated methods the recall goes down and precision goes up.",
                    "label": 0
                },
                {
                    "sent": "So your essentially the algorithm becomes more conservative becausw the taxonomical information is.",
                    "label": 0
                },
                {
                    "sent": "Tends to produce shorter annotations in the taxonomy, so the multi labels are sparser.",
                    "label": 0
                },
                {
                    "sent": "The algorithm becomes in it becomes very conservative in a way.",
                    "label": 0
                },
                {
                    "sent": "The this conservativeness we try to address it by introducing this this can sensitive parameters for instance in each base.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the Gen in general a good value of this cost sensitive parameter is 5 which means that you pay five times less a false negative.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "False positive, so we really want to make the algorithm or aggressive even though it must take into account taxonomical information.",
                    "label": 0
                },
                {
                    "sent": "However, it's somehow the simpler method achieves a better trade off than a more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, although we see that we have a high on average, we have a higher precision at all levels.",
                    "label": 0
                },
                {
                    "sent": "Then each base has a higher precision than HDD, so there is some room for maneuvering here, although it's not clear what kind of permit written parameterization of this will get better results.",
                    "label": 0
                },
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "We definitely see that.",
                    "label": 0
                },
                {
                    "sent": "Unscented these methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are extremely important in order to get good performance on such sparse annotations and hierarchical precision recall are definitely a match.",
                    "label": 0
                },
                {
                    "sent": "In assessable measure to user which provides a lot of information on the performance of the algorithm and unfortunately we didn't, we didn't need to succeed in having complex method perform better as a similar ones, although they have different notions they have different.",
                    "label": 0
                },
                {
                    "sent": "They perform different classifications.",
                    "label": 0
                },
                {
                    "sent": "And one one possibility is that division method gets overfit by the noise in the annotation of the lower levels.",
                    "label": 0
                },
                {
                    "sent": "OK, and no method.",
                    "label": 0
                },
                {
                    "sent": "None of the two methods we use, the yeah is actually designed to optimize the true performance measure, which is the hierarchical precision, recall and what we're trying to do now is to use a symmetric kernels that actually are able to compute our radical or closely computer radical precision recall, and then you can enclose this symmetric kernels in a tensor in tensor product support vector machine, which is.",
                    "label": 1
                },
                {
                    "sent": "Is not an example and I thought this is completely different approach, but at least you should be able to come up with an algorithm that is much more connected to the actual loss that you are trying to minimize.",
                    "label": 1
                },
                {
                    "sent": "And another thing is of course which are working on which is very tempting, is to combine these assembled hierarchical classification methods with data integration methods in order to.",
                    "label": 0
                },
                {
                    "sent": "Really use up all the information you have available.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you, that's it.",
                    "label": 0
                },
                {
                    "sent": "Biologist I would value much more highly predictions at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Value.",
                    "label": 0
                },
                {
                    "sent": "That's why I think it's a 2 performance measure.",
                    "label": 0
                },
                {
                    "sent": "Is that what you're doing?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Great for utility projectiles at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Useless.",
                    "label": 0
                },
                {
                    "sent": "Well, help me.",
                    "label": 0
                },
                {
                    "sent": "Experiments I know I know.",
                    "label": 0
                },
                {
                    "sent": "OK, the idea of these measures is to give you.",
                    "label": 0
                },
                {
                    "sent": "Flexibility in sense that you might not get it, but you might get close to it.",
                    "label": 0
                },
                {
                    "sent": "You might.",
                    "label": 0
                },
                {
                    "sent": "You might not get the exact annotation, but maybe you are in the right sub subtree.",
                    "label": 0
                },
                {
                    "sent": "And so these measures will give you will reward predictions that maybe are in the right subtree at the certain level, although they don't really get to the right annotation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Alternative is to say OK.",
                    "label": 0
                },
                {
                    "sent": "Either you get me, I just count.",
                    "label": 0
                },
                {
                    "sent": "I just take the symmetric difference between the correct annotation and the wrong annotation.",
                    "label": 0
                },
                {
                    "sent": "OK, so the predicted annotation and that rotations I take the symmetric difference is two sets that would be at the bottom level.",
                    "label": 0
                },
                {
                    "sent": "OK, that will be sort of a hard.",
                    "label": 0
                },
                {
                    "sent": "Hard measure of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the problem is that as you go down the problems becomes much much harder.",
                    "label": 0
                },
                {
                    "sent": "So yeah, yeah so.",
                    "label": 0
                },
                {
                    "sent": "But you have to add the algorithm somehow.",
                    "label": 0
                },
                {
                    "sent": "So if you insist on putting a lot of weight on the harder problems.",
                    "label": 0
                },
                {
                    "sent": "Be the right way to really get.",
                    "label": 0
                },
                {
                    "sent": "I agree that it's it's more interesting.",
                    "label": 0
                },
                {
                    "sent": "The fact that it's harder makes it probably.",
                    "label": 0
                },
                {
                    "sent": "Makes it unclear whether you should really.",
                    "label": 0
                },
                {
                    "sent": "Put all of your cost down there.",
                    "label": 0
                },
                {
                    "sent": "That's my point.",
                    "label": 0
                },
                {
                    "sent": "OK, now I totally agree with you that that's the most interesting information I'm trying to get that we get some kind of information that might be not at the level of not down at the bottom level, but still may be useful.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Price.",
                    "label": 0
                },
                {
                    "sent": "Protein sequences and then derive some features from it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there were feature sets over here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The main features.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm you should ask my Co author who's not there any any detailed information on those or you can just check the paper and.",
                    "label": 0
                },
                {
                    "sent": "These are the yeah.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "System properties.",
                    "label": 0
                },
                {
                    "sent": "Right so yeah.",
                    "label": 0
                },
                {
                    "sent": "Picture was about was that about trying to overcome that issue, so the.",
                    "label": 0
                },
                {
                    "sent": "Did the dealership, the underlying binary classifiers are fixed, so the idea is that you have these probabilities and then what you do with this probabilities.",
                    "label": 0
                },
                {
                    "sent": "So we had to translate these probabilities into a multi label which respects the true patrol.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic.",
                    "label": 0
                },
                {
                    "sent": "Problem and then you have some notion of loss that you compare this and the notion of loss we chose is this hierarchical precision recall, which is good for such sparse annotations, but unfortunately we don't have any direct method to optimize that one.",
                    "label": 0
                },
                {
                    "sent": "So we are using proxies like this H loss.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK OK OK it does that in does that in a way becausw for instance all the negative examples that are used to train each node.",
                    "label": 0
                },
                {
                    "sent": "Are those examples which are positive for the father for the parent in negative for the node itself?",
                    "label": 0
                },
                {
                    "sent": "So this is a way which is this way of training is consistent with the probabilistic model I am assuming for this age last thing, so there's a by I was trying to decouple the problem of coming up a good estimates of nodes and then use those estimates to construct a multi label.",
                    "label": 0
                },
                {
                    "sent": "So this is the way the coupling the problem.",
                    "label": 0
                },
                {
                    "sent": "But you're definitely right in saying that.",
                    "label": 0
                },
                {
                    "sent": "I am sort of an organic compound way of looking at the problem at once.",
                    "label": 0
                },
                {
                    "sent": "Might be much more effective, although I have no idea, it's more complicated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we use this calibrated SVM's and we try to do the calibration as best as possible.",
                    "label": 0
                },
                {
                    "sent": "But you're right.",
                    "label": 0
                },
                {
                    "sent": "And yes, yes, this is totally true.",
                    "label": 0
                },
                {
                    "sent": "I admit I was not so much.",
                    "label": 0
                },
                {
                    "sent": "Interested in in.",
                    "label": 0
                },
                {
                    "sent": "Well, it no, you're right.",
                    "label": 0
                },
                {
                    "sent": "I cannot say you're right.",
                    "label": 0
                },
                {
                    "sent": "That would be definitely a better thing to do.",
                    "label": 0
                },
                {
                    "sent": "We use SVM just out of convenience.",
                    "label": 0
                },
                {
                    "sent": "There he was.",
                    "label": 0
                },
                {
                    "sent": "He was.",
                    "label": 0
                },
                {
                    "sent": "Yes, I've been very much, yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is a quite a large number of systems that try to do such hierarchical function prediction.",
                    "label": 0
                },
                {
                    "sent": "Was wondering how this relates because some of these systems purposes they give projected first predict nodes on a higher level and then given that something gets a positive prediction for the higher level, they would make predictions on the lower level.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what this thing does.",
                    "label": 0
                },
                {
                    "sent": "This is exactly that.",
                    "label": 0
                },
                {
                    "sent": "So how does this compare them too?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, things like Bayesian.",
                    "label": 0
                },
                {
                    "sent": "There's still approaches that have been used.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we right?",
                    "label": 0
                },
                {
                    "sent": "So this exactly top down is because you can view this as a top down.",
                    "label": 0
                },
                {
                    "sent": "As long as you get a 0 prediction then you stop looking at the subtree.",
                    "label": 0
                },
                {
                    "sent": "So that's the way it does it.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we are in the process of establishing a benchmark of data set that allows us to make comparison because.",
                    "label": 0
                },
                {
                    "sent": "It's it's hard.",
                    "label": 0
                },
                {
                    "sent": "I mean, we found it hard to come up with the with the to make a comparison without reimplementing the algorithm of the paper.",
                    "label": 0
                },
                {
                    "sent": "We want to compare 2, which is a practice that is not very good.",
                    "label": 0
                },
                {
                    "sent": "So yeah, yeah yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "This is true.",
                    "label": 0
                },
                {
                    "sent": "We definitely need to relate better to the other methods.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Dependence on the depth of the tree.",
                    "label": 0
                },
                {
                    "sent": "No, you shouldn't be surprised because I did it, but I didn't tell you, and the way I did it is in the.",
                    "label": 0
                },
                {
                    "sent": "OK, in the evaluation of the label the costs go down OK.",
                    "label": 0
                },
                {
                    "sent": "The costs at the lower levels go down a certain rate that depends on the fan out of the tree.",
                    "label": 0
                },
                {
                    "sent": "However, we didn't take.",
                    "label": 0
                },
                {
                    "sent": "We didn't use this costs in the evaluation in the in the performance measure.",
                    "label": 0
                },
                {
                    "sent": "So to build the label views this cost.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the algorithm would be really, really conservative, so this decreasing costs are way too encouraging.",
                    "label": 0
                },
                {
                    "sent": "The argument to go down and then we use the game.",
                    "label": 0
                },
                {
                    "sent": "This cost sensitive parameter Alpha to tell the algorithm look first negative is going only to cost you a fifth of the false positive.",
                    "label": 0
                },
                {
                    "sent": "So go down.",
                    "label": 0
                },
                {
                    "sent": "Yet it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to strike the break even point between precision recall.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe you need more parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}