{
    "id": "vhbbaqjdoxeiq7q36ypkmhqep64mfhjf",
    "title": "MPI & OpenMP (Part 4)",
    "info": {
        "author": [
            "David Henty, EPCC, University of Edinburgh"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_henty_MPI_openMP_part4/",
    "segmentation": [
        [
            "OK, So what I was going to do was.",
            "I'll go through the Pi example for the laboriously 'cause I think it illustrates, although it's a very simple example, illustrates really some of the key issues with open MP.",
            "There's one issue, it doesn't illustrate, but the traffic model illustrates that.",
            "So then I'll.",
            "So what I'll do is, I'll.",
            "I'll go through the Pi example, then I'll give a brief lecture on.",
            "Parallel loops and then will.",
            "They'll do the pie example.",
            "I think that was.",
            "I think that's sort of my original schedule.",
            "I think I may be originally just thought I'd do some more.",
            "Anne.",
            "In depth stuff, but let's have a look.",
            "It was, it was.",
            "Now, sorry.",
            "Yeah, so that's that's kind of.",
            "I will do advance.",
            "I'll do some things with Worksharing orphaning.",
            "I'll see how far I get there, but I think this most important to go through this Pi exact, So what I'm going to do is basically I've just downloaded the code I'm going to see it, but actually could you put your hand up?",
            "Issue a Fortran programmer?",
            "And if you're a C programmer.",
            "It's kind of 5050.",
            "I'll do it and see 'cause it's.",
            "It's not really much different, but I'll just do it and see.",
            "So I'll take the serial code.",
            "And if we look at the serial code.",
            "So to convert this to an open MP code I.",
            "Can people read?",
            "I don't like the people.",
            "Read that with the colors.",
            "I don't like the colors but.",
            "OK.",
            "Anyway, so.",
            "So I'll take that.",
            "I'll take the.",
            "So, um.",
            "First thing you need to do is you just need to include a header hash.",
            "Include ompa dot H. Because although open MP is primarily compiler functionality, there are some runtime libraries.",
            "And then the classic thing you would do is.",
            "You would do.",
            "Print hello.",
            "From thread Sandy out of Sandy.",
            "And it's ompa.",
            "Get.",
            "Thread.",
            "Numb lips.",
            "Come on.",
            "Time and.",
            "NP get this is exactly the same in in Fortran.",
            "In 4 Chan you do.",
            "In Fortran, you just do.",
            "Include OMP_Lib.",
            "That's what you live, but that's what you do in Fortran so.",
            "Oh yeah, thank you.",
            "Thanks so now if I compile this.",
            "I get an error because it says I don't know what he get thread now and getting threads are and that's because by default on most compilers open MP is not turned on by default.",
            "So I'm using the Intel compiler.",
            "And it's minus Q open MP.",
            "And the Intel compiler if it was.",
            "If it was GCC, it'll be GCC minus F open MP.",
            "If you're interested.",
            "What is GCC but.",
            "So now we can do make again and it compiles it and if I run it.",
            "You see that no matter how many.",
            "Threads I run it on.",
            "I try and run it on four threads.",
            "Exporter oppidum threads equals four and I run it Whoops.",
            "I still get only one thread running hello from thread.",
            "The writer thread one.",
            "The reason is.",
            "That open MP.",
            "And completely unlike are MPI.",
            "Has a fork join model.",
            "So.",
            "Lost my.",
            "OK.",
            "It's not there yet.",
            "So.",
            "You only, this is the program.",
            "You only create multiple threads in parallel regions such as the first thing the parallel region is the fundamental construct in open MP, it spawns multiple threads.",
            "So what we can do is we can say OK. Whoops I'd say we can spawn apart Patrick and we can.",
            "Let's do hash pragma.",
            "OMP parallel and then we can maybe do hash pragma.",
            "That's fine.",
            "We let's just.",
            "And now we compile that.",
            "We get everyone printing so.",
            "That's the first point, you know.",
            "You only get parallelism when you want it, and that's the model of open MP.",
            "The model is.",
            "You take a serial code, you identify the regions you want to go faster.",
            "Sorry, the subroutines functions you want to go faster, and you paralyze them, but you can leave the whole of the rest of the code untouched.",
            "And that's different from MPI to open.",
            "MP is in principle an incremental parallel model, which is attractive as its downsides, but it is attractive, so you have to spawn a parallel region in open MP even to find out how many threads there are, because otherwise it runs on one thread.",
            "The next thing so.",
            "But there's no magic here, although a parallel deep region does.",
            "Is it spawns multiple threads?",
            "And all the threads do the code within that region.",
            "So what some people might do is, well, let's make this parallel hash pragma OMP parallel.",
            "And then put this in a parallel region.",
            "OK.",
            "There we go, that's that's great, isn't it?",
            "I've gotta do loop for loop which is parallel that let let's do that.",
            "What what happens?",
            "OK, so that didn't that.",
            "Why didn't that?",
            "That's weird.",
            "OK, so I thought this was going to go wrong.",
            "OK, that's interesting.",
            "This normally goes wrong.",
            "OK, so there's a well.",
            "OK, so this is I'm just now I'm confused.",
            "As to why, let's let's, let's print.",
            "Let's just print air with Hello World in there.",
            "I think that actually the compiler is doing something weird here which is.",
            "This time I'm in complete stupid.",
            "Am I OK?",
            "I'm being completely stupid.",
            "Fine, OK, so that's what I meant to do.",
            "Sorry, so I'll take that hello.",
            "Well sorry I didn't read past like that.",
            "That's the confusion with open MP is 'cause you sometimes don't recompile?",
            "Can you just change number?",
            "Thanks OK, fine so OK.",
            "So let's just do that.",
            "Let's run it on four threads.",
            "I don't need to recompile and we we run it.",
            "We run the code OK.",
            "Sometimes I get the right answer.",
            "Let's do more minutes.",
            "So actually I can.",
            "OK, I think I know what's going on here, I can.",
            "I'm having to.",
            "Yeah, OK, so I'm there.",
            "It's not death, it's not necessary.",
            "I'm having to play around a bit, but OK.",
            "So, So what it's done is it's done.",
            "\u03a0 = 12.566 OK. And absent errors 300%, what does that mean?",
            "This is exactly four times the value of Pi.",
            "OK, the reason it didn't go wrong before is that in this simple program, I think the threads are running exactly the same time, but there's.",
            "But anyway, what's happened is a parallel region does what it says on the tin, it says.",
            "I want every code.",
            "I want every it spawns threads and all the threads execute all the code OK.",
            "The compiler is not.",
            "The compiler doesn't go or he wants a parallel region.",
            "Oh, he's got a four loop out.",
            "You know, the compiler does what you say it does.",
            "Please give me 4 threads.",
            "I want them all to execute this code and they're all adding up to the same pivetta value and actually.",
            "\u03a0 here is actually within here Pirsa shared variable.",
            "And so they're all computing Pi into the same variable, so you're getting four times the result.",
            "OK, so you've done what you've done, what it asked it to do, you have spawned four parallel threads, and they've just all run that code.",
            "That's not what you want, so that we could do exactly what we did with, so I should be.",
            "I should be a bit more clear and say I'll do default.",
            "None.",
            "So this is if I do a parallel region I have to, I should scope within that parallel region variables can be shared or private and I should scope them.",
            "But you can use the compiler to help you.",
            "So what it says here is you've got a parallel region you haven't told me where the I is.",
            "What I is, what N is expired.",
            "So I need to say what I end and Pi are here.",
            "So I will now say.",
            "Private I could loop variables always have to be private shared.",
            "And they're all using the shared and I'll I'll do \u03c0 as well, but just not completely correct, but I'll do that OK.",
            "But what I also want to do is something like Private Eye.",
            "I start.",
            "I stop.",
            "OK, So what I'm going to do is now is just the same thing as I did in.",
            "Is the same thing as I did with the opening the MPI code.",
            "I stop.",
            "I'll get rid of that.",
            "And then I just have to say, well, I start.",
            "Equals.",
            "One plus.",
            "ROMP.",
            "Get thread.",
            "Numb.",
            "Times.",
            "An over.",
            "Clampi get numb.",
            "Threads, I think that's right.",
            "And I stop.",
            "Can't type.",
            "Equals I start.",
            "Plus minus one probably.",
            "And then this is a thing you know I can just print F of.",
            "Thread sent the istar percent DI stop sent D. Just to debug it.",
            "IMP get I should have put these in variables thread.",
            "Nam.",
            ", I start, I stopped so you can always print these things out.",
            "So let's see if that compiles.",
            "It probably won't.",
            "'cause I haven't done.",
            "I start my stop.",
            "It's going to work OK, so now we can do dot slash price serial.",
            "So these look right.",
            "OK so.",
            "I start.",
            "Is 1 to 1 two 10 to this is this guy got it right too.",
            "So I split the iteration space up by hand.",
            "So this gives me the right now.",
            "This is technically wrong because I said you should have just been lucky here.",
            "You shouldn't.",
            "You shouldn't accumulate into a shared variable 'cause they could be these race conditions, but for redo with this architecture reasons I don't quite understand.",
            "It doesn't show up here.",
            "We could maybe try a more threads, maybe 8.",
            "It still works OK.",
            "I don't understand why, but there is what?",
            "What you what we really want to do is we want to.",
            "The most efficient way to do this is for everybody to accumulate into a local variable and then to only at the end put that into the shit into the shared variable.",
            "OK, that's the way that that's the way that we want to do it.",
            "So what we want is we want to have.",
            "A local pie.",
            "Local pie, whoops, local pie, let's call it my pie.",
            "That's a bit more.",
            "OK. We have to make that.",
            "Private because everyone has their own value.",
            "Local pie.",
            "We have to say.",
            "Local pie equals then it's not local Pirates.",
            "My pie, isn't it?",
            "My pie.",
            "So I will see.",
            "So this is the.",
            "And now Pi at the end we do \u03c0 = \u03c0.",
            "Plus my pie, so you see what I'm doing here?",
            "I'm accumulating the value into a local.",
            "A local lamb.",
            "Variable and then.",
            "OK, so that was So what I'm doing here.",
            "Is I am.",
            "If I can get this all in the screen.",
            "And sporting a parallel region.",
            "I'm saying within that tie and Anna shared but all the other variables are local.",
            "I'm setting my local my \u03c0 to 0 and my hand splitting up the iteration space by this.",
            "Chunking it up and then looping over different spaces, incrementing into a variable my pie, and then include and then accumulating into pie.",
            "OK, so that's that's, that's what I showed on the slide as Yep.",
            "There yes exactly exactly, so there is a race condition there, so this code is wrong, but hopefully it will.",
            "It will.",
            "It will show that it's wrong, so let's hope it does.",
            "I have to take the prints out.",
            "So I think what's happening is.",
            "You get a race condition.",
            "You get a race condition.",
            "If two threads do something at the same time.",
            "But when I do a print statement which takes quite a lot of time, it kind of throws them out of sync.",
            "So let's see if this works.",
            "There we are.",
            "Yeah yeah, so we get it with four OK?",
            "So if I do it, I got the right answer there and I got the right answer and then I got the wrong answer.",
            "So this is a classic example of a race condition.",
            "Sometimes you're lucky, OK?",
            "Write value of \u03c0 right value of \u03c0.",
            "Sometimes you're unlucky and it's always going to be less.",
            "This always happens when two threads.",
            "You know two threads right at the same time, so only one of them take ad at the same time only so that so the value is always less.",
            "Here this is the classic, so you know this is, you know, I run my code, it's correct or it's not correct.",
            "OK, so this is the.",
            "This is surprising.",
            "It's surprisingly subtle, but you do.",
            "You do get these these errors.",
            "So this is a very naive and as you said.",
            "What we should, what we have to do and well spotted is that that is an incorrect code we have to do something down here.",
            "I haven't covered it, but the way to do it in in.",
            "An open MP is something called critical.",
            "That says.",
            "This is a critical section, which means that open MP says only one, only one thread is allowed to enter that piece of code at once so that there's a lock around here.",
            "OK, so.",
            "I could have what I could have done is I could have put a critical section here.",
            "I could have just had \u03c0 = \u03c0 Plus here and put a critical section around that update.",
            "OK, not not bother with all these my Pi stuff, but just just incremented into the shared variable pipe and put that in a critical section.",
            "Why is that a bad idea?",
            "It's effectively a serial code.",
            "It's saying you know every time anybody wants to update this, they have to wait for everyone else to do, and you've affected the serialized the whole code.",
            "So the idea is do everything is locally as much as possible, so this is happening 840 times this edition.",
            "This edition is happening four times and of course N and a real code would be millions so you know you should know that this is the way to do it.",
            "Do a local accumulation and the global accumulation and this kind of mirrors MPI.",
            "You know this sort of you do a local accumulation.",
            "This is like an all reduce in that in that.",
            "In that kind of sense, so this should now work.",
            "If I make it.",
            "And then I run it.",
            "I should get the right answer and it definitely goes wrong when I do lots of threads.",
            "So let's use 100 threads.",
            "And that's not going to work.",
            "Because I assume that it's divisible by.",
            "I assume that number threat?",
            "No, I don't do I?",
            "Yes I do.",
            "I'm assuming the numbers divisible by we can do 84.",
            "Yeah, and I get the right answer.",
            "So.",
            "I get the right answer there.",
            "So that.",
            "You know this is quite this shows.",
            "This shows what you have to do to correctly implement this.",
            "OK. Get it all on one screen.",
            "You have to spawn a parallel region which spawns parallel multiple threads, but it's not magic.",
            "It doesn't do any magic right?",
            "It just spawns multiple threads who all do all this code?",
            "OK, then I have private variable.",
            "My pipe the accumulation and I have private loop counters and start and stop.",
            "I compute them on each thread.",
            "I manually split up the loop.",
            "The iterations between threads to make sure they do different iterations.",
            "I then have to do the accumulation.",
            "A local Ek, now in fact.",
            "This is where open MP starts to help you that actually you don't need to do all this stuff.",
            "OK, so because this is such a common this so this doesn't look anything like my serial code.",
            "This is just garbage basically, but open MP recognizes that people want to do this a lot, so there are lots of shortcuts which I'll show you now, but it's really important you realize this is what the compiler is doing under the hood.",
            "It's doing all this stuff.",
            "Everything else is just shortcuts to get the compiler to do this stuff for you.",
            "OK, but this is what it's doing.",
            "It's not magic, it's spawning different threads.",
            "It's saying look, you do loop iterations.",
            "This to this you do that to that.",
            "You do that to that and come back OK and you can do it all by hand if you want, and sometimes you need to do it if open MP doesn't support what you want to do, but for most simple loops and calculations open MP supports it.",
            "So the first one is.",
            "I don't need to do all this stuff with my \u03c0. OK.",
            "I do this.",
            "I say my.",
            "Let's you say that Pi is a reduction variable.",
            "But you have to say what to do with whoopsie?",
            "Daisy, you have to say.",
            "So I'm at the Emacs is a bit slow.",
            "You have to say it.",
            "You have to tell the compiler what you do, he said, look, I'm computing this thing pie and then computing it via addition.",
            "So what I want you to do is to initialize a local variable.",
            "Do the computation and then at the end accumulate them back together into the global variable using addition.",
            "OK, so the telling the compiler you want to use addition tells it two things.",
            "First of all, it tells it that at the end of the loop it should accumulate them with addition, but it also tells it something else.",
            "It tells it what this value should be, what the what the default value of the local variable should be.",
            "'cause?",
            "What would it be if that was multiplication?",
            "What's the unit operation for multiplication one so it would have to be my pie equals one.",
            "So this this the compiler trusts you, the kind that doesn't check, or you said you're doing.",
            "Additionally it just you know all it says is instead of computing Pi, compute it locally on a thread into available.",
            "Could local pie or my pie then then accumulate them back together using this operation.",
            "OK, and if you're doing subtraction, it's addition.",
            "'cause additions just subtracted.",
            "Subtracted.",
            "Just adding a negative number.",
            "There's no such thing as reduction with negativity that need to think about that a bit.",
            "So now I can get rid of all this garbage here at least.",
            "So I get rid of that.",
            "I can get rid of this.",
            "MIPI stuff 'cause the compiler is going to do that for me and I can get rid of this 'cause the compiler does it all for me and I don't know where does that match.",
            "OK, so basically the only thing you have to realize is.",
            "The value of \u03c0 is defined at the end of the parallel region, but within the parallel region.",
            "The problem is open.",
            "MP is within the parallel region.",
            "The compilers rewritten pie to be my pie, or you know, and so you.",
            "But all you know is it correct at the end.",
            "Within here you don't know what's going on, but all it's done is reduced that reproduce.",
            "Replace that code.",
            "Let's check that this works.",
            "I'll do a sensible number of threads like 10, No 8.",
            "AJ.",
            "And then I get the right answer, OK?",
            "Reproducibly, so also the compiler is just doing their what I did before, but the compiler can do more than that.",
            "'cause clearly.",
            "All this stuff with I star I stop and all that is just mechanical.",
            "Yeah so.",
            "I can just go with the original loop.",
            "And there's a hash pragma.",
            "O MP4 that says.",
            "I want you to split this loop up so a lot of people think that OMP parallel splits loops up.",
            "It doesn't.",
            "It does nothing of the sort.",
            "It creates multiple threads.",
            "But oh, MP4 says OK. Split the iteration of this loop when it does nothing more than that code I did before I start, I stop on this stuff, but it's done for you and by default it split up into blocks.",
            "You can do more complicated things, but my so that code should now work.",
            "It still doesn't look quite like my.",
            "That should give me the right answer.",
            "And then I get the right answer.",
            "OK, is it reproducible?",
            "Yes it is, but but open MP does even more.",
            "It says, well, actually.",
            "I don't need this, I start and I stop anymore either.",
            "So I can get put that back on the same line.",
            "You could actually.",
            "This is so common hashtag where OMP parallel hashtag to MP4 that you can put them together.",
            "You can say hashtag by OMP parallel 4.",
            "And as long as that's in front of a for loop.",
            "Then you're OK.",
            "So this, but it's important to note this does two things conceptually.",
            "A it spawns lots of threads and be it splits up loop iteration space.",
            "There two separate things.",
            "We've just confirmed that that is just the serial code annotated with the directive, but under the hood it's doing.",
            "It's just doing what I did by hand before.",
            "There's no magic here, so I can do that.",
            "And that should work.",
            "And in fact.",
            "Open MP does even more for you, it says.",
            "The default skin that they you shouldn't really do this when you're ready, but I can take out if I know that if I can take out default none.",
            "Loop variable, nobody would want to loop variable to be shared.",
            "It's meaningless, so they are private by default.",
            "And anything and normal variables are shared by default.",
            "So in fact I can get away with that.",
            "Hopefully.",
            "Yeah, so so you know.",
            "Open MP can look quite compact and magic, but it is doing nothing more than the stuff I write by hand.",
            "But these are just shortcuts.",
            "OK, and in fact if you wonder what it's doing.",
            "I can just let's make ends smaller.",
            "Let's make N 12.",
            "Within the loop, you could still do stuff like saying.",
            "Printf of.",
            "Thread percent D doing loop.",
            "Percent D. And you can just do that.",
            "And.",
            "I'll do.",
            "I'll do.",
            "Or did I make it 12, didn't I?",
            "Yeah.",
            "So we do dot slash pyserial now.",
            "You can see that it split them up into the order you thought threads are doing loop 123 thread wondering loop 456 thread to do loops it 870 ninth just on my thought but what I'll go to onto the next thing is that.",
            "Although splitting loops up like this is mechanical, it's pretty.",
            "It's pretty tedious and open.",
            "MP allows you two different things.",
            "You might say, well, I don't want.",
            "I want a different distribution.",
            "I want the loops to be.",
            "Thread 0 does loop 0 thread one dustloop one thread do dot loop 2 back in it like dealing a pack of cards, not block it up, but in in a cyclic distribution and open MP supports that and I'll just.",
            "I'll talk about in the next lecture, but it's called the schedule clause, so you can say.",
            "Schedule.",
            "Of static, one which says split up by dealing at a pack of cards.",
            "And if we do that, you'll see that when we run it.",
            "Threads loaded loop.",
            "One thread wanted to loop 2 thread, two did loop 3 thread 3 did loop 4 then went back to the start again.",
            "Or you can even do other things you can.",
            "It's not, there's no, there's no advantage here.",
            "You can actually have a dynamic schedule.",
            "Which says.",
            "Dynamic it says.",
            "Give thread zero iteration, zero thread one iteration, one thread to iteration, two thread, three iteration 3, then wait for the first person to finish and give them the next iteration.",
            "And that's very useful if the iterations are load imbalanced, because basically you know you just give it the stupid thing to do here.",
            "But if the iterations if each iteration I is a lot of work and there's load imbalance, then it balances the load and the important point about this is that you don't know.",
            "Which thread is going to do which loop?",
            "So here.",
            "Oh OK, thread zero did loops one and five and nothing else.",
            "OK if I run it again or do a different it will do a different thing.",
            "Here, thread zero Ditto threaded loads here because they're running at different speeds.",
            "You know it's just.",
            "You know, it's just it's first come, first served, and that's why it's called dynamic.",
            "Each time you run it, it does something different, so that's just to try.",
            "Thought I'd go.",
            "I'll just now sort of go through the slides quite quick.",
            "Things we've covered, most of it explicitly.",
            "Are there any any questions though?",
            "Yeah.",
            "Plus I would get I would get garbage output, so I think because the compiler supports open MP, they've made printf thread safe so the input is interleaved on a line by line basis, but not on a character by character basis.",
            "So this does not mean that thread 3 did loop 2 after thread zero did loop 5, because you know, who knows.",
            "You know printing to the screen is like posting a letter, please you know so so you know this has nothing to do with the order they ran in between different threads.",
            "There's no global time.",
            "Who knows what printing to the screen actually does, but but an individual print is normally.",
            "You know, thread safe, one at least a line by line basis, but you can't.",
            "You cannot say here, so for it thread zeros output is in the right order, but you cannot say here that thread 3 did loop 2 after thread before threads invited count of fact it's impossible.",
            "Threads for he could not have done Loop 2 before thread Zero did Luke 9, but the print statements got messed up on the way to the screen.",
            "So each thread sends its print statements and they can get mangled up in different orders.",
            "But but an individual thread is in order.",
            "So yeah.",
            "So it's sort of semi threadsafe, but that's a good question.",
            "So what I was going to do is just go through.",
            "Um?",
            "That's not what I want to do.",
            "It's quite a long lecture, but that's why I think it's quicker to go through it by by hand.",
            "So this is worksharing lecture.",
            "So I'll just.",
            "This will just be pretty.",
            "I'll go through it quite quickly 'cause I've been through everything on the screen."
        ],
        [
            "We've got parallel doing for loop signal direction.",
            "I'll come back to this, but the most important parallel two or four."
        ],
        [
            "So as we said loops, the most common source of parallelism and you can divide the loop iterations up with two or four.",
            "You don't have to do this, you can do it by hand if you want, but these are normally the easiest way of doing it.",
            "Are there's a synchronization point at the end of the loop, all threads must finish their iterations before any thread can proceed, so this is, you know if you do a no MP4.",
            "There is a barrier at the end of the loop 'cause you don't see it, but the open MP puts in 'cause it says.",
            "Look when I go.",
            "When I go to the next line after this.",
            "The programmer clearly expects the entire for loop to have been completely computed, so by default open MP puts in barriers synchronization points at the end of the end of loops, which is not, which is a safe thing to do.",
            "You can take them out if you're brave, but by default it puts them into open.",
            "MP is quite conservative, it tends to put in things to make it easier for you to write correct programs."
        ],
        [
            "So we've seen it so MP do an end do in Fortran I didn't do four times and hashtag or MP4.",
            "And."
        ],
        [
            "With no additional clauses to do for directional partition iteration up as equally as possible between the threads.",
            "Now I've got about 7 iterations to three threads.",
            "It doesn't fit, so does it do 331?",
            "We don't, who cares, but but open MP does guarantee that if it splits a loop of iterative count 7 / 3 threads, it will always split it the same way, but you don't know what it is, but it probably does this.",
            "I don't know who knows, but of course it's round the margins you really spitting up a loofah 1,000,000 / 3 threads, so who cares for a thread .30 three 1333 or 30 three 1334?",
            "I mean, it's really lost in the noise.",
            "But it can be important to note."
        ],
        [
            "Based on the same, this is just a lot of stuff, but.",
            "I'll go back to.",
            "This is just saying that C is a bit of a weird language.",
            "Uh.",
            "So I'll just comment that out.",
            "What does that statement say?",
            "What does that tell the compiler to do?",
            "I mean, Cedric, the stolen tell obvious answer.",
            "OK, so this is executed end times.",
            "No, it's not.",
            "OK saturated once in that case.",
            "The C compiler cannot work out from this.",
            "How many times the loop is iterated.",
            "It's impossible because that is a totally legitimate vacancy.",
            "Madness you been idiot to do it, but this loop is it rated one so open MP says look.",
            "The If you've written a four loop, it's just a normal for loop.",
            "It's not.",
            "Frank was one I less than I less than day of the week I less than random number I less than you know, because you like to do that you can do like with one eye less than day of the week I less than random number 'cause nobody does but the C4 loop is far too general 'cause 'cause open MP dies to know how many iterations are going to divide the loops up.",
            "So there's a lot of because so busy says you can't do that.",
            "Just don't be an idiot.",
            "So all this all this stuff here about restrictions is just saying don't be an idiot, just do normal things.",
            "Fortran do loops are very prescriptive.",
            "A fortune do loop.",
            "Do what you guys want to end mandates.",
            "Unless there's an explicit exit statement, mandates that this will be done end times but but C doesn't."
        ],
        [
            "Accent so.",
            "How can you tell if a loop is parallel or not?",
            "If you if you put a oh MP4 and MP do construct in front of loop open MP will parallelize it.",
            "It will split the loop.",
            "I won't check if it's sensible or not, it will just it will just do it.",
            "So for example.",
            "A useful test if the loop gives the same answers if it's running reverse, then it's almost certainly parallel, but this loop here OK, can I parallelize this in a in a naive way?",
            "A split across loops can?",
            "I cannot get the same answer.",
            "No, why not?",
            "I know I well know that it's only a so pointer aliasing is an issue, but not here 'cause it's, but they're both A and that's four times.",
            "Well, actually, but even see it's OK.",
            "So that's another reason why this isn't, but that's a good that is a good point, but it doesn't apply here.",
            "This loop assumes that the iterations done in order, but because because the the I TH iteration depends explicitly on there, I minus want iteration.",
            "So if I tell thread thread three to start iteration 20, it's going to get the wrong answer because iteration 19 wanted being completed.",
            "So this loop and it Luckily you rarely get loops like this in scientific and technical programs, but this loop can't be naively paralyzed because if you run this in reverse you get a different answer.",
            "This is taking data from left to right.",
            "So that can't be Parral."
        ],
        [
            "Well, you can put it to MP4 MP do in front of it.",
            "You'll just get the wrong answer.",
            "But SIM."
        ],
        [
            "Loops can be.",
            "This is a more complicated one of that one that one can."
        ],
        [
            "Paralyzer not salty, just like this one though.",
            "Is an absolute classic loop.",
            "It's like computing finite difference and this can be parallelized.",
            "You might think well, wait a second.",
            "You're referencing AI and AI minus one, but it doesn't matter, you're only reading them.",
            "You're not writing them.",
            "OK, so you're only writing to be, and because the you can compute this in any order, you get the same answer.",
            "OK, iteration, I is it about, independent from iteration, I plus one you can do them in any order, and this is actually the kind of loops you get in reality in scientific and technical code."
        ],
        [
            "So that can be parallelized.",
            "But a good a good trick is run, run the loop in reverse from N to 1, not one to N, and see if you get the same answer.",
            "And that's a good."
        ],
        [
            "At the check.",
            "So these are just things that I've already done."
        ],
        [
            "Paralelo MP4"
        ],
        [
            "We saw that there's a single clause parallel do in parallel for, but I reiterate, it's very, very important.",
            "There are two fundamental concepts in open MP.",
            "One is a parallel region where you spawn the threads, one is a worksharing directive where you split up the iterations and this just conflates the two.",
            "But they are still two independent things."
        ],
        [
            "And reduction.",
            "So basically the parallel loop index variable is private by default.",
            "If you have nested loops, Yep.",
            "Play today.",
            "This kind of loop parallelization can be done by compiler directives."
        ],
        [
            "Yes, So what is the difference between using especially open MP for this part as opposed to using compiler directives which can just parallelize loop?",
            "For example, you have do concurrent in Fortran.",
            "Oh, becausw.",
            "First of all open MP's been around for 15 or so years.",
            "So it was the first one to come along.",
            "But Secondly open MP is prescriptive open.",
            "MP says you will spawn the parallel regions you will.",
            "Split this loop hop, I think do concurrent.",
            "Kind of, it's just kind of a hint, isn't it?",
            "Saying this can be done concurrently, but it doesn't mandate that it's done concurrently.",
            "Does it?",
            "It's just ahead.",
            "I mean automatic compiler parallelization is a nightmare because.",
            "Vendors love it.",
            "Use our compiler.",
            "Your code goes really fast, so you great.",
            "My code goes really fast.",
            "Then you try and move machine.",
            "It's a different guy, doesn't go it doesn't.",
            "It doesn't compile.",
            "It doesn't say you're locked in.",
            "You want to be explicit.",
            "That's why you don't want to rely on automatic parallelization because it may work.",
            "It may not work, so that's why what you'd like is an automatically parallelizing compiler that produced open MP.",
            "But then you could run that anywhere, so no, no, no compiler manufacture is going to do that.",
            "Use our compiler to generate code to run on somebody elses system and not do that, so.",
            "So that's really I don't like automatic so simple loops the compiler can parallelize them.",
            "But I.",
            "But I I don't like to rely on that have to say and it's so simple to do this, it's it's.",
            "It's probably.",
            "You have to worry that you don't want the compiler to automatically parallelize as well, so that, but if it probably turns that off, that's that's not going to happen."
        ],
        [
            "So we've covered that.",
            "There's some technicalities that the parallel loop index is private by default, so you don't have to say private I, it's already private by default.",
            "If you have, do I do J2K and four Tran, then all of those are parallel or private in C for technical reasons.",
            "If you if you have four I4J4K as in Frank was wanted then the the J&K aren't private but it's a bit of a subtlety."
        ],
        [
            "This is the schedule clause, so this is the default schedule is just to split the loop up into chunks, which is often what you want, but not always.",
            "So you can specify a schedule which can be static, dynamic, guided auto or runtime and a chunk size here."
        ],
        [
            "So with no chunks I specify, the iteration space is divided to approximately equal chunks.",
            "So that's the default, but if chunk sizes specified, issues based divide the chunks each of chunk size iterations of the chondrocytes cyclically to each thread in order.",
            "So I did cyclic.",
            "I did static, one and that meant divide the iteration space into blocks of one and give each block to a different thread.",
            "But I could have gone static, two and then thread 0 down iteration zero and one thread one would've done iterations two and three and then background again, yeah.",
            "Could you clarify again on the parallel 4 if you have some sort of nested for loop?",
            "Oh so, so there's two things to know with the parallel 4 if you have nested 4.",
            "First of all, just if I just do some code.",
            "So if you have four I equals nought I less than NI double plus.",
            "For Jake was not jail S&N J double plus.",
            "OK.",
            "If I do hash pragma.",
            "OMP parallel 4K.",
            "I have to do in CI.",
            "Have to do private.",
            "Jay because so this first of all.",
            "I think this building because see loops are so general an enforced on you wouldn't, but you have to.",
            "Do you have to do Private J probably.",
            "But more importantly, this directive applies to that for loop and that for loop alone it splits up the following iteration space amongst all the threads and it doesn't touch this one.",
            "So.",
            "It doesn't, it just so that that's like doing what's it like doing.",
            "It's like if you're operating on the matrix.",
            "Dividing up in strips, but not into blocks.",
            "Yeah, 'cause it's only it's splitting up one.",
            "You could do anything you want.",
            "You could define.",
            "I start, I stop J, start J, stop and play around yourself if you wanted.",
            "But by default.",
            "Unless you have something called nested parallelism turned on the directives applied only to this the following loop.",
            "So that may not be running again, that's normally OK.",
            "It's not perfect.",
            "Isn't it in general better to paralyze the inner loop as opposed to the outer loop?",
            "No, no, exactly the opposite, because here I come here, you spawn the threads once.",
            "There's an overhead to sporting the threads.",
            "If you paralyze the inner loop, you spawn the threads multiple times.",
            "And also for sort of cache memory layout layout operations in C. You always want to paralyze the outer loop.",
            "Now, whether you do for I4J4K or 4K, four J4I is C Fortran memory layout issue.",
            "But no, you want to now you're probably thinking about GPU programming.",
            "GPU's are funny, no.",
            "OK no you should paralyze the outer loop because then you create the threads once and remember there's also a.",
            "There's also a barrier at the end of the loop, so if you paralyze the inner loop, you spawn the loops N tight.",
            "You spawn the threads end times and you have any barriers.",
            "Whereas in this format you spawn the loop the threads.",
            "Once you have one barrier.",
            "So it's actually it is.",
            "It is the outer loop you want to parallelize.",
            "You can paralyze the eneloops.",
            "There's nothing to stop you doing that.",
            "This is perfectly legitimate.",
            "You know the logic may mean you have to do this, yeah?",
            "You can do this if you want.",
            "In fact, I would do anything I could just do that.",
            "But if you have a choice, you should.",
            "You should try and do the outer loop.",
            "So I talked about the chunk size."
        ],
        [
            "So this is a static schedule 546 iterations.",
            "It might split up.",
            "Oh sorry.",
            "I wanted to ask, can you parallelize both loops so you can, but you need to turn on something called so by default open MP.",
            "Doesn't have net, it's called nested parallelism, so in openapi nested parallelism means if you encounter a parallel region, don't use all the threads, you some of them and then use some of the threads later on.",
            "The problem is that's a lot more effort on the runtime system, so actually if you have nested parallelism turned off, which is normally the default, the runtime system I need to know one thing am I in a parallel regional receiver region, but if.",
            "You have a nested parallelism turned on.",
            "You have to say, well, I might be in a parallel region.",
            "Oh, but how many threads are active?",
            "And now I need to I just become whole thing becomes so yes, in the first version of open MP, nested parallelism wasn't possible.",
            "It is now possible, but typically it actually.",
            "The overhead of the runtime system means that typically slows things down.",
            "So if you wanted to do a 2D decomposition, just have a nice just do it by hand.",
            "Have a nice start.",
            "I stopped J start, J stop, compute them, stick 'em in a macro or function unit to make it neat and tidy, you know, compute limits of you know and then do it that way.",
            "It's not so elegant, but probably more efficient.",
            "So if I have nested parallelism turned on, both loops will be paralyzed.",
            "I don't need another pragma, OMP parallel or in front of.",
            "You know you do So what you have to do is you have to do.",
            "Anne.",
            "You'd have to have two 2 parallelized hash pragma.",
            "OMP parallel 4 I.",
            "And I think it's an.",
            "Is it non thread you have to say number of threads so you see how many threads to do here equals four and then you might do something like this.",
            "So this would then say this would spawn 16 threads where the first threads each of the first four threads would then spawn another split into another four threads.",
            "I should know the syntax, but I can't remember it.",
            "That's a bit poor.",
            "How does it work?",
            "I just can't quite remember the syntax.",
            "Taken over.",
            "So you have to.",
            "You have to set an environment variable.",
            "Which is yes or no.",
            "Is that yeah, OK, sorry.",
            "OK, fine right one.",
            "OK, so first of all, because it's an overhead.",
            "There's an environment variable called OMP nested, which you have to set to true.",
            "It could still.",
            "It could still ignore you.",
            "It could say look, I don't do that the problem, but if you, you can set OMP nested equals true and then.",
            "Sorry, so that it's it's numb hashtag my own people and, um_threads of two.",
            "Sorry, that's I got the syntax wrong.",
            "Sorry about that so.",
            "So it's actually.",
            "Numb so just to state the obvious also about the the nested loops in which one if you should do the outer the inner it you have to keep in mind it's always also dependent on what kind of problem you have, so you may not have enough parallelism in your outer loop, for instance, or your outer loop may.",
            "Actually if you only use the outer loop for the parallelization, you might have great load imbalance and so therefore maybe the inner loop would be a better idea like many times.",
            "I've worked, I've had the outer loop be the dimensions, for instance, that's only three or four.",
            "Yeah, yeah, yeah, so you're right.",
            "So you've got the statement that all other things being equal, you should paralyze the absolute.",
            "But yes, as Banner said, if the item was rifles nor I less than three, I double.",
            "Plus you'd be like, well, you know.",
            "Then I do this one, but it's sort of.",
            "Nested parallelism was used to load balance codes, but Meanwhile open MP tasks were invented, so usually you don't use nested parallelism any longer, but you would create some tasks inside the outer parallel region.",
            "Yeah, so nested parallelism can be used to paralyze simple loops like this at multiple levels.",
            "So without nested parallelism, you can parallelize simple loops, a single level, which is easy and efficient if you want to do very sophisticated load balancing.",
            "As Christian said, you probably want to do something called tasks, which is completely general and completely dynamic, and I may touch on them.",
            "But as you say in the middle, if I was doing this I would just do it by hand.",
            "I would just have a little function that computed.",
            "I start and I stopped J start, Jay stop.",
            "It's not a big deal and you can you can put into macros or something if you want to make it look pretty.",
            "Um?",
            "So.",
            "Let's have a look so.",
            "This is static and this is static, Forced Atticus have blocks of four iterations and divide them up once iterations amongst threads."
        ],
        [
            "Dynamic is dynamic, so device situation about chunks of size chunk size and assigns into threads on a first come, first serve basis.",
            "So but no chunk size, it defaults to one, so I might say to do iterations nought to 347.",
            "And.",
            "8 to 11 and then you finish first.",
            "Do you?",
            "Do you know 12 to 15?",
            "Then you have a block size dynamics.",
            "A bit of overkill because scheduling is like packing you've got.",
            "Imagine you're moving house and you have lots of stuff to pack.",
            "OK, and you have three.",
            "You have three vans to fill.",
            "OK, you don't?",
            "You don't go and put a book in van one and then a book in van two in a plant pot in van three you get the big stuff you put your sofa in van one.",
            "You put your chairs in van two.",
            "You put your fridge in Van 3 and then you fill up the gap for the little stuff.",
            "So if you're giving out, iterate, there's an overhead to giving out iterations.",
            "Somebody has to ask them for what you should do is at the start you should give out big chunks of iterations.",
            "You do the first 100 you do the next 100 right you do the next 100, and then as you start to come in, you start to give them smaller and smaller chunks to fill in the gaps, and that's called that is that."
        ],
        [
            "It's called the guided schedule.",
            "Similar dynamic with chunks.",
            "I starts off larger and get small exponentially and chunks.",
            "I specify the minimum size of Chuck.",
            "There's some formula for basically.",
            "Guided is normally better than dynamic.",
            "Normally not always, but normally because it still balances the load 'cause the end of the day.",
            "You're giving out small chunks of iterations, but you don't have such an overhead of coordination."
        ],
        [
            "And so this could be dynamic.",
            "And guided with start off big and get smaller."
        ],
        [
            "There is an auto schedule which says.",
            "Let the compiler decide.",
            "This is really.",
            "Put in there to let people who do compiler development do cool things.",
            "The idea is if you have a very complicated loop and it's iterated lots of execute a lot of times, the compiler can try different things at runtime and then remember which was the best.",
            "I haven't checked, I don't know if this is implemented, I mean I know research compilers have this, so like Barcelona have stuff which does this kind of thing, but whether it's I don't know if it's been, I don't know if it's being implemented in real compilers, so I would not tend to use it.",
            "'cause I kind of like to know what I mean.",
            "In principle it's a great thing, but in practice I quite like to know what it's doing."
        ],
        [
            "Choosing a schedule well.",
            "Static is best for load balanced loops.",
            "Dynamics is widely very close guided as best.",
            "I mean, I don't have time to go into all this, but you know you have to.",
            "The main thing is just experiment.",
            "It's very easy to experiment and open MP and for a lot of scientific and technical codes then just static is fine.",
            "If you're looping over a big matrix with an operating on each element it's load give everyone the same number of iterations, But then it may not be a.",
            "Then if that doesn't work, you can try other things here."
        ],
        [
            "If you want to experiment, you might say, well that means I have to recompile every time where you don't.",
            "You can set the scheduled to be something called runtime and that says the runtime open MP decides what the schedule is based on the value of an environment variable called OMP shared your.",
            "So what that does is you can take a loop, do schedule equals runtime, compile it, and then you can do export to PNG equals static time.",
            "The code export to PNG equals dynamic time the code.",
            "It just allows you to experiment easily without having to recompile."
        ],
        [
            "There's some stuff here, but nested.",
            "You can collapse loops.",
            "So if you have a loop with the first where the 1st.",
            "Loop limit is very small and N is very small with both.",
            "N&M are smallish OK if N&M are smaller for N * M is big.",
            "This says just So what people used to do is open.",
            "MP used to just apply to the 1st loop.",
            "So what you do rewrite this loop is you do for I4K equals nought K less than M * N K double plus and then within it you'd say I is K percent N&J's K modem or whatever that kind of stuff you have to do.",
            "But all the collapse does is it says combine this into a single iteration space and split that up amongst threads.",
            "It just allows you to get more parallelism.",
            "It's not doing a 2D decomposition, but it's useful if these are both relatively small, but their product is large, so collapse can be useful.",
            "Will form a single loop of length N * N paralyzed that."
        ],
        [
            "Single is a block of code extra by single thread only.",
            "The first threads reached the single directory.",
            "Execute the block and so this can be useful.",
            "I.",
            "There's a synchronization point in the block.",
            "All the other threads waiting for the blocks being executed.",
            "It's a slightly weird one that you can come up with examples where it's."
        ],
        [
            "Useful.",
            "Single and signal.",
            "There is an example here with."
        ],
        [
            "Says we've basically got a parallel.",
            "Set up routine.",
            "Yeah, if you want to parallel then we want to read some data in from from file and we only want to be done by one thread.",
            "OK she wanted a parallel operation.",
            "Then read something in on one thread but we don't care which thread it is so you don't want to say this is only done on thread zero 'cause maybe thread 0 takes a long time to do this you just want this to be done by the 1st thread that gets there so that single.",
            "Then there's a barrier here which means that work will not be executed into.",
            "Why has been read?",
            "Having said that in real it's.",
            "It's not common.",
            "I have to say to you, single the one you use all the time is."
        ],
        [
            "Faster."
        ],
        [
            "Master, you use for print statements.",
            "Master says.",
            "Only thread zero will do this, but there's no Barry.",
            "It's the equivalent of rank equals zero.",
            "Print Hello World in MPI.",
            "So you often use master directives to to protect print steps.",
            "However, you have to remember that the easiest way to print to protect your print statements is just just have them in the serial region.",
            "Often parallel regions around you know you don't have to.",
            "Outdoor pee is quite easy to parallelize because all the print statements at the start of your code are all in the serial region, you know, so it doesn't matter.",
            "It's actually quite rare you tend to use it for debugging.",
            "But it's quite rare you want to do a presence in the parallel region.",
            "It's probably doing some big heavy computation.",
            "Don't waste time printing.",
            "OK, so but the important point is single the first thread that gets there does it, and then waits for everyone to catch up and you would never want to do that with the print master.",
            "Thread 0 does it, but just carries on.",
            "So it's exactly the same as saying if thread equals zero, then do this."
        ],
        [
            "And so it's it's like this again.",
            "Fortran has a start and end C relies on a structure block, and if you have more than one statement, you put curly brackets around them."
        ],
        [
            "Sections I'm not going to talk about this, but you can just say parallel sections just says run."
        ],
        [
            "Shruti then parallel with that one in parallel."
        ],
        [
            "That one is not paralyzing a loop."
        ],
        [
            "It's saying run different bits of code on different threads, but it's rarely about much."
        ],
        [
            "Yeah, it's not not in."
        ],
        [
            "It'll be useful."
        ],
        [
            "So I won't really cover it."
        ],
        [
            "The one thing where you get into problems is with with with array syntax.",
            "So this is.",
            "This is a problem, so if you're a Fortran programmer.",
            "You would do.",
            "You would do a curl on: equals a colon: plus B:: that adds that says that's the same thing as do I want to end do Jake from one to NAIJ equals AIJ plus BIJK before trying a mathematical language is just matrix notation equals 8 plus be.",
            "In fact you can just write 8 with a plus B, but you can't paralyze that with opening because there's no loops.",
            "OK, there's no loops there, so how do you parallelize it?",
            "Well, they introduced this thing called work share.",
            "So this is a bit weird because the normal thing about open MP is it's completely explicit.",
            "But work share isn't work share is just more.",
            "Could you paralyze this please?",
            "And I don't really care how is rather unsatisfactory, but it's there to allow you to to parallelize things like array syntax to be honest, if I were to do it, what I would do, let's make it a 3D loop.",
            "What I would do is I would put in a sink I would do.",
            "Do I get the wrong way around?",
            "K = 1 to N. I take out one of them.",
            "Yeah, and I do.",
            "So you don't need to take out all the eraser that you just need to give yourself enough loops to paralyze.",
            "I would probably do that.",
            "'cause I like to know how it's paralyzed and I don't know the problem by letting the compiler parallelize it might parallelize it in a different way from what you want, and I don't know if you know the way that caches and things work on shared, but that can be a disaster for performance.",
            "I will still work.",
            "It's a bit of a subtle point, but I would I would do this.",
            "It's not as it's not as elegant as array syntax, but I I don't really like the work share directive in my mind.",
            "Open MP is that is explicit programming model where you say exactly what you want to do.",
            "So.",
            "I think that's basically it."
        ],
        [
            "There's some work share stuff, why not go through that?",
            "It's rather."
        ],
        [
            "Rather detail."
        ],
        [
            "So the exercise is actually is on the sheet and this is this is the I'm doing a different exercise so.",
            "For those of you, I think everyone here yesterday, but if you go to the web page.",
            "Where is the exceed?",
            "Exceed wiki.",
            "The 2016 wiki is here.",
            "And it's under.",
            "Presentations.",
            "Presentations.",
            "NP and Open MP materials.",
            "And.",
            "The rachford sheet here.",
            "Is this open MP exercises Section 4 so?",
            "So I've given you an open MP version, but the Open MP version doesn't actually do anything, it just prints Hello World on multiple threads.",
            "I haven't parallelized any of the loops, but it's it's such a straightforward exercise, and in fact it's almost identical to the.",
            "It's almost identical to the pie the pie exercise, so there are two.",
            "There were only two significant loops in the code, so if I got if I go to the code.",
            "Is it traffic?",
            "I'll do 4 Chan this time just to be fair.",
            "If I do traffic dot F90K so the nice thing about.",
            "About Open MP, is it MPI to generate the road?",
            "I had this.",
            "I had this function that generated the road in in cereal and then I had to broad.",
            "I had to scatter.",
            "I could say right now you have the first part of the road you have set in shared memory.",
            "Do I have to do that?",
            "Remember that the roads on the whiteboard OK if and if you can't parallelize the initialization routine, you just get one thread to do it.",
            "But you don't care, you just just do it.",
            "It's there.",
            "OK?",
            "So that's why open MP is nice if you got a bit that we can't be bothered paralyzing.",
            "You just leave it in cereal that you can't do that in MPI because the data is distributed in MPI, but in an open MP, any thread, even the even the serial region, the master thread has access to all the data, all the shared data at least, which is the important stuff.",
            "But the two things are the only thing I put in here.",
            "Hello worlds.",
            "The first thing is the time is all in this update Rd thing here.",
            "And that's in a different routine, which is called traffic Lib dot F-90, I think.",
            "So you want to.",
            "Paralyzed this update Rd routine.",
            "Which is quite straightforward.",
            "We have old Rd new Road and move and I are the variables so you want to parallelize that loop.",
            "But you also want to parallelize the.",
            "The copy back loop which is here.",
            "And it's worth doing that in two stages.",
            "The code spends most of its time.",
            "Most in the in the update routine.",
            "But it's difficult to say 'cause I don't really understand the architecture of bridges, but but on a lot of computers you will.",
            "You will see what I expect you to see as you don't get decent speedup, at least on large numbers of threads, unless you paralyze both loops.",
            "That's not obvious, but but it's so I would also run on say 28 threads run on a large number of threads to see the effects, or maybe.",
            "Well, it's even more comp 14.",
            "Maybe I I guess, but anyway.",
            "It's a useful exercise, although an open MP is in principle an incremental model.",
            "You have to paralyze the bits that are slow.",
            "For performance reasons, it turns out that you probably have to paralyze most loops.",
            "Sorry, after you have to paralyze more loops than you might think and maybe discuss rather is later, but that but you can.",
            "That's the idea here is just to put OMP parallel four in parallel.",
            "Do about the two major loops and look at the timing and see how long it takes, how long it.",
            "How long it's?",
            "Takes how well it scales and such like, so I think, but it is the reason why the pie example is not a realistic example is there was no.",
            "There were no shared arrays in the pie example who are just adding up a series and a real program.",
            "You have a big you have big arrays and that's this old Rd new Road.",
            "That and that will be true in the I'll put it up over the break but that will be true in the.",
            "The the hybrid challenge you're operating on big arrays, and it's exactly it's exactly the same.",
            "Structure as this, so I think probably the best have a.",
            "Have a practical session till about well quarter past three.",
            "Maybe then we can take a 15 minute break and start again at 3:30.",
            "Does that make sense?",
            "That's not OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I was going to do was.",
                    "label": 0
                },
                {
                    "sent": "I'll go through the Pi example for the laboriously 'cause I think it illustrates, although it's a very simple example, illustrates really some of the key issues with open MP.",
                    "label": 0
                },
                {
                    "sent": "There's one issue, it doesn't illustrate, but the traffic model illustrates that.",
                    "label": 0
                },
                {
                    "sent": "So then I'll.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do is, I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll go through the Pi example, then I'll give a brief lecture on.",
                    "label": 0
                },
                {
                    "sent": "Parallel loops and then will.",
                    "label": 0
                },
                {
                    "sent": "They'll do the pie example.",
                    "label": 0
                },
                {
                    "sent": "I think that was.",
                    "label": 0
                },
                {
                    "sent": "I think that's sort of my original schedule.",
                    "label": 0
                },
                {
                    "sent": "I think I may be originally just thought I'd do some more.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In depth stuff, but let's have a look.",
                    "label": 0
                },
                {
                    "sent": "It was, it was.",
                    "label": 0
                },
                {
                    "sent": "Now, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's that's kind of.",
                    "label": 0
                },
                {
                    "sent": "I will do advance.",
                    "label": 0
                },
                {
                    "sent": "I'll do some things with Worksharing orphaning.",
                    "label": 0
                },
                {
                    "sent": "I'll see how far I get there, but I think this most important to go through this Pi exact, So what I'm going to do is basically I've just downloaded the code I'm going to see it, but actually could you put your hand up?",
                    "label": 0
                },
                {
                    "sent": "Issue a Fortran programmer?",
                    "label": 0
                },
                {
                    "sent": "And if you're a C programmer.",
                    "label": 0
                },
                {
                    "sent": "It's kind of 5050.",
                    "label": 0
                },
                {
                    "sent": "I'll do it and see 'cause it's.",
                    "label": 0
                },
                {
                    "sent": "It's not really much different, but I'll just do it and see.",
                    "label": 0
                },
                {
                    "sent": "So I'll take the serial code.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the serial code.",
                    "label": 0
                },
                {
                    "sent": "So to convert this to an open MP code I.",
                    "label": 0
                },
                {
                    "sent": "Can people read?",
                    "label": 0
                },
                {
                    "sent": "I don't like the people.",
                    "label": 0
                },
                {
                    "sent": "Read that with the colors.",
                    "label": 0
                },
                {
                    "sent": "I don't like the colors but.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so.",
                    "label": 0
                },
                {
                    "sent": "So I'll take that.",
                    "label": 0
                },
                {
                    "sent": "I'll take the.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "First thing you need to do is you just need to include a header hash.",
                    "label": 0
                },
                {
                    "sent": "Include ompa dot H. Because although open MP is primarily compiler functionality, there are some runtime libraries.",
                    "label": 0
                },
                {
                    "sent": "And then the classic thing you would do is.",
                    "label": 0
                },
                {
                    "sent": "You would do.",
                    "label": 0
                },
                {
                    "sent": "Print hello.",
                    "label": 0
                },
                {
                    "sent": "From thread Sandy out of Sandy.",
                    "label": 0
                },
                {
                    "sent": "And it's ompa.",
                    "label": 0
                },
                {
                    "sent": "Get.",
                    "label": 0
                },
                {
                    "sent": "Thread.",
                    "label": 0
                },
                {
                    "sent": "Numb lips.",
                    "label": 0
                },
                {
                    "sent": "Come on.",
                    "label": 0
                },
                {
                    "sent": "Time and.",
                    "label": 0
                },
                {
                    "sent": "NP get this is exactly the same in in Fortran.",
                    "label": 0
                },
                {
                    "sent": "In 4 Chan you do.",
                    "label": 0
                },
                {
                    "sent": "In Fortran, you just do.",
                    "label": 0
                },
                {
                    "sent": "Include OMP_Lib.",
                    "label": 0
                },
                {
                    "sent": "That's what you live, but that's what you do in Fortran so.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks so now if I compile this.",
                    "label": 0
                },
                {
                    "sent": "I get an error because it says I don't know what he get thread now and getting threads are and that's because by default on most compilers open MP is not turned on by default.",
                    "label": 0
                },
                {
                    "sent": "So I'm using the Intel compiler.",
                    "label": 0
                },
                {
                    "sent": "And it's minus Q open MP.",
                    "label": 0
                },
                {
                    "sent": "And the Intel compiler if it was.",
                    "label": 0
                },
                {
                    "sent": "If it was GCC, it'll be GCC minus F open MP.",
                    "label": 0
                },
                {
                    "sent": "If you're interested.",
                    "label": 0
                },
                {
                    "sent": "What is GCC but.",
                    "label": 0
                },
                {
                    "sent": "So now we can do make again and it compiles it and if I run it.",
                    "label": 0
                },
                {
                    "sent": "You see that no matter how many.",
                    "label": 0
                },
                {
                    "sent": "Threads I run it on.",
                    "label": 0
                },
                {
                    "sent": "I try and run it on four threads.",
                    "label": 0
                },
                {
                    "sent": "Exporter oppidum threads equals four and I run it Whoops.",
                    "label": 0
                },
                {
                    "sent": "I still get only one thread running hello from thread.",
                    "label": 0
                },
                {
                    "sent": "The writer thread one.",
                    "label": 0
                },
                {
                    "sent": "The reason is.",
                    "label": 0
                },
                {
                    "sent": "That open MP.",
                    "label": 0
                },
                {
                    "sent": "And completely unlike are MPI.",
                    "label": 0
                },
                {
                    "sent": "Has a fork join model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Lost my.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's not there yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You only, this is the program.",
                    "label": 0
                },
                {
                    "sent": "You only create multiple threads in parallel regions such as the first thing the parallel region is the fundamental construct in open MP, it spawns multiple threads.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can say OK. Whoops I'd say we can spawn apart Patrick and we can.",
                    "label": 0
                },
                {
                    "sent": "Let's do hash pragma.",
                    "label": 0
                },
                {
                    "sent": "OMP parallel and then we can maybe do hash pragma.",
                    "label": 0
                },
                {
                    "sent": "That's fine.",
                    "label": 0
                },
                {
                    "sent": "We let's just.",
                    "label": 0
                },
                {
                    "sent": "And now we compile that.",
                    "label": 0
                },
                {
                    "sent": "We get everyone printing so.",
                    "label": 0
                },
                {
                    "sent": "That's the first point, you know.",
                    "label": 0
                },
                {
                    "sent": "You only get parallelism when you want it, and that's the model of open MP.",
                    "label": 0
                },
                {
                    "sent": "The model is.",
                    "label": 0
                },
                {
                    "sent": "You take a serial code, you identify the regions you want to go faster.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the subroutines functions you want to go faster, and you paralyze them, but you can leave the whole of the rest of the code untouched.",
                    "label": 0
                },
                {
                    "sent": "And that's different from MPI to open.",
                    "label": 0
                },
                {
                    "sent": "MP is in principle an incremental parallel model, which is attractive as its downsides, but it is attractive, so you have to spawn a parallel region in open MP even to find out how many threads there are, because otherwise it runs on one thread.",
                    "label": 0
                },
                {
                    "sent": "The next thing so.",
                    "label": 0
                },
                {
                    "sent": "But there's no magic here, although a parallel deep region does.",
                    "label": 0
                },
                {
                    "sent": "Is it spawns multiple threads?",
                    "label": 0
                },
                {
                    "sent": "And all the threads do the code within that region.",
                    "label": 0
                },
                {
                    "sent": "So what some people might do is, well, let's make this parallel hash pragma OMP parallel.",
                    "label": 0
                },
                {
                    "sent": "And then put this in a parallel region.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There we go, that's that's great, isn't it?",
                    "label": 0
                },
                {
                    "sent": "I've gotta do loop for loop which is parallel that let let's do that.",
                    "label": 0
                },
                {
                    "sent": "What what happens?",
                    "label": 0
                },
                {
                    "sent": "OK, so that didn't that.",
                    "label": 0
                },
                {
                    "sent": "Why didn't that?",
                    "label": 0
                },
                {
                    "sent": "That's weird.",
                    "label": 0
                },
                {
                    "sent": "OK, so I thought this was going to go wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, that's interesting.",
                    "label": 0
                },
                {
                    "sent": "This normally goes wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is I'm just now I'm confused.",
                    "label": 0
                },
                {
                    "sent": "As to why, let's let's, let's print.",
                    "label": 0
                },
                {
                    "sent": "Let's just print air with Hello World in there.",
                    "label": 0
                },
                {
                    "sent": "I think that actually the compiler is doing something weird here which is.",
                    "label": 0
                },
                {
                    "sent": "This time I'm in complete stupid.",
                    "label": 0
                },
                {
                    "sent": "Am I OK?",
                    "label": 0
                },
                {
                    "sent": "I'm being completely stupid.",
                    "label": 0
                },
                {
                    "sent": "Fine, OK, so that's what I meant to do.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so I'll take that hello.",
                    "label": 0
                },
                {
                    "sent": "Well sorry I didn't read past like that.",
                    "label": 0
                },
                {
                    "sent": "That's the confusion with open MP is 'cause you sometimes don't recompile?",
                    "label": 0
                },
                {
                    "sent": "Can you just change number?",
                    "label": 0
                },
                {
                    "sent": "Thanks OK, fine so OK.",
                    "label": 0
                },
                {
                    "sent": "So let's just do that.",
                    "label": 0
                },
                {
                    "sent": "Let's run it on four threads.",
                    "label": 0
                },
                {
                    "sent": "I don't need to recompile and we we run it.",
                    "label": 0
                },
                {
                    "sent": "We run the code OK.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I get the right answer.",
                    "label": 0
                },
                {
                    "sent": "Let's do more minutes.",
                    "label": 0
                },
                {
                    "sent": "So actually I can.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I know what's going on here, I can.",
                    "label": 0
                },
                {
                    "sent": "I'm having to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so I'm there.",
                    "label": 0
                },
                {
                    "sent": "It's not death, it's not necessary.",
                    "label": 0
                },
                {
                    "sent": "I'm having to play around a bit, but OK.",
                    "label": 0
                },
                {
                    "sent": "So, So what it's done is it's done.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 = 12.566 OK. And absent errors 300%, what does that mean?",
                    "label": 0
                },
                {
                    "sent": "This is exactly four times the value of Pi.",
                    "label": 0
                },
                {
                    "sent": "OK, the reason it didn't go wrong before is that in this simple program, I think the threads are running exactly the same time, but there's.",
                    "label": 0
                },
                {
                    "sent": "But anyway, what's happened is a parallel region does what it says on the tin, it says.",
                    "label": 0
                },
                {
                    "sent": "I want every code.",
                    "label": 0
                },
                {
                    "sent": "I want every it spawns threads and all the threads execute all the code OK.",
                    "label": 0
                },
                {
                    "sent": "The compiler is not.",
                    "label": 0
                },
                {
                    "sent": "The compiler doesn't go or he wants a parallel region.",
                    "label": 0
                },
                {
                    "sent": "Oh, he's got a four loop out.",
                    "label": 0
                },
                {
                    "sent": "You know, the compiler does what you say it does.",
                    "label": 0
                },
                {
                    "sent": "Please give me 4 threads.",
                    "label": 0
                },
                {
                    "sent": "I want them all to execute this code and they're all adding up to the same pivetta value and actually.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 here is actually within here Pirsa shared variable.",
                    "label": 0
                },
                {
                    "sent": "And so they're all computing Pi into the same variable, so you're getting four times the result.",
                    "label": 0
                },
                {
                    "sent": "OK, so you've done what you've done, what it asked it to do, you have spawned four parallel threads, and they've just all run that code.",
                    "label": 0
                },
                {
                    "sent": "That's not what you want, so that we could do exactly what we did with, so I should be.",
                    "label": 0
                },
                {
                    "sent": "I should be a bit more clear and say I'll do default.",
                    "label": 0
                },
                {
                    "sent": "None.",
                    "label": 0
                },
                {
                    "sent": "So this is if I do a parallel region I have to, I should scope within that parallel region variables can be shared or private and I should scope them.",
                    "label": 0
                },
                {
                    "sent": "But you can use the compiler to help you.",
                    "label": 0
                },
                {
                    "sent": "So what it says here is you've got a parallel region you haven't told me where the I is.",
                    "label": 0
                },
                {
                    "sent": "What I is, what N is expired.",
                    "label": 0
                },
                {
                    "sent": "So I need to say what I end and Pi are here.",
                    "label": 0
                },
                {
                    "sent": "So I will now say.",
                    "label": 0
                },
                {
                    "sent": "Private I could loop variables always have to be private shared.",
                    "label": 0
                },
                {
                    "sent": "And they're all using the shared and I'll I'll do \u03c0 as well, but just not completely correct, but I'll do that OK.",
                    "label": 0
                },
                {
                    "sent": "But what I also want to do is something like Private Eye.",
                    "label": 0
                },
                {
                    "sent": "I start.",
                    "label": 0
                },
                {
                    "sent": "I stop.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do is now is just the same thing as I did in.",
                    "label": 0
                },
                {
                    "sent": "Is the same thing as I did with the opening the MPI code.",
                    "label": 0
                },
                {
                    "sent": "I stop.",
                    "label": 0
                },
                {
                    "sent": "I'll get rid of that.",
                    "label": 0
                },
                {
                    "sent": "And then I just have to say, well, I start.",
                    "label": 0
                },
                {
                    "sent": "Equals.",
                    "label": 0
                },
                {
                    "sent": "One plus.",
                    "label": 0
                },
                {
                    "sent": "ROMP.",
                    "label": 0
                },
                {
                    "sent": "Get thread.",
                    "label": 0
                },
                {
                    "sent": "Numb.",
                    "label": 0
                },
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "An over.",
                    "label": 0
                },
                {
                    "sent": "Clampi get numb.",
                    "label": 0
                },
                {
                    "sent": "Threads, I think that's right.",
                    "label": 0
                },
                {
                    "sent": "And I stop.",
                    "label": 0
                },
                {
                    "sent": "Can't type.",
                    "label": 0
                },
                {
                    "sent": "Equals I start.",
                    "label": 0
                },
                {
                    "sent": "Plus minus one probably.",
                    "label": 0
                },
                {
                    "sent": "And then this is a thing you know I can just print F of.",
                    "label": 0
                },
                {
                    "sent": "Thread sent the istar percent DI stop sent D. Just to debug it.",
                    "label": 0
                },
                {
                    "sent": "IMP get I should have put these in variables thread.",
                    "label": 0
                },
                {
                    "sent": "Nam.",
                    "label": 0
                },
                {
                    "sent": ", I start, I stopped so you can always print these things out.",
                    "label": 0
                },
                {
                    "sent": "So let's see if that compiles.",
                    "label": 0
                },
                {
                    "sent": "It probably won't.",
                    "label": 0
                },
                {
                    "sent": "'cause I haven't done.",
                    "label": 0
                },
                {
                    "sent": "I start my stop.",
                    "label": 0
                },
                {
                    "sent": "It's going to work OK, so now we can do dot slash price serial.",
                    "label": 0
                },
                {
                    "sent": "So these look right.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "I start.",
                    "label": 0
                },
                {
                    "sent": "Is 1 to 1 two 10 to this is this guy got it right too.",
                    "label": 0
                },
                {
                    "sent": "So I split the iteration space up by hand.",
                    "label": 0
                },
                {
                    "sent": "So this gives me the right now.",
                    "label": 0
                },
                {
                    "sent": "This is technically wrong because I said you should have just been lucky here.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't accumulate into a shared variable 'cause they could be these race conditions, but for redo with this architecture reasons I don't quite understand.",
                    "label": 0
                },
                {
                    "sent": "It doesn't show up here.",
                    "label": 0
                },
                {
                    "sent": "We could maybe try a more threads, maybe 8.",
                    "label": 0
                },
                {
                    "sent": "It still works OK.",
                    "label": 0
                },
                {
                    "sent": "I don't understand why, but there is what?",
                    "label": 0
                },
                {
                    "sent": "What you what we really want to do is we want to.",
                    "label": 0
                },
                {
                    "sent": "The most efficient way to do this is for everybody to accumulate into a local variable and then to only at the end put that into the shit into the shared variable.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the way that that's the way that we want to do it.",
                    "label": 0
                },
                {
                    "sent": "So what we want is we want to have.",
                    "label": 0
                },
                {
                    "sent": "A local pie.",
                    "label": 0
                },
                {
                    "sent": "Local pie, whoops, local pie, let's call it my pie.",
                    "label": 0
                },
                {
                    "sent": "That's a bit more.",
                    "label": 0
                },
                {
                    "sent": "OK. We have to make that.",
                    "label": 0
                },
                {
                    "sent": "Private because everyone has their own value.",
                    "label": 0
                },
                {
                    "sent": "Local pie.",
                    "label": 0
                },
                {
                    "sent": "We have to say.",
                    "label": 0
                },
                {
                    "sent": "Local pie equals then it's not local Pirates.",
                    "label": 0
                },
                {
                    "sent": "My pie, isn't it?",
                    "label": 0
                },
                {
                    "sent": "My pie.",
                    "label": 0
                },
                {
                    "sent": "So I will see.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "And now Pi at the end we do \u03c0 = \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Plus my pie, so you see what I'm doing here?",
                    "label": 0
                },
                {
                    "sent": "I'm accumulating the value into a local.",
                    "label": 0
                },
                {
                    "sent": "A local lamb.",
                    "label": 0
                },
                {
                    "sent": "Variable and then.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was So what I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "Is I am.",
                    "label": 0
                },
                {
                    "sent": "If I can get this all in the screen.",
                    "label": 0
                },
                {
                    "sent": "And sporting a parallel region.",
                    "label": 0
                },
                {
                    "sent": "I'm saying within that tie and Anna shared but all the other variables are local.",
                    "label": 0
                },
                {
                    "sent": "I'm setting my local my \u03c0 to 0 and my hand splitting up the iteration space by this.",
                    "label": 0
                },
                {
                    "sent": "Chunking it up and then looping over different spaces, incrementing into a variable my pie, and then include and then accumulating into pie.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's, that's what I showed on the slide as Yep.",
                    "label": 0
                },
                {
                    "sent": "There yes exactly exactly, so there is a race condition there, so this code is wrong, but hopefully it will.",
                    "label": 0
                },
                {
                    "sent": "It will.",
                    "label": 0
                },
                {
                    "sent": "It will show that it's wrong, so let's hope it does.",
                    "label": 0
                },
                {
                    "sent": "I have to take the prints out.",
                    "label": 0
                },
                {
                    "sent": "So I think what's happening is.",
                    "label": 0
                },
                {
                    "sent": "You get a race condition.",
                    "label": 0
                },
                {
                    "sent": "You get a race condition.",
                    "label": 0
                },
                {
                    "sent": "If two threads do something at the same time.",
                    "label": 0
                },
                {
                    "sent": "But when I do a print statement which takes quite a lot of time, it kind of throws them out of sync.",
                    "label": 0
                },
                {
                    "sent": "So let's see if this works.",
                    "label": 0
                },
                {
                    "sent": "There we are.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so we get it with four OK?",
                    "label": 0
                },
                {
                    "sent": "So if I do it, I got the right answer there and I got the right answer and then I got the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "So this is a classic example of a race condition.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you're lucky, OK?",
                    "label": 0
                },
                {
                    "sent": "Write value of \u03c0 right value of \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you're unlucky and it's always going to be less.",
                    "label": 0
                },
                {
                    "sent": "This always happens when two threads.",
                    "label": 0
                },
                {
                    "sent": "You know two threads right at the same time, so only one of them take ad at the same time only so that so the value is always less.",
                    "label": 0
                },
                {
                    "sent": "Here this is the classic, so you know this is, you know, I run my code, it's correct or it's not correct.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                },
                {
                    "sent": "This is surprising.",
                    "label": 0
                },
                {
                    "sent": "It's surprisingly subtle, but you do.",
                    "label": 0
                },
                {
                    "sent": "You do get these these errors.",
                    "label": 0
                },
                {
                    "sent": "So this is a very naive and as you said.",
                    "label": 0
                },
                {
                    "sent": "What we should, what we have to do and well spotted is that that is an incorrect code we have to do something down here.",
                    "label": 0
                },
                {
                    "sent": "I haven't covered it, but the way to do it in in.",
                    "label": 0
                },
                {
                    "sent": "An open MP is something called critical.",
                    "label": 0
                },
                {
                    "sent": "That says.",
                    "label": 0
                },
                {
                    "sent": "This is a critical section, which means that open MP says only one, only one thread is allowed to enter that piece of code at once so that there's a lock around here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I could have what I could have done is I could have put a critical section here.",
                    "label": 0
                },
                {
                    "sent": "I could have just had \u03c0 = \u03c0 Plus here and put a critical section around that update.",
                    "label": 0
                },
                {
                    "sent": "OK, not not bother with all these my Pi stuff, but just just incremented into the shared variable pipe and put that in a critical section.",
                    "label": 0
                },
                {
                    "sent": "Why is that a bad idea?",
                    "label": 0
                },
                {
                    "sent": "It's effectively a serial code.",
                    "label": 0
                },
                {
                    "sent": "It's saying you know every time anybody wants to update this, they have to wait for everyone else to do, and you've affected the serialized the whole code.",
                    "label": 0
                },
                {
                    "sent": "So the idea is do everything is locally as much as possible, so this is happening 840 times this edition.",
                    "label": 0
                },
                {
                    "sent": "This edition is happening four times and of course N and a real code would be millions so you know you should know that this is the way to do it.",
                    "label": 0
                },
                {
                    "sent": "Do a local accumulation and the global accumulation and this kind of mirrors MPI.",
                    "label": 0
                },
                {
                    "sent": "You know this sort of you do a local accumulation.",
                    "label": 0
                },
                {
                    "sent": "This is like an all reduce in that in that.",
                    "label": 0
                },
                {
                    "sent": "In that kind of sense, so this should now work.",
                    "label": 0
                },
                {
                    "sent": "If I make it.",
                    "label": 0
                },
                {
                    "sent": "And then I run it.",
                    "label": 0
                },
                {
                    "sent": "I should get the right answer and it definitely goes wrong when I do lots of threads.",
                    "label": 0
                },
                {
                    "sent": "So let's use 100 threads.",
                    "label": 0
                },
                {
                    "sent": "And that's not going to work.",
                    "label": 0
                },
                {
                    "sent": "Because I assume that it's divisible by.",
                    "label": 0
                },
                {
                    "sent": "I assume that number threat?",
                    "label": 0
                },
                {
                    "sent": "No, I don't do I?",
                    "label": 0
                },
                {
                    "sent": "Yes I do.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming the numbers divisible by we can do 84.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I get the right answer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I get the right answer there.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "You know this is quite this shows.",
                    "label": 0
                },
                {
                    "sent": "This shows what you have to do to correctly implement this.",
                    "label": 0
                },
                {
                    "sent": "OK. Get it all on one screen.",
                    "label": 0
                },
                {
                    "sent": "You have to spawn a parallel region which spawns parallel multiple threads, but it's not magic.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do any magic right?",
                    "label": 0
                },
                {
                    "sent": "It just spawns multiple threads who all do all this code?",
                    "label": 0
                },
                {
                    "sent": "OK, then I have private variable.",
                    "label": 0
                },
                {
                    "sent": "My pipe the accumulation and I have private loop counters and start and stop.",
                    "label": 0
                },
                {
                    "sent": "I compute them on each thread.",
                    "label": 0
                },
                {
                    "sent": "I manually split up the loop.",
                    "label": 0
                },
                {
                    "sent": "The iterations between threads to make sure they do different iterations.",
                    "label": 0
                },
                {
                    "sent": "I then have to do the accumulation.",
                    "label": 0
                },
                {
                    "sent": "A local Ek, now in fact.",
                    "label": 0
                },
                {
                    "sent": "This is where open MP starts to help you that actually you don't need to do all this stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, so because this is such a common this so this doesn't look anything like my serial code.",
                    "label": 0
                },
                {
                    "sent": "This is just garbage basically, but open MP recognizes that people want to do this a lot, so there are lots of shortcuts which I'll show you now, but it's really important you realize this is what the compiler is doing under the hood.",
                    "label": 0
                },
                {
                    "sent": "It's doing all this stuff.",
                    "label": 0
                },
                {
                    "sent": "Everything else is just shortcuts to get the compiler to do this stuff for you.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is what it's doing.",
                    "label": 0
                },
                {
                    "sent": "It's not magic, it's spawning different threads.",
                    "label": 0
                },
                {
                    "sent": "It's saying look, you do loop iterations.",
                    "label": 0
                },
                {
                    "sent": "This to this you do that to that.",
                    "label": 0
                },
                {
                    "sent": "You do that to that and come back OK and you can do it all by hand if you want, and sometimes you need to do it if open MP doesn't support what you want to do, but for most simple loops and calculations open MP supports it.",
                    "label": 0
                },
                {
                    "sent": "So the first one is.",
                    "label": 0
                },
                {
                    "sent": "I don't need to do all this stuff with my \u03c0. OK.",
                    "label": 0
                },
                {
                    "sent": "I do this.",
                    "label": 0
                },
                {
                    "sent": "I say my.",
                    "label": 0
                },
                {
                    "sent": "Let's you say that Pi is a reduction variable.",
                    "label": 0
                },
                {
                    "sent": "But you have to say what to do with whoopsie?",
                    "label": 0
                },
                {
                    "sent": "Daisy, you have to say.",
                    "label": 0
                },
                {
                    "sent": "So I'm at the Emacs is a bit slow.",
                    "label": 0
                },
                {
                    "sent": "You have to say it.",
                    "label": 0
                },
                {
                    "sent": "You have to tell the compiler what you do, he said, look, I'm computing this thing pie and then computing it via addition.",
                    "label": 0
                },
                {
                    "sent": "So what I want you to do is to initialize a local variable.",
                    "label": 0
                },
                {
                    "sent": "Do the computation and then at the end accumulate them back together into the global variable using addition.",
                    "label": 0
                },
                {
                    "sent": "OK, so the telling the compiler you want to use addition tells it two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, it tells it that at the end of the loop it should accumulate them with addition, but it also tells it something else.",
                    "label": 0
                },
                {
                    "sent": "It tells it what this value should be, what the what the default value of the local variable should be.",
                    "label": 0
                },
                {
                    "sent": "'cause?",
                    "label": 0
                },
                {
                    "sent": "What would it be if that was multiplication?",
                    "label": 0
                },
                {
                    "sent": "What's the unit operation for multiplication one so it would have to be my pie equals one.",
                    "label": 0
                },
                {
                    "sent": "So this this the compiler trusts you, the kind that doesn't check, or you said you're doing.",
                    "label": 0
                },
                {
                    "sent": "Additionally it just you know all it says is instead of computing Pi, compute it locally on a thread into available.",
                    "label": 0
                },
                {
                    "sent": "Could local pie or my pie then then accumulate them back together using this operation.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you're doing subtraction, it's addition.",
                    "label": 0
                },
                {
                    "sent": "'cause additions just subtracted.",
                    "label": 0
                },
                {
                    "sent": "Subtracted.",
                    "label": 0
                },
                {
                    "sent": "Just adding a negative number.",
                    "label": 0
                },
                {
                    "sent": "There's no such thing as reduction with negativity that need to think about that a bit.",
                    "label": 0
                },
                {
                    "sent": "So now I can get rid of all this garbage here at least.",
                    "label": 0
                },
                {
                    "sent": "So I get rid of that.",
                    "label": 0
                },
                {
                    "sent": "I can get rid of this.",
                    "label": 0
                },
                {
                    "sent": "MIPI stuff 'cause the compiler is going to do that for me and I can get rid of this 'cause the compiler does it all for me and I don't know where does that match.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically the only thing you have to realize is.",
                    "label": 0
                },
                {
                    "sent": "The value of \u03c0 is defined at the end of the parallel region, but within the parallel region.",
                    "label": 0
                },
                {
                    "sent": "The problem is open.",
                    "label": 0
                },
                {
                    "sent": "MP is within the parallel region.",
                    "label": 0
                },
                {
                    "sent": "The compilers rewritten pie to be my pie, or you know, and so you.",
                    "label": 0
                },
                {
                    "sent": "But all you know is it correct at the end.",
                    "label": 0
                },
                {
                    "sent": "Within here you don't know what's going on, but all it's done is reduced that reproduce.",
                    "label": 0
                },
                {
                    "sent": "Replace that code.",
                    "label": 0
                },
                {
                    "sent": "Let's check that this works.",
                    "label": 0
                },
                {
                    "sent": "I'll do a sensible number of threads like 10, No 8.",
                    "label": 0
                },
                {
                    "sent": "AJ.",
                    "label": 0
                },
                {
                    "sent": "And then I get the right answer, OK?",
                    "label": 0
                },
                {
                    "sent": "Reproducibly, so also the compiler is just doing their what I did before, but the compiler can do more than that.",
                    "label": 0
                },
                {
                    "sent": "'cause clearly.",
                    "label": 0
                },
                {
                    "sent": "All this stuff with I star I stop and all that is just mechanical.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "I can just go with the original loop.",
                    "label": 0
                },
                {
                    "sent": "And there's a hash pragma.",
                    "label": 0
                },
                {
                    "sent": "O MP4 that says.",
                    "label": 0
                },
                {
                    "sent": "I want you to split this loop up so a lot of people think that OMP parallel splits loops up.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "It does nothing of the sort.",
                    "label": 0
                },
                {
                    "sent": "It creates multiple threads.",
                    "label": 0
                },
                {
                    "sent": "But oh, MP4 says OK. Split the iteration of this loop when it does nothing more than that code I did before I start, I stop on this stuff, but it's done for you and by default it split up into blocks.",
                    "label": 0
                },
                {
                    "sent": "You can do more complicated things, but my so that code should now work.",
                    "label": 0
                },
                {
                    "sent": "It still doesn't look quite like my.",
                    "label": 0
                },
                {
                    "sent": "That should give me the right answer.",
                    "label": 0
                },
                {
                    "sent": "And then I get the right answer.",
                    "label": 0
                },
                {
                    "sent": "OK, is it reproducible?",
                    "label": 0
                },
                {
                    "sent": "Yes it is, but but open MP does even more.",
                    "label": 0
                },
                {
                    "sent": "It says, well, actually.",
                    "label": 0
                },
                {
                    "sent": "I don't need this, I start and I stop anymore either.",
                    "label": 0
                },
                {
                    "sent": "So I can get put that back on the same line.",
                    "label": 0
                },
                {
                    "sent": "You could actually.",
                    "label": 0
                },
                {
                    "sent": "This is so common hashtag where OMP parallel hashtag to MP4 that you can put them together.",
                    "label": 0
                },
                {
                    "sent": "You can say hashtag by OMP parallel 4.",
                    "label": 0
                },
                {
                    "sent": "And as long as that's in front of a for loop.",
                    "label": 0
                },
                {
                    "sent": "Then you're OK.",
                    "label": 0
                },
                {
                    "sent": "So this, but it's important to note this does two things conceptually.",
                    "label": 0
                },
                {
                    "sent": "A it spawns lots of threads and be it splits up loop iteration space.",
                    "label": 0
                },
                {
                    "sent": "There two separate things.",
                    "label": 0
                },
                {
                    "sent": "We've just confirmed that that is just the serial code annotated with the directive, but under the hood it's doing.",
                    "label": 0
                },
                {
                    "sent": "It's just doing what I did by hand before.",
                    "label": 0
                },
                {
                    "sent": "There's no magic here, so I can do that.",
                    "label": 0
                },
                {
                    "sent": "And that should work.",
                    "label": 0
                },
                {
                    "sent": "And in fact.",
                    "label": 0
                },
                {
                    "sent": "Open MP does even more for you, it says.",
                    "label": 0
                },
                {
                    "sent": "The default skin that they you shouldn't really do this when you're ready, but I can take out if I know that if I can take out default none.",
                    "label": 0
                },
                {
                    "sent": "Loop variable, nobody would want to loop variable to be shared.",
                    "label": 0
                },
                {
                    "sent": "It's meaningless, so they are private by default.",
                    "label": 0
                },
                {
                    "sent": "And anything and normal variables are shared by default.",
                    "label": 0
                },
                {
                    "sent": "So in fact I can get away with that.",
                    "label": 0
                },
                {
                    "sent": "Hopefully.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so you know.",
                    "label": 0
                },
                {
                    "sent": "Open MP can look quite compact and magic, but it is doing nothing more than the stuff I write by hand.",
                    "label": 0
                },
                {
                    "sent": "But these are just shortcuts.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact if you wonder what it's doing.",
                    "label": 0
                },
                {
                    "sent": "I can just let's make ends smaller.",
                    "label": 0
                },
                {
                    "sent": "Let's make N 12.",
                    "label": 0
                },
                {
                    "sent": "Within the loop, you could still do stuff like saying.",
                    "label": 0
                },
                {
                    "sent": "Printf of.",
                    "label": 0
                },
                {
                    "sent": "Thread percent D doing loop.",
                    "label": 0
                },
                {
                    "sent": "Percent D. And you can just do that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'll do.",
                    "label": 0
                },
                {
                    "sent": "I'll do.",
                    "label": 0
                },
                {
                    "sent": "Or did I make it 12, didn't I?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So we do dot slash pyserial now.",
                    "label": 0
                },
                {
                    "sent": "You can see that it split them up into the order you thought threads are doing loop 123 thread wondering loop 456 thread to do loops it 870 ninth just on my thought but what I'll go to onto the next thing is that.",
                    "label": 0
                },
                {
                    "sent": "Although splitting loops up like this is mechanical, it's pretty.",
                    "label": 0
                },
                {
                    "sent": "It's pretty tedious and open.",
                    "label": 0
                },
                {
                    "sent": "MP allows you two different things.",
                    "label": 0
                },
                {
                    "sent": "You might say, well, I don't want.",
                    "label": 0
                },
                {
                    "sent": "I want a different distribution.",
                    "label": 0
                },
                {
                    "sent": "I want the loops to be.",
                    "label": 0
                },
                {
                    "sent": "Thread 0 does loop 0 thread one dustloop one thread do dot loop 2 back in it like dealing a pack of cards, not block it up, but in in a cyclic distribution and open MP supports that and I'll just.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about in the next lecture, but it's called the schedule clause, so you can say.",
                    "label": 0
                },
                {
                    "sent": "Schedule.",
                    "label": 0
                },
                {
                    "sent": "Of static, one which says split up by dealing at a pack of cards.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, you'll see that when we run it.",
                    "label": 0
                },
                {
                    "sent": "Threads loaded loop.",
                    "label": 0
                },
                {
                    "sent": "One thread wanted to loop 2 thread, two did loop 3 thread 3 did loop 4 then went back to the start again.",
                    "label": 0
                },
                {
                    "sent": "Or you can even do other things you can.",
                    "label": 0
                },
                {
                    "sent": "It's not, there's no, there's no advantage here.",
                    "label": 0
                },
                {
                    "sent": "You can actually have a dynamic schedule.",
                    "label": 0
                },
                {
                    "sent": "Which says.",
                    "label": 0
                },
                {
                    "sent": "Dynamic it says.",
                    "label": 0
                },
                {
                    "sent": "Give thread zero iteration, zero thread one iteration, one thread to iteration, two thread, three iteration 3, then wait for the first person to finish and give them the next iteration.",
                    "label": 0
                },
                {
                    "sent": "And that's very useful if the iterations are load imbalanced, because basically you know you just give it the stupid thing to do here.",
                    "label": 0
                },
                {
                    "sent": "But if the iterations if each iteration I is a lot of work and there's load imbalance, then it balances the load and the important point about this is that you don't know.",
                    "label": 0
                },
                {
                    "sent": "Which thread is going to do which loop?",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, thread zero did loops one and five and nothing else.",
                    "label": 0
                },
                {
                    "sent": "OK if I run it again or do a different it will do a different thing.",
                    "label": 0
                },
                {
                    "sent": "Here, thread zero Ditto threaded loads here because they're running at different speeds.",
                    "label": 0
                },
                {
                    "sent": "You know it's just.",
                    "label": 0
                },
                {
                    "sent": "You know, it's just it's first come, first served, and that's why it's called dynamic.",
                    "label": 0
                },
                {
                    "sent": "Each time you run it, it does something different, so that's just to try.",
                    "label": 0
                },
                {
                    "sent": "Thought I'd go.",
                    "label": 0
                },
                {
                    "sent": "I'll just now sort of go through the slides quite quick.",
                    "label": 0
                },
                {
                    "sent": "Things we've covered, most of it explicitly.",
                    "label": 0
                },
                {
                    "sent": "Are there any any questions though?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Plus I would get I would get garbage output, so I think because the compiler supports open MP, they've made printf thread safe so the input is interleaved on a line by line basis, but not on a character by character basis.",
                    "label": 0
                },
                {
                    "sent": "So this does not mean that thread 3 did loop 2 after thread zero did loop 5, because you know, who knows.",
                    "label": 0
                },
                {
                    "sent": "You know printing to the screen is like posting a letter, please you know so so you know this has nothing to do with the order they ran in between different threads.",
                    "label": 0
                },
                {
                    "sent": "There's no global time.",
                    "label": 0
                },
                {
                    "sent": "Who knows what printing to the screen actually does, but but an individual print is normally.",
                    "label": 0
                },
                {
                    "sent": "You know, thread safe, one at least a line by line basis, but you can't.",
                    "label": 0
                },
                {
                    "sent": "You cannot say here, so for it thread zeros output is in the right order, but you cannot say here that thread 3 did loop 2 after thread before threads invited count of fact it's impossible.",
                    "label": 0
                },
                {
                    "sent": "Threads for he could not have done Loop 2 before thread Zero did Luke 9, but the print statements got messed up on the way to the screen.",
                    "label": 0
                },
                {
                    "sent": "So each thread sends its print statements and they can get mangled up in different orders.",
                    "label": 0
                },
                {
                    "sent": "But but an individual thread is in order.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of semi threadsafe, but that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So what I was going to do is just go through.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That's not what I want to do.",
                    "label": 0
                },
                {
                    "sent": "It's quite a long lecture, but that's why I think it's quicker to go through it by by hand.",
                    "label": 0
                },
                {
                    "sent": "So this is worksharing lecture.",
                    "label": 0
                },
                {
                    "sent": "So I'll just.",
                    "label": 0
                },
                {
                    "sent": "This will just be pretty.",
                    "label": 0
                },
                {
                    "sent": "I'll go through it quite quickly 'cause I've been through everything on the screen.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've got parallel doing for loop signal direction.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to this, but the most important parallel two or four.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we said loops, the most common source of parallelism and you can divide the loop iterations up with two or four.",
                    "label": 1
                },
                {
                    "sent": "You don't have to do this, you can do it by hand if you want, but these are normally the easiest way of doing it.",
                    "label": 0
                },
                {
                    "sent": "Are there's a synchronization point at the end of the loop, all threads must finish their iterations before any thread can proceed, so this is, you know if you do a no MP4.",
                    "label": 1
                },
                {
                    "sent": "There is a barrier at the end of the loop 'cause you don't see it, but the open MP puts in 'cause it says.",
                    "label": 0
                },
                {
                    "sent": "Look when I go.",
                    "label": 0
                },
                {
                    "sent": "When I go to the next line after this.",
                    "label": 0
                },
                {
                    "sent": "The programmer clearly expects the entire for loop to have been completely computed, so by default open MP puts in barriers synchronization points at the end of the end of loops, which is not, which is a safe thing to do.",
                    "label": 0
                },
                {
                    "sent": "You can take them out if you're brave, but by default it puts them into open.",
                    "label": 0
                },
                {
                    "sent": "MP is quite conservative, it tends to put in things to make it easier for you to write correct programs.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've seen it so MP do an end do in Fortran I didn't do four times and hashtag or MP4.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With no additional clauses to do for directional partition iteration up as equally as possible between the threads.",
                    "label": 1
                },
                {
                    "sent": "Now I've got about 7 iterations to three threads.",
                    "label": 0
                },
                {
                    "sent": "It doesn't fit, so does it do 331?",
                    "label": 0
                },
                {
                    "sent": "We don't, who cares, but but open MP does guarantee that if it splits a loop of iterative count 7 / 3 threads, it will always split it the same way, but you don't know what it is, but it probably does this.",
                    "label": 0
                },
                {
                    "sent": "I don't know who knows, but of course it's round the margins you really spitting up a loofah 1,000,000 / 3 threads, so who cares for a thread .30 three 1333 or 30 three 1334?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's really lost in the noise.",
                    "label": 0
                },
                {
                    "sent": "But it can be important to note.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based on the same, this is just a lot of stuff, but.",
                    "label": 0
                },
                {
                    "sent": "I'll go back to.",
                    "label": 0
                },
                {
                    "sent": "This is just saying that C is a bit of a weird language.",
                    "label": 1
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So I'll just comment that out.",
                    "label": 0
                },
                {
                    "sent": "What does that statement say?",
                    "label": 0
                },
                {
                    "sent": "What does that tell the compiler to do?",
                    "label": 0
                },
                {
                    "sent": "I mean, Cedric, the stolen tell obvious answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is executed end times.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "OK saturated once in that case.",
                    "label": 0
                },
                {
                    "sent": "The C compiler cannot work out from this.",
                    "label": 0
                },
                {
                    "sent": "How many times the loop is iterated.",
                    "label": 0
                },
                {
                    "sent": "It's impossible because that is a totally legitimate vacancy.",
                    "label": 0
                },
                {
                    "sent": "Madness you been idiot to do it, but this loop is it rated one so open MP says look.",
                    "label": 0
                },
                {
                    "sent": "The If you've written a four loop, it's just a normal for loop.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Frank was one I less than I less than day of the week I less than random number I less than you know, because you like to do that you can do like with one eye less than day of the week I less than random number 'cause nobody does but the C4 loop is far too general 'cause 'cause open MP dies to know how many iterations are going to divide the loops up.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of because so busy says you can't do that.",
                    "label": 0
                },
                {
                    "sent": "Just don't be an idiot.",
                    "label": 0
                },
                {
                    "sent": "So all this all this stuff here about restrictions is just saying don't be an idiot, just do normal things.",
                    "label": 0
                },
                {
                    "sent": "Fortran do loops are very prescriptive.",
                    "label": 0
                },
                {
                    "sent": "A fortune do loop.",
                    "label": 0
                },
                {
                    "sent": "Do what you guys want to end mandates.",
                    "label": 0
                },
                {
                    "sent": "Unless there's an explicit exit statement, mandates that this will be done end times but but C doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Accent so.",
                    "label": 0
                },
                {
                    "sent": "How can you tell if a loop is parallel or not?",
                    "label": 1
                },
                {
                    "sent": "If you if you put a oh MP4 and MP do construct in front of loop open MP will parallelize it.",
                    "label": 0
                },
                {
                    "sent": "It will split the loop.",
                    "label": 0
                },
                {
                    "sent": "I won't check if it's sensible or not, it will just it will just do it.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 1
                },
                {
                    "sent": "A useful test if the loop gives the same answers if it's running reverse, then it's almost certainly parallel, but this loop here OK, can I parallelize this in a in a naive way?",
                    "label": 0
                },
                {
                    "sent": "A split across loops can?",
                    "label": 0
                },
                {
                    "sent": "I cannot get the same answer.",
                    "label": 0
                },
                {
                    "sent": "No, why not?",
                    "label": 0
                },
                {
                    "sent": "I know I well know that it's only a so pointer aliasing is an issue, but not here 'cause it's, but they're both A and that's four times.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, but even see it's OK.",
                    "label": 0
                },
                {
                    "sent": "So that's another reason why this isn't, but that's a good that is a good point, but it doesn't apply here.",
                    "label": 0
                },
                {
                    "sent": "This loop assumes that the iterations done in order, but because because the the I TH iteration depends explicitly on there, I minus want iteration.",
                    "label": 0
                },
                {
                    "sent": "So if I tell thread thread three to start iteration 20, it's going to get the wrong answer because iteration 19 wanted being completed.",
                    "label": 0
                },
                {
                    "sent": "So this loop and it Luckily you rarely get loops like this in scientific and technical programs, but this loop can't be naively paralyzed because if you run this in reverse you get a different answer.",
                    "label": 0
                },
                {
                    "sent": "This is taking data from left to right.",
                    "label": 0
                },
                {
                    "sent": "So that can't be Parral.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you can put it to MP4 MP do in front of it.",
                    "label": 0
                },
                {
                    "sent": "You'll just get the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "But SIM.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loops can be.",
                    "label": 0
                },
                {
                    "sent": "This is a more complicated one of that one that one can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paralyzer not salty, just like this one though.",
                    "label": 0
                },
                {
                    "sent": "Is an absolute classic loop.",
                    "label": 0
                },
                {
                    "sent": "It's like computing finite difference and this can be parallelized.",
                    "label": 0
                },
                {
                    "sent": "You might think well, wait a second.",
                    "label": 0
                },
                {
                    "sent": "You're referencing AI and AI minus one, but it doesn't matter, you're only reading them.",
                    "label": 0
                },
                {
                    "sent": "You're not writing them.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're only writing to be, and because the you can compute this in any order, you get the same answer.",
                    "label": 0
                },
                {
                    "sent": "OK, iteration, I is it about, independent from iteration, I plus one you can do them in any order, and this is actually the kind of loops you get in reality in scientific and technical code.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that can be parallelized.",
                    "label": 0
                },
                {
                    "sent": "But a good a good trick is run, run the loop in reverse from N to 1, not one to N, and see if you get the same answer.",
                    "label": 0
                },
                {
                    "sent": "And that's a good.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the check.",
                    "label": 0
                },
                {
                    "sent": "So these are just things that I've already done.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paralelo MP4",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We saw that there's a single clause parallel do in parallel for, but I reiterate, it's very, very important.",
                    "label": 1
                },
                {
                    "sent": "There are two fundamental concepts in open MP.",
                    "label": 1
                },
                {
                    "sent": "One is a parallel region where you spawn the threads, one is a worksharing directive where you split up the iterations and this just conflates the two.",
                    "label": 0
                },
                {
                    "sent": "But they are still two independent things.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And reduction.",
                    "label": 0
                },
                {
                    "sent": "So basically the parallel loop index variable is private by default.",
                    "label": 1
                },
                {
                    "sent": "If you have nested loops, Yep.",
                    "label": 0
                },
                {
                    "sent": "Play today.",
                    "label": 0
                },
                {
                    "sent": "This kind of loop parallelization can be done by compiler directives.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, So what is the difference between using especially open MP for this part as opposed to using compiler directives which can just parallelize loop?",
                    "label": 0
                },
                {
                    "sent": "For example, you have do concurrent in Fortran.",
                    "label": 0
                },
                {
                    "sent": "Oh, becausw.",
                    "label": 0
                },
                {
                    "sent": "First of all open MP's been around for 15 or so years.",
                    "label": 0
                },
                {
                    "sent": "So it was the first one to come along.",
                    "label": 0
                },
                {
                    "sent": "But Secondly open MP is prescriptive open.",
                    "label": 0
                },
                {
                    "sent": "MP says you will spawn the parallel regions you will.",
                    "label": 0
                },
                {
                    "sent": "Split this loop hop, I think do concurrent.",
                    "label": 0
                },
                {
                    "sent": "Kind of, it's just kind of a hint, isn't it?",
                    "label": 0
                },
                {
                    "sent": "Saying this can be done concurrently, but it doesn't mandate that it's done concurrently.",
                    "label": 0
                },
                {
                    "sent": "Does it?",
                    "label": 0
                },
                {
                    "sent": "It's just ahead.",
                    "label": 0
                },
                {
                    "sent": "I mean automatic compiler parallelization is a nightmare because.",
                    "label": 0
                },
                {
                    "sent": "Vendors love it.",
                    "label": 0
                },
                {
                    "sent": "Use our compiler.",
                    "label": 0
                },
                {
                    "sent": "Your code goes really fast, so you great.",
                    "label": 0
                },
                {
                    "sent": "My code goes really fast.",
                    "label": 0
                },
                {
                    "sent": "Then you try and move machine.",
                    "label": 0
                },
                {
                    "sent": "It's a different guy, doesn't go it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't compile.",
                    "label": 0
                },
                {
                    "sent": "It doesn't say you're locked in.",
                    "label": 0
                },
                {
                    "sent": "You want to be explicit.",
                    "label": 0
                },
                {
                    "sent": "That's why you don't want to rely on automatic parallelization because it may work.",
                    "label": 0
                },
                {
                    "sent": "It may not work, so that's why what you'd like is an automatically parallelizing compiler that produced open MP.",
                    "label": 0
                },
                {
                    "sent": "But then you could run that anywhere, so no, no, no compiler manufacture is going to do that.",
                    "label": 0
                },
                {
                    "sent": "Use our compiler to generate code to run on somebody elses system and not do that, so.",
                    "label": 0
                },
                {
                    "sent": "So that's really I don't like automatic so simple loops the compiler can parallelize them.",
                    "label": 0
                },
                {
                    "sent": "But I.",
                    "label": 0
                },
                {
                    "sent": "But I I don't like to rely on that have to say and it's so simple to do this, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's probably.",
                    "label": 0
                },
                {
                    "sent": "You have to worry that you don't want the compiler to automatically parallelize as well, so that, but if it probably turns that off, that's that's not going to happen.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've covered that.",
                    "label": 0
                },
                {
                    "sent": "There's some technicalities that the parallel loop index is private by default, so you don't have to say private I, it's already private by default.",
                    "label": 1
                },
                {
                    "sent": "If you have, do I do J2K and four Tran, then all of those are parallel or private in C for technical reasons.",
                    "label": 0
                },
                {
                    "sent": "If you if you have four I4J4K as in Frank was wanted then the the J&K aren't private but it's a bit of a subtlety.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the schedule clause, so this is the default schedule is just to split the loop up into chunks, which is often what you want, but not always.",
                    "label": 0
                },
                {
                    "sent": "So you can specify a schedule which can be static, dynamic, guided auto or runtime and a chunk size here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with no chunks I specify, the iteration space is divided to approximately equal chunks.",
                    "label": 1
                },
                {
                    "sent": "So that's the default, but if chunk sizes specified, issues based divide the chunks each of chunk size iterations of the chondrocytes cyclically to each thread in order.",
                    "label": 0
                },
                {
                    "sent": "So I did cyclic.",
                    "label": 0
                },
                {
                    "sent": "I did static, one and that meant divide the iteration space into blocks of one and give each block to a different thread.",
                    "label": 0
                },
                {
                    "sent": "But I could have gone static, two and then thread 0 down iteration zero and one thread one would've done iterations two and three and then background again, yeah.",
                    "label": 0
                },
                {
                    "sent": "Could you clarify again on the parallel 4 if you have some sort of nested for loop?",
                    "label": 0
                },
                {
                    "sent": "Oh so, so there's two things to know with the parallel 4 if you have nested 4.",
                    "label": 0
                },
                {
                    "sent": "First of all, just if I just do some code.",
                    "label": 0
                },
                {
                    "sent": "So if you have four I equals nought I less than NI double plus.",
                    "label": 0
                },
                {
                    "sent": "For Jake was not jail S&N J double plus.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If I do hash pragma.",
                    "label": 0
                },
                {
                    "sent": "OMP parallel 4K.",
                    "label": 0
                },
                {
                    "sent": "I have to do in CI.",
                    "label": 0
                },
                {
                    "sent": "Have to do private.",
                    "label": 0
                },
                {
                    "sent": "Jay because so this first of all.",
                    "label": 0
                },
                {
                    "sent": "I think this building because see loops are so general an enforced on you wouldn't, but you have to.",
                    "label": 0
                },
                {
                    "sent": "Do you have to do Private J probably.",
                    "label": 0
                },
                {
                    "sent": "But more importantly, this directive applies to that for loop and that for loop alone it splits up the following iteration space amongst all the threads and it doesn't touch this one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, it just so that that's like doing what's it like doing.",
                    "label": 0
                },
                {
                    "sent": "It's like if you're operating on the matrix.",
                    "label": 0
                },
                {
                    "sent": "Dividing up in strips, but not into blocks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 'cause it's only it's splitting up one.",
                    "label": 0
                },
                {
                    "sent": "You could do anything you want.",
                    "label": 0
                },
                {
                    "sent": "You could define.",
                    "label": 0
                },
                {
                    "sent": "I start, I stop J, start J, stop and play around yourself if you wanted.",
                    "label": 0
                },
                {
                    "sent": "But by default.",
                    "label": 0
                },
                {
                    "sent": "Unless you have something called nested parallelism turned on the directives applied only to this the following loop.",
                    "label": 0
                },
                {
                    "sent": "So that may not be running again, that's normally OK.",
                    "label": 0
                },
                {
                    "sent": "It's not perfect.",
                    "label": 0
                },
                {
                    "sent": "Isn't it in general better to paralyze the inner loop as opposed to the outer loop?",
                    "label": 0
                },
                {
                    "sent": "No, no, exactly the opposite, because here I come here, you spawn the threads once.",
                    "label": 0
                },
                {
                    "sent": "There's an overhead to sporting the threads.",
                    "label": 0
                },
                {
                    "sent": "If you paralyze the inner loop, you spawn the threads multiple times.",
                    "label": 0
                },
                {
                    "sent": "And also for sort of cache memory layout layout operations in C. You always want to paralyze the outer loop.",
                    "label": 0
                },
                {
                    "sent": "Now, whether you do for I4J4K or 4K, four J4I is C Fortran memory layout issue.",
                    "label": 0
                },
                {
                    "sent": "But no, you want to now you're probably thinking about GPU programming.",
                    "label": 0
                },
                {
                    "sent": "GPU's are funny, no.",
                    "label": 0
                },
                {
                    "sent": "OK no you should paralyze the outer loop because then you create the threads once and remember there's also a.",
                    "label": 0
                },
                {
                    "sent": "There's also a barrier at the end of the loop, so if you paralyze the inner loop, you spawn the loops N tight.",
                    "label": 0
                },
                {
                    "sent": "You spawn the threads end times and you have any barriers.",
                    "label": 0
                },
                {
                    "sent": "Whereas in this format you spawn the loop the threads.",
                    "label": 0
                },
                {
                    "sent": "Once you have one barrier.",
                    "label": 0
                },
                {
                    "sent": "So it's actually it is.",
                    "label": 0
                },
                {
                    "sent": "It is the outer loop you want to parallelize.",
                    "label": 0
                },
                {
                    "sent": "You can paralyze the eneloops.",
                    "label": 0
                },
                {
                    "sent": "There's nothing to stop you doing that.",
                    "label": 0
                },
                {
                    "sent": "This is perfectly legitimate.",
                    "label": 0
                },
                {
                    "sent": "You know the logic may mean you have to do this, yeah?",
                    "label": 0
                },
                {
                    "sent": "You can do this if you want.",
                    "label": 0
                },
                {
                    "sent": "In fact, I would do anything I could just do that.",
                    "label": 0
                },
                {
                    "sent": "But if you have a choice, you should.",
                    "label": 0
                },
                {
                    "sent": "You should try and do the outer loop.",
                    "label": 0
                },
                {
                    "sent": "So I talked about the chunk size.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a static schedule 546 iterations.",
                    "label": 0
                },
                {
                    "sent": "It might split up.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry.",
                    "label": 0
                },
                {
                    "sent": "I wanted to ask, can you parallelize both loops so you can, but you need to turn on something called so by default open MP.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have net, it's called nested parallelism, so in openapi nested parallelism means if you encounter a parallel region, don't use all the threads, you some of them and then use some of the threads later on.",
                    "label": 0
                },
                {
                    "sent": "The problem is that's a lot more effort on the runtime system, so actually if you have nested parallelism turned off, which is normally the default, the runtime system I need to know one thing am I in a parallel regional receiver region, but if.",
                    "label": 0
                },
                {
                    "sent": "You have a nested parallelism turned on.",
                    "label": 0
                },
                {
                    "sent": "You have to say, well, I might be in a parallel region.",
                    "label": 0
                },
                {
                    "sent": "Oh, but how many threads are active?",
                    "label": 0
                },
                {
                    "sent": "And now I need to I just become whole thing becomes so yes, in the first version of open MP, nested parallelism wasn't possible.",
                    "label": 0
                },
                {
                    "sent": "It is now possible, but typically it actually.",
                    "label": 0
                },
                {
                    "sent": "The overhead of the runtime system means that typically slows things down.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to do a 2D decomposition, just have a nice just do it by hand.",
                    "label": 0
                },
                {
                    "sent": "Have a nice start.",
                    "label": 0
                },
                {
                    "sent": "I stopped J start, J stop, compute them, stick 'em in a macro or function unit to make it neat and tidy, you know, compute limits of you know and then do it that way.",
                    "label": 0
                },
                {
                    "sent": "It's not so elegant, but probably more efficient.",
                    "label": 0
                },
                {
                    "sent": "So if I have nested parallelism turned on, both loops will be paralyzed.",
                    "label": 0
                },
                {
                    "sent": "I don't need another pragma, OMP parallel or in front of.",
                    "label": 0
                },
                {
                    "sent": "You know you do So what you have to do is you have to do.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You'd have to have two 2 parallelized hash pragma.",
                    "label": 0
                },
                {
                    "sent": "OMP parallel 4 I.",
                    "label": 0
                },
                {
                    "sent": "And I think it's an.",
                    "label": 0
                },
                {
                    "sent": "Is it non thread you have to say number of threads so you see how many threads to do here equals four and then you might do something like this.",
                    "label": 0
                },
                {
                    "sent": "So this would then say this would spawn 16 threads where the first threads each of the first four threads would then spawn another split into another four threads.",
                    "label": 0
                },
                {
                    "sent": "I should know the syntax, but I can't remember it.",
                    "label": 0
                },
                {
                    "sent": "That's a bit poor.",
                    "label": 0
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "I just can't quite remember the syntax.",
                    "label": 0
                },
                {
                    "sent": "Taken over.",
                    "label": 0
                },
                {
                    "sent": "So you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to set an environment variable.",
                    "label": 0
                },
                {
                    "sent": "Which is yes or no.",
                    "label": 0
                },
                {
                    "sent": "Is that yeah, OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, fine right one.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, because it's an overhead.",
                    "label": 0
                },
                {
                    "sent": "There's an environment variable called OMP nested, which you have to set to true.",
                    "label": 0
                },
                {
                    "sent": "It could still.",
                    "label": 0
                },
                {
                    "sent": "It could still ignore you.",
                    "label": 0
                },
                {
                    "sent": "It could say look, I don't do that the problem, but if you, you can set OMP nested equals true and then.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so that it's it's numb hashtag my own people and, um_threads of two.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that's I got the syntax wrong.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that so.",
                    "label": 0
                },
                {
                    "sent": "So it's actually.",
                    "label": 0
                },
                {
                    "sent": "Numb so just to state the obvious also about the the nested loops in which one if you should do the outer the inner it you have to keep in mind it's always also dependent on what kind of problem you have, so you may not have enough parallelism in your outer loop, for instance, or your outer loop may.",
                    "label": 0
                },
                {
                    "sent": "Actually if you only use the outer loop for the parallelization, you might have great load imbalance and so therefore maybe the inner loop would be a better idea like many times.",
                    "label": 0
                },
                {
                    "sent": "I've worked, I've had the outer loop be the dimensions, for instance, that's only three or four.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, so you're right.",
                    "label": 0
                },
                {
                    "sent": "So you've got the statement that all other things being equal, you should paralyze the absolute.",
                    "label": 0
                },
                {
                    "sent": "But yes, as Banner said, if the item was rifles nor I less than three, I double.",
                    "label": 0
                },
                {
                    "sent": "Plus you'd be like, well, you know.",
                    "label": 0
                },
                {
                    "sent": "Then I do this one, but it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Nested parallelism was used to load balance codes, but Meanwhile open MP tasks were invented, so usually you don't use nested parallelism any longer, but you would create some tasks inside the outer parallel region.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so nested parallelism can be used to paralyze simple loops like this at multiple levels.",
                    "label": 0
                },
                {
                    "sent": "So without nested parallelism, you can parallelize simple loops, a single level, which is easy and efficient if you want to do very sophisticated load balancing.",
                    "label": 0
                },
                {
                    "sent": "As Christian said, you probably want to do something called tasks, which is completely general and completely dynamic, and I may touch on them.",
                    "label": 0
                },
                {
                    "sent": "But as you say in the middle, if I was doing this I would just do it by hand.",
                    "label": 0
                },
                {
                    "sent": "I would just have a little function that computed.",
                    "label": 0
                },
                {
                    "sent": "I start and I stopped J start, Jay stop.",
                    "label": 0
                },
                {
                    "sent": "It's not a big deal and you can you can put into macros or something if you want to make it look pretty.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look so.",
                    "label": 0
                },
                {
                    "sent": "This is static and this is static, Forced Atticus have blocks of four iterations and divide them up once iterations amongst threads.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dynamic is dynamic, so device situation about chunks of size chunk size and assigns into threads on a first come, first serve basis.",
                    "label": 1
                },
                {
                    "sent": "So but no chunk size, it defaults to one, so I might say to do iterations nought to 347.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "8 to 11 and then you finish first.",
                    "label": 0
                },
                {
                    "sent": "Do you?",
                    "label": 0
                },
                {
                    "sent": "Do you know 12 to 15?",
                    "label": 0
                },
                {
                    "sent": "Then you have a block size dynamics.",
                    "label": 0
                },
                {
                    "sent": "A bit of overkill because scheduling is like packing you've got.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're moving house and you have lots of stuff to pack.",
                    "label": 0
                },
                {
                    "sent": "OK, and you have three.",
                    "label": 0
                },
                {
                    "sent": "You have three vans to fill.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't?",
                    "label": 0
                },
                {
                    "sent": "You don't go and put a book in van one and then a book in van two in a plant pot in van three you get the big stuff you put your sofa in van one.",
                    "label": 0
                },
                {
                    "sent": "You put your chairs in van two.",
                    "label": 0
                },
                {
                    "sent": "You put your fridge in Van 3 and then you fill up the gap for the little stuff.",
                    "label": 0
                },
                {
                    "sent": "So if you're giving out, iterate, there's an overhead to giving out iterations.",
                    "label": 0
                },
                {
                    "sent": "Somebody has to ask them for what you should do is at the start you should give out big chunks of iterations.",
                    "label": 0
                },
                {
                    "sent": "You do the first 100 you do the next 100 right you do the next 100, and then as you start to come in, you start to give them smaller and smaller chunks to fill in the gaps, and that's called that is that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's called the guided schedule.",
                    "label": 1
                },
                {
                    "sent": "Similar dynamic with chunks.",
                    "label": 0
                },
                {
                    "sent": "I starts off larger and get small exponentially and chunks.",
                    "label": 0
                },
                {
                    "sent": "I specify the minimum size of Chuck.",
                    "label": 1
                },
                {
                    "sent": "There's some formula for basically.",
                    "label": 0
                },
                {
                    "sent": "Guided is normally better than dynamic.",
                    "label": 0
                },
                {
                    "sent": "Normally not always, but normally because it still balances the load 'cause the end of the day.",
                    "label": 0
                },
                {
                    "sent": "You're giving out small chunks of iterations, but you don't have such an overhead of coordination.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this could be dynamic.",
                    "label": 0
                },
                {
                    "sent": "And guided with start off big and get smaller.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is an auto schedule which says.",
                    "label": 1
                },
                {
                    "sent": "Let the compiler decide.",
                    "label": 0
                },
                {
                    "sent": "This is really.",
                    "label": 0
                },
                {
                    "sent": "Put in there to let people who do compiler development do cool things.",
                    "label": 0
                },
                {
                    "sent": "The idea is if you have a very complicated loop and it's iterated lots of execute a lot of times, the compiler can try different things at runtime and then remember which was the best.",
                    "label": 0
                },
                {
                    "sent": "I haven't checked, I don't know if this is implemented, I mean I know research compilers have this, so like Barcelona have stuff which does this kind of thing, but whether it's I don't know if it's been, I don't know if it's being implemented in real compilers, so I would not tend to use it.",
                    "label": 0
                },
                {
                    "sent": "'cause I kind of like to know what I mean.",
                    "label": 0
                },
                {
                    "sent": "In principle it's a great thing, but in practice I quite like to know what it's doing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Choosing a schedule well.",
                    "label": 0
                },
                {
                    "sent": "Static is best for load balanced loops.",
                    "label": 1
                },
                {
                    "sent": "Dynamics is widely very close guided as best.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't have time to go into all this, but you know you have to.",
                    "label": 0
                },
                {
                    "sent": "The main thing is just experiment.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to experiment and open MP and for a lot of scientific and technical codes then just static is fine.",
                    "label": 0
                },
                {
                    "sent": "If you're looping over a big matrix with an operating on each element it's load give everyone the same number of iterations, But then it may not be a.",
                    "label": 0
                },
                {
                    "sent": "Then if that doesn't work, you can try other things here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you want to experiment, you might say, well that means I have to recompile every time where you don't.",
                    "label": 0
                },
                {
                    "sent": "You can set the scheduled to be something called runtime and that says the runtime open MP decides what the schedule is based on the value of an environment variable called OMP shared your.",
                    "label": 1
                },
                {
                    "sent": "So what that does is you can take a loop, do schedule equals runtime, compile it, and then you can do export to PNG equals static time.",
                    "label": 1
                },
                {
                    "sent": "The code export to PNG equals dynamic time the code.",
                    "label": 0
                },
                {
                    "sent": "It just allows you to experiment easily without having to recompile.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's some stuff here, but nested.",
                    "label": 0
                },
                {
                    "sent": "You can collapse loops.",
                    "label": 0
                },
                {
                    "sent": "So if you have a loop with the first where the 1st.",
                    "label": 0
                },
                {
                    "sent": "Loop limit is very small and N is very small with both.",
                    "label": 0
                },
                {
                    "sent": "N&M are smallish OK if N&M are smaller for N * M is big.",
                    "label": 0
                },
                {
                    "sent": "This says just So what people used to do is open.",
                    "label": 0
                },
                {
                    "sent": "MP used to just apply to the 1st loop.",
                    "label": 0
                },
                {
                    "sent": "So what you do rewrite this loop is you do for I4K equals nought K less than M * N K double plus and then within it you'd say I is K percent N&J's K modem or whatever that kind of stuff you have to do.",
                    "label": 0
                },
                {
                    "sent": "But all the collapse does is it says combine this into a single iteration space and split that up amongst threads.",
                    "label": 0
                },
                {
                    "sent": "It just allows you to get more parallelism.",
                    "label": 0
                },
                {
                    "sent": "It's not doing a 2D decomposition, but it's useful if these are both relatively small, but their product is large, so collapse can be useful.",
                    "label": 0
                },
                {
                    "sent": "Will form a single loop of length N * N paralyzed that.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Single is a block of code extra by single thread only.",
                    "label": 1
                },
                {
                    "sent": "The first threads reached the single directory.",
                    "label": 0
                },
                {
                    "sent": "Execute the block and so this can be useful.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "There's a synchronization point in the block.",
                    "label": 0
                },
                {
                    "sent": "All the other threads waiting for the blocks being executed.",
                    "label": 0
                },
                {
                    "sent": "It's a slightly weird one that you can come up with examples where it's.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Useful.",
                    "label": 0
                },
                {
                    "sent": "Single and signal.",
                    "label": 0
                },
                {
                    "sent": "There is an example here with.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says we've basically got a parallel.",
                    "label": 0
                },
                {
                    "sent": "Set up routine.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you want to parallel then we want to read some data in from from file and we only want to be done by one thread.",
                    "label": 0
                },
                {
                    "sent": "OK she wanted a parallel operation.",
                    "label": 0
                },
                {
                    "sent": "Then read something in on one thread but we don't care which thread it is so you don't want to say this is only done on thread zero 'cause maybe thread 0 takes a long time to do this you just want this to be done by the 1st thread that gets there so that single.",
                    "label": 0
                },
                {
                    "sent": "Then there's a barrier here which means that work will not be executed into.",
                    "label": 0
                },
                {
                    "sent": "Why has been read?",
                    "label": 0
                },
                {
                    "sent": "Having said that in real it's.",
                    "label": 0
                },
                {
                    "sent": "It's not common.",
                    "label": 0
                },
                {
                    "sent": "I have to say to you, single the one you use all the time is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faster.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Master, you use for print statements.",
                    "label": 0
                },
                {
                    "sent": "Master says.",
                    "label": 0
                },
                {
                    "sent": "Only thread zero will do this, but there's no Barry.",
                    "label": 0
                },
                {
                    "sent": "It's the equivalent of rank equals zero.",
                    "label": 0
                },
                {
                    "sent": "Print Hello World in MPI.",
                    "label": 0
                },
                {
                    "sent": "So you often use master directives to to protect print steps.",
                    "label": 0
                },
                {
                    "sent": "However, you have to remember that the easiest way to print to protect your print statements is just just have them in the serial region.",
                    "label": 0
                },
                {
                    "sent": "Often parallel regions around you know you don't have to.",
                    "label": 0
                },
                {
                    "sent": "Outdoor pee is quite easy to parallelize because all the print statements at the start of your code are all in the serial region, you know, so it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite rare you tend to use it for debugging.",
                    "label": 0
                },
                {
                    "sent": "But it's quite rare you want to do a presence in the parallel region.",
                    "label": 0
                },
                {
                    "sent": "It's probably doing some big heavy computation.",
                    "label": 0
                },
                {
                    "sent": "Don't waste time printing.",
                    "label": 0
                },
                {
                    "sent": "OK, so but the important point is single the first thread that gets there does it, and then waits for everyone to catch up and you would never want to do that with the print master.",
                    "label": 0
                },
                {
                    "sent": "Thread 0 does it, but just carries on.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly the same as saying if thread equals zero, then do this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so it's it's like this again.",
                    "label": 0
                },
                {
                    "sent": "Fortran has a start and end C relies on a structure block, and if you have more than one statement, you put curly brackets around them.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sections I'm not going to talk about this, but you can just say parallel sections just says run.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shruti then parallel with that one in parallel.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one is not paralyzing a loop.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's saying run different bits of code on different threads, but it's rarely about much.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, it's not not in.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It'll be useful.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I won't really cover it.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The one thing where you get into problems is with with with array syntax.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is a problem, so if you're a Fortran programmer.",
                    "label": 0
                },
                {
                    "sent": "You would do.",
                    "label": 0
                },
                {
                    "sent": "You would do a curl on: equals a colon: plus B:: that adds that says that's the same thing as do I want to end do Jake from one to NAIJ equals AIJ plus BIJK before trying a mathematical language is just matrix notation equals 8 plus be.",
                    "label": 0
                },
                {
                    "sent": "In fact you can just write 8 with a plus B, but you can't paralyze that with opening because there's no loops.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no loops there, so how do you parallelize it?",
                    "label": 0
                },
                {
                    "sent": "Well, they introduced this thing called work share.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit weird because the normal thing about open MP is it's completely explicit.",
                    "label": 0
                },
                {
                    "sent": "But work share isn't work share is just more.",
                    "label": 0
                },
                {
                    "sent": "Could you paralyze this please?",
                    "label": 0
                },
                {
                    "sent": "And I don't really care how is rather unsatisfactory, but it's there to allow you to to parallelize things like array syntax to be honest, if I were to do it, what I would do, let's make it a 3D loop.",
                    "label": 0
                },
                {
                    "sent": "What I would do is I would put in a sink I would do.",
                    "label": 0
                },
                {
                    "sent": "Do I get the wrong way around?",
                    "label": 0
                },
                {
                    "sent": "K = 1 to N. I take out one of them.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I do.",
                    "label": 0
                },
                {
                    "sent": "So you don't need to take out all the eraser that you just need to give yourself enough loops to paralyze.",
                    "label": 0
                },
                {
                    "sent": "I would probably do that.",
                    "label": 0
                },
                {
                    "sent": "'cause I like to know how it's paralyzed and I don't know the problem by letting the compiler parallelize it might parallelize it in a different way from what you want, and I don't know if you know the way that caches and things work on shared, but that can be a disaster for performance.",
                    "label": 0
                },
                {
                    "sent": "I will still work.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a subtle point, but I would I would do this.",
                    "label": 0
                },
                {
                    "sent": "It's not as it's not as elegant as array syntax, but I I don't really like the work share directive in my mind.",
                    "label": 0
                },
                {
                    "sent": "Open MP is that is explicit programming model where you say exactly what you want to do.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think that's basically it.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some work share stuff, why not go through that?",
                    "label": 0
                },
                {
                    "sent": "It's rather.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rather detail.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the exercise is actually is on the sheet and this is this is the I'm doing a different exercise so.",
                    "label": 0
                },
                {
                    "sent": "For those of you, I think everyone here yesterday, but if you go to the web page.",
                    "label": 0
                },
                {
                    "sent": "Where is the exceed?",
                    "label": 0
                },
                {
                    "sent": "Exceed wiki.",
                    "label": 0
                },
                {
                    "sent": "The 2016 wiki is here.",
                    "label": 0
                },
                {
                    "sent": "And it's under.",
                    "label": 0
                },
                {
                    "sent": "Presentations.",
                    "label": 0
                },
                {
                    "sent": "Presentations.",
                    "label": 0
                },
                {
                    "sent": "NP and Open MP materials.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The rachford sheet here.",
                    "label": 0
                },
                {
                    "sent": "Is this open MP exercises Section 4 so?",
                    "label": 0
                },
                {
                    "sent": "So I've given you an open MP version, but the Open MP version doesn't actually do anything, it just prints Hello World on multiple threads.",
                    "label": 0
                },
                {
                    "sent": "I haven't parallelized any of the loops, but it's it's such a straightforward exercise, and in fact it's almost identical to the.",
                    "label": 0
                },
                {
                    "sent": "It's almost identical to the pie the pie exercise, so there are two.",
                    "label": 0
                },
                {
                    "sent": "There were only two significant loops in the code, so if I got if I go to the code.",
                    "label": 0
                },
                {
                    "sent": "Is it traffic?",
                    "label": 0
                },
                {
                    "sent": "I'll do 4 Chan this time just to be fair.",
                    "label": 0
                },
                {
                    "sent": "If I do traffic dot F90K so the nice thing about.",
                    "label": 0
                },
                {
                    "sent": "About Open MP, is it MPI to generate the road?",
                    "label": 0
                },
                {
                    "sent": "I had this.",
                    "label": 0
                },
                {
                    "sent": "I had this function that generated the road in in cereal and then I had to broad.",
                    "label": 0
                },
                {
                    "sent": "I had to scatter.",
                    "label": 0
                },
                {
                    "sent": "I could say right now you have the first part of the road you have set in shared memory.",
                    "label": 0
                },
                {
                    "sent": "Do I have to do that?",
                    "label": 0
                },
                {
                    "sent": "Remember that the roads on the whiteboard OK if and if you can't parallelize the initialization routine, you just get one thread to do it.",
                    "label": 0
                },
                {
                    "sent": "But you don't care, you just just do it.",
                    "label": 0
                },
                {
                    "sent": "It's there.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So that's why open MP is nice if you got a bit that we can't be bothered paralyzing.",
                    "label": 0
                },
                {
                    "sent": "You just leave it in cereal that you can't do that in MPI because the data is distributed in MPI, but in an open MP, any thread, even the even the serial region, the master thread has access to all the data, all the shared data at least, which is the important stuff.",
                    "label": 0
                },
                {
                    "sent": "But the two things are the only thing I put in here.",
                    "label": 0
                },
                {
                    "sent": "Hello worlds.",
                    "label": 0
                },
                {
                    "sent": "The first thing is the time is all in this update Rd thing here.",
                    "label": 0
                },
                {
                    "sent": "And that's in a different routine, which is called traffic Lib dot F-90, I think.",
                    "label": 0
                },
                {
                    "sent": "So you want to.",
                    "label": 0
                },
                {
                    "sent": "Paralyzed this update Rd routine.",
                    "label": 0
                },
                {
                    "sent": "Which is quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "We have old Rd new Road and move and I are the variables so you want to parallelize that loop.",
                    "label": 0
                },
                {
                    "sent": "But you also want to parallelize the.",
                    "label": 0
                },
                {
                    "sent": "The copy back loop which is here.",
                    "label": 0
                },
                {
                    "sent": "And it's worth doing that in two stages.",
                    "label": 0
                },
                {
                    "sent": "The code spends most of its time.",
                    "label": 0
                },
                {
                    "sent": "Most in the in the update routine.",
                    "label": 0
                },
                {
                    "sent": "But it's difficult to say 'cause I don't really understand the architecture of bridges, but but on a lot of computers you will.",
                    "label": 0
                },
                {
                    "sent": "You will see what I expect you to see as you don't get decent speedup, at least on large numbers of threads, unless you paralyze both loops.",
                    "label": 0
                },
                {
                    "sent": "That's not obvious, but but it's so I would also run on say 28 threads run on a large number of threads to see the effects, or maybe.",
                    "label": 0
                },
                {
                    "sent": "Well, it's even more comp 14.",
                    "label": 0
                },
                {
                    "sent": "Maybe I I guess, but anyway.",
                    "label": 0
                },
                {
                    "sent": "It's a useful exercise, although an open MP is in principle an incremental model.",
                    "label": 0
                },
                {
                    "sent": "You have to paralyze the bits that are slow.",
                    "label": 0
                },
                {
                    "sent": "For performance reasons, it turns out that you probably have to paralyze most loops.",
                    "label": 0
                },
                {
                    "sent": "Sorry, after you have to paralyze more loops than you might think and maybe discuss rather is later, but that but you can.",
                    "label": 0
                },
                {
                    "sent": "That's the idea here is just to put OMP parallel four in parallel.",
                    "label": 0
                },
                {
                    "sent": "Do about the two major loops and look at the timing and see how long it takes, how long it.",
                    "label": 0
                },
                {
                    "sent": "How long it's?",
                    "label": 0
                },
                {
                    "sent": "Takes how well it scales and such like, so I think, but it is the reason why the pie example is not a realistic example is there was no.",
                    "label": 0
                },
                {
                    "sent": "There were no shared arrays in the pie example who are just adding up a series and a real program.",
                    "label": 0
                },
                {
                    "sent": "You have a big you have big arrays and that's this old Rd new Road.",
                    "label": 0
                },
                {
                    "sent": "That and that will be true in the I'll put it up over the break but that will be true in the.",
                    "label": 0
                },
                {
                    "sent": "The the hybrid challenge you're operating on big arrays, and it's exactly it's exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Structure as this, so I think probably the best have a.",
                    "label": 0
                },
                {
                    "sent": "Have a practical session till about well quarter past three.",
                    "label": 0
                },
                {
                    "sent": "Maybe then we can take a 15 minute break and start again at 3:30.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "That's not OK.",
                    "label": 0
                }
            ]
        }
    }
}