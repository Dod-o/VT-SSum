{
    "id": "qpjsdzwagc4q6y45o6fyka3gpd3ccqqt",
    "title": "Discriminative k-Metrics",
    "info": {
        "author": [
            "Arthur Szlam, Department of Mathematics, University of California, Los Angeles, UCLA"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml09_szlam_dkm/",
    "segmentation": [
        [
            "OK, OK, so this is actually so.",
            "This is actually a really simple talk in some sense.",
            "So why is everything highlighted?",
            "How do you unhighlight?",
            "OK, well everything will be highlighted through the talk.",
            "OK so."
        ],
        [
            "Basically, we're going to take a very simple clustering algorithm, which is the case subspace algorithm for Q flats at a discriminative term and make it into a supervised learning algorithm.",
            "And it's that simple.",
            "But it works really, really, really well.",
            "OK, so.",
            "Um?",
            "Yeah, actually this is how do you.",
            "How do you get rid of the.",
            "Oh OK. Well OK so.",
            "I'm supposed to be KQ Flats is a."
        ],
        [
            "OK. OK, OK so.",
            "Yeah, so we start with KQ Flats or K subspaces.",
            "Or actually has a million other.",
            "So this algorithm, Kaku Flats, has been rediscovered maybe 10 times in the last 10 years.",
            "In fact, there's three different penny papers that have the same thing and it's just K means.",
            "Except instead of cluster centers instead of centroids, as your representatives, use subspaces or affine subspaces as your cluster centers use exactly the same energy.",
            "So here the energy is the distance of a point.",
            "To its nearest subspace.",
            "OK, and so you evaluate the energy over choices of subspaces, an partitions and of course for each partition you want to pick as your subspace.",
            "The principle components of that particular partition.",
            "So it's really simple.",
            "You can use Lloyd, just like K means and it works out.",
            "It's actually a pretty nice clustering algorithm.",
            "It sits in the intersection of a lot of nice areas.",
            "Its intersection of what's called, I guess manifold learning or sparse approximation.",
            "So here is about here's a slide about sparse approximation.",
            "So this was the Cayman Center or the K subspaces energy.",
            "We have the distance of each point to its nearest subspace.",
            "If you just rearrange that a bit an, you use subspaces.",
            "Real subspace is not necessarily affine sets.",
            "You get this thing here, so this equation is simply sparse, sparse basis problem.",
            "You want to find a basis for the data set X.",
            "So that the coefficients are all sparse.",
            "So in other words, the coefficient for any element in X has less than or equal to Q.",
            "Has less than or equal to Q non zero entries and that will be KQ flats.",
            "So let me actually show you a picture of what it looks."
        ],
        [
            "Like it's a very special way of doing it.",
            "It's not that this this sparse dictionary design problem is actually combinatorial.",
            "Really hard, because any element X might have might be composed of the interaction of lots of basis elements."
        ],
        [
            "The K subspaces algorithm is much simpler.",
            "It kills a lot of the combinatorics, but you know you lose some.",
            "It's not as optimal, and the way you kill the combinatorics is you force the force the coefficients to have this block structure.",
            "In other words, all the.",
            "All the points using.",
            "A certain basis element are grouped together and all the points using this set of basis elements are grouped together and so on and so forth so.",
            "The case subspaces is just a sparse basis design problem, except you kill a lot of the combinatorics by forcing points to only use elements from one of the.",
            "Sets of bases OK. OK, so.",
            "At the same time, it's a manifold model of the data."
        ],
        [
            "If you expect that your data lies on a manifold and you may not expect that, but if you do.",
            "What it means is that each point is well approximated by its tangent plane.",
            "That's in some sense the definition of manifold, right?",
            "So once you decide that every point in your data set is well approximated but well approximated by its tangent plane, and someone says OK, compress this data well, one thing you might do is get some budget of K planes.",
            "And again your manifold to say dimension Q an you choose the best K for your manifold.",
            "Well, the best one with respect to mean Square.",
            "Error is exactly the case flats construction.",
            "Um?",
            "So this thing this subspace algorithm is kind of really simple, but at the same time it's it's in a nice place.",
            "It's a.",
            "It's a sparse dictionary design algorithm.",
            "Or if you want to think that way, it's a manifold approximation algorithm.",
            "OK. Now if you want it, you can use it for supervised learning."
        ],
        [
            "And the way you can use it for supervised learning is you simply take.",
            "Each class of data.",
            "So your data is divided into training points of class one training points of Class 2, training points of Class 3 etc etc etc and.",
            "You can simply make a model for each of the classes using this case subspace algorithm.",
            "What you do is you.",
            "You train a dictionary for each class, or if you want to think about it that way you build a parameterisation of the manifold of each class with these planes and then given a new data point.",
            "If you want to figure out which class it belongs to, you say OK.",
            "Which of these manifolds is closest to or in other words, which dictionary is at best represented in?",
            "Um?",
            "So this is really simple.",
            "If you were to do this with with K means, you would simply.",
            "Well, what does it mean?",
            "It means you find centroids for each class.",
            "And then given a new point, you say which of the centroids is close to the same thing you find, so talk.",
            "So what are we doing here?",
            "You have a bunch of points, say from class one, a bunch of points from Class 2.",
            "You train a dictionary.",
            "Anna Dictionary just means a set of flats for each of the dictionaries.",
            "For each of the classes given a new point, you say which guy am I closest to.",
            "And whichever one you're closest to, that's the one year sent to.",
            "So in equations, what are you doing?",
            "You find them in."
        ],
        [
            "Some of the of that energy, and I've I've.",
            "Elided, appoint, so here we're taking the energy with subspace is not without fine sets, and the reason will do that.",
            "It will be apparent in a minute.",
            "You can do this without fine sets as well.",
            "This energy is with subspaces, so again you find some partition of each of the classes.",
            "And for each of those partitions you find the best fit subspace is to those partitions, and then you optimize over all those things and actually you can do that very, very very fast.",
            "Actually K subspaces you can do in exactly the same way you do K means with the Lloyd algorithm, or McQueen or whatever and it converges super fast.",
            "It actually converges faster than case.",
            "Then K means.",
            "Practice.",
            "OK, So what?"
        ],
        [
            "So the problem that we want to address is that this is simply a representational method.",
            "If you want to do a supervised learning.",
            "If you want to solve a supervised learning problem, you should understand that some things are really different than others.",
            "You don't just want to make, you don't want to get the best approximation for each class you want to get the best approximation for each class, with the proviso that things in different classes should be far away from each other.",
            "Um?",
            "So what we're going to do is the common sense thing will simply change the energy with the discriminative term, and then something which comes out to be.",
            "In some sense of computational artifact, but in some sense it actually helps.",
            "We're going to generalize planes.",
            "We won't use planes.",
            "We use metrics.",
            "OK so here is."
        ],
        [
            "The energy that we're going to use.",
            "It looks really complicated, but it's really essentially the same thing as we had before.",
            "Again, if you, if you search through this, all you're doing is you're training planes for each class, so this is simply the case up space, thing, case, subspace, energy for each class.",
            "That's this bit here, except now you've added a discriminative term.",
            "So again, you train in energy and the energy takes is over partitions of each class.",
            "And it's over a basis for each of those partitions.",
            "The one thing that I said before that instead of subspaces were instead of planes, we're going to use metrics.",
            "Well, as we solve this as we minimize this energy.",
            "I'm not going to force the by Jay to be orthonormal, so before the by J were planes I was I was forcing some.",
            "They were set up as orthonormal sets so now.",
            "I'm not going to enforce that at all, as I minimize this energy.",
            "I'm just going to let them do whatever it is that they do and what that means is that.",
            "This norm here by JX norm well that's just the norm with respect to the Mahalanobis metric B transpose B.",
            "So what you're doing now is instead of approximating each, instead of approximating your data points by planes, you're approximating your data points by metrics, and at each point you have a different metric, so in some sense it's not that different, because a plane is a very simple version of a Mahalanobis metric.",
            "You just crush everything.",
            "It's a Mohammed Novus metric with.",
            "If you look at the single value combination, you only have ones and zeros on the diagonal, right?",
            "So here we do a.",
            "We approximate points by planes, but we don't necessarily have ones and zeros on the diagonal in the singular value decomposition.",
            "It's really the same thing, let me."
        ],
        [
            "Put the energies back to back.",
            "So this up here was the case.",
            "Subspace is an energy when used for classification.",
            "Again, this is.",
            "This is really just K means, but with subspaces instead of.",
            "Centroids as class representatives an downstairs.",
            "I've put the new energy and you can see it's the same here.",
            "G of G1 of this thing was just negative.",
            "So you can see that really the same thing.",
            "What's been done is completely trivial in some sense.",
            "All it's been done is you've taken this energy up here and added a descriptor, discriminative bit, and so a heuristic picture.",
            "This is the representational you try and find planes or you try and find a parameterisation of each of the two classes.",
            "This is the representational here, you try and find a good representation, but you want the you want the.",
            "Parameterisation of this class to be really far away from the pressurisation of this class, OK?",
            "So in this energy what we want we want G1 to be we want G1 to be very small when its input is large and we want G2 to be large when its input is large, right?",
            "Because we're going to try and minimize this energy.",
            "So every time somebody is close to somebody from it's from the enemy class, we want to incur a lot of loss.",
            "And every time where somebody is far away from somebody in its own class, we want to incur a lot of loss.",
            "In this case, being far from your own class means the inner product of you against your friendly basis vectors is large that's being close.",
            "So."
        ],
        [
            "Here is a choice of G1 and G2.",
            "Now what we found and experiments is that you actually could be really sloppy with your choice of G1 and G2.",
            "And I'll say I'll make one more proviso about that.",
            "So in all the experiments were going to do later, I'm going to normalize all the data points to the unit sphere.",
            "This is actually not really a big deal because you could always shift it.",
            "You could if whatever reason your problem depended on distance, norm of data points.",
            "You can shift the whole thing in project.",
            "But once you do this normalization, once every data point is projected onto the unit sphere, then actually you're really safe here.",
            "You can really use a lot of different choices for G1 and G2.",
            "There are some things that are really important though that matter.",
            "The things that matter well, you have to have a margin.",
            "So for our.",
            "Choice of G1 and G2.",
            "Here again, what do we need?",
            "We want G1 to be really small when its input is large.",
            "So what do we do?",
            "Well, if if the input is large for G1 then.",
            "Long as it's larger than the margin, we get actually zero.",
            "We get zero loss here.",
            "For G2 we have the same thing.",
            "We want it to be.",
            "We want it to be really large when its input is large, and again if we if we pick input to G2 to be smaller than the margin that we incur, loss otherwise.",
            "Otherwise we cost.",
            "The same thing here.",
            "The margin the idea of using a margin here is really really important, but the exact shape of the energy really doesn't matter.",
            "The exact shape shape of the G1 and G2 doesn't matter.",
            "You can kick it around quite a bit as long as you use a margin.",
            "It seems to be really, really stable, and again, that's because all the data points in the subsequent experiments are going to miss fear, so there's not too much of a difference between the costs here, so it doesn't matter if you put squared or whatever, that's in some sense just to help the optimization.",
            "As long as you penalize points, as long as you stop penalizing points when they are safe enough, you're happy.",
            "OK, so."
        ],
        [
            "We can use just a static gradient descent to minimize the new energy.",
            "I've written out the derivative simply because.",
            "Simply because I want you to notice that it's really good if the data A is sparse or be it has some special properties, the derivative really sees multiplications of the basis vectors against the data and against the data transpose, so it's really fast.",
            "It's really easy to encode a vector.",
            "A derivative step, and if things are sparse then it eats it up.",
            "It's great, it's very happy."
        ],
        [
            "OK, so let's see.",
            "Let's skip this slide."
        ],
        [
            "We use multiple restarts in the thing so OK, So what did we test this on?",
            "I tested this on the endless digits so ya'll are are kind of familiar with this.",
            "I think it's 70,028 by 28 images so here they were here.",
            "They were projected onto their first principle, 51st principle components.",
            "We work on the 20 newsgroups data set, so this is text data an it's represented by its term document matrix and.",
            "Um?",
            "Here I used the 5000 most common words.",
            "So this was to compare.",
            "There was a HTML paper of last year.",
            "The problem is this particular data set.",
            "There are a million different normalizations, so people people use this in lots of different papers as a data set.",
            "However, every data set uses a different normalization.",
            "I picked one from last year and I smell to compare against, but there's unfortunately many different choices.",
            "So here the data is the 5000.",
            "It's a term document matrix with the 5000 most common words.",
            "An everybody is either zero or one.",
            "Depending that you don't do word counts, you just do.",
            "Is there a word?",
            "Is there not a word?",
            "Um and I select is 200 speakers saying each letter of the alphabet twice an it's there's 617 audio features that have been extracted, but actually nobody knows what the audio features are anymore, so it's actually a really nice machine learning data set because it's impossible to apply domain knowledge, but OK, so all three of these things after it was done, it was the data was projected onto the unit sphere and so here are the results."
        ],
        [
            "Of running this algorithm.",
            "So this is this is.",
            "With restarts, this is out without restart, so this is with five restarts, and this is without restarts.",
            "What you'll notice is that restarts help some things more than others, and that's because some so so the parameters that are here in this parameters that are here are the K, the number of metrics that we use Q, which is the Max dimension of the metrics and the margins and all these things are chosen by cross validation in datasets where you need where you use lots of metrics.",
            "Then the number of restarts is really important.",
            "If you restart a lot then it helps in the 20 newsgroups.",
            "It only wanted one metric for class and so of course it didn't matter whether you restart or not because you only use one metric for class.",
            "So, OK, what you notice from this table?",
            "So a is that the method is pretty good.",
            "So the SVM baseline is on the bottom.",
            "It loses out on isolate, but it does really well on the other two."
        ],
        [
            "And the other thing.",
            "So here are some timings.",
            "So the graphs are passes through the data set, so we use stochastic gradient descent.",
            "So each graph is 40 runs through 40 passes, and the numbers here are the timings for the thing in seconds 440 passes and what you notice is you're getting really good results long before 40 passes.",
            "So for example, to get the really good results on M nest to get like a one point 1% error on M Nest, you need roughly 20 passes, so you need 150 seconds.",
            "Sorry, you need 5 * 150 seconds, so MNIST really was helped by the restarts.",
            "So you need roughly roughly 700 seconds to get a really good classifier for amnesty for 20 newsgroups.",
            "Again the same thing that you're getting really good.",
            "You're getting really good results after about 10 passes through the data, so that's about 200 seconds.",
            "By the way, notice that this really is happy with sparseness here.",
            "Use only 5000 words.",
            "If I use the full dictionary, which is something like 50,000 words.",
            "The total processing time.",
            "For 40 passes gets to 900 seconds.",
            "So it doesn't actually get hurt by, you know, by the fact that it's the.",
            "The 20 newsgroups is 18,000 by 60,000.",
            "Because it's sparse.",
            "The updates are all very sparse and so it's very happy.",
            "So The thing is, this is you can run this on gigantic datasets with no problem."
        ],
        [
            "OK, so here is a different data set, this is.",
            "This is the bikes from the grass image database, so the grass image database is pictures of bikes, people and cars and background, so we'll see what the pictures look like in a second and the task is to someone gives you.",
            "So the task in this database is.",
            "You're given a picture of a bike or car or person, whatever, but you're not giving the whole picture.",
            "You're just given the neighborhood of a pixel and you have to figure out if that pixel belongs to a bike or belongs to a car belongs to background, etc.",
            "And sorry, OK."
        ],
        [
            "OK Yep, yeah OK, so these are.",
            "These are what the features this is.",
            "What you get if you run through the."
        ],
        [
            "These are the."
        ],
        [
            "Teachers, I mean, these are the heat Maps of picking up the thing and all."
        ],
        [
            "These decisions are local by the way.",
            "OK, so I'll stop.",
            "We have time."
        ],
        [
            "A couple of quick questions.",
            "This one.",
            "Oh, OK. OK, so OK.",
            "So the really important problem.",
            "OK, so the conclusion is simply some supervised method.",
            "This is really good as a supervised method.",
            "It's fast, it's it gives great results, but.",
            "There should be a semi supervised method there and I don't know it yet, so somehow that's the conclusion is do this in a semi supervised manner and that would be cool.",
            "Add a question.",
            "So how does this compare to fitting mixture of Gaussians?",
            "Where basically these flats are simple Gaussians which have infinite variance in some dimensions with zero variance and others yes.",
            "It seems like the Faciane Lawrence all right, so this is really.",
            "I mean, it's really as soon as you assume you say I'm fitting flats like this, you're fitting Gaussians, there's I mean, it's almost the same thing.",
            "It's just a geometric version of it.",
            "So I would say it's very, very very similar actually.",
            "On that's it, but remember there is one additional feature which is once once we move to once we once we move to.",
            "So actually, especially once you move to metrics, it's even more because really, you're saying well, some dimensions were crushing a lot and some little so it's quite similar in that respect.",
            "It's kind of like an experiment of mixture factor analyzing.",
            "Yes, exactly that's correct.",
            "In that framework is pretty straightforward and doing bunch of different settings, different setups.",
            "Right?",
            "Type setup right?",
            "So I've tried.",
            "I've tried these sorts of things, but remember that so the way the energy is set up right now, it's very hard to do a straight M like it's just a nasty energy for actually computing a good.",
            "So right now the best way to do the minimization is simplistic gradient descent.",
            "You can't, you can't just find the minimum and then go from there for the current energy.",
            "So for the straight K subspaces, for the straight mixture effector without the discrimination.",
            "Actually, if you try and do anything like that, it doesn't work too well.",
            "Expected gradient as opposed to post form.",
            "Say it again, sorry.",
            "Closed form yeah sure sure, so I've tried.",
            "I've tried a lot of the first common sense.",
            "Things like the a lot of the first things that you would try.",
            "I've tried.",
            "The problem is the way this is working.",
            "Right now.",
            "It really leans on the guys that are close to the margin in a strong way.",
            "And as soon as you.",
            "As soon as you try and do a semi supervised, if you get one of those guys close to the margin wrong, the whole thing collapses.",
            "This.",
            "Bother you computacional costs for this.",
            "Yeah, it's it's really.",
            "I mean again.",
            "So in terms of where is it?",
            "So it's very fast.",
            "I mean, this is the computational cost, so again, so for example, MNIST is 70,000 points.",
            "To get good results you're taking about.",
            "Sorry, yeah, 60,000 points in the training set.",
            "So to get good results you're taking about 5 * 150 seconds or 5 * 100 seconds.",
            "So you know 1020 minutes at most.",
            "Faster.",
            "In the Commission one.",
            "It depends what the conventional one is, so it's faster than a lot of things.",
            "But I mean, for example straight K subspaces and much faster.",
            "Yes.",
            "Talk about cleaning so that makes good decisions.",
            "OK, So what happens far away won't impact you, yes?",
            "Say you have a plane at that very far away it can.",
            "It can make you believe that too very close.",
            "Right, so yes, so that's a low dimensional phenomenon, right?",
            "Because in high dimensions, if you're in 100 dimensions, if you're close to, uh, I mean so.",
            "This picture is extremely misleading, and in fact the picture I put up earlier is extremely misleading.",
            "'cause it's in 2D, the planes cross.",
            "If you give Me 2 planes in R-100, even if the data was really similar, the planes don't cross.",
            "They are very far from each other.",
            "This idea that that subspace is if for example in Edna's.",
            "I think the cross validated parameter was.",
            "10 dimensional subspaces in R50, right?",
            "The subspaces are all extremely far from each other, and it's very hard to get two subspaces close to each other in high dimension.",
            "No, no, but even with even with data, that's what I'm saying.",
            "Even in this case, if you look at the distance between the subspaces, they're always there, always reasonably far from each other.",
            "It's it's pretty hard, even when it's not, even when it's not random.",
            "You have to work.",
            "I mean, if the two subspaces are really hitting each other, it means your whole data lived on a low dimensional subspace.",
            "Let me say, let me say it a different way.",
            "Suppose you have.",
            "Suppose you have several subspaces and they hit right and you know these are generally low dimensional space compared to the ambient dimension.",
            "If they hit, that means that subset of the data lived in, say that dimension plus one or smaller.",
            "In a linear sense.",
            "Yeah, sorry."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, OK, so this is actually so.",
                    "label": 0
                },
                {
                    "sent": "This is actually a really simple talk in some sense.",
                    "label": 0
                },
                {
                    "sent": "So why is everything highlighted?",
                    "label": 0
                },
                {
                    "sent": "How do you unhighlight?",
                    "label": 0
                },
                {
                    "sent": "OK, well everything will be highlighted through the talk.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, we're going to take a very simple clustering algorithm, which is the case subspace algorithm for Q flats at a discriminative term and make it into a supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it's that simple.",
                    "label": 0
                },
                {
                    "sent": "But it works really, really, really well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually this is how do you.",
                    "label": 0
                },
                {
                    "sent": "How do you get rid of the.",
                    "label": 0
                },
                {
                    "sent": "Oh OK. Well OK so.",
                    "label": 0
                },
                {
                    "sent": "I'm supposed to be KQ Flats is a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, OK so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we start with KQ Flats or K subspaces.",
                    "label": 0
                },
                {
                    "sent": "Or actually has a million other.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm, Kaku Flats, has been rediscovered maybe 10 times in the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "In fact, there's three different penny papers that have the same thing and it's just K means.",
                    "label": 1
                },
                {
                    "sent": "Except instead of cluster centers instead of centroids, as your representatives, use subspaces or affine subspaces as your cluster centers use exactly the same energy.",
                    "label": 0
                },
                {
                    "sent": "So here the energy is the distance of a point.",
                    "label": 1
                },
                {
                    "sent": "To its nearest subspace.",
                    "label": 0
                },
                {
                    "sent": "OK, and so you evaluate the energy over choices of subspaces, an partitions and of course for each partition you want to pick as your subspace.",
                    "label": 0
                },
                {
                    "sent": "The principle components of that particular partition.",
                    "label": 0
                },
                {
                    "sent": "So it's really simple.",
                    "label": 0
                },
                {
                    "sent": "You can use Lloyd, just like K means and it works out.",
                    "label": 0
                },
                {
                    "sent": "It's actually a pretty nice clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "It sits in the intersection of a lot of nice areas.",
                    "label": 0
                },
                {
                    "sent": "Its intersection of what's called, I guess manifold learning or sparse approximation.",
                    "label": 0
                },
                {
                    "sent": "So here is about here's a slide about sparse approximation.",
                    "label": 0
                },
                {
                    "sent": "So this was the Cayman Center or the K subspaces energy.",
                    "label": 0
                },
                {
                    "sent": "We have the distance of each point to its nearest subspace.",
                    "label": 0
                },
                {
                    "sent": "If you just rearrange that a bit an, you use subspaces.",
                    "label": 0
                },
                {
                    "sent": "Real subspace is not necessarily affine sets.",
                    "label": 0
                },
                {
                    "sent": "You get this thing here, so this equation is simply sparse, sparse basis problem.",
                    "label": 0
                },
                {
                    "sent": "You want to find a basis for the data set X.",
                    "label": 0
                },
                {
                    "sent": "So that the coefficients are all sparse.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the coefficient for any element in X has less than or equal to Q.",
                    "label": 0
                },
                {
                    "sent": "Has less than or equal to Q non zero entries and that will be KQ flats.",
                    "label": 0
                },
                {
                    "sent": "So let me actually show you a picture of what it looks.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like it's a very special way of doing it.",
                    "label": 0
                },
                {
                    "sent": "It's not that this this sparse dictionary design problem is actually combinatorial.",
                    "label": 0
                },
                {
                    "sent": "Really hard, because any element X might have might be composed of the interaction of lots of basis elements.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The K subspaces algorithm is much simpler.",
                    "label": 0
                },
                {
                    "sent": "It kills a lot of the combinatorics, but you know you lose some.",
                    "label": 0
                },
                {
                    "sent": "It's not as optimal, and the way you kill the combinatorics is you force the force the coefficients to have this block structure.",
                    "label": 1
                },
                {
                    "sent": "In other words, all the.",
                    "label": 0
                },
                {
                    "sent": "All the points using.",
                    "label": 0
                },
                {
                    "sent": "A certain basis element are grouped together and all the points using this set of basis elements are grouped together and so on and so forth so.",
                    "label": 0
                },
                {
                    "sent": "The case subspaces is just a sparse basis design problem, except you kill a lot of the combinatorics by forcing points to only use elements from one of the.",
                    "label": 0
                },
                {
                    "sent": "Sets of bases OK. OK, so.",
                    "label": 1
                },
                {
                    "sent": "At the same time, it's a manifold model of the data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you expect that your data lies on a manifold and you may not expect that, but if you do.",
                    "label": 0
                },
                {
                    "sent": "What it means is that each point is well approximated by its tangent plane.",
                    "label": 1
                },
                {
                    "sent": "That's in some sense the definition of manifold, right?",
                    "label": 0
                },
                {
                    "sent": "So once you decide that every point in your data set is well approximated but well approximated by its tangent plane, and someone says OK, compress this data well, one thing you might do is get some budget of K planes.",
                    "label": 0
                },
                {
                    "sent": "And again your manifold to say dimension Q an you choose the best K for your manifold.",
                    "label": 0
                },
                {
                    "sent": "Well, the best one with respect to mean Square.",
                    "label": 1
                },
                {
                    "sent": "Error is exactly the case flats construction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this thing this subspace algorithm is kind of really simple, but at the same time it's it's in a nice place.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a sparse dictionary design algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to think that way, it's a manifold approximation algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK. Now if you want it, you can use it for supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the way you can use it for supervised learning is you simply take.",
                    "label": 1
                },
                {
                    "sent": "Each class of data.",
                    "label": 0
                },
                {
                    "sent": "So your data is divided into training points of class one training points of Class 2, training points of Class 3 etc etc etc and.",
                    "label": 0
                },
                {
                    "sent": "You can simply make a model for each of the classes using this case subspace algorithm.",
                    "label": 0
                },
                {
                    "sent": "What you do is you.",
                    "label": 0
                },
                {
                    "sent": "You train a dictionary for each class, or if you want to think about it that way you build a parameterisation of the manifold of each class with these planes and then given a new data point.",
                    "label": 1
                },
                {
                    "sent": "If you want to figure out which class it belongs to, you say OK.",
                    "label": 0
                },
                {
                    "sent": "Which of these manifolds is closest to or in other words, which dictionary is at best represented in?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is really simple.",
                    "label": 0
                },
                {
                    "sent": "If you were to do this with with K means, you would simply.",
                    "label": 0
                },
                {
                    "sent": "Well, what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means you find centroids for each class.",
                    "label": 0
                },
                {
                    "sent": "And then given a new point, you say which of the centroids is close to the same thing you find, so talk.",
                    "label": 0
                },
                {
                    "sent": "So what are we doing here?",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of points, say from class one, a bunch of points from Class 2.",
                    "label": 0
                },
                {
                    "sent": "You train a dictionary.",
                    "label": 0
                },
                {
                    "sent": "Anna Dictionary just means a set of flats for each of the dictionaries.",
                    "label": 0
                },
                {
                    "sent": "For each of the classes given a new point, you say which guy am I closest to.",
                    "label": 0
                },
                {
                    "sent": "And whichever one you're closest to, that's the one year sent to.",
                    "label": 0
                },
                {
                    "sent": "So in equations, what are you doing?",
                    "label": 0
                },
                {
                    "sent": "You find them in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the of that energy, and I've I've.",
                    "label": 0
                },
                {
                    "sent": "Elided, appoint, so here we're taking the energy with subspace is not without fine sets, and the reason will do that.",
                    "label": 0
                },
                {
                    "sent": "It will be apparent in a minute.",
                    "label": 0
                },
                {
                    "sent": "You can do this without fine sets as well.",
                    "label": 0
                },
                {
                    "sent": "This energy is with subspaces, so again you find some partition of each of the classes.",
                    "label": 0
                },
                {
                    "sent": "And for each of those partitions you find the best fit subspace is to those partitions, and then you optimize over all those things and actually you can do that very, very very fast.",
                    "label": 0
                },
                {
                    "sent": "Actually K subspaces you can do in exactly the same way you do K means with the Lloyd algorithm, or McQueen or whatever and it converges super fast.",
                    "label": 0
                },
                {
                    "sent": "It actually converges faster than case.",
                    "label": 0
                },
                {
                    "sent": "Then K means.",
                    "label": 0
                },
                {
                    "sent": "Practice.",
                    "label": 0
                },
                {
                    "sent": "OK, So what?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem that we want to address is that this is simply a representational method.",
                    "label": 0
                },
                {
                    "sent": "If you want to do a supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If you want to solve a supervised learning problem, you should understand that some things are really different than others.",
                    "label": 0
                },
                {
                    "sent": "You don't just want to make, you don't want to get the best approximation for each class you want to get the best approximation for each class, with the proviso that things in different classes should be far away from each other.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is the common sense thing will simply change the energy with the discriminative term, and then something which comes out to be.",
                    "label": 1
                },
                {
                    "sent": "In some sense of computational artifact, but in some sense it actually helps.",
                    "label": 1
                },
                {
                    "sent": "We're going to generalize planes.",
                    "label": 0
                },
                {
                    "sent": "We won't use planes.",
                    "label": 0
                },
                {
                    "sent": "We use metrics.",
                    "label": 0
                },
                {
                    "sent": "OK so here is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The energy that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "It looks really complicated, but it's really essentially the same thing as we had before.",
                    "label": 0
                },
                {
                    "sent": "Again, if you, if you search through this, all you're doing is you're training planes for each class, so this is simply the case up space, thing, case, subspace, energy for each class.",
                    "label": 0
                },
                {
                    "sent": "That's this bit here, except now you've added a discriminative term.",
                    "label": 0
                },
                {
                    "sent": "So again, you train in energy and the energy takes is over partitions of each class.",
                    "label": 0
                },
                {
                    "sent": "And it's over a basis for each of those partitions.",
                    "label": 0
                },
                {
                    "sent": "The one thing that I said before that instead of subspaces were instead of planes, we're going to use metrics.",
                    "label": 0
                },
                {
                    "sent": "Well, as we solve this as we minimize this energy.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to force the by Jay to be orthonormal, so before the by J were planes I was I was forcing some.",
                    "label": 0
                },
                {
                    "sent": "They were set up as orthonormal sets so now.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to enforce that at all, as I minimize this energy.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to let them do whatever it is that they do and what that means is that.",
                    "label": 0
                },
                {
                    "sent": "This norm here by JX norm well that's just the norm with respect to the Mahalanobis metric B transpose B.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing now is instead of approximating each, instead of approximating your data points by planes, you're approximating your data points by metrics, and at each point you have a different metric, so in some sense it's not that different, because a plane is a very simple version of a Mahalanobis metric.",
                    "label": 1
                },
                {
                    "sent": "You just crush everything.",
                    "label": 0
                },
                {
                    "sent": "It's a Mohammed Novus metric with.",
                    "label": 0
                },
                {
                    "sent": "If you look at the single value combination, you only have ones and zeros on the diagonal, right?",
                    "label": 0
                },
                {
                    "sent": "So here we do a.",
                    "label": 0
                },
                {
                    "sent": "We approximate points by planes, but we don't necessarily have ones and zeros on the diagonal in the singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "It's really the same thing, let me.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put the energies back to back.",
                    "label": 0
                },
                {
                    "sent": "So this up here was the case.",
                    "label": 0
                },
                {
                    "sent": "Subspace is an energy when used for classification.",
                    "label": 0
                },
                {
                    "sent": "Again, this is.",
                    "label": 0
                },
                {
                    "sent": "This is really just K means, but with subspaces instead of.",
                    "label": 0
                },
                {
                    "sent": "Centroids as class representatives an downstairs.",
                    "label": 0
                },
                {
                    "sent": "I've put the new energy and you can see it's the same here.",
                    "label": 0
                },
                {
                    "sent": "G of G1 of this thing was just negative.",
                    "label": 0
                },
                {
                    "sent": "So you can see that really the same thing.",
                    "label": 0
                },
                {
                    "sent": "What's been done is completely trivial in some sense.",
                    "label": 0
                },
                {
                    "sent": "All it's been done is you've taken this energy up here and added a descriptor, discriminative bit, and so a heuristic picture.",
                    "label": 0
                },
                {
                    "sent": "This is the representational you try and find planes or you try and find a parameterisation of each of the two classes.",
                    "label": 0
                },
                {
                    "sent": "This is the representational here, you try and find a good representation, but you want the you want the.",
                    "label": 0
                },
                {
                    "sent": "Parameterisation of this class to be really far away from the pressurisation of this class, OK?",
                    "label": 0
                },
                {
                    "sent": "So in this energy what we want we want G1 to be we want G1 to be very small when its input is large and we want G2 to be large when its input is large, right?",
                    "label": 0
                },
                {
                    "sent": "Because we're going to try and minimize this energy.",
                    "label": 0
                },
                {
                    "sent": "So every time somebody is close to somebody from it's from the enemy class, we want to incur a lot of loss.",
                    "label": 0
                },
                {
                    "sent": "And every time where somebody is far away from somebody in its own class, we want to incur a lot of loss.",
                    "label": 0
                },
                {
                    "sent": "In this case, being far from your own class means the inner product of you against your friendly basis vectors is large that's being close.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a choice of G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "Now what we found and experiments is that you actually could be really sloppy with your choice of G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "And I'll say I'll make one more proviso about that.",
                    "label": 0
                },
                {
                    "sent": "So in all the experiments were going to do later, I'm going to normalize all the data points to the unit sphere.",
                    "label": 0
                },
                {
                    "sent": "This is actually not really a big deal because you could always shift it.",
                    "label": 0
                },
                {
                    "sent": "You could if whatever reason your problem depended on distance, norm of data points.",
                    "label": 0
                },
                {
                    "sent": "You can shift the whole thing in project.",
                    "label": 0
                },
                {
                    "sent": "But once you do this normalization, once every data point is projected onto the unit sphere, then actually you're really safe here.",
                    "label": 0
                },
                {
                    "sent": "You can really use a lot of different choices for G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "There are some things that are really important though that matter.",
                    "label": 0
                },
                {
                    "sent": "The things that matter well, you have to have a margin.",
                    "label": 0
                },
                {
                    "sent": "So for our.",
                    "label": 0
                },
                {
                    "sent": "Choice of G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "Here again, what do we need?",
                    "label": 0
                },
                {
                    "sent": "We want G1 to be really small when its input is large.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, if if the input is large for G1 then.",
                    "label": 0
                },
                {
                    "sent": "Long as it's larger than the margin, we get actually zero.",
                    "label": 0
                },
                {
                    "sent": "We get zero loss here.",
                    "label": 0
                },
                {
                    "sent": "For G2 we have the same thing.",
                    "label": 0
                },
                {
                    "sent": "We want it to be.",
                    "label": 0
                },
                {
                    "sent": "We want it to be really large when its input is large, and again if we if we pick input to G2 to be smaller than the margin that we incur, loss otherwise.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we cost.",
                    "label": 0
                },
                {
                    "sent": "The same thing here.",
                    "label": 0
                },
                {
                    "sent": "The margin the idea of using a margin here is really really important, but the exact shape of the energy really doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "The exact shape shape of the G1 and G2 doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You can kick it around quite a bit as long as you use a margin.",
                    "label": 0
                },
                {
                    "sent": "It seems to be really, really stable, and again, that's because all the data points in the subsequent experiments are going to miss fear, so there's not too much of a difference between the costs here, so it doesn't matter if you put squared or whatever, that's in some sense just to help the optimization.",
                    "label": 0
                },
                {
                    "sent": "As long as you penalize points, as long as you stop penalizing points when they are safe enough, you're happy.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can use just a static gradient descent to minimize the new energy.",
                    "label": 1
                },
                {
                    "sent": "I've written out the derivative simply because.",
                    "label": 0
                },
                {
                    "sent": "Simply because I want you to notice that it's really good if the data A is sparse or be it has some special properties, the derivative really sees multiplications of the basis vectors against the data and against the data transpose, so it's really fast.",
                    "label": 0
                },
                {
                    "sent": "It's really easy to encode a vector.",
                    "label": 0
                },
                {
                    "sent": "A derivative step, and if things are sparse then it eats it up.",
                    "label": 0
                },
                {
                    "sent": "It's great, it's very happy.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's see.",
                    "label": 0
                },
                {
                    "sent": "Let's skip this slide.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use multiple restarts in the thing so OK, So what did we test this on?",
                    "label": 0
                },
                {
                    "sent": "I tested this on the endless digits so ya'll are are kind of familiar with this.",
                    "label": 0
                },
                {
                    "sent": "I think it's 70,028 by 28 images so here they were here.",
                    "label": 0
                },
                {
                    "sent": "They were projected onto their first principle, 51st principle components.",
                    "label": 0
                },
                {
                    "sent": "We work on the 20 newsgroups data set, so this is text data an it's represented by its term document matrix and.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here I used the 5000 most common words.",
                    "label": 0
                },
                {
                    "sent": "So this was to compare.",
                    "label": 0
                },
                {
                    "sent": "There was a HTML paper of last year.",
                    "label": 0
                },
                {
                    "sent": "The problem is this particular data set.",
                    "label": 0
                },
                {
                    "sent": "There are a million different normalizations, so people people use this in lots of different papers as a data set.",
                    "label": 0
                },
                {
                    "sent": "However, every data set uses a different normalization.",
                    "label": 0
                },
                {
                    "sent": "I picked one from last year and I smell to compare against, but there's unfortunately many different choices.",
                    "label": 0
                },
                {
                    "sent": "So here the data is the 5000.",
                    "label": 1
                },
                {
                    "sent": "It's a term document matrix with the 5000 most common words.",
                    "label": 1
                },
                {
                    "sent": "An everybody is either zero or one.",
                    "label": 0
                },
                {
                    "sent": "Depending that you don't do word counts, you just do.",
                    "label": 0
                },
                {
                    "sent": "Is there a word?",
                    "label": 0
                },
                {
                    "sent": "Is there not a word?",
                    "label": 0
                },
                {
                    "sent": "Um and I select is 200 speakers saying each letter of the alphabet twice an it's there's 617 audio features that have been extracted, but actually nobody knows what the audio features are anymore, so it's actually a really nice machine learning data set because it's impossible to apply domain knowledge, but OK, so all three of these things after it was done, it was the data was projected onto the unit sphere and so here are the results.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of running this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is this is.",
                    "label": 0
                },
                {
                    "sent": "With restarts, this is out without restart, so this is with five restarts, and this is without restarts.",
                    "label": 0
                },
                {
                    "sent": "What you'll notice is that restarts help some things more than others, and that's because some so so the parameters that are here in this parameters that are here are the K, the number of metrics that we use Q, which is the Max dimension of the metrics and the margins and all these things are chosen by cross validation in datasets where you need where you use lots of metrics.",
                    "label": 0
                },
                {
                    "sent": "Then the number of restarts is really important.",
                    "label": 0
                },
                {
                    "sent": "If you restart a lot then it helps in the 20 newsgroups.",
                    "label": 1
                },
                {
                    "sent": "It only wanted one metric for class and so of course it didn't matter whether you restart or not because you only use one metric for class.",
                    "label": 0
                },
                {
                    "sent": "So, OK, what you notice from this table?",
                    "label": 0
                },
                {
                    "sent": "So a is that the method is pretty good.",
                    "label": 0
                },
                {
                    "sent": "So the SVM baseline is on the bottom.",
                    "label": 0
                },
                {
                    "sent": "It loses out on isolate, but it does really well on the other two.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the other thing.",
                    "label": 0
                },
                {
                    "sent": "So here are some timings.",
                    "label": 0
                },
                {
                    "sent": "So the graphs are passes through the data set, so we use stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So each graph is 40 runs through 40 passes, and the numbers here are the timings for the thing in seconds 440 passes and what you notice is you're getting really good results long before 40 passes.",
                    "label": 0
                },
                {
                    "sent": "So for example, to get the really good results on M nest to get like a one point 1% error on M Nest, you need roughly 20 passes, so you need 150 seconds.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you need 5 * 150 seconds, so MNIST really was helped by the restarts.",
                    "label": 0
                },
                {
                    "sent": "So you need roughly roughly 700 seconds to get a really good classifier for amnesty for 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "Again the same thing that you're getting really good.",
                    "label": 1
                },
                {
                    "sent": "You're getting really good results after about 10 passes through the data, so that's about 200 seconds.",
                    "label": 0
                },
                {
                    "sent": "By the way, notice that this really is happy with sparseness here.",
                    "label": 0
                },
                {
                    "sent": "Use only 5000 words.",
                    "label": 0
                },
                {
                    "sent": "If I use the full dictionary, which is something like 50,000 words.",
                    "label": 1
                },
                {
                    "sent": "The total processing time.",
                    "label": 1
                },
                {
                    "sent": "For 40 passes gets to 900 seconds.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't actually get hurt by, you know, by the fact that it's the.",
                    "label": 0
                },
                {
                    "sent": "The 20 newsgroups is 18,000 by 60,000.",
                    "label": 0
                },
                {
                    "sent": "Because it's sparse.",
                    "label": 0
                },
                {
                    "sent": "The updates are all very sparse and so it's very happy.",
                    "label": 0
                },
                {
                    "sent": "So The thing is, this is you can run this on gigantic datasets with no problem.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is a different data set, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the bikes from the grass image database, so the grass image database is pictures of bikes, people and cars and background, so we'll see what the pictures look like in a second and the task is to someone gives you.",
                    "label": 1
                },
                {
                    "sent": "So the task in this database is.",
                    "label": 0
                },
                {
                    "sent": "You're given a picture of a bike or car or person, whatever, but you're not giving the whole picture.",
                    "label": 0
                },
                {
                    "sent": "You're just given the neighborhood of a pixel and you have to figure out if that pixel belongs to a bike or belongs to a car belongs to background, etc.",
                    "label": 0
                },
                {
                    "sent": "And sorry, OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK Yep, yeah OK, so these are.",
                    "label": 0
                },
                {
                    "sent": "These are what the features this is.",
                    "label": 0
                },
                {
                    "sent": "What you get if you run through the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teachers, I mean, these are the heat Maps of picking up the thing and all.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These decisions are local by the way.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'll stop.",
                    "label": 0
                },
                {
                    "sent": "We have time.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A couple of quick questions.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK. OK, so OK.",
                    "label": 0
                },
                {
                    "sent": "So the really important problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the conclusion is simply some supervised method.",
                    "label": 0
                },
                {
                    "sent": "This is really good as a supervised method.",
                    "label": 0
                },
                {
                    "sent": "It's fast, it's it gives great results, but.",
                    "label": 0
                },
                {
                    "sent": "There should be a semi supervised method there and I don't know it yet, so somehow that's the conclusion is do this in a semi supervised manner and that would be cool.",
                    "label": 0
                },
                {
                    "sent": "Add a question.",
                    "label": 0
                },
                {
                    "sent": "So how does this compare to fitting mixture of Gaussians?",
                    "label": 0
                },
                {
                    "sent": "Where basically these flats are simple Gaussians which have infinite variance in some dimensions with zero variance and others yes.",
                    "label": 0
                },
                {
                    "sent": "It seems like the Faciane Lawrence all right, so this is really.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's really as soon as you assume you say I'm fitting flats like this, you're fitting Gaussians, there's I mean, it's almost the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's just a geometric version of it.",
                    "label": 0
                },
                {
                    "sent": "So I would say it's very, very very similar actually.",
                    "label": 0
                },
                {
                    "sent": "On that's it, but remember there is one additional feature which is once once we move to once we once we move to.",
                    "label": 0
                },
                {
                    "sent": "So actually, especially once you move to metrics, it's even more because really, you're saying well, some dimensions were crushing a lot and some little so it's quite similar in that respect.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like an experiment of mixture factor analyzing.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly that's correct.",
                    "label": 0
                },
                {
                    "sent": "In that framework is pretty straightforward and doing bunch of different settings, different setups.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Type setup right?",
                    "label": 0
                },
                {
                    "sent": "So I've tried.",
                    "label": 0
                },
                {
                    "sent": "I've tried these sorts of things, but remember that so the way the energy is set up right now, it's very hard to do a straight M like it's just a nasty energy for actually computing a good.",
                    "label": 0
                },
                {
                    "sent": "So right now the best way to do the minimization is simplistic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "You can't, you can't just find the minimum and then go from there for the current energy.",
                    "label": 0
                },
                {
                    "sent": "So for the straight K subspaces, for the straight mixture effector without the discrimination.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you try and do anything like that, it doesn't work too well.",
                    "label": 0
                },
                {
                    "sent": "Expected gradient as opposed to post form.",
                    "label": 0
                },
                {
                    "sent": "Say it again, sorry.",
                    "label": 0
                },
                {
                    "sent": "Closed form yeah sure sure, so I've tried.",
                    "label": 0
                },
                {
                    "sent": "I've tried a lot of the first common sense.",
                    "label": 0
                },
                {
                    "sent": "Things like the a lot of the first things that you would try.",
                    "label": 0
                },
                {
                    "sent": "I've tried.",
                    "label": 0
                },
                {
                    "sent": "The problem is the way this is working.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "It really leans on the guys that are close to the margin in a strong way.",
                    "label": 0
                },
                {
                    "sent": "And as soon as you.",
                    "label": 0
                },
                {
                    "sent": "As soon as you try and do a semi supervised, if you get one of those guys close to the margin wrong, the whole thing collapses.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Bother you computacional costs for this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's really.",
                    "label": 0
                },
                {
                    "sent": "I mean again.",
                    "label": 0
                },
                {
                    "sent": "So in terms of where is it?",
                    "label": 0
                },
                {
                    "sent": "So it's very fast.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is the computational cost, so again, so for example, MNIST is 70,000 points.",
                    "label": 0
                },
                {
                    "sent": "To get good results you're taking about.",
                    "label": 0
                },
                {
                    "sent": "Sorry, yeah, 60,000 points in the training set.",
                    "label": 0
                },
                {
                    "sent": "So to get good results you're taking about 5 * 150 seconds or 5 * 100 seconds.",
                    "label": 0
                },
                {
                    "sent": "So you know 1020 minutes at most.",
                    "label": 0
                },
                {
                    "sent": "Faster.",
                    "label": 0
                },
                {
                    "sent": "In the Commission one.",
                    "label": 0
                },
                {
                    "sent": "It depends what the conventional one is, so it's faster than a lot of things.",
                    "label": 0
                },
                {
                    "sent": "But I mean, for example straight K subspaces and much faster.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Talk about cleaning so that makes good decisions.",
                    "label": 0
                },
                {
                    "sent": "OK, So what happens far away won't impact you, yes?",
                    "label": 0
                },
                {
                    "sent": "Say you have a plane at that very far away it can.",
                    "label": 0
                },
                {
                    "sent": "It can make you believe that too very close.",
                    "label": 0
                },
                {
                    "sent": "Right, so yes, so that's a low dimensional phenomenon, right?",
                    "label": 0
                },
                {
                    "sent": "Because in high dimensions, if you're in 100 dimensions, if you're close to, uh, I mean so.",
                    "label": 0
                },
                {
                    "sent": "This picture is extremely misleading, and in fact the picture I put up earlier is extremely misleading.",
                    "label": 0
                },
                {
                    "sent": "'cause it's in 2D, the planes cross.",
                    "label": 0
                },
                {
                    "sent": "If you give Me 2 planes in R-100, even if the data was really similar, the planes don't cross.",
                    "label": 0
                },
                {
                    "sent": "They are very far from each other.",
                    "label": 0
                },
                {
                    "sent": "This idea that that subspace is if for example in Edna's.",
                    "label": 0
                },
                {
                    "sent": "I think the cross validated parameter was.",
                    "label": 0
                },
                {
                    "sent": "10 dimensional subspaces in R50, right?",
                    "label": 0
                },
                {
                    "sent": "The subspaces are all extremely far from each other, and it's very hard to get two subspaces close to each other in high dimension.",
                    "label": 0
                },
                {
                    "sent": "No, no, but even with even with data, that's what I'm saying.",
                    "label": 0
                },
                {
                    "sent": "Even in this case, if you look at the distance between the subspaces, they're always there, always reasonably far from each other.",
                    "label": 0
                },
                {
                    "sent": "It's it's pretty hard, even when it's not, even when it's not random.",
                    "label": 0
                },
                {
                    "sent": "You have to work.",
                    "label": 0
                },
                {
                    "sent": "I mean, if the two subspaces are really hitting each other, it means your whole data lived on a low dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "Let me say, let me say it a different way.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have several subspaces and they hit right and you know these are generally low dimensional space compared to the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "If they hit, that means that subset of the data lived in, say that dimension plus one or smaller.",
                    "label": 0
                },
                {
                    "sent": "In a linear sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry.",
                    "label": 0
                }
            ]
        }
    }
}