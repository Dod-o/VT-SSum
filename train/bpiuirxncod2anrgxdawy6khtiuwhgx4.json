{
    "id": "bpiuirxncod2anrgxdawy6khtiuwhgx4",
    "title": "On Architectural Issues of Neural Networks in Speech Recognition",
    "info": {
        "author": [
            "Hermann Ney, Computer Science Department, RWTH Aachen University"
        ],
        "published": "July 31, 2016",
        "recorded": "July 2016",
        "category": [
            "Top->Computer Science",
            "Top->Technology"
        ]
    },
    "url": "http://videolectures.net/interACT2016_ney_neural_networks/",
    "segmentation": [
        [
            "It's not for me to be here.",
            "Thanks for the invitation.",
            "And the topic of my talk will actually be very close to what Florian Mezza introduced, and it will be maybe a little bit technical we will see."
        ],
        [
            "So."
        ],
        [
            "In my team at my chair we are working on human language technology and we're covering speech recognition, machine translation, handwriting recognition.",
            "And the unifying view here is that we convert input string to an output string.",
            "And the output string is always a string of words.",
            "Or let us in a natural language.",
            "But this is rather abstract view.",
            "And for traditional linguists, that may be difficult or hard to accept, but that's the position we take.",
            "Ah."
        ],
        [
            "And of course, these topics have a large overlap with the topics of the members of interact, so here I have tried to put together the joint projects that my team had with one of the interact members, be it KIT KIT, CMU or Hong Kong University of Science and Technology.",
            "So the first project here is Web mobile which at that time.",
            "Was a task that is, which is a task that is nowadays considered to be a toy task itself.",
            "Virtual capillary.",
            "Then we had CTC stop project that was mentioned already by Marcelo.",
            "So this was the first research system to the best of my knowledge, that worked on a real life task on open domain and large vocabulary and we decided in that project to work on speeches given in the European Parliament.",
            "And there were typically European partners, but there was also IBM from the US IBM research from US was involved.",
            "So here's the languages involved.",
            "Where are Spanish English?",
            "And in more less in parallel we had the Gale project.",
            "So here's the source.",
            "Languages were Chinese and Arabic, and the goal was to translate into.",
            "English, and an interesting aspect here is that as far as I know, this is the largest project ever on speech and language, so funding volume was something like 40,000,000 US dollars per year for a period of five years.",
            "Then Bolt was a sort of for a project.",
            "With emphasis more on colloquial language.",
            "Then we had the squirrel project.",
            "Which focused on European languages.",
            "And interesting here is this was mainly a French project.",
            "There were two non French partners.",
            "It was Alex team and my team.",
            "And then again, American project, baby.",
            "So the interesting thing here is that.",
            "There was only one German project.",
            "Uh and other projects had a federal funding source different.",
            "From Germany."
        ],
        [
            "So these were the projects, and within these projects, of course of our evaluation so important of aberrations was already mentioned by Marcelo.",
            "Off the organized within these.",
            "Projects our own our own evaluations.",
            "And in addition, we participated.",
            "In a couple of evaluation campaigns, overtime we participated means my team and also one of the interact teams.",
            "Typically it was KIT.",
            "Um, so in addition, here to the.",
            "Evaluations organized by NIST and National Institute of Standards and Technology.",
            "Linguistic data consortium, entropa.",
            "We also had have the IW idealistic evaluations which are organized by.",
            "Members of the interacting directly and also ACL workshop on machine translation, and in some cases I mean in most cases we had competitive submissions, but in some cases also we are joint submissions in order to build the best system that was possible.",
            "And this of course was done by doing system combination."
        ],
        [
            "So.",
            "The approach approach that we are using is a typical approach.",
            "To use the words of our Chancellor, I think there is no alternative.",
            "We want to build this system with optimum performance.",
            "Then you have to workout your mathematics and then you end up this pace decision rule.",
            "You end up with probability distributions and so on and coming back to the discussion we had this morning about the difference.",
            "I mean difference in apostrophe, Speedway in statistical empty and or empty.",
            "No, that works.",
            "Apart of statistics, there's no difference in a special type of statistics, but I think it's not correct.",
            "To my view.",
            "It's not correct to consider them to be something different from statistics."
        ],
        [
            "Maybe so from that point of view we need in a separate new name for it, but just calling it newer, empty and distinguish it from from static empty is, in my view, misleading, misleading, misleading terminology.",
            "So using hidden Markov models that were mentioned already by Florian so I can skip the details.",
            "Basically basically we have.",
            "Sequence of acoustic observations over the time axis.",
            "We have linear arrangement of states that represent phonetic events that we hypothesize or that are that are known in training in recognition.",
            "So they have to be hypothesized, and then we have to look at all possible, let's say ways in which we can.",
            "Put this.",
            "Observation sequence into our finite state automaton.",
            "So the main purpose here is really to handle the time alignment or synchronization problem."
        ],
        [
            "And the key quantity here place the emission probability.",
            "And typically this dependence is modeled by using phonetic labels, so the distribution of the observation does not really depend.",
            "That depends on the state.",
            "In the state within a word sequence with high positive word sequence.",
            "Via the Associated Phonetic label."
        ],
        [
            "And now the idea of the hybrid approach.",
            "Is to replace the this emission probability distribution by an artificial neural network, but the output of an artificial neural network.",
            "And, um.",
            "The justification is, well, this can.",
            "This distribution is easier to model because it's a distribution over discrete labels.",
            "This is easier to model then the distribution over the acoustic vectors, which are continuous valued vectors, which is a much more difficult problem.",
            "It's a problem with density estimation, and this was actually has been known for quite some time in pattern recognition.",
            "That this is a case, but somehow in speech recognition people use in Markov models and see EM algorithm and this has a certain mathematical beauty.",
            "And so this whole thing was more less ignored, ignored for quite some time by the majority of people working in this.",
            "Area so."
        ],
        [
            "So now coming.",
            "I'm now coming to the history in more detail, so I think the one or one of the first papers have found about this.",
            "Concept of using the output of networks for speech or phone recognition is a paper by Alex in 88.",
            "Then there was a paper by John Bridle who argued for the softmax operation to normalize the outputs of the neural network.",
            "Then board Bolan Velican say advocated.",
            "See hybrid approach.",
            "That is what I present, but I trust presented.",
            "The idea is really to replace the emission probabilities in hmm.",
            "By the outputs of the neural networks after after suitable rescaling.",
            "And never even subtleties that were considered at the time.",
            "So Patrick Haffner had a paper in which he considered the sum over the label sequence.",
            "Posterior probabilities, which brings us close to what to the CTC approach that was.",
            "Mentioned by or introduced by the previous session by Florian.",
            "And then there is a work by Tony Robinson recovered more networks.",
            "So he he and his team were able to get competitive results on the Wall Street Journal task.",
            "But somehow he's worked remained a singularity.",
            "In ASR and nobody continued continued along these lines.",
            "So the situation was that on both that until 2011 or 2012.",
            "Ian ends we're not really competitive to Gaussian mixture models, and this situation changed when leading and Jeff and had a Corporation and applied what is nowadays called deep learning with applied deep learning to ASR."
        ],
        [
            "So the more let's say papers approaches to be mentioned, convolution network by younger car, some more papers by Alex Waibel, Steam, then in particular, the long short term memory.",
            "In combination with recurrent neural network which turns out to be as far as I can see it, one of the most successful approaches.",
            "To the handling of sequences.",
            "People typically tend to think that deep learning started really around 2012 or so, but when you look at the papers you'll find a couple of papers that seem to indicate when you increase the complexity of your network, then you can improve the performance.",
            "And then around 2012 there were other teams that were able to confirm the significant significant reductions.",
            "By that could be obtained by artificial neural networks."
        ],
        [
            "So the time delay in network was mentioned already a couple of times, so it's basically a feedforward multilayer perceptron, but with special properties.",
            "Like using a long temporal context and using weight sharing.",
            "Which is a property that is also true for conventional convolutional neural networks.",
            "And here we have the structure.",
            "So basically at time T we look at the context of let's say plus minus 3 frames.",
            "Then they are used to compute the output of the hidden layer.",
            "At this point in time.",
            "Then the outputs of the neighboring hidden layers are neighboring in terms of time are combined in order to compute the output for a second hidden layer.",
            "And you can go on record like this.",
            "Or you can really use this.",
            "Stop you to see up.",
            "Output distribution."
        ],
        [
            "So the 1st paper I found, at least with Google, was the paper I mentioned in 88.",
            "This was a paper presented at Icast.",
            "Um, New York and around the same time I remember there was a SRU workshop.",
            "I'm not sure, but it was one year before or you want your after that, where there were really controversial discussions about neural networks and.",
            "And conventional hidden Markov models.",
            "Now the first Journal paper that I could find is a paper that was mentioned already here to the.",
            "Paper that came out in 89 IEEE transactions.",
            "Transactions on acoustics, speech and signal processing.",
            "And yesterday I looked up the citations over about 2000 citations according to Google Scholar and there were three more papers by Alex and his team and they have a citation number of about 1100.",
            "And what is interesting is there has been some recent work by then.",
            "Povey and his team at which they reported at Interspeech.",
            "Last year in Dresden and they could report significant improvements over the let's say conventional deep MLP approach and they confirmed this or they were able to get these improvements on many of the standard ASR tasks like Wall Street Journal Switchboard and Leprus Peach or Hyper speech.",
            "I don't know how to pronounce it and then also on the Aspire Challenge which involves reverberant speech in a far field speech recognition task."
        ],
        [
            "So the conventional deep.",
            "We have to come to an end already.",
            "OK as the conventional.",
            "Modular perception is shown here, so you can ask the question, what is today different from what we had 1025 years ago or so.",
            "So we have 10 hidden layers or more.",
            "We have more output units and we have learned how to handle the optimization problem and typically what is found is that the error it can be nearly half which is a figure.",
            "I think 42% were mentioned by Chris Chris Owens this morning and so that's about consistent with what many other teams report."
        ],
        [
            "So one important improvement can be obtained by using recurrent neural networks, specifically in combination with this long short term memory."
        ],
        [
            "Now I would like to come to this as this aspect of.",
            "Sequence labeling that was mentioned already by Florian Metzer.",
            "We could try to reformulate the problem with speech recognition by saying OK, we're really only interested in a sequence of phonetic labels, and if you have these labels, say I did, they uniquely uniquely determine the sequence of words.",
            "And what we need is basically a posterior distribution over these labels which can be computed by a neural network and then in addition to that we need some localization.",
            "So we need some sort of.",
            "Of time alignment pass or similar.",
            "Concept."
        ],
        [
            "And OK, I think I skip search."
        ],
        [
            "Same reason so we could try to apply this to a conventional HMM structure.",
            "So here we have three states for each of the assembled.",
            "So Alex, this was done.",
            "Let's say Richard for handwriting recognition we can use the same principle for phonemes, and so this looks very much to what we had in the case of a conventional hidden Markov model.",
            "But the difference is that we do not aim to have a joint model do not want to have a model over the observed acoustic vectors and see.",
            "Hope is that we can get away.",
            "Without the renormalization proper, which is a serious problem here, but the open issues that have to be handled is how to include transition probabilities, how to make use of the language model?",
            "What could be a consistent training criterion?",
            "Should we take the sum of all alignments?",
            "How should we do end to end training and so on and?"
        ],
        [
            "This is a ritual CTC concept that was introduced by Graves around 2006.",
            "Again, this is what Florian had just to present it.",
            "So topology here is for each symbol we have just a single state at the blank state.",
            "We don't have any transition probabilities.",
            "Training criterion is a sum the instructor used is typically an STM RNN, or maybe some other structure.",
            "And like Florian, as our teams also report good results, but I'm not really convinced whether this is due to the specific CTC property, because this really looks like a more or less like a specific hidden Markov model, and my speculation is these good good results are reported or maybe due to the LCM.",
            "So I think we need more work in order to direct comparisons and see what is really what are the possible advantages of this CTC concept?",
            "Now."
        ],
        [
            "When you accept this concepts and you could also say we could really invert our time alignment properly.",
            "We could try to say we just have to segment our speech signal.",
            "So we need a mapping from the states.",
            "To the time access and then you can reformulate.",
            "You are your modeling problem.",
            "That's what we're working on."
        ],
        [
            "My my team and it's interesting to see that this is getting close to the mechanism of attention that was originally introduced for machine translation by the Montreal team.",
            "So basically you have two recurrent neural networks to recurrent structures one over the time axis and 1 / Z state access log which you form your hypothesis for recognition.",
            "Um, as interesting thing is that this mechanism uses NN concepts only.",
            "There's nothing of a hidden Markov model in it.",
            "And the alignment direction here is also from the states to the time axis.",
            "And they make use of occupation probabilities, which can have an interpretation similar to the occupation probabilities.",
            "In the case of conventional hidden Markov models and.",
            "Many teams are working on that.",
            "This is also ongoing work in my team and we have to see which of these different architectures will have the advantage."
        ],
        [
            "So let me try to summarize.",
            "I think the experimental results.",
            "Shows that there is lots of room for improvements.",
            "But I think it is difficult at the moment to draw clear conclusions.",
            "What causes the the improvements?",
            "Is it just the STM aronin structure which is typically found to be very, very?",
            "Efficient in many applications?",
            "Or is it something in addition?"
        ],
        [
            "And of course, the most important conclusion.",
            "Most important summary is congratulations to interact and RX on 25 successful years and my best wishes for the coming 25 years."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not for me to be here.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the invitation.",
                    "label": 0
                },
                {
                    "sent": "And the topic of my talk will actually be very close to what Florian Mezza introduced, and it will be maybe a little bit technical we will see.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In my team at my chair we are working on human language technology and we're covering speech recognition, machine translation, handwriting recognition.",
                    "label": 1
                },
                {
                    "sent": "And the unifying view here is that we convert input string to an output string.",
                    "label": 1
                },
                {
                    "sent": "And the output string is always a string of words.",
                    "label": 0
                },
                {
                    "sent": "Or let us in a natural language.",
                    "label": 0
                },
                {
                    "sent": "But this is rather abstract view.",
                    "label": 0
                },
                {
                    "sent": "And for traditional linguists, that may be difficult or hard to accept, but that's the position we take.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, these topics have a large overlap with the topics of the members of interact, so here I have tried to put together the joint projects that my team had with one of the interact members, be it KIT KIT, CMU or Hong Kong University of Science and Technology.",
                    "label": 0
                },
                {
                    "sent": "So the first project here is Web mobile which at that time.",
                    "label": 0
                },
                {
                    "sent": "Was a task that is, which is a task that is nowadays considered to be a toy task itself.",
                    "label": 0
                },
                {
                    "sent": "Virtual capillary.",
                    "label": 0
                },
                {
                    "sent": "Then we had CTC stop project that was mentioned already by Marcelo.",
                    "label": 0
                },
                {
                    "sent": "So this was the first research system to the best of my knowledge, that worked on a real life task on open domain and large vocabulary and we decided in that project to work on speeches given in the European Parliament.",
                    "label": 0
                },
                {
                    "sent": "And there were typically European partners, but there was also IBM from the US IBM research from US was involved.",
                    "label": 0
                },
                {
                    "sent": "So here's the languages involved.",
                    "label": 0
                },
                {
                    "sent": "Where are Spanish English?",
                    "label": 0
                },
                {
                    "sent": "And in more less in parallel we had the Gale project.",
                    "label": 0
                },
                {
                    "sent": "So here's the source.",
                    "label": 0
                },
                {
                    "sent": "Languages were Chinese and Arabic, and the goal was to translate into.",
                    "label": 1
                },
                {
                    "sent": "English, and an interesting aspect here is that as far as I know, this is the largest project ever on speech and language, so funding volume was something like 40,000,000 US dollars per year for a period of five years.",
                    "label": 1
                },
                {
                    "sent": "Then Bolt was a sort of for a project.",
                    "label": 0
                },
                {
                    "sent": "With emphasis more on colloquial language.",
                    "label": 0
                },
                {
                    "sent": "Then we had the squirrel project.",
                    "label": 0
                },
                {
                    "sent": "Which focused on European languages.",
                    "label": 0
                },
                {
                    "sent": "And interesting here is this was mainly a French project.",
                    "label": 0
                },
                {
                    "sent": "There were two non French partners.",
                    "label": 0
                },
                {
                    "sent": "It was Alex team and my team.",
                    "label": 0
                },
                {
                    "sent": "And then again, American project, baby.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing here is that.",
                    "label": 0
                },
                {
                    "sent": "There was only one German project.",
                    "label": 0
                },
                {
                    "sent": "Uh and other projects had a federal funding source different.",
                    "label": 0
                },
                {
                    "sent": "From Germany.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these were the projects, and within these projects, of course of our evaluation so important of aberrations was already mentioned by Marcelo.",
                    "label": 0
                },
                {
                    "sent": "Off the organized within these.",
                    "label": 0
                },
                {
                    "sent": "Projects our own our own evaluations.",
                    "label": 0
                },
                {
                    "sent": "And in addition, we participated.",
                    "label": 0
                },
                {
                    "sent": "In a couple of evaluation campaigns, overtime we participated means my team and also one of the interact teams.",
                    "label": 1
                },
                {
                    "sent": "Typically it was KIT.",
                    "label": 0
                },
                {
                    "sent": "Um, so in addition, here to the.",
                    "label": 0
                },
                {
                    "sent": "Evaluations organized by NIST and National Institute of Standards and Technology.",
                    "label": 1
                },
                {
                    "sent": "Linguistic data consortium, entropa.",
                    "label": 0
                },
                {
                    "sent": "We also had have the IW idealistic evaluations which are organized by.",
                    "label": 0
                },
                {
                    "sent": "Members of the interacting directly and also ACL workshop on machine translation, and in some cases I mean in most cases we had competitive submissions, but in some cases also we are joint submissions in order to build the best system that was possible.",
                    "label": 0
                },
                {
                    "sent": "And this of course was done by doing system combination.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The approach approach that we are using is a typical approach.",
                    "label": 0
                },
                {
                    "sent": "To use the words of our Chancellor, I think there is no alternative.",
                    "label": 1
                },
                {
                    "sent": "We want to build this system with optimum performance.",
                    "label": 1
                },
                {
                    "sent": "Then you have to workout your mathematics and then you end up this pace decision rule.",
                    "label": 0
                },
                {
                    "sent": "You end up with probability distributions and so on and coming back to the discussion we had this morning about the difference.",
                    "label": 0
                },
                {
                    "sent": "I mean difference in apostrophe, Speedway in statistical empty and or empty.",
                    "label": 0
                },
                {
                    "sent": "No, that works.",
                    "label": 0
                },
                {
                    "sent": "Apart of statistics, there's no difference in a special type of statistics, but I think it's not correct.",
                    "label": 0
                },
                {
                    "sent": "To my view.",
                    "label": 0
                },
                {
                    "sent": "It's not correct to consider them to be something different from statistics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe so from that point of view we need in a separate new name for it, but just calling it newer, empty and distinguish it from from static empty is, in my view, misleading, misleading, misleading terminology.",
                    "label": 0
                },
                {
                    "sent": "So using hidden Markov models that were mentioned already by Florian so I can skip the details.",
                    "label": 0
                },
                {
                    "sent": "Basically basically we have.",
                    "label": 0
                },
                {
                    "sent": "Sequence of acoustic observations over the time axis.",
                    "label": 0
                },
                {
                    "sent": "We have linear arrangement of states that represent phonetic events that we hypothesize or that are that are known in training in recognition.",
                    "label": 0
                },
                {
                    "sent": "So they have to be hypothesized, and then we have to look at all possible, let's say ways in which we can.",
                    "label": 0
                },
                {
                    "sent": "Put this.",
                    "label": 0
                },
                {
                    "sent": "Observation sequence into our finite state automaton.",
                    "label": 0
                },
                {
                    "sent": "So the main purpose here is really to handle the time alignment or synchronization problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the key quantity here place the emission probability.",
                    "label": 0
                },
                {
                    "sent": "And typically this dependence is modeled by using phonetic labels, so the distribution of the observation does not really depend.",
                    "label": 0
                },
                {
                    "sent": "That depends on the state.",
                    "label": 0
                },
                {
                    "sent": "In the state within a word sequence with high positive word sequence.",
                    "label": 0
                },
                {
                    "sent": "Via the Associated Phonetic label.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now the idea of the hybrid approach.",
                    "label": 1
                },
                {
                    "sent": "Is to replace the this emission probability distribution by an artificial neural network, but the output of an artificial neural network.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "The justification is, well, this can.",
                    "label": 0
                },
                {
                    "sent": "This distribution is easier to model because it's a distribution over discrete labels.",
                    "label": 0
                },
                {
                    "sent": "This is easier to model then the distribution over the acoustic vectors, which are continuous valued vectors, which is a much more difficult problem.",
                    "label": 1
                },
                {
                    "sent": "It's a problem with density estimation, and this was actually has been known for quite some time in pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "That this is a case, but somehow in speech recognition people use in Markov models and see EM algorithm and this has a certain mathematical beauty.",
                    "label": 0
                },
                {
                    "sent": "And so this whole thing was more less ignored, ignored for quite some time by the majority of people working in this.",
                    "label": 0
                },
                {
                    "sent": "Area so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now coming.",
                    "label": 0
                },
                {
                    "sent": "I'm now coming to the history in more detail, so I think the one or one of the first papers have found about this.",
                    "label": 0
                },
                {
                    "sent": "Concept of using the output of networks for speech or phone recognition is a paper by Alex in 88.",
                    "label": 0
                },
                {
                    "sent": "Then there was a paper by John Bridle who argued for the softmax operation to normalize the outputs of the neural network.",
                    "label": 0
                },
                {
                    "sent": "Then board Bolan Velican say advocated.",
                    "label": 0
                },
                {
                    "sent": "See hybrid approach.",
                    "label": 0
                },
                {
                    "sent": "That is what I present, but I trust presented.",
                    "label": 0
                },
                {
                    "sent": "The idea is really to replace the emission probabilities in hmm.",
                    "label": 1
                },
                {
                    "sent": "By the outputs of the neural networks after after suitable rescaling.",
                    "label": 0
                },
                {
                    "sent": "And never even subtleties that were considered at the time.",
                    "label": 0
                },
                {
                    "sent": "So Patrick Haffner had a paper in which he considered the sum over the label sequence.",
                    "label": 0
                },
                {
                    "sent": "Posterior probabilities, which brings us close to what to the CTC approach that was.",
                    "label": 0
                },
                {
                    "sent": "Mentioned by or introduced by the previous session by Florian.",
                    "label": 0
                },
                {
                    "sent": "And then there is a work by Tony Robinson recovered more networks.",
                    "label": 0
                },
                {
                    "sent": "So he he and his team were able to get competitive results on the Wall Street Journal task.",
                    "label": 1
                },
                {
                    "sent": "But somehow he's worked remained a singularity.",
                    "label": 0
                },
                {
                    "sent": "In ASR and nobody continued continued along these lines.",
                    "label": 0
                },
                {
                    "sent": "So the situation was that on both that until 2011 or 2012.",
                    "label": 1
                },
                {
                    "sent": "Ian ends we're not really competitive to Gaussian mixture models, and this situation changed when leading and Jeff and had a Corporation and applied what is nowadays called deep learning with applied deep learning to ASR.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the more let's say papers approaches to be mentioned, convolution network by younger car, some more papers by Alex Waibel, Steam, then in particular, the long short term memory.",
                    "label": 0
                },
                {
                    "sent": "In combination with recurrent neural network which turns out to be as far as I can see it, one of the most successful approaches.",
                    "label": 0
                },
                {
                    "sent": "To the handling of sequences.",
                    "label": 0
                },
                {
                    "sent": "People typically tend to think that deep learning started really around 2012 or so, but when you look at the papers you'll find a couple of papers that seem to indicate when you increase the complexity of your network, then you can improve the performance.",
                    "label": 0
                },
                {
                    "sent": "And then around 2012 there were other teams that were able to confirm the significant significant reductions.",
                    "label": 0
                },
                {
                    "sent": "By that could be obtained by artificial neural networks.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the time delay in network was mentioned already a couple of times, so it's basically a feedforward multilayer perceptron, but with special properties.",
                    "label": 1
                },
                {
                    "sent": "Like using a long temporal context and using weight sharing.",
                    "label": 0
                },
                {
                    "sent": "Which is a property that is also true for conventional convolutional neural networks.",
                    "label": 0
                },
                {
                    "sent": "And here we have the structure.",
                    "label": 0
                },
                {
                    "sent": "So basically at time T we look at the context of let's say plus minus 3 frames.",
                    "label": 0
                },
                {
                    "sent": "Then they are used to compute the output of the hidden layer.",
                    "label": 0
                },
                {
                    "sent": "At this point in time.",
                    "label": 0
                },
                {
                    "sent": "Then the outputs of the neighboring hidden layers are neighboring in terms of time are combined in order to compute the output for a second hidden layer.",
                    "label": 0
                },
                {
                    "sent": "And you can go on record like this.",
                    "label": 0
                },
                {
                    "sent": "Or you can really use this.",
                    "label": 0
                },
                {
                    "sent": "Stop you to see up.",
                    "label": 0
                },
                {
                    "sent": "Output distribution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st paper I found, at least with Google, was the paper I mentioned in 88.",
                    "label": 0
                },
                {
                    "sent": "This was a paper presented at Icast.",
                    "label": 0
                },
                {
                    "sent": "Um, New York and around the same time I remember there was a SRU workshop.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, but it was one year before or you want your after that, where there were really controversial discussions about neural networks and.",
                    "label": 0
                },
                {
                    "sent": "And conventional hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "Now the first Journal paper that I could find is a paper that was mentioned already here to the.",
                    "label": 0
                },
                {
                    "sent": "Paper that came out in 89 IEEE transactions.",
                    "label": 0
                },
                {
                    "sent": "Transactions on acoustics, speech and signal processing.",
                    "label": 0
                },
                {
                    "sent": "And yesterday I looked up the citations over about 2000 citations according to Google Scholar and there were three more papers by Alex and his team and they have a citation number of about 1100.",
                    "label": 0
                },
                {
                    "sent": "And what is interesting is there has been some recent work by then.",
                    "label": 0
                },
                {
                    "sent": "Povey and his team at which they reported at Interspeech.",
                    "label": 0
                },
                {
                    "sent": "Last year in Dresden and they could report significant improvements over the let's say conventional deep MLP approach and they confirmed this or they were able to get these improvements on many of the standard ASR tasks like Wall Street Journal Switchboard and Leprus Peach or Hyper speech.",
                    "label": 1
                },
                {
                    "sent": "I don't know how to pronounce it and then also on the Aspire Challenge which involves reverberant speech in a far field speech recognition task.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conventional deep.",
                    "label": 0
                },
                {
                    "sent": "We have to come to an end already.",
                    "label": 0
                },
                {
                    "sent": "OK as the conventional.",
                    "label": 0
                },
                {
                    "sent": "Modular perception is shown here, so you can ask the question, what is today different from what we had 1025 years ago or so.",
                    "label": 1
                },
                {
                    "sent": "So we have 10 hidden layers or more.",
                    "label": 1
                },
                {
                    "sent": "We have more output units and we have learned how to handle the optimization problem and typically what is found is that the error it can be nearly half which is a figure.",
                    "label": 0
                },
                {
                    "sent": "I think 42% were mentioned by Chris Chris Owens this morning and so that's about consistent with what many other teams report.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one important improvement can be obtained by using recurrent neural networks, specifically in combination with this long short term memory.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I would like to come to this as this aspect of.",
                    "label": 0
                },
                {
                    "sent": "Sequence labeling that was mentioned already by Florian Metzer.",
                    "label": 0
                },
                {
                    "sent": "We could try to reformulate the problem with speech recognition by saying OK, we're really only interested in a sequence of phonetic labels, and if you have these labels, say I did, they uniquely uniquely determine the sequence of words.",
                    "label": 1
                },
                {
                    "sent": "And what we need is basically a posterior distribution over these labels which can be computed by a neural network and then in addition to that we need some localization.",
                    "label": 0
                },
                {
                    "sent": "So we need some sort of.",
                    "label": 0
                },
                {
                    "sent": "Of time alignment pass or similar.",
                    "label": 0
                },
                {
                    "sent": "Concept.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And OK, I think I skip search.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same reason so we could try to apply this to a conventional HMM structure.",
                    "label": 1
                },
                {
                    "sent": "So here we have three states for each of the assembled.",
                    "label": 0
                },
                {
                    "sent": "So Alex, this was done.",
                    "label": 0
                },
                {
                    "sent": "Let's say Richard for handwriting recognition we can use the same principle for phonemes, and so this looks very much to what we had in the case of a conventional hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "But the difference is that we do not aim to have a joint model do not want to have a model over the observed acoustic vectors and see.",
                    "label": 0
                },
                {
                    "sent": "Hope is that we can get away.",
                    "label": 0
                },
                {
                    "sent": "Without the renormalization proper, which is a serious problem here, but the open issues that have to be handled is how to include transition probabilities, how to make use of the language model?",
                    "label": 1
                },
                {
                    "sent": "What could be a consistent training criterion?",
                    "label": 0
                },
                {
                    "sent": "Should we take the sum of all alignments?",
                    "label": 0
                },
                {
                    "sent": "How should we do end to end training and so on and?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a ritual CTC concept that was introduced by Graves around 2006.",
                    "label": 0
                },
                {
                    "sent": "Again, this is what Florian had just to present it.",
                    "label": 0
                },
                {
                    "sent": "So topology here is for each symbol we have just a single state at the blank state.",
                    "label": 1
                },
                {
                    "sent": "We don't have any transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "Training criterion is a sum the instructor used is typically an STM RNN, or maybe some other structure.",
                    "label": 0
                },
                {
                    "sent": "And like Florian, as our teams also report good results, but I'm not really convinced whether this is due to the specific CTC property, because this really looks like a more or less like a specific hidden Markov model, and my speculation is these good good results are reported or maybe due to the LCM.",
                    "label": 0
                },
                {
                    "sent": "So I think we need more work in order to direct comparisons and see what is really what are the possible advantages of this CTC concept?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you accept this concepts and you could also say we could really invert our time alignment properly.",
                    "label": 0
                },
                {
                    "sent": "We could try to say we just have to segment our speech signal.",
                    "label": 0
                },
                {
                    "sent": "So we need a mapping from the states.",
                    "label": 0
                },
                {
                    "sent": "To the time access and then you can reformulate.",
                    "label": 0
                },
                {
                    "sent": "You are your modeling problem.",
                    "label": 0
                },
                {
                    "sent": "That's what we're working on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My my team and it's interesting to see that this is getting close to the mechanism of attention that was originally introduced for machine translation by the Montreal team.",
                    "label": 1
                },
                {
                    "sent": "So basically you have two recurrent neural networks to recurrent structures one over the time axis and 1 / Z state access log which you form your hypothesis for recognition.",
                    "label": 0
                },
                {
                    "sent": "Um, as interesting thing is that this mechanism uses NN concepts only.",
                    "label": 0
                },
                {
                    "sent": "There's nothing of a hidden Markov model in it.",
                    "label": 0
                },
                {
                    "sent": "And the alignment direction here is also from the states to the time axis.",
                    "label": 0
                },
                {
                    "sent": "And they make use of occupation probabilities, which can have an interpretation similar to the occupation probabilities.",
                    "label": 0
                },
                {
                    "sent": "In the case of conventional hidden Markov models and.",
                    "label": 1
                },
                {
                    "sent": "Many teams are working on that.",
                    "label": 0
                },
                {
                    "sent": "This is also ongoing work in my team and we have to see which of these different architectures will have the advantage.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me try to summarize.",
                    "label": 1
                },
                {
                    "sent": "I think the experimental results.",
                    "label": 1
                },
                {
                    "sent": "Shows that there is lots of room for improvements.",
                    "label": 1
                },
                {
                    "sent": "But I think it is difficult at the moment to draw clear conclusions.",
                    "label": 0
                },
                {
                    "sent": "What causes the the improvements?",
                    "label": 0
                },
                {
                    "sent": "Is it just the STM aronin structure which is typically found to be very, very?",
                    "label": 0
                },
                {
                    "sent": "Efficient in many applications?",
                    "label": 0
                },
                {
                    "sent": "Or is it something in addition?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, the most important conclusion.",
                    "label": 0
                },
                {
                    "sent": "Most important summary is congratulations to interact and RX on 25 successful years and my best wishes for the coming 25 years.",
                    "label": 0
                }
            ]
        }
    }
}