{
    "id": "bgpa5pmnztmbka7qgh4iddxwxm4mvwbs",
    "title": "A Scalable Approach for Efficiently Generating Structured Dataset Topic Profiles",
    "info": {
        "author": [
            "Besnik Fetahu, L3S Research Center, Leibniz University of Hannover"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_fetahu_topic_profiles/",
    "segmentation": [
        [
            "My name is Bethany, come from the Interest Research Center.",
            "So what I'm going to talk in this here is about generating data set profile.",
            "So by profiles I mean structure, topic, description and topics.",
            "In our case where the pedia categories and we are mostly interested in how to do this in a scalable and efficient way considering the large number of datasets to give."
        ],
        [
            "Just a brief outline of this talk, so we're going to see like what are the common problems and data and like what what we need.",
            "What we expect.",
            "Actually, when performing this profiling approach and what is the problem behind and what are the motivation points and then go on to the approach on the several steps that we consist on this profiling pipeline and then some experimental experimental setup.",
            "The baselines datasets used and the evaluation results with respect to the.",
            "Eficiency and the scalability and some concluding remarks so."
        ],
        [
            "All."
        ],
        [
            "Generating data set profiles is quite important, having in mind that we have an ever increasing amount of web data, so this data comes from is highly heterogeneous, has different representation, comes in different languages, the quality is highly variable and the domains basically what kind of topics these datasets cover is also quite heterogeneous and another factor is that the datasets in themselves are quite sparsely connected, so if you remember the very famous.",
            "Elodie cloud picture.",
            "You have them the links concentrated in DB, pedia, Freebase and so on.",
            "So these are also some kind of problem where in the long tail of the datasets you might have relevant resources but you're not aware of.",
            "And of course there is a huge lack of descriptive metadata about these datasets which we also want to tackle in this paper and usually you have to do exhaustive techniques to analyze the data and come up with something like say.",
            "What are the domains and then some profiling?",
            "Basically a different dimensions and also the profiles themselves are heavily dependent on the information needs.",
            "So depending what you want you might also consider the word what is a profile?",
            "In our case it was mostly we want to generate.",
            "What are the domains of these datasets and also if you have something like really descriptive and structured representation of these profiles then it's easier to access the data and represent themselves.",
            "Um?"
        ],
        [
            "So.",
            "Why we?"
        ],
        [
            "Dataset profiling in the first case is that considering only the Elodie cloud, we have there right now, I think it's 227 datasets which are part of the Elodie cloud, but this number is fluctuating up and down how datasets published as doing data is is quite high, and in this Elodie cloud there is 30 billions triples and based on some manual.",
            "Analysis We found that actually there is approximately like up to 18 languages and the broad set of topics covered there is a broad set of topics covered and you can see that from a manual, let's say clustering of the datasets you have.",
            "Like this few domains which in themselves are not so useful because you end up having like say in the government domain like 13 billion triples, which is not helping at all so."
        ],
        [
            "Your problem or information need like you want to find what are the datasets that cover the domain or topic of renewable energy.",
            "So the first problem you might encounter is this sparsity.",
            "So as I said in sparsity, in this case means that how many datasets actually have information which describes what topics they cover, and this is usually some of one of the most common vocabularies is the Dublin core metadata and.",
            "Based on the manual inspection, we found out that only 38 datasets have this information.",
            "However, this is not the whole truth because some datasets have their own specialized vocabularies, but they're not very.",
            "Let's say standard, and another thing is OK if you want to consider now the other part of datasets which do not have this information, then you need to use in sparkle.",
            "You have to do some use the filter clause, so if you consider some keyword matching then.",
            "You have to use regular expression and this is highly.",
            "This is really takes a lot of time, and of course there's different implementations where, like from the vehicles you can do indexing and so on, But this is not a common standard and then also disambiguity here like renewable energy.",
            "What does it mean actually?",
            "So what are the possible forms?",
            "Is it only the exact match or is there other forms of renewable energy?",
            "So just to name a few, there solar energy, wind energy, geothermal and so on.",
            "So basically you have this quite a big problem for a very simple.",
            "Task like just to find data set for a specific domain."
        ],
        [
            "Having in mind that we proposed."
        ],
        [
            "Our approach, which consists of several steps and in this pipeline with the first step is like first we need to extract the metadata.",
            "Obviously basically the sparkle endpoints where we can access the data.",
            "So this we do from the SQL Data Hub seek an API and grab the sparkle endpoints.",
            "The second step is considering the number high number of triples we need to sample resources, so which sample gives us the best representation of the data set and so forth.",
            "And I will describe in more details like what are the sampling approaches?",
            "And then is you need to kind of give structure to these textual literals that you find in resources and extract entities and topics.",
            "Basically named entity disambiguation and then generate.",
            "Finally this profile graph where you have a bipartite graph of datasets and topics and then links between them.",
            "And then of course representing them in a useful and easy accessible way.",
            "So just to make it more concrete so."
        ],
        [
            "How does the profile look?",
            "So this is basically for a data set you have any pedia and you have some information, like how many entities you found in that data set based on the sample and what are the top 20 topics?",
            "And this is basically represented as a using VoIP vocabulary.",
            "So you present the profile as link sets.",
            "So a link set has a set of links and in our case we distinguish two types of links, so the."
        ],
        [
            "First thing is the topic link which we are mostly interested in where we have this provenance information here like from which entities the topic is generated.",
            "So I'll explain later on.",
            "But this is basically DB pedia entities and categories and then you have what is the score because you don't want not all topics or have the same relevance to a specific data set.",
            "So you have you can use different ranking approaches to rank the topics and then you have the topic itself.",
            "And then."
        ],
        [
            "Entity link, so here we have the provenance information.",
            "So from where this entity was extracted, so you have the resources and the entity itself.",
            "So basically this is a small example of how a profile looks."
        ],
        [
            "Now going into the individual steps, first we need to extract the resources and their corresponding type.",
            "So this we do by using simple sparkle, select curious and and we tested a different cut offs like we first take like 5 percent, 10% and so on and we saw the performance time of different endpoints.",
            "So for 10% on average it goes to 7 minutes.",
            "The indexing time and four 100% roughly.",
            "4 hours, which is quite interesting is that not like this is only for this small subset of data sets like the Elodie cloud, but you have cases where a data set to to extract all these resources takes up to 626 hours.",
            "This is because it just limits you after the first Sparkle request, then you get only like 610 triple.",
            "So if you have millions then this just takes too long, so this clearly shows that actually you need to come up with something, some sampling strategy which gives you a representative sample and.",
            "You limit yourself only to a few resources, so they."
        ],
        [
            "Sampling approaches which we used were three, so first is random sampling.",
            "We just pick randomly a resource and then we have the weighted sampling.",
            "So the weight we assign away to each resource by computing a sort of the fraction of how many data type properties used to describe a resource over the, let's say maximum number of data type properties that describe a resource in a data set.",
            "So by this we kind of want to see that OK, this resource has quite some content and.",
            "Should be weighted higher, so we then we kind of put in buckets these resources and then within these buckets we select randomly and the same is for the centrality sampling.",
            "In this case we do not consider all data type properties but only like how many types are used to describe a resource and then after we sample we perform we consider the resources as sort of documents by combining all their textual literals and we perform the disambiguation process where we extract.",
            "Entities from DPW spotlight but can be any other tool, and then extract the corresponding subjects from the entities.",
            "So this kind of gives finishes the first step, and then after we have this information, how do you construct these pro?"
        ],
        [
            "So first is we have these different nodes like data set resources and topics and we want to have the topic and the data set the their edge has a weight, But this topic is associated with many datasets in many cases and this edge should be weighted differently.",
            "So how do we do that?",
            "We basically use graphical models.",
            "We adapted some some approaches by White and Smith from KDD 2003, where you can give like prior knowledge to your graphical model.",
            "So you have page rank a step Markov and so on.",
            "And there you basically as prior knowledge we give like the sampled resources because we want to rank the topic with respect to what we've seen and then bias the import of the computation of this importance towards these resources.",
            "And the nice thing is that after we have let's say, the first profile, we can incrementally add additional datasets into this profile by simply recompute computing only this edges for the new data set."
        ],
        [
            "And how do we do the ranking?",
            "So first we get lots of topics because many entities are assigned have many topics or categories, but not all categories are equally important and we do some prefiltering where we compute some normalized topic relevance score, which is basically checks like how many actual entities have within a data set.",
            "So these first the lower part checks how many entities have the specific topic in a data set and how many.",
            "Entities you have in a data set and the same is like how many entities have a specific topic in all datasets and so on.",
            "So this is basically to check like how?",
            "What is the discriminative power of topic with the specific datasets?",
            "Because we want kind of to have this distinguished datasets into the different domains and then after we filter so the filtering basically is like just taking the threshold below the average score of NTR.",
            "We just drop the topics and for the rest we compute the topic ranking so.",
            "The ranking as I said, we use Pagerank with priors, hits an K step Markov.",
            "So for this you can check the paper and the computation is kind of straightforward."
        ],
        [
            "So now we have this.",
            "All these steps in the profiles and how do we kind of evaluate the, let's say, the scalability and the efficient?"
        ],
        [
            "Of our approach, we do the profiling.",
            "First of all, on the whole Elodie cloud datasets.",
            "When we did the experimentation, only 129 datasets were actually available because many of them were sending different HTTP errors and we could not access them and to have validate our approach.",
            "We use 6 ground truth datasets which have manually assigned topics for their resources, so these are the datasets and they have for different, using different vocabularies and different.",
            "Data type properties.",
            "Find the topics which are in the form of keywords at some point or actual DB pedia entities, and then we extract this and when we have keywords, we map them to entities and get the topics and then we simply rank based on the frequency like say the profile of your Vista data set and to compute this like how well the efficiency of our profiling approaches, we use the N DCG ranking metric because it's a ranking problem and this is fits perfectly and.",
            "We compute compute the induced ranking by the graphical models against the ideal ranking from the ground truth."
        ],
        [
            "As baseline now here we kind of we could not find like really.",
            "Work that kind of goes into this analysis of the actual content of datasets.",
            "However, there is like from information retrieval and so on.",
            "There is these approaches which do the topic modeling, so you have TF IDF are really naive way and simple scenario, but it's useful to consider.",
            "So what we do there is we consider the resources as documents and extract these top K terms.",
            "So we consider like 50 to 200 and we consider with more.",
            "But however it's like adding like 500 or more.",
            "This did not help, like the profiling accuracy, so from these top K terms we just map them to DPD entities and then get the profiles and the save the rank.",
            "The weight of a topic is determined by the TF IDF score.",
            "Similar is for LDA where we tried different initialization like with different topics and different topic terms.",
            "And how are the best approach was that with 20 topics and 200 terms we would get the best profiling accuracy for the baseline.",
            "So."
        ],
        [
            "How are they?"
        ],
        [
            "Results results look like so we tested for efficiency and scalability.",
            "So here what we see is when we analyze the profiling accuracy on the full set of resources.",
            "So we analyzed 100% of resources here and you see here the profiling accuracy at different ranks using and ECG.",
            "And here we have K step Markovian page rank which are obviously the best.",
            "The reason why they are quite the same even though if you zoom in like you will see that there is a very negligible difference because K step Markov for a large scale.",
            "Which is the number of nodes path length you consider becomes actually page rank and here they clearly outperformed the others, while TF, IDF and LDR are very poor.",
            "Now here on the.",
            "Here on the lower part you see like the importance of the pre filtering using the NTR because this reduces quite a lot the noise and here in the graph is shown like what is the impact when you pre filter the topics and this is shown for N DCG at rank 100 and for the different sample sizes.",
            "So here you clearly see that actually you get a quite significant improvement in your profiling accuracy now."
        ],
        [
            "Seconds, let's say validation of the profiling approach was the scalability because clearly we want to do this in an iterative manner and it has to scale.",
            "So as I shown as you can see here we have this graph which kind of plots the results with respect to the profiling accuracy on the right axis.",
            "Here and here is the time it takes to rank.",
            "Actually the topics using these graphical models so you can see here that actually with five and 10%.",
            "The ranking profiling accuracy is quite high and the ranking time is the lowest.",
            "So basically we go down from 45 minutes ranking like topics when we consider 100% of resources to two minutes and of course as I showed in the beginning, if you consider the indexing time when you want to analyze or sample these resources for 10%, she takes like 7 minutes.",
            "Contrast to the 100%, which takes 4 hours.",
            "However, for the disambiguation process, this is not really deterministic, because it depends on many factors, like how many.",
            "If you use a web service from these tools, how many requests you do at once, and and so forth.",
            "So what is the load of the web server?",
            "So we cannot.",
            "Cannot really measure this.",
            "This this value spot.",
            "I can say that it actually goes from from weeks to days so it's like really a significant improvement."
        ],
        [
            "And revisiting again the example after having these generated profiles.",
            "So how would you actually find the profiles now?",
            "Thank you.",
            "Then you would basically just.",
            "Julie, the profiles using a keyword or map map it to the actual DB pedia topic, which is in the case quite simple.",
            "And then you would basically go into these in these links in the data set that you have and then from there you can rank the datasets based on these relevant scores that we compute by rank.",
            "The graphical models.",
            "Or you could basically just go from the topics to the entities and then retrieve the actual resources.",
            "So however this is only done on the 10% sample.",
            "But this is easily.",
            "You can extrapolate to the other other 90% of resources by just using some back of words similarity metrics with the resources so that you can actually go to the other resources and retrieve domain specific instances.",
            "And this is just like a really simple sparkle code query which you can issue it, and then you will get a ranked set of data sets."
        ],
        [
            "Now to conclude, here is we generated the structured data set profiles.",
            "We had a scalable approach was maintained through sampling of resources based on the weighted and centrality sampling and we achieved an efficient profiling approach by just using graphical models like page rank and using the topic filtering and the advantages that you can add incrementally new datasets as they become available in the ELODIE cloud or any other data set of interests.",
            "And the datasets profiles are easy to access as they are a set of links and with this you have the provenance information which you can do do further reasoning and so forth and the use cases of profiles.",
            "Apart from just giving a description like what is the data set about you can do for data set recommendations, search, interlinking and so on.",
            "So these these are the resources where you can actually download or Curie the profiles for the generated.",
            "Datasets and this is the web page where you can find a bit more information about the concrete process."
        ],
        [
            "Thank you and I'll be open for your questions.",
            "F2 small questions.",
            "One is a bit of a practical one.",
            "So why did you sparkle and not just the data dumps?",
            "Because I can imagine your can you can you speak louder.",
            "So my question is why did he use Sparkle and not regular data dumps?",
            "Because you want to be as accurate as possible, so dumps in some cases are good enough, but in many cases like data set providers do not like provide these dumps, which are, let's say, update it.",
            "So we want to use the actual end point because we assume that there there is the live data of a specific data set, so that was the main reason why we kind of Curie.",
            "And yeah, and in many cases you cannot have also the dump.",
            "So let's also problematic.",
            "OK, Ann.",
            "Another question I it wasn't clear for me who who assigns the grounds truths that you were talking about.",
            "Yeah, so just to go back there.",
            "So here we have like for instance like data set or semantic web dog food.",
            "So this this topic indicators are actually so basically here you'll find papers like ESWC and so on South.",
            "These correspond to actual keywords for the papers which authors themselves assigned.",
            "So we consider.",
            "This is the, let's say, the best ground truth we could get.",
            "Of course, like these keywords are, let's say, in many cases quite generic because you cannot in the content of the paper, you have much more specialized topics, but this is the best we could get as a ground truth.",
            "So these are manually assigned by authors and like in the Vista case you have their video lectures where those who actually like let's say in courses or something like that.",
            "They can actually tag them.",
            "Video with specific topics, so these are all manually assigned.",
            "Is the software available?",
            "No, it's not, but it will it be?",
            "Yes, So what what we're currently working is we want to do this in iterative manner and kind of to create this pipeline where you can basically just.",
            "You can download it by yourself or we provide in our in our end that you can actually access the profiles and generate the profiles for the different time points where you kind of construct these profiles.",
            "Or we just simply release the code and everybody can tweak and do their extra research on it.",
            "Thank you and the second the second question I have is we have 200 or so link datasets on the open web, but there are many.",
            "David Open datasets there not available is RDF, so how would you have to change this approach to do data profiling on say CSV files or other formats?",
            "So basically what I see there, it would be the indexing part or extracting these resources.",
            "There is obviously much more than datasets that are accessible available as CSV or some other formats, and there I think it's kind of if the data is not well structured.",
            "Then might be another problem like how you actually distinguish what is different resources, but obviously what would have to change is only this first step where you basically extract the resources and the rest is is the same we chose like to represent these as links the profiles, so the other part and the ranking would be all the same, so it's only like the extraction part and actually determining what is a resource in a CSV file so.",
            "Thank you very much.",
            "Let's thank the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Bethany, come from the Interest Research Center.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to talk in this here is about generating data set profile.",
                    "label": 0
                },
                {
                    "sent": "So by profiles I mean structure, topic, description and topics.",
                    "label": 0
                },
                {
                    "sent": "In our case where the pedia categories and we are mostly interested in how to do this in a scalable and efficient way considering the large number of datasets to give.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a brief outline of this talk, so we're going to see like what are the common problems and data and like what what we need.",
                    "label": 0
                },
                {
                    "sent": "What we expect.",
                    "label": 0
                },
                {
                    "sent": "Actually, when performing this profiling approach and what is the problem behind and what are the motivation points and then go on to the approach on the several steps that we consist on this profiling pipeline and then some experimental experimental setup.",
                    "label": 0
                },
                {
                    "sent": "The baselines datasets used and the evaluation results with respect to the.",
                    "label": 0
                },
                {
                    "sent": "Eficiency and the scalability and some concluding remarks so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generating data set profiles is quite important, having in mind that we have an ever increasing amount of web data, so this data comes from is highly heterogeneous, has different representation, comes in different languages, the quality is highly variable and the domains basically what kind of topics these datasets cover is also quite heterogeneous and another factor is that the datasets in themselves are quite sparsely connected, so if you remember the very famous.",
                    "label": 1
                },
                {
                    "sent": "Elodie cloud picture.",
                    "label": 0
                },
                {
                    "sent": "You have them the links concentrated in DB, pedia, Freebase and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are also some kind of problem where in the long tail of the datasets you might have relevant resources but you're not aware of.",
                    "label": 0
                },
                {
                    "sent": "And of course there is a huge lack of descriptive metadata about these datasets which we also want to tackle in this paper and usually you have to do exhaustive techniques to analyze the data and come up with something like say.",
                    "label": 1
                },
                {
                    "sent": "What are the domains and then some profiling?",
                    "label": 1
                },
                {
                    "sent": "Basically a different dimensions and also the profiles themselves are heavily dependent on the information needs.",
                    "label": 0
                },
                {
                    "sent": "So depending what you want you might also consider the word what is a profile?",
                    "label": 0
                },
                {
                    "sent": "In our case it was mostly we want to generate.",
                    "label": 0
                },
                {
                    "sent": "What are the domains of these datasets and also if you have something like really descriptive and structured representation of these profiles then it's easier to access the data and represent themselves.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Why we?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dataset profiling in the first case is that considering only the Elodie cloud, we have there right now, I think it's 227 datasets which are part of the Elodie cloud, but this number is fluctuating up and down how datasets published as doing data is is quite high, and in this Elodie cloud there is 30 billions triples and based on some manual.",
                    "label": 0
                },
                {
                    "sent": "Analysis We found that actually there is approximately like up to 18 languages and the broad set of topics covered there is a broad set of topics covered and you can see that from a manual, let's say clustering of the datasets you have.",
                    "label": 0
                },
                {
                    "sent": "Like this few domains which in themselves are not so useful because you end up having like say in the government domain like 13 billion triples, which is not helping at all so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your problem or information need like you want to find what are the datasets that cover the domain or topic of renewable energy.",
                    "label": 1
                },
                {
                    "sent": "So the first problem you might encounter is this sparsity.",
                    "label": 0
                },
                {
                    "sent": "So as I said in sparsity, in this case means that how many datasets actually have information which describes what topics they cover, and this is usually some of one of the most common vocabularies is the Dublin core metadata and.",
                    "label": 0
                },
                {
                    "sent": "Based on the manual inspection, we found out that only 38 datasets have this information.",
                    "label": 0
                },
                {
                    "sent": "However, this is not the whole truth because some datasets have their own specialized vocabularies, but they're not very.",
                    "label": 0
                },
                {
                    "sent": "Let's say standard, and another thing is OK if you want to consider now the other part of datasets which do not have this information, then you need to use in sparkle.",
                    "label": 0
                },
                {
                    "sent": "You have to do some use the filter clause, so if you consider some keyword matching then.",
                    "label": 0
                },
                {
                    "sent": "You have to use regular expression and this is highly.",
                    "label": 0
                },
                {
                    "sent": "This is really takes a lot of time, and of course there's different implementations where, like from the vehicles you can do indexing and so on, But this is not a common standard and then also disambiguity here like renewable energy.",
                    "label": 0
                },
                {
                    "sent": "What does it mean actually?",
                    "label": 1
                },
                {
                    "sent": "So what are the possible forms?",
                    "label": 1
                },
                {
                    "sent": "Is it only the exact match or is there other forms of renewable energy?",
                    "label": 1
                },
                {
                    "sent": "So just to name a few, there solar energy, wind energy, geothermal and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically you have this quite a big problem for a very simple.",
                    "label": 0
                },
                {
                    "sent": "Task like just to find data set for a specific domain.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having in mind that we proposed.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our approach, which consists of several steps and in this pipeline with the first step is like first we need to extract the metadata.",
                    "label": 0
                },
                {
                    "sent": "Obviously basically the sparkle endpoints where we can access the data.",
                    "label": 0
                },
                {
                    "sent": "So this we do from the SQL Data Hub seek an API and grab the sparkle endpoints.",
                    "label": 0
                },
                {
                    "sent": "The second step is considering the number high number of triples we need to sample resources, so which sample gives us the best representation of the data set and so forth.",
                    "label": 0
                },
                {
                    "sent": "And I will describe in more details like what are the sampling approaches?",
                    "label": 0
                },
                {
                    "sent": "And then is you need to kind of give structure to these textual literals that you find in resources and extract entities and topics.",
                    "label": 0
                },
                {
                    "sent": "Basically named entity disambiguation and then generate.",
                    "label": 0
                },
                {
                    "sent": "Finally this profile graph where you have a bipartite graph of datasets and topics and then links between them.",
                    "label": 0
                },
                {
                    "sent": "And then of course representing them in a useful and easy accessible way.",
                    "label": 0
                },
                {
                    "sent": "So just to make it more concrete so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How does the profile look?",
                    "label": 0
                },
                {
                    "sent": "So this is basically for a data set you have any pedia and you have some information, like how many entities you found in that data set based on the sample and what are the top 20 topics?",
                    "label": 0
                },
                {
                    "sent": "And this is basically represented as a using VoIP vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So you present the profile as link sets.",
                    "label": 0
                },
                {
                    "sent": "So a link set has a set of links and in our case we distinguish two types of links, so the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing is the topic link which we are mostly interested in where we have this provenance information here like from which entities the topic is generated.",
                    "label": 0
                },
                {
                    "sent": "So I'll explain later on.",
                    "label": 0
                },
                {
                    "sent": "But this is basically DB pedia entities and categories and then you have what is the score because you don't want not all topics or have the same relevance to a specific data set.",
                    "label": 0
                },
                {
                    "sent": "So you have you can use different ranking approaches to rank the topics and then you have the topic itself.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entity link, so here we have the provenance information.",
                    "label": 0
                },
                {
                    "sent": "So from where this entity was extracted, so you have the resources and the entity itself.",
                    "label": 0
                },
                {
                    "sent": "So basically this is a small example of how a profile looks.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now going into the individual steps, first we need to extract the resources and their corresponding type.",
                    "label": 0
                },
                {
                    "sent": "So this we do by using simple sparkle, select curious and and we tested a different cut offs like we first take like 5 percent, 10% and so on and we saw the performance time of different endpoints.",
                    "label": 0
                },
                {
                    "sent": "So for 10% on average it goes to 7 minutes.",
                    "label": 0
                },
                {
                    "sent": "The indexing time and four 100% roughly.",
                    "label": 0
                },
                {
                    "sent": "4 hours, which is quite interesting is that not like this is only for this small subset of data sets like the Elodie cloud, but you have cases where a data set to to extract all these resources takes up to 626 hours.",
                    "label": 0
                },
                {
                    "sent": "This is because it just limits you after the first Sparkle request, then you get only like 610 triple.",
                    "label": 0
                },
                {
                    "sent": "So if you have millions then this just takes too long, so this clearly shows that actually you need to come up with something, some sampling strategy which gives you a representative sample and.",
                    "label": 0
                },
                {
                    "sent": "You limit yourself only to a few resources, so they.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sampling approaches which we used were three, so first is random sampling.",
                    "label": 1
                },
                {
                    "sent": "We just pick randomly a resource and then we have the weighted sampling.",
                    "label": 0
                },
                {
                    "sent": "So the weight we assign away to each resource by computing a sort of the fraction of how many data type properties used to describe a resource over the, let's say maximum number of data type properties that describe a resource in a data set.",
                    "label": 1
                },
                {
                    "sent": "So by this we kind of want to see that OK, this resource has quite some content and.",
                    "label": 0
                },
                {
                    "sent": "Should be weighted higher, so we then we kind of put in buckets these resources and then within these buckets we select randomly and the same is for the centrality sampling.",
                    "label": 0
                },
                {
                    "sent": "In this case we do not consider all data type properties but only like how many types are used to describe a resource and then after we sample we perform we consider the resources as sort of documents by combining all their textual literals and we perform the disambiguation process where we extract.",
                    "label": 1
                },
                {
                    "sent": "Entities from DPW spotlight but can be any other tool, and then extract the corresponding subjects from the entities.",
                    "label": 0
                },
                {
                    "sent": "So this kind of gives finishes the first step, and then after we have this information, how do you construct these pro?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first is we have these different nodes like data set resources and topics and we want to have the topic and the data set the their edge has a weight, But this topic is associated with many datasets in many cases and this edge should be weighted differently.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "We basically use graphical models.",
                    "label": 0
                },
                {
                    "sent": "We adapted some some approaches by White and Smith from KDD 2003, where you can give like prior knowledge to your graphical model.",
                    "label": 0
                },
                {
                    "sent": "So you have page rank a step Markov and so on.",
                    "label": 0
                },
                {
                    "sent": "And there you basically as prior knowledge we give like the sampled resources because we want to rank the topic with respect to what we've seen and then bias the import of the computation of this importance towards these resources.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that after we have let's say, the first profile, we can incrementally add additional datasets into this profile by simply recompute computing only this edges for the new data set.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how do we do the ranking?",
                    "label": 0
                },
                {
                    "sent": "So first we get lots of topics because many entities are assigned have many topics or categories, but not all categories are equally important and we do some prefiltering where we compute some normalized topic relevance score, which is basically checks like how many actual entities have within a data set.",
                    "label": 0
                },
                {
                    "sent": "So these first the lower part checks how many entities have the specific topic in a data set and how many.",
                    "label": 0
                },
                {
                    "sent": "Entities you have in a data set and the same is like how many entities have a specific topic in all datasets and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is basically to check like how?",
                    "label": 0
                },
                {
                    "sent": "What is the discriminative power of topic with the specific datasets?",
                    "label": 0
                },
                {
                    "sent": "Because we want kind of to have this distinguished datasets into the different domains and then after we filter so the filtering basically is like just taking the threshold below the average score of NTR.",
                    "label": 0
                },
                {
                    "sent": "We just drop the topics and for the rest we compute the topic ranking so.",
                    "label": 0
                },
                {
                    "sent": "The ranking as I said, we use Pagerank with priors, hits an K step Markov.",
                    "label": 0
                },
                {
                    "sent": "So for this you can check the paper and the computation is kind of straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have this.",
                    "label": 0
                },
                {
                    "sent": "All these steps in the profiles and how do we kind of evaluate the, let's say, the scalability and the efficient?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of our approach, we do the profiling.",
                    "label": 0
                },
                {
                    "sent": "First of all, on the whole Elodie cloud datasets.",
                    "label": 0
                },
                {
                    "sent": "When we did the experimentation, only 129 datasets were actually available because many of them were sending different HTTP errors and we could not access them and to have validate our approach.",
                    "label": 0
                },
                {
                    "sent": "We use 6 ground truth datasets which have manually assigned topics for their resources, so these are the datasets and they have for different, using different vocabularies and different.",
                    "label": 1
                },
                {
                    "sent": "Data type properties.",
                    "label": 0
                },
                {
                    "sent": "Find the topics which are in the form of keywords at some point or actual DB pedia entities, and then we extract this and when we have keywords, we map them to entities and get the topics and then we simply rank based on the frequency like say the profile of your Vista data set and to compute this like how well the efficiency of our profiling approaches, we use the N DCG ranking metric because it's a ranking problem and this is fits perfectly and.",
                    "label": 0
                },
                {
                    "sent": "We compute compute the induced ranking by the graphical models against the ideal ranking from the ground truth.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As baseline now here we kind of we could not find like really.",
                    "label": 0
                },
                {
                    "sent": "Work that kind of goes into this analysis of the actual content of datasets.",
                    "label": 0
                },
                {
                    "sent": "However, there is like from information retrieval and so on.",
                    "label": 0
                },
                {
                    "sent": "There is these approaches which do the topic modeling, so you have TF IDF are really naive way and simple scenario, but it's useful to consider.",
                    "label": 0
                },
                {
                    "sent": "So what we do there is we consider the resources as documents and extract these top K terms.",
                    "label": 1
                },
                {
                    "sent": "So we consider like 50 to 200 and we consider with more.",
                    "label": 0
                },
                {
                    "sent": "But however it's like adding like 500 or more.",
                    "label": 0
                },
                {
                    "sent": "This did not help, like the profiling accuracy, so from these top K terms we just map them to DPD entities and then get the profiles and the save the rank.",
                    "label": 0
                },
                {
                    "sent": "The weight of a topic is determined by the TF IDF score.",
                    "label": 1
                },
                {
                    "sent": "Similar is for LDA where we tried different initialization like with different topics and different topic terms.",
                    "label": 0
                },
                {
                    "sent": "And how are the best approach was that with 20 topics and 200 terms we would get the best profiling accuracy for the baseline.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How are they?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results results look like so we tested for efficiency and scalability.",
                    "label": 0
                },
                {
                    "sent": "So here what we see is when we analyze the profiling accuracy on the full set of resources.",
                    "label": 0
                },
                {
                    "sent": "So we analyzed 100% of resources here and you see here the profiling accuracy at different ranks using and ECG.",
                    "label": 0
                },
                {
                    "sent": "And here we have K step Markovian page rank which are obviously the best.",
                    "label": 0
                },
                {
                    "sent": "The reason why they are quite the same even though if you zoom in like you will see that there is a very negligible difference because K step Markov for a large scale.",
                    "label": 0
                },
                {
                    "sent": "Which is the number of nodes path length you consider becomes actually page rank and here they clearly outperformed the others, while TF, IDF and LDR are very poor.",
                    "label": 0
                },
                {
                    "sent": "Now here on the.",
                    "label": 0
                },
                {
                    "sent": "Here on the lower part you see like the importance of the pre filtering using the NTR because this reduces quite a lot the noise and here in the graph is shown like what is the impact when you pre filter the topics and this is shown for N DCG at rank 100 and for the different sample sizes.",
                    "label": 0
                },
                {
                    "sent": "So here you clearly see that actually you get a quite significant improvement in your profiling accuracy now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seconds, let's say validation of the profiling approach was the scalability because clearly we want to do this in an iterative manner and it has to scale.",
                    "label": 0
                },
                {
                    "sent": "So as I shown as you can see here we have this graph which kind of plots the results with respect to the profiling accuracy on the right axis.",
                    "label": 0
                },
                {
                    "sent": "Here and here is the time it takes to rank.",
                    "label": 0
                },
                {
                    "sent": "Actually the topics using these graphical models so you can see here that actually with five and 10%.",
                    "label": 0
                },
                {
                    "sent": "The ranking profiling accuracy is quite high and the ranking time is the lowest.",
                    "label": 0
                },
                {
                    "sent": "So basically we go down from 45 minutes ranking like topics when we consider 100% of resources to two minutes and of course as I showed in the beginning, if you consider the indexing time when you want to analyze or sample these resources for 10%, she takes like 7 minutes.",
                    "label": 0
                },
                {
                    "sent": "Contrast to the 100%, which takes 4 hours.",
                    "label": 0
                },
                {
                    "sent": "However, for the disambiguation process, this is not really deterministic, because it depends on many factors, like how many.",
                    "label": 0
                },
                {
                    "sent": "If you use a web service from these tools, how many requests you do at once, and and so forth.",
                    "label": 0
                },
                {
                    "sent": "So what is the load of the web server?",
                    "label": 0
                },
                {
                    "sent": "So we cannot.",
                    "label": 0
                },
                {
                    "sent": "Cannot really measure this.",
                    "label": 0
                },
                {
                    "sent": "This this value spot.",
                    "label": 0
                },
                {
                    "sent": "I can say that it actually goes from from weeks to days so it's like really a significant improvement.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And revisiting again the example after having these generated profiles.",
                    "label": 0
                },
                {
                    "sent": "So how would you actually find the profiles now?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Then you would basically just.",
                    "label": 0
                },
                {
                    "sent": "Julie, the profiles using a keyword or map map it to the actual DB pedia topic, which is in the case quite simple.",
                    "label": 0
                },
                {
                    "sent": "And then you would basically go into these in these links in the data set that you have and then from there you can rank the datasets based on these relevant scores that we compute by rank.",
                    "label": 0
                },
                {
                    "sent": "The graphical models.",
                    "label": 0
                },
                {
                    "sent": "Or you could basically just go from the topics to the entities and then retrieve the actual resources.",
                    "label": 0
                },
                {
                    "sent": "So however this is only done on the 10% sample.",
                    "label": 0
                },
                {
                    "sent": "But this is easily.",
                    "label": 0
                },
                {
                    "sent": "You can extrapolate to the other other 90% of resources by just using some back of words similarity metrics with the resources so that you can actually go to the other resources and retrieve domain specific instances.",
                    "label": 0
                },
                {
                    "sent": "And this is just like a really simple sparkle code query which you can issue it, and then you will get a ranked set of data sets.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now to conclude, here is we generated the structured data set profiles.",
                    "label": 0
                },
                {
                    "sent": "We had a scalable approach was maintained through sampling of resources based on the weighted and centrality sampling and we achieved an efficient profiling approach by just using graphical models like page rank and using the topic filtering and the advantages that you can add incrementally new datasets as they become available in the ELODIE cloud or any other data set of interests.",
                    "label": 1
                },
                {
                    "sent": "And the datasets profiles are easy to access as they are a set of links and with this you have the provenance information which you can do do further reasoning and so forth and the use cases of profiles.",
                    "label": 1
                },
                {
                    "sent": "Apart from just giving a description like what is the data set about you can do for data set recommendations, search, interlinking and so on.",
                    "label": 0
                },
                {
                    "sent": "So these these are the resources where you can actually download or Curie the profiles for the generated.",
                    "label": 0
                },
                {
                    "sent": "Datasets and this is the web page where you can find a bit more information about the concrete process.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and I'll be open for your questions.",
                    "label": 0
                },
                {
                    "sent": "F2 small questions.",
                    "label": 0
                },
                {
                    "sent": "One is a bit of a practical one.",
                    "label": 0
                },
                {
                    "sent": "So why did you sparkle and not just the data dumps?",
                    "label": 0
                },
                {
                    "sent": "Because I can imagine your can you can you speak louder.",
                    "label": 0
                },
                {
                    "sent": "So my question is why did he use Sparkle and not regular data dumps?",
                    "label": 0
                },
                {
                    "sent": "Because you want to be as accurate as possible, so dumps in some cases are good enough, but in many cases like data set providers do not like provide these dumps, which are, let's say, update it.",
                    "label": 0
                },
                {
                    "sent": "So we want to use the actual end point because we assume that there there is the live data of a specific data set, so that was the main reason why we kind of Curie.",
                    "label": 0
                },
                {
                    "sent": "And yeah, and in many cases you cannot have also the dump.",
                    "label": 0
                },
                {
                    "sent": "So let's also problematic.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                },
                {
                    "sent": "Another question I it wasn't clear for me who who assigns the grounds truths that you were talking about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so just to go back there.",
                    "label": 0
                },
                {
                    "sent": "So here we have like for instance like data set or semantic web dog food.",
                    "label": 0
                },
                {
                    "sent": "So this this topic indicators are actually so basically here you'll find papers like ESWC and so on South.",
                    "label": 0
                },
                {
                    "sent": "These correspond to actual keywords for the papers which authors themselves assigned.",
                    "label": 0
                },
                {
                    "sent": "So we consider.",
                    "label": 0
                },
                {
                    "sent": "This is the, let's say, the best ground truth we could get.",
                    "label": 0
                },
                {
                    "sent": "Of course, like these keywords are, let's say, in many cases quite generic because you cannot in the content of the paper, you have much more specialized topics, but this is the best we could get as a ground truth.",
                    "label": 0
                },
                {
                    "sent": "So these are manually assigned by authors and like in the Vista case you have their video lectures where those who actually like let's say in courses or something like that.",
                    "label": 0
                },
                {
                    "sent": "They can actually tag them.",
                    "label": 0
                },
                {
                    "sent": "Video with specific topics, so these are all manually assigned.",
                    "label": 0
                },
                {
                    "sent": "Is the software available?",
                    "label": 0
                },
                {
                    "sent": "No, it's not, but it will it be?",
                    "label": 0
                },
                {
                    "sent": "Yes, So what what we're currently working is we want to do this in iterative manner and kind of to create this pipeline where you can basically just.",
                    "label": 0
                },
                {
                    "sent": "You can download it by yourself or we provide in our in our end that you can actually access the profiles and generate the profiles for the different time points where you kind of construct these profiles.",
                    "label": 0
                },
                {
                    "sent": "Or we just simply release the code and everybody can tweak and do their extra research on it.",
                    "label": 0
                },
                {
                    "sent": "Thank you and the second the second question I have is we have 200 or so link datasets on the open web, but there are many.",
                    "label": 0
                },
                {
                    "sent": "David Open datasets there not available is RDF, so how would you have to change this approach to do data profiling on say CSV files or other formats?",
                    "label": 0
                },
                {
                    "sent": "So basically what I see there, it would be the indexing part or extracting these resources.",
                    "label": 0
                },
                {
                    "sent": "There is obviously much more than datasets that are accessible available as CSV or some other formats, and there I think it's kind of if the data is not well structured.",
                    "label": 0
                },
                {
                    "sent": "Then might be another problem like how you actually distinguish what is different resources, but obviously what would have to change is only this first step where you basically extract the resources and the rest is is the same we chose like to represent these as links the profiles, so the other part and the ranking would be all the same, so it's only like the extraction part and actually determining what is a resource in a CSV file so.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker.",
                    "label": 0
                }
            ]
        }
    }
}