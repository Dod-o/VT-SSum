{
    "id": "6ogfll5bzra5fdyc4qgrkmsfpnmjlci5",
    "title": "Effective Multi-Label Active Learning for Text Classification",
    "info": {
        "author": [
            "Bishan Yang, Peking University"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_yang_emlaltc/",
    "segmentation": [
        [
            "And here's the outline.",
            "I will start with the motivation about why multi."
        ],
        [
            "Active learning and discuss the limitation of the related work and then I will introduce our approach as being based active learning for multi label text classification.",
            "After that are experimenting."
        ],
        [
            "Summary and with the with the booming web applications, tax resources becomes more and more important in our daily life and text classification problems are everywhere in applications like web search, news classification, email classification and etc.",
            "Um, many concerns are put on multiple text classification because many text data are multi label.",
            "Let's take and use data for example, and these are the news from msnbc.com and this other categories setting.",
            "For you, our readers convenience and usually are news, can belong to more than one category.",
            "For example the news about China, US to resume high level talks can belong to category, business, politics and all."
        ],
        [
            "So World News.",
            "And concerns that the labeling effort on multiple data is is quite huge in traditional supervised learning approach.",
            "The model is trained on a set of random label data and it requires a sufficient amount of labeled data to ensure the quality of the model.",
            "That means an experts needs to go through a large number of text data and check for each predefined category to see whether the document belongs to it.",
            "And obviously, the more categories, the more judging effort for each document and more data needed to be labeled.",
            "And for the widely used routers corpora's there are more than 100 categories, so you can imagine how expensive the human labeling effort is.",
            "So."
        ],
        [
            "The problem is how to reduce the labeling effort.",
            "Active learning is the solution, and let's look at the process of active learning in the data pulled.",
            "The red blocks represents the label data and blue blocks represent the unlabeled data and at the beginning there is a very limited amount of labeled data in the labor pool and classifieds training.",
            "Based on this on this label data and based on this classifier, the active learner will select an optimal set of unlabelled data according to a selection strategy.",
            "And then queries for is true labels.",
            "And after the after the selected sample request, its label and it will be added to the current label sets and this process will runs iteratively until the learner achieves a sufficient accuracy.",
            "And the flaxen strategy is the core of active learning, which aims to select the most informative data to label.",
            "So with an effective selection strategy, the active learner can obtain incomparable accuracy with supervised learner use using much less label data.",
            "So it is important to consider active learning or multiple text classification to reduce the human labeling at."
        ],
        [
            "Prince and the challenges for multiple active learning is how to select the most informative multi label data.",
            "How many active learning results on single label classification?",
            "So can we.",
            "You just use the flexion strategies in multiple problem and the answer is no.",
            "Let's look at this example.",
            "Suppose we are giving the classification probabilities on each category for data X1 and X2.",
            "And the active learner aims to select the most uncertain data to label and in the single label case the measure of uncertainty will focus on just one category, usually the one with the largest classification probability, and in this case is clicked category C1 for data X1 and X2.",
            "So in this case X2 maybe more informative than maybe maybe more uncertain than X1.",
            "So it is more informative to label.",
            "But in multi label case the Data Explorer is restricted to have only one categories.",
            "So if X1 actually has two cats, two labels X C1 and C2 Annex two has one label C1, then labeling X one may be more informative, may contribute more information to the learner than data X2.",
            "So just considering one optimize on one category.",
            "Selection strategy is not enough for multi label active learning.",
            "And for the related work single label active."
        ],
        [
            "Learning has been well researched and they can be classified into three types according to the selection strategy they use an uncertainty sampling strategy aims to label the most uncertain data an expected error reduction strategy aims to label data to minimize expected error of the learner and the comedy based approach labels data which has the largest disagreement among several committee members.",
            "Also, we can score classifiers from the version space.",
            "For multiple active learning, there is very limited research.",
            "The beaming approach, designed for text classification, aims to minimize the loss on the most uncertain category for each data, and it doesn't consider multiple information in the in its selection strategy and the second approach, Merle designs for image classification and optimize the mean of the SVM hinge loss for the predicted predicted classes and.",
            "This matter is not a very effective in on our tax data.",
            "In our experiments I will talk about it later in the experiment part and the two dimensional active learning approach.",
            "Minimize the classification error on picture label pairs and it is it is not appropriate to decomposed multi label data into several document labeled pairs because it will introduce additional cost to for human tool label document again again and again.",
            "So we proposed approach SVM based active learning for multi label text classifica."
        ],
        [
            "Ocean, and we consider SVM here becauses SVM has gained significant success on tax classification task and the optimal optimization goal of the selection strategy here is to maximize the reduction of the expected model loss and hear the.",
            "The red box represents the label, the label data and the blue blocks represent the unlabeled pool, and at each active activity cycle, the learner will evaluate on each possible set of unlabeled data DS and see if it is labeled an added to the current level set.",
            "What will be the last reduction?",
            "And here, after FDL represents the model build on the original label data and FDL apostrophe represents the model building built on the new label data, and LD represents the loss function and the conditional probability WHI is the label set of data X and it is represented as K dimensional vectors.",
            "Supposed error K classes Ann.",
            "If data X belongs to category J than wide range equals 1.",
            "Otherwise I = -- 1.",
            "And after some of approximation we can come to our optimization goal here which which aims to maximize the expected loss model, last reduction on the selected some."
        ],
        [
            "Both.",
            "So there are two main issues in the optimization.",
            "One is how to measure the last reduction of the multi label classifier and the 2nd is how to provide a good probability estimation for the conditional."
        ],
        [
            "Mobility, let's first look at the estimation of last reduction, and we decompose the multi level problem into several binary classifiers, and for each binary classifier the model loss is measured by the size of the version space, which is the set of hypothesis that correctly separated data and the aspirin as version space is defined as as follows.",
            "According to symptoms paper and W. Here is the parimeter space, the size of a version space is defined as the surface area.",
            "Of the hyper sphere with."
        ],
        [
            "Just one in W. Anne, with the version space duality, the last reduction rate can be approximated by using the SVM output margins and here after after adding and you label data XY I and this is the last reduction rate on Class I and this is the reduction rate of the size of version space and this is the the approximation using SVN output.",
            "And we maximize the sum of the last reduction rate for all binary classifier and.",
            "And this this optimization can also be explained in uncertainty POV.",
            "If we consider classifier F correctly predict data X, then as the absolute value of FX decreases, the degree of uncertainty increases.",
            "And if F does not correctly predict X, then the absolute value increases the degree of uncertainty increases and this is this is consistent with our version space explanation here."
        ],
        [
            "And X is the probability estimation since it is intractable to directly compute the expected loss function, because there is very limited like training data an it is hard to provide conditional probabilities for for each possible label vector for each data X.",
            "So we approximated by the loss function with the largest conditional probability.",
            "So we can rewrite our optimization goal here.",
            "White Hat is the label vector with the largest conditional probability."
        ],
        [
            "So the problem becomes how to predict yhat and the main idea is is to 1st break.",
            "First build a classification model to predict the possible naval number label number.",
            "Each data may have.",
            "I had based on the predict."
        ],
        [
            "And result, and let's look at process.",
            "First we assign probability output for each class using the parametric model implies paper to feed the probability and this steps aims to make the prediction on each class compatible because the SVM output is uncalibrated and then we for each data X we saw the probabilities probabilities in decreasing order, an nominal normalized to make their sum equals one and then we train logistic.",
            "Regression model where features are with the transform values as features and true label, number of access label and then for each label unlabeled data, we predict the probabilities of having different number of labels.",
            "And if the number if the label number with the largest probability is J, then we can obtain the predicted label vector with component.",
            "I want to IJ equals one and undress."
        ],
        [
            "Equals minus one.",
            "And for the experiments we used to ask me one with two datasets, which contains Reuters news stories and it is widely used benchmark on text classification task and we also use six datasets from Yahoo's Web page collections which are contained which contains hyperlinks from Yahoo's top Directory.",
            "It is also used to evaluate multiple text classification tasks and these are the details of the datasets."
        ],
        [
            "And there are four comparing methods.",
            "The first one is our proposed method MMC and the second one being mean is mentioned in the related work which minimize the smallest SVM and margin.",
            "And the problem is it doesn't does not consider multiple information here and the MML approach also mentioned in the related work that optimized the mean SVM hinge loss.",
            "And since the label prediction part is not very effective on our tax data, we will replace the label prediction part of this method with our proposed method and focus on evaluating effectiveness of its last optimization and the 4th is random approach that randomly select data to label and SBM life is used as the basis based classifier in the active learning and we use micro average F1 score.",
            "As performance measures an we we run the each experiment 10.",
            "Our time trials and get the average score."
        ],
        [
            "And these are the results on RSV One V2 datasets and we first evaluate our label prediction methods and compare with two widely use prediction methods.",
            "The one is God in Lewis paper, which till the threshold for each class and determine the predicted labels by the threshold and Scott with threshold zero, just pick the positive that the just picked the classes were positive as being output.",
            "As the predicted labels.",
            "And we veriest training examples from 100 to 1000 an evaluate our performance on the unlabeled pool, and we can see our proposed method is provides provides better performance than the baseline methods, especially the training example is very limited, which is typical in active learning."
        ],
        [
            "And this this experiments are evaluated for active learning methods.",
            "Without we randomly select 500 examples to form the initial label set and we run 50 active learning iterations an at each.",
            "At each iteration, the sampling size is set to 20.",
            "And this this is the active learning curves.",
            "We can see that the performance of our proposed method grows much faster than the baseline methods, and we can see that MLM beaming method only provide flight.",
            "OK, only provides slight improvement over random method.",
            "This is because women does not consider multiple information in their optimization an approach.",
            "Since this them aproach adopt sustainable prediction method as our approach, so it indicates that the last optimization of its of ML is not effective on the on the text data and this table summarized performance with different training training data added.",
            "We can see that as the training examples growth, our improvement of our algorithm becomes more significant."
        ],
        [
            "And we also vary the size of initial label sets to evaluate for active learning methods.",
            "We run US50 active learning iterations.",
            "The performance is valuated.",
            "After 1010 ten 1000 examples added.",
            "And then we can see that our method consistently improves over the baseline methods with various size of initial."
        ],
        [
            "Full set.",
            "And we also varied sampling size program to see the impact on our proposed method.",
            "And here the initial label set is also 500 examples and.",
            "And this is the active learning curve with different sides of of sampling data.",
            "And we can see that as the sampling size decreases, the performance increases, but the difference is not."
        ],
        [
            "So obvious.",
            "And these are the results on Yahoo datasets.",
            "This table summarizes the performance after 2500 training samples added on on the six datasets.",
            "We can see that our proposed method provides consistently provide better performance than the baseline methods on an R6 datasets and this other activity curves on the arts datasets and computer.",
            "The computer stated that for Morris."
        ],
        [
            "But you can refer to the paper.",
            "And so to sum up, this paper is far is about multiple active learning for tax classification, and it is important to consider active learning on multiple text classification.",
            "And it is also a challenging task and our we propose the next game based multi level active learning approach which aims to optimize the loss reduction rate based on SVN version space.",
            "And we also proposed an effective label prediction method.",
            "We from the experiments on several real world datasets we consider our.",
            "Proposed approach game significantly concerning successfully reduced labeling efforts and for future work will pay more attention on a more efficient evaluation on their labor pool and more multi label classification tasks such as image."
        ],
        [
            "Classification.",
            "And that's all, thank you.",
            "Well, since we're still behind us, we started 10 minutes late and will when this talk a little bit early next.",
            "If you have any questions for speakers for some dental problems you after break so you can answer questions that so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the outline.",
                    "label": 0
                },
                {
                    "sent": "I will start with the motivation about why multi.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active learning and discuss the limitation of the related work and then I will introduce our approach as being based active learning for multi label text classification.",
                    "label": 0
                },
                {
                    "sent": "After that are experimenting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary and with the with the booming web applications, tax resources becomes more and more important in our daily life and text classification problems are everywhere in applications like web search, news classification, email classification and etc.",
                    "label": 1
                },
                {
                    "sent": "Um, many concerns are put on multiple text classification because many text data are multi label.",
                    "label": 1
                },
                {
                    "sent": "Let's take and use data for example, and these are the news from msnbc.com and this other categories setting.",
                    "label": 0
                },
                {
                    "sent": "For you, our readers convenience and usually are news, can belong to more than one category.",
                    "label": 0
                },
                {
                    "sent": "For example the news about China, US to resume high level talks can belong to category, business, politics and all.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So World News.",
                    "label": 0
                },
                {
                    "sent": "And concerns that the labeling effort on multiple data is is quite huge in traditional supervised learning approach.",
                    "label": 0
                },
                {
                    "sent": "The model is trained on a set of random label data and it requires a sufficient amount of labeled data to ensure the quality of the model.",
                    "label": 1
                },
                {
                    "sent": "That means an experts needs to go through a large number of text data and check for each predefined category to see whether the document belongs to it.",
                    "label": 1
                },
                {
                    "sent": "And obviously, the more categories, the more judging effort for each document and more data needed to be labeled.",
                    "label": 0
                },
                {
                    "sent": "And for the widely used routers corpora's there are more than 100 categories, so you can imagine how expensive the human labeling effort is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is how to reduce the labeling effort.",
                    "label": 0
                },
                {
                    "sent": "Active learning is the solution, and let's look at the process of active learning in the data pulled.",
                    "label": 0
                },
                {
                    "sent": "The red blocks represents the label data and blue blocks represent the unlabeled data and at the beginning there is a very limited amount of labeled data in the labor pool and classifieds training.",
                    "label": 0
                },
                {
                    "sent": "Based on this on this label data and based on this classifier, the active learner will select an optimal set of unlabelled data according to a selection strategy.",
                    "label": 0
                },
                {
                    "sent": "And then queries for is true labels.",
                    "label": 0
                },
                {
                    "sent": "And after the after the selected sample request, its label and it will be added to the current label sets and this process will runs iteratively until the learner achieves a sufficient accuracy.",
                    "label": 0
                },
                {
                    "sent": "And the flaxen strategy is the core of active learning, which aims to select the most informative data to label.",
                    "label": 0
                },
                {
                    "sent": "So with an effective selection strategy, the active learner can obtain incomparable accuracy with supervised learner use using much less label data.",
                    "label": 1
                },
                {
                    "sent": "So it is important to consider active learning or multiple text classification to reduce the human labeling at.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prince and the challenges for multiple active learning is how to select the most informative multi label data.",
                    "label": 1
                },
                {
                    "sent": "How many active learning results on single label classification?",
                    "label": 0
                },
                {
                    "sent": "So can we.",
                    "label": 0
                },
                {
                    "sent": "You just use the flexion strategies in multiple problem and the answer is no.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we are giving the classification probabilities on each category for data X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "And the active learner aims to select the most uncertain data to label and in the single label case the measure of uncertainty will focus on just one category, usually the one with the largest classification probability, and in this case is clicked category C1 for data X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "So in this case X2 maybe more informative than maybe maybe more uncertain than X1.",
                    "label": 1
                },
                {
                    "sent": "So it is more informative to label.",
                    "label": 0
                },
                {
                    "sent": "But in multi label case the Data Explorer is restricted to have only one categories.",
                    "label": 0
                },
                {
                    "sent": "So if X1 actually has two cats, two labels X C1 and C2 Annex two has one label C1, then labeling X one may be more informative, may contribute more information to the learner than data X2.",
                    "label": 0
                },
                {
                    "sent": "So just considering one optimize on one category.",
                    "label": 0
                },
                {
                    "sent": "Selection strategy is not enough for multi label active learning.",
                    "label": 0
                },
                {
                    "sent": "And for the related work single label active.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning has been well researched and they can be classified into three types according to the selection strategy they use an uncertainty sampling strategy aims to label the most uncertain data an expected error reduction strategy aims to label data to minimize expected error of the learner and the comedy based approach labels data which has the largest disagreement among several committee members.",
                    "label": 1
                },
                {
                    "sent": "Also, we can score classifiers from the version space.",
                    "label": 0
                },
                {
                    "sent": "For multiple active learning, there is very limited research.",
                    "label": 0
                },
                {
                    "sent": "The beaming approach, designed for text classification, aims to minimize the loss on the most uncertain category for each data, and it doesn't consider multiple information in the in its selection strategy and the second approach, Merle designs for image classification and optimize the mean of the SVM hinge loss for the predicted predicted classes and.",
                    "label": 1
                },
                {
                    "sent": "This matter is not a very effective in on our tax data.",
                    "label": 0
                },
                {
                    "sent": "In our experiments I will talk about it later in the experiment part and the two dimensional active learning approach.",
                    "label": 0
                },
                {
                    "sent": "Minimize the classification error on picture label pairs and it is it is not appropriate to decomposed multi label data into several document labeled pairs because it will introduce additional cost to for human tool label document again again and again.",
                    "label": 0
                },
                {
                    "sent": "So we proposed approach SVM based active learning for multi label text classifica.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean, and we consider SVM here becauses SVM has gained significant success on tax classification task and the optimal optimization goal of the selection strategy here is to maximize the reduction of the expected model loss and hear the.",
                    "label": 1
                },
                {
                    "sent": "The red box represents the label, the label data and the blue blocks represent the unlabeled pool, and at each active activity cycle, the learner will evaluate on each possible set of unlabeled data DS and see if it is labeled an added to the current level set.",
                    "label": 0
                },
                {
                    "sent": "What will be the last reduction?",
                    "label": 0
                },
                {
                    "sent": "And here, after FDL represents the model build on the original label data and FDL apostrophe represents the model building built on the new label data, and LD represents the loss function and the conditional probability WHI is the label set of data X and it is represented as K dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "Supposed error K classes Ann.",
                    "label": 1
                },
                {
                    "sent": "If data X belongs to category J than wide range equals 1.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I = -- 1.",
                    "label": 0
                },
                {
                    "sent": "And after some of approximation we can come to our optimization goal here which which aims to maximize the expected loss model, last reduction on the selected some.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "So there are two main issues in the optimization.",
                    "label": 1
                },
                {
                    "sent": "One is how to measure the last reduction of the multi label classifier and the 2nd is how to provide a good probability estimation for the conditional.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mobility, let's first look at the estimation of last reduction, and we decompose the multi level problem into several binary classifiers, and for each binary classifier the model loss is measured by the size of the version space, which is the set of hypothesis that correctly separated data and the aspirin as version space is defined as as follows.",
                    "label": 1
                },
                {
                    "sent": "According to symptoms paper and W. Here is the parimeter space, the size of a version space is defined as the surface area.",
                    "label": 0
                },
                {
                    "sent": "Of the hyper sphere with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just one in W. Anne, with the version space duality, the last reduction rate can be approximated by using the SVM output margins and here after after adding and you label data XY I and this is the last reduction rate on Class I and this is the reduction rate of the size of version space and this is the the approximation using SVN output.",
                    "label": 1
                },
                {
                    "sent": "And we maximize the sum of the last reduction rate for all binary classifier and.",
                    "label": 0
                },
                {
                    "sent": "And this this optimization can also be explained in uncertainty POV.",
                    "label": 1
                },
                {
                    "sent": "If we consider classifier F correctly predict data X, then as the absolute value of FX decreases, the degree of uncertainty increases.",
                    "label": 0
                },
                {
                    "sent": "And if F does not correctly predict X, then the absolute value increases the degree of uncertainty increases and this is this is consistent with our version space explanation here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And X is the probability estimation since it is intractable to directly compute the expected loss function, because there is very limited like training data an it is hard to provide conditional probabilities for for each possible label vector for each data X.",
                    "label": 1
                },
                {
                    "sent": "So we approximated by the loss function with the largest conditional probability.",
                    "label": 1
                },
                {
                    "sent": "So we can rewrite our optimization goal here.",
                    "label": 0
                },
                {
                    "sent": "White Hat is the label vector with the largest conditional probability.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem becomes how to predict yhat and the main idea is is to 1st break.",
                    "label": 0
                },
                {
                    "sent": "First build a classification model to predict the possible naval number label number.",
                    "label": 1
                },
                {
                    "sent": "Each data may have.",
                    "label": 0
                },
                {
                    "sent": "I had based on the predict.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And result, and let's look at process.",
                    "label": 0
                },
                {
                    "sent": "First we assign probability output for each class using the parametric model implies paper to feed the probability and this steps aims to make the prediction on each class compatible because the SVM output is uncalibrated and then we for each data X we saw the probabilities probabilities in decreasing order, an nominal normalized to make their sum equals one and then we train logistic.",
                    "label": 1
                },
                {
                    "sent": "Regression model where features are with the transform values as features and true label, number of access label and then for each label unlabeled data, we predict the probabilities of having different number of labels.",
                    "label": 1
                },
                {
                    "sent": "And if the number if the label number with the largest probability is J, then we can obtain the predicted label vector with component.",
                    "label": 0
                },
                {
                    "sent": "I want to IJ equals one and undress.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Equals minus one.",
                    "label": 0
                },
                {
                    "sent": "And for the experiments we used to ask me one with two datasets, which contains Reuters news stories and it is widely used benchmark on text classification task and we also use six datasets from Yahoo's Web page collections which are contained which contains hyperlinks from Yahoo's top Directory.",
                    "label": 1
                },
                {
                    "sent": "It is also used to evaluate multiple text classification tasks and these are the details of the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are four comparing methods.",
                    "label": 0
                },
                {
                    "sent": "The first one is our proposed method MMC and the second one being mean is mentioned in the related work which minimize the smallest SVM and margin.",
                    "label": 0
                },
                {
                    "sent": "And the problem is it doesn't does not consider multiple information here and the MML approach also mentioned in the related work that optimized the mean SVM hinge loss.",
                    "label": 0
                },
                {
                    "sent": "And since the label prediction part is not very effective on our tax data, we will replace the label prediction part of this method with our proposed method and focus on evaluating effectiveness of its last optimization and the 4th is random approach that randomly select data to label and SBM life is used as the basis based classifier in the active learning and we use micro average F1 score.",
                    "label": 1
                },
                {
                    "sent": "As performance measures an we we run the each experiment 10.",
                    "label": 0
                },
                {
                    "sent": "Our time trials and get the average score.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are the results on RSV One V2 datasets and we first evaluate our label prediction methods and compare with two widely use prediction methods.",
                    "label": 1
                },
                {
                    "sent": "The one is God in Lewis paper, which till the threshold for each class and determine the predicted labels by the threshold and Scott with threshold zero, just pick the positive that the just picked the classes were positive as being output.",
                    "label": 0
                },
                {
                    "sent": "As the predicted labels.",
                    "label": 0
                },
                {
                    "sent": "And we veriest training examples from 100 to 1000 an evaluate our performance on the unlabeled pool, and we can see our proposed method is provides provides better performance than the baseline methods, especially the training example is very limited, which is typical in active learning.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this this experiments are evaluated for active learning methods.",
                    "label": 0
                },
                {
                    "sent": "Without we randomly select 500 examples to form the initial label set and we run 50 active learning iterations an at each.",
                    "label": 1
                },
                {
                    "sent": "At each iteration, the sampling size is set to 20.",
                    "label": 0
                },
                {
                    "sent": "And this this is the active learning curves.",
                    "label": 0
                },
                {
                    "sent": "We can see that the performance of our proposed method grows much faster than the baseline methods, and we can see that MLM beaming method only provide flight.",
                    "label": 0
                },
                {
                    "sent": "OK, only provides slight improvement over random method.",
                    "label": 0
                },
                {
                    "sent": "This is because women does not consider multiple information in their optimization an approach.",
                    "label": 0
                },
                {
                    "sent": "Since this them aproach adopt sustainable prediction method as our approach, so it indicates that the last optimization of its of ML is not effective on the on the text data and this table summarized performance with different training training data added.",
                    "label": 0
                },
                {
                    "sent": "We can see that as the training examples growth, our improvement of our algorithm becomes more significant.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also vary the size of initial label sets to evaluate for active learning methods.",
                    "label": 1
                },
                {
                    "sent": "We run US50 active learning iterations.",
                    "label": 0
                },
                {
                    "sent": "The performance is valuated.",
                    "label": 0
                },
                {
                    "sent": "After 1010 ten 1000 examples added.",
                    "label": 0
                },
                {
                    "sent": "And then we can see that our method consistently improves over the baseline methods with various size of initial.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full set.",
                    "label": 0
                },
                {
                    "sent": "And we also varied sampling size program to see the impact on our proposed method.",
                    "label": 0
                },
                {
                    "sent": "And here the initial label set is also 500 examples and.",
                    "label": 1
                },
                {
                    "sent": "And this is the active learning curve with different sides of of sampling data.",
                    "label": 0
                },
                {
                    "sent": "And we can see that as the sampling size decreases, the performance increases, but the difference is not.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So obvious.",
                    "label": 0
                },
                {
                    "sent": "And these are the results on Yahoo datasets.",
                    "label": 1
                },
                {
                    "sent": "This table summarizes the performance after 2500 training samples added on on the six datasets.",
                    "label": 0
                },
                {
                    "sent": "We can see that our proposed method provides consistently provide better performance than the baseline methods on an R6 datasets and this other activity curves on the arts datasets and computer.",
                    "label": 0
                },
                {
                    "sent": "The computer stated that for Morris.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But you can refer to the paper.",
                    "label": 0
                },
                {
                    "sent": "And so to sum up, this paper is far is about multiple active learning for tax classification, and it is important to consider active learning on multiple text classification.",
                    "label": 1
                },
                {
                    "sent": "And it is also a challenging task and our we propose the next game based multi level active learning approach which aims to optimize the loss reduction rate based on SVN version space.",
                    "label": 1
                },
                {
                    "sent": "And we also proposed an effective label prediction method.",
                    "label": 1
                },
                {
                    "sent": "We from the experiments on several real world datasets we consider our.",
                    "label": 0
                },
                {
                    "sent": "Proposed approach game significantly concerning successfully reduced labeling efforts and for future work will pay more attention on a more efficient evaluation on their labor pool and more multi label classification tasks such as image.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification.",
                    "label": 0
                },
                {
                    "sent": "And that's all, thank you.",
                    "label": 0
                },
                {
                    "sent": "Well, since we're still behind us, we started 10 minutes late and will when this talk a little bit early next.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions for speakers for some dental problems you after break so you can answer questions that so thank you.",
                    "label": 0
                }
            ]
        }
    }
}