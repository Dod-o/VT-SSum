{
    "id": "b3nxzh4jojvmntv2ayqr7gpkgbgyzlkk",
    "title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets",
    "info": {
        "author": [
            "Nicolas Le Roux, Criteo"
        ],
        "published": "Jan. 22, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization",
            "Top->Computer Science->Optimization Methods->Convex Optimization"
        ]
    },
    "url": "http://videolectures.net/nips2012_le_roux_gradient_method/",
    "segmentation": [
        [
            "I was a bit stressed this morning, so several people told me that it didn't make any difference whether we talk in front of 40 people or 1000.",
            "Well, they lied.",
            "So I'm going to present the paper I worked on with my 2 great coauthors Mark Smith and Frances back.",
            "This is a great pleasure to collaborate with them, so the paper is stochastic gradient method with an exponential convergence rate for finite training sets.",
            "So don't be afraid by the length of the title, it's the result of Marx theory that the more words you put in the title, the more likely you are to have adequate reviewers.",
            "So here I am, so it seems to have worked so you know what you do for next year.",
            "So the basis for this work is we're going to be interested in two in.",
            "Anyway.",
            "We're going to be interested."
        ],
        [
            "In large scale problems, so large and large P when N is the number of observations you have NPS the number of parameters in your model.",
            "So if you have linear model, P is going to be the dimension of your observations.",
            "But basically what that means is we're going to want to try to learn complex models with lots of data points, so this has a number of applications at various domains vision by informatics, speech, language and to give you an idea of the scale of NP.",
            "For instance when you have industrial datasets and can be in the order of hundreds of millions or billions.",
            "NP can the like 10s of 10s of millions.",
            "So the main computational challenge is really how do you design algorithms which are able to learn for very large annand."
        ],
        [
            "Key.",
            "So we're going to see that as a standard machine learning optimization problem.",
            "So you define the function G which is going to be a loss function depending on some parameters Theta and you don't want to minimize the sum, so you're going to 5G as the sum of a finite number of functions FI.",
            "So N is really large, but really N is finite, and people usually assume that you have an infinite number of data points.",
            "In practice, they don't.",
            "They have a very large finite number of data points, and they see the data phone several times.",
            "So that's the setting which we're going to place ourselves in.",
            "And for instance, in the case of logistic regression, you can think of each function FI as the log loss plus a ^2 L two norm regularizer, Y ^2 L two norm regularizer.",
            "Because for this talk I'm really going to focus on strongly convex functions G."
        ],
        [
            "So there's been a whole lot of methods to solve this kind of problems, But there are really 2 main classes of methods to 1st class are deterministic methods, so you have your function G which you're trying to minimize.",
            "So you're going to each rates over the set of parameters by computing the gradient of that function G. And since G is the average of functions FI, well, you need to compute the average of all these.",
            "All these gradients of the functions FI.",
            "So since you have ingredients to compute the iteration costs, isn't of over North.",
            "And that's a problem when end is really large.",
            "On the other hand, since we have the true gradients, you get to linear convergence rates, which means every time you have data parameters, you're going to decrease you error.",
            "But you fixed amount.",
            "So we get a fast convergence rate, but each iteration is extremely costly.",
            "That's the dumb planeful gradients.",
            "You have many friends here methods, but even with dispenser methods you get faster convergence speed, but you don't get around the iteration costs in over.",
            "So to get around that problem of overnutrition costs, you have another class of methods."
        ],
        [
            "Which are the stochastic methods for the stochastic methods instead of computing the true gradient of the function G you're interested in, which implies computing the gradient for end functions F. Are you just going to pick one at random, and then you're going to compute the gradient for that one particular function and update your parameters accordingly.",
            "Since you only pick one function, then your iteration Kostadinov one is independent of the end, so that's really nice.",
            "The problem is certainly no, you have a bias gradients.",
            "You only have well the base.",
            "You have a noisy gradients, you only have one data point instead of the entire data set.",
            "So we don't converge as quickly.",
            "You actually have sublinear convergence rates in all of 1 / K. As a side note, which is something which is usually overlooked when you use the stochastic method.",
            "If you see each data point only once, the boundary gets is also the bound on the test error.",
            "So for data points you have not seen as soon as you start to see one data point several times, you lose that bond on the tester and you only have a bound on the training error, and in practice you always see data points or almost always, except maybe if you work at Google you always always see data phone several times.",
            "So to summarize, the difference between stochastic methods and deterministic methods.",
            "That's the the log of develop.",
            "That's the excess cost as a function of time, so deterministic math."
        ],
        [
            "Did you start slowly?",
            "Because of this, all of an iteration costs, but then you steadily make progress and the steps are because you have to compute gradients to make an update stochastic.",
            "You start very quickly because of this all of 1, but then you kind of stall, So what?"
        ],
        [
            "I'd like to have is the best of both worlds.",
            "We'd like to have the old one iteration cost to be very good since the very beginning and then the linear rates that we can also be good later on and we don't have to do some kind of fancy switching between one or the other."
        ],
        [
            "So there's a whole lot of literature about how to accelerate optimization methods, so you have stochastic version of full grid methods, momentum gradient averaging.",
            "These methods can improve your convergence speed, but they never improve on the oh of 1 / K rates.",
            "And remember the two goals have are having over one iteration cost, and the linear convergence rate, so we don't get the linear convergence with these methods.",
            "You have."
        ],
        [
            "Other methods like constant stepsize, stochastic gradient, saturated stochastic gradient, hybrid methods, incremental average gradient, stochastic methods in the dual they reach leaner rates, but there's always some caveats are not applicable in every setting or its leader rate in specific setting, so none of these methods achieve the Holy Grail, which is when you have general function G. How do you get out of one iteration?",
            "Costs and linear conversion?"
        ],
        [
            "Right?",
            "So the method we propose, which is called the stochastic average gradient methods.",
            "Explain that method.",
            "I'm going to start from the full gradient."
        ],
        [
            "So the full gradient updates you're going to update your parameters by.",
            "Anyway, no laser then, so you're going to compute your updates by computing the gradient."
        ],
        [
            "Overall, the functions FI and of course these ingredients kill you every time.",
            "These are the.",
            "This is the bottom of the oh of an iteration costs.",
            "So instead of computing having to compute the end gradients every single time, we're going to compute, replace the true gradients by some approximate value Yi.",
            "What is this approximate value?",
            "Why I?",
            "Well, it's going to be the last time we compute the last gradient we computed on data point I.",
            "So what I mean by that is, at every time step, we're going to randomly sample the data points we're going to compute the gradient for that data point, and we know store that gradients as why I discarding the previous gradients we have computed for the specific data points, replacing it by the one we've just computed and all the other gradients we have stored.",
            "We don't touch them, they're not there, be told they're not as good as the one we just computed because they use all values of Theta, but it's fine.",
            "We are willing to have that cost.",
            "And store all gradients as well, and then the next time step we're going to sample the new data point, replace the stored value of the gradient for that data point by the one we've just computed, and we go on.",
            "So we always have an average of N gradients computed, except that they've been computed at different time steps in one gradients for data points, OK?",
            "Actually, this is the stochastic variance of the paper from 2007 from let it all, which is called incremental average gradient.",
            "They do the exact same thing, the only difference being that's where we randomly sampled the data point at each time step they cycle through the examples every time.",
            "It seems like a minor difference, and originally we developed this algorithm just to have a similar analysis of the convergence rates.",
            "It so happens that replacing the cycling through the examples by random sampling allows us to have much faster.",
            "Vergence"
        ],
        [
            "So the convergence analysis of tag.",
            "So we make some assumptions.",
            "We make the assumption that if each of the gradient FI prime is Lipschitz continuous and the average function G is mean strongly convex.",
            "Seem obscured some of you in a machine learning context.",
            "You can imagine that we assume that all the data points to live within evolve, and that we have an L2 squared L2 norm regularizer.",
            "That's enough to justify these assumptions.",
            "If you use a small step size, constant step size, Alpha equals 1 / 2 and L, we get the linear convergence rates.",
            "So sag actually gets the oh of one iteration costs and the linear convergence rates.",
            "However.",
            "That's convergence is not really fast, so we don't converge any faster than full gradient descent.",
            "An.",
            "In practice we don't converge faster than incremental average gradients.",
            "However, since we interested in large scale setting as soon as you have enough data points then you can use a much bigger step size so you can use Alpha equals one over to an mu instead of 1 / 2 NL.",
            "That's much bigger, and in that case the expectation of the decrease in error expectation because there are some random Ness due to sampling then is also linear, but then the convergence rate is much faster.",
            "It's 1 -- 1 / 8 N. That's rate is independent of the condition number, and for those of you who.",
            "No, but the theory behind optimization, the convergence rate always involves the condition number.",
            "Here doesn't involve the caveat being that the number of examples, the minimum number of examples you need to have depends on the condition number, so that's still the dependency we have.",
            "But once this condition is satisfied, once you have enough data points, you have no dependency on the condition number anymore, so.",
            "What happens, let's say after I've done any iterations, so after I've seen N samples, well, my error has been multiplied by .88, so I have a 12% reduction in my air.",
            "Is it good?",
            "Is it bad?",
            "Well, it so happens.",
            "It's really good.",
            "So for instance."
        ],
        [
            "To take L equals hundreds musicals, .01 Ann, you have 80,000 data points.",
            "If you take full green methods, then after having seen any examples you have only multiplied your error by .9998, so it's actually very slow convergence.",
            "If you do, the accelerated gradient method from Nesterov, you get .99 is faster, but we get .8825.",
            "Let's be honest.",
            "And actually, what's really interesting is that the fastest possible 1st order method.",
            "Only gets .96.",
            "So we are faster than the fastest possible 1st order method.",
            "So how do we do that without cheating?",
            "Obviously well, in fact that's one bound beat and we beat another bound.",
            "So the way we beat these two bounds is because we make additional assumptions.",
            "So we beat the stochastic over 1 / K gradient bound.",
            "Why?",
            "Because we assume the finite data set and say this is usually the case in machine learning.",
            "We also be the full gradient bound Huawei because we assume we didn't have a general function G, but our function was the average of functions FI.",
            "So by making these two extra assumptions which are actually really model in context of machine learning, we are able to get faster convergence rate than all these other methods.",
            "So that's the theory."
        ],
        [
            "In practice, how does it perform?",
            "Well, that's one data set, so 50,000 data points.",
            "We compared to five different methods.",
            "In Brown you have the active full gradient, so first order deterministic method.",
            "In green you have LBF 2nd order deterministic method in orange you have Picassos 1st order stochastic method and in the two pink curves are the two version of SAG.",
            "Actually the dotted one is the one from the position.",
            "We use a constant learning rate as in a proposition the.",
            "Plane line is where we don't even set the learning rate.",
            "We use the juristic linesearch to automatically find the learning rates and adjust the learning rate as we go along.",
            "So the plane line doesn't have any hyperparameter.",
            "You can just do that anything is automatically set that."
        ],
        [
            "Another data set.",
            "We get the nice linear convergence faster, so before someone makes a remark that you only have 5 curves on this, do 2D relentless reviewers request if you go to the poster, you're going to have 12 curves on five datasets using three different metrics, and that's not enough.",
            "You can check out the paper.",
            "We have another five curves on some more datasets.",
            "So that's just for you now.",
            "So since it's a machine learning conference, these other training errors, I'm also going to test errors.",
            "So for these we have absolutely no theoretical guarantee, is just so happens that it works kind of well."
        ],
        [
            "So on this datasets, so performs better than all the others, because those actually only have one point.",
            "After that it went up.",
            "So that's just for this particular data set on this."
        ],
        [
            "But it's it's because those actually performs exactly the same as tag.",
            "Saga is not always going to be the best one in test error.",
            "Again, come to our poster to see the full extent of the results."
        ],
        [
            "Quickly something with that might bother you is that you have to store one gradient for data points.",
            "So if you have any data points in dimension P, that storage computation that storage cost of NP as soon as you have structure new model.",
            "For instance, if you have linear model, you can definitely use that storage cost.",
            "So if you have linear model, you don't actually need to store one gradient for data points.",
            "You can store one scalar for data points, so then the storage costs become more of an itself over NP.",
            "You can also do something more standard if you don't have any structure, you can use mini batches and we've shown we've seen empirically that works with me.",
            "Is of size 1000, so if you have mini batches of size of thousands you only need to store one gradient for mini batch.",
            "So you've reduced your storage cost by thousands.",
            "OK, that works for linear model, also neural networks."
        ],
        [
            "So in conclusion, we have what we wanted.",
            "We have a fast theoretical convergence.",
            "The oh of one iteration cost the linear convergence rates by using the sound structure.",
            "It's a very simple algorithm.",
            "It is actually literally two extra lines of code compared to stochastic gradient descent.",
            "Empirically, it actually works better than with theory predicted.",
            "It does allow line search.",
            "It also allows approximate optimality measures, so we have a stopping criterion which we can use, so these are really nice things.",
            "There are some things we haven't fully understood yet and we would be interesting to explore.",
            "The first one is large scale distributed implementation, so STACK relies on the fact that some gradients are old.",
            "But that's fine.",
            "You can still use them, so it seems particularly vulnerable to distributed computation, where some nodes basically don't send the gradients in time.",
            "You can still use them.",
            "Tight convergence rates in all cases, the fast convergence rate I showed you is when the number of data points is bigger than some constant.",
            "Empirically it works in all the cases, but we don't have the theory to back up that.",
            "We'd like to apply the method to constrain them smooth problems who have proximal version, and also would like to have maybe some 2nd order version, since tag is basically an approximation to the full gradient, you can use any fancy method on top of the full gradient on sag, and there's no at least every reason for that not to work, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was a bit stressed this morning, so several people told me that it didn't make any difference whether we talk in front of 40 people or 1000.",
                    "label": 0
                },
                {
                    "sent": "Well, they lied.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to present the paper I worked on with my 2 great coauthors Mark Smith and Frances back.",
                    "label": 0
                },
                {
                    "sent": "This is a great pleasure to collaborate with them, so the paper is stochastic gradient method with an exponential convergence rate for finite training sets.",
                    "label": 1
                },
                {
                    "sent": "So don't be afraid by the length of the title, it's the result of Marx theory that the more words you put in the title, the more likely you are to have adequate reviewers.",
                    "label": 0
                },
                {
                    "sent": "So here I am, so it seems to have worked so you know what you do for next year.",
                    "label": 0
                },
                {
                    "sent": "So the basis for this work is we're going to be interested in two in.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "We're going to be interested.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In large scale problems, so large and large P when N is the number of observations you have NPS the number of parameters in your model.",
                    "label": 1
                },
                {
                    "sent": "So if you have linear model, P is going to be the dimension of your observations.",
                    "label": 0
                },
                {
                    "sent": "But basically what that means is we're going to want to try to learn complex models with lots of data points, so this has a number of applications at various domains vision by informatics, speech, language and to give you an idea of the scale of NP.",
                    "label": 1
                },
                {
                    "sent": "For instance when you have industrial datasets and can be in the order of hundreds of millions or billions.",
                    "label": 0
                },
                {
                    "sent": "NP can the like 10s of 10s of millions.",
                    "label": 1
                },
                {
                    "sent": "So the main computational challenge is really how do you design algorithms which are able to learn for very large annand.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Key.",
                    "label": 0
                },
                {
                    "sent": "So we're going to see that as a standard machine learning optimization problem.",
                    "label": 1
                },
                {
                    "sent": "So you define the function G which is going to be a loss function depending on some parameters Theta and you don't want to minimize the sum, so you're going to 5G as the sum of a finite number of functions FI.",
                    "label": 0
                },
                {
                    "sent": "So N is really large, but really N is finite, and people usually assume that you have an infinite number of data points.",
                    "label": 0
                },
                {
                    "sent": "In practice, they don't.",
                    "label": 0
                },
                {
                    "sent": "They have a very large finite number of data points, and they see the data phone several times.",
                    "label": 0
                },
                {
                    "sent": "So that's the setting which we're going to place ourselves in.",
                    "label": 1
                },
                {
                    "sent": "And for instance, in the case of logistic regression, you can think of each function FI as the log loss plus a ^2 L two norm regularizer, Y ^2 L two norm regularizer.",
                    "label": 0
                },
                {
                    "sent": "Because for this talk I'm really going to focus on strongly convex functions G.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's been a whole lot of methods to solve this kind of problems, But there are really 2 main classes of methods to 1st class are deterministic methods, so you have your function G which you're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "So you're going to each rates over the set of parameters by computing the gradient of that function G. And since G is the average of functions FI, well, you need to compute the average of all these.",
                    "label": 0
                },
                {
                    "sent": "All these gradients of the functions FI.",
                    "label": 0
                },
                {
                    "sent": "So since you have ingredients to compute the iteration costs, isn't of over North.",
                    "label": 0
                },
                {
                    "sent": "And that's a problem when end is really large.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, since we have the true gradients, you get to linear convergence rates, which means every time you have data parameters, you're going to decrease you error.",
                    "label": 0
                },
                {
                    "sent": "But you fixed amount.",
                    "label": 0
                },
                {
                    "sent": "So we get a fast convergence rate, but each iteration is extremely costly.",
                    "label": 0
                },
                {
                    "sent": "That's the dumb planeful gradients.",
                    "label": 0
                },
                {
                    "sent": "You have many friends here methods, but even with dispenser methods you get faster convergence speed, but you don't get around the iteration costs in over.",
                    "label": 0
                },
                {
                    "sent": "So to get around that problem of overnutrition costs, you have another class of methods.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which are the stochastic methods for the stochastic methods instead of computing the true gradient of the function G you're interested in, which implies computing the gradient for end functions F. Are you just going to pick one at random, and then you're going to compute the gradient for that one particular function and update your parameters accordingly.",
                    "label": 0
                },
                {
                    "sent": "Since you only pick one function, then your iteration Kostadinov one is independent of the end, so that's really nice.",
                    "label": 0
                },
                {
                    "sent": "The problem is certainly no, you have a bias gradients.",
                    "label": 0
                },
                {
                    "sent": "You only have well the base.",
                    "label": 0
                },
                {
                    "sent": "You have a noisy gradients, you only have one data point instead of the entire data set.",
                    "label": 0
                },
                {
                    "sent": "So we don't converge as quickly.",
                    "label": 0
                },
                {
                    "sent": "You actually have sublinear convergence rates in all of 1 / K. As a side note, which is something which is usually overlooked when you use the stochastic method.",
                    "label": 0
                },
                {
                    "sent": "If you see each data point only once, the boundary gets is also the bound on the test error.",
                    "label": 1
                },
                {
                    "sent": "So for data points you have not seen as soon as you start to see one data point several times, you lose that bond on the tester and you only have a bound on the training error, and in practice you always see data points or almost always, except maybe if you work at Google you always always see data phone several times.",
                    "label": 1
                },
                {
                    "sent": "So to summarize, the difference between stochastic methods and deterministic methods.",
                    "label": 0
                },
                {
                    "sent": "That's the the log of develop.",
                    "label": 0
                },
                {
                    "sent": "That's the excess cost as a function of time, so deterministic math.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did you start slowly?",
                    "label": 0
                },
                {
                    "sent": "Because of this, all of an iteration costs, but then you steadily make progress and the steps are because you have to compute gradients to make an update stochastic.",
                    "label": 0
                },
                {
                    "sent": "You start very quickly because of this all of 1, but then you kind of stall, So what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to have is the best of both worlds.",
                    "label": 0
                },
                {
                    "sent": "We'd like to have the old one iteration cost to be very good since the very beginning and then the linear rates that we can also be good later on and we don't have to do some kind of fancy switching between one or the other.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a whole lot of literature about how to accelerate optimization methods, so you have stochastic version of full grid methods, momentum gradient averaging.",
                    "label": 1
                },
                {
                    "sent": "These methods can improve your convergence speed, but they never improve on the oh of 1 / K rates.",
                    "label": 1
                },
                {
                    "sent": "And remember the two goals have are having over one iteration cost, and the linear convergence rate, so we don't get the linear convergence with these methods.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other methods like constant stepsize, stochastic gradient, saturated stochastic gradient, hybrid methods, incremental average gradient, stochastic methods in the dual they reach leaner rates, but there's always some caveats are not applicable in every setting or its leader rate in specific setting, so none of these methods achieve the Holy Grail, which is when you have general function G. How do you get out of one iteration?",
                    "label": 0
                },
                {
                    "sent": "Costs and linear conversion?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So the method we propose, which is called the stochastic average gradient methods.",
                    "label": 1
                },
                {
                    "sent": "Explain that method.",
                    "label": 1
                },
                {
                    "sent": "I'm going to start from the full gradient.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the full gradient updates you're going to update your parameters by.",
                    "label": 0
                },
                {
                    "sent": "Anyway, no laser then, so you're going to compute your updates by computing the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overall, the functions FI and of course these ingredients kill you every time.",
                    "label": 0
                },
                {
                    "sent": "These are the.",
                    "label": 0
                },
                {
                    "sent": "This is the bottom of the oh of an iteration costs.",
                    "label": 0
                },
                {
                    "sent": "So instead of computing having to compute the end gradients every single time, we're going to compute, replace the true gradients by some approximate value Yi.",
                    "label": 0
                },
                {
                    "sent": "What is this approximate value?",
                    "label": 0
                },
                {
                    "sent": "Why I?",
                    "label": 0
                },
                {
                    "sent": "Well, it's going to be the last time we compute the last gradient we computed on data point I.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by that is, at every time step, we're going to randomly sample the data points we're going to compute the gradient for that data point, and we know store that gradients as why I discarding the previous gradients we have computed for the specific data points, replacing it by the one we've just computed and all the other gradients we have stored.",
                    "label": 0
                },
                {
                    "sent": "We don't touch them, they're not there, be told they're not as good as the one we just computed because they use all values of Theta, but it's fine.",
                    "label": 0
                },
                {
                    "sent": "We are willing to have that cost.",
                    "label": 0
                },
                {
                    "sent": "And store all gradients as well, and then the next time step we're going to sample the new data point, replace the stored value of the gradient for that data point by the one we've just computed, and we go on.",
                    "label": 0
                },
                {
                    "sent": "So we always have an average of N gradients computed, except that they've been computed at different time steps in one gradients for data points, OK?",
                    "label": 0
                },
                {
                    "sent": "Actually, this is the stochastic variance of the paper from 2007 from let it all, which is called incremental average gradient.",
                    "label": 1
                },
                {
                    "sent": "They do the exact same thing, the only difference being that's where we randomly sampled the data point at each time step they cycle through the examples every time.",
                    "label": 0
                },
                {
                    "sent": "It seems like a minor difference, and originally we developed this algorithm just to have a similar analysis of the convergence rates.",
                    "label": 0
                },
                {
                    "sent": "It so happens that replacing the cycling through the examples by random sampling allows us to have much faster.",
                    "label": 0
                },
                {
                    "sent": "Vergence",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the convergence analysis of tag.",
                    "label": 1
                },
                {
                    "sent": "So we make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "We make the assumption that if each of the gradient FI prime is Lipschitz continuous and the average function G is mean strongly convex.",
                    "label": 0
                },
                {
                    "sent": "Seem obscured some of you in a machine learning context.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that we assume that all the data points to live within evolve, and that we have an L2 squared L2 norm regularizer.",
                    "label": 0
                },
                {
                    "sent": "That's enough to justify these assumptions.",
                    "label": 0
                },
                {
                    "sent": "If you use a small step size, constant step size, Alpha equals 1 / 2 and L, we get the linear convergence rates.",
                    "label": 1
                },
                {
                    "sent": "So sag actually gets the oh of one iteration costs and the linear convergence rates.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "That's convergence is not really fast, so we don't converge any faster than full gradient descent.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "In practice we don't converge faster than incremental average gradients.",
                    "label": 0
                },
                {
                    "sent": "However, since we interested in large scale setting as soon as you have enough data points then you can use a much bigger step size so you can use Alpha equals one over to an mu instead of 1 / 2 NL.",
                    "label": 0
                },
                {
                    "sent": "That's much bigger, and in that case the expectation of the decrease in error expectation because there are some random Ness due to sampling then is also linear, but then the convergence rate is much faster.",
                    "label": 0
                },
                {
                    "sent": "It's 1 -- 1 / 8 N. That's rate is independent of the condition number, and for those of you who.",
                    "label": 1
                },
                {
                    "sent": "No, but the theory behind optimization, the convergence rate always involves the condition number.",
                    "label": 0
                },
                {
                    "sent": "Here doesn't involve the caveat being that the number of examples, the minimum number of examples you need to have depends on the condition number, so that's still the dependency we have.",
                    "label": 0
                },
                {
                    "sent": "But once this condition is satisfied, once you have enough data points, you have no dependency on the condition number anymore, so.",
                    "label": 0
                },
                {
                    "sent": "What happens, let's say after I've done any iterations, so after I've seen N samples, well, my error has been multiplied by .88, so I have a 12% reduction in my air.",
                    "label": 0
                },
                {
                    "sent": "Is it good?",
                    "label": 0
                },
                {
                    "sent": "Is it bad?",
                    "label": 0
                },
                {
                    "sent": "Well, it so happens.",
                    "label": 0
                },
                {
                    "sent": "It's really good.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To take L equals hundreds musicals, .01 Ann, you have 80,000 data points.",
                    "label": 0
                },
                {
                    "sent": "If you take full green methods, then after having seen any examples you have only multiplied your error by .9998, so it's actually very slow convergence.",
                    "label": 0
                },
                {
                    "sent": "If you do, the accelerated gradient method from Nesterov, you get .99 is faster, but we get .8825.",
                    "label": 1
                },
                {
                    "sent": "Let's be honest.",
                    "label": 0
                },
                {
                    "sent": "And actually, what's really interesting is that the fastest possible 1st order method.",
                    "label": 0
                },
                {
                    "sent": "Only gets .96.",
                    "label": 0
                },
                {
                    "sent": "So we are faster than the fastest possible 1st order method.",
                    "label": 1
                },
                {
                    "sent": "So how do we do that without cheating?",
                    "label": 0
                },
                {
                    "sent": "Obviously well, in fact that's one bound beat and we beat another bound.",
                    "label": 1
                },
                {
                    "sent": "So the way we beat these two bounds is because we make additional assumptions.",
                    "label": 0
                },
                {
                    "sent": "So we beat the stochastic over 1 / K gradient bound.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because we assume the finite data set and say this is usually the case in machine learning.",
                    "label": 1
                },
                {
                    "sent": "We also be the full gradient bound Huawei because we assume we didn't have a general function G, but our function was the average of functions FI.",
                    "label": 0
                },
                {
                    "sent": "So by making these two extra assumptions which are actually really model in context of machine learning, we are able to get faster convergence rate than all these other methods.",
                    "label": 0
                },
                {
                    "sent": "So that's the theory.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, how does it perform?",
                    "label": 0
                },
                {
                    "sent": "Well, that's one data set, so 50,000 data points.",
                    "label": 0
                },
                {
                    "sent": "We compared to five different methods.",
                    "label": 0
                },
                {
                    "sent": "In Brown you have the active full gradient, so first order deterministic method.",
                    "label": 0
                },
                {
                    "sent": "In green you have LBF 2nd order deterministic method in orange you have Picassos 1st order stochastic method and in the two pink curves are the two version of SAG.",
                    "label": 0
                },
                {
                    "sent": "Actually the dotted one is the one from the position.",
                    "label": 0
                },
                {
                    "sent": "We use a constant learning rate as in a proposition the.",
                    "label": 0
                },
                {
                    "sent": "Plane line is where we don't even set the learning rate.",
                    "label": 0
                },
                {
                    "sent": "We use the juristic linesearch to automatically find the learning rates and adjust the learning rate as we go along.",
                    "label": 0
                },
                {
                    "sent": "So the plane line doesn't have any hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "You can just do that anything is automatically set that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another data set.",
                    "label": 0
                },
                {
                    "sent": "We get the nice linear convergence faster, so before someone makes a remark that you only have 5 curves on this, do 2D relentless reviewers request if you go to the poster, you're going to have 12 curves on five datasets using three different metrics, and that's not enough.",
                    "label": 0
                },
                {
                    "sent": "You can check out the paper.",
                    "label": 0
                },
                {
                    "sent": "We have another five curves on some more datasets.",
                    "label": 0
                },
                {
                    "sent": "So that's just for you now.",
                    "label": 0
                },
                {
                    "sent": "So since it's a machine learning conference, these other training errors, I'm also going to test errors.",
                    "label": 0
                },
                {
                    "sent": "So for these we have absolutely no theoretical guarantee, is just so happens that it works kind of well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on this datasets, so performs better than all the others, because those actually only have one point.",
                    "label": 0
                },
                {
                    "sent": "After that it went up.",
                    "label": 0
                },
                {
                    "sent": "So that's just for this particular data set on this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's it's because those actually performs exactly the same as tag.",
                    "label": 0
                },
                {
                    "sent": "Saga is not always going to be the best one in test error.",
                    "label": 0
                },
                {
                    "sent": "Again, come to our poster to see the full extent of the results.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly something with that might bother you is that you have to store one gradient for data points.",
                    "label": 0
                },
                {
                    "sent": "So if you have any data points in dimension P, that storage computation that storage cost of NP as soon as you have structure new model.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have linear model, you can definitely use that storage cost.",
                    "label": 0
                },
                {
                    "sent": "So if you have linear model, you don't actually need to store one gradient for data points.",
                    "label": 0
                },
                {
                    "sent": "You can store one scalar for data points, so then the storage costs become more of an itself over NP.",
                    "label": 0
                },
                {
                    "sent": "You can also do something more standard if you don't have any structure, you can use mini batches and we've shown we've seen empirically that works with me.",
                    "label": 0
                },
                {
                    "sent": "Is of size 1000, so if you have mini batches of size of thousands you only need to store one gradient for mini batch.",
                    "label": 0
                },
                {
                    "sent": "So you've reduced your storage cost by thousands.",
                    "label": 0
                },
                {
                    "sent": "OK, that works for linear model, also neural networks.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we have what we wanted.",
                    "label": 0
                },
                {
                    "sent": "We have a fast theoretical convergence.",
                    "label": 1
                },
                {
                    "sent": "The oh of one iteration cost the linear convergence rates by using the sound structure.",
                    "label": 1
                },
                {
                    "sent": "It's a very simple algorithm.",
                    "label": 1
                },
                {
                    "sent": "It is actually literally two extra lines of code compared to stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Empirically, it actually works better than with theory predicted.",
                    "label": 0
                },
                {
                    "sent": "It does allow line search.",
                    "label": 0
                },
                {
                    "sent": "It also allows approximate optimality measures, so we have a stopping criterion which we can use, so these are really nice things.",
                    "label": 0
                },
                {
                    "sent": "There are some things we haven't fully understood yet and we would be interesting to explore.",
                    "label": 0
                },
                {
                    "sent": "The first one is large scale distributed implementation, so STACK relies on the fact that some gradients are old.",
                    "label": 0
                },
                {
                    "sent": "But that's fine.",
                    "label": 0
                },
                {
                    "sent": "You can still use them, so it seems particularly vulnerable to distributed computation, where some nodes basically don't send the gradients in time.",
                    "label": 0
                },
                {
                    "sent": "You can still use them.",
                    "label": 0
                },
                {
                    "sent": "Tight convergence rates in all cases, the fast convergence rate I showed you is when the number of data points is bigger than some constant.",
                    "label": 1
                },
                {
                    "sent": "Empirically it works in all the cases, but we don't have the theory to back up that.",
                    "label": 0
                },
                {
                    "sent": "We'd like to apply the method to constrain them smooth problems who have proximal version, and also would like to have maybe some 2nd order version, since tag is basically an approximation to the full gradient, you can use any fancy method on top of the full gradient on sag, and there's no at least every reason for that not to work, thank you.",
                    "label": 0
                }
            ]
        }
    }
}