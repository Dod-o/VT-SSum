{
    "id": "2jx24pldelkqr22dkkkbvpnf46whrgxk",
    "title": "The Online Discovery Problem and Its Application to Lifelong Reinforcement Learning",
    "info": {
        "author": [
            "Lihong Li, Microsoft Research"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_li_discovery_problem/",
    "segmentation": [
        [
            "This talk is about lifelong reinforcement learning.",
            "This is joint work with Emma Brunskill from CMU.",
            "Um, and in case you're interested, our paper will be available on archive very soon.",
            "Um?"
        ],
        [
            "OK, so let me start with an example of lifelong reinforcement learning.",
            "Let's say you have an intelligent tutoring system that tries to teach students how to master geography, for example.",
            "Then when student Alice comes, you have some information about a student."
        ],
        [
            "Like what she has learned before in the grades in the past and then the system can take sequence of actions."
        ],
        [
            "A predefined set of actions.",
            "For example this and then the goal, is to take the sequence of actions so that the total reward is maximized, meaning that the student Alice can learn the topic as soon as possible or as well as possible.",
            "But in reality, so this is standard single task reinforcement learning.",
            "But in reality Alice is not is probably not the first student that the system has taught in the past."
        ],
        [
            "So the system has sort of a number of students before and she's not the last one.",
            "The system is going to teach."
        ],
        [
            "So the system is now has the questions of how to buy."
        ],
        [
            "How to benefit Alice from previous teaching experience with students in the past and how to."
        ],
        [
            "Teach Alice to collect information so that system can benefit students in the future, so the system is now facing a lifelong reinforcement learning problem where each student is a task and a student comes in a sequence.",
            "Farm."
        ],
        [
            "In this talk, I'm going to formulate a task as a Markov decision process or MVP.",
            "I guess everyone else so very quickly you have a set state set of actions, transitions, rewards in discount factor in every state you the system can take an action resulting in a reward whose expectation is given by the work function and the next state is sample from the transition probabilities.",
            "And the goal is to maximize total reward.",
            "So this is a single task or else setting.",
            "In"
        ],
        [
            "Lifelong or L. Here we focus on the specialized case of lifelong learning, so we're not claiming to solve general lifelong learning, but it's a very special case that as well as very good structure.",
            "And so here the system, the learning agent.",
            "Is given the spine itself, states opponents of action and a discount, so these things are known.",
            "What is not known is that the agent is going to face a sequence of task.",
            "Each one is drawn from a set of CMDPS, so this CMDS have the same stay and actions, but they may differ in the transition and reward functions.",
            "And in life long learning the for that each task the agent observes are MVP while the agent is is encountered with the encounters with an MVP whose identity is unknown and it's given by the environment.",
            "And then he's going to add in this MVP for each steps trying to maximize reward and such a.",
            "And as I said there many kinds of lifelong learning problem and this is the special case that we focus here.",
            "So you may wonder why is that?",
            "Why does it make sense to assume there is finitely many amps?",
            "The reason is that here we focus on fine."
        ],
        [
            "8 M DPS.",
            "So it turns out that the finitely many amps with sufficiently different model parameters, meaning that only a finite number of policy czar are able to cover near optimal policies for all the MPs and with the same state elections."
        ],
        [
            "And in the past people have study applications that are in this setting, for example in tutoring.",
            "In teaching, people have realized students with different learning types can be taught in different ways.",
            "Also in human robot interaction, it's helpful to identify user types that the robot can better assist the user.",
            "Also in software task assistant will help patients etc also helps to identify user goals of the current task.",
            "So All in all these examples your system is facing one of a finite number of possible MVP's and the goal is to identify the MVP and then maximize reward."
        ],
        [
            "I'm an in lifelong reinforcement learning that it turns out that the two kinds of two kinds of exploration there is quite unique, so let's say empty is the current task or Alice in this teaching example, this is the past task that has been solved, and this is our future tasks.",
            "Had to be the set of different types of MPs that has been discovered by the system in the last T -- 1 task.",
            "Um, then then you there 2."
        ],
        [
            "Kinds of exploration were the 2 two goes that maybe they may conflict with each other.",
            "The first one is within task learning, so this is a standard exploration in RL with."
        ],
        [
            "The goal is to maximize reward, but once in awhile need to explore promising states in order to converse to the two optimal policy.",
            "So this is standard.",
            "What is new here?"
        ],
        [
            "There is that there is also the need for cross task knowledge transfer, meaning that once in awhile the agent needs to explore possibly order states in the new in the current task, not just to not for maximizing reward, but to identify possible.",
            "Whether this is a new type of MDP and if it's new type then this information could be useful in the future to benefits solving future tasks so that overall this over sequence of Pete asked the total rewards maximized, not just.",
            "The reward in this particular task."
        ],
        [
            "For now we are facing a.",
            "A mix of two kinds of exploration simultaneously.",
            "So we have the crossed heart exploration exploitation tradeoff over within cars exploration.",
            "Exploitation tradeoff?",
            "So now the question is how we can balance these two kinds of tradeoff while in this task we are trying to balance exploration, exploitation as usual."
        ],
        [
            "So here's the how the online discovery problem comes to help.",
            "So this is an abstraction of the cross pass expiration.",
            "In the previous slide, the here's the problem.",
            "The environment has a set of finite elements.",
            "You can think of that isn't even MVP's.",
            "The agent started with nothing, knowing nothing, knowing none of the possible types and then in T when in the round T the environment select one of these elements.",
            "The agent has two choices you can.",
            "Choose them, explore, have taken a equals one or two, exploit and explore.",
            "Then then with high probability is going to discover this task identity over the task and it is normal task and you can add it to it as mhat the set of different types of MVP's that he has discovered so far.",
            "Otherwise he doesn't discover that the task.",
            "The identity of the task and then the."
        ],
        [
            "Also the agent by choosing this action is given by this two by two loss matrix exploration, well exploitation the 1st Rd exploitation.",
            "The columns correspond to whether the current task is normal task or not.",
            "And the color of the agent is to maximize, minimize total loss over 2 rounds.",
            "So here we specify the structure as follows.",
            "The Ro Zero is less than 01 less than row two less than real quick.",
            "So this is not we don't set the numbers arbitrarily, so in in the first row, when the agent decides not to explore, meaning that the agent tries to use information transfer from the past to do say, to accelerate solving current tasks.",
            "Then if the task is turns out to be an 01, then transfer helps.",
            "This is what so row one correspond to the loss of successful transfer.",
            "That's why is the lowest one.",
            "On the other hand, row three correspond to the case where the agent decide not to do task identification or exploration, but the task is novel, so the agent may miss the opportunity to learn the auto policy in the current task, therefore suffering the maximum loss.",
            "That's why row three is the largest one, and in between row one, row two that correspond to the action of trying to probe or trying to explore the current tasks.",
            "So this costly, but they're not.",
            "They're closely actions, but not as high as row 3.",
            "So that's why we set this order here."
        ],
        [
            "So now I'm going to give two very simple algorithms for solving on a discovery, and then we'll use one of these two instant file to create and lifelong learning, lifelong reinforcement learning algorithm.",
            "The first one for online discovery is called exploration.",
            "Explore first is very simple.",
            "It makes the stochastic assumption that the tasks are drawn IID from some unknown distribution over all possible or possible task.",
            "And then so in these the kostic's case it can be shown that the best strategy would be to use all the exploration budget up front and then switch to exploitation.",
            "So that's so.",
            "That's why this algorithm makes sense.",
            "So it has a threshold E before E. It always explores.",
            "After that it switches to the exploitation mode and start doing exploitation.",
            "Um?",
            "And then if you know the minimum probability of task, probably the minimum probability of each task being drawn, then you can set it up properly by this and then you can show that the average loss of this algorithm is at most the optimal loss or the minimum loss plus an additional term of this.",
            "As you can see, as T goes to Infinity.",
            "Term goes to 0, so this explore first algorithm is going to compete with the best or converges to the best strategy.",
            "But it relies on this assumption and relies on the knowledge of minimum probability.",
            "If Muse M is small than this term is big because one over mu is bigger.",
            "So now let me."
        ],
        [
            "Give you a second algorithm for sexploration.",
            "It doesn't require their stochastic assumption.",
            "So in fact the set of MPs can be generated by an adversary in an arbitrary way.",
            "The way it works is to prefix a set of Explorer sequence of nonincreasing exploration.",
            "Rate ETA want 8 T and then for this for that eastep enjoying an exploration action from.",
            "Well it decides where they do.",
            "Exploration by joinable new Leaf with the success probability a dirty.",
            "And then if you set 80 to be one over square root T, then you can show it can show that the average loss of this algorithm is again convergence to the optimal loss with an additional term of one that scales on the other one over square root T. So again, it converges the optimal policy, and in fact we have also a lower bound, showing that this is the best you can do, so there's nothing you can do that is better beyond this term.",
            "So it means the force expiration is.",
            "Optimal up to a constant factor."
        ],
        [
            "So so here I'm going to use force exploration to create a lifelong learning algorithm.",
            "This morning, so this algorithm in high level it, it's just forced exploracion.",
            "This probability it tries to do exploration, otherwise it tries to exploitation.",
            "So what is needed now is just filling the details of reinforcement, learning their messy.",
            "But this is the high level message.",
            "So for exploration we can run the algorithm called pack explore that tries to explore, try to reach every state in this MDP to identify whether this is a new task or not.",
            "If it's new task, then we can.",
            "Shut up to my set of normal tasks.",
            "Otherwise we can do an exploitation.",
            "In this case we can use a variant of our Max who takes advantage of.",
            "This assumes that the task is one of these tasks in this M hat, and instead of learning trying to learn while trying to explore every state, it just try to explore stays of are sufficient to identify which MVP we're in from this task.",
            "So this algorithm tries to do transfer learning.",
            "Efficiently.",
            "Optionally, you can also have some improvement.",
            "For example, you tried to do an exploitation and you realize that the task is new.",
            "Then you can always switch to the exploration mode."
        ],
        [
            "So the so this is 1 slide of theory, so for for lifelong reinforcement here we try to quantify the performance by the overall sample complexity.",
            "So what is sample complexity?",
            "It says that for any algorithm, we treat it as a stochastic, well, nonstationary policy and then for it every step we whether if this policy is sub optimal.",
            "By this, with this vessel epsilon, then we call it a mistake and we try to bound the total number of mistakes in the whole library algorithm.",
            "In this particular case, I would try to bound the total number of mistakes summed over all all the tasks.",
            "And then this here's a theorem when the agent is is allowed to act in the MVP for long enough time when H is longer, then with high probability.",
            "Here's a sample complexity of the agent.",
            "It's so the first term as the number of tasks and the diameter D is the diameter, which is the steps required to reach the state from any other state in MDP and gamma is the number that measures the difference between two types.",
            "Two MPs in the set.",
            "On an essay is the number of state elections and N is the maximum number of possible next states.",
            "So as you can see, the first term is a dominating term as T goes to Infinity.",
            "The second term diminishes, so we're ending up with your synthetic performance of the first term.",
            "That doesn't scale with the number of states where the size of MVP, but rather the complexity of the lifelong learning problem.",
            "Steven, D and gamma.",
            "That, in contrast, in single task RL you always suffer if you if you solve these tasks as from scratch using a single task learning algorithm, then there's a.",
            "There's a lower bound that shows that you can't beat the.",
            "It has to be linear in a number of states.",
            "A number of actions.",
            "So by using my transfer knowledge now you're able to take advantage of the structure of the problem and then would result in the lower sample complexity."
        ],
        [
            "So let me quickly give an example or experiment of how this may help.",
            "We have a, so here's a 5 by 5 grid problem.",
            "Very simple.",
            "So this is just approval for a concept experiment.",
            "The shaded area knows center of the greatest star state and then we have the four MPs share the same star state.",
            "They have the same noisy state transition probabilities.",
            "What is different in this MPs is the rewarding states.",
            "So in this state is this one the green one?",
            "The green one here in green.",
            "One here for the last MPM four is similar to M3 except that it hasn't even greener or higher rewarding stays in this corner which make up this example so that now it's necessary to do sufficient exploration.",
            "To realize the M4 is different 3.",
            "And then we also hear the algorithms for comparison.",
            "We have the force exploration that in this work also we have the explore first algorithm which correspond to the previous work.",
            "We have two years ago and then and then there's also the one of the state of the art basin approaches for multi task learning.",
            "So tries to infer the posterior task identity and then follow the optimal policy of the maximum.",
            "A posterior model in this posterior distribution, so it doesn't do very active exploration, but has been shown to be very.",
            "Very strong in in a challenging game."
        ],
        [
            "So in the first plot, this is the hardest case, where in the four MVP S one of them has very small task probability task generation probability bill M is very small for the basin one.",
            "The Hierarchical multi task learning.",
            "It's very good for our 4th exploration, the blue one it's converging and approaching the getting close an for the two.",
            "Faced in that in two years ago it's not as good because it has.",
            "To spend a long time in the first phase, two just to just to uncover all the possible tasks, because the main one is Milam small.",
            "So Phase one is very very long.",
            "And there's a gap here is explained by the possibility by the necessary of force expiration to continue doing exploration in every task.",
            "So that's the price we pay for a constant exploration across task, and this will be helpful actually in another."
        ],
        [
            "Problems, for example, when you have this changing distribution or when the sequence of classes generated by an adversary, then this is helpful.",
            "Constant exploration is helpful, so again we have the two phase which is this phase one is bad.",
            "And this one is the basic one is very strong from the first, but then when at this point we we we change the distribution of the MPs so that M4 comes into play.",
            "It remember M4 is the same as M3, except that he has a much higher rewarding state and then because of the constant exploration algorithm, is able to catch up and realize then find the optimal policy but not the other algorithm.",
            "The two phase.",
            "Sometimes it's about to do discover, but it's not consistently.",
            "Able to do so."
        ],
        [
            "So, so conclude so.",
            "In this talk we emphasize two kinds of exploration.",
            "There are immediate to needed in lifelong reinforcement learning.",
            "For Cross task exploration we proposed would formulate the online discovery problem is an abstract way to to study this problem and then based based on an algorithm, an optimal algorithm for all online discovery.",
            "We create an algorithm for lifelong reinforcement learning that has low sample complexity and also demonstrate the desired behavior in a proof of concept experiments.",
            "So for future work it would be good to extend this work to function approximation or.",
            "An to use prior knowledge rather than doing this worst case analysis here.",
            "OK, so that's the kind of talking.",
            "Thanks for attention.",
            "Maybe just one quick question because we're out of time.",
            "So I have a question about the cross task exploration of your strategy, so it seems to me that you're deciding whether you transfer knowledge and art without knowing any information about the task at hand.",
            "Is that correct?",
            "Yeah, yeah, that's right.",
            "That's that's how we approach here.",
            "So so, in which setting would this be realistic?",
            "Well, it's so the reason that we should do this so there's so in the one I described, the algorithm I mentioned there is an improvement where if you decide to do explore exploitation and then realize that the task is normal, then you can switch to exploration.",
            "So that's possible.",
            "But on the other hand, you try to switch from exploration to exploitation.",
            "Then it's risky.",
            "It's showing the experiment where you have two MPs M3 and M. Or they they are almost identical except for one state when M4 has a much higher reward and then if you don't do a sufficient exploration then you can't.",
            "You can realize that M4 has a much higher, much better policy.",
            "So so unless you do sufficient exploration, there's no orders would be risky to switch from exploration to exploitation, so that's why it doesn't really help too much to use data.",
            "In the worst case to decide when to exploit.",
            "But it is possible to use data to decide when to explore.",
            "So this is the image is asymmetric."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This talk is about lifelong reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Emma Brunskill from CMU.",
                    "label": 1
                },
                {
                    "sent": "Um, and in case you're interested, our paper will be available on archive very soon.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me start with an example of lifelong reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have an intelligent tutoring system that tries to teach students how to master geography, for example.",
                    "label": 1
                },
                {
                    "sent": "Then when student Alice comes, you have some information about a student.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like what she has learned before in the grades in the past and then the system can take sequence of actions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A predefined set of actions.",
                    "label": 0
                },
                {
                    "sent": "For example this and then the goal, is to take the sequence of actions so that the total reward is maximized, meaning that the student Alice can learn the topic as soon as possible or as well as possible.",
                    "label": 0
                },
                {
                    "sent": "But in reality, so this is standard single task reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "But in reality Alice is not is probably not the first student that the system has taught in the past.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the system has sort of a number of students before and she's not the last one.",
                    "label": 0
                },
                {
                    "sent": "The system is going to teach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the system is now has the questions of how to buy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to benefit Alice from previous teaching experience with students in the past and how to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teach Alice to collect information so that system can benefit students in the future, so the system is now facing a lifelong reinforcement learning problem where each student is a task and a student comes in a sequence.",
                    "label": 0
                },
                {
                    "sent": "Farm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this talk, I'm going to formulate a task as a Markov decision process or MVP.",
                    "label": 1
                },
                {
                    "sent": "I guess everyone else so very quickly you have a set state set of actions, transitions, rewards in discount factor in every state you the system can take an action resulting in a reward whose expectation is given by the work function and the next state is sample from the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to maximize total reward.",
                    "label": 0
                },
                {
                    "sent": "So this is a single task or else setting.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lifelong or L. Here we focus on the specialized case of lifelong learning, so we're not claiming to solve general lifelong learning, but it's a very special case that as well as very good structure.",
                    "label": 0
                },
                {
                    "sent": "And so here the system, the learning agent.",
                    "label": 0
                },
                {
                    "sent": "Is given the spine itself, states opponents of action and a discount, so these things are known.",
                    "label": 0
                },
                {
                    "sent": "What is not known is that the agent is going to face a sequence of task.",
                    "label": 0
                },
                {
                    "sent": "Each one is drawn from a set of CMDPS, so this CMDS have the same stay and actions, but they may differ in the transition and reward functions.",
                    "label": 0
                },
                {
                    "sent": "And in life long learning the for that each task the agent observes are MVP while the agent is is encountered with the encounters with an MVP whose identity is unknown and it's given by the environment.",
                    "label": 0
                },
                {
                    "sent": "And then he's going to add in this MVP for each steps trying to maximize reward and such a.",
                    "label": 0
                },
                {
                    "sent": "And as I said there many kinds of lifelong learning problem and this is the special case that we focus here.",
                    "label": 0
                },
                {
                    "sent": "So you may wonder why is that?",
                    "label": 0
                },
                {
                    "sent": "Why does it make sense to assume there is finitely many amps?",
                    "label": 0
                },
                {
                    "sent": "The reason is that here we focus on fine.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "8 M DPS.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the finitely many amps with sufficiently different model parameters, meaning that only a finite number of policy czar are able to cover near optimal policies for all the MPs and with the same state elections.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the past people have study applications that are in this setting, for example in tutoring.",
                    "label": 0
                },
                {
                    "sent": "In teaching, people have realized students with different learning types can be taught in different ways.",
                    "label": 1
                },
                {
                    "sent": "Also in human robot interaction, it's helpful to identify user types that the robot can better assist the user.",
                    "label": 1
                },
                {
                    "sent": "Also in software task assistant will help patients etc also helps to identify user goals of the current task.",
                    "label": 0
                },
                {
                    "sent": "So All in all these examples your system is facing one of a finite number of possible MVP's and the goal is to identify the MVP and then maximize reward.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm an in lifelong reinforcement learning that it turns out that the two kinds of two kinds of exploration there is quite unique, so let's say empty is the current task or Alice in this teaching example, this is the past task that has been solved, and this is our future tasks.",
                    "label": 1
                },
                {
                    "sent": "Had to be the set of different types of MPs that has been discovered by the system in the last T -- 1 task.",
                    "label": 0
                },
                {
                    "sent": "Um, then then you there 2.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kinds of exploration were the 2 two goes that maybe they may conflict with each other.",
                    "label": 0
                },
                {
                    "sent": "The first one is within task learning, so this is a standard exploration in RL with.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goal is to maximize reward, but once in awhile need to explore promising states in order to converse to the two optimal policy.",
                    "label": 1
                },
                {
                    "sent": "So this is standard.",
                    "label": 0
                },
                {
                    "sent": "What is new here?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is that there is also the need for cross task knowledge transfer, meaning that once in awhile the agent needs to explore possibly order states in the new in the current task, not just to not for maximizing reward, but to identify possible.",
                    "label": 1
                },
                {
                    "sent": "Whether this is a new type of MDP and if it's new type then this information could be useful in the future to benefits solving future tasks so that overall this over sequence of Pete asked the total rewards maximized, not just.",
                    "label": 1
                },
                {
                    "sent": "The reward in this particular task.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For now we are facing a.",
                    "label": 0
                },
                {
                    "sent": "A mix of two kinds of exploration simultaneously.",
                    "label": 1
                },
                {
                    "sent": "So we have the crossed heart exploration exploitation tradeoff over within cars exploration.",
                    "label": 0
                },
                {
                    "sent": "Exploitation tradeoff?",
                    "label": 0
                },
                {
                    "sent": "So now the question is how we can balance these two kinds of tradeoff while in this task we are trying to balance exploration, exploitation as usual.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the how the online discovery problem comes to help.",
                    "label": 1
                },
                {
                    "sent": "So this is an abstraction of the cross pass expiration.",
                    "label": 0
                },
                {
                    "sent": "In the previous slide, the here's the problem.",
                    "label": 1
                },
                {
                    "sent": "The environment has a set of finite elements.",
                    "label": 0
                },
                {
                    "sent": "You can think of that isn't even MVP's.",
                    "label": 0
                },
                {
                    "sent": "The agent started with nothing, knowing nothing, knowing none of the possible types and then in T when in the round T the environment select one of these elements.",
                    "label": 0
                },
                {
                    "sent": "The agent has two choices you can.",
                    "label": 0
                },
                {
                    "sent": "Choose them, explore, have taken a equals one or two, exploit and explore.",
                    "label": 0
                },
                {
                    "sent": "Then then with high probability is going to discover this task identity over the task and it is normal task and you can add it to it as mhat the set of different types of MVP's that he has discovered so far.",
                    "label": 0
                },
                {
                    "sent": "Otherwise he doesn't discover that the task.",
                    "label": 0
                },
                {
                    "sent": "The identity of the task and then the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also the agent by choosing this action is given by this two by two loss matrix exploration, well exploitation the 1st Rd exploitation.",
                    "label": 0
                },
                {
                    "sent": "The columns correspond to whether the current task is normal task or not.",
                    "label": 0
                },
                {
                    "sent": "And the color of the agent is to maximize, minimize total loss over 2 rounds.",
                    "label": 0
                },
                {
                    "sent": "So here we specify the structure as follows.",
                    "label": 0
                },
                {
                    "sent": "The Ro Zero is less than 01 less than row two less than real quick.",
                    "label": 0
                },
                {
                    "sent": "So this is not we don't set the numbers arbitrarily, so in in the first row, when the agent decides not to explore, meaning that the agent tries to use information transfer from the past to do say, to accelerate solving current tasks.",
                    "label": 0
                },
                {
                    "sent": "Then if the task is turns out to be an 01, then transfer helps.",
                    "label": 0
                },
                {
                    "sent": "This is what so row one correspond to the loss of successful transfer.",
                    "label": 0
                },
                {
                    "sent": "That's why is the lowest one.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, row three correspond to the case where the agent decide not to do task identification or exploration, but the task is novel, so the agent may miss the opportunity to learn the auto policy in the current task, therefore suffering the maximum loss.",
                    "label": 0
                },
                {
                    "sent": "That's why row three is the largest one, and in between row one, row two that correspond to the action of trying to probe or trying to explore the current tasks.",
                    "label": 0
                },
                {
                    "sent": "So this costly, but they're not.",
                    "label": 0
                },
                {
                    "sent": "They're closely actions, but not as high as row 3.",
                    "label": 0
                },
                {
                    "sent": "So that's why we set this order here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to give two very simple algorithms for solving on a discovery, and then we'll use one of these two instant file to create and lifelong learning, lifelong reinforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "The first one for online discovery is called exploration.",
                    "label": 0
                },
                {
                    "sent": "Explore first is very simple.",
                    "label": 0
                },
                {
                    "sent": "It makes the stochastic assumption that the tasks are drawn IID from some unknown distribution over all possible or possible task.",
                    "label": 0
                },
                {
                    "sent": "And then so in these the kostic's case it can be shown that the best strategy would be to use all the exploration budget up front and then switch to exploitation.",
                    "label": 0
                },
                {
                    "sent": "So that's so.",
                    "label": 0
                },
                {
                    "sent": "That's why this algorithm makes sense.",
                    "label": 0
                },
                {
                    "sent": "So it has a threshold E before E. It always explores.",
                    "label": 0
                },
                {
                    "sent": "After that it switches to the exploitation mode and start doing exploitation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then if you know the minimum probability of task, probably the minimum probability of each task being drawn, then you can set it up properly by this and then you can show that the average loss of this algorithm is at most the optimal loss or the minimum loss plus an additional term of this.",
                    "label": 0
                },
                {
                    "sent": "As you can see, as T goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Term goes to 0, so this explore first algorithm is going to compete with the best or converges to the best strategy.",
                    "label": 0
                },
                {
                    "sent": "But it relies on this assumption and relies on the knowledge of minimum probability.",
                    "label": 0
                },
                {
                    "sent": "If Muse M is small than this term is big because one over mu is bigger.",
                    "label": 0
                },
                {
                    "sent": "So now let me.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give you a second algorithm for sexploration.",
                    "label": 0
                },
                {
                    "sent": "It doesn't require their stochastic assumption.",
                    "label": 1
                },
                {
                    "sent": "So in fact the set of MPs can be generated by an adversary in an arbitrary way.",
                    "label": 0
                },
                {
                    "sent": "The way it works is to prefix a set of Explorer sequence of nonincreasing exploration.",
                    "label": 0
                },
                {
                    "sent": "Rate ETA want 8 T and then for this for that eastep enjoying an exploration action from.",
                    "label": 0
                },
                {
                    "sent": "Well it decides where they do.",
                    "label": 0
                },
                {
                    "sent": "Exploration by joinable new Leaf with the success probability a dirty.",
                    "label": 0
                },
                {
                    "sent": "And then if you set 80 to be one over square root T, then you can show it can show that the average loss of this algorithm is again convergence to the optimal loss with an additional term of one that scales on the other one over square root T. So again, it converges the optimal policy, and in fact we have also a lower bound, showing that this is the best you can do, so there's nothing you can do that is better beyond this term.",
                    "label": 0
                },
                {
                    "sent": "So it means the force expiration is.",
                    "label": 0
                },
                {
                    "sent": "Optimal up to a constant factor.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so here I'm going to use force exploration to create a lifelong learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "This morning, so this algorithm in high level it, it's just forced exploracion.",
                    "label": 0
                },
                {
                    "sent": "This probability it tries to do exploration, otherwise it tries to exploitation.",
                    "label": 0
                },
                {
                    "sent": "So what is needed now is just filling the details of reinforcement, learning their messy.",
                    "label": 0
                },
                {
                    "sent": "But this is the high level message.",
                    "label": 0
                },
                {
                    "sent": "So for exploration we can run the algorithm called pack explore that tries to explore, try to reach every state in this MDP to identify whether this is a new task or not.",
                    "label": 0
                },
                {
                    "sent": "If it's new task, then we can.",
                    "label": 0
                },
                {
                    "sent": "Shut up to my set of normal tasks.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we can do an exploitation.",
                    "label": 0
                },
                {
                    "sent": "In this case we can use a variant of our Max who takes advantage of.",
                    "label": 0
                },
                {
                    "sent": "This assumes that the task is one of these tasks in this M hat, and instead of learning trying to learn while trying to explore every state, it just try to explore stays of are sufficient to identify which MVP we're in from this task.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm tries to do transfer learning.",
                    "label": 0
                },
                {
                    "sent": "Efficiently.",
                    "label": 0
                },
                {
                    "sent": "Optionally, you can also have some improvement.",
                    "label": 0
                },
                {
                    "sent": "For example, you tried to do an exploitation and you realize that the task is new.",
                    "label": 0
                },
                {
                    "sent": "Then you can always switch to the exploration mode.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the so this is 1 slide of theory, so for for lifelong reinforcement here we try to quantify the performance by the overall sample complexity.",
                    "label": 1
                },
                {
                    "sent": "So what is sample complexity?",
                    "label": 1
                },
                {
                    "sent": "It says that for any algorithm, we treat it as a stochastic, well, nonstationary policy and then for it every step we whether if this policy is sub optimal.",
                    "label": 0
                },
                {
                    "sent": "By this, with this vessel epsilon, then we call it a mistake and we try to bound the total number of mistakes in the whole library algorithm.",
                    "label": 1
                },
                {
                    "sent": "In this particular case, I would try to bound the total number of mistakes summed over all all the tasks.",
                    "label": 0
                },
                {
                    "sent": "And then this here's a theorem when the agent is is allowed to act in the MVP for long enough time when H is longer, then with high probability.",
                    "label": 0
                },
                {
                    "sent": "Here's a sample complexity of the agent.",
                    "label": 1
                },
                {
                    "sent": "It's so the first term as the number of tasks and the diameter D is the diameter, which is the steps required to reach the state from any other state in MDP and gamma is the number that measures the difference between two types.",
                    "label": 0
                },
                {
                    "sent": "Two MPs in the set.",
                    "label": 1
                },
                {
                    "sent": "On an essay is the number of state elections and N is the maximum number of possible next states.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, the first term is a dominating term as T goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The second term diminishes, so we're ending up with your synthetic performance of the first term.",
                    "label": 0
                },
                {
                    "sent": "That doesn't scale with the number of states where the size of MVP, but rather the complexity of the lifelong learning problem.",
                    "label": 0
                },
                {
                    "sent": "Steven, D and gamma.",
                    "label": 0
                },
                {
                    "sent": "That, in contrast, in single task RL you always suffer if you if you solve these tasks as from scratch using a single task learning algorithm, then there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a lower bound that shows that you can't beat the.",
                    "label": 0
                },
                {
                    "sent": "It has to be linear in a number of states.",
                    "label": 0
                },
                {
                    "sent": "A number of actions.",
                    "label": 0
                },
                {
                    "sent": "So by using my transfer knowledge now you're able to take advantage of the structure of the problem and then would result in the lower sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly give an example or experiment of how this may help.",
                    "label": 0
                },
                {
                    "sent": "We have a, so here's a 5 by 5 grid problem.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "So this is just approval for a concept experiment.",
                    "label": 0
                },
                {
                    "sent": "The shaded area knows center of the greatest star state and then we have the four MPs share the same star state.",
                    "label": 0
                },
                {
                    "sent": "They have the same noisy state transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "What is different in this MPs is the rewarding states.",
                    "label": 0
                },
                {
                    "sent": "So in this state is this one the green one?",
                    "label": 0
                },
                {
                    "sent": "The green one here in green.",
                    "label": 0
                },
                {
                    "sent": "One here for the last MPM four is similar to M3 except that it hasn't even greener or higher rewarding stays in this corner which make up this example so that now it's necessary to do sufficient exploration.",
                    "label": 0
                },
                {
                    "sent": "To realize the M4 is different 3.",
                    "label": 0
                },
                {
                    "sent": "And then we also hear the algorithms for comparison.",
                    "label": 1
                },
                {
                    "sent": "We have the force exploration that in this work also we have the explore first algorithm which correspond to the previous work.",
                    "label": 0
                },
                {
                    "sent": "We have two years ago and then and then there's also the one of the state of the art basin approaches for multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So tries to infer the posterior task identity and then follow the optimal policy of the maximum.",
                    "label": 0
                },
                {
                    "sent": "A posterior model in this posterior distribution, so it doesn't do very active exploration, but has been shown to be very.",
                    "label": 0
                },
                {
                    "sent": "Very strong in in a challenging game.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the first plot, this is the hardest case, where in the four MVP S one of them has very small task probability task generation probability bill M is very small for the basin one.",
                    "label": 0
                },
                {
                    "sent": "The Hierarchical multi task learning.",
                    "label": 0
                },
                {
                    "sent": "It's very good for our 4th exploration, the blue one it's converging and approaching the getting close an for the two.",
                    "label": 0
                },
                {
                    "sent": "Faced in that in two years ago it's not as good because it has.",
                    "label": 0
                },
                {
                    "sent": "To spend a long time in the first phase, two just to just to uncover all the possible tasks, because the main one is Milam small.",
                    "label": 0
                },
                {
                    "sent": "So Phase one is very very long.",
                    "label": 0
                },
                {
                    "sent": "And there's a gap here is explained by the possibility by the necessary of force expiration to continue doing exploration in every task.",
                    "label": 0
                },
                {
                    "sent": "So that's the price we pay for a constant exploration across task, and this will be helpful actually in another.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems, for example, when you have this changing distribution or when the sequence of classes generated by an adversary, then this is helpful.",
                    "label": 1
                },
                {
                    "sent": "Constant exploration is helpful, so again we have the two phase which is this phase one is bad.",
                    "label": 0
                },
                {
                    "sent": "And this one is the basic one is very strong from the first, but then when at this point we we we change the distribution of the MPs so that M4 comes into play.",
                    "label": 0
                },
                {
                    "sent": "It remember M4 is the same as M3, except that he has a much higher rewarding state and then because of the constant exploration algorithm, is able to catch up and realize then find the optimal policy but not the other algorithm.",
                    "label": 0
                },
                {
                    "sent": "The two phase.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's about to do discover, but it's not consistently.",
                    "label": 0
                },
                {
                    "sent": "Able to do so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so conclude so.",
                    "label": 0
                },
                {
                    "sent": "In this talk we emphasize two kinds of exploration.",
                    "label": 1
                },
                {
                    "sent": "There are immediate to needed in lifelong reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "For Cross task exploration we proposed would formulate the online discovery problem is an abstract way to to study this problem and then based based on an algorithm, an optimal algorithm for all online discovery.",
                    "label": 0
                },
                {
                    "sent": "We create an algorithm for lifelong reinforcement learning that has low sample complexity and also demonstrate the desired behavior in a proof of concept experiments.",
                    "label": 1
                },
                {
                    "sent": "So for future work it would be good to extend this work to function approximation or.",
                    "label": 0
                },
                {
                    "sent": "An to use prior knowledge rather than doing this worst case analysis here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the kind of talking.",
                    "label": 0
                },
                {
                    "sent": "Thanks for attention.",
                    "label": 0
                },
                {
                    "sent": "Maybe just one quick question because we're out of time.",
                    "label": 0
                },
                {
                    "sent": "So I have a question about the cross task exploration of your strategy, so it seems to me that you're deciding whether you transfer knowledge and art without knowing any information about the task at hand.",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "That's that's how we approach here.",
                    "label": 0
                },
                {
                    "sent": "So so, in which setting would this be realistic?",
                    "label": 0
                },
                {
                    "sent": "Well, it's so the reason that we should do this so there's so in the one I described, the algorithm I mentioned there is an improvement where if you decide to do explore exploitation and then realize that the task is normal, then you can switch to exploration.",
                    "label": 0
                },
                {
                    "sent": "So that's possible.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, you try to switch from exploration to exploitation.",
                    "label": 0
                },
                {
                    "sent": "Then it's risky.",
                    "label": 0
                },
                {
                    "sent": "It's showing the experiment where you have two MPs M3 and M. Or they they are almost identical except for one state when M4 has a much higher reward and then if you don't do a sufficient exploration then you can't.",
                    "label": 0
                },
                {
                    "sent": "You can realize that M4 has a much higher, much better policy.",
                    "label": 0
                },
                {
                    "sent": "So so unless you do sufficient exploration, there's no orders would be risky to switch from exploration to exploitation, so that's why it doesn't really help too much to use data.",
                    "label": 0
                },
                {
                    "sent": "In the worst case to decide when to exploit.",
                    "label": 0
                },
                {
                    "sent": "But it is possible to use data to decide when to explore.",
                    "label": 0
                },
                {
                    "sent": "So this is the image is asymmetric.",
                    "label": 0
                }
            ]
        }
    }
}