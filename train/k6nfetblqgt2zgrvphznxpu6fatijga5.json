{
    "id": "k6nfetblqgt2zgrvphznxpu6fatijga5",
    "title": "Introduction to Reinforcement Learning",
    "info": {
        "author": [
            "Csaba Szepesv\u00e1ri, Department of Computing Science, University of Alberta"
        ],
        "published": "March 17, 2008",
        "recorded": "March 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/mlss08au_szepesvari_rele/",
    "segmentation": [
        [
            "Approximately regular.",
            "And he will talk about spring forcement learning.",
            "He's from the University of Reinforcement learning, yes.",
            "Thank you, say.",
            "My talk is going to have three parts.",
            "One is about dynamic programming.",
            "And.",
            "So we start with that and I hope that we'll be able to finish and say, let's start."
        ],
        [
            "I.",
            "So first first of all, what reinforcement learning and why I am talking about dynamic programming is a free enforcement learning the 1st place.",
            "Well, right enforcement learning according to Rich who is also in the University of Reinforcement Learning.",
            "Is a.",
            "Every method that uses samples to solve optimal control problems can be considered strange first learning.",
            "So we take this definition and so therefore we are going to study first dynamic programming, which is sort of the basis forms the basis of the solutions for optimal control problems.",
            "So by the way, OK again, this is your chance to learn about reinforcement learning.",
            "Please stop me and be rude.",
            "Ask questions.",
            "I don't care.",
            "As someone else also said.",
            "There are no stupid questions there, only maybe stupid answers, so don't feel embarrassed if you don't understand something.",
            "I talked with some people.",
            "And then they told me that.",
            "That they don't have enough time to understand during lectures all this stuff and they want to look after lots and slides and they want to study this thing.",
            "And that's not a good approach.",
            "You are here to learn and this is your single chance to learn it.",
            "Maybe if you are not asking those questions, maybe this chance will be gone because you will be busy with other things and you will not have time to study in details.",
            "All these different fields.",
            "So just please stop me.",
            "Maybe we won't get to the end but but at least you will have a better foundation so.",
            "I hope it's clear so we can start.",
            "So first of all, why do we talk about reinforcement learning?",
            "What the excitement about reinforcement learning?",
            "Why do I care about reinforcement learning?",
            "So that's the first thing that I like to talk about and that that goes under the title defining AI and then we will deal with some specifics which go under the name of MDP's dynamic programming.",
            "And then we do a little bit of approximate dynamic programming."
        ],
        [
            "So far so good I I believe so.",
            "Regarding the literature, there are basically two books which are a little bit outdated at this time, so one book is by rich Sudden and Andy Bardo and you can see the book over there.",
            "And these are the guys who committed it.",
            "And so this is an introductory book targeted to generate code ANS and doesn't require much of mathematical background.",
            "The other book is is more like devoted to the theory part, and the title is neurodynamic programming, and you know this is just a sales title.",
            "This is really, really reinforcement learning what they're talking about.",
            "And they also saw Dimitri bicycles and an Chanticleers who did Alotta in the theory offering for small learning.",
            "And in particular, Dimitri Basic OS has worked on dynamic programming.",
            "I don't know since many, many years.",
            "I was probably not born when he was already working here.",
            "So regarding journals and conferences, the main channels Archie Miller emoji Chair and the I Journal also are relevant, but maybe a little bit less relevant regarding conferences that usual set.",
            "Basically the AI conferences, so the primary machine learning conferences, NIPS, and I see a mouth.",
            "And regarding theory, you want to look at code.",
            "There are sometimes papers there.",
            "And regarding AI kind of approaches, triple AI, HK and in UI, sometimes you will be surprised to find reinforcement learning papers.",
            "Quite a few papers appeared in."
        ],
        [
            "May I?",
            "So some more books.",
            "Of course.",
            "Reinforcement learning builds on on various fields like optimal control etc and so these other books give you.",
            "A little tougher.",
            "Out a little opportunity to look around and if you are not only interested in the core of reinforcement learning then then maybe it's a good idea to look at these books."
        ],
        [
            "OK, so some resources, so some software.",
            "So this is a this software this library.",
            "This is not really library.",
            "This is an API that's meant as a generic API.",
            "If you want to.",
            "Versus some reinforcement learning software.",
            "The idea is that this API is sort of standard API, and if you want to develop a new agent or a new environment, then maybe you want to consider using that API.",
            "And so this is a very thin layer and it supports multiple languages.",
            "So the second guy here is the reinforcement learning library.",
            "So this all comes from the University of Frame first.",
            "So from our University, so the reinforcement Learning Library has some algorithmica implementations as well, and there are quite a few of other good libraries.",
            "And there is a competition that is just ongoing and the test runs are going to be going to begin on the 1st of June, so it's still not too late to participate.",
            "And after seeing this lecture you will be in a perfect position to do that.",
            "So just do that.",
            "It's a lot of fun.",
            "So related fields, so operations, rich, research, control theory and simulation optimization, and these are the main journals for this related fields.",
            "Mathematics of operations research Operations Research is IEEE transaction on automatic control, automatica and those are the main control conferences, I, Tripoli.",
            "Decision control something?",
            "I forgot, CDC.",
            "And this is another interesting conference or a community and the conference, the winter Simulation conference.",
            "The only problem with these conferences is that they are the same time of NIPS.",
            "So not many people from our community.",
            "Are going to this conferences, but but otherwise these are pretty high quality conferences.",
            "So in simulation optimization, people are are studying pretty much the same issues of how to design better controllers by sampling basically."
        ],
        [
            "So where I'm at conferences, so defining the problem.",
            "So this figure shows the abstract control model, so this is how you should think about the problem.",
            "So you have a controller, an agent interacting with an environment through some actions, and it receives some feedbacks in the form of sensations and rewards rewards that part of the sensations.",
            "And so you see that.",
            "This is a closed loop system, so once you have specified the controller and the environment is given, then everything just throws out in time and everything is specified in the indefinite future.",
            "So that's why it's closed.",
            "That's why it's called a closed loop system.",
            "Sometimes this is called the perception action loop as well, and the purpose of the agent is to manipulate the environment in such a way that somehow the cumulative cumulative total reward received by the agent is maximized.",
            "I'm purposefully vague on defining the performance criterion at this stage.",
            "It will be.",
            "It will hopefully become later.",
            "Clear.",
            "So how to imagine problems like this?",
            "So we will see many examples of these problems, but any operations, research or control problem could be put into into this formulation.",
            "So this is this is very channery and so that could be one problem with it because it's so generic it's very tough as a problem."
        ],
        [
            "So, but this picture was a little bit vague and it was vague because.",
            "Depending so, you can put the boundary at different places, so you can put the boundary of this robot or agent.",
            "At the other place where there is a physical interaction with the environment like you have a robot and it has wheels and wheels are acting on the environment and the environment is is putting energy until the sensors of the robot.",
            "You can put the boundary there, but on the other hand, if you don't want to deal with the physical interaction but you mainly are interested with the software with the brain part, then probably you want to get somebody deeper.",
            "And this is what we are going to do.",
            "So this is the viewpoint that we are going to take.",
            "We don't want to care about the physical processes.",
            "We say that after this interaction happens with the environment, that will be some mechanism that transforms everything into your cities of bits or whatever.",
            "So we just want to care about the software part, the brain part.",
            "And so this figure shows shows how this is happening.",
            "So there are these external sensations coming in.",
            "And the actual agent that we do care about is somewhere inside and many things can happen just within the boundary of the robot.",
            "You know the physical boundary of the robot, so we do not necessarily care with the pure sensory readings, right?",
            "So we don't want to care about the voltages or whatever, so there will be some nice and clever mechanism that do some filtering, retraction of noise or whatever, but what's important for us is that.",
            "After this is done, we can really just care about the brain after robot.",
            "So in particular.",
            "It is good to keep in mind that you could have sensations coming from the XML word and the internal word of the robot.",
            "Let's put it this way.",
            "So what do I mean by that?",
            "So if you have your, if you have a body then maybe you have.",
            "I'm sorry, something like that.",
            "If you're robot and arms could have various joints and then you can measure the angles of the choice or the angular velocity of those joints and those would be internal sensations and and those sensations are really part of the sensations.",
            "So in particular there is one more thing that will hopefully become more clear.",
            "So we have we see here an arrow that says state.",
            "So in particular what happens is that there should be a mechanism in the robot that does some processing that constructs something that's called the state.",
            "So it was the state.",
            "So the state should be something like sufficient statistics for the future, and this will be our main assumption that that in this series of talks that that is some mechanism that's able to construct the state if we will see the consequences if there is no such mechanism.",
            "Or how you can approximate such a mechanism but but this will be our working hypothesis.",
            "And then you know this brain part just decides about the actions.",
            "But actions could be high level actions.",
            "They do not necessarily have to be really low level actions like turning the wheel by just one degree or something like that.",
            "So anyways, we are going to abstract away some details by."
        ],
        [
            "Proposing a mathematical model.",
            "So this is a generic mathematical model, so you have a a plant a controlled up trap.",
            "The state of the plant is developing according to this activation, so this is a very generic activation, so it has no structure or what server.",
            "I'm just saying that the next date is dependent on the previous state and the current action, and there is some noise injected into the system.",
            "So this state I was in a noisy way.",
            "And then what the robot can observe is not actually the state, but just something that depends on the state.",
            "And observations could be noisy.",
            "So there is this other function chi that takes the state and take some noise.",
            "So that's some disturbance and it returns an observation to the robot.",
            "Is this clear?",
            "So what's the state?",
            "So I said that the state is a sufficient statistics for the future, and I just defined these two acquisitions in a quiet, arbitrary manner.",
            "But if you define these two applications this way, then it automatically holds that the state is sufficient statistics for the future.",
            "But this could be a really problematic definition if you think a little bit more, because if you only have a robot and it has some sensations of the environment then.",
            "How do I construct something like acceptee?",
            "So how do I know that such an object exists?",
            "So first of all, the first idea is just to collect all the histories.",
            "So just to have everything.",
            "So we have a history that should be enough to determine the future.",
            "So if the system is crucial, like the actions like future future things happening in the future do not influence things happening Now, then the history should be a sufficient statistics.",
            "But the problem is the history is that it's not really compact, so you are looking for some compact representation of the history that's sufficient for predicting.",
            "Whatever is going to happen in the future depending on of course, what actions you want to take.",
            "So that that would be the definition.",
            "But what happens is that you can define the state in two different ways and traditionally in control theory people define the state independently of what what you measure, so you matching that your robot could be accurate with as many sensors as he wanted.",
            "So.",
            "You could put a on the robot at sensor GPS sensor, let's say or you could put a camera and a robot.",
            "Are just imagine equipping that the robot with many many sensors and then in the limit if you just put all these sensors and robot then the state is defined as sufficient statistics with respect to the set of all those measurements.",
            "OK?",
            "So there's the traditional definition of upstate and well.",
            "This is a little bit problematic because if you have a robot, then pressure bleed your boat so you don't have space to put that many sensors and robot.",
            "First of all, right?",
            "So that's just impossible to do.",
            "Then why do you care?",
            "So when I say it's a sufficient statistics for the future, what should I care about?",
            "I should only care about what the robot is able to upset.",
            "I should not care about what.",
            "Unobservable, right?",
            "So if the robot doesn't have any kind of some sort of sensors like it's not able to sense its position with respect to the universe, why do I care about the position of the robot with respect to the universe, right?",
            "So I should not care about that.",
            "So if you define the state as a sufficient statistics with respect to the measurements with things that the robot can observe, then you get what's called subjective state.",
            "From our point of view, we don't care about this distinction distinction anymore, I just wanted to be very clear about what do we mean by state.",
            "Either definition of states is good enough for us.",
            "Just say that we have we have a state somehow.",
            "OK, so then, what's the purpose of of control and what's a controller?",
            "So the controller is basically just taking past measurements past observations, and it's returning an action.",
            "AB, so it's such a controller, cycled, feedback controllers, and So what was on the previous slide is that if you feedback this action into these activations, then you get what's called a closed loop system.",
            "It's closed loop as opposed to open loop, because the future is completely determined by capital F&F&G.",
            "And the resulting control is called closed loop controls and design problem is how to choose F and in reinforcement learning we care about the problems that can be formulated.",
            "Something like this at every time step you are receiving some reward depending on observations and actions maybe future like one step ahead observations or whatever.",
            "This is not very important.",
            "And maybe you are just summing up this device and you want to maximize this rewards the sum of this rewards, so it's important to realize that you are not not maximizing the immediate reward, you are maximizing the sum of the rewards and capital T should not be taken too seriously here, so that some people do not see the bottom, I believe.",
            "I don't know what to do with that, so maybe I can.",
            "Is not good this one.",
            "I don't know.",
            "It's a little bit too much.",
            "It would be, but this work.",
            "The cookie OK. Where is the cookie this?",
            "Is it in the box?",
            "Do you want to do?",
            "Tear.",
            "Wow, wow, a technical solution, Bravo.",
            "Do people see in the back at the bottom?",
            "Yeah.",
            "So that.",
            "Uh huh.",
            "Looks good.",
            "So you don't want to just maximize the immediately bar so the immediate reward is called.",
            "Just you're at time step T and you are receiving this reward, so that's called the immediate reward.",
            "And maximizing that is a different problem.",
            "So if you maximize this long term cumulative reward, this makes the problem really challenging, and so as an example, let's consider navigation.",
            "So you have a little robot that should get to the.",
            "I don't know why robots would go to the beach, but anyway, so that seems to be that like the idea target here.",
            "So you have a robot that wants to go to the beach.",
            "And so, how do you formulate this problem?",
            "To formulate this problem by defining a reward of one of the robot gas to the beach.",
            "Otherwise the reward received is just zero.",
            "This might not be actually a very good idea.",
            "If you really want to solve a navigation problem, because the reward in this case is sparse, but.",
            "On the other hand, it just works if the robot is able to pick a controller that maximizes the cumulative total reward, then the only way it could achieve reward not zero is to go to the beach, right?",
            "But if the robot wants to just maximize the immediate reward when the robots here, it cannot fly, unfortunately, so there is no teleportation action and the robot, so it can go too.",
            "Like name sort of just somewhere closing the neighborhood so that you can move smoothly in the environment so the immediate reward received will be 0 for a long long time before it gets a one.",
            "Right, so it has to plan ahead in time in order to find out the the sequence of actions or or is this maybe this function capital F such that at the end it gets to the beach?",
            "Is it clear?",
            "This is an important distinction.",
            "I."
        ],
        [
            "So limit of classification of controllers, so some so in the early days of of control people looked at what's called feedforward control, where the sequence of actions is decided ahead in time.",
            "So is this a good idea?",
            "What do you think?",
            "OK so yeah.",
            "So if you have a robot that that is perfect in executing the actions and the effects of the actions are just just as expected, always that this might work right, but what if you have some noise in the environment?",
            "So there are some unexpected changes happening in environment, so is this going to work?",
            "Do you expect this to work?",
            "Not quite right.",
            "So the problem is.",
            "This approach is that if there is some uncertainty in the environment, there is some noise there.",
            "Then the robot can well end up in maybe Sydney or whatever.",
            "Instead of going to the beach and so you need something that's called a feedback controller.",
            "So purely reactive system would just act based on its observations and some people in robotics are.",
            "Publicizing this view that that you should really try to do this.",
            "You don't need any memory and whatever, so what's the problem with this approach?",
            "So what do you think?",
            "Sufficient support?",
            "For example.",
            "Works the same, so yeah.",
            "So then then you cannot have based on your observationes you cannot be very clever about which way you want to go, right?",
            "So purely interactive systems might work in for some limited tasks, but in general you need something more and what's what's more is is memory.",
            "So basically what you want to do is to build up some memory, and basically when you build up some memory, what you do is that you are interpreting your sensations and then based on the memory you want to come up with the decision.",
            "And the decision-making itself could take some time, in which case it's called the deliberative control.",
            "Or it could be just, you know, one shot and it's done.",
            "It's called reactive control.",
            "We don't really care about this distinction in the rest of the talk, just wanted to mention so."
        ],
        [
            "Regarding feedback controllers, so we sent it.",
            "It might be a good idea to build up this memory so the generic form of building up the memory is just that.",
            "You know you have your previous memory state and, well, I'm sorry this should be set of TI guess, so this is an observation.",
            "If you have your observation you have your previous action and you just update your memory based on your memory you come up with some action.",
            "And then you can realize that, well, there could be a connection here to Ronaldo's talking about previously.",
            "So if we said that acts of tea is a state, so accepte is sufficient to know anything about the future.",
            "In particular, it's sufficient to do any kind of planning you want it to do.",
            "So wouldn't it be a good idea to have had a memory buildup that tries to approximate the state?",
            "It seems that, well, that should work.",
            "So here you can connect to filtering and state estimation and interest.",
            "I'm just going to assume that there is some mechanism that's already doing that.",
            "So I don't care about how this is done.",
            "Particle filtering is is great.",
            "That could be a really good solution for this.",
            "I just assumed that that we somehow has estimated the state.",
            "In particular, I don't.",
            "I will not at the beginning I will not care about errors resulting from state estimation, so I will just.",
            "Mimic that, but we have the state information.",
            "So if it's if you accept this, OK, So what I'm doing here is that I'm simplifying the problem from its functionality in various ways.",
            "And and I want to get to a place where I can really do something so we will see that even if we do this, if we assume that the state is measurable, the problem is going to be really tough.",
            "And so we are going to study that problem in quite a detail and then maybe later, if you have time, we can come back to this even tougher problem, yes?",
            "New action.",
            "OK, so yes.",
            "OK, so that's true, but.",
            "So the the length of the observation.",
            "So the length of the history is growing this time.",
            "So if you want to somehow get away with bounded computation resources.",
            "Then somehow you want to compress the histories.",
            "So the problem is that it's pretty hard to generalize across histories and.",
            "See, that's the primary reason.",
            "OK.",
            "This is a good question.",
            "I should get Cookie.",
            "Sorry.",
            "So please ask more questions like that because it seems that I need some practice during this cookies.",
            "OK, how much do I owe you?",
            "Uh, no.",
            "Sorry, sorry, sorry.",
            "I didn't know how far it goes.",
            "I'm going to talk about this.",
            "This is exploration.",
            "OK, but Luckily you have already finish your talk, so we're saying OK, so if we are sad that we have state information, then the next question is where?",
            "How do you compute a good action?",
            "And well, basically there are two approaches, so one is called the model based control.",
            "And the and the other is modific can't roll in the case of model based control.",
            "You somehow try to estimate the model the dynamics.",
            "OK, so we're trying to estimate the model, so if you forgot about the observation acquisition because we said that.",
            "We will serve the state then what is left is just act.",
            "So if you knew F then this would be just a planning problem.",
            "Just the planning problem.",
            "So we talk about that.",
            "We have some gas.",
            "They should ask questions, I'm sorry.",
            "So why should we set up T?",
            "As I said previously?",
            "Sorry.",
            "So.",
            "The problem is that.",
            "That different notations in different communities and wiate in control typically denotes observation and.",
            "510 to use a contract here annotation, but here in MDPSY FT sometimes Dinos the next state and so then everything would be confused and so unfortunately this state.",
            "Somehow they survived.",
            "Somehow the changes that I made to the slides many times.",
            "So anyway, so there are these two different ways of approaching the problem.",
            "So one approach is to estimate the model.",
            "So you say that you said that.",
            "We assume that the state is observable.",
            "And.",
            "Then you could try to just estimate F. So given the current XFT current AF T and the next X of T, this is just if you want the regression problem right, or maybe a density estimation problem because the next state could be stochastic an, so you want to know the next state distribution and not just the mean next state.",
            "So then you can build up that model and once you have built.",
            "The model just use a technique that assumes that you are given a model and just solve for the optimal control.",
            "We will see some approaches that work like that or that.",
            "They assume that you are given a model, will see that.",
            "Even if you have, if you are given a model, coming up is a good action is a highly nontrivial problem.",
            "So surprisingly, there is this other approach which is called the model free approach.",
            "When you're not building a model at all, just somehow magically come up with a with a good controller, well, it's not magical at all, so.",
            "One very simple way of doing that is that why you say that I'm going to have this capital F function parameterized with some parameters and parameters live in some parameter space Euclidean space, and I just view this problem as an optimization problem.",
            "I can use a gradient asked procedure to optimize my parameters, and so that's called policy search.",
            "So that's one approach, but there are some other approaches and we are going to take.",
            "I'll look at them.",
            "It's it's very interesting.",
            "So what was the deal here?",
            "Which approach should be better?",
            "Now we don't know.",
            "We have no idea to tell you the truth.",
            "So if you have a.",
            "A good prior knowledge about the what the model might be that you want to use that knowledge so that it's the odds the same old story if you have some prior knowledge you want to put that prior knowledge into the system and use it.",
            "If you don't have such a prior knowledge, then while you can still try both approaches then you have to somehow do this in a non parametric way.",
            "And the problem there is that one when you're building the model, you will introduce some errors and those errors are going to propagate through the planning phase and it's not very much clear from the point of view of a particle application which approach should be preferred.",
            "So I cannot have too much.",
            "So one thing here though is that building model relied heavily on being able to estimate the state or node state, and so one problem with that approach could be that if that assumption is not met.",
            "Or is violated?",
            "Strongly Dan, your models could be way off and you are planning solution is is going to be way off as well, yeah?",
            "Is that the model?",
            "Patiently more efficient but data equations in the model based ones are very data efficient, but computationally copper.",
            "Yes, so this is sometimes true, yeah?",
            "Because you can come up with model free approaches that like to be data efficient an it's it's.",
            "It's largely unclear to me if this is in China tree, but it's true that some of the model free approaches are not data efficient and all but it depends on the design of the algorithm.",
            "If the nature of rewards is changing overtime so that.",
            "Now read things are rewarding and later on we're coming up the green things as rewarding.",
            "So yeah, with the Model 3 with you if you don't know red or green but you're just getting the policy to get there, then when the the natural reward changes then suddenly not working very well.",
            "Whereas if he got model based, presumably you've learned the dynamics of the world Sebokeng idea how to get from here to something.",
            "Yeah, so first of all, if the reward changing that's outside of the scope of this model.",
            "So we said that the reward is dependent on, let's say in the.",
            "In this case the state and an action.",
            "So your state information is not correct or not complete.",
            "If you can say that the reward is changing.",
            "So this is a limitation of the model and some people are looking at extensions of this model.",
            "But on the other hand, well, it's true that the model based approach could be more ad vantages.",
            "If you have, maybe let's say multiple task.",
            "It's not.",
            "The reward function is changing, but you have the same dynamics but you could like you have the same robot but robot could fulfill multiple tasks and then you learn the dynamics for the for the robot and so you can use the data more efficiently in that case so.",
            "OK cookie.",
            "Thank you.",
            "Very good.",
            "So yeah.",
            "So you can do that, but the problem with just extending the state variable indefinitely or or to make it very large is then that it's not obvious how to generalize then in high dimensional spaces.",
            "So you added this new component.",
            "But the new component you have this prior knowledge about the new component.",
            "It only influences the rewards.",
            "So then if you don't use that prior knowledge, you don't build that prior knowledge into the way you're estimating things.",
            "Then you made the problem unnecessarily harder.",
            "So if you extend the state and you do that, then it looks like as if you didn't do anything with the state, right?",
            "So yes, I know but.",
            "Different tactics.",
            "Robot yeah.",
            "No.",
            "So we're making very strong assumptions here, but you'll see that despite a strong assumption, this is a tough problem and you'll enjoy it.",
            "I just thinking about it and then.",
            "Well then you can think about relaxing this assumptions.",
            "OK, so the next part.",
            "So that was sort of the introduction OK, and So what does it?",
            "What does this have to do with AI?",
            "So one hope too.",
            "To approach AI is to to formulate the problem with the with this very generic formulation and just have a learning system that able to, you know, solve the whole thing.",
            "And I don't know if Marcus you are going to talk about that next week.",
            "OK, so wait for the talk of Marcus to to learn more about this next week.",
            "I guess it's Monday, right?",
            "OK, but we're not going to that far today, so we are going to look at MVP's next.",
            "So what's an MDP?",
            "So mathematically, an MDP is just a fourth Apple, you have a set of states, a set of actions, transition probabilities, and rewards.",
            "And so this is just yet another way of formalizing what we said before, so the set of states is, you know, just all possible states that the agent can encounter the set of actions is just all possible controls.",
            "The agent can execute the transition probability and codes how the state changes overtime.",
            "So this is a probability distribution.",
            "And for the sake of simplicity, I'm going to talk a lot about finite MDP's.",
            "But many of the conclusions that we all have just straightforwardly generalizes to infinite and EPS should not vary that much about infinite State space in its infinite action spaces at this stage.",
            "But for simplicity, video with finite MDP's so that I can say that this is just a probability distribution over possible wise so that this distribution sums to one if I sum it over Y, right?",
            "So given X current state an given a current action that the agent has just taken, was the probability distribution over the next 8, so that that is what this object is giving to us.",
            "Transition probabilities is that clear so.",
            "So OK, so I explained different way.",
            "So if you choose an action you have chosen an Markov transition kernel, right?",
            "So you have you are somewhere in the state.",
            "And you have chosen action and your next state is a random quantity.",
            "What is this?",
            "What is the distribution of this random quantity?",
            "The transition probabilities are giving you this distribution clear.",
            "Yeah, so the next time step and when you executed the action, you are going to upset by.",
            "The next state.",
            "So you feed it back to the agent.",
            "Fire.",
            "They have to be over watered well.",
            "It could include the rewards, but traditional divorce is not included so the reward is something that's derived from the state in this formulation.",
            "The state just anchors all the information that's needed to to predict the future.",
            "So if you.",
            "If you start saying that the reward is going to change, then the state is not complete.",
            "Things like that, so if you.",
            "So if you if you are not able to.",
            "If you see that the environment is changing or somehow there is some non stationarity then that's usually caused because the state variable is not complete.",
            "So you want to extend the state variable.",
            "You only want to just have a better description of what the pass was, yes.",
            "OK, so this is just a mathematical framework right model and we talk about learning and that kind of stuff and then it will turn out that.",
            "Typically you don't deal with the stares directly at all.",
            "You deal with observations only, so you have your sensors and based on the sensory information, maybe you do some feature extraction that involves computations which are history dependent or whatever.",
            "So you say your features and your features are discribing so.",
            "So to say the state or or at least what you know about the state.",
            "And many of the arguments are going to work at the level of features.",
            "But in order to understand how these arguments are designed and the properties of these arguments, we consider this mathematical model.",
            "When you imagine or the set of all States and everything.",
            "OK.",
            "So these are transition probabilities and rewards, so sometimes people.",
            "Define the words immediate rewards as dependent on the current state, current action and the next state.",
            "Sometimes they just define it as dependent on current state and the current action or just current state.",
            "All definitions are good.",
            "There is not much of a difference between them.",
            "Yeah.",
            "You are dependent on the observation, yes.",
            "Yeah, because I said that the observation is going to be equal to the state.",
            "So that was the conclusion of the last slide, so that you could do this memory building process when you are estimating the state.",
            "But instead of considering that in full detail, we just say that we observe the state in the first place.",
            "OK.",
            "So that's why the reward is dependent on the state right here.",
            "And so.",
            "Being more specific about the kind of problems that we are going to consider, we are going to consider what's called the total expected discounted reward criterion criterion and in order to specify that you need some another quantity that's called discount factor.",
            "So I'm going to be more specific about the role of discount factor and right here on the next slide.",
            "So what does it all this?",
            "Our model means, so there is another view of of this model, and that's the process you.",
            "So you have a stochastic process, acts of TFT and RFT.",
            "By the way I'm going to use mostly capital letters to denote stochastic quantities as usual in mathematics.",
            "So access the FT NRT are your current state and current action and the current immediately reward that you have received in the current timestamp.",
            "So this has stochastic process.",
            "These are random variables that are indexed by time, so by stochastic process we mean that.",
            "So.",
            "And so, how does what are the laws determining the?",
            "The development of the stochastic process.",
            "So this is how it's connected to the previous slide, so the next state XD plus one is distributed according to the transition probability kernel.",
            "So we have here P and so P determines given access T&AFT at probability distribution.",
            "So the next state, which is distributed according to that kernel.",
            "OK, So what about the current action?",
            "So the action is chosen based on the history, so that's the most generic form of our motion, Rick way off of choosing an action.",
            "So you take a look at your history and you compute a probability distribution over the action.",
            "So that's \u03c0.",
            "And and then you just throw randomly an action from that probability distribution.",
            "Now you can wonder about why do we care about choosing actions in a stochastic manner, and later it will be clear that that if you do learning then then probably you want to do that to some extent.",
            "Or it might be a good idea to do that, but otherwise for many of the developments that we will do here, we don't need stochastic policies at all, so the policy could be just, you know, this distribution could be adjusted direct data assigning an infinite mass to some action and zero mass to all the other actions.",
            "So deterministic it should just choose an action given the history.",
            "So that's a policy, and the history is about the history.",
            "Everything that you have seen so far, and the reward is, well, it could be that the reward.",
            "So the River could be stochastic, but here we just say that, well, let's use this model at the reward is dependent on the current state, current action, and next state.",
            "And we just say that the reward is just read out by this reward function.",
            "So this is the way.",
            "Things are developing in time.",
            "Once you have fixed the controller and once you are given an MDP.",
            "Is that clear?",
            "Yep.",
            "Sometimes.",
            "Something actually will say good, but it's not really.",
            "For example you have chest.",
            "Bad, but getting them to tack on your faces.",
            "So robots are rewarded for taking a certain place.",
            "Then I left him a traffic turned out without.",
            "So.",
            "The reward function.",
            "The choice of the reward function is is is is sometimes non trivial.",
            "In the case of chess, the trivial reward function would be just to give a reward of of unity to the agent when he has won the game and give it 0 otherwise, and give it mine.",
            "Give it minus one if it has lost the game.",
            "So if you do this encoding then those problems go away if you try to somehow shape the reward function to have the agent with its immediate decisions, then.",
            "Yeah, then then it's a non trivial problem.",
            "How to do that?",
            "So right now we are going to assume that the reward function is given some clever engineer designed the reward function.",
            "It's an auto to define good reward functions for real real world problems, except in some problems like operations research problems or control problems where the reward function is very clear.",
            "And so, regarding this choice of reward function of, if you got a 0 pretty much everywhere except at the end of the game.",
            "So as I said, this makes the problem really really tough.",
            "So in practice, if you use a reward function like that, then learning could be very slow.",
            "But if you don't have any better idea than that, then maybe that's still better than to come up with some surrogate reward function that could mislead the agent in a very bad way.",
            "So maybe what you are differing, so maybe we can come back to your question later when we talk about value functions, because I think value functions are more related to your question.",
            "Yeah.",
            "Well, OK, you have to wait one more step.",
            "Before you see it, receive the reward, but it's not a big deal.",
            "Well, basically the assumption is that that the delay is not very large.",
            "It cannot be that if you are not receiving the rewards like indefinitely, then.",
            "Then then.",
            "Well, you should add that well, I would say we assume that.",
            "I cannot see.",
            "So you receive rewards every time step.",
            "OK, so this model shows that.",
            "And you can maybe band that a little bit, but I don't know how much value that has.",
            "Because so the key point here is that you are not looking at maximizing immediate reward.",
            "You are going to sum up all these rewards.",
            "And look at the sum and maximize.",
            "Let's say the expected value of the sum.",
            "OK.",
            "The state depends on all your previous actions, right?",
            "So that's a there's a thing.",
            "So when you are executing an action.",
            "You are not interested in the immediate reward received.",
            "You are trying to influence the state in such a way that you go to the path of the state space where the future rewards are really great.",
            "This is what you are trying to do.",
            "OK. Why?",
            "So this Y this one here right?",
            "So they just.",
            "Able that that's there, filling in position.",
            "OK.",
            "So was the control problem.",
            "So what was the goal of control?",
            "So if you are given a policy, then you can define this quantity.",
            "You assume that you start at some state XO, so the 1st at the first time step the state is just something little X and then you execute policy \u03c0.",
            "So that generates the sequence of random states, random actions, random rewards.",
            "So what you do is that you just sum up the rewards.",
            "Maybe you use discounting the way is shown here.",
            "So this is the discount factor here.",
            "And you sum up the rewards up to infinite.",
            "And you take the expectation of that.",
            "And so this is going to define a value.",
            "So that's called the value of state X.",
            "So what is this defines is that once the future total discounted expected return if I start my policy Pi instead X?",
            "So this is the value of the policy at that state.",
            "OK. Yeah, so when you say stop.",
            "Red State Bank.",
            "Look to see what action.",
            "Yes.",
            "So the agent is put by some external force or some some hole in that particular state, and then it starts to execute the given policy.",
            "Good.",
            "OK.",
            "So.",
            "Optimal value function is just you take the maximum with respect to all possible policies of the value function in every state.",
            "So you fix a given state and then this Maxim is over all possible policies, so there's the best possible value that you could hope for, right?",
            "You cannot have a policy that does anything.",
            "Oh, annex accent better than we start up X.",
            "Right, and the goal is going to be to find a policy such that the value function of the policy matches the optimal value function.",
            "There's a goal.",
            "And maybe this is a good.",
            "Well, OK, maybe this is a good place to stop and.",
            "So see you after break.",
            "To speed up."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approximately regular.",
                    "label": 0
                },
                {
                    "sent": "And he will talk about spring forcement learning.",
                    "label": 0
                },
                {
                    "sent": "He's from the University of Reinforcement learning, yes.",
                    "label": 1
                },
                {
                    "sent": "Thank you, say.",
                    "label": 0
                },
                {
                    "sent": "My talk is going to have three parts.",
                    "label": 0
                },
                {
                    "sent": "One is about dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So we start with that and I hope that we'll be able to finish and say, let's start.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So first first of all, what reinforcement learning and why I am talking about dynamic programming is a free enforcement learning the 1st place.",
                    "label": 0
                },
                {
                    "sent": "Well, right enforcement learning according to Rich who is also in the University of Reinforcement Learning.",
                    "label": 0
                },
                {
                    "sent": "Is a.",
                    "label": 0
                },
                {
                    "sent": "Every method that uses samples to solve optimal control problems can be considered strange first learning.",
                    "label": 1
                },
                {
                    "sent": "So we take this definition and so therefore we are going to study first dynamic programming, which is sort of the basis forms the basis of the solutions for optimal control problems.",
                    "label": 0
                },
                {
                    "sent": "So by the way, OK again, this is your chance to learn about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Please stop me and be rude.",
                    "label": 0
                },
                {
                    "sent": "Ask questions.",
                    "label": 0
                },
                {
                    "sent": "I don't care.",
                    "label": 0
                },
                {
                    "sent": "As someone else also said.",
                    "label": 0
                },
                {
                    "sent": "There are no stupid questions there, only maybe stupid answers, so don't feel embarrassed if you don't understand something.",
                    "label": 0
                },
                {
                    "sent": "I talked with some people.",
                    "label": 0
                },
                {
                    "sent": "And then they told me that.",
                    "label": 0
                },
                {
                    "sent": "That they don't have enough time to understand during lectures all this stuff and they want to look after lots and slides and they want to study this thing.",
                    "label": 0
                },
                {
                    "sent": "And that's not a good approach.",
                    "label": 0
                },
                {
                    "sent": "You are here to learn and this is your single chance to learn it.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you are not asking those questions, maybe this chance will be gone because you will be busy with other things and you will not have time to study in details.",
                    "label": 0
                },
                {
                    "sent": "All these different fields.",
                    "label": 0
                },
                {
                    "sent": "So just please stop me.",
                    "label": 0
                },
                {
                    "sent": "Maybe we won't get to the end but but at least you will have a better foundation so.",
                    "label": 0
                },
                {
                    "sent": "I hope it's clear so we can start.",
                    "label": 0
                },
                {
                    "sent": "So first of all, why do we talk about reinforcement learning?",
                    "label": 1
                },
                {
                    "sent": "What the excitement about reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "Why do I care about reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "So that's the first thing that I like to talk about and that that goes under the title defining AI and then we will deal with some specifics which go under the name of MDP's dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "And then we do a little bit of approximate dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far so good I I believe so.",
                    "label": 0
                },
                {
                    "sent": "Regarding the literature, there are basically two books which are a little bit outdated at this time, so one book is by rich Sudden and Andy Bardo and you can see the book over there.",
                    "label": 0
                },
                {
                    "sent": "And these are the guys who committed it.",
                    "label": 0
                },
                {
                    "sent": "And so this is an introductory book targeted to generate code ANS and doesn't require much of mathematical background.",
                    "label": 0
                },
                {
                    "sent": "The other book is is more like devoted to the theory part, and the title is neurodynamic programming, and you know this is just a sales title.",
                    "label": 0
                },
                {
                    "sent": "This is really, really reinforcement learning what they're talking about.",
                    "label": 0
                },
                {
                    "sent": "And they also saw Dimitri bicycles and an Chanticleers who did Alotta in the theory offering for small learning.",
                    "label": 0
                },
                {
                    "sent": "And in particular, Dimitri Basic OS has worked on dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "I don't know since many, many years.",
                    "label": 0
                },
                {
                    "sent": "I was probably not born when he was already working here.",
                    "label": 0
                },
                {
                    "sent": "So regarding journals and conferences, the main channels Archie Miller emoji Chair and the I Journal also are relevant, but maybe a little bit less relevant regarding conferences that usual set.",
                    "label": 0
                },
                {
                    "sent": "Basically the AI conferences, so the primary machine learning conferences, NIPS, and I see a mouth.",
                    "label": 1
                },
                {
                    "sent": "And regarding theory, you want to look at code.",
                    "label": 0
                },
                {
                    "sent": "There are sometimes papers there.",
                    "label": 0
                },
                {
                    "sent": "And regarding AI kind of approaches, triple AI, HK and in UI, sometimes you will be surprised to find reinforcement learning papers.",
                    "label": 0
                },
                {
                    "sent": "Quite a few papers appeared in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "May I?",
                    "label": 0
                },
                {
                    "sent": "So some more books.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning builds on on various fields like optimal control etc and so these other books give you.",
                    "label": 0
                },
                {
                    "sent": "A little tougher.",
                    "label": 0
                },
                {
                    "sent": "Out a little opportunity to look around and if you are not only interested in the core of reinforcement learning then then maybe it's a good idea to look at these books.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so some resources, so some software.",
                    "label": 0
                },
                {
                    "sent": "So this is a this software this library.",
                    "label": 0
                },
                {
                    "sent": "This is not really library.",
                    "label": 0
                },
                {
                    "sent": "This is an API that's meant as a generic API.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "Versus some reinforcement learning software.",
                    "label": 0
                },
                {
                    "sent": "The idea is that this API is sort of standard API, and if you want to develop a new agent or a new environment, then maybe you want to consider using that API.",
                    "label": 0
                },
                {
                    "sent": "And so this is a very thin layer and it supports multiple languages.",
                    "label": 0
                },
                {
                    "sent": "So the second guy here is the reinforcement learning library.",
                    "label": 0
                },
                {
                    "sent": "So this all comes from the University of Frame first.",
                    "label": 0
                },
                {
                    "sent": "So from our University, so the reinforcement Learning Library has some algorithmica implementations as well, and there are quite a few of other good libraries.",
                    "label": 0
                },
                {
                    "sent": "And there is a competition that is just ongoing and the test runs are going to be going to begin on the 1st of June, so it's still not too late to participate.",
                    "label": 0
                },
                {
                    "sent": "And after seeing this lecture you will be in a perfect position to do that.",
                    "label": 0
                },
                {
                    "sent": "So just do that.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of fun.",
                    "label": 0
                },
                {
                    "sent": "So related fields, so operations, rich, research, control theory and simulation optimization, and these are the main journals for this related fields.",
                    "label": 1
                },
                {
                    "sent": "Mathematics of operations research Operations Research is IEEE transaction on automatic control, automatica and those are the main control conferences, I, Tripoli.",
                    "label": 0
                },
                {
                    "sent": "Decision control something?",
                    "label": 0
                },
                {
                    "sent": "I forgot, CDC.",
                    "label": 1
                },
                {
                    "sent": "And this is another interesting conference or a community and the conference, the winter Simulation conference.",
                    "label": 0
                },
                {
                    "sent": "The only problem with these conferences is that they are the same time of NIPS.",
                    "label": 0
                },
                {
                    "sent": "So not many people from our community.",
                    "label": 0
                },
                {
                    "sent": "Are going to this conferences, but but otherwise these are pretty high quality conferences.",
                    "label": 0
                },
                {
                    "sent": "So in simulation optimization, people are are studying pretty much the same issues of how to design better controllers by sampling basically.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where I'm at conferences, so defining the problem.",
                    "label": 0
                },
                {
                    "sent": "So this figure shows the abstract control model, so this is how you should think about the problem.",
                    "label": 1
                },
                {
                    "sent": "So you have a controller, an agent interacting with an environment through some actions, and it receives some feedbacks in the form of sensations and rewards rewards that part of the sensations.",
                    "label": 0
                },
                {
                    "sent": "And so you see that.",
                    "label": 0
                },
                {
                    "sent": "This is a closed loop system, so once you have specified the controller and the environment is given, then everything just throws out in time and everything is specified in the indefinite future.",
                    "label": 0
                },
                {
                    "sent": "So that's why it's closed.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called a closed loop system.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this is called the perception action loop as well, and the purpose of the agent is to manipulate the environment in such a way that somehow the cumulative cumulative total reward received by the agent is maximized.",
                    "label": 0
                },
                {
                    "sent": "I'm purposefully vague on defining the performance criterion at this stage.",
                    "label": 0
                },
                {
                    "sent": "It will be.",
                    "label": 0
                },
                {
                    "sent": "It will hopefully become later.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "So how to imagine problems like this?",
                    "label": 0
                },
                {
                    "sent": "So we will see many examples of these problems, but any operations, research or control problem could be put into into this formulation.",
                    "label": 0
                },
                {
                    "sent": "So this is this is very channery and so that could be one problem with it because it's so generic it's very tough as a problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but this picture was a little bit vague and it was vague because.",
                    "label": 0
                },
                {
                    "sent": "Depending so, you can put the boundary at different places, so you can put the boundary of this robot or agent.",
                    "label": 0
                },
                {
                    "sent": "At the other place where there is a physical interaction with the environment like you have a robot and it has wheels and wheels are acting on the environment and the environment is is putting energy until the sensors of the robot.",
                    "label": 0
                },
                {
                    "sent": "You can put the boundary there, but on the other hand, if you don't want to deal with the physical interaction but you mainly are interested with the software with the brain part, then probably you want to get somebody deeper.",
                    "label": 0
                },
                {
                    "sent": "And this is what we are going to do.",
                    "label": 0
                },
                {
                    "sent": "So this is the viewpoint that we are going to take.",
                    "label": 0
                },
                {
                    "sent": "We don't want to care about the physical processes.",
                    "label": 0
                },
                {
                    "sent": "We say that after this interaction happens with the environment, that will be some mechanism that transforms everything into your cities of bits or whatever.",
                    "label": 0
                },
                {
                    "sent": "So we just want to care about the software part, the brain part.",
                    "label": 0
                },
                {
                    "sent": "And so this figure shows shows how this is happening.",
                    "label": 0
                },
                {
                    "sent": "So there are these external sensations coming in.",
                    "label": 1
                },
                {
                    "sent": "And the actual agent that we do care about is somewhere inside and many things can happen just within the boundary of the robot.",
                    "label": 0
                },
                {
                    "sent": "You know the physical boundary of the robot, so we do not necessarily care with the pure sensory readings, right?",
                    "label": 0
                },
                {
                    "sent": "So we don't want to care about the voltages or whatever, so there will be some nice and clever mechanism that do some filtering, retraction of noise or whatever, but what's important for us is that.",
                    "label": 0
                },
                {
                    "sent": "After this is done, we can really just care about the brain after robot.",
                    "label": 0
                },
                {
                    "sent": "So in particular.",
                    "label": 0
                },
                {
                    "sent": "It is good to keep in mind that you could have sensations coming from the XML word and the internal word of the robot.",
                    "label": 0
                },
                {
                    "sent": "Let's put it this way.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "So if you have your, if you have a body then maybe you have.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, something like that.",
                    "label": 0
                },
                {
                    "sent": "If you're robot and arms could have various joints and then you can measure the angles of the choice or the angular velocity of those joints and those would be internal sensations and and those sensations are really part of the sensations.",
                    "label": 0
                },
                {
                    "sent": "So in particular there is one more thing that will hopefully become more clear.",
                    "label": 0
                },
                {
                    "sent": "So we have we see here an arrow that says state.",
                    "label": 0
                },
                {
                    "sent": "So in particular what happens is that there should be a mechanism in the robot that does some processing that constructs something that's called the state.",
                    "label": 0
                },
                {
                    "sent": "So it was the state.",
                    "label": 0
                },
                {
                    "sent": "So the state should be something like sufficient statistics for the future, and this will be our main assumption that that in this series of talks that that is some mechanism that's able to construct the state if we will see the consequences if there is no such mechanism.",
                    "label": 0
                },
                {
                    "sent": "Or how you can approximate such a mechanism but but this will be our working hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And then you know this brain part just decides about the actions.",
                    "label": 0
                },
                {
                    "sent": "But actions could be high level actions.",
                    "label": 0
                },
                {
                    "sent": "They do not necessarily have to be really low level actions like turning the wheel by just one degree or something like that.",
                    "label": 0
                },
                {
                    "sent": "So anyways, we are going to abstract away some details by.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proposing a mathematical model.",
                    "label": 0
                },
                {
                    "sent": "So this is a generic mathematical model, so you have a a plant a controlled up trap.",
                    "label": 0
                },
                {
                    "sent": "The state of the plant is developing according to this activation, so this is a very generic activation, so it has no structure or what server.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying that the next date is dependent on the previous state and the current action, and there is some noise injected into the system.",
                    "label": 0
                },
                {
                    "sent": "So this state I was in a noisy way.",
                    "label": 0
                },
                {
                    "sent": "And then what the robot can observe is not actually the state, but just something that depends on the state.",
                    "label": 0
                },
                {
                    "sent": "And observations could be noisy.",
                    "label": 0
                },
                {
                    "sent": "So there is this other function chi that takes the state and take some noise.",
                    "label": 0
                },
                {
                    "sent": "So that's some disturbance and it returns an observation to the robot.",
                    "label": 0
                },
                {
                    "sent": "Is this clear?",
                    "label": 0
                },
                {
                    "sent": "So what's the state?",
                    "label": 0
                },
                {
                    "sent": "So I said that the state is a sufficient statistics for the future, and I just defined these two acquisitions in a quiet, arbitrary manner.",
                    "label": 0
                },
                {
                    "sent": "But if you define these two applications this way, then it automatically holds that the state is sufficient statistics for the future.",
                    "label": 0
                },
                {
                    "sent": "But this could be a really problematic definition if you think a little bit more, because if you only have a robot and it has some sensations of the environment then.",
                    "label": 0
                },
                {
                    "sent": "How do I construct something like acceptee?",
                    "label": 0
                },
                {
                    "sent": "So how do I know that such an object exists?",
                    "label": 0
                },
                {
                    "sent": "So first of all, the first idea is just to collect all the histories.",
                    "label": 0
                },
                {
                    "sent": "So just to have everything.",
                    "label": 0
                },
                {
                    "sent": "So we have a history that should be enough to determine the future.",
                    "label": 0
                },
                {
                    "sent": "So if the system is crucial, like the actions like future future things happening in the future do not influence things happening Now, then the history should be a sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "But the problem is the history is that it's not really compact, so you are looking for some compact representation of the history that's sufficient for predicting.",
                    "label": 0
                },
                {
                    "sent": "Whatever is going to happen in the future depending on of course, what actions you want to take.",
                    "label": 0
                },
                {
                    "sent": "So that that would be the definition.",
                    "label": 0
                },
                {
                    "sent": "But what happens is that you can define the state in two different ways and traditionally in control theory people define the state independently of what what you measure, so you matching that your robot could be accurate with as many sensors as he wanted.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You could put a on the robot at sensor GPS sensor, let's say or you could put a camera and a robot.",
                    "label": 0
                },
                {
                    "sent": "Are just imagine equipping that the robot with many many sensors and then in the limit if you just put all these sensors and robot then the state is defined as sufficient statistics with respect to the set of all those measurements.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So there's the traditional definition of upstate and well.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit problematic because if you have a robot, then pressure bleed your boat so you don't have space to put that many sensors and robot.",
                    "label": 0
                },
                {
                    "sent": "First of all, right?",
                    "label": 0
                },
                {
                    "sent": "So that's just impossible to do.",
                    "label": 0
                },
                {
                    "sent": "Then why do you care?",
                    "label": 0
                },
                {
                    "sent": "So when I say it's a sufficient statistics for the future, what should I care about?",
                    "label": 1
                },
                {
                    "sent": "I should only care about what the robot is able to upset.",
                    "label": 0
                },
                {
                    "sent": "I should not care about what.",
                    "label": 0
                },
                {
                    "sent": "Unobservable, right?",
                    "label": 0
                },
                {
                    "sent": "So if the robot doesn't have any kind of some sort of sensors like it's not able to sense its position with respect to the universe, why do I care about the position of the robot with respect to the universe, right?",
                    "label": 0
                },
                {
                    "sent": "So I should not care about that.",
                    "label": 0
                },
                {
                    "sent": "So if you define the state as a sufficient statistics with respect to the measurements with things that the robot can observe, then you get what's called subjective state.",
                    "label": 0
                },
                {
                    "sent": "From our point of view, we don't care about this distinction distinction anymore, I just wanted to be very clear about what do we mean by state.",
                    "label": 0
                },
                {
                    "sent": "Either definition of states is good enough for us.",
                    "label": 0
                },
                {
                    "sent": "Just say that we have we have a state somehow.",
                    "label": 0
                },
                {
                    "sent": "OK, so then, what's the purpose of of control and what's a controller?",
                    "label": 0
                },
                {
                    "sent": "So the controller is basically just taking past measurements past observations, and it's returning an action.",
                    "label": 0
                },
                {
                    "sent": "AB, so it's such a controller, cycled, feedback controllers, and So what was on the previous slide is that if you feedback this action into these activations, then you get what's called a closed loop system.",
                    "label": 0
                },
                {
                    "sent": "It's closed loop as opposed to open loop, because the future is completely determined by capital F&F&G.",
                    "label": 0
                },
                {
                    "sent": "And the resulting control is called closed loop controls and design problem is how to choose F and in reinforcement learning we care about the problems that can be formulated.",
                    "label": 0
                },
                {
                    "sent": "Something like this at every time step you are receiving some reward depending on observations and actions maybe future like one step ahead observations or whatever.",
                    "label": 0
                },
                {
                    "sent": "This is not very important.",
                    "label": 0
                },
                {
                    "sent": "And maybe you are just summing up this device and you want to maximize this rewards the sum of this rewards, so it's important to realize that you are not not maximizing the immediate reward, you are maximizing the sum of the rewards and capital T should not be taken too seriously here, so that some people do not see the bottom, I believe.",
                    "label": 0
                },
                {
                    "sent": "I don't know what to do with that, so maybe I can.",
                    "label": 0
                },
                {
                    "sent": "Is not good this one.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit too much.",
                    "label": 0
                },
                {
                    "sent": "It would be, but this work.",
                    "label": 0
                },
                {
                    "sent": "The cookie OK. Where is the cookie this?",
                    "label": 0
                },
                {
                    "sent": "Is it in the box?",
                    "label": 0
                },
                {
                    "sent": "Do you want to do?",
                    "label": 0
                },
                {
                    "sent": "Tear.",
                    "label": 0
                },
                {
                    "sent": "Wow, wow, a technical solution, Bravo.",
                    "label": 0
                },
                {
                    "sent": "Do people see in the back at the bottom?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "Looks good.",
                    "label": 0
                },
                {
                    "sent": "So you don't want to just maximize the immediately bar so the immediate reward is called.",
                    "label": 0
                },
                {
                    "sent": "Just you're at time step T and you are receiving this reward, so that's called the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "And maximizing that is a different problem.",
                    "label": 0
                },
                {
                    "sent": "So if you maximize this long term cumulative reward, this makes the problem really challenging, and so as an example, let's consider navigation.",
                    "label": 0
                },
                {
                    "sent": "So you have a little robot that should get to the.",
                    "label": 0
                },
                {
                    "sent": "I don't know why robots would go to the beach, but anyway, so that seems to be that like the idea target here.",
                    "label": 0
                },
                {
                    "sent": "So you have a robot that wants to go to the beach.",
                    "label": 0
                },
                {
                    "sent": "And so, how do you formulate this problem?",
                    "label": 0
                },
                {
                    "sent": "To formulate this problem by defining a reward of one of the robot gas to the beach.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the reward received is just zero.",
                    "label": 0
                },
                {
                    "sent": "This might not be actually a very good idea.",
                    "label": 0
                },
                {
                    "sent": "If you really want to solve a navigation problem, because the reward in this case is sparse, but.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it just works if the robot is able to pick a controller that maximizes the cumulative total reward, then the only way it could achieve reward not zero is to go to the beach, right?",
                    "label": 0
                },
                {
                    "sent": "But if the robot wants to just maximize the immediate reward when the robots here, it cannot fly, unfortunately, so there is no teleportation action and the robot, so it can go too.",
                    "label": 0
                },
                {
                    "sent": "Like name sort of just somewhere closing the neighborhood so that you can move smoothly in the environment so the immediate reward received will be 0 for a long long time before it gets a one.",
                    "label": 0
                },
                {
                    "sent": "Right, so it has to plan ahead in time in order to find out the the sequence of actions or or is this maybe this function capital F such that at the end it gets to the beach?",
                    "label": 0
                },
                {
                    "sent": "Is it clear?",
                    "label": 0
                },
                {
                    "sent": "This is an important distinction.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So limit of classification of controllers, so some so in the early days of of control people looked at what's called feedforward control, where the sequence of actions is decided ahead in time.",
                    "label": 1
                },
                {
                    "sent": "So is this a good idea?",
                    "label": 0
                },
                {
                    "sent": "What do you think?",
                    "label": 0
                },
                {
                    "sent": "OK so yeah.",
                    "label": 0
                },
                {
                    "sent": "So if you have a robot that that is perfect in executing the actions and the effects of the actions are just just as expected, always that this might work right, but what if you have some noise in the environment?",
                    "label": 0
                },
                {
                    "sent": "So there are some unexpected changes happening in environment, so is this going to work?",
                    "label": 0
                },
                {
                    "sent": "Do you expect this to work?",
                    "label": 0
                },
                {
                    "sent": "Not quite right.",
                    "label": 0
                },
                {
                    "sent": "So the problem is.",
                    "label": 0
                },
                {
                    "sent": "This approach is that if there is some uncertainty in the environment, there is some noise there.",
                    "label": 0
                },
                {
                    "sent": "Then the robot can well end up in maybe Sydney or whatever.",
                    "label": 0
                },
                {
                    "sent": "Instead of going to the beach and so you need something that's called a feedback controller.",
                    "label": 0
                },
                {
                    "sent": "So purely reactive system would just act based on its observations and some people in robotics are.",
                    "label": 0
                },
                {
                    "sent": "Publicizing this view that that you should really try to do this.",
                    "label": 0
                },
                {
                    "sent": "You don't need any memory and whatever, so what's the problem with this approach?",
                    "label": 0
                },
                {
                    "sent": "So what do you think?",
                    "label": 0
                },
                {
                    "sent": "Sufficient support?",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Works the same, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So then then you cannot have based on your observationes you cannot be very clever about which way you want to go, right?",
                    "label": 0
                },
                {
                    "sent": "So purely interactive systems might work in for some limited tasks, but in general you need something more and what's what's more is is memory.",
                    "label": 0
                },
                {
                    "sent": "So basically what you want to do is to build up some memory, and basically when you build up some memory, what you do is that you are interpreting your sensations and then based on the memory you want to come up with the decision.",
                    "label": 0
                },
                {
                    "sent": "And the decision-making itself could take some time, in which case it's called the deliberative control.",
                    "label": 0
                },
                {
                    "sent": "Or it could be just, you know, one shot and it's done.",
                    "label": 0
                },
                {
                    "sent": "It's called reactive control.",
                    "label": 0
                },
                {
                    "sent": "We don't really care about this distinction in the rest of the talk, just wanted to mention so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regarding feedback controllers, so we sent it.",
                    "label": 1
                },
                {
                    "sent": "It might be a good idea to build up this memory so the generic form of building up the memory is just that.",
                    "label": 0
                },
                {
                    "sent": "You know you have your previous memory state and, well, I'm sorry this should be set of TI guess, so this is an observation.",
                    "label": 0
                },
                {
                    "sent": "If you have your observation you have your previous action and you just update your memory based on your memory you come up with some action.",
                    "label": 0
                },
                {
                    "sent": "And then you can realize that, well, there could be a connection here to Ronaldo's talking about previously.",
                    "label": 0
                },
                {
                    "sent": "So if we said that acts of tea is a state, so accepte is sufficient to know anything about the future.",
                    "label": 0
                },
                {
                    "sent": "In particular, it's sufficient to do any kind of planning you want it to do.",
                    "label": 0
                },
                {
                    "sent": "So wouldn't it be a good idea to have had a memory buildup that tries to approximate the state?",
                    "label": 0
                },
                {
                    "sent": "It seems that, well, that should work.",
                    "label": 0
                },
                {
                    "sent": "So here you can connect to filtering and state estimation and interest.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to assume that there is some mechanism that's already doing that.",
                    "label": 0
                },
                {
                    "sent": "So I don't care about how this is done.",
                    "label": 0
                },
                {
                    "sent": "Particle filtering is is great.",
                    "label": 0
                },
                {
                    "sent": "That could be a really good solution for this.",
                    "label": 0
                },
                {
                    "sent": "I just assumed that that we somehow has estimated the state.",
                    "label": 0
                },
                {
                    "sent": "In particular, I don't.",
                    "label": 0
                },
                {
                    "sent": "I will not at the beginning I will not care about errors resulting from state estimation, so I will just.",
                    "label": 0
                },
                {
                    "sent": "Mimic that, but we have the state information.",
                    "label": 0
                },
                {
                    "sent": "So if it's if you accept this, OK, So what I'm doing here is that I'm simplifying the problem from its functionality in various ways.",
                    "label": 0
                },
                {
                    "sent": "And and I want to get to a place where I can really do something so we will see that even if we do this, if we assume that the state is measurable, the problem is going to be really tough.",
                    "label": 0
                },
                {
                    "sent": "And so we are going to study that problem in quite a detail and then maybe later, if you have time, we can come back to this even tougher problem, yes?",
                    "label": 0
                },
                {
                    "sent": "New action.",
                    "label": 0
                },
                {
                    "sent": "OK, so yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's true, but.",
                    "label": 0
                },
                {
                    "sent": "So the the length of the observation.",
                    "label": 0
                },
                {
                    "sent": "So the length of the history is growing this time.",
                    "label": 0
                },
                {
                    "sent": "So if you want to somehow get away with bounded computation resources.",
                    "label": 0
                },
                {
                    "sent": "Then somehow you want to compress the histories.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that it's pretty hard to generalize across histories and.",
                    "label": 0
                },
                {
                    "sent": "See, that's the primary reason.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is a good question.",
                    "label": 0
                },
                {
                    "sent": "I should get Cookie.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So please ask more questions like that because it seems that I need some practice during this cookies.",
                    "label": 0
                },
                {
                    "sent": "OK, how much do I owe you?",
                    "label": 0
                },
                {
                    "sent": "Uh, no.",
                    "label": 0
                },
                {
                    "sent": "Sorry, sorry, sorry.",
                    "label": 0
                },
                {
                    "sent": "I didn't know how far it goes.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about this.",
                    "label": 0
                },
                {
                    "sent": "This is exploration.",
                    "label": 0
                },
                {
                    "sent": "OK, but Luckily you have already finish your talk, so we're saying OK, so if we are sad that we have state information, then the next question is where?",
                    "label": 1
                },
                {
                    "sent": "How do you compute a good action?",
                    "label": 0
                },
                {
                    "sent": "And well, basically there are two approaches, so one is called the model based control.",
                    "label": 0
                },
                {
                    "sent": "And the and the other is modific can't roll in the case of model based control.",
                    "label": 0
                },
                {
                    "sent": "You somehow try to estimate the model the dynamics.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're trying to estimate the model, so if you forgot about the observation acquisition because we said that.",
                    "label": 0
                },
                {
                    "sent": "We will serve the state then what is left is just act.",
                    "label": 0
                },
                {
                    "sent": "So if you knew F then this would be just a planning problem.",
                    "label": 0
                },
                {
                    "sent": "Just the planning problem.",
                    "label": 0
                },
                {
                    "sent": "So we talk about that.",
                    "label": 0
                },
                {
                    "sent": "We have some gas.",
                    "label": 0
                },
                {
                    "sent": "They should ask questions, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So why should we set up T?",
                    "label": 0
                },
                {
                    "sent": "As I said previously?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The problem is that.",
                    "label": 0
                },
                {
                    "sent": "That different notations in different communities and wiate in control typically denotes observation and.",
                    "label": 0
                },
                {
                    "sent": "510 to use a contract here annotation, but here in MDPSY FT sometimes Dinos the next state and so then everything would be confused and so unfortunately this state.",
                    "label": 0
                },
                {
                    "sent": "Somehow they survived.",
                    "label": 0
                },
                {
                    "sent": "Somehow the changes that I made to the slides many times.",
                    "label": 0
                },
                {
                    "sent": "So anyway, so there are these two different ways of approaching the problem.",
                    "label": 0
                },
                {
                    "sent": "So one approach is to estimate the model.",
                    "label": 0
                },
                {
                    "sent": "So you say that you said that.",
                    "label": 0
                },
                {
                    "sent": "We assume that the state is observable.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Then you could try to just estimate F. So given the current XFT current AF T and the next X of T, this is just if you want the regression problem right, or maybe a density estimation problem because the next state could be stochastic an, so you want to know the next state distribution and not just the mean next state.",
                    "label": 0
                },
                {
                    "sent": "So then you can build up that model and once you have built.",
                    "label": 0
                },
                {
                    "sent": "The model just use a technique that assumes that you are given a model and just solve for the optimal control.",
                    "label": 0
                },
                {
                    "sent": "We will see some approaches that work like that or that.",
                    "label": 0
                },
                {
                    "sent": "They assume that you are given a model, will see that.",
                    "label": 0
                },
                {
                    "sent": "Even if you have, if you are given a model, coming up is a good action is a highly nontrivial problem.",
                    "label": 0
                },
                {
                    "sent": "So surprisingly, there is this other approach which is called the model free approach.",
                    "label": 0
                },
                {
                    "sent": "When you're not building a model at all, just somehow magically come up with a with a good controller, well, it's not magical at all, so.",
                    "label": 1
                },
                {
                    "sent": "One very simple way of doing that is that why you say that I'm going to have this capital F function parameterized with some parameters and parameters live in some parameter space Euclidean space, and I just view this problem as an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "I can use a gradient asked procedure to optimize my parameters, and so that's called policy search.",
                    "label": 0
                },
                {
                    "sent": "So that's one approach, but there are some other approaches and we are going to take.",
                    "label": 0
                },
                {
                    "sent": "I'll look at them.",
                    "label": 0
                },
                {
                    "sent": "It's it's very interesting.",
                    "label": 0
                },
                {
                    "sent": "So what was the deal here?",
                    "label": 0
                },
                {
                    "sent": "Which approach should be better?",
                    "label": 0
                },
                {
                    "sent": "Now we don't know.",
                    "label": 0
                },
                {
                    "sent": "We have no idea to tell you the truth.",
                    "label": 0
                },
                {
                    "sent": "So if you have a.",
                    "label": 0
                },
                {
                    "sent": "A good prior knowledge about the what the model might be that you want to use that knowledge so that it's the odds the same old story if you have some prior knowledge you want to put that prior knowledge into the system and use it.",
                    "label": 0
                },
                {
                    "sent": "If you don't have such a prior knowledge, then while you can still try both approaches then you have to somehow do this in a non parametric way.",
                    "label": 0
                },
                {
                    "sent": "And the problem there is that one when you're building the model, you will introduce some errors and those errors are going to propagate through the planning phase and it's not very much clear from the point of view of a particle application which approach should be preferred.",
                    "label": 0
                },
                {
                    "sent": "So I cannot have too much.",
                    "label": 0
                },
                {
                    "sent": "So one thing here though is that building model relied heavily on being able to estimate the state or node state, and so one problem with that approach could be that if that assumption is not met.",
                    "label": 0
                },
                {
                    "sent": "Or is violated?",
                    "label": 0
                },
                {
                    "sent": "Strongly Dan, your models could be way off and you are planning solution is is going to be way off as well, yeah?",
                    "label": 0
                },
                {
                    "sent": "Is that the model?",
                    "label": 0
                },
                {
                    "sent": "Patiently more efficient but data equations in the model based ones are very data efficient, but computationally copper.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is sometimes true, yeah?",
                    "label": 0
                },
                {
                    "sent": "Because you can come up with model free approaches that like to be data efficient an it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's largely unclear to me if this is in China tree, but it's true that some of the model free approaches are not data efficient and all but it depends on the design of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "If the nature of rewards is changing overtime so that.",
                    "label": 0
                },
                {
                    "sent": "Now read things are rewarding and later on we're coming up the green things as rewarding.",
                    "label": 0
                },
                {
                    "sent": "So yeah, with the Model 3 with you if you don't know red or green but you're just getting the policy to get there, then when the the natural reward changes then suddenly not working very well.",
                    "label": 0
                },
                {
                    "sent": "Whereas if he got model based, presumably you've learned the dynamics of the world Sebokeng idea how to get from here to something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so first of all, if the reward changing that's outside of the scope of this model.",
                    "label": 0
                },
                {
                    "sent": "So we said that the reward is dependent on, let's say in the.",
                    "label": 0
                },
                {
                    "sent": "In this case the state and an action.",
                    "label": 0
                },
                {
                    "sent": "So your state information is not correct or not complete.",
                    "label": 0
                },
                {
                    "sent": "If you can say that the reward is changing.",
                    "label": 0
                },
                {
                    "sent": "So this is a limitation of the model and some people are looking at extensions of this model.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, well, it's true that the model based approach could be more ad vantages.",
                    "label": 0
                },
                {
                    "sent": "If you have, maybe let's say multiple task.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "The reward function is changing, but you have the same dynamics but you could like you have the same robot but robot could fulfill multiple tasks and then you learn the dynamics for the for the robot and so you can use the data more efficiently in that case so.",
                    "label": 0
                },
                {
                    "sent": "OK cookie.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Very good.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So you can do that, but the problem with just extending the state variable indefinitely or or to make it very large is then that it's not obvious how to generalize then in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "So you added this new component.",
                    "label": 0
                },
                {
                    "sent": "But the new component you have this prior knowledge about the new component.",
                    "label": 0
                },
                {
                    "sent": "It only influences the rewards.",
                    "label": 0
                },
                {
                    "sent": "So then if you don't use that prior knowledge, you don't build that prior knowledge into the way you're estimating things.",
                    "label": 0
                },
                {
                    "sent": "Then you made the problem unnecessarily harder.",
                    "label": 0
                },
                {
                    "sent": "So if you extend the state and you do that, then it looks like as if you didn't do anything with the state, right?",
                    "label": 0
                },
                {
                    "sent": "So yes, I know but.",
                    "label": 0
                },
                {
                    "sent": "Different tactics.",
                    "label": 0
                },
                {
                    "sent": "Robot yeah.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So we're making very strong assumptions here, but you'll see that despite a strong assumption, this is a tough problem and you'll enjoy it.",
                    "label": 0
                },
                {
                    "sent": "I just thinking about it and then.",
                    "label": 0
                },
                {
                    "sent": "Well then you can think about relaxing this assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK, so the next part.",
                    "label": 0
                },
                {
                    "sent": "So that was sort of the introduction OK, and So what does it?",
                    "label": 0
                },
                {
                    "sent": "What does this have to do with AI?",
                    "label": 0
                },
                {
                    "sent": "So one hope too.",
                    "label": 0
                },
                {
                    "sent": "To approach AI is to to formulate the problem with the with this very generic formulation and just have a learning system that able to, you know, solve the whole thing.",
                    "label": 0
                },
                {
                    "sent": "And I don't know if Marcus you are going to talk about that next week.",
                    "label": 0
                },
                {
                    "sent": "OK, so wait for the talk of Marcus to to learn more about this next week.",
                    "label": 0
                },
                {
                    "sent": "I guess it's Monday, right?",
                    "label": 0
                },
                {
                    "sent": "OK, but we're not going to that far today, so we are going to look at MVP's next.",
                    "label": 0
                },
                {
                    "sent": "So what's an MDP?",
                    "label": 0
                },
                {
                    "sent": "So mathematically, an MDP is just a fourth Apple, you have a set of states, a set of actions, transition probabilities, and rewards.",
                    "label": 0
                },
                {
                    "sent": "And so this is just yet another way of formalizing what we said before, so the set of states is, you know, just all possible states that the agent can encounter the set of actions is just all possible controls.",
                    "label": 0
                },
                {
                    "sent": "The agent can execute the transition probability and codes how the state changes overtime.",
                    "label": 0
                },
                {
                    "sent": "So this is a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And for the sake of simplicity, I'm going to talk a lot about finite MDP's.",
                    "label": 0
                },
                {
                    "sent": "But many of the conclusions that we all have just straightforwardly generalizes to infinite and EPS should not vary that much about infinite State space in its infinite action spaces at this stage.",
                    "label": 0
                },
                {
                    "sent": "But for simplicity, video with finite MDP's so that I can say that this is just a probability distribution over possible wise so that this distribution sums to one if I sum it over Y, right?",
                    "label": 0
                },
                {
                    "sent": "So given X current state an given a current action that the agent has just taken, was the probability distribution over the next 8, so that that is what this object is giving to us.",
                    "label": 0
                },
                {
                    "sent": "Transition probabilities is that clear so.",
                    "label": 0
                },
                {
                    "sent": "So OK, so I explained different way.",
                    "label": 0
                },
                {
                    "sent": "So if you choose an action you have chosen an Markov transition kernel, right?",
                    "label": 0
                },
                {
                    "sent": "So you have you are somewhere in the state.",
                    "label": 0
                },
                {
                    "sent": "And you have chosen action and your next state is a random quantity.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "What is the distribution of this random quantity?",
                    "label": 0
                },
                {
                    "sent": "The transition probabilities are giving you this distribution clear.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the next time step and when you executed the action, you are going to upset by.",
                    "label": 0
                },
                {
                    "sent": "The next state.",
                    "label": 0
                },
                {
                    "sent": "So you feed it back to the agent.",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "They have to be over watered well.",
                    "label": 0
                },
                {
                    "sent": "It could include the rewards, but traditional divorce is not included so the reward is something that's derived from the state in this formulation.",
                    "label": 0
                },
                {
                    "sent": "The state just anchors all the information that's needed to to predict the future.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                },
                {
                    "sent": "If you start saying that the reward is going to change, then the state is not complete.",
                    "label": 0
                },
                {
                    "sent": "Things like that, so if you.",
                    "label": 0
                },
                {
                    "sent": "So if you if you are not able to.",
                    "label": 0
                },
                {
                    "sent": "If you see that the environment is changing or somehow there is some non stationarity then that's usually caused because the state variable is not complete.",
                    "label": 0
                },
                {
                    "sent": "So you want to extend the state variable.",
                    "label": 0
                },
                {
                    "sent": "You only want to just have a better description of what the pass was, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a mathematical framework right model and we talk about learning and that kind of stuff and then it will turn out that.",
                    "label": 0
                },
                {
                    "sent": "Typically you don't deal with the stares directly at all.",
                    "label": 0
                },
                {
                    "sent": "You deal with observations only, so you have your sensors and based on the sensory information, maybe you do some feature extraction that involves computations which are history dependent or whatever.",
                    "label": 0
                },
                {
                    "sent": "So you say your features and your features are discribing so.",
                    "label": 0
                },
                {
                    "sent": "So to say the state or or at least what you know about the state.",
                    "label": 0
                },
                {
                    "sent": "And many of the arguments are going to work at the level of features.",
                    "label": 0
                },
                {
                    "sent": "But in order to understand how these arguments are designed and the properties of these arguments, we consider this mathematical model.",
                    "label": 0
                },
                {
                    "sent": "When you imagine or the set of all States and everything.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these are transition probabilities and rewards, so sometimes people.",
                    "label": 0
                },
                {
                    "sent": "Define the words immediate rewards as dependent on the current state, current action and the next state.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they just define it as dependent on current state and the current action or just current state.",
                    "label": 0
                },
                {
                    "sent": "All definitions are good.",
                    "label": 0
                },
                {
                    "sent": "There is not much of a difference between them.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You are dependent on the observation, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because I said that the observation is going to be equal to the state.",
                    "label": 0
                },
                {
                    "sent": "So that was the conclusion of the last slide, so that you could do this memory building process when you are estimating the state.",
                    "label": 0
                },
                {
                    "sent": "But instead of considering that in full detail, we just say that we observe the state in the first place.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's why the reward is dependent on the state right here.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Being more specific about the kind of problems that we are going to consider, we are going to consider what's called the total expected discounted reward criterion criterion and in order to specify that you need some another quantity that's called discount factor.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be more specific about the role of discount factor and right here on the next slide.",
                    "label": 0
                },
                {
                    "sent": "So what does it all this?",
                    "label": 0
                },
                {
                    "sent": "Our model means, so there is another view of of this model, and that's the process you.",
                    "label": 0
                },
                {
                    "sent": "So you have a stochastic process, acts of TFT and RFT.",
                    "label": 0
                },
                {
                    "sent": "By the way I'm going to use mostly capital letters to denote stochastic quantities as usual in mathematics.",
                    "label": 0
                },
                {
                    "sent": "So access the FT NRT are your current state and current action and the current immediately reward that you have received in the current timestamp.",
                    "label": 0
                },
                {
                    "sent": "So this has stochastic process.",
                    "label": 0
                },
                {
                    "sent": "These are random variables that are indexed by time, so by stochastic process we mean that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And so, how does what are the laws determining the?",
                    "label": 0
                },
                {
                    "sent": "The development of the stochastic process.",
                    "label": 0
                },
                {
                    "sent": "So this is how it's connected to the previous slide, so the next state XD plus one is distributed according to the transition probability kernel.",
                    "label": 0
                },
                {
                    "sent": "So we have here P and so P determines given access T&AFT at probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So the next state, which is distributed according to that kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, So what about the current action?",
                    "label": 0
                },
                {
                    "sent": "So the action is chosen based on the history, so that's the most generic form of our motion, Rick way off of choosing an action.",
                    "label": 0
                },
                {
                    "sent": "So you take a look at your history and you compute a probability distribution over the action.",
                    "label": 0
                },
                {
                    "sent": "So that's \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And and then you just throw randomly an action from that probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Now you can wonder about why do we care about choosing actions in a stochastic manner, and later it will be clear that that if you do learning then then probably you want to do that to some extent.",
                    "label": 0
                },
                {
                    "sent": "Or it might be a good idea to do that, but otherwise for many of the developments that we will do here, we don't need stochastic policies at all, so the policy could be just, you know, this distribution could be adjusted direct data assigning an infinite mass to some action and zero mass to all the other actions.",
                    "label": 0
                },
                {
                    "sent": "So deterministic it should just choose an action given the history.",
                    "label": 0
                },
                {
                    "sent": "So that's a policy, and the history is about the history.",
                    "label": 0
                },
                {
                    "sent": "Everything that you have seen so far, and the reward is, well, it could be that the reward.",
                    "label": 0
                },
                {
                    "sent": "So the River could be stochastic, but here we just say that, well, let's use this model at the reward is dependent on the current state, current action, and next state.",
                    "label": 0
                },
                {
                    "sent": "And we just say that the reward is just read out by this reward function.",
                    "label": 0
                },
                {
                    "sent": "So this is the way.",
                    "label": 0
                },
                {
                    "sent": "Things are developing in time.",
                    "label": 0
                },
                {
                    "sent": "Once you have fixed the controller and once you are given an MDP.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sometimes.",
                    "label": 0
                },
                {
                    "sent": "Something actually will say good, but it's not really.",
                    "label": 0
                },
                {
                    "sent": "For example you have chest.",
                    "label": 0
                },
                {
                    "sent": "Bad, but getting them to tack on your faces.",
                    "label": 0
                },
                {
                    "sent": "So robots are rewarded for taking a certain place.",
                    "label": 0
                },
                {
                    "sent": "Then I left him a traffic turned out without.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The reward function.",
                    "label": 0
                },
                {
                    "sent": "The choice of the reward function is is is is sometimes non trivial.",
                    "label": 0
                },
                {
                    "sent": "In the case of chess, the trivial reward function would be just to give a reward of of unity to the agent when he has won the game and give it 0 otherwise, and give it mine.",
                    "label": 0
                },
                {
                    "sent": "Give it minus one if it has lost the game.",
                    "label": 0
                },
                {
                    "sent": "So if you do this encoding then those problems go away if you try to somehow shape the reward function to have the agent with its immediate decisions, then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then then it's a non trivial problem.",
                    "label": 0
                },
                {
                    "sent": "How to do that?",
                    "label": 0
                },
                {
                    "sent": "So right now we are going to assume that the reward function is given some clever engineer designed the reward function.",
                    "label": 0
                },
                {
                    "sent": "It's an auto to define good reward functions for real real world problems, except in some problems like operations research problems or control problems where the reward function is very clear.",
                    "label": 0
                },
                {
                    "sent": "And so, regarding this choice of reward function of, if you got a 0 pretty much everywhere except at the end of the game.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this makes the problem really really tough.",
                    "label": 0
                },
                {
                    "sent": "So in practice, if you use a reward function like that, then learning could be very slow.",
                    "label": 0
                },
                {
                    "sent": "But if you don't have any better idea than that, then maybe that's still better than to come up with some surrogate reward function that could mislead the agent in a very bad way.",
                    "label": 0
                },
                {
                    "sent": "So maybe what you are differing, so maybe we can come back to your question later when we talk about value functions, because I think value functions are more related to your question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, you have to wait one more step.",
                    "label": 0
                },
                {
                    "sent": "Before you see it, receive the reward, but it's not a big deal.",
                    "label": 0
                },
                {
                    "sent": "Well, basically the assumption is that that the delay is not very large.",
                    "label": 0
                },
                {
                    "sent": "It cannot be that if you are not receiving the rewards like indefinitely, then.",
                    "label": 0
                },
                {
                    "sent": "Then then.",
                    "label": 0
                },
                {
                    "sent": "Well, you should add that well, I would say we assume that.",
                    "label": 0
                },
                {
                    "sent": "I cannot see.",
                    "label": 0
                },
                {
                    "sent": "So you receive rewards every time step.",
                    "label": 0
                },
                {
                    "sent": "OK, so this model shows that.",
                    "label": 0
                },
                {
                    "sent": "And you can maybe band that a little bit, but I don't know how much value that has.",
                    "label": 0
                },
                {
                    "sent": "Because so the key point here is that you are not looking at maximizing immediate reward.",
                    "label": 0
                },
                {
                    "sent": "You are going to sum up all these rewards.",
                    "label": 0
                },
                {
                    "sent": "And look at the sum and maximize.",
                    "label": 0
                },
                {
                    "sent": "Let's say the expected value of the sum.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The state depends on all your previous actions, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a there's a thing.",
                    "label": 0
                },
                {
                    "sent": "So when you are executing an action.",
                    "label": 0
                },
                {
                    "sent": "You are not interested in the immediate reward received.",
                    "label": 0
                },
                {
                    "sent": "You are trying to influence the state in such a way that you go to the path of the state space where the future rewards are really great.",
                    "label": 0
                },
                {
                    "sent": "This is what you are trying to do.",
                    "label": 0
                },
                {
                    "sent": "OK. Why?",
                    "label": 0
                },
                {
                    "sent": "So this Y this one here right?",
                    "label": 0
                },
                {
                    "sent": "So they just.",
                    "label": 0
                },
                {
                    "sent": "Able that that's there, filling in position.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So was the control problem.",
                    "label": 0
                },
                {
                    "sent": "So what was the goal of control?",
                    "label": 0
                },
                {
                    "sent": "So if you are given a policy, then you can define this quantity.",
                    "label": 0
                },
                {
                    "sent": "You assume that you start at some state XO, so the 1st at the first time step the state is just something little X and then you execute policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So that generates the sequence of random states, random actions, random rewards.",
                    "label": 0
                },
                {
                    "sent": "So what you do is that you just sum up the rewards.",
                    "label": 0
                },
                {
                    "sent": "Maybe you use discounting the way is shown here.",
                    "label": 0
                },
                {
                    "sent": "So this is the discount factor here.",
                    "label": 0
                },
                {
                    "sent": "And you sum up the rewards up to infinite.",
                    "label": 0
                },
                {
                    "sent": "And you take the expectation of that.",
                    "label": 0
                },
                {
                    "sent": "And so this is going to define a value.",
                    "label": 0
                },
                {
                    "sent": "So that's called the value of state X.",
                    "label": 0
                },
                {
                    "sent": "So what is this defines is that once the future total discounted expected return if I start my policy Pi instead X?",
                    "label": 0
                },
                {
                    "sent": "So this is the value of the policy at that state.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, so when you say stop.",
                    "label": 0
                },
                {
                    "sent": "Red State Bank.",
                    "label": 0
                },
                {
                    "sent": "Look to see what action.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So the agent is put by some external force or some some hole in that particular state, and then it starts to execute the given policy.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Optimal value function is just you take the maximum with respect to all possible policies of the value function in every state.",
                    "label": 0
                },
                {
                    "sent": "So you fix a given state and then this Maxim is over all possible policies, so there's the best possible value that you could hope for, right?",
                    "label": 0
                },
                {
                    "sent": "You cannot have a policy that does anything.",
                    "label": 0
                },
                {
                    "sent": "Oh, annex accent better than we start up X.",
                    "label": 0
                },
                {
                    "sent": "Right, and the goal is going to be to find a policy such that the value function of the policy matches the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "There's a goal.",
                    "label": 0
                },
                {
                    "sent": "And maybe this is a good.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, maybe this is a good place to stop and.",
                    "label": 0
                },
                {
                    "sent": "So see you after break.",
                    "label": 0
                },
                {
                    "sent": "To speed up.",
                    "label": 0
                }
            ]
        }
    }
}