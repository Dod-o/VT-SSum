{
    "id": "6hr3q7bn6xnbvdciqzjw4rcetqxgukpb",
    "title": "Torch/PyTorch",
    "info": {
        "author": [
            "Soumith Chintala, Facebook"
        ],
        "published": "July 27, 2017",
        "recorded": "June 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_chintala_torch/",
    "segmentation": [
        [
            "Excellent.",
            "I.",
            "Right?",
            "Hello.",
            "This sounds good, OK?",
            "Hello.",
            "So.",
            "I heard this is the future of deep learning right here.",
            "That's a lot of people.",
            "Well then it came in.",
            "I thought oh higher, higher higher.",
            "Hello yes.",
            "I was just saying how this is the future of deep learning.",
            "Anne.",
            "When I came in, I actually thought this was just the Miller lab and.",
            "Have you analysis somewhere alright?"
        ],
        [
            "So today I will be talking about Pytorch.",
            "And that is a list of people who have been working on Pytorch for the last few months in various ways.",
            "And it's not a full list of obviously.",
            "An we Pytorch is a framework for deep learning.",
            "But"
        ],
        [
            "My first slide is.",
            "What is pytorch?"
        ],
        [
            "An basically Pytorch has a few competence.",
            "It has it's an interior library, like also called tensors.",
            "These days, which it has strong GPU support, so you can think of it as NUM PY, but with GPU support an.",
            "It is also an automatic differentiation engine.",
            "So you can find the gradients of one thing with respect to another thing in your library.",
            "And you also have gradient based optimization methods such as SGD, RMS, prop, Adam etc.",
            "And we also have utilities for various things you would want to do when you're doing experimentation, such as data loading, etc."
        ],
        [
            "So in terms of end area library, Pytorch.",
            "Has a class of.",
            "Tensors called torch tensor, torso float, tensor torch, double tensor, etc.",
            "They are equal into non pies ndra an we have quite a few operations that do linear algebra, tensor manipulation, slicing, etc.",
            "An we have the same API also available to run both on the CPU an on the GPU with full parity.",
            "So to."
        ],
        [
            "Quickly you can if you.",
            "If you want to see py torch, you can see it as an exactly Clinton on pie on the left side is a program written in NUM, PY and on the right side it's written in Py torch.",
            "I actually can't remember what this is for.",
            "Let's just say it has some values in there and some gradients so.",
            "I think it's Justin Johnson's homework stuff from Stanford courses I can't remember, but anyways, they look exactly equivalent.",
            "You create tensors, you do operations on them, and so on."
        ],
        [
            "So this is kind of how it looks like in the code you import torch and then.",
            "You create tensors and you can print them."
        ],
        [
            "An you can get their size you can create."
        ],
        [
            "Random tensors you can slice tensors, do standard indexing with bells and whistles.",
            "You can.",
            "Add tensors, multiply tensors metric."
        ],
        [
            "Multiply linear solves etc.",
            "So apart from just being a full tensor library."
        ],
        [
            "We understand that Python And NUM.",
            "Py have a very huge ecosystem, so py torch has NUM py bridge, where you can convert a pytorch array into an umpire, and vice versa in an extremely efficient fashion where there's no mem."
        ],
        [
            "Copy being done.",
            "It's almost like a free operation an when you convert a pressure sensor to an umpire, you changed an umpire.",
            "The original Pytorch tensor is changed an if you change the values of the Pytorch tensor, then the NUM PY arrays values are changed.",
            "Anne.",
            "Basically we shared the underlying data pointer."
        ],
        [
            "Anne.",
            "This is just an example of that."
        ],
        [
            "We also have GPU tensors, so if you have a tensor that you create, if you want to transfer it over to the GPU here is called CUDA on it and return is a torch, CUDA dot float tensor for example an if you want to.",
            "If you have a GPU tensor and a few other transferred back to the CPU, just called out CPU on it and then the GPU tensor is transferred back onto the CPU.",
            "It's almost like you're essentially controlling memory regions, and then you can.",
            "You view them as Python objects, sure.",
            "So where tensor here is a different meaning than the.",
            "Tensor flow.",
            "It's the same tensor flow tensor.",
            "I was actually thinking that yes, it's not like it's not a physics tensor.",
            "It's deep learning tensor.",
            "Alright."
        ],
        [
            "So.",
            "We also have an automatic differentiation engine."
        ],
        [
            "Oh should I move to pipers it's faster or does the same back if you're using torch, you would benefit from moving to Pytorch.",
            "But they are the same speed.",
            "It is true.",
            "And in fighters we have unit tests and everything.",
            "So.",
            "Anti towards we have an automatic differentiation engine just like every other deep learning framework an so you this is small example where you're creating.",
            "The Pi torches, autograph engines, variables and what variables here are variables or lose wrappers around tensors.",
            "An if you do any operation on the variable then the variable remembers what happened to it and then when you do a backward on some final variable then the you will compute gradients with respect to all of the leaf variables in the graph and by leave variables I mean variables that the user themselves created.",
            "So in this example, as a user I'm creating X pre Patch WHWX and then I'm doing some matrix multiplies on there and then I'm just adding them up.",
            "An I do a tennis, obviously out of fashion.",
            "And then I do a backward on the final variable with just say, like one gradients an.",
            "Then you will have gradients accumulated in X prefetch WH&WX with respect to this chain of operations that were done.",
            "So you can create neural net.",
            "Oh wait, there's a question.",
            "More precisely, backward will apply the chain rule an with respect to the gradient.",
            "The torch dot once over there, which are treated as like.",
            "Gradient with respect to your output.",
            "Let's say there's some final scalar output.",
            "You will compute the gradient of each of those ones with respect to, like X, prove HWHNWX.",
            "Anne.",
            "I can use the whiteboard.",
            "Which.",
            "I think this is the deep learning summer school someone should have taught this stuff.",
            "But so if you have L = L of F of X, W. Where F is a function.",
            "An then you take some loss with respect to some target value and you get a final loss.",
            "Now you call L dot backward.",
            "An L is a scalar value, so you called out backward and then what will be computed is DL by DW.",
            "Anne Anne DL by DX.",
            "An generally this is the gradient that will be stored in the dot grad attribute of this variable an you can do an HDD step on this with a small learning rate and you repeatedly do this an that's basically all of what I learned.",
            "It's still not clear to me, so what's the input to the backward?",
            "The input to the back?",
            "So if L is a scalar loss, we just trade the backward as a like you just have one that is being propagated back, but if it's not a scalar loss then you will need some.",
            "Some gradient values that are representing some future, like some way to do scalar.",
            "120 yes, exactly.",
            "That will be great, and so the next time to score Edge.",
            "So does it.",
            "It doesn't actually output these gradients or just assigns them to the dot grad of variables.",
            "Yes, exactly awesome, we're making progress."
        ],
        [
            "So.",
            "Next is a small example of going from."
        ],
        [
            "Like OK, you have this autograde engine?",
            "How do you painlessly create neural networks an so?"
        ],
        [
            "So we have a loose wrapper around this AutoCAD engine that lets you construct neural networks without having to deal with like a gazillion objects that you manually handle, so you can create a more formal neural network by subclassing from North module, which is available in the torch package an in the constructor.",
            "Let's say you create all of the.",
            "Layers that you would want to use.",
            "For example, a convolution layer is just like a convolution object that holds on to the weights and biases of the convolution that is being done.",
            "And the FC layer is like an affine transform an in the forward function you actually represent.",
            "You take the input in the forward function and your present how the input goes to the output an in this forward function you can do any kind of operations.",
            "You can do all kind of pythonic operations like not only just referencing your layers that you define in the constructor, but also like some functional.",
            "Some functional.",
            "Operations so X here and forward is just a variable that is wrapped around a tensor and then you can manipulate X as you wish and then you need to return the output and that is a neural network, an here net which in the last line here, which is an object of the net.",
            "I will have a backward define for it, and when you call backward on the net with some some gradient.",
            "All of the leaf variables here, which are basically in this particular case, all of the parameters of the layers that are defined in the constructor, will have gradients filled into them in the dot grad attributes of those particular variables.",
            "So that's covering briefly neural networks and then."
        ],
        [
            "Then the other part that we have is an optimization package that implements a bunch of popular.",
            "Wait, there's a bunch of questions here, maybe here first.",
            "Yeah, so by default we give you a good default weight initialization, which is like a 99 efficient back prop.",
            "But we do have.",
            "You can initialize weights using alternative.",
            "Stuff like you know Gloria at initialization or hey initialization or other stuff.",
            "We have an end init sub package that implements all these different initializations, but by default we just do only one method, which is like an 98 or 99.",
            "Someone else had a question there.",
            "They did OK. Is there any plan?",
            "Have both porch and Piper quote at the same screen.",
            "But being able to use both of them at the same script?",
            "Well, the question is, can I use Torjan Pytorch code in the same script?",
            "The answer is no.",
            "And the reason is really stupid, it's because tortuous one based indexing and Pytorch is zero based indexing.",
            "So we actually compile our back end C libraries for torch using hash defined.",
            "That makes it do one based indexing and py torch.",
            "Using a zero based indexing.",
            "So if you load both of them into the same program, you probably will have symbol complex and like only one of them resolve an even if you do a Pytorch operation, it might resolve to using one based indexing or something.",
            "So shortly the answer is no.",
            "We don't plan to add a solution.",
            "There is a solution possible, but yeah.",
            "Question what is actually the relationship between torsion Pytorch?",
            "Py Torch is a better torch.",
            "OK, so.",
            "Yeah so.",
            "Between the torch and Pytorch projects, there's a few people who are common.",
            "We at some point we realized that tortuous and Luann like no one was using Lua.",
            "We an torches design is aging like if you ever used in graphene torch it's real pain to debug an in general like it was like a 7 year old design and we were like OK fine we need to rewrite it for whatever is like currently.",
            "Better design and so we wrote a new version of Torch and Python, but it uses the same back end C libraries where appropriate because we didn't want to write.",
            "An entire new framework.",
            "Separation, like if I go on GitHub of like the Pytorch stuff.",
            "And like the back end stuff and the non back end tour stuff.",
            "Yes.",
            "Pytorch has has a folder called Lib which has a bunch of C libraries and OC libraries are shared between Tordsen, Pytorch.",
            "OK, but they're not like.",
            "Their own separate places, and they just happen to their subtrees.",
            "If you want to be more specific.",
            "Clarified events Cape.",
            "Spencer on Monday PM on Tuesday.",
            "Those two framework or were more like computation based or disorder Brad stuff and then we mentioned a Monday tensor flow will support this eager mode, yes.",
            "Yes, so I delete it.",
            "A few slides actually.",
            "I don't know.",
            "Clarify distinctions about how this photographic works.",
            "Sure, I can do that when I might have a slide or two on that.",
            "If not, I will definitely cover like Huawei like the compilation based or symbolic.",
            "Differentiation packages are different from like auto.",
            "Autograde style packages.",
            "Anymore questions over here.",
            "OK so.",
            "So Py torch has an optimization package as well.",
            "Which has a bunch of popular gradient descent based optimization methods implemented an.",
            "It's pretty simple to use and it's very like nicely tide into our neural network module, so you use both of them hand in hand.",
            "OK, so this is pretty much all of Pytorch.",
            "And a few other things.",
            "But now I'll show you like by example, verify torch is actually nicer to use."
        ],
        [
            "Anne."
        ],
        [
            "So here so."
        ],
        [
            "Go through what research workflows look like.",
            "What are the main pain points an what are the what's like?",
            "Some guiding principles of Pytorch, Angest upcoming features.",
            "So Reese?"
        ],
        [
            "Work clothes usually."
        ],
        [
            "You have some idea or like a theory, or your professor has an idea an you know you guys design experiments and then you pick some datasets to like try to up the accuracy on the data set or something an or you pick some environments an like.",
            "I don't know Atari games for example and then you implement your models or ideas and you train your model.",
            "And then you add the results to your paper.",
            "That's sort of typically what happens, maybe in for, not everyone, so you also do."
        ],
        [
            "Literature review, because yeah, Smitter is watching you.",
            "So.",
            "That's a nice picture."
        ],
        [
            "So typically what what you have is you write data, set floaters like you picked your datasets, and then you're like that data set was written in some format that only another grad student knows and they wrote some documentation.",
            "So you have to write something that parses that data set into the format you want, and then you build your models.",
            "You implement a training loop and then because jobs crash you want to check pointer models occasionally.",
            "Ann, you build some baseline models, not always strong an then you also have to deal with GPU's because everyone in this room kind of wants to neural networks and neural networks are fast and GPS, and you build your optimizers and you do all of this.",
            "So Py, torch and Python together provide you an environment to do all of these steps nicely.",
            "Let's take."
        ],
        [
            "Writing data orders no one wants to do this.",
            "This is like the worst part of experimentation.",
            "Every data set is slightly differently formatted.",
            "You have to preprocess them in a nice way because otherwise your algorithms don't work.",
            "AI Ann.",
            "You need a multithreaded data loader because GPU's are fast and you want to preprocess your datasets fast enough to feed them to the GPU.",
            "Otherwise they become a bottleneck and all of this is a lot of boilerplate code that you can avoid what."
        ],
        [
            "We do in Pytorch is we provide data or data loaders that you can share with the Community, an.",
            "It's kind of nice like if you want to go do experiments on image, net, or in cocoa, you just use the vision package, which has all these data loaders and then you just.",
            "Do your experiments and."
        ],
        [
            "Similarly, we have a text package that is slightly more experimental, but it also has some data loaders.",
            "An oh, there is a question.",
            "Modular and can I get some other code or is it?",
            "Do they somehow only work with QuickBooks now?",
            "This date alerts are very, very modular.",
            "You can use the path or state of loaders an.",
            "Something else maybe?",
            "So the way the data loaders are written in Python, I'll get to that in a couple of slides, but."
        ],
        [
            "Another thing you can do is because Python is basically just deeply integrated into Python.",
            "You can use the Python interface is written to some datasets as part of your like data loader scheme.",
            "So as an example, Facebook not related to me at all.",
            "Just kidding.",
            "We released a thing called parlay which has data loaders for like 20 different text based datasets and you can just use that.",
            "To you know, do experiments, for example so."
        ],
        [
            "Oh, and practice."
        ],
        [
            "How do you use stuff like this?",
            "So this is small example.",
            "Small code snippet I took from the DC an example where if some command line option is equal to image net or like other datasets you create like an image folder data loader and you know you just specify what kind of vision transforms you want to do, and similarly if it's also in, you create a different data loader or sifar and within this small snippet of code.",
            "You get datasets and then with the lines 83 and 80 for the last two lines you get a multi process data loader that will take these datasets and then instantiate them in different processes and then basically do parallel processing to like preprocess your data set and load them from disk and so on.",
            "So in practice it works really well and like you don't have to write separate two code or anything you can do pythonic stuff.",
            "Anne and torch.",
            "I mean, pytorch.",
            "When you exchange sensors between processes using multi processing.",
            "We actually don't do any memory copy.",
            "We just use cyst like in Linux and OSX and Windows as well.",
            "You have this concept called shared memory where you can share memory between two processes.",
            "So we use that an.",
            "So it's generally pretty efficient to load your data into one process and then send them to you.",
            "Main process for transferring to the GPU and processing it."
        ],
        [
            "And generally, when you have to write a new data loader, it's also quite simple.",
            "You just subclass from like Tours dot Utils.",
            "Data set and then you implement two functions you implement to get item which basically given a particular index.",
            "It just like loads that index and returns it an you just filled up.",
            "Fill up that that function and you also define a length operator that defines how big your data set is.",
            "If you just have these two particular functions implemented, you basically have a data set that you can pass to the data loader.",
            "There is a question there.",
            "If you have a C Farlow Duran, you want to split the validation set.",
            "I thought everyone just or fits on the test set on cifar, but if you actually want to do validation, you probably just split it yourself.",
            "I mean, you can write this loader that has like certain indices, go into the validation set like basic programming thing.",
            "There's a question there.",
            "Can you scikit learn?",
            "Random split function maybe?",
            "I mean you can.",
            "There's no restrictions on what Python you put in here.",
            "You can do whatever you want, but I never tried scikit learn's random split.",
            "OK, let's move on."
        ],
        [
            "Anne Anne when it comes to interfacing with other end like say Enron means like video games.",
            "Mostly all of these environments these days provide a Python API, and because PY torch is just an extra extension, an Python you don't actually have to separately do some interfacing."
        ],
        [
            "You just need to interact with that enrollment directly and."
        ],
        [
            "Coming to the next pain point, everyone deals with this day-to-day debugging.",
            "It's one of the biggest pain points you have because somehow we tend to write bugs.",
            "So Pytorch is just the Python extension, so you can actually use your favorite Python debugger.",
            "You can use PDB, you can you."
        ],
        [
            "There's this thing that I googled and found.",
            "It looks like a fancy UI based debugger.",
            "It's called py charm.",
            "Ann, you could also you."
        ],
        [
            "As you know, the most popular debugger you can, you know, add print functions in between your code.",
            "That's kind of what I always do.",
            "So once you're done debugging your program, you have the next step, which is like, OK, my program now runs it sort of looks like it's training.",
            "How do I make it fast?",
            "So you want to identify?"
        ],
        [
            "I bottlenecks in your program and again because PY torch is just a Python extension.",
            "Use your favorite Python profiler.",
            "I like Snake quiz and."
        ],
        [
            "And.",
            "Like Andre likes, line profiler.",
            "A few people.",
            "I mean there's so many Python tools out there.",
            "You can use any of them.",
            "Anne, there's a question.",
            "That's not.",
            "But you cannot.",
            "Is any Python?",
            "Profile profile Princess NUM py code because calls into NUM py C code will not be measured.",
            "I see.",
            "Well, so with the.",
            "You will you will know which call was made into see, but if that SQL subsequently calls other calls, I think you don't know those.",
            "At least with like PDB and Snake with."
        ],
        [
            "So we do get traces of like which C function was actually called.",
            "Do you have how much time did each call take?",
            "Yes."
        ],
        [
            "That's kind of how we were.",
            "Read I do things day-to-day.",
            "If it involves CUDA then you can use like NVIDIA is like envy.",
            "Visual profiler for example.",
            "OK.",
            "The next pain point which I can't relate to, but I heard many people can."
        ],
        [
            "His compilation time so.",
            "So Py, torch, and torch were written for like the most impatient kind of people, so you don't have like a separate compilation process once you build your model or anything like there's no like model, compile and then like model that run you just like build your model.",
            "An like all of our CUDA code and like coral of RCP code.",
            "Their pre compiled and shipped.",
            "So you basically wait for nothing.",
            "There's no question.",
            "Yes, Pytorch integrates codeine in it integrates MKL.",
            "It integrates nickel, it accelerates.",
            "Whatever things we can accelerate without, we doing the actual work.",
            "OK.",
            "So.",
            "Another pain point which I can relate to is ecosystem because I used to use torch a lot.",
            "An ecosystem is like you just have a lot of other people also using something similar so you have a lot of open source code or packages available."
        ],
        [
            "The two so because PY torch is the Python extension and it has a strong NUM py bridge.",
            "That's essentially free.",
            "You can actually use Scipy or Scikit learn or whatever in.",
            "Writing your new layers, for example.",
            "Like when we release Pytorch one of them."
        ],
        [
            "This feedbacks we got.",
            "It was like this guy.",
            "Brandon Amos is a PhD student at CMU, he wrote.",
            "A primal I don't know.",
            "He wrote really complicated code that.",
            "That I know like it.",
            "You couldn't write it.",
            "It was just like some kind of equation solving thing and it wasn't obvious how to do it with other frameworks, so you can actually integrate, say, Scipy is like whatever algorithm an your forward or backward."
        ],
        [
            "We actually have a tutorial that exactly shows you how to do this.",
            "You can how to like create fire torch extensions using NUM, PY and SCI py.",
            "So you're saying that if you go back to the previous slide?"
        ],
        [
            "Here as well.",
            "Yes, because Pytorch is a tape based auto differentiation method.",
            "What it does is in your forward phase like let's say you have a while loop or for loop or whatever.",
            "It will record all the operations that happened in your forward and then when you finally call backward on some for variable you it will just replay all the operations on the tape and like backward order and then.",
            "Do the chain rule.",
            "So yes, you can do exactly what you described.",
            "Um?",
            "They should be because like your input is a tensor an you're doing like tensor operations or like you're applying some autograde function on it and in the backward those functions backwards or replayed essentially.",
            "You could try to use a Max on a variable.",
            "It will error out.",
            "So on a variable, if you try to do some like weird nondifferentiable thing, it will complain.",
            "But if you try to directly access your variables dot data attribute and then do a Max on it, you will get a result.",
            "But yes, it won't variable one.",
            "Know that this operation has happened.",
            "Cool so."
        ],
        [
            "Oh yes, we have.",
            "We have an example on how to, you know.",
            "Use sci-fi for example.",
            "Another part of the ecosystem is having a common model zoo.",
            "Of various kinds, Ann."
        ],
        [
            "We do have a shared model zoo.",
            "This is an example of our vision package where you can create like your favorite like image classification models and load some pre trained Imagenet training rates on to it.",
            "An out of the box it actually just downloads the weights from like my account on Amazon S3 an.",
            "And you will get those weights somewhere in your local disk an it's pretty convenient.",
            "Because it's a one liner, it makes it much more convenient an."
        ],
        [
            "What you can do is like use like some of the models in the model zoo and like do some random research on all these models together because they all come in the same interface and they're so convenient to use.",
            "Here is an example of someone who took all the models in Pytorch model Zoo and then measure like if you quantized weights.",
            "What kind of accuracy loss you would get an at, like what quantization level.",
            "Anne.",
            "So one of the lab."
        ],
        [
            "Best things I'm going to talk about is which I think is a pain point is.",
            "Most of the frameworks don't have a linear style of programming, and what I mean by that is.",
            "You first have an idea and then you have to think about how to map that idea into creating a model that that will create that idea, and then you create that model and you take it and then you will compile it or whatever optionally, and then you will use it in like pump data into it and backward.",
            "Now if you have.",
            "An issue with your model.",
            "You want a debugger model.",
            "You have to 1st figure out what's going wrong in your model and then map it back to like where you created the node in your model.",
            "An debugging itself becomes very painful, so.",
            "Bich"
        ],
        [
            "Does Pytorch has a very imperative style of programming?",
            "You don't necessarily create or have to create a separate model, you can.",
            "Write line, write code line by line and expect it to be differentiated properly and you expect that the debugger will stop in the exact place where you get an error an this.",
            "It's not unique to Pytorch.",
            "This is very important to us.",
            "Who are building Pytorch because we think it's very essential when you're doing like a fast iterative research cycle.",
            "You want to be able to debug as fast as possible, and you want to map your idea.",
            "Straight away into whatever your code is rather than like having some kind of slightly nonlinear or convoluted process so."
        ],
        [
            "As I mentioned, this is not unique to PY torch.",
            "There is a bunch of other frameworks that do this.",
            "But the way we did that we think it's important enough that we build Pytorch from the ground up.",
            "With this in mind, we want this imperative style of programming an we optimized all of the core code.",
            "So that even if you do like small models like fairly tiny models like amnesty models, you won't see any framework overhead when you're doing like this imperative style of programming on average are.",
            "For creating a node in the Pytorch graph is in the order of, like 20 to 30 microseconds.",
            "But if you try to do something similar in the imperative modes of these other frameworks, you will see the overhead rack up to milliseconds an seconds as well.",
            "And if there's a compilation process, maybe like it'll take an hour to compile, I don't know, but this we find this to be a senchal when you're doing research an.",
            "That with that IN.",
            "How you have inner research workflow?",
            "A lot of pain points and how Pytorch tries to solve them, and then in the."
        ],
        [
            "Next, 5 minutes I'm going to try to go through an example of implementing.",
            "Generative adversarial network with you it's like about 200 two 50 lines of code so I have to find my website.",
            "OK, it's here.",
            "So I can zoom in OK.",
            "So this is just an example of an what you see is a bunch of imports.",
            "14 lines of imports of all kinds.",
            "And then another bunch of like command line parsing stuff.",
            "But and you said some random seeds to hyper seed optimization.",
            "This is where you create your datasets.",
            "Depending on which command and option I have, I will create my data set object and then I will give my data set object to a data loader right there an in my data loader I'm giving what batch size I want the data loader to return and the number of workers that is the number of processes I want to.",
            "Simultaneously load my data and whether I want to shuffle my data set or not.",
            "There's a different there's more options there.",
            "This is just some of the options we used here, and then I create a small custom weight initialization function and then here I create my generator network.",
            "This is a DC Gan, so in the constructor and just creating sequential.",
            "Come sequential neural network and in the forward function I.",
            "If this is a CUDA that float tensor, that is, if it's on the GPU an.",
            "If I asked to use more than one GPU's, I am paralyzing my model to run on like as many GPS as as does a one liner in PY torch to use multiple GPUs.",
            "Also, I just, you know, just send the input through Maine and get an output under.",
            "Return the output.",
            "That's the generator network an I created generated network and I apply this weights in that function and the weights net function.",
            "It gets each module in the network and then it will try to find if it's a common layer.",
            "I've initialized my weights differently.",
            "If it's a batch norm then I initialized my weights differently.",
            "And then if this is an option to like load weights from this click from previous checkpoints.",
            "If I have some previous checkpoint that is specified, then I load the checkpoint from disk and load those weights into my network itself.",
            "If you do your first tutorial, you will cover all the subtleties and in the discriminator network I have sequential network with some convolutions and stuff and then.",
            "Also data parallel and then I have like some view operation.",
            "And I initialize my weights differently as well.",
            "My loss function here is a binary cross entropy loss, and here I pre create buffers for the inputs and labels just to reuse them.",
            "I guess I don't have to do this if I want to use CUDA.",
            "I push my model onto CUDA Ann.",
            "This is my training loop.",
            "So here I create optimizers.",
            "An Adam optimizer for my generator network Canon Adam Optimizer for my.",
            "Discriminator network and these optimizers get the an iterable over the para meters in your network.",
            "And I just have a double for loop now.",
            "With the training loop itself, I zero the gradients of my network and I.",
            "Create variables with my inputs I sent, pass them through the outputs and then I do a backward.",
            "Yes, there's a question there.",
            "Zero gradient thing.",
            "So in Pytorch all the gradients are accumulated into their buffers.",
            "They are not overwritten.",
            "So for example, if I do one forward backward and I get some gradients there and then I do another forward backward on the same variables, then you will have a sum of both of these gradients sitting in your grad buffers.",
            "There's various benefits to it so.",
            "Before you do your like next iteration, you would want to 0 all the gradients.",
            "Anne looks like people are either following or sleeping.",
            "I'm not sure.",
            "Ann you basically.",
            "I just passed your output of your net be through the criterion, which is a binary cross entropy loss, and then you just called backward an.",
            "Then you just record that is just a debugging, just like something for logging purposes, and similarly you train it fake.",
            "So you pass your noise through your net G and you get a fake image and then you pass the fake image so the output.",
            "You get some output and then you pass through the loss and then you do like a backward and then like optimizer dot step will do like the gradient like SGD step in this case like the Adam step.",
            "Similarly you have another section to forward through the real as a fake and after some kind of gang stuff.",
            "And here you print.",
            "Can you print some last stuff?",
            "I have every hundred iterations you save an image to disk with like generated samples and real samples and this is just checkpointing code.",
            "You just save the parameters to disk.",
            "So in this 265 lines an in like close to 5 minutes you just implemented a DC Gan.",
            "And hopefully this did help understand anything about Py torch.",
            "If not, you can go home and do the tutorials.",
            "What kind of functionality is there for the checkpointing?",
            "Is it similar to tensor flow in the same type?",
            "Really like bundles everything together?",
            "So we actually have two levels of.",
            "I lost my browser window anyway, so we have two kinds of checkpointing, usually for neural networks.",
            "You can just checkpoint the weights by.",
            "You see this, I'm supposed to go here.",
            "You see this function calls state dict here that basically returns a Dictionary of para meters with like names of the parameters and their dictionaries.",
            "So like you can have net D dot con, one dot weight is the string and then its corresponding torch tensor.",
            "So that's stating that you can save and then you can create another new Netley and then load it.",
            "Load these parameters back.",
            "That's one more simplistic way.",
            "The other option is to just pickle the Python class as is, and both of them are supported.",
            "I usually prefer the state Dick thing because it's stupid an so your checkpoints are more interpretable in a later situation.",
            "Station.",
            "Questions geovisualization thing you can use like whatever is your favorite Python visualization tool like you can plot some stuff with Matplotlib.",
            "You can use tensor board if you want.",
            "I mean you can do any use any of the Python tools pretty much.",
            "OK.",
            "I have about 7 minutes OK quickly."
        ],
        [
            "The philosophy of paper."
        ],
        [
            "Course, most people here don't care, so.",
            "Stay out of the way is one of our big things like we want to minimize the abstractions we have.",
            "Like we don't want you to debug through seven different classes before you get to the actual code that's running it.",
            "Cater to the impatient, just like don't let users wait on anything.",
            "No compilation times, just like have everything be very imperative.",
            "Promote linear code flow.",
            "I explained that full interop with the Python ecosystem.",
            "An last one is be as fast as any other framework really like are like we don't want to sacrifice flexibility for speed and we do a lot of low level core engineering.",
            "To make sure that we keep things flexible, but we also are really, really fast an where within 10% of most, like all frameworks.",
            "I think unlike most usual deep learning workloads, an if we are slower on some workloads and people get these to our attention, we make sure that we find ways to like fix these performance issues.",
            "Then we take that be as fast as possible.",
            "Very seriously.",
            "OK, last."
        ],
        [
            "Only upcoming features.",
            "We are going to release a .2 version of fighter fighters now has like Zero point 1.1 two which is like the current version that we distribute in .2.",
            "We will be having the following features."
        ],
        [
            "So we will have distributed Pytorch.",
            "So you will have an MPI style distributed communication, which means that you will have functions in Pytorch Dot distributed that will let you send a tensor to another node.",
            "And receive a tensor from other node or like reduced answers among all nodes an.",
            "And this is usually pretty popular in the computer science community, and there's a big library that does this, call MPI that's very popular and very old.",
            "And it has a certain style of an that's basically like doing this, like exchanging data between nodes.",
            "So we follow that style.",
            "But we can we provide primitives that let you exchange tensors among all the disputed nodes an."
        ],
        [
            "We are also introducing allowing you to do higher order derivatives so you can actually do something like this an it will obviously be incorrect because of floating point issues, but mathematically it will try to do the right thing.",
            "Anne Anne, it's usually like there is some papers in recent times that have been using like higher order directives.",
            "So yeah, you can implement crazy ideas than you are.",
            "I guess going to implement crazy ideas or at least some of you."
        ],
        [
            "We will also be introducing NUM PY style, broadcasting an advanced indexing.",
            "This is mostly like sugar to like.",
            "Avoid for users to avoid writing like a lot of additional code, and this is something that users are usually very very familiar from.",
            "Vendor using NUM Py, so we just are introducing that as well.",
            "An these three features I order derivative, distributed Pytorch, an advanced indexing, broadcasting are coming out in .2.",
            "But we're also working more in like the slightly longer time frame.",
            "Maybe .3."
        ],
        [
            "Is adding a compiler to Pytorch that is just in time compiled as a user?",
            "You won't notice anything you don't have to notice anything but.",
            "At the back end, what we're doing is we're looking at a bunch of your computation that's happening and seeing if we can.",
            "Code generate much more efficient code that does the same computation that were repeatedly seeing an so equal, and there's so few columns for this in the community.",
            "Tensor flow has this project called XLA that tries to do this an the MX net guys have this project called NVM and their upcoming TVM so.",
            "We are doing something similar.",
            "We actually also leverage some of the components that the Amazon MX net team has built.",
            "And All in all, let's just say that you will see faster things, Ann you.",
            "Then you will know why.",
            "And generally, the way this is done, this is done in several ways.",
            "We haven't really converged on which way we're going to go, but one way."
        ],
        [
            "Is you can do something lazy evaluation where when you execute all this code, it actually doesn't execute the code like it executes the code, but it doesn't actually do the mathematical operations themselves, just queues up the operations and keeps them somewhere an when you ask for."
        ],
        [
            "The actual data.",
            "Let's say we print it or you like.",
            "Use it in some other OPEC function or whatever.",
            "It will then execute all these queued up operations and we."
        ],
        [
            "We'll then have a little bit of time between building the graph and executing the graph, and we can use that little bit of time to compile this code.",
            "We can also do stuff like graph caching, which means you can if we we will compile this workload for if we see it again in the future, then it will go faster, But the first time it won't go faster, so we're still deciding a bunch of things.",
            "Underlying this compiler feature, but it is happening."
        ],
        [
            "One of the advantages have once."
        ],
        [
            "Yeah, that's kind of cute."
        ],
        [
            "The advantages of compilation is you can get things to go faster using these buzzwords.",
            "Lastly, I might have.",
            "Time Oh my God, I have 30 more minutes.",
            "I thought this was a one hour presentation, my bad.",
            "Anyways, my last slide."
        ],
        [
            "Is.",
            "This.",
            "Anne.",
            "So we really Don Janet in.",
            "We have a wide range of people working on an using Pytorch.",
            "I'm hoping to get a bunch of people here who are not already Pytorch users to also use it an we want to hopefully add more names there.",
            "Ann, you can ask me any questions you want."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Excellent.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "This sounds good, OK?",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I heard this is the future of deep learning right here.",
                    "label": 0
                },
                {
                    "sent": "That's a lot of people.",
                    "label": 0
                },
                {
                    "sent": "Well then it came in.",
                    "label": 0
                },
                {
                    "sent": "I thought oh higher, higher higher.",
                    "label": 0
                },
                {
                    "sent": "Hello yes.",
                    "label": 0
                },
                {
                    "sent": "I was just saying how this is the future of deep learning.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "When I came in, I actually thought this was just the Miller lab and.",
                    "label": 0
                },
                {
                    "sent": "Have you analysis somewhere alright?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I will be talking about Pytorch.",
                    "label": 0
                },
                {
                    "sent": "And that is a list of people who have been working on Pytorch for the last few months in various ways.",
                    "label": 0
                },
                {
                    "sent": "And it's not a full list of obviously.",
                    "label": 0
                },
                {
                    "sent": "An we Pytorch is a framework for deep learning.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My first slide is.",
                    "label": 0
                },
                {
                    "sent": "What is pytorch?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An basically Pytorch has a few competence.",
                    "label": 0
                },
                {
                    "sent": "It has it's an interior library, like also called tensors.",
                    "label": 0
                },
                {
                    "sent": "These days, which it has strong GPU support, so you can think of it as NUM PY, but with GPU support an.",
                    "label": 1
                },
                {
                    "sent": "It is also an automatic differentiation engine.",
                    "label": 1
                },
                {
                    "sent": "So you can find the gradients of one thing with respect to another thing in your library.",
                    "label": 1
                },
                {
                    "sent": "And you also have gradient based optimization methods such as SGD, RMS, prop, Adam etc.",
                    "label": 0
                },
                {
                    "sent": "And we also have utilities for various things you would want to do when you're doing experimentation, such as data loading, etc.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of end area library, Pytorch.",
                    "label": 0
                },
                {
                    "sent": "Has a class of.",
                    "label": 0
                },
                {
                    "sent": "Tensors called torch tensor, torso float, tensor torch, double tensor, etc.",
                    "label": 0
                },
                {
                    "sent": "They are equal into non pies ndra an we have quite a few operations that do linear algebra, tensor manipulation, slicing, etc.",
                    "label": 0
                },
                {
                    "sent": "An we have the same API also available to run both on the CPU an on the GPU with full parity.",
                    "label": 0
                },
                {
                    "sent": "So to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly you can if you.",
                    "label": 0
                },
                {
                    "sent": "If you want to see py torch, you can see it as an exactly Clinton on pie on the left side is a program written in NUM, PY and on the right side it's written in Py torch.",
                    "label": 0
                },
                {
                    "sent": "I actually can't remember what this is for.",
                    "label": 0
                },
                {
                    "sent": "Let's just say it has some values in there and some gradients so.",
                    "label": 0
                },
                {
                    "sent": "I think it's Justin Johnson's homework stuff from Stanford courses I can't remember, but anyways, they look exactly equivalent.",
                    "label": 0
                },
                {
                    "sent": "You create tensors, you do operations on them, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is kind of how it looks like in the code you import torch and then.",
                    "label": 0
                },
                {
                    "sent": "You create tensors and you can print them.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An you can get their size you can create.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random tensors you can slice tensors, do standard indexing with bells and whistles.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Add tensors, multiply tensors metric.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiply linear solves etc.",
                    "label": 0
                },
                {
                    "sent": "So apart from just being a full tensor library.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We understand that Python And NUM.",
                    "label": 0
                },
                {
                    "sent": "Py have a very huge ecosystem, so py torch has NUM py bridge, where you can convert a pytorch array into an umpire, and vice versa in an extremely efficient fashion where there's no mem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Copy being done.",
                    "label": 0
                },
                {
                    "sent": "It's almost like a free operation an when you convert a pressure sensor to an umpire, you changed an umpire.",
                    "label": 0
                },
                {
                    "sent": "The original Pytorch tensor is changed an if you change the values of the Pytorch tensor, then the NUM PY arrays values are changed.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Basically we shared the underlying data pointer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This is just an example of that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have GPU tensors, so if you have a tensor that you create, if you want to transfer it over to the GPU here is called CUDA on it and return is a torch, CUDA dot float tensor for example an if you want to.",
                    "label": 0
                },
                {
                    "sent": "If you have a GPU tensor and a few other transferred back to the CPU, just called out CPU on it and then the GPU tensor is transferred back onto the CPU.",
                    "label": 0
                },
                {
                    "sent": "It's almost like you're essentially controlling memory regions, and then you can.",
                    "label": 0
                },
                {
                    "sent": "You view them as Python objects, sure.",
                    "label": 0
                },
                {
                    "sent": "So where tensor here is a different meaning than the.",
                    "label": 0
                },
                {
                    "sent": "Tensor flow.",
                    "label": 0
                },
                {
                    "sent": "It's the same tensor flow tensor.",
                    "label": 0
                },
                {
                    "sent": "I was actually thinking that yes, it's not like it's not a physics tensor.",
                    "label": 0
                },
                {
                    "sent": "It's deep learning tensor.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We also have an automatic differentiation engine.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh should I move to pipers it's faster or does the same back if you're using torch, you would benefit from moving to Pytorch.",
                    "label": 0
                },
                {
                    "sent": "But they are the same speed.",
                    "label": 0
                },
                {
                    "sent": "It is true.",
                    "label": 0
                },
                {
                    "sent": "And in fighters we have unit tests and everything.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Anti towards we have an automatic differentiation engine just like every other deep learning framework an so you this is small example where you're creating.",
                    "label": 0
                },
                {
                    "sent": "The Pi torches, autograph engines, variables and what variables here are variables or lose wrappers around tensors.",
                    "label": 0
                },
                {
                    "sent": "An if you do any operation on the variable then the variable remembers what happened to it and then when you do a backward on some final variable then the you will compute gradients with respect to all of the leaf variables in the graph and by leave variables I mean variables that the user themselves created.",
                    "label": 0
                },
                {
                    "sent": "So in this example, as a user I'm creating X pre Patch WHWX and then I'm doing some matrix multiplies on there and then I'm just adding them up.",
                    "label": 0
                },
                {
                    "sent": "An I do a tennis, obviously out of fashion.",
                    "label": 0
                },
                {
                    "sent": "And then I do a backward on the final variable with just say, like one gradients an.",
                    "label": 0
                },
                {
                    "sent": "Then you will have gradients accumulated in X prefetch WH&WX with respect to this chain of operations that were done.",
                    "label": 0
                },
                {
                    "sent": "So you can create neural net.",
                    "label": 0
                },
                {
                    "sent": "Oh wait, there's a question.",
                    "label": 0
                },
                {
                    "sent": "More precisely, backward will apply the chain rule an with respect to the gradient.",
                    "label": 0
                },
                {
                    "sent": "The torch dot once over there, which are treated as like.",
                    "label": 0
                },
                {
                    "sent": "Gradient with respect to your output.",
                    "label": 0
                },
                {
                    "sent": "Let's say there's some final scalar output.",
                    "label": 0
                },
                {
                    "sent": "You will compute the gradient of each of those ones with respect to, like X, prove HWHNWX.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I can use the whiteboard.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "I think this is the deep learning summer school someone should have taught this stuff.",
                    "label": 0
                },
                {
                    "sent": "But so if you have L = L of F of X, W. Where F is a function.",
                    "label": 0
                },
                {
                    "sent": "An then you take some loss with respect to some target value and you get a final loss.",
                    "label": 0
                },
                {
                    "sent": "Now you call L dot backward.",
                    "label": 0
                },
                {
                    "sent": "An L is a scalar value, so you called out backward and then what will be computed is DL by DW.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne DL by DX.",
                    "label": 0
                },
                {
                    "sent": "An generally this is the gradient that will be stored in the dot grad attribute of this variable an you can do an HDD step on this with a small learning rate and you repeatedly do this an that's basically all of what I learned.",
                    "label": 0
                },
                {
                    "sent": "It's still not clear to me, so what's the input to the backward?",
                    "label": 0
                },
                {
                    "sent": "The input to the back?",
                    "label": 0
                },
                {
                    "sent": "So if L is a scalar loss, we just trade the backward as a like you just have one that is being propagated back, but if it's not a scalar loss then you will need some.",
                    "label": 0
                },
                {
                    "sent": "Some gradient values that are representing some future, like some way to do scalar.",
                    "label": 0
                },
                {
                    "sent": "120 yes, exactly.",
                    "label": 0
                },
                {
                    "sent": "That will be great, and so the next time to score Edge.",
                    "label": 0
                },
                {
                    "sent": "So does it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't actually output these gradients or just assigns them to the dot grad of variables.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly awesome, we're making progress.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next is a small example of going from.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like OK, you have this autograde engine?",
                    "label": 0
                },
                {
                    "sent": "How do you painlessly create neural networks an so?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a loose wrapper around this AutoCAD engine that lets you construct neural networks without having to deal with like a gazillion objects that you manually handle, so you can create a more formal neural network by subclassing from North module, which is available in the torch package an in the constructor.",
                    "label": 0
                },
                {
                    "sent": "Let's say you create all of the.",
                    "label": 0
                },
                {
                    "sent": "Layers that you would want to use.",
                    "label": 0
                },
                {
                    "sent": "For example, a convolution layer is just like a convolution object that holds on to the weights and biases of the convolution that is being done.",
                    "label": 0
                },
                {
                    "sent": "And the FC layer is like an affine transform an in the forward function you actually represent.",
                    "label": 0
                },
                {
                    "sent": "You take the input in the forward function and your present how the input goes to the output an in this forward function you can do any kind of operations.",
                    "label": 0
                },
                {
                    "sent": "You can do all kind of pythonic operations like not only just referencing your layers that you define in the constructor, but also like some functional.",
                    "label": 0
                },
                {
                    "sent": "Some functional.",
                    "label": 0
                },
                {
                    "sent": "Operations so X here and forward is just a variable that is wrapped around a tensor and then you can manipulate X as you wish and then you need to return the output and that is a neural network, an here net which in the last line here, which is an object of the net.",
                    "label": 0
                },
                {
                    "sent": "I will have a backward define for it, and when you call backward on the net with some some gradient.",
                    "label": 0
                },
                {
                    "sent": "All of the leaf variables here, which are basically in this particular case, all of the parameters of the layers that are defined in the constructor, will have gradients filled into them in the dot grad attributes of those particular variables.",
                    "label": 0
                },
                {
                    "sent": "So that's covering briefly neural networks and then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the other part that we have is an optimization package that implements a bunch of popular.",
                    "label": 0
                },
                {
                    "sent": "Wait, there's a bunch of questions here, maybe here first.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so by default we give you a good default weight initialization, which is like a 99 efficient back prop.",
                    "label": 0
                },
                {
                    "sent": "But we do have.",
                    "label": 0
                },
                {
                    "sent": "You can initialize weights using alternative.",
                    "label": 0
                },
                {
                    "sent": "Stuff like you know Gloria at initialization or hey initialization or other stuff.",
                    "label": 0
                },
                {
                    "sent": "We have an end init sub package that implements all these different initializations, but by default we just do only one method, which is like an 98 or 99.",
                    "label": 0
                },
                {
                    "sent": "Someone else had a question there.",
                    "label": 0
                },
                {
                    "sent": "They did OK. Is there any plan?",
                    "label": 0
                },
                {
                    "sent": "Have both porch and Piper quote at the same screen.",
                    "label": 0
                },
                {
                    "sent": "But being able to use both of them at the same script?",
                    "label": 0
                },
                {
                    "sent": "Well, the question is, can I use Torjan Pytorch code in the same script?",
                    "label": 0
                },
                {
                    "sent": "The answer is no.",
                    "label": 0
                },
                {
                    "sent": "And the reason is really stupid, it's because tortuous one based indexing and Pytorch is zero based indexing.",
                    "label": 0
                },
                {
                    "sent": "So we actually compile our back end C libraries for torch using hash defined.",
                    "label": 0
                },
                {
                    "sent": "That makes it do one based indexing and py torch.",
                    "label": 0
                },
                {
                    "sent": "Using a zero based indexing.",
                    "label": 0
                },
                {
                    "sent": "So if you load both of them into the same program, you probably will have symbol complex and like only one of them resolve an even if you do a Pytorch operation, it might resolve to using one based indexing or something.",
                    "label": 0
                },
                {
                    "sent": "So shortly the answer is no.",
                    "label": 0
                },
                {
                    "sent": "We don't plan to add a solution.",
                    "label": 0
                },
                {
                    "sent": "There is a solution possible, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Question what is actually the relationship between torsion Pytorch?",
                    "label": 0
                },
                {
                    "sent": "Py Torch is a better torch.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Between the torch and Pytorch projects, there's a few people who are common.",
                    "label": 0
                },
                {
                    "sent": "We at some point we realized that tortuous and Luann like no one was using Lua.",
                    "label": 0
                },
                {
                    "sent": "We an torches design is aging like if you ever used in graphene torch it's real pain to debug an in general like it was like a 7 year old design and we were like OK fine we need to rewrite it for whatever is like currently.",
                    "label": 0
                },
                {
                    "sent": "Better design and so we wrote a new version of Torch and Python, but it uses the same back end C libraries where appropriate because we didn't want to write.",
                    "label": 0
                },
                {
                    "sent": "An entire new framework.",
                    "label": 0
                },
                {
                    "sent": "Separation, like if I go on GitHub of like the Pytorch stuff.",
                    "label": 0
                },
                {
                    "sent": "And like the back end stuff and the non back end tour stuff.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Pytorch has has a folder called Lib which has a bunch of C libraries and OC libraries are shared between Tordsen, Pytorch.",
                    "label": 0
                },
                {
                    "sent": "OK, but they're not like.",
                    "label": 0
                },
                {
                    "sent": "Their own separate places, and they just happen to their subtrees.",
                    "label": 0
                },
                {
                    "sent": "If you want to be more specific.",
                    "label": 0
                },
                {
                    "sent": "Clarified events Cape.",
                    "label": 0
                },
                {
                    "sent": "Spencer on Monday PM on Tuesday.",
                    "label": 0
                },
                {
                    "sent": "Those two framework or were more like computation based or disorder Brad stuff and then we mentioned a Monday tensor flow will support this eager mode, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I delete it.",
                    "label": 0
                },
                {
                    "sent": "A few slides actually.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Clarify distinctions about how this photographic works.",
                    "label": 0
                },
                {
                    "sent": "Sure, I can do that when I might have a slide or two on that.",
                    "label": 0
                },
                {
                    "sent": "If not, I will definitely cover like Huawei like the compilation based or symbolic.",
                    "label": 0
                },
                {
                    "sent": "Differentiation packages are different from like auto.",
                    "label": 0
                },
                {
                    "sent": "Autograde style packages.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions over here.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "So Py torch has an optimization package as well.",
                    "label": 1
                },
                {
                    "sent": "Which has a bunch of popular gradient descent based optimization methods implemented an.",
                    "label": 0
                },
                {
                    "sent": "It's pretty simple to use and it's very like nicely tide into our neural network module, so you use both of them hand in hand.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is pretty much all of Pytorch.",
                    "label": 0
                },
                {
                    "sent": "And a few other things.",
                    "label": 0
                },
                {
                    "sent": "But now I'll show you like by example, verify torch is actually nicer to use.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go through what research workflows look like.",
                    "label": 1
                },
                {
                    "sent": "What are the main pain points an what are the what's like?",
                    "label": 1
                },
                {
                    "sent": "Some guiding principles of Pytorch, Angest upcoming features.",
                    "label": 0
                },
                {
                    "sent": "So Reese?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work clothes usually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have some idea or like a theory, or your professor has an idea an you know you guys design experiments and then you pick some datasets to like try to up the accuracy on the data set or something an or you pick some environments an like.",
                    "label": 0
                },
                {
                    "sent": "I don't know Atari games for example and then you implement your models or ideas and you train your model.",
                    "label": 0
                },
                {
                    "sent": "And then you add the results to your paper.",
                    "label": 1
                },
                {
                    "sent": "That's sort of typically what happens, maybe in for, not everyone, so you also do.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Literature review, because yeah, Smitter is watching you.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's a nice picture.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So typically what what you have is you write data, set floaters like you picked your datasets, and then you're like that data set was written in some format that only another grad student knows and they wrote some documentation.",
                    "label": 0
                },
                {
                    "sent": "So you have to write something that parses that data set into the format you want, and then you build your models.",
                    "label": 0
                },
                {
                    "sent": "You implement a training loop and then because jobs crash you want to check pointer models occasionally.",
                    "label": 0
                },
                {
                    "sent": "Ann, you build some baseline models, not always strong an then you also have to deal with GPU's because everyone in this room kind of wants to neural networks and neural networks are fast and GPS, and you build your optimizers and you do all of this.",
                    "label": 0
                },
                {
                    "sent": "So Py, torch and Python together provide you an environment to do all of these steps nicely.",
                    "label": 1
                },
                {
                    "sent": "Let's take.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Writing data orders no one wants to do this.",
                    "label": 0
                },
                {
                    "sent": "This is like the worst part of experimentation.",
                    "label": 0
                },
                {
                    "sent": "Every data set is slightly differently formatted.",
                    "label": 1
                },
                {
                    "sent": "You have to preprocess them in a nice way because otherwise your algorithms don't work.",
                    "label": 0
                },
                {
                    "sent": "AI Ann.",
                    "label": 0
                },
                {
                    "sent": "You need a multithreaded data loader because GPU's are fast and you want to preprocess your datasets fast enough to feed them to the GPU.",
                    "label": 0
                },
                {
                    "sent": "Otherwise they become a bottleneck and all of this is a lot of boilerplate code that you can avoid what.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do in Pytorch is we provide data or data loaders that you can share with the Community, an.",
                    "label": 1
                },
                {
                    "sent": "It's kind of nice like if you want to go do experiments on image, net, or in cocoa, you just use the vision package, which has all these data loaders and then you just.",
                    "label": 0
                },
                {
                    "sent": "Do your experiments and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similarly, we have a text package that is slightly more experimental, but it also has some data loaders.",
                    "label": 1
                },
                {
                    "sent": "An oh, there is a question.",
                    "label": 0
                },
                {
                    "sent": "Modular and can I get some other code or is it?",
                    "label": 0
                },
                {
                    "sent": "Do they somehow only work with QuickBooks now?",
                    "label": 0
                },
                {
                    "sent": "This date alerts are very, very modular.",
                    "label": 0
                },
                {
                    "sent": "You can use the path or state of loaders an.",
                    "label": 0
                },
                {
                    "sent": "Something else maybe?",
                    "label": 1
                },
                {
                    "sent": "So the way the data loaders are written in Python, I'll get to that in a couple of slides, but.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing you can do is because Python is basically just deeply integrated into Python.",
                    "label": 0
                },
                {
                    "sent": "You can use the Python interface is written to some datasets as part of your like data loader scheme.",
                    "label": 0
                },
                {
                    "sent": "So as an example, Facebook not related to me at all.",
                    "label": 0
                },
                {
                    "sent": "Just kidding.",
                    "label": 0
                },
                {
                    "sent": "We released a thing called parlay which has data loaders for like 20 different text based datasets and you can just use that.",
                    "label": 1
                },
                {
                    "sent": "To you know, do experiments, for example so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, and practice.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do you use stuff like this?",
                    "label": 0
                },
                {
                    "sent": "So this is small example.",
                    "label": 0
                },
                {
                    "sent": "Small code snippet I took from the DC an example where if some command line option is equal to image net or like other datasets you create like an image folder data loader and you know you just specify what kind of vision transforms you want to do, and similarly if it's also in, you create a different data loader or sifar and within this small snippet of code.",
                    "label": 0
                },
                {
                    "sent": "You get datasets and then with the lines 83 and 80 for the last two lines you get a multi process data loader that will take these datasets and then instantiate them in different processes and then basically do parallel processing to like preprocess your data set and load them from disk and so on.",
                    "label": 0
                },
                {
                    "sent": "So in practice it works really well and like you don't have to write separate two code or anything you can do pythonic stuff.",
                    "label": 1
                },
                {
                    "sent": "Anne and torch.",
                    "label": 0
                },
                {
                    "sent": "I mean, pytorch.",
                    "label": 0
                },
                {
                    "sent": "When you exchange sensors between processes using multi processing.",
                    "label": 0
                },
                {
                    "sent": "We actually don't do any memory copy.",
                    "label": 0
                },
                {
                    "sent": "We just use cyst like in Linux and OSX and Windows as well.",
                    "label": 0
                },
                {
                    "sent": "You have this concept called shared memory where you can share memory between two processes.",
                    "label": 0
                },
                {
                    "sent": "So we use that an.",
                    "label": 0
                },
                {
                    "sent": "So it's generally pretty efficient to load your data into one process and then send them to you.",
                    "label": 0
                },
                {
                    "sent": "Main process for transferring to the GPU and processing it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And generally, when you have to write a new data loader, it's also quite simple.",
                    "label": 0
                },
                {
                    "sent": "You just subclass from like Tours dot Utils.",
                    "label": 0
                },
                {
                    "sent": "Data set and then you implement two functions you implement to get item which basically given a particular index.",
                    "label": 0
                },
                {
                    "sent": "It just like loads that index and returns it an you just filled up.",
                    "label": 0
                },
                {
                    "sent": "Fill up that that function and you also define a length operator that defines how big your data set is.",
                    "label": 0
                },
                {
                    "sent": "If you just have these two particular functions implemented, you basically have a data set that you can pass to the data loader.",
                    "label": 0
                },
                {
                    "sent": "There is a question there.",
                    "label": 0
                },
                {
                    "sent": "If you have a C Farlow Duran, you want to split the validation set.",
                    "label": 0
                },
                {
                    "sent": "I thought everyone just or fits on the test set on cifar, but if you actually want to do validation, you probably just split it yourself.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can write this loader that has like certain indices, go into the validation set like basic programming thing.",
                    "label": 0
                },
                {
                    "sent": "There's a question there.",
                    "label": 0
                },
                {
                    "sent": "Can you scikit learn?",
                    "label": 0
                },
                {
                    "sent": "Random split function maybe?",
                    "label": 0
                },
                {
                    "sent": "I mean you can.",
                    "label": 0
                },
                {
                    "sent": "There's no restrictions on what Python you put in here.",
                    "label": 0
                },
                {
                    "sent": "You can do whatever you want, but I never tried scikit learn's random split.",
                    "label": 0
                },
                {
                    "sent": "OK, let's move on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne Anne when it comes to interfacing with other end like say Enron means like video games.",
                    "label": 0
                },
                {
                    "sent": "Mostly all of these environments these days provide a Python API, and because PY torch is just an extra extension, an Python you don't actually have to separately do some interfacing.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just need to interact with that enrollment directly and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coming to the next pain point, everyone deals with this day-to-day debugging.",
                    "label": 0
                },
                {
                    "sent": "It's one of the biggest pain points you have because somehow we tend to write bugs.",
                    "label": 0
                },
                {
                    "sent": "So Pytorch is just the Python extension, so you can actually use your favorite Python debugger.",
                    "label": 1
                },
                {
                    "sent": "You can use PDB, you can you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's this thing that I googled and found.",
                    "label": 0
                },
                {
                    "sent": "It looks like a fancy UI based debugger.",
                    "label": 0
                },
                {
                    "sent": "It's called py charm.",
                    "label": 0
                },
                {
                    "sent": "Ann, you could also you.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you know, the most popular debugger you can, you know, add print functions in between your code.",
                    "label": 0
                },
                {
                    "sent": "That's kind of what I always do.",
                    "label": 0
                },
                {
                    "sent": "So once you're done debugging your program, you have the next step, which is like, OK, my program now runs it sort of looks like it's training.",
                    "label": 0
                },
                {
                    "sent": "How do I make it fast?",
                    "label": 0
                },
                {
                    "sent": "So you want to identify?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I bottlenecks in your program and again because PY torch is just a Python extension.",
                    "label": 0
                },
                {
                    "sent": "Use your favorite Python profiler.",
                    "label": 1
                },
                {
                    "sent": "I like Snake quiz and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Like Andre likes, line profiler.",
                    "label": 0
                },
                {
                    "sent": "A few people.",
                    "label": 0
                },
                {
                    "sent": "I mean there's so many Python tools out there.",
                    "label": 0
                },
                {
                    "sent": "You can use any of them.",
                    "label": 0
                },
                {
                    "sent": "Anne, there's a question.",
                    "label": 0
                },
                {
                    "sent": "That's not.",
                    "label": 0
                },
                {
                    "sent": "But you cannot.",
                    "label": 0
                },
                {
                    "sent": "Is any Python?",
                    "label": 0
                },
                {
                    "sent": "Profile profile Princess NUM py code because calls into NUM py C code will not be measured.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "Well, so with the.",
                    "label": 0
                },
                {
                    "sent": "You will you will know which call was made into see, but if that SQL subsequently calls other calls, I think you don't know those.",
                    "label": 0
                },
                {
                    "sent": "At least with like PDB and Snake with.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we do get traces of like which C function was actually called.",
                    "label": 0
                },
                {
                    "sent": "Do you have how much time did each call take?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's kind of how we were.",
                    "label": 0
                },
                {
                    "sent": "Read I do things day-to-day.",
                    "label": 0
                },
                {
                    "sent": "If it involves CUDA then you can use like NVIDIA is like envy.",
                    "label": 0
                },
                {
                    "sent": "Visual profiler for example.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The next pain point which I can't relate to, but I heard many people can.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His compilation time so.",
                    "label": 0
                },
                {
                    "sent": "So Py, torch, and torch were written for like the most impatient kind of people, so you don't have like a separate compilation process once you build your model or anything like there's no like model, compile and then like model that run you just like build your model.",
                    "label": 0
                },
                {
                    "sent": "An like all of our CUDA code and like coral of RCP code.",
                    "label": 0
                },
                {
                    "sent": "Their pre compiled and shipped.",
                    "label": 0
                },
                {
                    "sent": "So you basically wait for nothing.",
                    "label": 0
                },
                {
                    "sent": "There's no question.",
                    "label": 0
                },
                {
                    "sent": "Yes, Pytorch integrates codeine in it integrates MKL.",
                    "label": 0
                },
                {
                    "sent": "It integrates nickel, it accelerates.",
                    "label": 0
                },
                {
                    "sent": "Whatever things we can accelerate without, we doing the actual work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another pain point which I can relate to is ecosystem because I used to use torch a lot.",
                    "label": 0
                },
                {
                    "sent": "An ecosystem is like you just have a lot of other people also using something similar so you have a lot of open source code or packages available.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The two so because PY torch is the Python extension and it has a strong NUM py bridge.",
                    "label": 0
                },
                {
                    "sent": "That's essentially free.",
                    "label": 0
                },
                {
                    "sent": "You can actually use Scipy or Scikit learn or whatever in.",
                    "label": 0
                },
                {
                    "sent": "Writing your new layers, for example.",
                    "label": 0
                },
                {
                    "sent": "Like when we release Pytorch one of them.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This feedbacks we got.",
                    "label": 0
                },
                {
                    "sent": "It was like this guy.",
                    "label": 0
                },
                {
                    "sent": "Brandon Amos is a PhD student at CMU, he wrote.",
                    "label": 0
                },
                {
                    "sent": "A primal I don't know.",
                    "label": 0
                },
                {
                    "sent": "He wrote really complicated code that.",
                    "label": 0
                },
                {
                    "sent": "That I know like it.",
                    "label": 0
                },
                {
                    "sent": "You couldn't write it.",
                    "label": 0
                },
                {
                    "sent": "It was just like some kind of equation solving thing and it wasn't obvious how to do it with other frameworks, so you can actually integrate, say, Scipy is like whatever algorithm an your forward or backward.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually have a tutorial that exactly shows you how to do this.",
                    "label": 0
                },
                {
                    "sent": "You can how to like create fire torch extensions using NUM, PY and SCI py.",
                    "label": 0
                },
                {
                    "sent": "So you're saying that if you go back to the previous slide?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here as well.",
                    "label": 0
                },
                {
                    "sent": "Yes, because Pytorch is a tape based auto differentiation method.",
                    "label": 0
                },
                {
                    "sent": "What it does is in your forward phase like let's say you have a while loop or for loop or whatever.",
                    "label": 0
                },
                {
                    "sent": "It will record all the operations that happened in your forward and then when you finally call backward on some for variable you it will just replay all the operations on the tape and like backward order and then.",
                    "label": 0
                },
                {
                    "sent": "Do the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So yes, you can do exactly what you described.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "They should be because like your input is a tensor an you're doing like tensor operations or like you're applying some autograde function on it and in the backward those functions backwards or replayed essentially.",
                    "label": 0
                },
                {
                    "sent": "You could try to use a Max on a variable.",
                    "label": 0
                },
                {
                    "sent": "It will error out.",
                    "label": 0
                },
                {
                    "sent": "So on a variable, if you try to do some like weird nondifferentiable thing, it will complain.",
                    "label": 0
                },
                {
                    "sent": "But if you try to directly access your variables dot data attribute and then do a Max on it, you will get a result.",
                    "label": 0
                },
                {
                    "sent": "But yes, it won't variable one.",
                    "label": 0
                },
                {
                    "sent": "Know that this operation has happened.",
                    "label": 0
                },
                {
                    "sent": "Cool so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yes, we have.",
                    "label": 0
                },
                {
                    "sent": "We have an example on how to, you know.",
                    "label": 0
                },
                {
                    "sent": "Use sci-fi for example.",
                    "label": 0
                },
                {
                    "sent": "Another part of the ecosystem is having a common model zoo.",
                    "label": 0
                },
                {
                    "sent": "Of various kinds, Ann.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do have a shared model zoo.",
                    "label": 0
                },
                {
                    "sent": "This is an example of our vision package where you can create like your favorite like image classification models and load some pre trained Imagenet training rates on to it.",
                    "label": 0
                },
                {
                    "sent": "An out of the box it actually just downloads the weights from like my account on Amazon S3 an.",
                    "label": 0
                },
                {
                    "sent": "And you will get those weights somewhere in your local disk an it's pretty convenient.",
                    "label": 0
                },
                {
                    "sent": "Because it's a one liner, it makes it much more convenient an.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you can do is like use like some of the models in the model zoo and like do some random research on all these models together because they all come in the same interface and they're so convenient to use.",
                    "label": 0
                },
                {
                    "sent": "Here is an example of someone who took all the models in Pytorch model Zoo and then measure like if you quantized weights.",
                    "label": 0
                },
                {
                    "sent": "What kind of accuracy loss you would get an at, like what quantization level.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So one of the lab.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best things I'm going to talk about is which I think is a pain point is.",
                    "label": 0
                },
                {
                    "sent": "Most of the frameworks don't have a linear style of programming, and what I mean by that is.",
                    "label": 1
                },
                {
                    "sent": "You first have an idea and then you have to think about how to map that idea into creating a model that that will create that idea, and then you create that model and you take it and then you will compile it or whatever optionally, and then you will use it in like pump data into it and backward.",
                    "label": 0
                },
                {
                    "sent": "Now if you have.",
                    "label": 1
                },
                {
                    "sent": "An issue with your model.",
                    "label": 0
                },
                {
                    "sent": "You want a debugger model.",
                    "label": 0
                },
                {
                    "sent": "You have to 1st figure out what's going wrong in your model and then map it back to like where you created the node in your model.",
                    "label": 0
                },
                {
                    "sent": "An debugging itself becomes very painful, so.",
                    "label": 0
                },
                {
                    "sent": "Bich",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does Pytorch has a very imperative style of programming?",
                    "label": 1
                },
                {
                    "sent": "You don't necessarily create or have to create a separate model, you can.",
                    "label": 0
                },
                {
                    "sent": "Write line, write code line by line and expect it to be differentiated properly and you expect that the debugger will stop in the exact place where you get an error an this.",
                    "label": 0
                },
                {
                    "sent": "It's not unique to Pytorch.",
                    "label": 0
                },
                {
                    "sent": "This is very important to us.",
                    "label": 0
                },
                {
                    "sent": "Who are building Pytorch because we think it's very essential when you're doing like a fast iterative research cycle.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to debug as fast as possible, and you want to map your idea.",
                    "label": 0
                },
                {
                    "sent": "Straight away into whatever your code is rather than like having some kind of slightly nonlinear or convoluted process so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I mentioned, this is not unique to PY torch.",
                    "label": 0
                },
                {
                    "sent": "There is a bunch of other frameworks that do this.",
                    "label": 0
                },
                {
                    "sent": "But the way we did that we think it's important enough that we build Pytorch from the ground up.",
                    "label": 0
                },
                {
                    "sent": "With this in mind, we want this imperative style of programming an we optimized all of the core code.",
                    "label": 1
                },
                {
                    "sent": "So that even if you do like small models like fairly tiny models like amnesty models, you won't see any framework overhead when you're doing like this imperative style of programming on average are.",
                    "label": 0
                },
                {
                    "sent": "For creating a node in the Pytorch graph is in the order of, like 20 to 30 microseconds.",
                    "label": 0
                },
                {
                    "sent": "But if you try to do something similar in the imperative modes of these other frameworks, you will see the overhead rack up to milliseconds an seconds as well.",
                    "label": 0
                },
                {
                    "sent": "And if there's a compilation process, maybe like it'll take an hour to compile, I don't know, but this we find this to be a senchal when you're doing research an.",
                    "label": 0
                },
                {
                    "sent": "That with that IN.",
                    "label": 0
                },
                {
                    "sent": "How you have inner research workflow?",
                    "label": 0
                },
                {
                    "sent": "A lot of pain points and how Pytorch tries to solve them, and then in the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, 5 minutes I'm going to try to go through an example of implementing.",
                    "label": 0
                },
                {
                    "sent": "Generative adversarial network with you it's like about 200 two 50 lines of code so I have to find my website.",
                    "label": 0
                },
                {
                    "sent": "OK, it's here.",
                    "label": 0
                },
                {
                    "sent": "So I can zoom in OK.",
                    "label": 0
                },
                {
                    "sent": "So this is just an example of an what you see is a bunch of imports.",
                    "label": 0
                },
                {
                    "sent": "14 lines of imports of all kinds.",
                    "label": 0
                },
                {
                    "sent": "And then another bunch of like command line parsing stuff.",
                    "label": 0
                },
                {
                    "sent": "But and you said some random seeds to hyper seed optimization.",
                    "label": 0
                },
                {
                    "sent": "This is where you create your datasets.",
                    "label": 0
                },
                {
                    "sent": "Depending on which command and option I have, I will create my data set object and then I will give my data set object to a data loader right there an in my data loader I'm giving what batch size I want the data loader to return and the number of workers that is the number of processes I want to.",
                    "label": 0
                },
                {
                    "sent": "Simultaneously load my data and whether I want to shuffle my data set or not.",
                    "label": 0
                },
                {
                    "sent": "There's a different there's more options there.",
                    "label": 0
                },
                {
                    "sent": "This is just some of the options we used here, and then I create a small custom weight initialization function and then here I create my generator network.",
                    "label": 0
                },
                {
                    "sent": "This is a DC Gan, so in the constructor and just creating sequential.",
                    "label": 0
                },
                {
                    "sent": "Come sequential neural network and in the forward function I.",
                    "label": 0
                },
                {
                    "sent": "If this is a CUDA that float tensor, that is, if it's on the GPU an.",
                    "label": 0
                },
                {
                    "sent": "If I asked to use more than one GPU's, I am paralyzing my model to run on like as many GPS as as does a one liner in PY torch to use multiple GPUs.",
                    "label": 0
                },
                {
                    "sent": "Also, I just, you know, just send the input through Maine and get an output under.",
                    "label": 0
                },
                {
                    "sent": "Return the output.",
                    "label": 0
                },
                {
                    "sent": "That's the generator network an I created generated network and I apply this weights in that function and the weights net function.",
                    "label": 0
                },
                {
                    "sent": "It gets each module in the network and then it will try to find if it's a common layer.",
                    "label": 0
                },
                {
                    "sent": "I've initialized my weights differently.",
                    "label": 0
                },
                {
                    "sent": "If it's a batch norm then I initialized my weights differently.",
                    "label": 0
                },
                {
                    "sent": "And then if this is an option to like load weights from this click from previous checkpoints.",
                    "label": 0
                },
                {
                    "sent": "If I have some previous checkpoint that is specified, then I load the checkpoint from disk and load those weights into my network itself.",
                    "label": 0
                },
                {
                    "sent": "If you do your first tutorial, you will cover all the subtleties and in the discriminator network I have sequential network with some convolutions and stuff and then.",
                    "label": 0
                },
                {
                    "sent": "Also data parallel and then I have like some view operation.",
                    "label": 0
                },
                {
                    "sent": "And I initialize my weights differently as well.",
                    "label": 0
                },
                {
                    "sent": "My loss function here is a binary cross entropy loss, and here I pre create buffers for the inputs and labels just to reuse them.",
                    "label": 0
                },
                {
                    "sent": "I guess I don't have to do this if I want to use CUDA.",
                    "label": 0
                },
                {
                    "sent": "I push my model onto CUDA Ann.",
                    "label": 0
                },
                {
                    "sent": "This is my training loop.",
                    "label": 0
                },
                {
                    "sent": "So here I create optimizers.",
                    "label": 0
                },
                {
                    "sent": "An Adam optimizer for my generator network Canon Adam Optimizer for my.",
                    "label": 0
                },
                {
                    "sent": "Discriminator network and these optimizers get the an iterable over the para meters in your network.",
                    "label": 0
                },
                {
                    "sent": "And I just have a double for loop now.",
                    "label": 0
                },
                {
                    "sent": "With the training loop itself, I zero the gradients of my network and I.",
                    "label": 0
                },
                {
                    "sent": "Create variables with my inputs I sent, pass them through the outputs and then I do a backward.",
                    "label": 0
                },
                {
                    "sent": "Yes, there's a question there.",
                    "label": 0
                },
                {
                    "sent": "Zero gradient thing.",
                    "label": 0
                },
                {
                    "sent": "So in Pytorch all the gradients are accumulated into their buffers.",
                    "label": 0
                },
                {
                    "sent": "They are not overwritten.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I do one forward backward and I get some gradients there and then I do another forward backward on the same variables, then you will have a sum of both of these gradients sitting in your grad buffers.",
                    "label": 0
                },
                {
                    "sent": "There's various benefits to it so.",
                    "label": 0
                },
                {
                    "sent": "Before you do your like next iteration, you would want to 0 all the gradients.",
                    "label": 0
                },
                {
                    "sent": "Anne looks like people are either following or sleeping.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Ann you basically.",
                    "label": 0
                },
                {
                    "sent": "I just passed your output of your net be through the criterion, which is a binary cross entropy loss, and then you just called backward an.",
                    "label": 0
                },
                {
                    "sent": "Then you just record that is just a debugging, just like something for logging purposes, and similarly you train it fake.",
                    "label": 0
                },
                {
                    "sent": "So you pass your noise through your net G and you get a fake image and then you pass the fake image so the output.",
                    "label": 0
                },
                {
                    "sent": "You get some output and then you pass through the loss and then you do like a backward and then like optimizer dot step will do like the gradient like SGD step in this case like the Adam step.",
                    "label": 0
                },
                {
                    "sent": "Similarly you have another section to forward through the real as a fake and after some kind of gang stuff.",
                    "label": 0
                },
                {
                    "sent": "And here you print.",
                    "label": 0
                },
                {
                    "sent": "Can you print some last stuff?",
                    "label": 0
                },
                {
                    "sent": "I have every hundred iterations you save an image to disk with like generated samples and real samples and this is just checkpointing code.",
                    "label": 0
                },
                {
                    "sent": "You just save the parameters to disk.",
                    "label": 0
                },
                {
                    "sent": "So in this 265 lines an in like close to 5 minutes you just implemented a DC Gan.",
                    "label": 0
                },
                {
                    "sent": "And hopefully this did help understand anything about Py torch.",
                    "label": 0
                },
                {
                    "sent": "If not, you can go home and do the tutorials.",
                    "label": 0
                },
                {
                    "sent": "What kind of functionality is there for the checkpointing?",
                    "label": 0
                },
                {
                    "sent": "Is it similar to tensor flow in the same type?",
                    "label": 0
                },
                {
                    "sent": "Really like bundles everything together?",
                    "label": 0
                },
                {
                    "sent": "So we actually have two levels of.",
                    "label": 0
                },
                {
                    "sent": "I lost my browser window anyway, so we have two kinds of checkpointing, usually for neural networks.",
                    "label": 0
                },
                {
                    "sent": "You can just checkpoint the weights by.",
                    "label": 0
                },
                {
                    "sent": "You see this, I'm supposed to go here.",
                    "label": 0
                },
                {
                    "sent": "You see this function calls state dict here that basically returns a Dictionary of para meters with like names of the parameters and their dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So like you can have net D dot con, one dot weight is the string and then its corresponding torch tensor.",
                    "label": 0
                },
                {
                    "sent": "So that's stating that you can save and then you can create another new Netley and then load it.",
                    "label": 0
                },
                {
                    "sent": "Load these parameters back.",
                    "label": 0
                },
                {
                    "sent": "That's one more simplistic way.",
                    "label": 0
                },
                {
                    "sent": "The other option is to just pickle the Python class as is, and both of them are supported.",
                    "label": 0
                },
                {
                    "sent": "I usually prefer the state Dick thing because it's stupid an so your checkpoints are more interpretable in a later situation.",
                    "label": 0
                },
                {
                    "sent": "Station.",
                    "label": 0
                },
                {
                    "sent": "Questions geovisualization thing you can use like whatever is your favorite Python visualization tool like you can plot some stuff with Matplotlib.",
                    "label": 0
                },
                {
                    "sent": "You can use tensor board if you want.",
                    "label": 0
                },
                {
                    "sent": "I mean you can do any use any of the Python tools pretty much.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I have about 7 minutes OK quickly.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The philosophy of paper.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Course, most people here don't care, so.",
                    "label": 0
                },
                {
                    "sent": "Stay out of the way is one of our big things like we want to minimize the abstractions we have.",
                    "label": 0
                },
                {
                    "sent": "Like we don't want you to debug through seven different classes before you get to the actual code that's running it.",
                    "label": 0
                },
                {
                    "sent": "Cater to the impatient, just like don't let users wait on anything.",
                    "label": 0
                },
                {
                    "sent": "No compilation times, just like have everything be very imperative.",
                    "label": 0
                },
                {
                    "sent": "Promote linear code flow.",
                    "label": 0
                },
                {
                    "sent": "I explained that full interop with the Python ecosystem.",
                    "label": 1
                },
                {
                    "sent": "An last one is be as fast as any other framework really like are like we don't want to sacrifice flexibility for speed and we do a lot of low level core engineering.",
                    "label": 0
                },
                {
                    "sent": "To make sure that we keep things flexible, but we also are really, really fast an where within 10% of most, like all frameworks.",
                    "label": 0
                },
                {
                    "sent": "I think unlike most usual deep learning workloads, an if we are slower on some workloads and people get these to our attention, we make sure that we find ways to like fix these performance issues.",
                    "label": 0
                },
                {
                    "sent": "Then we take that be as fast as possible.",
                    "label": 0
                },
                {
                    "sent": "Very seriously.",
                    "label": 0
                },
                {
                    "sent": "OK, last.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only upcoming features.",
                    "label": 0
                },
                {
                    "sent": "We are going to release a .2 version of fighter fighters now has like Zero point 1.1 two which is like the current version that we distribute in .2.",
                    "label": 0
                },
                {
                    "sent": "We will be having the following features.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we will have distributed Pytorch.",
                    "label": 1
                },
                {
                    "sent": "So you will have an MPI style distributed communication, which means that you will have functions in Pytorch Dot distributed that will let you send a tensor to another node.",
                    "label": 1
                },
                {
                    "sent": "And receive a tensor from other node or like reduced answers among all nodes an.",
                    "label": 0
                },
                {
                    "sent": "And this is usually pretty popular in the computer science community, and there's a big library that does this, call MPI that's very popular and very old.",
                    "label": 0
                },
                {
                    "sent": "And it has a certain style of an that's basically like doing this, like exchanging data between nodes.",
                    "label": 0
                },
                {
                    "sent": "So we follow that style.",
                    "label": 0
                },
                {
                    "sent": "But we can we provide primitives that let you exchange tensors among all the disputed nodes an.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are also introducing allowing you to do higher order derivatives so you can actually do something like this an it will obviously be incorrect because of floating point issues, but mathematically it will try to do the right thing.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne, it's usually like there is some papers in recent times that have been using like higher order directives.",
                    "label": 0
                },
                {
                    "sent": "So yeah, you can implement crazy ideas than you are.",
                    "label": 0
                },
                {
                    "sent": "I guess going to implement crazy ideas or at least some of you.",
                    "label": 1
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will also be introducing NUM PY style, broadcasting an advanced indexing.",
                    "label": 1
                },
                {
                    "sent": "This is mostly like sugar to like.",
                    "label": 0
                },
                {
                    "sent": "Avoid for users to avoid writing like a lot of additional code, and this is something that users are usually very very familiar from.",
                    "label": 0
                },
                {
                    "sent": "Vendor using NUM Py, so we just are introducing that as well.",
                    "label": 0
                },
                {
                    "sent": "An these three features I order derivative, distributed Pytorch, an advanced indexing, broadcasting are coming out in .2.",
                    "label": 0
                },
                {
                    "sent": "But we're also working more in like the slightly longer time frame.",
                    "label": 0
                },
                {
                    "sent": "Maybe .3.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is adding a compiler to Pytorch that is just in time compiled as a user?",
                    "label": 1
                },
                {
                    "sent": "You won't notice anything you don't have to notice anything but.",
                    "label": 1
                },
                {
                    "sent": "At the back end, what we're doing is we're looking at a bunch of your computation that's happening and seeing if we can.",
                    "label": 0
                },
                {
                    "sent": "Code generate much more efficient code that does the same computation that were repeatedly seeing an so equal, and there's so few columns for this in the community.",
                    "label": 0
                },
                {
                    "sent": "Tensor flow has this project called XLA that tries to do this an the MX net guys have this project called NVM and their upcoming TVM so.",
                    "label": 0
                },
                {
                    "sent": "We are doing something similar.",
                    "label": 0
                },
                {
                    "sent": "We actually also leverage some of the components that the Amazon MX net team has built.",
                    "label": 0
                },
                {
                    "sent": "And All in all, let's just say that you will see faster things, Ann you.",
                    "label": 0
                },
                {
                    "sent": "Then you will know why.",
                    "label": 0
                },
                {
                    "sent": "And generally, the way this is done, this is done in several ways.",
                    "label": 0
                },
                {
                    "sent": "We haven't really converged on which way we're going to go, but one way.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is you can do something lazy evaluation where when you execute all this code, it actually doesn't execute the code like it executes the code, but it doesn't actually do the mathematical operations themselves, just queues up the operations and keeps them somewhere an when you ask for.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual data.",
                    "label": 0
                },
                {
                    "sent": "Let's say we print it or you like.",
                    "label": 0
                },
                {
                    "sent": "Use it in some other OPEC function or whatever.",
                    "label": 0
                },
                {
                    "sent": "It will then execute all these queued up operations and we.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We'll then have a little bit of time between building the graph and executing the graph, and we can use that little bit of time to compile this code.",
                    "label": 1
                },
                {
                    "sent": "We can also do stuff like graph caching, which means you can if we we will compile this workload for if we see it again in the future, then it will go faster, But the first time it won't go faster, so we're still deciding a bunch of things.",
                    "label": 0
                },
                {
                    "sent": "Underlying this compiler feature, but it is happening.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the advantages have once.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, that's kind of cute.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The advantages of compilation is you can get things to go faster using these buzzwords.",
                    "label": 0
                },
                {
                    "sent": "Lastly, I might have.",
                    "label": 0
                },
                {
                    "sent": "Time Oh my God, I have 30 more minutes.",
                    "label": 0
                },
                {
                    "sent": "I thought this was a one hour presentation, my bad.",
                    "label": 0
                },
                {
                    "sent": "Anyways, my last slide.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So we really Don Janet in.",
                    "label": 0
                },
                {
                    "sent": "We have a wide range of people working on an using Pytorch.",
                    "label": 0
                },
                {
                    "sent": "I'm hoping to get a bunch of people here who are not already Pytorch users to also use it an we want to hopefully add more names there.",
                    "label": 0
                },
                {
                    "sent": "Ann, you can ask me any questions you want.",
                    "label": 0
                }
            ]
        }
    }
}