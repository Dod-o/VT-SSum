{
    "id": "2x6fojr6l3ur6th4eaklt54rorl2mjst",
    "title": "Defence of the Doctoral Dissertation: Machine Learning of Semantics for Text Understanding",
    "info": {
        "author": [
            "Janez Starc, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "June 12, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/single_starc_text_understanding/",
    "segmentation": [
        [
            "I'll start this presentation with an overview in the in the introduction I will explain general approach to text understanding and how domains, scientific contributions fit into that approach.",
            "Then I'll dig in into each of the scientific contributions, starting with the context.",
            "Free grammar reduction approach, and then continuing with the second contribution, which is semantic trees coupled with two ontology learning approaches.",
            "So both the first 2 contributions have been.",
            "Presented in a paper that was published in the Intelligent Data Analysis Journal and then I'll proceed to contributions regarding natural language inference or NLI so.",
            "The third contribution is tax generative neural networks for generating analyze hypothesis and the last.",
            "Contribution is the procedure for the evaluation of this hypothesis, and I'll finish the presentation with conclusions."
        ],
        [
            "So this is a general approach to text understanding, so usually have textual data.",
            "Written in a particular human language.",
            "And we would like to train machines to understand the meaning of this text and use it in the target applications like some.",
            "To answer some questions.",
            "Or maybe some infer some new facts that were not obvious before, and so for that we use meaning representations Texas map to meaning representations which are easier to use in target applications than just raw text.",
            "And there are various approaches to map in text to meaning representations, and we will focus on machine learning methods."
        ],
        [
            "This is a slide of how our work fits into this schema, so we start with text text.",
            "The basic unit is a sentence, and this sentence may include some additional.",
            "Patience, so we preprocess this text into a more structured textual representations and then as a part of our preliminary work we did some work on pattern rules and rules on syntax trees to transform the text into meaning representation, predicate logic and as an extension of that we have developed the grammar induction approach so the grammar parses text into semantic trees which are also meaning.",
            "Intentions, but there are also two other ways too.",
            "Transform text into predicate logic, one is by taxonomy extraction from grammar rules and the other is by performing relation extraction instance discovery on semantic trees and the last path of our work is.",
            "Uses neural networks and distributed representations for the problem of natural language inference and text generation."
        ],
        [
            "So here is a quick introduction to predicate logic.",
            "So here we have an example of a sentence.",
            "And below that is a is a relation that can be extracted from this sentence.",
            "So this relation has a predicate position of person organization, an three arguments of person, the company and the position of the person in the company so.",
            "And as a part of our preliminary work, we did some.",
            "We propose some methodology that uses pattern rules.",
            "So patterns consist of the textual pattern which is applied to text, for instance here this pattern matches this first part of the sentence, and then we also have the logical pattern.",
            "And when the rule matches the text, then with this logical pattern we can construct a relation like this.",
            "And for one of the papers, we have proposed a method for indentifying promising textual path patterns and in another paper we have proposed a method for combining parse trees and Cyc knowledge base to extract new predicate logic facts."
        ],
        [
            "And based on the advantages and disadvantages of these two approaches, we have proposed the grammar induction approach.",
            "So this is a semantic tree.",
            "An semantic tree consists of nodes, and each node has its own semantic class like person, relation or life role, and also each node represents a part of the sentences.",
            "Furthermore, the leaf nodes also represent some instances in the target ontology, and so on.",
            "Here in the left bottom of the slide we can see a context free grammar and these were the rules that were used.",
            "To parse this sentence here into this semantic tree by performing top bound top down parsing."
        ],
        [
            "But what happens if we are missing this rule at the bottom?",
            "So then this note cannot be parsed, and what we can do we can do.",
            "We can send this rule to a rule induction phase two."
        ],
        [
            "Extract a new rule.",
            "So actually this.",
            "This impossible note here is presented as a rule, and then we want to generalize this rule by applying other rules that we already have, and once we cannot apply any rules more than we have our new rule, which can then be added to the grammar.",
            "And now let's."
        ],
        [
            "See how this whole grammar reduction approach looks like.",
            "We start off by defining small set of Cedros and these rules then parse the text into semantic trees.",
            "Then we collect all the impossible nodes and in the induction phase many new rules are induced and we select the best of them according to some proposed measure and then this rule goes to the human user which assigns it a certain property.",
            "So if the if the property is positive, that means that the rule will be used in all phases.",
            "If the rule is.",
            "Non invisible, then the rule will be just used in the top down face and if the rule is neutral then the rule will not be used for parsing and now."
        ],
        [
            "I will show how we how we extract taxonomy from the grammar rules, so the grammar rules consists of non terminals and terminals so non terminals will represent classes in the new relations and the terminals will represent instances.",
            "So for instance if we have a rule that has a single non terminal on the right hand side then we can form subclass relations.",
            "So here we have an example.",
            "Where a class of city is a subclass of class location.",
            "An if you have a single terminal on the right hand side, then we confirm is a relations where instance Paris is a city."
        ],
        [
            "And now let's go to the grammar induction results.",
            "So our data set consisted of 1st sentences of Wikipedia personal tickles.",
            "So there are about 1 million of those sentences, and here is an example of one such sentence.",
            "So as you can see, it contains many links and this links link to other Wikipedia pages as well as DB pedia ontology instances.",
            "And we can leverage that information.",
            "So in the in the this conducted experiments, six 688 rules were induced and this rules parsed about 1/4 of all sentences fully.",
            "This means that fully parsed sentences, the semantic trees of fully parse sentences don't have any possible nodes.",
            "On the other hand, the average coverage of words were 78%.",
            "That means that 78% of all words.",
            "Were fully passed so and then from the rules.",
            "The attacks animal was extracted so there were 95 easier relations and 30 three subclass relations."
        ],
        [
            "Now let's go to the next contribution and here is relation extraction from semantic trees.",
            "So we have semantic trees on one side and we have a bunch of training relations on the other side.",
            "So imagine that we want to learn the professional relations, and here we have a one such training correlation.",
            "So this relation has two arguments and we want to find the semantic tree that contains both of these.",
            "Arguments and then we want to find the this path between the.",
            "These nodes and then we extract the path.",
            "So except the path so that we.",
            "In our notes, we just remembered the rules an for the lift notes, like this one and this one.",
            "We just remember the the semantic class of the nodes.",
            "So then when we are searching for new relations, the semantic tree is checked if it contains any of these trained three paths.",
            "And if they do, we just.",
            "Extract the the leaf nodes of that matched path and we can form new relations like this one."
        ],
        [
            "So basically I just described how this basic model works like and but we also have a math model.",
            "Net model works very similarly, just that all parts are combined combined into an act like this.",
            "And then when we are searching for new.",
            "For new installations, we just checked if some part of the semantic tree is found in this net model and.",
            "But here this this.",
            "Net model represents the DPD relation institution.",
            "So in the last model is the logistic regression model where we.",
            "We just take all the nodes and forget about the edges and create a bag of nodes approach.",
            "So a bag of nodes approach is very similar to more well known bag of words approach.",
            "Just instead of words, we have nodes and so the first 2 models just had a positive examples.",
            "But here with logistic regression model we also have the negative example of three parts so that we can then.",
            "Perform logistic regression on this bag of nodes representation."
        ],
        [
            "And then let's go to the results of the first of the relation extraction.",
            "So we've performed relation extraction on 79 DB pedia relations.",
            "In this experiment we just took semantic trees that were fully parse, and as we can see here, as the complexity of the model arises also the recall an F1 score rises.",
            "And our best model, which is logistic regression and context nodes plus lexical items, means that context nodes are the notes that are connected to the three path and the lexical items are the actual words in.",
            "In the notes.",
            "And so we can see that this this model performs compatible to model named conditional random fields, which is known to be good for sequence learning.",
            "And so this part is the relation extraction part.",
            "And now we go to the instance Discovery part.",
            "So when we when we perform the parsing, quits.",
            "Of the grammar, we still have some impossible nodes, and then there are two ways why they can be unpassable.",
            "Either we are missing a rule.",
            "And or.",
            "The rule that would split the note or this unpassable note is actually represents an instance, and we have performed the various simple model to detect such nodes that represent instances.",
            "And the results that were performed of on 6 classes.",
            "In this experiment, 24,000 new instances were extracted with the estimated accuracy of 5065.",
            "56%."
        ],
        [
            "Now let's go to the second part of the presentation, which is about natural language inference, which is also sometimes known as recognizing textural attainment.",
            "So in this case, we have two sentences, premise and hypothesis, and if the premise entails the hypothesis, then we give the example the entailment label.",
            "If the two sentences are in the contradiction, then give example contradiction, labor otherwise.",
            "The.",
            "Sentences are in the neutral relationship.",
            "So usually natural language inference is presented as a classification problem where we have the premise and the hypothesis and we want to predict the correct label.",
            "But we have somehow flipped this problem into a generation generation problem, so you are given a premise and the label and you want to generate such hypothesis that the example is.",
            "Still valid, so for the generation problem you can see that the system not only requires a good understanding of text, but also the ability to generate text, and so for the."
        ],
        [
            "We propose several neural networks as the is our third contribution.",
            "So usually when you want to.",
            "Generates text with neural networks.",
            "Use some kind of a coder, encoder decoder architecture, and here we adapted this encoder decoder architecture to our problem of analyze generation.",
            "So the encoder.",
            "So it's all the all the information that we have in this low dimensional vector Z and then the decoder tries to reconstruct the hypothesis given premise and label and Z.",
            "So that means that Z actually represents the mapping between premise and label on one side, and hypothesis on the other side.",
            "Here on the right hand side is our most novel model.",
            "Which doesn't contain the encoder.",
            "But instead of the encoder we are learning and embedding metrics of learnable parameters.",
            "So that means that each row in this matrix corresponds to one example in the into the training set.",
            "And we again want to optimize this mapping between premise and label on one side and hypothesis on the other side by updating that row vector.",
            "So and when we want to.",
            "Generate.",
            "Hypothesis we in this case remove the encoder, and here this embedding matrix generate a random.",
            "Vector Z and together with premise and label, the decoder can generate a new hypothesis.",
            "So.",
            "And here the main advantage of this model is that it has less parameters that it has has to be updated on each iteration."
        ],
        [
            "OK, so we are now at the last contribution, which is the procedure for evaluating such hypothesis and consequently this procedure also evaluates the generative models.",
            "So the generative models is trained on the training set of the original data set.",
            "And once it is strength, it generates new hypothesis and this hypothesis then replace.",
            "The hypothesis in the original data set.",
            "Two to construct a new data set.",
            "And then we also construct two classifiers here and which are one is trained on the original data set and one is trained on the generated data set and then this.",
            "This classifiers are both evaluated on the testing data of the original data set and then we can compare the accuracy to see how good this our new classifier is.",
            "And So what is the motivation behind?",
            "Behind this procedure.",
            "So usually the metrics that are commonly used in machine translation.",
            "Analyze the generated hypothesis that are that are different from the original hypothesis.",
            "And.",
            "But here in this in our problem, the generated hypothesis can be quite different from the original one, but it is still correct.",
            "So because of that we propose this hypothesis that a good data set for training and analyze classifier consists of variety of accurate, nontrivial and comprehensible examples, and I'll try to show that a little bit of evidence on the next slide."
        ],
        [
            "Slide which is which are the results.",
            "So we have conducted our experiments on the SNL I data set and here is one of the graph that shows the results.",
            "So here each point represents a data set generated data set and for each of these datasets we have trained the classifier on particular data set.",
            "And then we measure the accuracy of this data set of these classifiers here and on the on this axis we have different values for Latin Dimensions Z.",
            "But we also have different thresholds, and each line here.",
            "Represents one threshold.",
            "So this is the cause when we generated new examples.",
            "These examples were not so were not so accurate, so we filtered out the inaccurate examples.",
            "So if an example has lower probability than the thresholds according to the original classifier, then the example is filtered out and then we can see here that the best threshold.",
            "Is 0.6 that means somewhere in between?",
            "So and so that means that we still need accurate examples.",
            "But if we set the threshold too high, for instance 0.9, then we just get a lot of trivial examples and accuracy.",
            "Performance gets lower and this is also somewhat seen in this table of the examples.",
            "So if.",
            "Oh sorry, just one more thing and.",
            "Also, on this graph, it can be seen that the best Latin dimensionality for vector Z is somewhere around 8 and 16.",
            "And on this, as we can see on this table, somehow, if you have a low dimension for vector Z, then the examples are very trivial.",
            "But if the this Latin dimensionality is set too high, then the examples are more creative.",
            "But on the other hand there are less accurate.",
            "There are more grammatical and there are more grammatical errors.",
            "And."
        ],
        [
            "Here are the main results of this experiments.",
            "So this table shows the accuracy of classifiers.",
            "Trains trained on the data set generated by different models, and you can see that our best model, the classifier of our best generative model, has accuracy that is fairly close to the classifier that was trained on the original data.",
            "But if we combine.",
            "The original data and the data of our best model.",
            "Then the.",
            "This accuracy is the highest.",
            "An we also made another experiment, discriminative analysis, and in this experiment we wanted to distinguish between the generated hypothesis and the original 1.",
            "So if some tester like here, we have two testers, wrongly distinguishes the generated hypothesis from the original 1, then we have an error and we.",
            "Measure the error rate and if the error rate is high, that means that the generated examples are good because they cannot be distinguished from the from the original ones.",
            "And here we can see that actually our trained discriminative model actually performed better than the human, But this is just cause the discriminative model has seen much more training examples."
        ],
        [
            "And so here are the conclusions I'll go again through the main scientific contributions.",
            "So the first contribution is context free grammar reduction approach, so the so the grammar is used to build semantic trees and the grammar is also the grammar rules are also used to extract taxonomy taxonomic relations in one of the experiments we extracted more than 100 taxonomic relations.",
            "The next contributions are semantic trees coupled with the two ontology learning approaches.",
            "The first one is the relation extraction and the second one is instance discovery, and if we take take a look at this, both the first 2 contributions together we can see that we can build several different aspects of the ontology in.",
            "More in a controllable and transparent way, and so now the third contribution.",
            "We are the text generative neural networks for natural language inference and our best model is novel because it doesn't contain the decoder and instead of the decoder the model is learning mapping, mapping embeddings for each training example separately.",
            "So in the last procedure, the last contribution is the procedure for evaluating.",
            "Such.",
            "Text hypothesis, where we construct new analyzed data set and then train the classifier on this data set and then we can compare the accuracy to the classifier trained on the original data set and we can see that the accuracy our best model is somewhat close to the accuracy of the regional model.",
            "Plus if we combine the data from.",
            "Our best model and the original data set.",
            "Then we get the highest accuracy."
        ],
        [
            "So here are also some publication I've been coauthor on.",
            "Three Journal papers, two of them are directly connected to dissertations to this dissertation, and the rest one are conference papers that are also related to this dissertation, and I will stop here.",
            "Thank you.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll start this presentation with an overview in the in the introduction I will explain general approach to text understanding and how domains, scientific contributions fit into that approach.",
                    "label": 0
                },
                {
                    "sent": "Then I'll dig in into each of the scientific contributions, starting with the context.",
                    "label": 0
                },
                {
                    "sent": "Free grammar reduction approach, and then continuing with the second contribution, which is semantic trees coupled with two ontology learning approaches.",
                    "label": 1
                },
                {
                    "sent": "So both the first 2 contributions have been.",
                    "label": 1
                },
                {
                    "sent": "Presented in a paper that was published in the Intelligent Data Analysis Journal and then I'll proceed to contributions regarding natural language inference or NLI so.",
                    "label": 0
                },
                {
                    "sent": "The third contribution is tax generative neural networks for generating analyze hypothesis and the last.",
                    "label": 1
                },
                {
                    "sent": "Contribution is the procedure for the evaluation of this hypothesis, and I'll finish the presentation with conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a general approach to text understanding, so usually have textual data.",
                    "label": 1
                },
                {
                    "sent": "Written in a particular human language.",
                    "label": 1
                },
                {
                    "sent": "And we would like to train machines to understand the meaning of this text and use it in the target applications like some.",
                    "label": 0
                },
                {
                    "sent": "To answer some questions.",
                    "label": 0
                },
                {
                    "sent": "Or maybe some infer some new facts that were not obvious before, and so for that we use meaning representations Texas map to meaning representations which are easier to use in target applications than just raw text.",
                    "label": 0
                },
                {
                    "sent": "And there are various approaches to map in text to meaning representations, and we will focus on machine learning methods.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a slide of how our work fits into this schema, so we start with text text.",
                    "label": 0
                },
                {
                    "sent": "The basic unit is a sentence, and this sentence may include some additional.",
                    "label": 0
                },
                {
                    "sent": "Patience, so we preprocess this text into a more structured textual representations and then as a part of our preliminary work we did some work on pattern rules and rules on syntax trees to transform the text into meaning representation, predicate logic and as an extension of that we have developed the grammar induction approach so the grammar parses text into semantic trees which are also meaning.",
                    "label": 0
                },
                {
                    "sent": "Intentions, but there are also two other ways too.",
                    "label": 0
                },
                {
                    "sent": "Transform text into predicate logic, one is by taxonomy extraction from grammar rules and the other is by performing relation extraction instance discovery on semantic trees and the last path of our work is.",
                    "label": 1
                },
                {
                    "sent": "Uses neural networks and distributed representations for the problem of natural language inference and text generation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a quick introduction to predicate logic.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example of a sentence.",
                    "label": 0
                },
                {
                    "sent": "And below that is a is a relation that can be extracted from this sentence.",
                    "label": 0
                },
                {
                    "sent": "So this relation has a predicate position of person organization, an three arguments of person, the company and the position of the person in the company so.",
                    "label": 0
                },
                {
                    "sent": "And as a part of our preliminary work, we did some.",
                    "label": 0
                },
                {
                    "sent": "We propose some methodology that uses pattern rules.",
                    "label": 0
                },
                {
                    "sent": "So patterns consist of the textual pattern which is applied to text, for instance here this pattern matches this first part of the sentence, and then we also have the logical pattern.",
                    "label": 0
                },
                {
                    "sent": "And when the rule matches the text, then with this logical pattern we can construct a relation like this.",
                    "label": 0
                },
                {
                    "sent": "And for one of the papers, we have proposed a method for indentifying promising textual path patterns and in another paper we have proposed a method for combining parse trees and Cyc knowledge base to extract new predicate logic facts.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And based on the advantages and disadvantages of these two approaches, we have proposed the grammar induction approach.",
                    "label": 0
                },
                {
                    "sent": "So this is a semantic tree.",
                    "label": 1
                },
                {
                    "sent": "An semantic tree consists of nodes, and each node has its own semantic class like person, relation or life role, and also each node represents a part of the sentences.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the leaf nodes also represent some instances in the target ontology, and so on.",
                    "label": 1
                },
                {
                    "sent": "Here in the left bottom of the slide we can see a context free grammar and these were the rules that were used.",
                    "label": 0
                },
                {
                    "sent": "To parse this sentence here into this semantic tree by performing top bound top down parsing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what happens if we are missing this rule at the bottom?",
                    "label": 0
                },
                {
                    "sent": "So then this note cannot be parsed, and what we can do we can do.",
                    "label": 1
                },
                {
                    "sent": "We can send this rule to a rule induction phase two.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extract a new rule.",
                    "label": 0
                },
                {
                    "sent": "So actually this.",
                    "label": 0
                },
                {
                    "sent": "This impossible note here is presented as a rule, and then we want to generalize this rule by applying other rules that we already have, and once we cannot apply any rules more than we have our new rule, which can then be added to the grammar.",
                    "label": 0
                },
                {
                    "sent": "And now let's.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See how this whole grammar reduction approach looks like.",
                    "label": 0
                },
                {
                    "sent": "We start off by defining small set of Cedros and these rules then parse the text into semantic trees.",
                    "label": 1
                },
                {
                    "sent": "Then we collect all the impossible nodes and in the induction phase many new rules are induced and we select the best of them according to some proposed measure and then this rule goes to the human user which assigns it a certain property.",
                    "label": 0
                },
                {
                    "sent": "So if the if the property is positive, that means that the rule will be used in all phases.",
                    "label": 0
                },
                {
                    "sent": "If the rule is.",
                    "label": 0
                },
                {
                    "sent": "Non invisible, then the rule will be just used in the top down face and if the rule is neutral then the rule will not be used for parsing and now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will show how we how we extract taxonomy from the grammar rules, so the grammar rules consists of non terminals and terminals so non terminals will represent classes in the new relations and the terminals will represent instances.",
                    "label": 1
                },
                {
                    "sent": "So for instance if we have a rule that has a single non terminal on the right hand side then we can form subclass relations.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example.",
                    "label": 0
                },
                {
                    "sent": "Where a class of city is a subclass of class location.",
                    "label": 0
                },
                {
                    "sent": "An if you have a single terminal on the right hand side, then we confirm is a relations where instance Paris is a city.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now let's go to the grammar induction results.",
                    "label": 1
                },
                {
                    "sent": "So our data set consisted of 1st sentences of Wikipedia personal tickles.",
                    "label": 0
                },
                {
                    "sent": "So there are about 1 million of those sentences, and here is an example of one such sentence.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, it contains many links and this links link to other Wikipedia pages as well as DB pedia ontology instances.",
                    "label": 0
                },
                {
                    "sent": "And we can leverage that information.",
                    "label": 1
                },
                {
                    "sent": "So in the in the this conducted experiments, six 688 rules were induced and this rules parsed about 1/4 of all sentences fully.",
                    "label": 0
                },
                {
                    "sent": "This means that fully parsed sentences, the semantic trees of fully parse sentences don't have any possible nodes.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, the average coverage of words were 78%.",
                    "label": 0
                },
                {
                    "sent": "That means that 78% of all words.",
                    "label": 0
                },
                {
                    "sent": "Were fully passed so and then from the rules.",
                    "label": 0
                },
                {
                    "sent": "The attacks animal was extracted so there were 95 easier relations and 30 three subclass relations.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's go to the next contribution and here is relation extraction from semantic trees.",
                    "label": 1
                },
                {
                    "sent": "So we have semantic trees on one side and we have a bunch of training relations on the other side.",
                    "label": 0
                },
                {
                    "sent": "So imagine that we want to learn the professional relations, and here we have a one such training correlation.",
                    "label": 0
                },
                {
                    "sent": "So this relation has two arguments and we want to find the semantic tree that contains both of these.",
                    "label": 0
                },
                {
                    "sent": "Arguments and then we want to find the this path between the.",
                    "label": 0
                },
                {
                    "sent": "These nodes and then we extract the path.",
                    "label": 0
                },
                {
                    "sent": "So except the path so that we.",
                    "label": 0
                },
                {
                    "sent": "In our notes, we just remembered the rules an for the lift notes, like this one and this one.",
                    "label": 0
                },
                {
                    "sent": "We just remember the the semantic class of the nodes.",
                    "label": 1
                },
                {
                    "sent": "So then when we are searching for new relations, the semantic tree is checked if it contains any of these trained three paths.",
                    "label": 1
                },
                {
                    "sent": "And if they do, we just.",
                    "label": 0
                },
                {
                    "sent": "Extract the the leaf nodes of that matched path and we can form new relations like this one.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically I just described how this basic model works like and but we also have a math model.",
                    "label": 0
                },
                {
                    "sent": "Net model works very similarly, just that all parts are combined combined into an act like this.",
                    "label": 0
                },
                {
                    "sent": "And then when we are searching for new.",
                    "label": 0
                },
                {
                    "sent": "For new installations, we just checked if some part of the semantic tree is found in this net model and.",
                    "label": 0
                },
                {
                    "sent": "But here this this.",
                    "label": 0
                },
                {
                    "sent": "Net model represents the DPD relation institution.",
                    "label": 1
                },
                {
                    "sent": "So in the last model is the logistic regression model where we.",
                    "label": 0
                },
                {
                    "sent": "We just take all the nodes and forget about the edges and create a bag of nodes approach.",
                    "label": 0
                },
                {
                    "sent": "So a bag of nodes approach is very similar to more well known bag of words approach.",
                    "label": 0
                },
                {
                    "sent": "Just instead of words, we have nodes and so the first 2 models just had a positive examples.",
                    "label": 0
                },
                {
                    "sent": "But here with logistic regression model we also have the negative example of three parts so that we can then.",
                    "label": 0
                },
                {
                    "sent": "Perform logistic regression on this bag of nodes representation.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then let's go to the results of the first of the relation extraction.",
                    "label": 0
                },
                {
                    "sent": "So we've performed relation extraction on 79 DB pedia relations.",
                    "label": 0
                },
                {
                    "sent": "In this experiment we just took semantic trees that were fully parse, and as we can see here, as the complexity of the model arises also the recall an F1 score rises.",
                    "label": 0
                },
                {
                    "sent": "And our best model, which is logistic regression and context nodes plus lexical items, means that context nodes are the notes that are connected to the three path and the lexical items are the actual words in.",
                    "label": 1
                },
                {
                    "sent": "In the notes.",
                    "label": 0
                },
                {
                    "sent": "And so we can see that this this model performs compatible to model named conditional random fields, which is known to be good for sequence learning.",
                    "label": 1
                },
                {
                    "sent": "And so this part is the relation extraction part.",
                    "label": 0
                },
                {
                    "sent": "And now we go to the instance Discovery part.",
                    "label": 0
                },
                {
                    "sent": "So when we when we perform the parsing, quits.",
                    "label": 0
                },
                {
                    "sent": "Of the grammar, we still have some impossible nodes, and then there are two ways why they can be unpassable.",
                    "label": 0
                },
                {
                    "sent": "Either we are missing a rule.",
                    "label": 0
                },
                {
                    "sent": "And or.",
                    "label": 0
                },
                {
                    "sent": "The rule that would split the note or this unpassable note is actually represents an instance, and we have performed the various simple model to detect such nodes that represent instances.",
                    "label": 0
                },
                {
                    "sent": "And the results that were performed of on 6 classes.",
                    "label": 1
                },
                {
                    "sent": "In this experiment, 24,000 new instances were extracted with the estimated accuracy of 5065.",
                    "label": 0
                },
                {
                    "sent": "56%.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's go to the second part of the presentation, which is about natural language inference, which is also sometimes known as recognizing textural attainment.",
                    "label": 0
                },
                {
                    "sent": "So in this case, we have two sentences, premise and hypothesis, and if the premise entails the hypothesis, then we give the example the entailment label.",
                    "label": 0
                },
                {
                    "sent": "If the two sentences are in the contradiction, then give example contradiction, labor otherwise.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Sentences are in the neutral relationship.",
                    "label": 0
                },
                {
                    "sent": "So usually natural language inference is presented as a classification problem where we have the premise and the hypothesis and we want to predict the correct label.",
                    "label": 1
                },
                {
                    "sent": "But we have somehow flipped this problem into a generation generation problem, so you are given a premise and the label and you want to generate such hypothesis that the example is.",
                    "label": 0
                },
                {
                    "sent": "Still valid, so for the generation problem you can see that the system not only requires a good understanding of text, but also the ability to generate text, and so for the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We propose several neural networks as the is our third contribution.",
                    "label": 0
                },
                {
                    "sent": "So usually when you want to.",
                    "label": 0
                },
                {
                    "sent": "Generates text with neural networks.",
                    "label": 1
                },
                {
                    "sent": "Use some kind of a coder, encoder decoder architecture, and here we adapted this encoder decoder architecture to our problem of analyze generation.",
                    "label": 0
                },
                {
                    "sent": "So the encoder.",
                    "label": 0
                },
                {
                    "sent": "So it's all the all the information that we have in this low dimensional vector Z and then the decoder tries to reconstruct the hypothesis given premise and label and Z.",
                    "label": 0
                },
                {
                    "sent": "So that means that Z actually represents the mapping between premise and label on one side, and hypothesis on the other side.",
                    "label": 0
                },
                {
                    "sent": "Here on the right hand side is our most novel model.",
                    "label": 0
                },
                {
                    "sent": "Which doesn't contain the encoder.",
                    "label": 0
                },
                {
                    "sent": "But instead of the encoder we are learning and embedding metrics of learnable parameters.",
                    "label": 1
                },
                {
                    "sent": "So that means that each row in this matrix corresponds to one example in the into the training set.",
                    "label": 0
                },
                {
                    "sent": "And we again want to optimize this mapping between premise and label on one side and hypothesis on the other side by updating that row vector.",
                    "label": 0
                },
                {
                    "sent": "So and when we want to.",
                    "label": 0
                },
                {
                    "sent": "Generate.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis we in this case remove the encoder, and here this embedding matrix generate a random.",
                    "label": 0
                },
                {
                    "sent": "Vector Z and together with premise and label, the decoder can generate a new hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And here the main advantage of this model is that it has less parameters that it has has to be updated on each iteration.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we are now at the last contribution, which is the procedure for evaluating such hypothesis and consequently this procedure also evaluates the generative models.",
                    "label": 0
                },
                {
                    "sent": "So the generative models is trained on the training set of the original data set.",
                    "label": 0
                },
                {
                    "sent": "And once it is strength, it generates new hypothesis and this hypothesis then replace.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis in the original data set.",
                    "label": 0
                },
                {
                    "sent": "Two to construct a new data set.",
                    "label": 0
                },
                {
                    "sent": "And then we also construct two classifiers here and which are one is trained on the original data set and one is trained on the generated data set and then this.",
                    "label": 0
                },
                {
                    "sent": "This classifiers are both evaluated on the testing data of the original data set and then we can compare the accuracy to see how good this our new classifier is.",
                    "label": 0
                },
                {
                    "sent": "And So what is the motivation behind?",
                    "label": 0
                },
                {
                    "sent": "Behind this procedure.",
                    "label": 0
                },
                {
                    "sent": "So usually the metrics that are commonly used in machine translation.",
                    "label": 0
                },
                {
                    "sent": "Analyze the generated hypothesis that are that are different from the original hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But here in this in our problem, the generated hypothesis can be quite different from the original one, but it is still correct.",
                    "label": 0
                },
                {
                    "sent": "So because of that we propose this hypothesis that a good data set for training and analyze classifier consists of variety of accurate, nontrivial and comprehensible examples, and I'll try to show that a little bit of evidence on the next slide.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide which is which are the results.",
                    "label": 0
                },
                {
                    "sent": "So we have conducted our experiments on the SNL I data set and here is one of the graph that shows the results.",
                    "label": 0
                },
                {
                    "sent": "So here each point represents a data set generated data set and for each of these datasets we have trained the classifier on particular data set.",
                    "label": 0
                },
                {
                    "sent": "And then we measure the accuracy of this data set of these classifiers here and on the on this axis we have different values for Latin Dimensions Z.",
                    "label": 0
                },
                {
                    "sent": "But we also have different thresholds, and each line here.",
                    "label": 0
                },
                {
                    "sent": "Represents one threshold.",
                    "label": 0
                },
                {
                    "sent": "So this is the cause when we generated new examples.",
                    "label": 0
                },
                {
                    "sent": "These examples were not so were not so accurate, so we filtered out the inaccurate examples.",
                    "label": 0
                },
                {
                    "sent": "So if an example has lower probability than the thresholds according to the original classifier, then the example is filtered out and then we can see here that the best threshold.",
                    "label": 0
                },
                {
                    "sent": "Is 0.6 that means somewhere in between?",
                    "label": 0
                },
                {
                    "sent": "So and so that means that we still need accurate examples.",
                    "label": 0
                },
                {
                    "sent": "But if we set the threshold too high, for instance 0.9, then we just get a lot of trivial examples and accuracy.",
                    "label": 0
                },
                {
                    "sent": "Performance gets lower and this is also somewhat seen in this table of the examples.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, just one more thing and.",
                    "label": 0
                },
                {
                    "sent": "Also, on this graph, it can be seen that the best Latin dimensionality for vector Z is somewhere around 8 and 16.",
                    "label": 0
                },
                {
                    "sent": "And on this, as we can see on this table, somehow, if you have a low dimension for vector Z, then the examples are very trivial.",
                    "label": 0
                },
                {
                    "sent": "But if the this Latin dimensionality is set too high, then the examples are more creative.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand there are less accurate.",
                    "label": 0
                },
                {
                    "sent": "There are more grammatical and there are more grammatical errors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the main results of this experiments.",
                    "label": 0
                },
                {
                    "sent": "So this table shows the accuracy of classifiers.",
                    "label": 1
                },
                {
                    "sent": "Trains trained on the data set generated by different models, and you can see that our best model, the classifier of our best generative model, has accuracy that is fairly close to the classifier that was trained on the original data.",
                    "label": 1
                },
                {
                    "sent": "But if we combine.",
                    "label": 0
                },
                {
                    "sent": "The original data and the data of our best model.",
                    "label": 0
                },
                {
                    "sent": "Then the.",
                    "label": 0
                },
                {
                    "sent": "This accuracy is the highest.",
                    "label": 1
                },
                {
                    "sent": "An we also made another experiment, discriminative analysis, and in this experiment we wanted to distinguish between the generated hypothesis and the original 1.",
                    "label": 0
                },
                {
                    "sent": "So if some tester like here, we have two testers, wrongly distinguishes the generated hypothesis from the original 1, then we have an error and we.",
                    "label": 0
                },
                {
                    "sent": "Measure the error rate and if the error rate is high, that means that the generated examples are good because they cannot be distinguished from the from the original ones.",
                    "label": 0
                },
                {
                    "sent": "And here we can see that actually our trained discriminative model actually performed better than the human, But this is just cause the discriminative model has seen much more training examples.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so here are the conclusions I'll go again through the main scientific contributions.",
                    "label": 0
                },
                {
                    "sent": "So the first contribution is context free grammar reduction approach, so the so the grammar is used to build semantic trees and the grammar is also the grammar rules are also used to extract taxonomy taxonomic relations in one of the experiments we extracted more than 100 taxonomic relations.",
                    "label": 0
                },
                {
                    "sent": "The next contributions are semantic trees coupled with the two ontology learning approaches.",
                    "label": 1
                },
                {
                    "sent": "The first one is the relation extraction and the second one is instance discovery, and if we take take a look at this, both the first 2 contributions together we can see that we can build several different aspects of the ontology in.",
                    "label": 0
                },
                {
                    "sent": "More in a controllable and transparent way, and so now the third contribution.",
                    "label": 0
                },
                {
                    "sent": "We are the text generative neural networks for natural language inference and our best model is novel because it doesn't contain the decoder and instead of the decoder the model is learning mapping, mapping embeddings for each training example separately.",
                    "label": 1
                },
                {
                    "sent": "So in the last procedure, the last contribution is the procedure for evaluating.",
                    "label": 0
                },
                {
                    "sent": "Such.",
                    "label": 0
                },
                {
                    "sent": "Text hypothesis, where we construct new analyzed data set and then train the classifier on this data set and then we can compare the accuracy to the classifier trained on the original data set and we can see that the accuracy our best model is somewhat close to the accuracy of the regional model.",
                    "label": 1
                },
                {
                    "sent": "Plus if we combine the data from.",
                    "label": 1
                },
                {
                    "sent": "Our best model and the original data set.",
                    "label": 0
                },
                {
                    "sent": "Then we get the highest accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are also some publication I've been coauthor on.",
                    "label": 0
                },
                {
                    "sent": "Three Journal papers, two of them are directly connected to dissertations to this dissertation, and the rest one are conference papers that are also related to this dissertation, and I will stop here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}