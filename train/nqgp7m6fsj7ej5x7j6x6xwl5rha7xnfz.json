{
    "id": "nqgp7m6fsj7ej5x7j6x6xwl5rha7xnfz",
    "title": "Learning Linear Dynamical Systems without Sequence Information",
    "info": {
        "author": [
            "Tzu-Kuo Huang, The Auton Lab, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_huang_llds/",
    "segmentation": [
        [
            "Questions like these, how can we learn dynamic models?"
        ],
        [
            "And in this work we consider linear discrete time, continuous state and fully observable systems.",
            "Another way to say this is we consider a multi variate order one auto regressive model.",
            "We use X to indicate a state vector at least in a continuous space.",
            "And we assume that these axes are fully observable, so the state space is the observation of space and we don't distinguish between them.",
            "And you superscript to denote the time index of state vector and the transition from one point to the next is governed by a linear transformation parametrized by this matrix A and thus a process noise term which is normally distributed random variable with spherical variance.",
            "So."
        ],
        [
            "What do we mean exactly by a sequence sample or sequence observation?",
            "So the definition is that we can look at the picture on the right.",
            "So we've been looking at multiple and independent trajectories of a dynamical system.",
            "But from each trajectory indicated by blue curve here from each one of them we only observe one single point.",
            "Which are represented by the red dots here and the time index of that point.",
            "Is unknown, but it's assumed to be uniformly distributed between one and the maximum value Tmax.",
            "So if there are N such trajectories, we would end up with a simple having an observations without time indices.",
            "So."
        ],
        [
            "Given such an unsequenced sample.",
            "Our goal in this work is to estimate the transition matrix A and the noise variance Sigma squared.",
            "So just to give you an idea of the results we obtained.",
            "On the left is a simple generated from a simple 2 dimensional dynamic system.",
            "You have the top industries.",
            "After applying our method to this sample, we get the vector plot.",
            "Which it shows the estimated gradient.",
            "Which is we apply the estimated transition matrix they had to the simple point.",
            "This is why we have so as I will show later these arrows.",
            "Fairly consistent with the dynamics."
        ],
        [
            "So how do we do this?",
            "We start by investigating the true likelihood under our model.",
            "Some ship.",
            "So X is the point in this space and we use T of X.",
            "To denote its time index which is unknown.",
            "And since there is process noise.",
            "The system would induce a density over the entire state space.",
            "By considering all time steps and all possible trajectories.",
            "And this is illustrated by this region surrounded by the red curve here.",
            "So we denote this entity by F of X.",
            "And if we only look at a specific time point, say J.",
            "The system also induced a density.",
            "So it would be not this time specific conditional density as G of X given J.",
            "See last traded by this region surrounded by the proper."
        ],
        [
            "As I just mentioned, each XI in our sample is from a different trajectory.",
            "But suppose no, we also know for each XI, the true predecessor XI~ we may write down based on our model assumption, the joint likelihood.",
            "So each term in this product.",
            "Is due to the fact that we obtain XI.",
            "Applying the transition matrix A to X~ I and then then add spherical Gaussian noise.",
            "And since we assume the trajectory sign dependent, the joint likelihood is simply a product overall observations."
        ],
        [
            "But we don't actually have extra die.",
            "So what we do is we interpret it out with respect to the desert at the previous time point to that of XI, which is T of time minus one.",
            "More quickly.",
            "For instance, here in the product.",
            "Replace exterior I with a placeholder.",
            "And we integrate over X with respect to this time conditional density G of X given TXI minus one.",
            "So this function here is called the true likelihood in this work.",
            "Up"
        ],
        [
            "This traducteur holding hand a straightforward estimation procedure is next in likelihood, however, directly maximizing this function on A and Sigma square is because we don't have the true time index and this time conditional density G. Based on our model assumption, prints out to contain older terms in age and older depends on this missing index.",
            "So instead we propose to maximize approximate likelihood and propose two kinds of approximation.",
            "The true likelihood leads to these two models in order to model, and a partial order model.",
            "So."
        ],
        [
            "So for an order of abbreviated as UN recorder, assume the time index is uniformly sampled from the set 1 to the banks.",
            "And because it's missing.",
            "So we simply marginalized over the missing time index, and more specifically in this product.",
            "So we not only integrate over X, we also sum over T of X.",
            "With respect to a uniform mass function, one over Tmax.",
            "So by looking at this term and recalling that this G is the time conditional density, this term is actually a joint density function of X and time.",
            "So by taking this action inside that would give us back approximately if of X."
        ],
        [
            "And from here.",
            "We approximate the state space density of X with empirical density, which is the sample we have.",
            "So the integration.",
            "Is replaced by a summation.",
            "Over over observations, except for X itself to devote to avoid degeneracy.",
            "So a result of this approximation is that.",
            "For each, I will assume that it can actually find its true immediate assessor from the temple, which is actually, which is not true.",
            "So this is a result of the approximation."
        ],
        [
            "And this is the likelihood for UN.",
            "So for estimation with any algorithm, by introducing a latent variable resistor variable Z, which is a binary matrix.",
            "And with my JS entry, indicates whether it's Jai Jai comes from or is generated by Excel and this coming from here.",
            "Approximation.",
            "And the step is similar to that of Gaussian mixture models and then step for solving A and Sigma squared is simply this linear regression.",
            "So the."
        ],
        [
            "Thinks about you instead.",
            "It's a reasonable approximation, has a simple estimation procedure.",
            "And in our experiment works generally pretty well, but the problem is that as we marginalized over the meeting time index key of XI, we kind of obscure the underlying time structure of the sample points.",
            "And this in some cases may lead to generate estimates which globally evolving structure globally evolving dynamics for example.",
            "The left shows the true gradient on a 3D system.",
            "And if we apply you into this sample.",
            "We're going to get the estimate.",
            "The gradients are on the right, which is clearly very far from the truth, and we see the dynamics between these two sides.",
            "For many small cycles.",
            "And this motivator proposed our second approach.",
            "We"
        ],
        [
            "This is the partial order model abbreviated as P, and so the idea of it is instead of marginalizing over the time index, we try to estimate it.",
            "However, estimating this time index directly is difficult because it made finding a total order simple point which has to do with maximization over the space of permutations.",
            "So our solution is we break the total order into pairwise relationships and they will take a partial order."
        ],
        [
            "So what do we do here in the true likelihood?",
            "We don't do marginalization.",
            "Instead, we target directly the conditional density G, and we approximated the approximate this integration over space.",
            "Fascination overall observations.",
            "And we also replace this density function by probability mass function.",
            "So that for fixed I.",
            "The summation over.",
            "Equals one."
        ],
        [
            "An renaming jihad as Omega IJ Char Apparel, which are pairwise parameters that bear the underlying hold time.",
            "We obtained the approximate likelihood for PN.",
            "And clearly."
        ],
        [
            "To have consistency in time direction, we need to put some constraints on Omega.",
            "So recall that Guy Jay is head of XJ, given TXI minus one.",
            "So you can think of it as the probability that X Ray comes one time step before XI.",
            "So the matrix Omega, which collects will make a hyge should be no negative, and each row sums to one or 00 correspond to the observations are earlier in time.",
            "And so this is the first set of constraints.",
            "More importantly, we want the time direction to be consistent, which is to say that an adjacency matrix Omega represents a directed acyclic graph.",
            "To have more efficient computation, we further modify these constraints to be that Omega is the binary matrix and it represented directly treat.",
            "Combining the."
        ],
        [
            "Strange, with the approximate likelihood at the estimation, 4:00 PM is a constraint maximization problem.",
            "Whose variables are the matrix, a simple square and the matrix Omega and also an index of the root of the tree.",
            "And we solve problems by alternating optimization procedure where we first fix A and small square South for Omega and those are.",
            "This is equivalent to finding a maximum spanning tree undirected graph, which can be solved in extra time and then we fixed Omega.",
            "And maximize for ANC my square, which is again linear, least square regression."
        ],
        [
            "So the estimation procedures for these two models are solving nonconvex problems, so we follow the standard.",
            "Approach trying multiple random initializations to appeal with local up.",
            "We also tried a deterministic alternative which is we take a meaningful learning or dimensionality reduction technique and then we project.",
            "They have pointing to fundamental space and then we talked the points according to this one day projection.",
            "Then we have a sequence Temple.",
            "From there we can use any technique for learning dynamic systems to learn the model parameters.",
            "So and then we can use them to initialize here and PM.",
            "OK."
        ],
        [
            "In our experiments, we tried several things added datasets.",
            "Go to the system as illustrated here.",
            "We generated 40 samples of 200 points each and Sigma was .2 and we also have two 3 dimensional distance shown here, 3 Dash 1 and three dash 2.",
            "And we have different sample sizes.",
            "The small ones have 200 points each.",
            "Large ones have 2000 points each, and we also very signal from .2 to .8."
        ],
        [
            "Compare 6 measures.",
            "Maximum variance folding?",
            "And are two proposed models.",
            "And you end with multiple random initializations and also the combination of PNUN&NVU.",
            "And also we reported the result of guesses of day in Sigma squared.",
            "So to evaluate."
        ],
        [
            "3rd we have two criteria matrix error which measures how far away the estimate is to the truth and also a gradient cosine score which measures the celebrity between the true gradient and the estimate."
        ],
        [
            "Gradient.",
            "So here are the results on the 2D system.",
            "We can see that.",
            "All methods perform pretty well except rent.",
            "Anthracosis core things are similar and PM was the best on the left or the true gradient ones that hold on tight are the estimated gradient we can see yet fairly consistent with the truth."
        ],
        [
            "And for three dimensional systems.",
            "So so I first do some vector plot so that we have true gradients and on the right we have estimated gradients.",
            "We can see there."
        ],
        [
            "Be consistent and for three days to have the same results."
        ],
        [
            "And for some qualitative result I show box plots of matrix.",
            "So the red bar here is the median.",
            "And the two ends are the two courthouse.",
            "So for matrix error rent and UN are works for small samples, but you and becomes competitive with P and when several times increased and for cosine score things are pretty similar, but in video doesn't work as well.",
            "As."
        ],
        [
            "430 dash 2.",
            "Hope all methods work quite well in terms of matrix error and you is very different for both small samples.",
            "And focus or we see the same pattern here."
        ],
        [
            "So somebody and from our experiments, we found that the manifold learning technique is that you know in three, then in the international system."
        ],
        [
            "And in the first 30, three 7:00 PM works better than UN on small samples.",
            "More PM does a simple set, Chris.",
            "Ender 3 Dash 2 up to see that PM doesn't get as much improvement as you must, so we suspect that the directionality constrains and do some bias."
        ],
        [
            "And for you in.",
            "In some cases, for example in 30 Dash 1, when Sigma was .2.",
            "It's pretty bad.",
            "So.",
            "This leads us to the questions.",
            "What are the limitations of it and under what conditions to what extent, problem can be solved.",
            "This will be our future direct."
        ],
        [
            "So in conclusion, we proposed this problem learning fully adjustable linear dynamic systems from non sequence data post to approximate likelihood approach is an unordered management partial order.",
            "They work both.",
            "They both work well on synthetic data, so we have many interesting features.",
            "Directions we would like to apply them to real data and as I mentioned, we would like to investigate their theoretical properties and also we tried.",
            "We extended to nonlinear case and partial observability that is.",
            "So is everybody.",
            "You observe the time but noisy.",
            "Normally you have perfect knowledge of time.",
            "You looked at where you have no knowledge of time and this seems to be an easier problems for people supposes noise on the time measurement and you can see how that propagates through in the linear system.",
            "Instruction noise model of this.",
            "That's a good suggestion, but we haven't looked at it yet, so thanks.",
            "I would just add that.",
            "What's up bro?",
            "Exactly, yeah.",
            "Question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions like these, how can we learn dynamic models?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this work we consider linear discrete time, continuous state and fully observable systems.",
                    "label": 1
                },
                {
                    "sent": "Another way to say this is we consider a multi variate order one auto regressive model.",
                    "label": 0
                },
                {
                    "sent": "We use X to indicate a state vector at least in a continuous space.",
                    "label": 0
                },
                {
                    "sent": "And we assume that these axes are fully observable, so the state space is the observation of space and we don't distinguish between them.",
                    "label": 0
                },
                {
                    "sent": "And you superscript to denote the time index of state vector and the transition from one point to the next is governed by a linear transformation parametrized by this matrix A and thus a process noise term which is normally distributed random variable with spherical variance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do we mean exactly by a sequence sample or sequence observation?",
                    "label": 0
                },
                {
                    "sent": "So the definition is that we can look at the picture on the right.",
                    "label": 0
                },
                {
                    "sent": "So we've been looking at multiple and independent trajectories of a dynamical system.",
                    "label": 0
                },
                {
                    "sent": "But from each trajectory indicated by blue curve here from each one of them we only observe one single point.",
                    "label": 0
                },
                {
                    "sent": "Which are represented by the red dots here and the time index of that point.",
                    "label": 0
                },
                {
                    "sent": "Is unknown, but it's assumed to be uniformly distributed between one and the maximum value Tmax.",
                    "label": 0
                },
                {
                    "sent": "So if there are N such trajectories, we would end up with a simple having an observations without time indices.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given such an unsequenced sample.",
                    "label": 0
                },
                {
                    "sent": "Our goal in this work is to estimate the transition matrix A and the noise variance Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an idea of the results we obtained.",
                    "label": 0
                },
                {
                    "sent": "On the left is a simple generated from a simple 2 dimensional dynamic system.",
                    "label": 0
                },
                {
                    "sent": "You have the top industries.",
                    "label": 0
                },
                {
                    "sent": "After applying our method to this sample, we get the vector plot.",
                    "label": 0
                },
                {
                    "sent": "Which it shows the estimated gradient.",
                    "label": 0
                },
                {
                    "sent": "Which is we apply the estimated transition matrix they had to the simple point.",
                    "label": 0
                },
                {
                    "sent": "This is why we have so as I will show later these arrows.",
                    "label": 0
                },
                {
                    "sent": "Fairly consistent with the dynamics.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we do this?",
                    "label": 0
                },
                {
                    "sent": "We start by investigating the true likelihood under our model.",
                    "label": 0
                },
                {
                    "sent": "Some ship.",
                    "label": 0
                },
                {
                    "sent": "So X is the point in this space and we use T of X.",
                    "label": 0
                },
                {
                    "sent": "To denote its time index which is unknown.",
                    "label": 1
                },
                {
                    "sent": "And since there is process noise.",
                    "label": 1
                },
                {
                    "sent": "The system would induce a density over the entire state space.",
                    "label": 0
                },
                {
                    "sent": "By considering all time steps and all possible trajectories.",
                    "label": 0
                },
                {
                    "sent": "And this is illustrated by this region surrounded by the red curve here.",
                    "label": 0
                },
                {
                    "sent": "So we denote this entity by F of X.",
                    "label": 0
                },
                {
                    "sent": "And if we only look at a specific time point, say J.",
                    "label": 0
                },
                {
                    "sent": "The system also induced a density.",
                    "label": 0
                },
                {
                    "sent": "So it would be not this time specific conditional density as G of X given J.",
                    "label": 0
                },
                {
                    "sent": "See last traded by this region surrounded by the proper.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I just mentioned, each XI in our sample is from a different trajectory.",
                    "label": 0
                },
                {
                    "sent": "But suppose no, we also know for each XI, the true predecessor XI~ we may write down based on our model assumption, the joint likelihood.",
                    "label": 1
                },
                {
                    "sent": "So each term in this product.",
                    "label": 0
                },
                {
                    "sent": "Is due to the fact that we obtain XI.",
                    "label": 0
                },
                {
                    "sent": "Applying the transition matrix A to X~ I and then then add spherical Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "And since we assume the trajectory sign dependent, the joint likelihood is simply a product overall observations.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we don't actually have extra die.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we interpret it out with respect to the desert at the previous time point to that of XI, which is T of time minus one.",
                    "label": 1
                },
                {
                    "sent": "More quickly.",
                    "label": 0
                },
                {
                    "sent": "For instance, here in the product.",
                    "label": 0
                },
                {
                    "sent": "Replace exterior I with a placeholder.",
                    "label": 0
                },
                {
                    "sent": "And we integrate over X with respect to this time conditional density G of X given TXI minus one.",
                    "label": 0
                },
                {
                    "sent": "So this function here is called the true likelihood in this work.",
                    "label": 1
                },
                {
                    "sent": "Up",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This traducteur holding hand a straightforward estimation procedure is next in likelihood, however, directly maximizing this function on A and Sigma square is because we don't have the true time index and this time conditional density G. Based on our model assumption, prints out to contain older terms in age and older depends on this missing index.",
                    "label": 1
                },
                {
                    "sent": "So instead we propose to maximize approximate likelihood and propose two kinds of approximation.",
                    "label": 1
                },
                {
                    "sent": "The true likelihood leads to these two models in order to model, and a partial order model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for an order of abbreviated as UN recorder, assume the time index is uniformly sampled from the set 1 to the banks.",
                    "label": 1
                },
                {
                    "sent": "And because it's missing.",
                    "label": 0
                },
                {
                    "sent": "So we simply marginalized over the missing time index, and more specifically in this product.",
                    "label": 1
                },
                {
                    "sent": "So we not only integrate over X, we also sum over T of X.",
                    "label": 0
                },
                {
                    "sent": "With respect to a uniform mass function, one over Tmax.",
                    "label": 0
                },
                {
                    "sent": "So by looking at this term and recalling that this G is the time conditional density, this term is actually a joint density function of X and time.",
                    "label": 0
                },
                {
                    "sent": "So by taking this action inside that would give us back approximately if of X.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from here.",
                    "label": 0
                },
                {
                    "sent": "We approximate the state space density of X with empirical density, which is the sample we have.",
                    "label": 0
                },
                {
                    "sent": "So the integration.",
                    "label": 0
                },
                {
                    "sent": "Is replaced by a summation.",
                    "label": 0
                },
                {
                    "sent": "Over over observations, except for X itself to devote to avoid degeneracy.",
                    "label": 0
                },
                {
                    "sent": "So a result of this approximation is that.",
                    "label": 0
                },
                {
                    "sent": "For each, I will assume that it can actually find its true immediate assessor from the temple, which is actually, which is not true.",
                    "label": 0
                },
                {
                    "sent": "So this is a result of the approximation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the likelihood for UN.",
                    "label": 0
                },
                {
                    "sent": "So for estimation with any algorithm, by introducing a latent variable resistor variable Z, which is a binary matrix.",
                    "label": 0
                },
                {
                    "sent": "And with my JS entry, indicates whether it's Jai Jai comes from or is generated by Excel and this coming from here.",
                    "label": 0
                },
                {
                    "sent": "Approximation.",
                    "label": 0
                },
                {
                    "sent": "And the step is similar to that of Gaussian mixture models and then step for solving A and Sigma squared is simply this linear regression.",
                    "label": 1
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thinks about you instead.",
                    "label": 0
                },
                {
                    "sent": "It's a reasonable approximation, has a simple estimation procedure.",
                    "label": 1
                },
                {
                    "sent": "And in our experiment works generally pretty well, but the problem is that as we marginalized over the meeting time index key of XI, we kind of obscure the underlying time structure of the sample points.",
                    "label": 1
                },
                {
                    "sent": "And this in some cases may lead to generate estimates which globally evolving structure globally evolving dynamics for example.",
                    "label": 0
                },
                {
                    "sent": "The left shows the true gradient on a 3D system.",
                    "label": 0
                },
                {
                    "sent": "And if we apply you into this sample.",
                    "label": 0
                },
                {
                    "sent": "We're going to get the estimate.",
                    "label": 0
                },
                {
                    "sent": "The gradients are on the right, which is clearly very far from the truth, and we see the dynamics between these two sides.",
                    "label": 0
                },
                {
                    "sent": "For many small cycles.",
                    "label": 0
                },
                {
                    "sent": "And this motivator proposed our second approach.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the partial order model abbreviated as P, and so the idea of it is instead of marginalizing over the time index, we try to estimate it.",
                    "label": 1
                },
                {
                    "sent": "However, estimating this time index directly is difficult because it made finding a total order simple point which has to do with maximization over the space of permutations.",
                    "label": 1
                },
                {
                    "sent": "So our solution is we break the total order into pairwise relationships and they will take a partial order.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we do here in the true likelihood?",
                    "label": 0
                },
                {
                    "sent": "We don't do marginalization.",
                    "label": 0
                },
                {
                    "sent": "Instead, we target directly the conditional density G, and we approximated the approximate this integration over space.",
                    "label": 0
                },
                {
                    "sent": "Fascination overall observations.",
                    "label": 0
                },
                {
                    "sent": "And we also replace this density function by probability mass function.",
                    "label": 0
                },
                {
                    "sent": "So that for fixed I.",
                    "label": 0
                },
                {
                    "sent": "The summation over.",
                    "label": 0
                },
                {
                    "sent": "Equals one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An renaming jihad as Omega IJ Char Apparel, which are pairwise parameters that bear the underlying hold time.",
                    "label": 0
                },
                {
                    "sent": "We obtained the approximate likelihood for PN.",
                    "label": 0
                },
                {
                    "sent": "And clearly.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To have consistency in time direction, we need to put some constraints on Omega.",
                    "label": 0
                },
                {
                    "sent": "So recall that Guy Jay is head of XJ, given TXI minus one.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as the probability that X Ray comes one time step before XI.",
                    "label": 0
                },
                {
                    "sent": "So the matrix Omega, which collects will make a hyge should be no negative, and each row sums to one or 00 correspond to the observations are earlier in time.",
                    "label": 0
                },
                {
                    "sent": "And so this is the first set of constraints.",
                    "label": 0
                },
                {
                    "sent": "More importantly, we want the time direction to be consistent, which is to say that an adjacency matrix Omega represents a directed acyclic graph.",
                    "label": 1
                },
                {
                    "sent": "To have more efficient computation, we further modify these constraints to be that Omega is the binary matrix and it represented directly treat.",
                    "label": 0
                },
                {
                    "sent": "Combining the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strange, with the approximate likelihood at the estimation, 4:00 PM is a constraint maximization problem.",
                    "label": 0
                },
                {
                    "sent": "Whose variables are the matrix, a simple square and the matrix Omega and also an index of the root of the tree.",
                    "label": 0
                },
                {
                    "sent": "And we solve problems by alternating optimization procedure where we first fix A and small square South for Omega and those are.",
                    "label": 1
                },
                {
                    "sent": "This is equivalent to finding a maximum spanning tree undirected graph, which can be solved in extra time and then we fixed Omega.",
                    "label": 0
                },
                {
                    "sent": "And maximize for ANC my square, which is again linear, least square regression.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the estimation procedures for these two models are solving nonconvex problems, so we follow the standard.",
                    "label": 0
                },
                {
                    "sent": "Approach trying multiple random initializations to appeal with local up.",
                    "label": 0
                },
                {
                    "sent": "We also tried a deterministic alternative which is we take a meaningful learning or dimensionality reduction technique and then we project.",
                    "label": 0
                },
                {
                    "sent": "They have pointing to fundamental space and then we talked the points according to this one day projection.",
                    "label": 0
                },
                {
                    "sent": "Then we have a sequence Temple.",
                    "label": 0
                },
                {
                    "sent": "From there we can use any technique for learning dynamic systems to learn the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So and then we can use them to initialize here and PM.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our experiments, we tried several things added datasets.",
                    "label": 0
                },
                {
                    "sent": "Go to the system as illustrated here.",
                    "label": 0
                },
                {
                    "sent": "We generated 40 samples of 200 points each and Sigma was .2 and we also have two 3 dimensional distance shown here, 3 Dash 1 and three dash 2.",
                    "label": 1
                },
                {
                    "sent": "And we have different sample sizes.",
                    "label": 0
                },
                {
                    "sent": "The small ones have 200 points each.",
                    "label": 1
                },
                {
                    "sent": "Large ones have 2000 points each, and we also very signal from .2 to .8.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Compare 6 measures.",
                    "label": 0
                },
                {
                    "sent": "Maximum variance folding?",
                    "label": 0
                },
                {
                    "sent": "And are two proposed models.",
                    "label": 0
                },
                {
                    "sent": "And you end with multiple random initializations and also the combination of PNUN&NVU.",
                    "label": 1
                },
                {
                    "sent": "And also we reported the result of guesses of day in Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So to evaluate.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd we have two criteria matrix error which measures how far away the estimate is to the truth and also a gradient cosine score which measures the celebrity between the true gradient and the estimate.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gradient.",
                    "label": 0
                },
                {
                    "sent": "So here are the results on the 2D system.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "All methods perform pretty well except rent.",
                    "label": 0
                },
                {
                    "sent": "Anthracosis core things are similar and PM was the best on the left or the true gradient ones that hold on tight are the estimated gradient we can see yet fairly consistent with the truth.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for three dimensional systems.",
                    "label": 0
                },
                {
                    "sent": "So so I first do some vector plot so that we have true gradients and on the right we have estimated gradients.",
                    "label": 0
                },
                {
                    "sent": "We can see there.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be consistent and for three days to have the same results.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for some qualitative result I show box plots of matrix.",
                    "label": 0
                },
                {
                    "sent": "So the red bar here is the median.",
                    "label": 0
                },
                {
                    "sent": "And the two ends are the two courthouse.",
                    "label": 0
                },
                {
                    "sent": "So for matrix error rent and UN are works for small samples, but you and becomes competitive with P and when several times increased and for cosine score things are pretty similar, but in video doesn't work as well.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "430 dash 2.",
                    "label": 0
                },
                {
                    "sent": "Hope all methods work quite well in terms of matrix error and you is very different for both small samples.",
                    "label": 0
                },
                {
                    "sent": "And focus or we see the same pattern here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So somebody and from our experiments, we found that the manifold learning technique is that you know in three, then in the international system.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the first 30, three 7:00 PM works better than UN on small samples.",
                    "label": 1
                },
                {
                    "sent": "More PM does a simple set, Chris.",
                    "label": 0
                },
                {
                    "sent": "Ender 3 Dash 2 up to see that PM doesn't get as much improvement as you must, so we suspect that the directionality constrains and do some bias.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for you in.",
                    "label": 0
                },
                {
                    "sent": "In some cases, for example in 30 Dash 1, when Sigma was .2.",
                    "label": 0
                },
                {
                    "sent": "It's pretty bad.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This leads us to the questions.",
                    "label": 0
                },
                {
                    "sent": "What are the limitations of it and under what conditions to what extent, problem can be solved.",
                    "label": 1
                },
                {
                    "sent": "This will be our future direct.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we proposed this problem learning fully adjustable linear dynamic systems from non sequence data post to approximate likelihood approach is an unordered management partial order.",
                    "label": 1
                },
                {
                    "sent": "They work both.",
                    "label": 0
                },
                {
                    "sent": "They both work well on synthetic data, so we have many interesting features.",
                    "label": 1
                },
                {
                    "sent": "Directions we would like to apply them to real data and as I mentioned, we would like to investigate their theoretical properties and also we tried.",
                    "label": 0
                },
                {
                    "sent": "We extended to nonlinear case and partial observability that is.",
                    "label": 0
                },
                {
                    "sent": "So is everybody.",
                    "label": 0
                },
                {
                    "sent": "You observe the time but noisy.",
                    "label": 0
                },
                {
                    "sent": "Normally you have perfect knowledge of time.",
                    "label": 0
                },
                {
                    "sent": "You looked at where you have no knowledge of time and this seems to be an easier problems for people supposes noise on the time measurement and you can see how that propagates through in the linear system.",
                    "label": 0
                },
                {
                    "sent": "Instruction noise model of this.",
                    "label": 0
                },
                {
                    "sent": "That's a good suggestion, but we haven't looked at it yet, so thanks.",
                    "label": 0
                },
                {
                    "sent": "I would just add that.",
                    "label": 0
                },
                {
                    "sent": "What's up bro?",
                    "label": 0
                },
                {
                    "sent": "Exactly, yeah.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        }
    }
}