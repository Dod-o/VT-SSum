{
    "id": "ji7fl5p27s3fevdpcnzqtoypojv2bgqr",
    "title": "Localized Multiple Kernel Learning",
    "info": {
        "author": [
            "Mehmet G\u00f6nen, Department of Information and Computer Science, Aalto University"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods->Multiple Kernel Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_gonen_lmk/",
    "segmentation": [
        [
            "This is the outline of my talk.",
            "First, I will introduce multiple kernel learning and give our motivation for localising multiple kernel learning and explain the details of our algorithm, and I will discuss the key properties of our methods and before conclusions I give the experimental results on benchmark datasets and biological."
        ],
        [
            "Datasets.",
            "In kernel based methods we use a mapping function to move training instances to a feature space.",
            "Then we replaced that products of these mapping functions with a kernel function.",
            "But we have to choose a good kernel function for this purpose, and this is performed by using a statistical cross validation procedure.",
            "But instead of using a single feature space, we can combine different.",
            "Feature Space is an obtain a bitter discriminants.",
            "By combining different feature space we can.",
            "Actually combine different kernel functions and decide their rates in this."
        ],
        [
            "There are two basic approaches for.",
            "Purpose the first one is using an unweighted sum before training we.",
            "Give equal equal weights to all kernel functions, then perform support vector training.",
            "The second approach is using weighted sum.",
            "For example, we can learn a convex combination of kernel functions, but in this approach we also have to optimize this combination parameters.",
            "But these two approaches uses these combination weights over the whole input space and this may not be the optimal choice.",
            "In order to better model the training data, we can use the data dependent combination of these kernels.",
            "For example, in two previous works, a generator model and the compositional methods methods were proposed.",
            "In this work, we propose using a. Paramit parametric gating model for assigning kernel weight."
        ],
        [
            "In order to really illustrate the idea, we create a toy datasets which consist of four Gaussian components.",
            "These dashed lines are the Gaussian components.",
            "We have two for.",
            "Each class is class and these rectangles and circles represents positive class negative class.",
            "These dashed line.",
            "Is the optimal vice boundary if you use constant combination rates by using multiple kernel learning, we obtain this blue line as a discriminants and this is not a good approximation to optimal bias boundary.",
            "If you had, if you have gantic models like this red line and it gives higher weights to polynomial kernel in the left region and it gives higher rates to linear kernel in the right region so.",
            "We have a very good approximation to optimal bias boundary.",
            "I will explain in detail these figures."
        ],
        [
            "Enter.",
            "This is our mathematical model.",
            "We rewrite the decision function by using this gating model and to obtain these following optimization problem.",
            "This optimization problem is very similar to original SVM primal problem.",
            "There are two basic differences.",
            "Instead of minimizing the skirt non, now we are minimizing the summation of all squared norms and the second one in the separation constraint in the separation constraint.",
            "Now we have the gated model outputs.",
            "So.",
            "This model is not convex anymore due to these parameters in the separation constraint, we have to perform an iterative approach.",
            "We use a two step alternate optimization algorithm.",
            "In the first step we fixed the gating model parameters and solve for the support vector corrections.",
            "In the second step.",
            "We update the gating model parameters by the obtained support vector coefficient."
        ],
        [
            "It's.",
            "First of all, if we fix the gating model, this problem becomes a convex optimization problem.",
            "So we can write the Lagrangian dual function like this and we take the derivatives with respect to primal variables and obtain these three equations.",
            "These last two equations are very familiar because they are also in original SVM dual formulation and these.",
            "These are the new constant.",
            "These are the new equations which contain.",
            "Parametric gating model outputs.",
            "If you plug this into LaGrange."
        ],
        [
            "Dual function.",
            "We obtain these dual problem as you see this dual problem is exactly equivalent, two originally from dual problem except we use.",
            "Local combined kernel matrix instead of a single kernel matrix.",
            "This local combined kernel matrix simply pointwise weighted summation of all kernel matrices."
        ],
        [
            "After solving this dual problem in the second step, we have to update the gaiting model parameters.",
            "We used.",
            "Object value of the dual problem in gradient descent.",
            "Gradient descent updates.",
            "Objective value is an upper bound on the original problem.",
            "So if we decrease this upper bounds, we also we also decrease the objective value of the original problem.",
            "And we have to choose a gating model for the algorithm.",
            "For example, we can use a linear gating model with softmax function.",
            "This function has two basic properties.",
            "First of all, it is differentiable and we can use in gradient descent formulation the second one.",
            "This gating model always gives non negative output, so we obtain always positive semidefinite combined matrices."
        ],
        [
            "This is the complete algorithm it in the first step we initialize gating model parameters too small random numbers.",
            "By doing this we give almost equal combination weights to all kernels at the first iteration.",
            "Then we repeat the following steps until convergence.",
            "First we calculate the local combined kernel matrix with the current gated model, then sold.",
            "Canonical SVM dual problem with this mattress and after obtaining support vector coefficients, we simply update gating model parameters.",
            "After convergence we have two sets of parameters, now Alpha and Theta.",
            "And we obtain this decision function.",
            "Note that we are required to calculate the kernel.",
            "The output of kernel an if and only if.",
            "Both these gating model outputs are non 0."
        ],
        [
            "If you compare.",
            "Our algorithm with previous methods first.",
            "One is a mixture of experts framework.",
            "Mixture of experts frameworks combines local discriminate kernels with softmax function.",
            "If you use multiple linear kernels in our method, it is very similar to a mixture of experts framework.",
            "The other one is mixture face films.",
            "In this work, we partition the training data into clusters and training separate SVM on each cluster.",
            "Then the.",
            "After SVM training, the reassign this training data into clusters, but by using a different cost function.",
            "But in this work we use the same objective function for both training and the clustering.",
            "We can also generalize our method for regression and one class classification problems easily."
        ],
        [
            "If you look at the computational complexity in training phase, we have two main factors.",
            "The first one is the complexity of test VM servers used in the main loop.",
            "The second one is the number of iterations performed.",
            "In testing phase we again have two.",
            "Main factors The first one is number of support vectors to the other one is the gating model outputs.",
            "Multiple kernel learning extracts waits for kernels, and if a kernel has larger weight, stand other kernels, it means that this kernel have.",
            "More important information about this classification test.",
            "Our methods.",
            "Also extract this kind of information for local regions."
        ],
        [
            "He implements automated by using C++ and Mosaic optimization software and we used three kernels in experiments which are linear kernel, second degree, polynomial kernel and the Gaussian kernel.",
            "We choose the right parameter for the Gaussian kernel is the average nearest neighbor distance in the training set."
        ],
        [
            "If you look at these figures again by using a convex combination.",
            "The obtained this discriminants multiple kernel learning assign zero Point 3 way too linear kernel and 0.7 way two polynomial kernel and obtain these blue line as a discriminants.",
            "But our algorithm.",
            "Converts to this point, and this gating model.",
            "Assign larger weights to polynomial kernel in this region and separate these two components from this one.",
            "And in this region, assign larger ways to linear kernel.",
            "An separates these components from.",
            "The others.",
            "Then we combine these two local discriminants.",
            "We obtain this blue line as the final discriminate and this is a very good approximation to optimal wise boundary."
        ],
        [
            "If you combine three linear kernels, our algorithm converts to these red lines, and these red lines divides the input space into three regions and each region we learned local linear discriminants like this.",
            "And this bond.",
            "This one.",
            "And we combine these three local discriminants.",
            "We obtain again a very good approximation to optimal bias boundary, not that smooth transition between the regions because of the softmax function."
        ],
        [
            "These are the kernel matrices in the first iteration, and the last iteration, we obtain these kernel matrices by sorting rows with respect to their data values, and in this region linear kernel is used with larger weights, and in this region polynomial kernel is used with larger rates.",
            "In the first iteration we see this great point and.",
            "David, come right in the last generation."
        ],
        [
            "We perform experiments on 10 benchmark datasets from the UCI data repository and.",
            "We have results for conical SPM, multiple kernel learning.",
            "On our method we have the results for polynomial kernel, Gaussian kernel and.",
            "Polynomial angles, internal combinations if you.",
            "If you compare our method with multiple kernel learning, it obtains compatible average test accuracies but.",
            "We use statistically.",
            "Significantly fewer support vectors on three out of this time datasets, but according to Wilcoxon signed rank test, this is a significant win."
        ],
        [
            "If you combine three linear kernels with our method and combine this.",
            "A conical SVM with linear kernel.",
            "We obtain better accuracy results on three datasets.",
            "Restore fever support vectors on 6 out of 10 datasets, according to Wilcoxon signed rank test be obtained.",
            "Statistical, better accuracy results and.",
            "Store same amount of support vectors on these datasets."
        ],
        [
            "In addition to.",
            "These benchmark datasets.",
            "We use two biological datasets.",
            "In these datasets we again combine polynomial kernel and Gaussian kernel.",
            "With multiple kernel learning and with our method.",
            "In this combination setting we obtain.",
            "Statistically similar accuracy results by using nearly 25% fewer support vectors.",
            "If you combine 3.",
            "Linear kernels we obtain statistically better accuracy results by storing statistical fever support vectors."
        ],
        [
            "In conclusion, we introduced the localized multiple kernel learning framework, which consists of two main components.",
            "The first one is the parametric gating model which assigned kernel weights in an input Department manner.",
            "The second one is a kernel based learning algorithm and we performed coupled optimization with a 2 step alternate optimization procedure and our methods allows us.",
            "The.",
            "To use multiple copies of the same kernel.",
            "Anti only experiments.",
            "We see that by combining different kernel functions we obtained similar accuracy compatible accuracy results by using fever support statistically.",
            "Fever support vectors by using the same kernel functions in combination, we obtain better statistically better accuracy results by using fever."
        ],
        [
            "Support vectors.",
            "Currently we are working on kernel based gating models.",
            "Instead of learning to gating modeling in the original input space, we can also learn the gated model in a different feature space by using a mapping function and this.",
            "Variant allows us to use non vectorial data indicating models.",
            "Question.",
            "Think of.",
            "Next selection.",
            "Yeah.",
            "For."
        ],
        [
            "If we combine four different linear kernels, for example in this figure.",
            "The 4th one.",
            "Will be placed in the further region of the input space and it.",
            "It is not used by the combination group.",
            "For example.",
            "Alright, we'll talk later.",
            "Yeah, yeah, we tried this with four linear kernels or five linear kernels.",
            "It effectively used two or three.",
            "My question as well.",
            "This method of nukes and piecewise linear approximation income better local setup that we think become 100 kernels and then it takes a couple of relevant one for each error, in this case and then you end up basically building and quality non linear separation service using a number of very simple but relevant to specific localities, exactly actually instead of using a local kernel like Gaussian kernel.",
            "We can use very simple linear kernels to approximate the same effect.",
            "Like on a really large number of girls, do you know what comes out?",
            "No, we try to up to five kernels.",
            "Quick question, why do you choose to optimize the dual function when you optimize with respect to the gating parameters?",
            "Where do you choose to optimize the dual objective rather than the primal objective?"
        ],
        [
            "This is do objective has stated parameters in it so we can differentiate with respect to Theta and use integrate into that but.",
            "Super angle 4."
        ],
        [
            "Emulation.",
            "The primary object function doesn't have data.",
            "Data parameters in it.",
            "Right there you can express the slack variables in terms of the constraints, and then you do have the test.",
            "Yeah yeah, but this time we cannot use kernelized version.",
            "We don't know the W parameters."
        ],
        [
            "Exactly because we don't know the five function exactly for, for example, polynomial kernels or RBF kernel.",
            "Possible to express the primal in terms of the kernel for the linear kernel, yes.",
            "In the general case, using their position theorem, yeah we can talk about this.",
            "OK, OK?",
            "Can you please go to slide very special for K. So let's let's product between between beta M of XY and Z.",
            "The MXJ itself.",
            "You can be seen as eternal, correct?",
            "Yes, this is actually Sam over products, of course yeah.",
            "So basically when we learn to him you are effectively learning an entire kernel, right?",
            "I mean.",
            "A set of entire kernels 'cause we have a different.",
            "Multiplication.",
            "So does it in any effect masks the importance of the original kernels?",
            "Am what I mean is?",
            "Since you are learning this, it M parameters pointwise.",
            "It doesn't tend to make you relevant, which kind of spam.",
            "No, not bitch.",
            "The.",
            "Maybe depending on the parameters that depend, so I think it makes.",
            "So you're still only limiting yourself to combination of the cadence of the workload, so it makes the choice of games this important sense.",
            "For example, you have one wrong kernel, right?",
            "But?",
            "But it's not completely unimportant.",
            "The other questions.",
            "Great, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "First, I will introduce multiple kernel learning and give our motivation for localising multiple kernel learning and explain the details of our algorithm, and I will discuss the key properties of our methods and before conclusions I give the experimental results on benchmark datasets and biological.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datasets.",
                    "label": 0
                },
                {
                    "sent": "In kernel based methods we use a mapping function to move training instances to a feature space.",
                    "label": 0
                },
                {
                    "sent": "Then we replaced that products of these mapping functions with a kernel function.",
                    "label": 0
                },
                {
                    "sent": "But we have to choose a good kernel function for this purpose, and this is performed by using a statistical cross validation procedure.",
                    "label": 0
                },
                {
                    "sent": "But instead of using a single feature space, we can combine different.",
                    "label": 0
                },
                {
                    "sent": "Feature Space is an obtain a bitter discriminants.",
                    "label": 0
                },
                {
                    "sent": "By combining different feature space we can.",
                    "label": 0
                },
                {
                    "sent": "Actually combine different kernel functions and decide their rates in this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are two basic approaches for.",
                    "label": 0
                },
                {
                    "sent": "Purpose the first one is using an unweighted sum before training we.",
                    "label": 0
                },
                {
                    "sent": "Give equal equal weights to all kernel functions, then perform support vector training.",
                    "label": 0
                },
                {
                    "sent": "The second approach is using weighted sum.",
                    "label": 0
                },
                {
                    "sent": "For example, we can learn a convex combination of kernel functions, but in this approach we also have to optimize this combination parameters.",
                    "label": 0
                },
                {
                    "sent": "But these two approaches uses these combination weights over the whole input space and this may not be the optimal choice.",
                    "label": 0
                },
                {
                    "sent": "In order to better model the training data, we can use the data dependent combination of these kernels.",
                    "label": 0
                },
                {
                    "sent": "For example, in two previous works, a generator model and the compositional methods methods were proposed.",
                    "label": 0
                },
                {
                    "sent": "In this work, we propose using a. Paramit parametric gating model for assigning kernel weight.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to really illustrate the idea, we create a toy datasets which consist of four Gaussian components.",
                    "label": 0
                },
                {
                    "sent": "These dashed lines are the Gaussian components.",
                    "label": 0
                },
                {
                    "sent": "We have two for.",
                    "label": 0
                },
                {
                    "sent": "Each class is class and these rectangles and circles represents positive class negative class.",
                    "label": 0
                },
                {
                    "sent": "These dashed line.",
                    "label": 0
                },
                {
                    "sent": "Is the optimal vice boundary if you use constant combination rates by using multiple kernel learning, we obtain this blue line as a discriminants and this is not a good approximation to optimal bias boundary.",
                    "label": 0
                },
                {
                    "sent": "If you had, if you have gantic models like this red line and it gives higher weights to polynomial kernel in the left region and it gives higher rates to linear kernel in the right region so.",
                    "label": 0
                },
                {
                    "sent": "We have a very good approximation to optimal bias boundary.",
                    "label": 0
                },
                {
                    "sent": "I will explain in detail these figures.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "This is our mathematical model.",
                    "label": 0
                },
                {
                    "sent": "We rewrite the decision function by using this gating model and to obtain these following optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This optimization problem is very similar to original SVM primal problem.",
                    "label": 0
                },
                {
                    "sent": "There are two basic differences.",
                    "label": 0
                },
                {
                    "sent": "Instead of minimizing the skirt non, now we are minimizing the summation of all squared norms and the second one in the separation constraint in the separation constraint.",
                    "label": 0
                },
                {
                    "sent": "Now we have the gated model outputs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This model is not convex anymore due to these parameters in the separation constraint, we have to perform an iterative approach.",
                    "label": 0
                },
                {
                    "sent": "We use a two step alternate optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the first step we fixed the gating model parameters and solve for the support vector corrections.",
                    "label": 0
                },
                {
                    "sent": "In the second step.",
                    "label": 0
                },
                {
                    "sent": "We update the gating model parameters by the obtained support vector coefficient.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "First of all, if we fix the gating model, this problem becomes a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we can write the Lagrangian dual function like this and we take the derivatives with respect to primal variables and obtain these three equations.",
                    "label": 0
                },
                {
                    "sent": "These last two equations are very familiar because they are also in original SVM dual formulation and these.",
                    "label": 0
                },
                {
                    "sent": "These are the new constant.",
                    "label": 0
                },
                {
                    "sent": "These are the new equations which contain.",
                    "label": 0
                },
                {
                    "sent": "Parametric gating model outputs.",
                    "label": 0
                },
                {
                    "sent": "If you plug this into LaGrange.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dual function.",
                    "label": 0
                },
                {
                    "sent": "We obtain these dual problem as you see this dual problem is exactly equivalent, two originally from dual problem except we use.",
                    "label": 0
                },
                {
                    "sent": "Local combined kernel matrix instead of a single kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "This local combined kernel matrix simply pointwise weighted summation of all kernel matrices.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After solving this dual problem in the second step, we have to update the gaiting model parameters.",
                    "label": 0
                },
                {
                    "sent": "We used.",
                    "label": 0
                },
                {
                    "sent": "Object value of the dual problem in gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Gradient descent updates.",
                    "label": 0
                },
                {
                    "sent": "Objective value is an upper bound on the original problem.",
                    "label": 0
                },
                {
                    "sent": "So if we decrease this upper bounds, we also we also decrease the objective value of the original problem.",
                    "label": 0
                },
                {
                    "sent": "And we have to choose a gating model for the algorithm.",
                    "label": 0
                },
                {
                    "sent": "For example, we can use a linear gating model with softmax function.",
                    "label": 0
                },
                {
                    "sent": "This function has two basic properties.",
                    "label": 0
                },
                {
                    "sent": "First of all, it is differentiable and we can use in gradient descent formulation the second one.",
                    "label": 0
                },
                {
                    "sent": "This gating model always gives non negative output, so we obtain always positive semidefinite combined matrices.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the complete algorithm it in the first step we initialize gating model parameters too small random numbers.",
                    "label": 1
                },
                {
                    "sent": "By doing this we give almost equal combination weights to all kernels at the first iteration.",
                    "label": 0
                },
                {
                    "sent": "Then we repeat the following steps until convergence.",
                    "label": 0
                },
                {
                    "sent": "First we calculate the local combined kernel matrix with the current gated model, then sold.",
                    "label": 1
                },
                {
                    "sent": "Canonical SVM dual problem with this mattress and after obtaining support vector coefficients, we simply update gating model parameters.",
                    "label": 0
                },
                {
                    "sent": "After convergence we have two sets of parameters, now Alpha and Theta.",
                    "label": 0
                },
                {
                    "sent": "And we obtain this decision function.",
                    "label": 0
                },
                {
                    "sent": "Note that we are required to calculate the kernel.",
                    "label": 0
                },
                {
                    "sent": "The output of kernel an if and only if.",
                    "label": 0
                },
                {
                    "sent": "Both these gating model outputs are non 0.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you compare.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm with previous methods first.",
                    "label": 0
                },
                {
                    "sent": "One is a mixture of experts framework.",
                    "label": 0
                },
                {
                    "sent": "Mixture of experts frameworks combines local discriminate kernels with softmax function.",
                    "label": 0
                },
                {
                    "sent": "If you use multiple linear kernels in our method, it is very similar to a mixture of experts framework.",
                    "label": 0
                },
                {
                    "sent": "The other one is mixture face films.",
                    "label": 0
                },
                {
                    "sent": "In this work, we partition the training data into clusters and training separate SVM on each cluster.",
                    "label": 0
                },
                {
                    "sent": "Then the.",
                    "label": 0
                },
                {
                    "sent": "After SVM training, the reassign this training data into clusters, but by using a different cost function.",
                    "label": 0
                },
                {
                    "sent": "But in this work we use the same objective function for both training and the clustering.",
                    "label": 0
                },
                {
                    "sent": "We can also generalize our method for regression and one class classification problems easily.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at the computational complexity in training phase, we have two main factors.",
                    "label": 0
                },
                {
                    "sent": "The first one is the complexity of test VM servers used in the main loop.",
                    "label": 0
                },
                {
                    "sent": "The second one is the number of iterations performed.",
                    "label": 0
                },
                {
                    "sent": "In testing phase we again have two.",
                    "label": 0
                },
                {
                    "sent": "Main factors The first one is number of support vectors to the other one is the gating model outputs.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning extracts waits for kernels, and if a kernel has larger weight, stand other kernels, it means that this kernel have.",
                    "label": 1
                },
                {
                    "sent": "More important information about this classification test.",
                    "label": 0
                },
                {
                    "sent": "Our methods.",
                    "label": 0
                },
                {
                    "sent": "Also extract this kind of information for local regions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He implements automated by using C++ and Mosaic optimization software and we used three kernels in experiments which are linear kernel, second degree, polynomial kernel and the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "We choose the right parameter for the Gaussian kernel is the average nearest neighbor distance in the training set.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at these figures again by using a convex combination.",
                    "label": 0
                },
                {
                    "sent": "The obtained this discriminants multiple kernel learning assign zero Point 3 way too linear kernel and 0.7 way two polynomial kernel and obtain these blue line as a discriminants.",
                    "label": 0
                },
                {
                    "sent": "But our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Converts to this point, and this gating model.",
                    "label": 0
                },
                {
                    "sent": "Assign larger weights to polynomial kernel in this region and separate these two components from this one.",
                    "label": 0
                },
                {
                    "sent": "And in this region, assign larger ways to linear kernel.",
                    "label": 0
                },
                {
                    "sent": "An separates these components from.",
                    "label": 0
                },
                {
                    "sent": "The others.",
                    "label": 0
                },
                {
                    "sent": "Then we combine these two local discriminants.",
                    "label": 0
                },
                {
                    "sent": "We obtain this blue line as the final discriminate and this is a very good approximation to optimal wise boundary.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you combine three linear kernels, our algorithm converts to these red lines, and these red lines divides the input space into three regions and each region we learned local linear discriminants like this.",
                    "label": 0
                },
                {
                    "sent": "And this bond.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "And we combine these three local discriminants.",
                    "label": 0
                },
                {
                    "sent": "We obtain again a very good approximation to optimal bias boundary, not that smooth transition between the regions because of the softmax function.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the kernel matrices in the first iteration, and the last iteration, we obtain these kernel matrices by sorting rows with respect to their data values, and in this region linear kernel is used with larger weights, and in this region polynomial kernel is used with larger rates.",
                    "label": 0
                },
                {
                    "sent": "In the first iteration we see this great point and.",
                    "label": 0
                },
                {
                    "sent": "David, come right in the last generation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We perform experiments on 10 benchmark datasets from the UCI data repository and.",
                    "label": 0
                },
                {
                    "sent": "We have results for conical SPM, multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "On our method we have the results for polynomial kernel, Gaussian kernel and.",
                    "label": 0
                },
                {
                    "sent": "Polynomial angles, internal combinations if you.",
                    "label": 0
                },
                {
                    "sent": "If you compare our method with multiple kernel learning, it obtains compatible average test accuracies but.",
                    "label": 0
                },
                {
                    "sent": "We use statistically.",
                    "label": 0
                },
                {
                    "sent": "Significantly fewer support vectors on three out of this time datasets, but according to Wilcoxon signed rank test, this is a significant win.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you combine three linear kernels with our method and combine this.",
                    "label": 0
                },
                {
                    "sent": "A conical SVM with linear kernel.",
                    "label": 0
                },
                {
                    "sent": "We obtain better accuracy results on three datasets.",
                    "label": 0
                },
                {
                    "sent": "Restore fever support vectors on 6 out of 10 datasets, according to Wilcoxon signed rank test be obtained.",
                    "label": 0
                },
                {
                    "sent": "Statistical, better accuracy results and.",
                    "label": 0
                },
                {
                    "sent": "Store same amount of support vectors on these datasets.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition to.",
                    "label": 0
                },
                {
                    "sent": "These benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "We use two biological datasets.",
                    "label": 0
                },
                {
                    "sent": "In these datasets we again combine polynomial kernel and Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "With multiple kernel learning and with our method.",
                    "label": 0
                },
                {
                    "sent": "In this combination setting we obtain.",
                    "label": 0
                },
                {
                    "sent": "Statistically similar accuracy results by using nearly 25% fewer support vectors.",
                    "label": 0
                },
                {
                    "sent": "If you combine 3.",
                    "label": 0
                },
                {
                    "sent": "Linear kernels we obtain statistically better accuracy results by storing statistical fever support vectors.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion, we introduced the localized multiple kernel learning framework, which consists of two main components.",
                    "label": 1
                },
                {
                    "sent": "The first one is the parametric gating model which assigned kernel weights in an input Department manner.",
                    "label": 0
                },
                {
                    "sent": "The second one is a kernel based learning algorithm and we performed coupled optimization with a 2 step alternate optimization procedure and our methods allows us.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "To use multiple copies of the same kernel.",
                    "label": 0
                },
                {
                    "sent": "Anti only experiments.",
                    "label": 0
                },
                {
                    "sent": "We see that by combining different kernel functions we obtained similar accuracy compatible accuracy results by using fever support statistically.",
                    "label": 0
                },
                {
                    "sent": "Fever support vectors by using the same kernel functions in combination, we obtain better statistically better accuracy results by using fever.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Support vectors.",
                    "label": 0
                },
                {
                    "sent": "Currently we are working on kernel based gating models.",
                    "label": 0
                },
                {
                    "sent": "Instead of learning to gating modeling in the original input space, we can also learn the gated model in a different feature space by using a mapping function and this.",
                    "label": 0
                },
                {
                    "sent": "Variant allows us to use non vectorial data indicating models.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Think of.",
                    "label": 0
                },
                {
                    "sent": "Next selection.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we combine four different linear kernels, for example in this figure.",
                    "label": 0
                },
                {
                    "sent": "The 4th one.",
                    "label": 0
                },
                {
                    "sent": "Will be placed in the further region of the input space and it.",
                    "label": 0
                },
                {
                    "sent": "It is not used by the combination group.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Alright, we'll talk later.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, we tried this with four linear kernels or five linear kernels.",
                    "label": 0
                },
                {
                    "sent": "It effectively used two or three.",
                    "label": 0
                },
                {
                    "sent": "My question as well.",
                    "label": 0
                },
                {
                    "sent": "This method of nukes and piecewise linear approximation income better local setup that we think become 100 kernels and then it takes a couple of relevant one for each error, in this case and then you end up basically building and quality non linear separation service using a number of very simple but relevant to specific localities, exactly actually instead of using a local kernel like Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "We can use very simple linear kernels to approximate the same effect.",
                    "label": 0
                },
                {
                    "sent": "Like on a really large number of girls, do you know what comes out?",
                    "label": 0
                },
                {
                    "sent": "No, we try to up to five kernels.",
                    "label": 0
                },
                {
                    "sent": "Quick question, why do you choose to optimize the dual function when you optimize with respect to the gating parameters?",
                    "label": 0
                },
                {
                    "sent": "Where do you choose to optimize the dual objective rather than the primal objective?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is do objective has stated parameters in it so we can differentiate with respect to Theta and use integrate into that but.",
                    "label": 0
                },
                {
                    "sent": "Super angle 4.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Emulation.",
                    "label": 0
                },
                {
                    "sent": "The primary object function doesn't have data.",
                    "label": 0
                },
                {
                    "sent": "Data parameters in it.",
                    "label": 0
                },
                {
                    "sent": "Right there you can express the slack variables in terms of the constraints, and then you do have the test.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, but this time we cannot use kernelized version.",
                    "label": 0
                },
                {
                    "sent": "We don't know the W parameters.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly because we don't know the five function exactly for, for example, polynomial kernels or RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "Possible to express the primal in terms of the kernel for the linear kernel, yes.",
                    "label": 0
                },
                {
                    "sent": "In the general case, using their position theorem, yeah we can talk about this.",
                    "label": 0
                },
                {
                    "sent": "OK, OK?",
                    "label": 0
                },
                {
                    "sent": "Can you please go to slide very special for K. So let's let's product between between beta M of XY and Z.",
                    "label": 0
                },
                {
                    "sent": "The MXJ itself.",
                    "label": 0
                },
                {
                    "sent": "You can be seen as eternal, correct?",
                    "label": 0
                },
                {
                    "sent": "Yes, this is actually Sam over products, of course yeah.",
                    "label": 0
                },
                {
                    "sent": "So basically when we learn to him you are effectively learning an entire kernel, right?",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "A set of entire kernels 'cause we have a different.",
                    "label": 0
                },
                {
                    "sent": "Multiplication.",
                    "label": 0
                },
                {
                    "sent": "So does it in any effect masks the importance of the original kernels?",
                    "label": 0
                },
                {
                    "sent": "Am what I mean is?",
                    "label": 0
                },
                {
                    "sent": "Since you are learning this, it M parameters pointwise.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tend to make you relevant, which kind of spam.",
                    "label": 0
                },
                {
                    "sent": "No, not bitch.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Maybe depending on the parameters that depend, so I think it makes.",
                    "label": 0
                },
                {
                    "sent": "So you're still only limiting yourself to combination of the cadence of the workload, so it makes the choice of games this important sense.",
                    "label": 0
                },
                {
                    "sent": "For example, you have one wrong kernel, right?",
                    "label": 0
                },
                {
                    "sent": "But?",
                    "label": 0
                },
                {
                    "sent": "But it's not completely unimportant.",
                    "label": 0
                },
                {
                    "sent": "The other questions.",
                    "label": 0
                },
                {
                    "sent": "Great, thank you.",
                    "label": 0
                }
            ]
        }
    }
}