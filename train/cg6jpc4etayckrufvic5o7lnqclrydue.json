{
    "id": "cg6jpc4etayckrufvic5o7lnqclrydue",
    "title": "Neighbourhood Components Analysis and Metric Learning",
    "info": {
        "author": [
            "Sam Roweis"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis"
        ]
    },
    "url": "http://videolectures.net/lce06_roweis_ncaml/",
    "segmentation": [
        [
            "Everyone will be able to hear and people will be drifting in.",
            "Thank you all for interrupting your skiing to come.",
            "So just a quick show of hands.",
            "How many people have heard either Jacob or myself talk about neighborhood components analysis before.",
            "OK, so maybe about half so I'll try and go a little bit quickly through that and then get to some of the newer work with with the mirror.",
            "So feel free to interrupt me at anytime and ask questions or make insulting comments or whatever.",
            "Start an argument.",
            "OK, so as you all know, the sort of theme for today is distance metric learning, and the idea is that there's a lot of times in the core of an algorithm when we need."
        ],
        [
            "I'm kind of distance measure.",
            "Often we needed to have the properties of a metric, but maybe not always, which compares examples, so it's a scalar function which eats 2 examples and tells you how different or similar they are an unless the structure strongly specifies this metric.",
            "Or maybe some constraints on that metric, then it seems like the preferred approach is to learn the metric as part of your problem.",
            "So in other words, you have a lot of data off and the data has labels which implicitly tell you what the right metric is and you want to perform the learning along with the rest of the parameters based on the training set.",
            "So today I'm going to focus on something very very simple.",
            "Semi parametric classifiers like K nearest neighbor or Gaussian kernel support vector machines, Gaussian processes, you know very local machines.",
            "And the idea is, given a labeled set, so we have the training set of inputs, so I'll use X for inputs and labels C or Y.",
            "Sometimes I'll use.",
            "How should we learn a metric that will give good generalization when used in such a classifier?",
            "So what's the idea of these classifiers?",
            "They just look locally in the neighborhood of a test example at nearby training examples, and they sort of average their labels or their regression outputs or whatever.",
            "And the idea is to make this measurement of what does local mean?",
            "Good for whatever you're trying to do.",
            "Classification or prediction.",
            "Intuitively, you should think that our goal is to make directions of the space which are irrelevant for classification.",
            "We want to shrink them and directions of the space which are very informative.",
            "We want to stretch them out.",
            "That's that's the job of the metric.",
            "OK.",
            "So there's this sort of at this point in my talk.",
            "Usually someone puts up their hand and says, but your whole line of research is totally stupid because localist classifiers are ridiculous.",
            "They need an exponential amount of training data to just memorize the space, and they don't generalize in any meaningful way.",
            "OK, so that's actually a somewhat valid criticism, but it's not a criticism I'm going to get into."
        ],
        [
            "A battle about today.",
            "I'm going to sort of defer the criticism to this old paper by Rob Holt, which I think is still somewhat true today, which is that there are a lot of situations in which very simple classifiers tend to do surprisingly well, so you should think of this as a conditional talk if you're interested in some kind of a local method like nearest neighbor or Gaussian kernel support, vector machine or whatever, then I have something interesting to tell you about learning the distance if you're not interested, there's a wireless connection in this room.",
            "And I'll be done in 45 minutes.",
            "So, OK.",
            "So just to remind you all the set up, for instance based or memory."
        ],
        [
            "Based or nonparametric classifiers.",
            "The idea is that you just have the training set an you take say the K nearest neighbors of the training set.",
            "You take their majority vote and that's our class label.",
            "So people like to make fun of K nearest neighbor.",
            "But it's pretty interesting the decision services are nonlinear.",
            "It's semi parametric or nonparametric so it has high capacity without having really to train a lot of parameters.",
            "So what is this point?",
            "I think this is a very important point that's under appreciated.",
            "If you want your classifier to do something complicated.",
            "It's in some sense it has to have a lot of bits inside its brain.",
            "It has to have somewhere it has to have reasonable capacity, but we're not very good at setting huge numbers of parameters.",
            "So what are we going to do?",
            "We're going to say, well, we'll make the training set itself in some sense, store all those bits 'cause we're pretty sure we didn't screw up those bits, and then we're only going to set, you know, one number K or one bandwidth parameter Alpha and the quality of the prediction.",
            "Sort of automatically improves with more data.",
            "There are these results about it being asymptotically close to optimal.",
            "Which you may or may not be interested in because of the word asymptotic, but some people think that this word is just the gateway to lots of very interesting theoretical results, and some people think that this word means you should ignore everything that comes after it.",
            "OK, there's only a few parameters to tune.",
            "Usually you can do it by cross validation or some kind of simple optimization gradient on.",
            "Leave one out of something.",
            "OK, so what's the problem?",
            "The problem is what does nearest mean?",
            "You need to specify some kind of distance metric on the input, so that's one problem with these classifiers.",
            "And the second problem is computational."
        ],
        [
            "First you have to sort of store your whole training set and carry it around in your suitcase with you every time you want to do classification.",
            "So we'd like to sort of try and solve both of these problems.",
            "So today I'm going to talk mostly about the first problem.",
            "How do you learn a distance metric?",
            "And then I'll sort of show you how if you need to, you can make this distance metric low rank, which corresponds to measuring everything in an implicit feature space, which is low dimensional and that gives you the opportunity to take your training data and project it into that low dimensional space.",
            "And then have lower computational overhead, both in terms of memory footprint and time to search, so that's the that's the basic plan.",
            "So, as Jan mentioned this morning."
        ],
        [
            "And it's been touched on in many of the talks so far.",
            "In the workshop, you can either think of our goal as learning a metric.",
            "So in a second I'm going to sort of introduce the class of quadratic metrics, but for the moment, imagine that we're going to measure things by Gaussian type of metric Mahalanobis distance.",
            "You can either think of the goal is learning that metric, or as learning the square root of that metric, which is a transformation or projection or feature set.",
            "So there's a very strong link, obviously between learning metrics and learning.",
            "Features and you can make the link very explicit by saying whatever metric you learn.",
            "I'm going to call that a feature.",
            "And I'm just going to measure distance using Euclidean distance.",
            "After that feature transformation.",
            "So you can ask the question for any features that you extract.",
            "What would happen if we just went into the feature space and then used um Euclidean spherical distance?",
            "Or you can say for any metric, what is the implicit feature space such that measuring things with this metric is equivalent and that there isn't always an answer to that second question.",
            "OK.",
            "So let's start off with the simplest possible thing you might think of doing."
        ],
        [
            "OK, let's take K. Nearest neighbor classification is your goal here and let's ask what's the right distance metric for K nearest neighbor classification.",
            "And of course the answer is the one that optimizes the test error.",
            "OK, that's not very useful, but let's try and approximate the metric which optimizes the test error as the one which optimizes the training error defined using leave one out cross validation.",
            "So for each point in the data set we pretend we don't know his label.",
            "We apply K nearest neighbor.",
            "We vote the neighbors and we see does the majority vote of those neighbors agree with the label or not?",
            "Leave one out cross validation.",
            "So if I gave you a finite set of distance metrics metric one metric to metric three and I asked you which do you prefer.",
            "My philosophy here is that you should measure the cross validation performance of metric one.",
            "The cross validation performance of metric to the cross validation performance of metric three, whichever one is lower.",
            "That's the one you prefer.",
            "OK.",
            "So whichever one has the lower error estimate by cross validation is going to be the preferred metric, and that will work for a discrete set.",
            "But what if I gave you a continuous parameter, continuously parameterized family of metrics?",
            "Can you find the one which maximizes this performance and what are we going to be OK?",
            "Are we going to get cake?",
            "So here is the problem.",
            "The problem is that cross."
        ],
        [
            "Allocation performance is very hard to optimize.",
            "Why?",
            "Because leave one out error literal.",
            "Leave one out, errors it completely discontinuous function of the distance metric.",
            "Imagine you have your neighborhood graph.",
            "So each node, each training point is connected to its K nearest neighbors.",
            "And now I adjust the way that I measure distance a little bit.",
            "So I change the distance metric slightly.",
            "Maybe the neighborhood graph doesn't change at all.",
            "OK, so then, if the neighborhood graph doesn't change, what will the leave one out performance do?",
            "It will stay the same.",
            "So now imagine graphing the leave one out performance as a function of some parameter of this distance metric.",
            "It's constant and making a small change it's constant.",
            "And then what's going to happen.",
            "Of course I'll make a small change an you know instead of David being a neighbor of me, someone else becomes my neighbor and then the distance the leave one out performance is going to jump up a finite amount.",
            "So this function leave one out cross validation performance as a function of the distance metric.",
            "Is basically flat and then it has these discontinuous jumps, so that's a very unpleasant function to try and optimize.",
            "So we need some kind of smoother function, or at least the continuous cost function to attack.",
            "So this is the basic trick mode."
        ],
        [
            "If you have probably seen this trick by now, a lot of us have been recycling this trick over and over again, but it's a good trick, so it's worth recycling.",
            "Here's the idea.",
            "Instead of thinking of a fixed number of K nearest neighbor and then voting those, we're going to consider randomize neighbor selection.",
            "OK, so imagine that the way we're going to do classification is, each point would randomly pick one of the other points as his neighbor, with the probability that depended on the distance.",
            "So if the point is closer, you have higher probability of picking it.",
            "If it's very far away, you have very low probability of picking it, and now we're going to ask what's the expected performance of this randomized nearest neighbor strategy.",
            "If we averaged over random choice of knickers.",
            "So here's the setup.",
            "There is this quantity PJ, which is the probability that point I.",
            "Select Point J as his neighbor, and that's just the exponential of the distance.",
            "Maybe you would have preferred it if I put a square here.",
            "If you would prefer that, then just add a square for all these for the rest of the talk.",
            "And normalized, so this distribution sums to one for every I as a function of J.",
            "It sums to one, and there's no probability of picking yourself 'cause that would be cheating.",
            "It wouldn't believe one out.",
            "It would be leaving nothing out, OK?",
            "So what's the fraction of the time at this point I will be correctly classified under this randomized rule.",
            "Well, it's just the probability of all the points J in the same class as I.",
            "Because if I randomly choose a point that has the same class as me, I'm going to get it right.",
            "And if I randomly choose a point that has a different class, I'm going to get it wrong.",
            "So I just add up the mass in this distribution, PJ summing over all of my friends J who are in the same class.",
            "That total probability is the chance the average fraction of the time that this randomized neighbor rule would have got point I correct under this.",
            "Leave one out metric.",
            "So you probably see where we're going with."
        ],
        [
            "This is now the expected leave one out classification performance, so I'm just going to average or sum over all the training points.",
            "OK, So what did I do?",
            "I just converted leave one out.",
            "Which is the simplest thing you could imagine into a sort of differentiable version of leave one out by introducing this sort of stochastic neighbor selection.",
            "And now I'm going to take the derivative of the differentiable leave one out.",
            "That's the idea.",
            "So this is the objective function that we're going to try and optimize, and it's much smoother.",
            "Of course, with respect to the distances than the actual leave one out error.",
            "And notice that there's no explicit parameter K, so I'll talk more about this.",
            "But in this randomized rule, you don't have to pick K, you just ask the expected fraction of the time that you would get the right class.",
            "So K is sort of implicitly buried in the scale of these distances, and we're going to effectively learn that scale at the same time question.",
            "CSUB I is the class of point I so JNC SBY is all the points J in the same class as I.",
            "Really bad.",
            "Distance is very large.",
            "Yeah, sorry, I didn't mean that there is a distance for which this is exactly K nearest neighbor.",
            "Of course it's soft weighting of the neighbors, but just the size of the neighborhood informally is controlled by the scale of D. If D is measured in very large units.",
            "Then the size of the neighborhood is essentially equals one, and if D is measured in very small units, then you're averaging overlap, but it will never be, of course equivalent to K nearest neighbor.",
            "OK, so now the idea is to learn the metric by adjusting these these to maximize this expected."
        ],
        [
            "Formants and having done the simplest thing I could possibly do for the objective function, I'm now going to do the simplest thing I could possibly do for the model.",
            "So the simplest thing you could possibly do is just measure the distance between two points XI and XJ by putting a positive semidefinite matrix in here.",
            "OK, so just Gaussian or quadratic metric.",
            "So Q is some symmetric positive semidefinite matrix, and we're going to rewrite this distance using the square root of Q.",
            "So I just replaced Q with a transpose A.",
            "There's many ways to do this that aren't unique, but any square root of Q will make the point here.",
            "What is the distance turned out to be?",
            "The distance turns out to just be Euclidean distance in some transformed space.",
            "So what you can imagine is that I just found a transformation A. Afterwhich the right distance metric to use is just Euclidean distance.",
            "So you can summarize everything I told you so far by the following.",
            "Find me a linear transformation of the original data so that when I look in that linear transformation and I do dumb Euclidean K nearest neighbor things will workout well.",
            "As estimated by leave one out cross validation or a soft version of it.",
            "OK, any questions everybody with me Pascal.",
            "The previous yeah this in regression this would be very close to parzen window.",
            "With the exception that this normalization is sort of per point.",
            "Right now.",
            "Yeah, yeah, yeah, exactly exactly."
        ],
        [
            "So you can think of this as sort of stealing the bandwidth trick from the density estimation and trying to make it work for classification.",
            "OK, so this is probably.",
            "Too many equations on one slide, but I just want to make a point here because Jan touched on and I think it's very very crucial if you just do what I told you you're going to end up with an algorithm that's quadratic in the number of data points, and that's probably going to make you unhappy.",
            "So I'm going to try and tell you a little bit about how to approximate or sample to get around that, so here it is.",
            "This is the expected classification performance, so this is the average overall data points of the chance of getting that data point correct in the leave one out scheme.",
            "OK, so if we take the derivative of this with respect to the distances where the distances are defined like this, then what we're really taking the derivative with respect to.",
            "Is this transformation a, so we're adjusting the linear transformation so that.",
            "Effective performance is fine Now if you write down this expression for this gradient, the exact expression, it's sort of quadratic.",
            "You have to sum over all the data points because each data point contributes a gradient term, and in order to compute the gradient term of that data point exactly, you need to consider all the other data points first.",
            "The ones in your class, and then the ones not in your class.",
            "So it would be quadratic if you did it.",
            "Now there's two observations here.",
            "One, this is just gradient descent.",
            "So we don't really care about the exact gradient, right?",
            "It's not a holy artifact, it's just the gradient.",
            "We're just going to take it and adjust our parameters and drop it on the floor so we don't really need the gradient to be right.",
            "We just need it to be sort of right.",
            "In fact, we don't even need to be sort of right, we just need it to be not 90 degrees wrong.",
            "OK, so just subsample here, just grab a few of your data points, use them to estimate the gradient nudger parameters, and start again.",
            "That will be fine.",
            "I won't tell.",
            "OK, stochastic gradient optimization perfectly legitimate.",
            "OK, the other observation is that in these interior sums you're waiting.",
            "The other data points so.",
            "Oh, it appeared back here on the screen.",
            "OK, good.",
            "I guess there's a penalty for spending too long on a slide.",
            "So you're waiting the other data points and they're waiting them by the neighborhood selection probability.",
            "And most of these probabilities are almost zero, right?",
            "I have my little neighborhood, my friends who are around me, and the probability of picking someone all the way over there is basically 0, so you don't need to go through all these painful calculations.",
            "You can keep an active list of who's really in the effective neighborhood domain, and then update that active list every few 100 iterations when you retouch the parameters.",
            "So there's some tricks for approximation here.",
            "I don't want to get into them, but.",
            "If you just write down this equation, it looks pretty nasty.",
            "OK."
        ],
        [
            "So this is the algorithm, the first one that I wanted to tell you about.",
            "It's called Neighborhood Components analysis, so it was in nips a couple of years ago and it learns a linear transformation of the input space after which nearest neighbor performs well.",
            "So the transformation intuitively scales up directions which are useful for discrimination and tries to project out dimensions which are not informative about the class identity.",
            "And the algorithm is very simple.",
            "Write down this objective function.",
            "And optimize the heck out of it.",
            "OK.",
            "So and then you learn, you take this a that you learned and you project your training set into that knew feature space.",
            "So for every training point XI, you create a new training point.",
            "Why I?",
            "By multiplying by this transformation, and then you put the wise as the training data for your old K nearest neighbor code.",
            "And at Test time when you get a new test point, you also multiply it by A to get it into the right test space, and then you present that.",
            "Why test to your code so you don't really even have to write any new code for test time?",
            "OK, so if you have KD trees and fancy data structures LSH, that does.",
            "You know retrieval for the neighbors and stuff.",
            "You can use all that.",
            "This is just a preprocessing step that warps the inputs so that Euclidean distance is not totally stupid.",
            "OK, So what about Ki?",
            "Keep sweeping K under the rug here?",
            "Or actually it somehow vanished and that seems like unfair.",
            "So the scale of transformation A is also learned.",
            "That's the crucial thing in the op."
        ],
        [
            "To misers to space, there is a direction in which it can scale up all the components of a uniformly, or scale them down uniformly, which means that the optimizer can effectively change the unit system for measuring D. And that means that you're effectively also learning a real valued estimate of the optimal neighbor size.",
            "So if you want to make the neighborhood size effectively smaller, the optimizer can make D bigger by just scaling up A and if it wants to make the neighborhood size bigger, it can make D smaller by scaling down a.",
            "Now, of course it's the same neighborhood size everywhere.",
            "Ideally, we'd like to have a different sort of neighborhood in different parts of the space, and we can sort of effectively estimate the effective K in different parts of the space by looking at these average perplexities.",
            "But we usually just when we do it, we learn this transformation.",
            "Whatever scale it has, that's fine.",
            "And then on top of that we do.",
            "We go back to doing K nearest neighbor and we set K by cross validation by straight up cross validation."
        ],
        [
            "OK, so the real win as far as I'm concerned with this method is not just that it learns a good distance metric, it's that you can just take everything.",
            "I told you an replace the Matrix a which you were probably thinking of as a square matrix by a really really skinny rectangular matrix, and that's equivalent to making the metric that you're learning low rank.",
            "OK, so if a transpose A is the metric, an A is a skinny matrix like this, then the metric will be low rank.",
            "You'll be essentially projecting your high dimensional original inputs into a low dimensional space in which nearest neighbor is expected to do well.",
            "So that's really I think a huge benefit, so you can seriously reduce the storage and computation requirements, and if you really want to go down to two or three dimensions, then you can do essentially sort of linear visualization.",
            "I'll show you a picture of that.",
            "In the second guy.",
            "The objective function is it, yeah?",
            "OK so guys question is is the objective function convex and the answer is definitely not.",
            "There's lots of local minima.",
            "And it's also not identifiable because you can apply a unitary transformation too.",
            "To a in the square case.",
            "Or you can rotate the low dimensional space so but.",
            "I don't even know what identifiability really means.",
            "I mean, so identifiability is supposed to mean that in the hypothetical world which never exists in which our model is exactly right and we had infinite data which we never do, then the parameters that we estimate would go to the real parameters.",
            "OK.",
            "I'm happy to admit that that's not the case here.",
            "But I mean, you shouldn't worry about it, because all you're really learning is the metric right?",
            "All you care about is the outer product of a.",
            "So if you learn in A and I learn in a, it doesn't really matter if they are not the same.",
            "That's absolutely true.",
            "So there is a big issue of how you regularize.",
            "I'm being a bit glib, but there is an issue of Regularising A and I'll talk maybe a little bit about that in the square case, you could regularize a by making it diagonal.",
            "You could say I just want to learn scale for each dimension, but no interactions between the dimensions.",
            "That's one thing you can do.",
            "You can also just regularize the Frobenius norm of A and just put that in the gradient, so that keeps.",
            "Post a definite plus.",
            "Yeah, we haven't tried that.",
            "A factor analysis kind of covariance.",
            "Yeah, we haven't tried that, but you could do it.",
            "You would then have to do the gradient descent becomes a little bit more complicated 'cause you have to sort of solve the credit assignment problem.",
            "OK, so let me show you.",
            "Yeah.",
            "Can you use a?",
            "Yes, yeah.",
            "So the question is instead of a linear."
        ],
        [
            "Transformation, could you use a differentiable nonlinear transformation absolutely and in the paper the original NIPS paper for this we show some results at the end using some neural networks to identify, you know, pose of faces and stuff, which is a very nonlinear function.",
            "So definitely definitely OK.",
            "So just some illustrative results here and then I'll move on to the second, the second topic.",
            "So here's some data synthetic data.",
            "Two of the dimensions contain these sort of concentric rings, which you can see here an all the other dimensions contain noise, sort of Gaussian noise with the variants drawn from some gamma.",
            "Distribution, and I think that there's 498 noise dimensions and two real dimensions in this problem.",
            "And then what we did is we just gave the algorithm the labels so it knows the labels of the points and we said find you 2 dimensional projection of the data where nearest neighbor would be expected to do well.",
            "So of course it's just a setup, it's perfectly suited to our algorithm, but just to show you if you do the neighborhood components analysis you get recovered the rings as you should.",
            "So that's just a sanity check.",
            "If you do Fishers discriminants, you get this kind of jumble, and that's because it's LDA.",
            "Fisher's discriminant makes the assumption that each class is distributed according to a Gaussian.",
            "And then it tries to assume they all have the same covariance and then that gives you a linear discriminants.",
            "After you solve an eigenvalue problem.",
            "So that's really a bad assumption here that every class is a Gaussian with the same variance.",
            "That's a terrible assumption.",
            "And then if you do PCA where you ignore the labels and you just find the principle directions of directions of most variants, then of course you also get a jumble because there's lots of high variance noise directions and all PCA cares about is noise and it just goes variants that just goes and finds those directions.",
            "Maybe a little bit, I'll just skip over that a little bit better exam."
        ],
        [
            "Apple is some simple face classification task, so these are sort of raw data.",
            "Is raw images taken off a really low res camera and there's 18 people in the data set whose identity or the class labels?",
            "And so again here this is Fishers discriminants.",
            "This is principle components and this is what you get if you do neighborhood components analysis.",
            "So this is just literally a 560 by two matrix.",
            "In which I multiply each of the raw frames by that matrix.",
            "That gives Me 2 dimensions and I just plot the point here, and then I colored them according to their labels.",
            "So there is a linear projection which does a very good job at separating things here.",
            "And you can convince yourself that this also helps by measuring the test error.",
            "So this plot shows you a mixture of training and test points, but you might be worried that it's overfitting and at least in this case the errors show you that it's really not.",
            "OK."
        ],
        [
            "So I want to move on to a sort of related objective function that Amir Globerson had the idea of optimizing which I think is very interesting in its own right.",
            "So let's look at the original NCA objective, and I'm just going to put a log in front of it, which shouldn't disturb anyone.",
            "OK, so the original NCA objective was this, it was the expected number of correct classifications that you would get in your data set under this randomized rule.",
            "So maximizing that is the same as maximizing the log.",
            "And here I just wrote out.",
            "I expanded this probability of getting data point I correctly in terms of the probability of landing on any of the other data points J.",
            "So there's a couple of things you could do.",
            "One thing is, you could take this log and you could push it inside here.",
            "What does that do?",
            "That is, the chance of labeling the data set exactly right.",
            "So you might be a bit worried that this objective function can make confident errors, and that's true, right?",
            "If there's one data point that's really irritating, it doesn't matter.",
            "It just gets that data point wrong.",
            "It's only one out of any of the data points, right?",
            "So who cares?",
            "It just puts it in the deep zone of class labels, which have nothing to do with it, and it gets it wrong, and it just suffers the loss.",
            "And it doesn't matter.",
            "But if you have a domain where you think your data is very noise free your training data and you really don't want your classifier confidently giving you wrong answers at Test time.",
            "Then this might concern you, so then you can put the log inside, in which case you're not maximizing the expected number of correct labels.",
            "You're maximizing the chance of getting every label exactly right.",
            "And then what Amir suggested is, you know why?",
            "Stop there.",
            "Let's push the log inside one more time.",
            "OK, so then this is the objective function.",
            "And this objective function has a very interesting geometrical interpretation."
        ],
        [
            "So here's the geometrical interpretation.",
            "It's collapsing classes, so this objective function is only good if each class is just one unimodal distribution.",
            "It doesn't have to be Gaussian, but it's some kind of unimodal blob, and in that case, what would a good metric do?",
            "Well, a good metric would be one under which all the points of the same class got squished together in infinitely close, and all the other ones got spread infinitely far away.",
            "OK.",
            "So a perfect metric would collapse, say for the green class, it would collapse all the green points together and send all the other points very very far away.",
            "Assuming each class is just a single blob.",
            "OK, so in order to kind of get from that geometric intuition to that objective function that I showed you, here's what Amir and I did.",
            "We said well."
        ],
        [
            "Let's call this distribution the ideal distribution over the neighbors possible neighbors J for each class.",
            "For each data point, I so the ideal distribution over neighbors for data point I is some constant value if the class of I in the class of J are equal.",
            "So you put some constant mass on all of the friends who are in the same class as you and you put zero probability of landing on anyone who isn't in your class.",
            "Well, that would be very nice, right?",
            "If we could get this distribution then our Leave leave one out estimate would be perfect.",
            "We would estimate that we would be getting everything labeled perfect.",
            "So here's the idea.",
            "We're going to find the metric or the transformation by minimizing the average KL divergences from this ideal distribution to the actual distribution induced by a.",
            "So each metric that we pick induces some distribution over neighbors.",
            "And there's the ideal distribution, which puts constant probability on the correct class and zero probability ever else.",
            "And we just measure the KL divergent's.",
            "So for each data point I I ask what distribution over other data points J is being induced by the metric.",
            "That's this guy I said.",
            "What would you love to have as your distribution?",
            "That's this guy and I just penalize the KL distribution.",
            "I average or sum over data points and we just minimize that.",
            "OK, so if you massage this function you would see that it comes back to that original NCA thing, But the good news here and this relates to what Guy was talking about.",
            "This objective function is convex in the metric.",
            "There's a single optimal metric which solves this problem, and so Amir and I called it the maximum."
        ],
        [
            "The collapsing metric.",
            "So MCM.",
            "The maximally collapsing metric and the what it does is it pulls all of the data points in a single class as close together as possible and pushes all the data points in the other class as far away as possible.",
            "So you can find it by various methods.",
            "As far as I know, this particular optimization problem doesn't have a nice name.",
            "It's not like SCP or QP or anything like that, but it is convex.",
            "You can sort of use the log sum, Exp kind of trick to show that it's a convex function of the distance is there, and it's a linear function of the distances here, so that's that's all fine for the experiments all show.",
            "What we did is we just use gradient descent.",
            "Followed by projection.",
            "So we just did gradient descent in the full metric and then we projected back onto the semidefinite cone and then another gradient step.",
            "So, but certainly that's not the best way to optimize this, I don't think so, OK?",
            "So, um."
        ],
        [
            "This a few speculations here about the relationship between maximally collapsing metrics and Fisher's discriminant.",
            "So MCM is similar to Fishers discriminate in that it tries to sort of minimize within class distances, variances and maximize between class differences.",
            "But Fishers discriminate is a purely 2nd order method.",
            "The sort of sufficient statistics for Fisher's discriminant are just the you know, mean and covariance of each of each class.",
            "So I. MCM, the maximum collapsing metric is sort of a generalization of LDA that makes only the much weaker assumption that each class is unimodal, but it doesn't assume that there Gaussian or that all the Gaussians have equal variance.",
            "It just assumes that it can be separated from the other class blobs.",
            "So that's a kind of nice generalization, but maximum collapsing metric like linear discriminate analysis is going to fail.",
            "For example, on this concentric rings, because the assumption that each class is a unimodal blob.",
            "Is not really valid here.",
            "Each class is sort of a ring here, so that's not.",
            "We wouldn't expect it to work.",
            "You might imagine some kind of mixture model trick for getting around this, but we haven't really explored that."
        ],
        [
            "So now guys question becomes really relevant.",
            "What if you want to learn a low rank metric that's maximally collapsing?",
            "Well, unfortunately this is one of the great tragedies of modern optimization.",
            "The set of low rank matrices is not convex, right?",
            "Take the outer product of two vectors and the outer product of two other vectors and average them.",
            "You don't have a rank one matrix anymore, so that's bad news.",
            "So there's sort of two ways to proceed.",
            "One way to proceed is what was suggested this morning.",
            "You find the optimal full rank metric and there should be a sort of star here that says you know.",
            "If you can actually succeed in performing the computation required to do that, and then you keep only it's K largest eigenvalues, you just, you know, zero out the other eigenvalues.",
            "An that procedure is well defined, it has no local minima because the first step is convex and the next step SVD is also convex, and so it will be close to optimal if the full rank metric has a rapidly decaying eigen spectrum.",
            "And the other thing you can do is explicitly parameterized.",
            "The set of low rank matrices using say the Lu decomposition of the metric and then optimize the KL objective just using gradient or local search like we did before.",
            "And that's really your only option when you have a big data set running around.",
            "So just show you a few few graphs of results here so."
        ],
        [
            "These are just sort of random toy datasets that we downloaded off of UCI, like like you would imagine for NIPS paper.",
            "And then this horizontal axis shows the rank the effective rank of the metric and the vertical axis shows the shows the error.",
            "The blue is the maximally collapsing metric.",
            "The red triangles are linear discriminant analysis in some of the examples, we compared to Eric Jing, atolls metric learning and to some white and PCA, which doesn't even use the labels.",
            "But generally MTM is doing.",
            "Is doing pretty well.",
            "And what's really interesting is not where these methods rank relative to each other.",
            "What's really interesting is the fact that you know, as you decrease the dimension, it usually takes a long while before your error rate takes off.",
            "So the effective dimensionality of feature spaces you need to keep around is much lower than the ambient dimension.",
            "All the action is happening in a low dimensional space, and so that's really what Jan was saying this morning that there's, you know, if you really can't afford to learn these features as part of the whole classification thing, you don't need very many features.",
            "To get the job done so he showed you these, you know eight.",
            "You know weird rotation things for airplanes where if you just multiply the image of the airplane by that that low dimensional space tells you everything you need to know.",
            "OK.",
            "So just let me finish off here in the absence of straw."
        ],
        [
            "On prior knowledge about how to pick your classification distance metric, I think learning the metric seems like a good idea.",
            "I think you know just hoping that everything is going to be fine with K nearest neighbor or normalizing your data arbitrarily and then hoping everything will be fine is just not the way to go.",
            "We have tons of data that data should be able to tell us the metric.",
            "So learning low rank metrics gives you a natural way to reduce memory and computation while still doing very well on classification.",
            "So I told you about these two algorithms, neighborhood components analysis, so NCA assumes nothing about the form of the class distributions.",
            "It doesn't assume that they are unimodal or Gaussian or connected.",
            "Even an it doesn't assume anything about the shape of the separating surface that it's linear or linear in feature space.",
            "It's purely non parametric method.",
            "And then maximally collapsing metrics.",
            "If you know we're really believe that each class is unimodal, they're nice because it's a convex optimization problem.",
            "So that's that's the attraction.",
            "Either way, I think it's very surprising how far you can go with just quadratic mappings.",
            "Quadratic metrics which are just linear mappings of your features.",
            "So why linear?",
            "Well, linear mappings are hard.",
            "Maybe I should have put harder to overfit, you can overfit them of course, but they're harder to overfit then very complicated nonlinear things.",
            "So if you're lazy and you don't want to spend a lot of time regularising things, then that's one way to start off.",
            "They're very compact to represent their very fast to implement the test time, and it turns out that there are extremely good linear mappings out there for a lot of problems.",
            "I think we now we're starting to get a handle on how to find them so.",
            "OK, so that's it.",
            "I'd be happy to take questions.",
            "Just before the questions, I don't see Jacob Goldberger, but I want to say that Russ is standing over by the door, so he's the student who did a lot of the heavy lifting on the original paper there.",
            "So Tommy.",
            "Why not question?",
            "OK. Press 2 object expecting shower.",
            "Easily intern EM like algorithms for at least at this Contacts.",
            "Why not, why not?",
            "Regularise with Tracy.",
            "Blank.",
            "Right?",
            "So let me answer the second question first.",
            "Palmese question.",
            "The second question was, well, why don't we use that race as a surrogate for the rank and then try and put that in?",
            "And by penalising that race would hope that we would encourage you know metrics which don't have very many big eigenvalues and would be very close if not exactly lowering.",
            "So I think that.",
            "Oh the the trace.",
            "You want to penalize.",
            "So you want to penalize the sum of the eigenvalues of the metric, yes?",
            "Right, yeah, yeah, yeah.",
            "So I think that's a great idea and that's pretty easy to put in in the gradient formulation 'cause you just take the gradient directly.",
            "So I think that's a really good idea.",
            "The to go back to the first question, so I think what Tommy is suggesting is to say, well, you can think of there being a hidden variable as which was the other data point that you picked right?",
            "And we could just estimate that hidden variable.",
            "Is that what you're suggesting and then?",
            "Right?",
            "But the problem with that is that if you really want to keep around for each data point to distribution over other data points, then I think it's going to be sort of tricky.",
            "You'll have to do a lot of approximation for that.",
            "Because otherwise you'll have to have.",
            "You have to have N squared matrix in memory, which is you know single stochastic matrix at the step.",
            "Solution.",
            "One step of that year Makram, are you fix the Easter by starting from all zero A that gives U uniform over the correct class I see.",
            "I see.",
            "Right, yes, I guess that's what we're doing so then.",
            "So then that seems like maybe it's a good idea.",
            "OK, so then the answer is.",
            "Because we never really thought of it I guess.",
            "Or maybe if we had thought of it, I would have misunderstood it in the way that I did when you first explained it so.",
            "But that's a good idea.",
            "I hear a mirror is really busy with his new post Doc, but maybe I'll see if he wants to waste some time on here is now doing a postdoc with Tommy.",
            "For those of you who missed his amazing Taco was amazing talk at NIPS.",
            "By the way, OK, other questions.",
            "Comments from a guy that changes actually uses it.",
            "The good news is that this thing, I think, is the best thing you can buy for dimensionality reduction for classification.",
            "And the best thing is that if you do, we just much larger then.",
            "104 two plus problems.",
            "It will it rain and the global trend very significantly.",
            "And The funny thing is that it doesn't help to add more data.",
            "So hang on, you're saying it overfits when the metric is low rank or when it isn't.",
            "High dimensionality, like several hundred to two relatively normal dimensionality, like 30 or 40, not to one or two, and then our whole lot of complicated reasons why I think.",
            "So it does happen.",
            "So you do need regularization here.",
            "So if you want to try this something, some real project you will have to think about regularization and.",
            "Approximate gradient.",
            "Are you alright that we don't give exact gradient but the other thing is that you need more than the closer you are to the minimum, the more accurate gradients in it.",
            "So it might make sense, right?",
            "Right, right?",
            "So that's certainly true.",
            "On the other hand, there's nothing to stop you from, just, you know, running this for awhile and then taking whatever transformation you have and using that one.",
            "I mean, you don't need to get to the minimum, right?",
            "If you imagine that you started off with the identity matrix, OK, you were in a terrible situation originally, so you're going to be in a much better situation.",
            "So one way to think about it is if you run it for X hours, how much does it help rather than how long does it take to get to the minimum?",
            "But yeah, those are both good comments.",
            "Graph.",
            "The ones for MCM.",
            "For.",
            "The original nearest.",
            "No.",
            "We don't.",
            "We don't have the original nearest neighbor performance.",
            "But it's it's generally, it's generally.",
            "Right, so there's generally two kinds of problems.",
            "There's problems in which.",
            "The original in the original problem, nearest neighbor works pretty well.",
            "And in those kinds of problems, then you would expect it at the high dimensions it would be fine.",
            "But of course, in order to make this you would only be able to make that plot on that point on the extreme right hand side of the plot, so.",
            "So yeah, you could say that and that we should add.",
            "But the to me what's interesting is that you can get down to here.",
            "Oh yeah, well in some of them like we can go back to this.",
            "What is PCA?",
            "Oh, the PCA curve is if you do PCA and then you divide each of the principle directions by the eigenvalue.",
            "So White and PCA.",
            "So like you're basically whitening the data and then projecting out some dimensions.",
            "Yeah, but oops, I'm going the wrong way here, I think.",
            "But let me just show you.",
            "Like for example, in the NCA case.",
            "If you just use K nearest neighbor in the original space, you get 18% errors.",
            "If you normalize each dimension to have variance one then you get actually slightly worse and then if you do NCL you get 4.",
            "So that's a.",
            "In that example, it tells you.",
            "One more.",
            "Yes, Tim.",
            "Normalize that you align the eyes along the faces at all.",
            "Or no, we just took the raw data right off the now.",
            "Of course this is a very easy problem.",
            "I'm not claiming that this is, you know, any reasonable tough classifier would go eat this for breakfast, but my point is just if for whatever reason you were going to do K nearest neighbor, you would be.",
            "In really bad state, even if you pick K bye leave one out so.",
            "Got it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everyone will be able to hear and people will be drifting in.",
                    "label": 0
                },
                {
                    "sent": "Thank you all for interrupting your skiing to come.",
                    "label": 0
                },
                {
                    "sent": "So just a quick show of hands.",
                    "label": 0
                },
                {
                    "sent": "How many people have heard either Jacob or myself talk about neighborhood components analysis before.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe about half so I'll try and go a little bit quickly through that and then get to some of the newer work with with the mirror.",
                    "label": 0
                },
                {
                    "sent": "So feel free to interrupt me at anytime and ask questions or make insulting comments or whatever.",
                    "label": 0
                },
                {
                    "sent": "Start an argument.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you all know, the sort of theme for today is distance metric learning, and the idea is that there's a lot of times in the core of an algorithm when we need.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm kind of distance measure.",
                    "label": 1
                },
                {
                    "sent": "Often we needed to have the properties of a metric, but maybe not always, which compares examples, so it's a scalar function which eats 2 examples and tells you how different or similar they are an unless the structure strongly specifies this metric.",
                    "label": 0
                },
                {
                    "sent": "Or maybe some constraints on that metric, then it seems like the preferred approach is to learn the metric as part of your problem.",
                    "label": 1
                },
                {
                    "sent": "So in other words, you have a lot of data off and the data has labels which implicitly tell you what the right metric is and you want to perform the learning along with the rest of the parameters based on the training set.",
                    "label": 1
                },
                {
                    "sent": "So today I'm going to focus on something very very simple.",
                    "label": 0
                },
                {
                    "sent": "Semi parametric classifiers like K nearest neighbor or Gaussian kernel support vector machines, Gaussian processes, you know very local machines.",
                    "label": 0
                },
                {
                    "sent": "And the idea is, given a labeled set, so we have the training set of inputs, so I'll use X for inputs and labels C or Y.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I'll use.",
                    "label": 0
                },
                {
                    "sent": "How should we learn a metric that will give good generalization when used in such a classifier?",
                    "label": 1
                },
                {
                    "sent": "So what's the idea of these classifiers?",
                    "label": 0
                },
                {
                    "sent": "They just look locally in the neighborhood of a test example at nearby training examples, and they sort of average their labels or their regression outputs or whatever.",
                    "label": 0
                },
                {
                    "sent": "And the idea is to make this measurement of what does local mean?",
                    "label": 0
                },
                {
                    "sent": "Good for whatever you're trying to do.",
                    "label": 0
                },
                {
                    "sent": "Classification or prediction.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, you should think that our goal is to make directions of the space which are irrelevant for classification.",
                    "label": 0
                },
                {
                    "sent": "We want to shrink them and directions of the space which are very informative.",
                    "label": 0
                },
                {
                    "sent": "We want to stretch them out.",
                    "label": 0
                },
                {
                    "sent": "That's that's the job of the metric.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there's this sort of at this point in my talk.",
                    "label": 0
                },
                {
                    "sent": "Usually someone puts up their hand and says, but your whole line of research is totally stupid because localist classifiers are ridiculous.",
                    "label": 0
                },
                {
                    "sent": "They need an exponential amount of training data to just memorize the space, and they don't generalize in any meaningful way.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's actually a somewhat valid criticism, but it's not a criticism I'm going to get into.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A battle about today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of defer the criticism to this old paper by Rob Holt, which I think is still somewhat true today, which is that there are a lot of situations in which very simple classifiers tend to do surprisingly well, so you should think of this as a conditional talk if you're interested in some kind of a local method like nearest neighbor or Gaussian kernel support, vector machine or whatever, then I have something interesting to tell you about learning the distance if you're not interested, there's a wireless connection in this room.",
                    "label": 0
                },
                {
                    "sent": "And I'll be done in 45 minutes.",
                    "label": 0
                },
                {
                    "sent": "So, OK.",
                    "label": 0
                },
                {
                    "sent": "So just to remind you all the set up, for instance based or memory.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based or nonparametric classifiers.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you just have the training set an you take say the K nearest neighbors of the training set.",
                    "label": 0
                },
                {
                    "sent": "You take their majority vote and that's our class label.",
                    "label": 0
                },
                {
                    "sent": "So people like to make fun of K nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "But it's pretty interesting the decision services are nonlinear.",
                    "label": 1
                },
                {
                    "sent": "It's semi parametric or nonparametric so it has high capacity without having really to train a lot of parameters.",
                    "label": 1
                },
                {
                    "sent": "So what is this point?",
                    "label": 0
                },
                {
                    "sent": "I think this is a very important point that's under appreciated.",
                    "label": 0
                },
                {
                    "sent": "If you want your classifier to do something complicated.",
                    "label": 0
                },
                {
                    "sent": "It's in some sense it has to have a lot of bits inside its brain.",
                    "label": 0
                },
                {
                    "sent": "It has to have somewhere it has to have reasonable capacity, but we're not very good at setting huge numbers of parameters.",
                    "label": 0
                },
                {
                    "sent": "So what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "We're going to say, well, we'll make the training set itself in some sense, store all those bits 'cause we're pretty sure we didn't screw up those bits, and then we're only going to set, you know, one number K or one bandwidth parameter Alpha and the quality of the prediction.",
                    "label": 0
                },
                {
                    "sent": "Sort of automatically improves with more data.",
                    "label": 1
                },
                {
                    "sent": "There are these results about it being asymptotically close to optimal.",
                    "label": 0
                },
                {
                    "sent": "Which you may or may not be interested in because of the word asymptotic, but some people think that this word is just the gateway to lots of very interesting theoretical results, and some people think that this word means you should ignore everything that comes after it.",
                    "label": 0
                },
                {
                    "sent": "OK, there's only a few parameters to tune.",
                    "label": 1
                },
                {
                    "sent": "Usually you can do it by cross validation or some kind of simple optimization gradient on.",
                    "label": 0
                },
                {
                    "sent": "Leave one out of something.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the problem?",
                    "label": 0
                },
                {
                    "sent": "The problem is what does nearest mean?",
                    "label": 0
                },
                {
                    "sent": "You need to specify some kind of distance metric on the input, so that's one problem with these classifiers.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is computational.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First you have to sort of store your whole training set and carry it around in your suitcase with you every time you want to do classification.",
                    "label": 1
                },
                {
                    "sent": "So we'd like to sort of try and solve both of these problems.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to talk mostly about the first problem.",
                    "label": 1
                },
                {
                    "sent": "How do you learn a distance metric?",
                    "label": 0
                },
                {
                    "sent": "And then I'll sort of show you how if you need to, you can make this distance metric low rank, which corresponds to measuring everything in an implicit feature space, which is low dimensional and that gives you the opportunity to take your training data and project it into that low dimensional space.",
                    "label": 1
                },
                {
                    "sent": "And then have lower computational overhead, both in terms of memory footprint and time to search, so that's the that's the basic plan.",
                    "label": 0
                },
                {
                    "sent": "So, as Jan mentioned this morning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's been touched on in many of the talks so far.",
                    "label": 0
                },
                {
                    "sent": "In the workshop, you can either think of our goal as learning a metric.",
                    "label": 1
                },
                {
                    "sent": "So in a second I'm going to sort of introduce the class of quadratic metrics, but for the moment, imagine that we're going to measure things by Gaussian type of metric Mahalanobis distance.",
                    "label": 1
                },
                {
                    "sent": "You can either think of the goal is learning that metric, or as learning the square root of that metric, which is a transformation or projection or feature set.",
                    "label": 0
                },
                {
                    "sent": "So there's a very strong link, obviously between learning metrics and learning.",
                    "label": 0
                },
                {
                    "sent": "Features and you can make the link very explicit by saying whatever metric you learn.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call that a feature.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to measure distance using Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "After that feature transformation.",
                    "label": 0
                },
                {
                    "sent": "So you can ask the question for any features that you extract.",
                    "label": 0
                },
                {
                    "sent": "What would happen if we just went into the feature space and then used um Euclidean spherical distance?",
                    "label": 0
                },
                {
                    "sent": "Or you can say for any metric, what is the implicit feature space such that measuring things with this metric is equivalent and that there isn't always an answer to that second question.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's start off with the simplest possible thing you might think of doing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's take K. Nearest neighbor classification is your goal here and let's ask what's the right distance metric for K nearest neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "And of course the answer is the one that optimizes the test error.",
                    "label": 1
                },
                {
                    "sent": "OK, that's not very useful, but let's try and approximate the metric which optimizes the test error as the one which optimizes the training error defined using leave one out cross validation.",
                    "label": 1
                },
                {
                    "sent": "So for each point in the data set we pretend we don't know his label.",
                    "label": 0
                },
                {
                    "sent": "We apply K nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "We vote the neighbors and we see does the majority vote of those neighbors agree with the label or not?",
                    "label": 0
                },
                {
                    "sent": "Leave one out cross validation.",
                    "label": 0
                },
                {
                    "sent": "So if I gave you a finite set of distance metrics metric one metric to metric three and I asked you which do you prefer.",
                    "label": 1
                },
                {
                    "sent": "My philosophy here is that you should measure the cross validation performance of metric one.",
                    "label": 0
                },
                {
                    "sent": "The cross validation performance of metric to the cross validation performance of metric three, whichever one is lower.",
                    "label": 0
                },
                {
                    "sent": "That's the one you prefer.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So whichever one has the lower error estimate by cross validation is going to be the preferred metric, and that will work for a discrete set.",
                    "label": 1
                },
                {
                    "sent": "But what if I gave you a continuous parameter, continuously parameterized family of metrics?",
                    "label": 0
                },
                {
                    "sent": "Can you find the one which maximizes this performance and what are we going to be OK?",
                    "label": 0
                },
                {
                    "sent": "Are we going to get cake?",
                    "label": 0
                },
                {
                    "sent": "So here is the problem.",
                    "label": 0
                },
                {
                    "sent": "The problem is that cross.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Allocation performance is very hard to optimize.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because leave one out error literal.",
                    "label": 0
                },
                {
                    "sent": "Leave one out, errors it completely discontinuous function of the distance metric.",
                    "label": 1
                },
                {
                    "sent": "Imagine you have your neighborhood graph.",
                    "label": 0
                },
                {
                    "sent": "So each node, each training point is connected to its K nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "And now I adjust the way that I measure distance a little bit.",
                    "label": 0
                },
                {
                    "sent": "So I change the distance metric slightly.",
                    "label": 0
                },
                {
                    "sent": "Maybe the neighborhood graph doesn't change at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so then, if the neighborhood graph doesn't change, what will the leave one out performance do?",
                    "label": 0
                },
                {
                    "sent": "It will stay the same.",
                    "label": 0
                },
                {
                    "sent": "So now imagine graphing the leave one out performance as a function of some parameter of this distance metric.",
                    "label": 0
                },
                {
                    "sent": "It's constant and making a small change it's constant.",
                    "label": 0
                },
                {
                    "sent": "And then what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "Of course I'll make a small change an you know instead of David being a neighbor of me, someone else becomes my neighbor and then the distance the leave one out performance is going to jump up a finite amount.",
                    "label": 0
                },
                {
                    "sent": "So this function leave one out cross validation performance as a function of the distance metric.",
                    "label": 0
                },
                {
                    "sent": "Is basically flat and then it has these discontinuous jumps, so that's a very unpleasant function to try and optimize.",
                    "label": 0
                },
                {
                    "sent": "So we need some kind of smoother function, or at least the continuous cost function to attack.",
                    "label": 1
                },
                {
                    "sent": "So this is the basic trick mode.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you have probably seen this trick by now, a lot of us have been recycling this trick over and over again, but it's a good trick, so it's worth recycling.",
                    "label": 0
                },
                {
                    "sent": "Here's the idea.",
                    "label": 0
                },
                {
                    "sent": "Instead of thinking of a fixed number of K nearest neighbor and then voting those, we're going to consider randomize neighbor selection.",
                    "label": 1
                },
                {
                    "sent": "OK, so imagine that the way we're going to do classification is, each point would randomly pick one of the other points as his neighbor, with the probability that depended on the distance.",
                    "label": 1
                },
                {
                    "sent": "So if the point is closer, you have higher probability of picking it.",
                    "label": 0
                },
                {
                    "sent": "If it's very far away, you have very low probability of picking it, and now we're going to ask what's the expected performance of this randomized nearest neighbor strategy.",
                    "label": 0
                },
                {
                    "sent": "If we averaged over random choice of knickers.",
                    "label": 0
                },
                {
                    "sent": "So here's the setup.",
                    "label": 0
                },
                {
                    "sent": "There is this quantity PJ, which is the probability that point I.",
                    "label": 0
                },
                {
                    "sent": "Select Point J as his neighbor, and that's just the exponential of the distance.",
                    "label": 0
                },
                {
                    "sent": "Maybe you would have preferred it if I put a square here.",
                    "label": 0
                },
                {
                    "sent": "If you would prefer that, then just add a square for all these for the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "And normalized, so this distribution sums to one for every I as a function of J.",
                    "label": 0
                },
                {
                    "sent": "It sums to one, and there's no probability of picking yourself 'cause that would be cheating.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't believe one out.",
                    "label": 0
                },
                {
                    "sent": "It would be leaving nothing out, OK?",
                    "label": 0
                },
                {
                    "sent": "So what's the fraction of the time at this point I will be correctly classified under this randomized rule.",
                    "label": 1
                },
                {
                    "sent": "Well, it's just the probability of all the points J in the same class as I.",
                    "label": 0
                },
                {
                    "sent": "Because if I randomly choose a point that has the same class as me, I'm going to get it right.",
                    "label": 0
                },
                {
                    "sent": "And if I randomly choose a point that has a different class, I'm going to get it wrong.",
                    "label": 0
                },
                {
                    "sent": "So I just add up the mass in this distribution, PJ summing over all of my friends J who are in the same class.",
                    "label": 0
                },
                {
                    "sent": "That total probability is the chance the average fraction of the time that this randomized neighbor rule would have got point I correct under this.",
                    "label": 0
                },
                {
                    "sent": "Leave one out metric.",
                    "label": 0
                },
                {
                    "sent": "So you probably see where we're going with.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is now the expected leave one out classification performance, so I'm just going to average or sum over all the training points.",
                    "label": 0
                },
                {
                    "sent": "OK, So what did I do?",
                    "label": 0
                },
                {
                    "sent": "I just converted leave one out.",
                    "label": 0
                },
                {
                    "sent": "Which is the simplest thing you could imagine into a sort of differentiable version of leave one out by introducing this sort of stochastic neighbor selection.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to take the derivative of the differentiable leave one out.",
                    "label": 0
                },
                {
                    "sent": "That's the idea.",
                    "label": 0
                },
                {
                    "sent": "So this is the objective function that we're going to try and optimize, and it's much smoother.",
                    "label": 1
                },
                {
                    "sent": "Of course, with respect to the distances than the actual leave one out error.",
                    "label": 1
                },
                {
                    "sent": "And notice that there's no explicit parameter K, so I'll talk more about this.",
                    "label": 0
                },
                {
                    "sent": "But in this randomized rule, you don't have to pick K, you just ask the expected fraction of the time that you would get the right class.",
                    "label": 0
                },
                {
                    "sent": "So K is sort of implicitly buried in the scale of these distances, and we're going to effectively learn that scale at the same time question.",
                    "label": 0
                },
                {
                    "sent": "CSUB I is the class of point I so JNC SBY is all the points J in the same class as I.",
                    "label": 0
                },
                {
                    "sent": "Really bad.",
                    "label": 0
                },
                {
                    "sent": "Distance is very large.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, I didn't mean that there is a distance for which this is exactly K nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Of course it's soft weighting of the neighbors, but just the size of the neighborhood informally is controlled by the scale of D. If D is measured in very large units.",
                    "label": 0
                },
                {
                    "sent": "Then the size of the neighborhood is essentially equals one, and if D is measured in very small units, then you're averaging overlap, but it will never be, of course equivalent to K nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the idea is to learn the metric by adjusting these these to maximize this expected.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formants and having done the simplest thing I could possibly do for the objective function, I'm now going to do the simplest thing I could possibly do for the model.",
                    "label": 0
                },
                {
                    "sent": "So the simplest thing you could possibly do is just measure the distance between two points XI and XJ by putting a positive semidefinite matrix in here.",
                    "label": 0
                },
                {
                    "sent": "OK, so just Gaussian or quadratic metric.",
                    "label": 0
                },
                {
                    "sent": "So Q is some symmetric positive semidefinite matrix, and we're going to rewrite this distance using the square root of Q.",
                    "label": 0
                },
                {
                    "sent": "So I just replaced Q with a transpose A.",
                    "label": 0
                },
                {
                    "sent": "There's many ways to do this that aren't unique, but any square root of Q will make the point here.",
                    "label": 0
                },
                {
                    "sent": "What is the distance turned out to be?",
                    "label": 0
                },
                {
                    "sent": "The distance turns out to just be Euclidean distance in some transformed space.",
                    "label": 0
                },
                {
                    "sent": "So what you can imagine is that I just found a transformation A. Afterwhich the right distance metric to use is just Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "So you can summarize everything I told you so far by the following.",
                    "label": 0
                },
                {
                    "sent": "Find me a linear transformation of the original data so that when I look in that linear transformation and I do dumb Euclidean K nearest neighbor things will workout well.",
                    "label": 0
                },
                {
                    "sent": "As estimated by leave one out cross validation or a soft version of it.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions everybody with me Pascal.",
                    "label": 0
                },
                {
                    "sent": "The previous yeah this in regression this would be very close to parzen window.",
                    "label": 0
                },
                {
                    "sent": "With the exception that this normalization is sort of per point.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, exactly exactly.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can think of this as sort of stealing the bandwidth trick from the density estimation and trying to make it work for classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is probably.",
                    "label": 0
                },
                {
                    "sent": "Too many equations on one slide, but I just want to make a point here because Jan touched on and I think it's very very crucial if you just do what I told you you're going to end up with an algorithm that's quadratic in the number of data points, and that's probably going to make you unhappy.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try and tell you a little bit about how to approximate or sample to get around that, so here it is.",
                    "label": 0
                },
                {
                    "sent": "This is the expected classification performance, so this is the average overall data points of the chance of getting that data point correct in the leave one out scheme.",
                    "label": 1
                },
                {
                    "sent": "OK, so if we take the derivative of this with respect to the distances where the distances are defined like this, then what we're really taking the derivative with respect to.",
                    "label": 0
                },
                {
                    "sent": "Is this transformation a, so we're adjusting the linear transformation so that.",
                    "label": 0
                },
                {
                    "sent": "Effective performance is fine Now if you write down this expression for this gradient, the exact expression, it's sort of quadratic.",
                    "label": 0
                },
                {
                    "sent": "You have to sum over all the data points because each data point contributes a gradient term, and in order to compute the gradient term of that data point exactly, you need to consider all the other data points first.",
                    "label": 0
                },
                {
                    "sent": "The ones in your class, and then the ones not in your class.",
                    "label": 0
                },
                {
                    "sent": "So it would be quadratic if you did it.",
                    "label": 0
                },
                {
                    "sent": "Now there's two observations here.",
                    "label": 0
                },
                {
                    "sent": "One, this is just gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So we don't really care about the exact gradient, right?",
                    "label": 0
                },
                {
                    "sent": "It's not a holy artifact, it's just the gradient.",
                    "label": 0
                },
                {
                    "sent": "We're just going to take it and adjust our parameters and drop it on the floor so we don't really need the gradient to be right.",
                    "label": 0
                },
                {
                    "sent": "We just need it to be sort of right.",
                    "label": 0
                },
                {
                    "sent": "In fact, we don't even need to be sort of right, we just need it to be not 90 degrees wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so just subsample here, just grab a few of your data points, use them to estimate the gradient nudger parameters, and start again.",
                    "label": 0
                },
                {
                    "sent": "That will be fine.",
                    "label": 0
                },
                {
                    "sent": "I won't tell.",
                    "label": 0
                },
                {
                    "sent": "OK, stochastic gradient optimization perfectly legitimate.",
                    "label": 0
                },
                {
                    "sent": "OK, the other observation is that in these interior sums you're waiting.",
                    "label": 0
                },
                {
                    "sent": "The other data points so.",
                    "label": 0
                },
                {
                    "sent": "Oh, it appeared back here on the screen.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "I guess there's a penalty for spending too long on a slide.",
                    "label": 0
                },
                {
                    "sent": "So you're waiting the other data points and they're waiting them by the neighborhood selection probability.",
                    "label": 0
                },
                {
                    "sent": "And most of these probabilities are almost zero, right?",
                    "label": 0
                },
                {
                    "sent": "I have my little neighborhood, my friends who are around me, and the probability of picking someone all the way over there is basically 0, so you don't need to go through all these painful calculations.",
                    "label": 0
                },
                {
                    "sent": "You can keep an active list of who's really in the effective neighborhood domain, and then update that active list every few 100 iterations when you retouch the parameters.",
                    "label": 1
                },
                {
                    "sent": "So there's some tricks for approximation here.",
                    "label": 0
                },
                {
                    "sent": "I don't want to get into them, but.",
                    "label": 0
                },
                {
                    "sent": "If you just write down this equation, it looks pretty nasty.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the algorithm, the first one that I wanted to tell you about.",
                    "label": 0
                },
                {
                    "sent": "It's called Neighborhood Components analysis, so it was in nips a couple of years ago and it learns a linear transformation of the input space after which nearest neighbor performs well.",
                    "label": 1
                },
                {
                    "sent": "So the transformation intuitively scales up directions which are useful for discrimination and tries to project out dimensions which are not informative about the class identity.",
                    "label": 1
                },
                {
                    "sent": "And the algorithm is very simple.",
                    "label": 0
                },
                {
                    "sent": "Write down this objective function.",
                    "label": 0
                },
                {
                    "sent": "And optimize the heck out of it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So and then you learn, you take this a that you learned and you project your training set into that knew feature space.",
                    "label": 0
                },
                {
                    "sent": "So for every training point XI, you create a new training point.",
                    "label": 0
                },
                {
                    "sent": "Why I?",
                    "label": 0
                },
                {
                    "sent": "By multiplying by this transformation, and then you put the wise as the training data for your old K nearest neighbor code.",
                    "label": 0
                },
                {
                    "sent": "And at Test time when you get a new test point, you also multiply it by A to get it into the right test space, and then you present that.",
                    "label": 0
                },
                {
                    "sent": "Why test to your code so you don't really even have to write any new code for test time?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have KD trees and fancy data structures LSH, that does.",
                    "label": 0
                },
                {
                    "sent": "You know retrieval for the neighbors and stuff.",
                    "label": 0
                },
                {
                    "sent": "You can use all that.",
                    "label": 0
                },
                {
                    "sent": "This is just a preprocessing step that warps the inputs so that Euclidean distance is not totally stupid.",
                    "label": 0
                },
                {
                    "sent": "OK, So what about Ki?",
                    "label": 0
                },
                {
                    "sent": "Keep sweeping K under the rug here?",
                    "label": 0
                },
                {
                    "sent": "Or actually it somehow vanished and that seems like unfair.",
                    "label": 0
                },
                {
                    "sent": "So the scale of transformation A is also learned.",
                    "label": 0
                },
                {
                    "sent": "That's the crucial thing in the op.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To misers to space, there is a direction in which it can scale up all the components of a uniformly, or scale them down uniformly, which means that the optimizer can effectively change the unit system for measuring D. And that means that you're effectively also learning a real valued estimate of the optimal neighbor size.",
                    "label": 1
                },
                {
                    "sent": "So if you want to make the neighborhood size effectively smaller, the optimizer can make D bigger by just scaling up A and if it wants to make the neighborhood size bigger, it can make D smaller by scaling down a.",
                    "label": 0
                },
                {
                    "sent": "Now, of course it's the same neighborhood size everywhere.",
                    "label": 0
                },
                {
                    "sent": "Ideally, we'd like to have a different sort of neighborhood in different parts of the space, and we can sort of effectively estimate the effective K in different parts of the space by looking at these average perplexities.",
                    "label": 0
                },
                {
                    "sent": "But we usually just when we do it, we learn this transformation.",
                    "label": 0
                },
                {
                    "sent": "Whatever scale it has, that's fine.",
                    "label": 0
                },
                {
                    "sent": "And then on top of that we do.",
                    "label": 0
                },
                {
                    "sent": "We go back to doing K nearest neighbor and we set K by cross validation by straight up cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the real win as far as I'm concerned with this method is not just that it learns a good distance metric, it's that you can just take everything.",
                    "label": 0
                },
                {
                    "sent": "I told you an replace the Matrix a which you were probably thinking of as a square matrix by a really really skinny rectangular matrix, and that's equivalent to making the metric that you're learning low rank.",
                    "label": 0
                },
                {
                    "sent": "OK, so if a transpose A is the metric, an A is a skinny matrix like this, then the metric will be low rank.",
                    "label": 1
                },
                {
                    "sent": "You'll be essentially projecting your high dimensional original inputs into a low dimensional space in which nearest neighbor is expected to do well.",
                    "label": 1
                },
                {
                    "sent": "So that's really I think a huge benefit, so you can seriously reduce the storage and computation requirements, and if you really want to go down to two or three dimensions, then you can do essentially sort of linear visualization.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a picture of that.",
                    "label": 1
                },
                {
                    "sent": "In the second guy.",
                    "label": 0
                },
                {
                    "sent": "The objective function is it, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK so guys question is is the objective function convex and the answer is definitely not.",
                    "label": 0
                },
                {
                    "sent": "There's lots of local minima.",
                    "label": 1
                },
                {
                    "sent": "And it's also not identifiable because you can apply a unitary transformation too.",
                    "label": 0
                },
                {
                    "sent": "To a in the square case.",
                    "label": 0
                },
                {
                    "sent": "Or you can rotate the low dimensional space so but.",
                    "label": 0
                },
                {
                    "sent": "I don't even know what identifiability really means.",
                    "label": 0
                },
                {
                    "sent": "I mean, so identifiability is supposed to mean that in the hypothetical world which never exists in which our model is exactly right and we had infinite data which we never do, then the parameters that we estimate would go to the real parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to admit that that's not the case here.",
                    "label": 0
                },
                {
                    "sent": "But I mean, you shouldn't worry about it, because all you're really learning is the metric right?",
                    "label": 0
                },
                {
                    "sent": "All you care about is the outer product of a.",
                    "label": 0
                },
                {
                    "sent": "So if you learn in A and I learn in a, it doesn't really matter if they are not the same.",
                    "label": 0
                },
                {
                    "sent": "That's absolutely true.",
                    "label": 0
                },
                {
                    "sent": "So there is a big issue of how you regularize.",
                    "label": 0
                },
                {
                    "sent": "I'm being a bit glib, but there is an issue of Regularising A and I'll talk maybe a little bit about that in the square case, you could regularize a by making it diagonal.",
                    "label": 0
                },
                {
                    "sent": "You could say I just want to learn scale for each dimension, but no interactions between the dimensions.",
                    "label": 0
                },
                {
                    "sent": "That's one thing you can do.",
                    "label": 0
                },
                {
                    "sent": "You can also just regularize the Frobenius norm of A and just put that in the gradient, so that keeps.",
                    "label": 0
                },
                {
                    "sent": "Post a definite plus.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we haven't tried that.",
                    "label": 0
                },
                {
                    "sent": "A factor analysis kind of covariance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we haven't tried that, but you could do it.",
                    "label": 0
                },
                {
                    "sent": "You would then have to do the gradient descent becomes a little bit more complicated 'cause you have to sort of solve the credit assignment problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me show you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Can you use a?",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "So the question is instead of a linear.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Transformation, could you use a differentiable nonlinear transformation absolutely and in the paper the original NIPS paper for this we show some results at the end using some neural networks to identify, you know, pose of faces and stuff, which is a very nonlinear function.",
                    "label": 0
                },
                {
                    "sent": "So definitely definitely OK.",
                    "label": 0
                },
                {
                    "sent": "So just some illustrative results here and then I'll move on to the second, the second topic.",
                    "label": 0
                },
                {
                    "sent": "So here's some data synthetic data.",
                    "label": 1
                },
                {
                    "sent": "Two of the dimensions contain these sort of concentric rings, which you can see here an all the other dimensions contain noise, sort of Gaussian noise with the variants drawn from some gamma.",
                    "label": 1
                },
                {
                    "sent": "Distribution, and I think that there's 498 noise dimensions and two real dimensions in this problem.",
                    "label": 0
                },
                {
                    "sent": "And then what we did is we just gave the algorithm the labels so it knows the labels of the points and we said find you 2 dimensional projection of the data where nearest neighbor would be expected to do well.",
                    "label": 0
                },
                {
                    "sent": "So of course it's just a setup, it's perfectly suited to our algorithm, but just to show you if you do the neighborhood components analysis you get recovered the rings as you should.",
                    "label": 0
                },
                {
                    "sent": "So that's just a sanity check.",
                    "label": 0
                },
                {
                    "sent": "If you do Fishers discriminants, you get this kind of jumble, and that's because it's LDA.",
                    "label": 0
                },
                {
                    "sent": "Fisher's discriminant makes the assumption that each class is distributed according to a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And then it tries to assume they all have the same covariance and then that gives you a linear discriminants.",
                    "label": 0
                },
                {
                    "sent": "After you solve an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "So that's really a bad assumption here that every class is a Gaussian with the same variance.",
                    "label": 0
                },
                {
                    "sent": "That's a terrible assumption.",
                    "label": 0
                },
                {
                    "sent": "And then if you do PCA where you ignore the labels and you just find the principle directions of directions of most variants, then of course you also get a jumble because there's lots of high variance noise directions and all PCA cares about is noise and it just goes variants that just goes and finds those directions.",
                    "label": 0
                },
                {
                    "sent": "Maybe a little bit, I'll just skip over that a little bit better exam.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apple is some simple face classification task, so these are sort of raw data.",
                    "label": 0
                },
                {
                    "sent": "Is raw images taken off a really low res camera and there's 18 people in the data set whose identity or the class labels?",
                    "label": 1
                },
                {
                    "sent": "And so again here this is Fishers discriminants.",
                    "label": 0
                },
                {
                    "sent": "This is principle components and this is what you get if you do neighborhood components analysis.",
                    "label": 0
                },
                {
                    "sent": "So this is just literally a 560 by two matrix.",
                    "label": 0
                },
                {
                    "sent": "In which I multiply each of the raw frames by that matrix.",
                    "label": 0
                },
                {
                    "sent": "That gives Me 2 dimensions and I just plot the point here, and then I colored them according to their labels.",
                    "label": 0
                },
                {
                    "sent": "So there is a linear projection which does a very good job at separating things here.",
                    "label": 0
                },
                {
                    "sent": "And you can convince yourself that this also helps by measuring the test error.",
                    "label": 0
                },
                {
                    "sent": "So this plot shows you a mixture of training and test points, but you might be worried that it's overfitting and at least in this case the errors show you that it's really not.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to move on to a sort of related objective function that Amir Globerson had the idea of optimizing which I think is very interesting in its own right.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the original NCA objective, and I'm just going to put a log in front of it, which shouldn't disturb anyone.",
                    "label": 1
                },
                {
                    "sent": "OK, so the original NCA objective was this, it was the expected number of correct classifications that you would get in your data set under this randomized rule.",
                    "label": 1
                },
                {
                    "sent": "So maximizing that is the same as maximizing the log.",
                    "label": 0
                },
                {
                    "sent": "And here I just wrote out.",
                    "label": 0
                },
                {
                    "sent": "I expanded this probability of getting data point I correctly in terms of the probability of landing on any of the other data points J.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple of things you could do.",
                    "label": 0
                },
                {
                    "sent": "One thing is, you could take this log and you could push it inside here.",
                    "label": 0
                },
                {
                    "sent": "What does that do?",
                    "label": 1
                },
                {
                    "sent": "That is, the chance of labeling the data set exactly right.",
                    "label": 0
                },
                {
                    "sent": "So you might be a bit worried that this objective function can make confident errors, and that's true, right?",
                    "label": 0
                },
                {
                    "sent": "If there's one data point that's really irritating, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "It just gets that data point wrong.",
                    "label": 0
                },
                {
                    "sent": "It's only one out of any of the data points, right?",
                    "label": 0
                },
                {
                    "sent": "So who cares?",
                    "label": 0
                },
                {
                    "sent": "It just puts it in the deep zone of class labels, which have nothing to do with it, and it gets it wrong, and it just suffers the loss.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "But if you have a domain where you think your data is very noise free your training data and you really don't want your classifier confidently giving you wrong answers at Test time.",
                    "label": 1
                },
                {
                    "sent": "Then this might concern you, so then you can put the log inside, in which case you're not maximizing the expected number of correct labels.",
                    "label": 0
                },
                {
                    "sent": "You're maximizing the chance of getting every label exactly right.",
                    "label": 0
                },
                {
                    "sent": "And then what Amir suggested is, you know why?",
                    "label": 0
                },
                {
                    "sent": "Stop there.",
                    "label": 0
                },
                {
                    "sent": "Let's push the log inside one more time.",
                    "label": 0
                },
                {
                    "sent": "OK, so then this is the objective function.",
                    "label": 0
                },
                {
                    "sent": "And this objective function has a very interesting geometrical interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the geometrical interpretation.",
                    "label": 0
                },
                {
                    "sent": "It's collapsing classes, so this objective function is only good if each class is just one unimodal distribution.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be Gaussian, but it's some kind of unimodal blob, and in that case, what would a good metric do?",
                    "label": 0
                },
                {
                    "sent": "Well, a good metric would be one under which all the points of the same class got squished together in infinitely close, and all the other ones got spread infinitely far away.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So a perfect metric would collapse, say for the green class, it would collapse all the green points together and send all the other points very very far away.",
                    "label": 1
                },
                {
                    "sent": "Assuming each class is just a single blob.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to kind of get from that geometric intuition to that objective function that I showed you, here's what Amir and I did.",
                    "label": 0
                },
                {
                    "sent": "We said well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's call this distribution the ideal distribution over the neighbors possible neighbors J for each class.",
                    "label": 0
                },
                {
                    "sent": "For each data point, I so the ideal distribution over neighbors for data point I is some constant value if the class of I in the class of J are equal.",
                    "label": 0
                },
                {
                    "sent": "So you put some constant mass on all of the friends who are in the same class as you and you put zero probability of landing on anyone who isn't in your class.",
                    "label": 0
                },
                {
                    "sent": "Well, that would be very nice, right?",
                    "label": 0
                },
                {
                    "sent": "If we could get this distribution then our Leave leave one out estimate would be perfect.",
                    "label": 0
                },
                {
                    "sent": "We would estimate that we would be getting everything labeled perfect.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "We're going to find the metric or the transformation by minimizing the average KL divergences from this ideal distribution to the actual distribution induced by a.",
                    "label": 1
                },
                {
                    "sent": "So each metric that we pick induces some distribution over neighbors.",
                    "label": 0
                },
                {
                    "sent": "And there's the ideal distribution, which puts constant probability on the correct class and zero probability ever else.",
                    "label": 0
                },
                {
                    "sent": "And we just measure the KL divergent's.",
                    "label": 0
                },
                {
                    "sent": "So for each data point I I ask what distribution over other data points J is being induced by the metric.",
                    "label": 0
                },
                {
                    "sent": "That's this guy I said.",
                    "label": 0
                },
                {
                    "sent": "What would you love to have as your distribution?",
                    "label": 0
                },
                {
                    "sent": "That's this guy and I just penalize the KL distribution.",
                    "label": 0
                },
                {
                    "sent": "I average or sum over data points and we just minimize that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you massage this function you would see that it comes back to that original NCA thing, But the good news here and this relates to what Guy was talking about.",
                    "label": 1
                },
                {
                    "sent": "This objective function is convex in the metric.",
                    "label": 0
                },
                {
                    "sent": "There's a single optimal metric which solves this problem, and so Amir and I called it the maximum.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The collapsing metric.",
                    "label": 0
                },
                {
                    "sent": "So MCM.",
                    "label": 0
                },
                {
                    "sent": "The maximally collapsing metric and the what it does is it pulls all of the data points in a single class as close together as possible and pushes all the data points in the other class as far away as possible.",
                    "label": 1
                },
                {
                    "sent": "So you can find it by various methods.",
                    "label": 1
                },
                {
                    "sent": "As far as I know, this particular optimization problem doesn't have a nice name.",
                    "label": 1
                },
                {
                    "sent": "It's not like SCP or QP or anything like that, but it is convex.",
                    "label": 0
                },
                {
                    "sent": "You can sort of use the log sum, Exp kind of trick to show that it's a convex function of the distance is there, and it's a linear function of the distances here, so that's that's all fine for the experiments all show.",
                    "label": 0
                },
                {
                    "sent": "What we did is we just use gradient descent.",
                    "label": 1
                },
                {
                    "sent": "Followed by projection.",
                    "label": 0
                },
                {
                    "sent": "So we just did gradient descent in the full metric and then we projected back onto the semidefinite cone and then another gradient step.",
                    "label": 0
                },
                {
                    "sent": "So, but certainly that's not the best way to optimize this, I don't think so, OK?",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This a few speculations here about the relationship between maximally collapsing metrics and Fisher's discriminant.",
                    "label": 0
                },
                {
                    "sent": "So MCM is similar to Fishers discriminate in that it tries to sort of minimize within class distances, variances and maximize between class differences.",
                    "label": 1
                },
                {
                    "sent": "But Fishers discriminate is a purely 2nd order method.",
                    "label": 0
                },
                {
                    "sent": "The sort of sufficient statistics for Fisher's discriminant are just the you know, mean and covariance of each of each class.",
                    "label": 1
                },
                {
                    "sent": "So I. MCM, the maximum collapsing metric is sort of a generalization of LDA that makes only the much weaker assumption that each class is unimodal, but it doesn't assume that there Gaussian or that all the Gaussians have equal variance.",
                    "label": 1
                },
                {
                    "sent": "It just assumes that it can be separated from the other class blobs.",
                    "label": 0
                },
                {
                    "sent": "So that's a kind of nice generalization, but maximum collapsing metric like linear discriminate analysis is going to fail.",
                    "label": 0
                },
                {
                    "sent": "For example, on this concentric rings, because the assumption that each class is a unimodal blob.",
                    "label": 0
                },
                {
                    "sent": "Is not really valid here.",
                    "label": 0
                },
                {
                    "sent": "Each class is sort of a ring here, so that's not.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't expect it to work.",
                    "label": 0
                },
                {
                    "sent": "You might imagine some kind of mixture model trick for getting around this, but we haven't really explored that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now guys question becomes really relevant.",
                    "label": 0
                },
                {
                    "sent": "What if you want to learn a low rank metric that's maximally collapsing?",
                    "label": 0
                },
                {
                    "sent": "Well, unfortunately this is one of the great tragedies of modern optimization.",
                    "label": 0
                },
                {
                    "sent": "The set of low rank matrices is not convex, right?",
                    "label": 0
                },
                {
                    "sent": "Take the outer product of two vectors and the outer product of two other vectors and average them.",
                    "label": 0
                },
                {
                    "sent": "You don't have a rank one matrix anymore, so that's bad news.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of two ways to proceed.",
                    "label": 0
                },
                {
                    "sent": "One way to proceed is what was suggested this morning.",
                    "label": 0
                },
                {
                    "sent": "You find the optimal full rank metric and there should be a sort of star here that says you know.",
                    "label": 0
                },
                {
                    "sent": "If you can actually succeed in performing the computation required to do that, and then you keep only it's K largest eigenvalues, you just, you know, zero out the other eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "An that procedure is well defined, it has no local minima because the first step is convex and the next step SVD is also convex, and so it will be close to optimal if the full rank metric has a rapidly decaying eigen spectrum.",
                    "label": 1
                },
                {
                    "sent": "And the other thing you can do is explicitly parameterized.",
                    "label": 1
                },
                {
                    "sent": "The set of low rank matrices using say the Lu decomposition of the metric and then optimize the KL objective just using gradient or local search like we did before.",
                    "label": 0
                },
                {
                    "sent": "And that's really your only option when you have a big data set running around.",
                    "label": 0
                },
                {
                    "sent": "So just show you a few few graphs of results here so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are just sort of random toy datasets that we downloaded off of UCI, like like you would imagine for NIPS paper.",
                    "label": 0
                },
                {
                    "sent": "And then this horizontal axis shows the rank the effective rank of the metric and the vertical axis shows the shows the error.",
                    "label": 1
                },
                {
                    "sent": "The blue is the maximally collapsing metric.",
                    "label": 0
                },
                {
                    "sent": "The red triangles are linear discriminant analysis in some of the examples, we compared to Eric Jing, atolls metric learning and to some white and PCA, which doesn't even use the labels.",
                    "label": 1
                },
                {
                    "sent": "But generally MTM is doing.",
                    "label": 0
                },
                {
                    "sent": "Is doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "And what's really interesting is not where these methods rank relative to each other.",
                    "label": 0
                },
                {
                    "sent": "What's really interesting is the fact that you know, as you decrease the dimension, it usually takes a long while before your error rate takes off.",
                    "label": 0
                },
                {
                    "sent": "So the effective dimensionality of feature spaces you need to keep around is much lower than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "All the action is happening in a low dimensional space, and so that's really what Jan was saying this morning that there's, you know, if you really can't afford to learn these features as part of the whole classification thing, you don't need very many features.",
                    "label": 0
                },
                {
                    "sent": "To get the job done so he showed you these, you know eight.",
                    "label": 0
                },
                {
                    "sent": "You know weird rotation things for airplanes where if you just multiply the image of the airplane by that that low dimensional space tells you everything you need to know.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just let me finish off here in the absence of straw.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On prior knowledge about how to pick your classification distance metric, I think learning the metric seems like a good idea.",
                    "label": 1
                },
                {
                    "sent": "I think you know just hoping that everything is going to be fine with K nearest neighbor or normalizing your data arbitrarily and then hoping everything will be fine is just not the way to go.",
                    "label": 0
                },
                {
                    "sent": "We have tons of data that data should be able to tell us the metric.",
                    "label": 1
                },
                {
                    "sent": "So learning low rank metrics gives you a natural way to reduce memory and computation while still doing very well on classification.",
                    "label": 1
                },
                {
                    "sent": "So I told you about these two algorithms, neighborhood components analysis, so NCA assumes nothing about the form of the class distributions.",
                    "label": 1
                },
                {
                    "sent": "It doesn't assume that they are unimodal or Gaussian or connected.",
                    "label": 0
                },
                {
                    "sent": "Even an it doesn't assume anything about the shape of the separating surface that it's linear or linear in feature space.",
                    "label": 0
                },
                {
                    "sent": "It's purely non parametric method.",
                    "label": 0
                },
                {
                    "sent": "And then maximally collapsing metrics.",
                    "label": 1
                },
                {
                    "sent": "If you know we're really believe that each class is unimodal, they're nice because it's a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the attraction.",
                    "label": 0
                },
                {
                    "sent": "Either way, I think it's very surprising how far you can go with just quadratic mappings.",
                    "label": 0
                },
                {
                    "sent": "Quadratic metrics which are just linear mappings of your features.",
                    "label": 1
                },
                {
                    "sent": "So why linear?",
                    "label": 1
                },
                {
                    "sent": "Well, linear mappings are hard.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should have put harder to overfit, you can overfit them of course, but they're harder to overfit then very complicated nonlinear things.",
                    "label": 0
                },
                {
                    "sent": "So if you're lazy and you don't want to spend a lot of time regularising things, then that's one way to start off.",
                    "label": 0
                },
                {
                    "sent": "They're very compact to represent their very fast to implement the test time, and it turns out that there are extremely good linear mappings out there for a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "I think we now we're starting to get a handle on how to find them so.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "I'd be happy to take questions.",
                    "label": 0
                },
                {
                    "sent": "Just before the questions, I don't see Jacob Goldberger, but I want to say that Russ is standing over by the door, so he's the student who did a lot of the heavy lifting on the original paper there.",
                    "label": 0
                },
                {
                    "sent": "So Tommy.",
                    "label": 0
                },
                {
                    "sent": "Why not question?",
                    "label": 0
                },
                {
                    "sent": "OK. Press 2 object expecting shower.",
                    "label": 0
                },
                {
                    "sent": "Easily intern EM like algorithms for at least at this Contacts.",
                    "label": 0
                },
                {
                    "sent": "Why not, why not?",
                    "label": 0
                },
                {
                    "sent": "Regularise with Tracy.",
                    "label": 0
                },
                {
                    "sent": "Blank.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So let me answer the second question first.",
                    "label": 0
                },
                {
                    "sent": "Palmese question.",
                    "label": 0
                },
                {
                    "sent": "The second question was, well, why don't we use that race as a surrogate for the rank and then try and put that in?",
                    "label": 0
                },
                {
                    "sent": "And by penalising that race would hope that we would encourage you know metrics which don't have very many big eigenvalues and would be very close if not exactly lowering.",
                    "label": 0
                },
                {
                    "sent": "So I think that.",
                    "label": 0
                },
                {
                    "sent": "Oh the the trace.",
                    "label": 0
                },
                {
                    "sent": "You want to penalize.",
                    "label": 0
                },
                {
                    "sent": "So you want to penalize the sum of the eigenvalues of the metric, yes?",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a great idea and that's pretty easy to put in in the gradient formulation 'cause you just take the gradient directly.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a really good idea.",
                    "label": 0
                },
                {
                    "sent": "The to go back to the first question, so I think what Tommy is suggesting is to say, well, you can think of there being a hidden variable as which was the other data point that you picked right?",
                    "label": 0
                },
                {
                    "sent": "And we could just estimate that hidden variable.",
                    "label": 0
                },
                {
                    "sent": "Is that what you're suggesting and then?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "But the problem with that is that if you really want to keep around for each data point to distribution over other data points, then I think it's going to be sort of tricky.",
                    "label": 0
                },
                {
                    "sent": "You'll have to do a lot of approximation for that.",
                    "label": 0
                },
                {
                    "sent": "Because otherwise you'll have to have.",
                    "label": 0
                },
                {
                    "sent": "You have to have N squared matrix in memory, which is you know single stochastic matrix at the step.",
                    "label": 0
                },
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "One step of that year Makram, are you fix the Easter by starting from all zero A that gives U uniform over the correct class I see.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "Right, yes, I guess that's what we're doing so then.",
                    "label": 0
                },
                {
                    "sent": "So then that seems like maybe it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so then the answer is.",
                    "label": 0
                },
                {
                    "sent": "Because we never really thought of it I guess.",
                    "label": 0
                },
                {
                    "sent": "Or maybe if we had thought of it, I would have misunderstood it in the way that I did when you first explained it so.",
                    "label": 0
                },
                {
                    "sent": "But that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "I hear a mirror is really busy with his new post Doc, but maybe I'll see if he wants to waste some time on here is now doing a postdoc with Tommy.",
                    "label": 0
                },
                {
                    "sent": "For those of you who missed his amazing Taco was amazing talk at NIPS.",
                    "label": 0
                },
                {
                    "sent": "By the way, OK, other questions.",
                    "label": 0
                },
                {
                    "sent": "Comments from a guy that changes actually uses it.",
                    "label": 0
                },
                {
                    "sent": "The good news is that this thing, I think, is the best thing you can buy for dimensionality reduction for classification.",
                    "label": 0
                },
                {
                    "sent": "And the best thing is that if you do, we just much larger then.",
                    "label": 0
                },
                {
                    "sent": "104 two plus problems.",
                    "label": 0
                },
                {
                    "sent": "It will it rain and the global trend very significantly.",
                    "label": 0
                },
                {
                    "sent": "And The funny thing is that it doesn't help to add more data.",
                    "label": 0
                },
                {
                    "sent": "So hang on, you're saying it overfits when the metric is low rank or when it isn't.",
                    "label": 0
                },
                {
                    "sent": "High dimensionality, like several hundred to two relatively normal dimensionality, like 30 or 40, not to one or two, and then our whole lot of complicated reasons why I think.",
                    "label": 0
                },
                {
                    "sent": "So it does happen.",
                    "label": 0
                },
                {
                    "sent": "So you do need regularization here.",
                    "label": 0
                },
                {
                    "sent": "So if you want to try this something, some real project you will have to think about regularization and.",
                    "label": 0
                },
                {
                    "sent": "Approximate gradient.",
                    "label": 0
                },
                {
                    "sent": "Are you alright that we don't give exact gradient but the other thing is that you need more than the closer you are to the minimum, the more accurate gradients in it.",
                    "label": 0
                },
                {
                    "sent": "So it might make sense, right?",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So that's certainly true.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there's nothing to stop you from, just, you know, running this for awhile and then taking whatever transformation you have and using that one.",
                    "label": 0
                },
                {
                    "sent": "I mean, you don't need to get to the minimum, right?",
                    "label": 0
                },
                {
                    "sent": "If you imagine that you started off with the identity matrix, OK, you were in a terrible situation originally, so you're going to be in a much better situation.",
                    "label": 0
                },
                {
                    "sent": "So one way to think about it is if you run it for X hours, how much does it help rather than how long does it take to get to the minimum?",
                    "label": 0
                },
                {
                    "sent": "But yeah, those are both good comments.",
                    "label": 0
                },
                {
                    "sent": "Graph.",
                    "label": 0
                },
                {
                    "sent": "The ones for MCM.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "The original nearest.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We don't have the original nearest neighbor performance.",
                    "label": 0
                },
                {
                    "sent": "But it's it's generally, it's generally.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's generally two kinds of problems.",
                    "label": 0
                },
                {
                    "sent": "There's problems in which.",
                    "label": 0
                },
                {
                    "sent": "The original in the original problem, nearest neighbor works pretty well.",
                    "label": 0
                },
                {
                    "sent": "And in those kinds of problems, then you would expect it at the high dimensions it would be fine.",
                    "label": 0
                },
                {
                    "sent": "But of course, in order to make this you would only be able to make that plot on that point on the extreme right hand side of the plot, so.",
                    "label": 0
                },
                {
                    "sent": "So yeah, you could say that and that we should add.",
                    "label": 0
                },
                {
                    "sent": "But the to me what's interesting is that you can get down to here.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, well in some of them like we can go back to this.",
                    "label": 0
                },
                {
                    "sent": "What is PCA?",
                    "label": 0
                },
                {
                    "sent": "Oh, the PCA curve is if you do PCA and then you divide each of the principle directions by the eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So White and PCA.",
                    "label": 0
                },
                {
                    "sent": "So like you're basically whitening the data and then projecting out some dimensions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but oops, I'm going the wrong way here, I think.",
                    "label": 0
                },
                {
                    "sent": "But let me just show you.",
                    "label": 0
                },
                {
                    "sent": "Like for example, in the NCA case.",
                    "label": 0
                },
                {
                    "sent": "If you just use K nearest neighbor in the original space, you get 18% errors.",
                    "label": 0
                },
                {
                    "sent": "If you normalize each dimension to have variance one then you get actually slightly worse and then if you do NCL you get 4.",
                    "label": 0
                },
                {
                    "sent": "So that's a.",
                    "label": 0
                },
                {
                    "sent": "In that example, it tells you.",
                    "label": 0
                },
                {
                    "sent": "One more.",
                    "label": 0
                },
                {
                    "sent": "Yes, Tim.",
                    "label": 0
                },
                {
                    "sent": "Normalize that you align the eyes along the faces at all.",
                    "label": 0
                },
                {
                    "sent": "Or no, we just took the raw data right off the now.",
                    "label": 0
                },
                {
                    "sent": "Of course this is a very easy problem.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming that this is, you know, any reasonable tough classifier would go eat this for breakfast, but my point is just if for whatever reason you were going to do K nearest neighbor, you would be.",
                    "label": 0
                },
                {
                    "sent": "In really bad state, even if you pick K bye leave one out so.",
                    "label": 0
                },
                {
                    "sent": "Got it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}