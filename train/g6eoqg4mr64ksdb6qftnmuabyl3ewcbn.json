{
    "id": "g6eoqg4mr64ksdb6qftnmuabyl3ewcbn",
    "title": "AMUSE: Multilingual Semantic Parsing for Question Answering over Linked Data",
    "info": {
        "author": [
            "Sherzod Hakimov, Center of Excellence Cognitive Interaction Technology (CITEC), Bielefeld University"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_hakimov_amuse/",
    "segmentation": [
        [
            "Yeah hello everyone, my name is Charlotte Hakima, PhD student from Semantic Computing group.",
            "Today I'm going to be presenting multilingual semantic parsing approach for qual data set.",
            "So now."
        ],
        [
            "Would we have?",
            "I think everyone has smartphone and every smartphone nowadays have a virtual assistance and these are just some of the big ones that are out there.",
            "But what they can do is they can do various tasks such as creating a reminders booking at the restaurant.",
            "And ordering stuff like in in terms of Alexa."
        ],
        [
            "From Amazon, but how good are they for factual question answering?",
            "And especially in a multilingual setting.",
            "So for instance, if you have a question who created Wikipedia in English, see replies that the Founders are Jimmy Wales and Larry Sanger, which is the right answer.",
            "And if we ask in German about hot Wikipedia grounded.",
            "It gives me as an answer.",
            "Wikipedia and it's 2001, which is maybe creation, starting date of the Wikipedia.",
            "So which is not the right answer?",
            "How about in Spanish?",
            "Can clear Wikipedia?",
            "It gives me Wikipedia pages, but there is.",
            "Jimmy Wales somewhere as a link, but not as a direct answer so we can see that even though this virtual system work quite well.",
            "But when we switch to another language except English, they don't work that well in this setting."
        ],
        [
            "So essentially we are tackling the same problem here.",
            "If you have a question such as this, who created Wikipedia, the task is to create a sparkle query based on DB pedia such as this Wikipedia author X and we are looking for the X.",
            "And the same problem can be shown in other languages, as is because the.",
            "The GPS is same data for every language, so all the challenge here is to map these things.",
            "OPS.",
            "Where the created here means author in the query.",
            "Wikipedia is the same entity in all languages and the grounded or hot hot kingroot net is the verb here.",
            "And that means also ran in Spanish is the Creo.",
            "So the task is here to know that these words mean the same thing in DB pedia ontology.",
            "And we're taking this problem here in a multilingual setting.",
            "So we're using.",
            "Quality data set which is known for over there for many years, and there was already a 8 addition yesterday, so they called 7 for instance.",
            "Had questions in eight languages and there were 215 training instances.",
            "An 44 test instances and in qualitative if I'm not mistaken the numbers are almost the same and there can be added additional languages as well.",
            "So much."
        ],
        [
            "Additional For our work is that we want to tackle this problem in a multilingual setting because we know that other languages except English don't have that many good systems out there, so our motivation for this is syntax, which is dependency parse tree with the effort within the universal dependencies.",
            "Now there are so relatively well dependencies, parts 3 generators in universal dependencies for over 50 languages now.",
            "And you can contribute to these as well if you are native speaker of your own country or even a region in your country.",
            "So if you look at the dependencies patches here.",
            "Do you see similarities?",
            "They look the same because that was the effort with universal dependencies that there will be same set of relations like any subject and the object for every language that is.",
            "That is, in the universal dependencies and the same post tags everything that we can think of it.",
            "Will be there and there's only difference here is that hot is an auxiliary verb, but that doesn't affect the semantics in here.",
            "So our motivation here is that if we have the same syntax, if we have the same query to map the language, which shouldn't be a problem anymore, 'cause if we are doing semantic parsing, we based on some syntactic knowledge here which is universal dependencies and that is our motivation.",
            "Next motivation is of course."
        ],
        [
            "So we have seen from our.",
            "Morning session that Pedia has been already 1010 years and it's very good knowledge base now.",
            "It supports more than 125 languages.",
            "There is data out there and it's a freely available.",
            "There are over 1000 object properties and over again 1000 data type properties.",
            "So motivation is now we have a universal dependencies.",
            "We have same syntax for all languages and we have same data.",
            "So why not bridge the gap and then create one model that can support every language that we want to?"
        ],
        [
            "Do it so before we dive into the model and the details.",
            "I would like to explain a couple of preliminaries which are logical forms and the symbolic semantic composition.",
            "That gives us the semantics of a sentence.",
            "Using dependencies partially and very shortly about factor graphs, which is the model that we used to train our supervised models."
        ],
        [
            "So those logical forms we use.",
            "Dudes, which stands for dependency based under special discourse representation structures.",
            "So why we use dudes is that it is a formalism for specifying meaning representations.",
            "It is flexible in terms of the order of application.",
            "If you if you compare this, for instance to CCG, this G takes inputs based on the order, so the order matters.",
            "Here the order doesn't matter.",
            "And it is based on the symbolic dependencies.",
            "In the case of dependency parse tree.",
            "So this fits our task perfectly, so I will give you 5 do times that we use so resource."
        ],
        [
            "Suits research dude, so do so you can think of it as a box that has a couple of things inside.",
            "So there's a main variable.",
            "There's a set of variables can be empty.",
            "These are projection variables, and there's a label.",
            "And then they arrest and slots.",
            "So resource uses for expressing individuals such as DB pedia.",
            "It doesn't have any projection variables, it has a label eupedia.",
            "There's a property dudes, which is basically the DPD object properties.",
            "It has two slots available.",
            "Subject and object.",
            "We can define this as however we want.",
            "And there is no projection variable here.",
            "The label is has to be initialized with a given object property.",
            "Then there's a class dude.",
            "Which is since we're using the pedia.",
            "We we initialize this with the RDF type and given the ontology class and there's a special case of this class is called restriction classes, which is for something expressing meaning for something like Swedish.",
            "Swedish means it cannot be expressed with one UI.",
            "It has to be 2, one is.",
            "It's a country and the resource should be Sweden.",
            "And then there's a special dude called query variable dudes, which is the one that holds the projection variable.",
            "It doesn't have any slots, it doesn't have any.",
            "You are.",
            "I just have a variable and that is the.",
            "These are the five types that we use."
        ],
        [
            "So now the most important part of our approach is semantic composition.",
            "How we do this along the path string?",
            "So we are given an input such as question and its corresponding dependency parse tree.",
            "The idea is that if we have some Oracle that gives us this assignments here, so each node here has two assignments.",
            "One is the do type and the other one is the UI.",
            "So if you look closer, created has a UI author Anne.",
            "It's a property type dudes.",
            "Wikipedia has resource tools and it has a resource UI who has a query variable do it, but it doesn't have any UI because semantically doesn't hold meaning in terms of your eyes."
        ],
        [
            "Now our approach is semantic composition using bottom up approach.",
            "So we merge nodes along the parse tree using their edges.",
            "So if we want to merge these together this node Wikipedia and created, we can do this along the edge.",
            "And if we merge it, the idea is if we merge this, let's say to the.",
            "To the slow DB object that the Y here will be replaced with this and the resulting.",
            "Thing would be that Wikipedia would be has been replaced with Y and this is the resulting dude for these two merge nodes and if we apply the same procedure to the other edge here, which is an subject."
        ],
        [
            "And we merge that with other one.",
            "So here there who has this and the result resulting one was this.",
            "And we replace X, but this one has a projection variable.",
            "So which means this would be added here, but it doesn't have any label to replace."
        ],
        [
            "And this is what we get as a result.",
            "So our box has a projection variable.",
            "It has a property, there's a resource initialized, and this is the.",
            "Resulting dude that has been merged.",
            "So as I said before, dudes have.",
            "The order of application doesn't matter, so we could have applied 1st and subject then the object.",
            "So in this case we will get the same.",
            "Dude."
        ],
        [
            "As a result, so if we look into the.",
            "What we get as a logical form in terms of a different representation?",
            "We have some Wikipedia in a subject position.",
            "There's a author, and there's an X, which is a projection variable."
        ],
        [
            "So if we convert this into sparkle, it's pretty straightforward.",
            "If you have projection variable that ends up in an ASK query.",
            "If there is no projection variable, it ends up in a story, ends up in a select query.",
            "If there is a projection variable, it ends up in a.",
            "There's no, and then it ends up in the ass query, so here there's a project variable X and then we get the query select X and so on.",
            "So this is our bottom up approach for semantic composition."
        ],
        [
            "And we use.",
            "Model based on the factor graph.",
            "We train supervised models for each language, but we our model has the same structure for the input is just a question in the in the in that language and we use the same features.",
            "We use the same so we have one model for all three languages.",
            "The idea about factor graph is that given observed variables such as this, the idea is learn such a model that maximizes the assignment such as hidden variables which would be knowledge based IDs and logical forms and slot assignments which were when we define them as an subject before.",
            "But we can add any label here we can add 2123 ABC.",
            "So the idea is to learn such assignments so the model maximizes with the given features.",
            "Now the approach."
        ],
        [
            "As I said before, we use dependencies partially for this.",
            "It's language independent and the model is based on factor graphs.",
            "We use sample rank to optimize our features, and we use inferencing MCMC strategy."
        ],
        [
            "So this is the overview the the given the independence partial.",
            "We assign these assignments based on the model and then we apply bottom up approach and then we get the query.",
            "So our inference model is based on a set decoding, so so the idea is given initial."
        ],
        [
            "Wait so state is a partial solution.",
            "In this case it's initial because there is no solution here.",
            "The idea is to apply a couple multiple steps such as M steps and to get this state here.",
            "So this state here is a sample state.",
            "And we we di D is to apply this M steps and then get the highest ranking."
        ],
        [
            "So our inferencing is based on two parts.",
            "One is linking to KBR, the one is query construction.",
            "Linking to KB is very basic, like you can think of it as entity dissemination or linking project where the idea is to.",
            "The objective is to compare set of UI to expect your eyes, which is just looking up the index and then assigning the your eyes.",
            "The query construction is basically get the query an compared to the expected query."
        ],
        [
            "So the linking to KB.",
            "If we can think of it, the exploration is based on the edge where we're looking for this and we now try to assign two your eyes for this both nodes for that age.",
            "So we take dimensions we look up in our index and we retrieve all possible UI players.",
            "Next thing we do is we query those triples so this is sort of intermediate triple here and if we query that this triple here.",
            "Doesn't end up in a.",
            "There's no data in DB pedia, so there's no need for that to be in as a state.",
            "So we take this one here and then.",
            "We say that the resource here is on the first position, like subject position."
        ],
        [
            "And if he if we look at it as a whole, it looks like a chain.",
            "So we start from a zero solution.",
            "We move on along the chain.",
            "And on each step will have multiple partial solutions.",
            "We ranked him.",
            "We take the top K and we move along to the next step, son attend.",
            "We get some sample state which is the maximum one.",
            "The query construction is.",
            "Tai"
        ],
        [
            "Xinput states from the previous sampling step and if you see here these are already assigned for this edge.",
            "So the missing thing is.",
            "So the the property dudes can have two slots, so one of one of them is filled.",
            "So now the idea is to explore for the potential other ones."
        ],
        [
            "So here we see that two was empty and then we can assign every possible.",
            "Every possible do type here.",
            "So due to the time constraints I have to move on a bit faster, so this was missing and then we can assign this as a.",
            "Next possible step."
        ],
        [
            "The features are based on the edges.",
            "We use them as POS tags, dependence relations.",
            "We use lot numbers, knowledge based IDs, domain restrictions.",
            "Similarity between lemma and the knowledge base UI and so on."
        ],
        [
            "So one of the most important part is the addressing the lexical gap.",
            "And we did this by have to go a bit faster.",
            "We did this by."
        ],
        [
            "Training.",
            "Graph embeddings we use them at all, which is a system that gives us lexecon, and then we have the pedia labels when we merge all of them together, we see that immortality is not good enough for English and Spanish, but it's only good.",
            "Relatively good for on the English.",
            "Then when we assign.",
            "Lexicon from word embeddings using the similarity measure between award and the PRI, we see that or our model can increase quite a lot."
        ],
        [
            "So now the actual question answering is first we applied the Oracle based approach where we where we guided the solutions using the objective function and that gives us our upper bound.",
            "So we cannot reach 100% cause the dependency parsers can be wrong.",
            "Sometimes there's no way of exploring the edge because those that edge doesn't exist between two ur eyes.",
            "So this is our upper bound.",
            "This is the maximum we can reach with the current setting then."
        ],
        [
            "Our actual test and we see that for English we close the gap here.",
            "So this part here using the pedia labels imoto land embeddings.",
            "We see that from 30% we can reach 26%, which is quite good.",
            "And.",
            "On top of that, what I didn't mention is we also added manually added Dictionary dictionary, which is the expected lexecon.",
            "So we wanted to see what is affecting our question answering system and we see that if we train with the Oracle it's 51% and if we actually apply the test we can reach to 34%.",
            "So the idea is to bring along this results to this because we're here, we're using dictionary, which has been provided manually."
        ],
        [
            "So we our system makes of course a lot of errors.",
            "Most of the errors were assigning the property to the given.",
            "So for instance so I I'm already sharing the slides, so I will skip this."
        ],
        [
            "Here is now if we have the same dependencies partial, we can train multilingual question answering system because the there's a knowledge base, there is a syntactic parse trees and so it has to be multi language independent."
        ],
        [
            "So we can do a lot of stuff for future directions like focusing on those, like improving the query type classification, apply different ranking functions maybe.",
            "And of course, adding more lexicon always improves question answering system, so that's what we have evaluated already.",
            "But there's other resources of for that to do that, and maybe paraphrasing training instances can be also another way of creating more training data, because we're only training with hundred instances here.",
            "OK.",
            "Thank you very much."
        ],
        [
            "Thank you.",
            "So I understand your research was more on syntax, but I have a question on lexical semantics.",
            "So how you get from created from created to author?",
            "So you you are a bit fast on this.",
            "You mentioned word embeddings, but I suppose that word embeddings can give you lots of candidates, creativity, creation, etc.",
            "So how do you choose from all those?",
            "So we have three sources of lexical knowledge.",
            "One is DPD.",
            "As I said, the other one is Emma told him at all.",
            "Is the system that creates such data from learning from Wikipedia text to the DB pedia ontology triples.",
            "It already gives such information, but we also use word embedding similarities.",
            "We have.",
            "We have defined some thresholds where we cut out some parts.",
            "So if you look here.",
            "Total population has similarity to population total with 100% and then it decreases by looking at other ones.",
            "So we have some set up some thresholds.",
            "And then we don't take everything.",
            "Of course, as you have mentioned.",
            "And our approach is not based on the syntax but actually based on the semantics semantic parsing.",
            "So we take the syntax directly from universal dependencies, so we don't need to work on the syntax, but we focus on the semantic composition of different lexical.",
            "Lexico hi, I'm sorry if I missed something, but which benchmark do you use?",
            "We use quotes question answering for linked data.",
            "OK QRD yeah OK thank you.",
            "Well I will just something as well.",
            "By the way I love the name of the use of your algorithm so these logical form in the sample you put you have linguistic property created.",
            "So when you search in the ontology for mapping the property, do you focus on?",
            "Only on property, so you also look for other type of entities like right?",
            "It can be maybe a class created.",
            "The idea is we explore everything.",
            "So if we want to assign property dude for the word created we only search in our that index.",
            "But we also create possible ways.",
            "If there's a individual called created X and if that's in our index.",
            "We also create that as well.",
            "So our model learns that given the post stack of created, that means it's most like to be a property rather than being individual.",
            "'cause of the training data?",
            "OK, and there's a final question regarding the experiments, how it compares to a state of there.",
            "I guess it's very difficult to compare to other multilingual systems because there are not many.",
            "But in the case of English, yeah, in the case of comparison is that we have everything automatically done.",
            "So there are a lot of approaches out there that right?",
            "For instance, dependency.",
            "I know your work there was a last year that you you provide some graph patterns, that sort of comes with the semantics.",
            "In our case we don't do that, so we're trying to learn that automatically.",
            "Because we think that we cannot do this for every language an if the if there are many graph patterns, we cannot write them manually, so our work is sort of pushing everything automatically and this is as far as we get with our first iteration.",
            "OK, that's that's good.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah hello everyone, my name is Charlotte Hakima, PhD student from Semantic Computing group.",
                    "label": 1
                },
                {
                    "sent": "Today I'm going to be presenting multilingual semantic parsing approach for qual data set.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would we have?",
                    "label": 0
                },
                {
                    "sent": "I think everyone has smartphone and every smartphone nowadays have a virtual assistance and these are just some of the big ones that are out there.",
                    "label": 0
                },
                {
                    "sent": "But what they can do is they can do various tasks such as creating a reminders booking at the restaurant.",
                    "label": 0
                },
                {
                    "sent": "And ordering stuff like in in terms of Alexa.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From Amazon, but how good are they for factual question answering?",
                    "label": 1
                },
                {
                    "sent": "And especially in a multilingual setting.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have a question who created Wikipedia in English, see replies that the Founders are Jimmy Wales and Larry Sanger, which is the right answer.",
                    "label": 0
                },
                {
                    "sent": "And if we ask in German about hot Wikipedia grounded.",
                    "label": 0
                },
                {
                    "sent": "It gives me as an answer.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia and it's 2001, which is maybe creation, starting date of the Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So which is not the right answer?",
                    "label": 0
                },
                {
                    "sent": "How about in Spanish?",
                    "label": 0
                },
                {
                    "sent": "Can clear Wikipedia?",
                    "label": 0
                },
                {
                    "sent": "It gives me Wikipedia pages, but there is.",
                    "label": 0
                },
                {
                    "sent": "Jimmy Wales somewhere as a link, but not as a direct answer so we can see that even though this virtual system work quite well.",
                    "label": 0
                },
                {
                    "sent": "But when we switch to another language except English, they don't work that well in this setting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So essentially we are tackling the same problem here.",
                    "label": 0
                },
                {
                    "sent": "If you have a question such as this, who created Wikipedia, the task is to create a sparkle query based on DB pedia such as this Wikipedia author X and we are looking for the X.",
                    "label": 1
                },
                {
                    "sent": "And the same problem can be shown in other languages, as is because the.",
                    "label": 0
                },
                {
                    "sent": "The GPS is same data for every language, so all the challenge here is to map these things.",
                    "label": 0
                },
                {
                    "sent": "OPS.",
                    "label": 0
                },
                {
                    "sent": "Where the created here means author in the query.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia is the same entity in all languages and the grounded or hot hot kingroot net is the verb here.",
                    "label": 0
                },
                {
                    "sent": "And that means also ran in Spanish is the Creo.",
                    "label": 0
                },
                {
                    "sent": "So the task is here to know that these words mean the same thing in DB pedia ontology.",
                    "label": 0
                },
                {
                    "sent": "And we're taking this problem here in a multilingual setting.",
                    "label": 0
                },
                {
                    "sent": "So we're using.",
                    "label": 0
                },
                {
                    "sent": "Quality data set which is known for over there for many years, and there was already a 8 addition yesterday, so they called 7 for instance.",
                    "label": 0
                },
                {
                    "sent": "Had questions in eight languages and there were 215 training instances.",
                    "label": 0
                },
                {
                    "sent": "An 44 test instances and in qualitative if I'm not mistaken the numbers are almost the same and there can be added additional languages as well.",
                    "label": 1
                },
                {
                    "sent": "So much.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Additional For our work is that we want to tackle this problem in a multilingual setting because we know that other languages except English don't have that many good systems out there, so our motivation for this is syntax, which is dependency parse tree with the effort within the universal dependencies.",
                    "label": 0
                },
                {
                    "sent": "Now there are so relatively well dependencies, parts 3 generators in universal dependencies for over 50 languages now.",
                    "label": 1
                },
                {
                    "sent": "And you can contribute to these as well if you are native speaker of your own country or even a region in your country.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the dependencies patches here.",
                    "label": 0
                },
                {
                    "sent": "Do you see similarities?",
                    "label": 0
                },
                {
                    "sent": "They look the same because that was the effort with universal dependencies that there will be same set of relations like any subject and the object for every language that is.",
                    "label": 0
                },
                {
                    "sent": "That is, in the universal dependencies and the same post tags everything that we can think of it.",
                    "label": 0
                },
                {
                    "sent": "Will be there and there's only difference here is that hot is an auxiliary verb, but that doesn't affect the semantics in here.",
                    "label": 0
                },
                {
                    "sent": "So our motivation here is that if we have the same syntax, if we have the same query to map the language, which shouldn't be a problem anymore, 'cause if we are doing semantic parsing, we based on some syntactic knowledge here which is universal dependencies and that is our motivation.",
                    "label": 0
                },
                {
                    "sent": "Next motivation is of course.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have seen from our.",
                    "label": 0
                },
                {
                    "sent": "Morning session that Pedia has been already 1010 years and it's very good knowledge base now.",
                    "label": 1
                },
                {
                    "sent": "It supports more than 125 languages.",
                    "label": 1
                },
                {
                    "sent": "There is data out there and it's a freely available.",
                    "label": 1
                },
                {
                    "sent": "There are over 1000 object properties and over again 1000 data type properties.",
                    "label": 0
                },
                {
                    "sent": "So motivation is now we have a universal dependencies.",
                    "label": 0
                },
                {
                    "sent": "We have same syntax for all languages and we have same data.",
                    "label": 0
                },
                {
                    "sent": "So why not bridge the gap and then create one model that can support every language that we want to?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do it so before we dive into the model and the details.",
                    "label": 0
                },
                {
                    "sent": "I would like to explain a couple of preliminaries which are logical forms and the symbolic semantic composition.",
                    "label": 1
                },
                {
                    "sent": "That gives us the semantics of a sentence.",
                    "label": 1
                },
                {
                    "sent": "Using dependencies partially and very shortly about factor graphs, which is the model that we used to train our supervised models.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So those logical forms we use.",
                    "label": 0
                },
                {
                    "sent": "Dudes, which stands for dependency based under special discourse representation structures.",
                    "label": 1
                },
                {
                    "sent": "So why we use dudes is that it is a formalism for specifying meaning representations.",
                    "label": 1
                },
                {
                    "sent": "It is flexible in terms of the order of application.",
                    "label": 1
                },
                {
                    "sent": "If you if you compare this, for instance to CCG, this G takes inputs based on the order, so the order matters.",
                    "label": 0
                },
                {
                    "sent": "Here the order doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "And it is based on the symbolic dependencies.",
                    "label": 0
                },
                {
                    "sent": "In the case of dependency parse tree.",
                    "label": 0
                },
                {
                    "sent": "So this fits our task perfectly, so I will give you 5 do times that we use so resource.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suits research dude, so do so you can think of it as a box that has a couple of things inside.",
                    "label": 0
                },
                {
                    "sent": "So there's a main variable.",
                    "label": 0
                },
                {
                    "sent": "There's a set of variables can be empty.",
                    "label": 1
                },
                {
                    "sent": "These are projection variables, and there's a label.",
                    "label": 0
                },
                {
                    "sent": "And then they arrest and slots.",
                    "label": 0
                },
                {
                    "sent": "So resource uses for expressing individuals such as DB pedia.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have any projection variables, it has a label eupedia.",
                    "label": 0
                },
                {
                    "sent": "There's a property dudes, which is basically the DPD object properties.",
                    "label": 0
                },
                {
                    "sent": "It has two slots available.",
                    "label": 0
                },
                {
                    "sent": "Subject and object.",
                    "label": 0
                },
                {
                    "sent": "We can define this as however we want.",
                    "label": 0
                },
                {
                    "sent": "And there is no projection variable here.",
                    "label": 1
                },
                {
                    "sent": "The label is has to be initialized with a given object property.",
                    "label": 0
                },
                {
                    "sent": "Then there's a class dude.",
                    "label": 0
                },
                {
                    "sent": "Which is since we're using the pedia.",
                    "label": 0
                },
                {
                    "sent": "We we initialize this with the RDF type and given the ontology class and there's a special case of this class is called restriction classes, which is for something expressing meaning for something like Swedish.",
                    "label": 0
                },
                {
                    "sent": "Swedish means it cannot be expressed with one UI.",
                    "label": 0
                },
                {
                    "sent": "It has to be 2, one is.",
                    "label": 0
                },
                {
                    "sent": "It's a country and the resource should be Sweden.",
                    "label": 0
                },
                {
                    "sent": "And then there's a special dude called query variable dudes, which is the one that holds the projection variable.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have any slots, it doesn't have any.",
                    "label": 0
                },
                {
                    "sent": "You are.",
                    "label": 0
                },
                {
                    "sent": "I just have a variable and that is the.",
                    "label": 0
                },
                {
                    "sent": "These are the five types that we use.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the most important part of our approach is semantic composition.",
                    "label": 1
                },
                {
                    "sent": "How we do this along the path string?",
                    "label": 0
                },
                {
                    "sent": "So we are given an input such as question and its corresponding dependency parse tree.",
                    "label": 0
                },
                {
                    "sent": "The idea is that if we have some Oracle that gives us this assignments here, so each node here has two assignments.",
                    "label": 0
                },
                {
                    "sent": "One is the do type and the other one is the UI.",
                    "label": 0
                },
                {
                    "sent": "So if you look closer, created has a UI author Anne.",
                    "label": 0
                },
                {
                    "sent": "It's a property type dudes.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia has resource tools and it has a resource UI who has a query variable do it, but it doesn't have any UI because semantically doesn't hold meaning in terms of your eyes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now our approach is semantic composition using bottom up approach.",
                    "label": 1
                },
                {
                    "sent": "So we merge nodes along the parse tree using their edges.",
                    "label": 0
                },
                {
                    "sent": "So if we want to merge these together this node Wikipedia and created, we can do this along the edge.",
                    "label": 0
                },
                {
                    "sent": "And if we merge it, the idea is if we merge this, let's say to the.",
                    "label": 0
                },
                {
                    "sent": "To the slow DB object that the Y here will be replaced with this and the resulting.",
                    "label": 0
                },
                {
                    "sent": "Thing would be that Wikipedia would be has been replaced with Y and this is the resulting dude for these two merge nodes and if we apply the same procedure to the other edge here, which is an subject.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we merge that with other one.",
                    "label": 0
                },
                {
                    "sent": "So here there who has this and the result resulting one was this.",
                    "label": 0
                },
                {
                    "sent": "And we replace X, but this one has a projection variable.",
                    "label": 0
                },
                {
                    "sent": "So which means this would be added here, but it doesn't have any label to replace.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what we get as a result.",
                    "label": 0
                },
                {
                    "sent": "So our box has a projection variable.",
                    "label": 0
                },
                {
                    "sent": "It has a property, there's a resource initialized, and this is the.",
                    "label": 0
                },
                {
                    "sent": "Resulting dude that has been merged.",
                    "label": 0
                },
                {
                    "sent": "So as I said before, dudes have.",
                    "label": 0
                },
                {
                    "sent": "The order of application doesn't matter, so we could have applied 1st and subject then the object.",
                    "label": 0
                },
                {
                    "sent": "So in this case we will get the same.",
                    "label": 0
                },
                {
                    "sent": "Dude.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a result, so if we look into the.",
                    "label": 0
                },
                {
                    "sent": "What we get as a logical form in terms of a different representation?",
                    "label": 0
                },
                {
                    "sent": "We have some Wikipedia in a subject position.",
                    "label": 0
                },
                {
                    "sent": "There's a author, and there's an X, which is a projection variable.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we convert this into sparkle, it's pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "If you have projection variable that ends up in an ASK query.",
                    "label": 0
                },
                {
                    "sent": "If there is no projection variable, it ends up in a story, ends up in a select query.",
                    "label": 0
                },
                {
                    "sent": "If there is a projection variable, it ends up in a.",
                    "label": 0
                },
                {
                    "sent": "There's no, and then it ends up in the ass query, so here there's a project variable X and then we get the query select X and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is our bottom up approach for semantic composition.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we use.",
                    "label": 0
                },
                {
                    "sent": "Model based on the factor graph.",
                    "label": 0
                },
                {
                    "sent": "We train supervised models for each language, but we our model has the same structure for the input is just a question in the in the in that language and we use the same features.",
                    "label": 0
                },
                {
                    "sent": "We use the same so we have one model for all three languages.",
                    "label": 0
                },
                {
                    "sent": "The idea about factor graph is that given observed variables such as this, the idea is learn such a model that maximizes the assignment such as hidden variables which would be knowledge based IDs and logical forms and slot assignments which were when we define them as an subject before.",
                    "label": 1
                },
                {
                    "sent": "But we can add any label here we can add 2123 ABC.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to learn such assignments so the model maximizes with the given features.",
                    "label": 0
                },
                {
                    "sent": "Now the approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said before, we use dependencies partially for this.",
                    "label": 0
                },
                {
                    "sent": "It's language independent and the model is based on factor graphs.",
                    "label": 1
                },
                {
                    "sent": "We use sample rank to optimize our features, and we use inferencing MCMC strategy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the overview the the given the independence partial.",
                    "label": 0
                },
                {
                    "sent": "We assign these assignments based on the model and then we apply bottom up approach and then we get the query.",
                    "label": 0
                },
                {
                    "sent": "So our inference model is based on a set decoding, so so the idea is given initial.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait so state is a partial solution.",
                    "label": 0
                },
                {
                    "sent": "In this case it's initial because there is no solution here.",
                    "label": 0
                },
                {
                    "sent": "The idea is to apply a couple multiple steps such as M steps and to get this state here.",
                    "label": 0
                },
                {
                    "sent": "So this state here is a sample state.",
                    "label": 0
                },
                {
                    "sent": "And we we di D is to apply this M steps and then get the highest ranking.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our inferencing is based on two parts.",
                    "label": 0
                },
                {
                    "sent": "One is linking to KBR, the one is query construction.",
                    "label": 1
                },
                {
                    "sent": "Linking to KB is very basic, like you can think of it as entity dissemination or linking project where the idea is to.",
                    "label": 1
                },
                {
                    "sent": "The objective is to compare set of UI to expect your eyes, which is just looking up the index and then assigning the your eyes.",
                    "label": 0
                },
                {
                    "sent": "The query construction is basically get the query an compared to the expected query.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the linking to KB.",
                    "label": 1
                },
                {
                    "sent": "If we can think of it, the exploration is based on the edge where we're looking for this and we now try to assign two your eyes for this both nodes for that age.",
                    "label": 0
                },
                {
                    "sent": "So we take dimensions we look up in our index and we retrieve all possible UI players.",
                    "label": 0
                },
                {
                    "sent": "Next thing we do is we query those triples so this is sort of intermediate triple here and if we query that this triple here.",
                    "label": 0
                },
                {
                    "sent": "Doesn't end up in a.",
                    "label": 0
                },
                {
                    "sent": "There's no data in DB pedia, so there's no need for that to be in as a state.",
                    "label": 0
                },
                {
                    "sent": "So we take this one here and then.",
                    "label": 0
                },
                {
                    "sent": "We say that the resource here is on the first position, like subject position.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if he if we look at it as a whole, it looks like a chain.",
                    "label": 0
                },
                {
                    "sent": "So we start from a zero solution.",
                    "label": 0
                },
                {
                    "sent": "We move on along the chain.",
                    "label": 0
                },
                {
                    "sent": "And on each step will have multiple partial solutions.",
                    "label": 0
                },
                {
                    "sent": "We ranked him.",
                    "label": 0
                },
                {
                    "sent": "We take the top K and we move along to the next step, son attend.",
                    "label": 0
                },
                {
                    "sent": "We get some sample state which is the maximum one.",
                    "label": 0
                },
                {
                    "sent": "The query construction is.",
                    "label": 0
                },
                {
                    "sent": "Tai",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Xinput states from the previous sampling step and if you see here these are already assigned for this edge.",
                    "label": 1
                },
                {
                    "sent": "So the missing thing is.",
                    "label": 0
                },
                {
                    "sent": "So the the property dudes can have two slots, so one of one of them is filled.",
                    "label": 0
                },
                {
                    "sent": "So now the idea is to explore for the potential other ones.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see that two was empty and then we can assign every possible.",
                    "label": 0
                },
                {
                    "sent": "Every possible do type here.",
                    "label": 0
                },
                {
                    "sent": "So due to the time constraints I have to move on a bit faster, so this was missing and then we can assign this as a.",
                    "label": 0
                },
                {
                    "sent": "Next possible step.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The features are based on the edges.",
                    "label": 0
                },
                {
                    "sent": "We use them as POS tags, dependence relations.",
                    "label": 0
                },
                {
                    "sent": "We use lot numbers, knowledge based IDs, domain restrictions.",
                    "label": 0
                },
                {
                    "sent": "Similarity between lemma and the knowledge base UI and so on.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the most important part is the addressing the lexical gap.",
                    "label": 1
                },
                {
                    "sent": "And we did this by have to go a bit faster.",
                    "label": 0
                },
                {
                    "sent": "We did this by.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "Graph embeddings we use them at all, which is a system that gives us lexecon, and then we have the pedia labels when we merge all of them together, we see that immortality is not good enough for English and Spanish, but it's only good.",
                    "label": 0
                },
                {
                    "sent": "Relatively good for on the English.",
                    "label": 0
                },
                {
                    "sent": "Then when we assign.",
                    "label": 0
                },
                {
                    "sent": "Lexicon from word embeddings using the similarity measure between award and the PRI, we see that or our model can increase quite a lot.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the actual question answering is first we applied the Oracle based approach where we where we guided the solutions using the objective function and that gives us our upper bound.",
                    "label": 0
                },
                {
                    "sent": "So we cannot reach 100% cause the dependency parsers can be wrong.",
                    "label": 0
                },
                {
                    "sent": "Sometimes there's no way of exploring the edge because those that edge doesn't exist between two ur eyes.",
                    "label": 0
                },
                {
                    "sent": "So this is our upper bound.",
                    "label": 0
                },
                {
                    "sent": "This is the maximum we can reach with the current setting then.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our actual test and we see that for English we close the gap here.",
                    "label": 0
                },
                {
                    "sent": "So this part here using the pedia labels imoto land embeddings.",
                    "label": 0
                },
                {
                    "sent": "We see that from 30% we can reach 26%, which is quite good.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "On top of that, what I didn't mention is we also added manually added Dictionary dictionary, which is the expected lexecon.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to see what is affecting our question answering system and we see that if we train with the Oracle it's 51% and if we actually apply the test we can reach to 34%.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to bring along this results to this because we're here, we're using dictionary, which has been provided manually.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we our system makes of course a lot of errors.",
                    "label": 0
                },
                {
                    "sent": "Most of the errors were assigning the property to the given.",
                    "label": 0
                },
                {
                    "sent": "So for instance so I I'm already sharing the slides, so I will skip this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is now if we have the same dependencies partial, we can train multilingual question answering system because the there's a knowledge base, there is a syntactic parse trees and so it has to be multi language independent.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do a lot of stuff for future directions like focusing on those, like improving the query type classification, apply different ranking functions maybe.",
                    "label": 0
                },
                {
                    "sent": "And of course, adding more lexicon always improves question answering system, so that's what we have evaluated already.",
                    "label": 0
                },
                {
                    "sent": "But there's other resources of for that to do that, and maybe paraphrasing training instances can be also another way of creating more training data, because we're only training with hundred instances here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So I understand your research was more on syntax, but I have a question on lexical semantics.",
                    "label": 0
                },
                {
                    "sent": "So how you get from created from created to author?",
                    "label": 0
                },
                {
                    "sent": "So you you are a bit fast on this.",
                    "label": 0
                },
                {
                    "sent": "You mentioned word embeddings, but I suppose that word embeddings can give you lots of candidates, creativity, creation, etc.",
                    "label": 0
                },
                {
                    "sent": "So how do you choose from all those?",
                    "label": 0
                },
                {
                    "sent": "So we have three sources of lexical knowledge.",
                    "label": 1
                },
                {
                    "sent": "One is DPD.",
                    "label": 0
                },
                {
                    "sent": "As I said, the other one is Emma told him at all.",
                    "label": 0
                },
                {
                    "sent": "Is the system that creates such data from learning from Wikipedia text to the DB pedia ontology triples.",
                    "label": 0
                },
                {
                    "sent": "It already gives such information, but we also use word embedding similarities.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "We have defined some thresholds where we cut out some parts.",
                    "label": 0
                },
                {
                    "sent": "So if you look here.",
                    "label": 0
                },
                {
                    "sent": "Total population has similarity to population total with 100% and then it decreases by looking at other ones.",
                    "label": 0
                },
                {
                    "sent": "So we have some set up some thresholds.",
                    "label": 0
                },
                {
                    "sent": "And then we don't take everything.",
                    "label": 0
                },
                {
                    "sent": "Of course, as you have mentioned.",
                    "label": 0
                },
                {
                    "sent": "And our approach is not based on the syntax but actually based on the semantics semantic parsing.",
                    "label": 0
                },
                {
                    "sent": "So we take the syntax directly from universal dependencies, so we don't need to work on the syntax, but we focus on the semantic composition of different lexical.",
                    "label": 0
                },
                {
                    "sent": "Lexico hi, I'm sorry if I missed something, but which benchmark do you use?",
                    "label": 0
                },
                {
                    "sent": "We use quotes question answering for linked data.",
                    "label": 0
                },
                {
                    "sent": "OK QRD yeah OK thank you.",
                    "label": 0
                },
                {
                    "sent": "Well I will just something as well.",
                    "label": 0
                },
                {
                    "sent": "By the way I love the name of the use of your algorithm so these logical form in the sample you put you have linguistic property created.",
                    "label": 0
                },
                {
                    "sent": "So when you search in the ontology for mapping the property, do you focus on?",
                    "label": 0
                },
                {
                    "sent": "Only on property, so you also look for other type of entities like right?",
                    "label": 0
                },
                {
                    "sent": "It can be maybe a class created.",
                    "label": 0
                },
                {
                    "sent": "The idea is we explore everything.",
                    "label": 0
                },
                {
                    "sent": "So if we want to assign property dude for the word created we only search in our that index.",
                    "label": 0
                },
                {
                    "sent": "But we also create possible ways.",
                    "label": 0
                },
                {
                    "sent": "If there's a individual called created X and if that's in our index.",
                    "label": 0
                },
                {
                    "sent": "We also create that as well.",
                    "label": 0
                },
                {
                    "sent": "So our model learns that given the post stack of created, that means it's most like to be a property rather than being individual.",
                    "label": 0
                },
                {
                    "sent": "'cause of the training data?",
                    "label": 1
                },
                {
                    "sent": "OK, and there's a final question regarding the experiments, how it compares to a state of there.",
                    "label": 0
                },
                {
                    "sent": "I guess it's very difficult to compare to other multilingual systems because there are not many.",
                    "label": 0
                },
                {
                    "sent": "But in the case of English, yeah, in the case of comparison is that we have everything automatically done.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of approaches out there that right?",
                    "label": 0
                },
                {
                    "sent": "For instance, dependency.",
                    "label": 1
                },
                {
                    "sent": "I know your work there was a last year that you you provide some graph patterns, that sort of comes with the semantics.",
                    "label": 0
                },
                {
                    "sent": "In our case we don't do that, so we're trying to learn that automatically.",
                    "label": 0
                },
                {
                    "sent": "Because we think that we cannot do this for every language an if the if there are many graph patterns, we cannot write them manually, so our work is sort of pushing everything automatically and this is as far as we get with our first iteration.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's good.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}