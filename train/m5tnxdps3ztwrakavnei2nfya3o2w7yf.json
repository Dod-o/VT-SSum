{
    "id": "m5tnxdps3ztwrakavnei2nfya3o2w7yf",
    "title": "Hierarchical label queries with data-dependent partitions",
    "info": {
        "author": [
            "Samory Kpotufe, Department of Operations Research and Financial Engineering, Princeton University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_kpotufe_data_dependent_partitions/",
    "segmentation": [
        [
            "Hi everyone, thank you so this is joint work with the Ruth Urner who just Spokane scheiben David so."
        ],
        [
            "It's essentially also about the sort of active learning, so we are in a setting where classification where labels in classification are expensive and the procedure we analyzing is a process that is sort of an old procedure which essentially combines aspects of active learning and aspects of the class option.",
            "So essentially the closer assumption being simply that labels don't change much on the cluster.",
            "And then he uses a partitioning tree.",
            "So the main motivation is that this algorithm is implementable.",
            "It turns out that a lot of theoretical active learning procedures sort of hard to implement, and this is one of the few that is easy to implement, but it's sort of slightly hard to analyze.",
            "There's been only a few analysis of it so far, and So what?"
        ],
        [
            "We want to do is give a refund analysis in more practical settings.",
            "It's practical, but we want the settings in which we analyze it to be fully practical to be as practical as possible.",
            "And so here I'll just give you an idea of what the procedure is.",
            "The procedure was first formalized.",
            "It's a very natural procedure, first formalized by Dasgupta, and so in 2008, and so essentially imagine you have access to a partition tree and you have data.",
            "You partition the data first, then you.",
            "Query a few labels in each cell of the partition.",
            "If the cell happens to be pure in labels, then you fully label it.",
            "If he happens to be on pure, then you refine the cell.",
            "You refine the partition, you partition it further and then you keep repeating this an at the end you try to you try to label the whole data with error less than epsilon and then you can pass that over to a supervised learner and so it's a very simple idea.",
            "After this paper is it seems like.",
            "Practitioners use it because I mean these people was at least on Google.",
            "It was.",
            "It has like 200 or so citations.",
            "So the overall appeal again is that it's implementable.",
            "An.",
            "Another appeal is that it doesn't really require the closer assumption to hold the cluster assumption, only need to hold approximately, and it's also safe if the cluster assumption does not hold it.",
            "Still, it will still not mislabel by more than epsilon, so it's safe in that sense so."
        ],
        [
            "What's the labeling goal delivering goal is that we want to request less than one over epsilon square labels, where that's being essentially the label complexity of agnostic agnostic learning.",
            "And the guarantees on label queries, and this guarantees are essentially the same guarantees that were shown before but under different assumptions.",
            "But the guarantees take this form from T over epsilon to one over epsilon squared, and the guarantees, so it goes in this range.",
            "T aversion to one over Epsilon Square and the range sort of depends on which assumptions we made.",
            "We make about the niceness and will explain niceness later at the poster.",
            "Which assumptions we make about the niceness of the distribution PXY.",
            "And T itself is sort of what we care about a bit here in this.",
            "In this work T is.",
            "Essentially can be viewed as the size of the tree of the partitioning procedure, but it's not really the size, it's more like the rate at which this partitioning procedure can quantize data.",
            "And so this part is very important, and so that now look at.",
            "Let's look at some of the assumptions made in the past.",
            "So in the original analysis by the script and so.",
            "They make assumptions nicely.",
            "Assumptions directly owned the data at hand, so it's not statistical in nature to not assumptions on the.",
            "It's not assumptions on the on the distributions, but rather directly on the data in the match of the data in the partitioning tree.",
            "So then earner Wulfen Bendavid had further result where the niceness assumptions are made directly on the distribution itself.",
            "So now it's it's statistical.",
            "However, they make, they assume that there is no noise and why, and they also assume that the partitioning procedure T is independent of the data, so it's seen first before you sample the data.",
            "In this sort of case, it might be that the data quantization rate is not nice because you haven't seen the data.",
            "So in this result we try to push that further, we make again niceness as new, sort of more general niceness assumptions on pxy.",
            "We work in the low noise setting rather than in the noise setting.",
            "So in the low noise setting and we also allow T to depend fully on the on the data, which then allows partitioning procedures that.",
            "And quantized the data really fast, and so that's essentially it."
        ],
        [
            "I'll talk more.",
            "I hope you come to the poster, so this is my French mercy so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, thank you so this is joint work with the Ruth Urner who just Spokane scheiben David so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's essentially also about the sort of active learning, so we are in a setting where classification where labels in classification are expensive and the procedure we analyzing is a process that is sort of an old procedure which essentially combines aspects of active learning and aspects of the class option.",
                    "label": 0
                },
                {
                    "sent": "So essentially the closer assumption being simply that labels don't change much on the cluster.",
                    "label": 0
                },
                {
                    "sent": "And then he uses a partitioning tree.",
                    "label": 0
                },
                {
                    "sent": "So the main motivation is that this algorithm is implementable.",
                    "label": 0
                },
                {
                    "sent": "It turns out that a lot of theoretical active learning procedures sort of hard to implement, and this is one of the few that is easy to implement, but it's sort of slightly hard to analyze.",
                    "label": 1
                },
                {
                    "sent": "There's been only a few analysis of it so far, and So what?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to do is give a refund analysis in more practical settings.",
                    "label": 0
                },
                {
                    "sent": "It's practical, but we want the settings in which we analyze it to be fully practical to be as practical as possible.",
                    "label": 0
                },
                {
                    "sent": "And so here I'll just give you an idea of what the procedure is.",
                    "label": 0
                },
                {
                    "sent": "The procedure was first formalized.",
                    "label": 0
                },
                {
                    "sent": "It's a very natural procedure, first formalized by Dasgupta, and so in 2008, and so essentially imagine you have access to a partition tree and you have data.",
                    "label": 0
                },
                {
                    "sent": "You partition the data first, then you.",
                    "label": 0
                },
                {
                    "sent": "Query a few labels in each cell of the partition.",
                    "label": 1
                },
                {
                    "sent": "If the cell happens to be pure in labels, then you fully label it.",
                    "label": 0
                },
                {
                    "sent": "If he happens to be on pure, then you refine the cell.",
                    "label": 0
                },
                {
                    "sent": "You refine the partition, you partition it further and then you keep repeating this an at the end you try to you try to label the whole data with error less than epsilon and then you can pass that over to a supervised learner and so it's a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "After this paper is it seems like.",
                    "label": 0
                },
                {
                    "sent": "Practitioners use it because I mean these people was at least on Google.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                },
                {
                    "sent": "It has like 200 or so citations.",
                    "label": 0
                },
                {
                    "sent": "So the overall appeal again is that it's implementable.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "Another appeal is that it doesn't really require the closer assumption to hold the cluster assumption, only need to hold approximately, and it's also safe if the cluster assumption does not hold it.",
                    "label": 0
                },
                {
                    "sent": "Still, it will still not mislabel by more than epsilon, so it's safe in that sense so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the labeling goal delivering goal is that we want to request less than one over epsilon square labels, where that's being essentially the label complexity of agnostic agnostic learning.",
                    "label": 0
                },
                {
                    "sent": "And the guarantees on label queries, and this guarantees are essentially the same guarantees that were shown before but under different assumptions.",
                    "label": 1
                },
                {
                    "sent": "But the guarantees take this form from T over epsilon to one over epsilon squared, and the guarantees, so it goes in this range.",
                    "label": 0
                },
                {
                    "sent": "T aversion to one over Epsilon Square and the range sort of depends on which assumptions we made.",
                    "label": 0
                },
                {
                    "sent": "We make about the niceness and will explain niceness later at the poster.",
                    "label": 1
                },
                {
                    "sent": "Which assumptions we make about the niceness of the distribution PXY.",
                    "label": 0
                },
                {
                    "sent": "And T itself is sort of what we care about a bit here in this.",
                    "label": 0
                },
                {
                    "sent": "In this work T is.",
                    "label": 0
                },
                {
                    "sent": "Essentially can be viewed as the size of the tree of the partitioning procedure, but it's not really the size, it's more like the rate at which this partitioning procedure can quantize data.",
                    "label": 0
                },
                {
                    "sent": "And so this part is very important, and so that now look at.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some of the assumptions made in the past.",
                    "label": 0
                },
                {
                    "sent": "So in the original analysis by the script and so.",
                    "label": 0
                },
                {
                    "sent": "They make assumptions nicely.",
                    "label": 0
                },
                {
                    "sent": "Assumptions directly owned the data at hand, so it's not statistical in nature to not assumptions on the.",
                    "label": 0
                },
                {
                    "sent": "It's not assumptions on the on the distributions, but rather directly on the data in the match of the data in the partitioning tree.",
                    "label": 0
                },
                {
                    "sent": "So then earner Wulfen Bendavid had further result where the niceness assumptions are made directly on the distribution itself.",
                    "label": 0
                },
                {
                    "sent": "So now it's it's statistical.",
                    "label": 0
                },
                {
                    "sent": "However, they make, they assume that there is no noise and why, and they also assume that the partitioning procedure T is independent of the data, so it's seen first before you sample the data.",
                    "label": 0
                },
                {
                    "sent": "In this sort of case, it might be that the data quantization rate is not nice because you haven't seen the data.",
                    "label": 1
                },
                {
                    "sent": "So in this result we try to push that further, we make again niceness as new, sort of more general niceness assumptions on pxy.",
                    "label": 0
                },
                {
                    "sent": "We work in the low noise setting rather than in the noise setting.",
                    "label": 0
                },
                {
                    "sent": "So in the low noise setting and we also allow T to depend fully on the on the data, which then allows partitioning procedures that.",
                    "label": 0
                },
                {
                    "sent": "And quantized the data really fast, and so that's essentially it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll talk more.",
                    "label": 0
                },
                {
                    "sent": "I hope you come to the poster, so this is my French mercy so.",
                    "label": 0
                }
            ]
        }
    }
}