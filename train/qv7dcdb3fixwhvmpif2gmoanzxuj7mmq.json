{
    "id": "qv7dcdb3fixwhvmpif2gmoanzxuj7mmq",
    "title": "Heteroscedastic Probabilistic Linear Discriminant Analysis with Semi-Supervised Extension",
    "info": {
        "author": [
            "Yu Zhang, The Hong Kong University of Science and Technology"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_zhang_hpldasse/",
    "segmentation": [
        [
            "So OK, good afternoon everyone.",
            "I'm young, I'm from Hong Kong, University of Science and Technology.",
            "The paper titled I Present is just hilarious Tastic probabilistic linear discriminant analysis with same spice extension.",
            "This is joint work with Divi Young."
        ],
        [
            "So we'll give some introduction to ayodya, so linear discriminant analysis is just a linear.",
            "This time dimensionality reduction method.",
            "When the labeled data data are available.",
            "So this is just surprised discriminate analysis.",
            "IoT is widely used in many applications, but it also has some limitations.",
            "The first one is just a small sample size problem on this less means when the sample size is much smaller than the feature dimensionality.",
            "Then the within class get metrics will become singular and then ayodya.",
            "The performance of Idea will will not satisfactory.",
            "This problem often exist in many applications such as face recognition, text classification, and microarray data analysis.",
            "The second limitation of idea is just the idea requires the classes home homoscedastic.",
            "This means each class is in Gaussian distribution with the same covariance matrix.",
            "The certain limitation of idea is just the idea cannot produce a probabilistic output and also cannot handle the missing data problem, but probably probabilistic output can help the subsequent decision-making process procedure and missing data.",
            "Problem is, how many encountered in many applications.",
            "There are."
        ],
        [
            "How many many methods to deal with the swing imitations of LDA?",
            "But most existing limitation can only handle one or two.",
            "Limitations not not all, not all of them.",
            "In our work we want to kill the three birds with one stone.",
            "And two anyway, the three limitations of idea in our paper two models are proposed.",
            "The first we propose the heteroscedastic probabilistic idea, which is M2 anyway, to the 2nd and the 3rd limitations.",
            "And also it's me spies extension, which is M2 anyway, to all the imitations of idea."
        ],
        [
            "First, we can introduce some notations used in wallpaper.",
            "We assume old neighbor data point X12 XL is available in the training data set and we in the training data set.",
            "We have C classes.",
            "And each data point XI, Nisin Capital D dimensional output while I comes from the set of 1 to see.",
            "And then we assume the case cars contain in Kane labeled examples and force me Spice extensions.",
            "Mu, you unlabeled data point is also available."
        ],
        [
            "Then we will first introduce our first model, the HP idea hitters tastic probabilistic idea.",
            "Soap the."
        ],
        [
            "Spell there is latent variable model.",
            "It can be defined as.",
            "The following equation XI is a data point.",
            "Wii is just the corresponding transformation in for ice class work for IYY ice class T is just latent variable for XI mu I.",
            "He's just offset for wise class and the website I here is just a noise term.",
            "So we assume that EI is from the standard.",
            "Causing distribution and the noise term is just the Gaussian noise term, so this is the graphical model for HBL dear.",
            "So we can use that all key to generate the noise term and using the T and epsilon IWK and milk to generate the XI here.",
            "So by integrating out the TI, we can see the class probability is the Gaussian distribution but with different.",
            "On covariance matrix.",
            "So this is different from the LDA, so so this is just the heteroscedastic model.",
            "So for permit learning."
        ],
        [
            "WK and torque can be obtained by maximizing the following note, nominally hood.",
            "So SK here is just an estimate covariance matrix for case class.",
            "And the five K here is related to WK and talk here.",
            "So this is this objective function is similar to that of properly probabilistic PCA.",
            "Here, so from the analysis in PC we can conclude the WK consist of the top again vectors of SK and the inverse of talk here equals to the meaning of the discarded eigenvalues.",
            "So let me give some."
        ],
        [
            "Cushing to our model HP or deer from the above analysis.",
            "WK here is just estimate using the neighbor data point from case class.",
            "But in some applications on the scales on the neighbor data, men need to the Anvers an accurate estimation of WK here.",
            "So our solution is just using unlabeled data.",
            "And the second limitation of our model is just the dimension negative WK plans.",
            "Important rule in the performance of our model.",
            "So we hope to determine automatically one solution is just using the automatic relevance determination method.",
            "This is just a ID method in used in Beijing.",
            "Learning.",
            "Then we introduce the second Model, Square HPD.",
            "Let's just say misplaced an extension or HP or D. So different from the settings in."
        ],
        [
            "HBO DM in the transfer training data state.",
            "We can we contains.",
            "Condense you unlabeled data point, as well as labeled data point.",
            "So the models for.",
            "Four X squared.",
            "There is similar to that of 4S and HP, or there's a difference is in this one.",
            "This is just for unlabeled data point.",
            "We model the model data density as the mixture model and each component in mixture model is just the class probability of the training data set an.",
            "We also place a LD player on each column of WK that is, the blue cage is from normal distribution and.",
            "UK here UK J He's just determine the importance of the GS.",
            "The importance of the future.",
            "So this."
        ],
        [
            "The graphic model for square HP.",
            "Oh dear.",
            "So in this part this is the generative process for labeled data, and in this part this is for unlabeled data.",
            "So."
        ],
        [
            "So for Pam with learning the model parameters are.",
            "UK The settlement K The test set of Turkey and Pie KUKJ&WK here.",
            "So we first introduce the the I hear the I is just the hidden indicator vector for each unlabeled data point XI.",
            "So ZI K = 2 one If XI belongs to case class an otherwise just zero.",
            "Because the number of model parameters is not much larger, so we introduce A2 fold EM to speed up the convergence."
        ],
        [
            "So in the out folder in the hidden variable is just the I and the model parameters is spiky and Milky.",
            "So in his step we just estimate the probability of ZI K = 2 one.",
            "This is just the posterior of XI belongs to case class.",
            "And in M step we just update the pikey and Milky as defined in this equations.",
            "So in."
        ],
        [
            "Dinner for the year.",
            "The Hidden Valley boys.",
            "Just TK here T here and model parameters is just WK to K&UKJ here.",
            "On the Internet, instead, we want to we just estimate the posterior of T even given XI.",
            "Anne.",
            "In M step we update the WK to K&MUK.",
            "Jack here."
        ],
        [
            "When will introduce the experiments in all experiments, we just invested get two applications.",
            "First one is fast recognition.",
            "We use the oil and pie data set and fall on.",
            "The second application is object recognition.",
            "We use the coil data set.",
            "So for each data set we just random select P datapoint as labeled data, and Q -- P as unlabeled data.",
            "So the training data set we have Q data point in each class and then remaining data from the test set.",
            "So we just performance 10 random splits and reported mean and the standard operation.",
            "As a result."
        ],
        [
            "So we compare with some dimensionality reduction method.",
            "The first one is the probabilistic LDA, the second one is supervised probabilistic PCA.",
            "The one on the third one is just the same spiced PPC.",
            "So we can analyze PLD RZA home, Homoscedastic supervised learning method and SPP, says Homoscedastic.",
            "Supervised learning method.",
            "Square pizza is just how majestic servicewise method and our method is, just the heteroscedastic semi supervised learning method."
        ],
        [
            "So this is just the recognition error rates on oil.",
            "So here the queue is equals 2 seven and for the first table the P = 2 two and the second table P = 2 three.",
            "This is just this is a examine experiment result."
        ],
        [
            "Um Pi data set with P = 2, two and three and four and this is."
        ],
        [
            "P = 2, five and six."
        ],
        [
            "So this is just a experiments on coil data set."
        ],
        [
            "So from this experiment result we can find our method achieved best performance among the compared methods."
        ],
        [
            "Sewing."
        ],
        [
            "This paper we just proposed Historia distich probabilistic idea, an SMS pies extension which is which is M2.",
            "Some anyway to some limitations of LDA?",
            "In our future work around 2 weeks and investigate the kernel extension to deal with non any non linearity and also for Beijing extension to improve the performance further.",
            "OK."
        ],
        [
            "Thank you for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "I'm young, I'm from Hong Kong, University of Science and Technology.",
                    "label": 1
                },
                {
                    "sent": "The paper titled I Present is just hilarious Tastic probabilistic linear discriminant analysis with same spice extension.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Divi Young.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll give some introduction to ayodya, so linear discriminant analysis is just a linear.",
                    "label": 0
                },
                {
                    "sent": "This time dimensionality reduction method.",
                    "label": 1
                },
                {
                    "sent": "When the labeled data data are available.",
                    "label": 1
                },
                {
                    "sent": "So this is just surprised discriminate analysis.",
                    "label": 0
                },
                {
                    "sent": "IoT is widely used in many applications, but it also has some limitations.",
                    "label": 0
                },
                {
                    "sent": "The first one is just a small sample size problem on this less means when the sample size is much smaller than the feature dimensionality.",
                    "label": 1
                },
                {
                    "sent": "Then the within class get metrics will become singular and then ayodya.",
                    "label": 0
                },
                {
                    "sent": "The performance of Idea will will not satisfactory.",
                    "label": 1
                },
                {
                    "sent": "This problem often exist in many applications such as face recognition, text classification, and microarray data analysis.",
                    "label": 0
                },
                {
                    "sent": "The second limitation of idea is just the idea requires the classes home homoscedastic.",
                    "label": 0
                },
                {
                    "sent": "This means each class is in Gaussian distribution with the same covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "The certain limitation of idea is just the idea cannot produce a probabilistic output and also cannot handle the missing data problem, but probably probabilistic output can help the subsequent decision-making process procedure and missing data.",
                    "label": 1
                },
                {
                    "sent": "Problem is, how many encountered in many applications.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How many many methods to deal with the swing imitations of LDA?",
                    "label": 0
                },
                {
                    "sent": "But most existing limitation can only handle one or two.",
                    "label": 1
                },
                {
                    "sent": "Limitations not not all, not all of them.",
                    "label": 1
                },
                {
                    "sent": "In our work we want to kill the three birds with one stone.",
                    "label": 1
                },
                {
                    "sent": "And two anyway, the three limitations of idea in our paper two models are proposed.",
                    "label": 0
                },
                {
                    "sent": "The first we propose the heteroscedastic probabilistic idea, which is M2 anyway, to the 2nd and the 3rd limitations.",
                    "label": 0
                },
                {
                    "sent": "And also it's me spies extension, which is M2 anyway, to all the imitations of idea.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, we can introduce some notations used in wallpaper.",
                    "label": 0
                },
                {
                    "sent": "We assume old neighbor data point X12 XL is available in the training data set and we in the training data set.",
                    "label": 0
                },
                {
                    "sent": "We have C classes.",
                    "label": 0
                },
                {
                    "sent": "And each data point XI, Nisin Capital D dimensional output while I comes from the set of 1 to see.",
                    "label": 0
                },
                {
                    "sent": "And then we assume the case cars contain in Kane labeled examples and force me Spice extensions.",
                    "label": 0
                },
                {
                    "sent": "Mu, you unlabeled data point is also available.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we will first introduce our first model, the HP idea hitters tastic probabilistic idea.",
                    "label": 0
                },
                {
                    "sent": "Soap the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spell there is latent variable model.",
                    "label": 0
                },
                {
                    "sent": "It can be defined as.",
                    "label": 1
                },
                {
                    "sent": "The following equation XI is a data point.",
                    "label": 0
                },
                {
                    "sent": "Wii is just the corresponding transformation in for ice class work for IYY ice class T is just latent variable for XI mu I.",
                    "label": 0
                },
                {
                    "sent": "He's just offset for wise class and the website I here is just a noise term.",
                    "label": 0
                },
                {
                    "sent": "So we assume that EI is from the standard.",
                    "label": 1
                },
                {
                    "sent": "Causing distribution and the noise term is just the Gaussian noise term, so this is the graphical model for HBL dear.",
                    "label": 0
                },
                {
                    "sent": "So we can use that all key to generate the noise term and using the T and epsilon IWK and milk to generate the XI here.",
                    "label": 0
                },
                {
                    "sent": "So by integrating out the TI, we can see the class probability is the Gaussian distribution but with different.",
                    "label": 0
                },
                {
                    "sent": "On covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "So this is different from the LDA, so so this is just the heteroscedastic model.",
                    "label": 0
                },
                {
                    "sent": "So for permit learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "WK and torque can be obtained by maximizing the following note, nominally hood.",
                    "label": 0
                },
                {
                    "sent": "So SK here is just an estimate covariance matrix for case class.",
                    "label": 0
                },
                {
                    "sent": "And the five K here is related to WK and talk here.",
                    "label": 0
                },
                {
                    "sent": "So this is this objective function is similar to that of properly probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "Here, so from the analysis in PC we can conclude the WK consist of the top again vectors of SK and the inverse of talk here equals to the meaning of the discarded eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So let me give some.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cushing to our model HP or deer from the above analysis.",
                    "label": 1
                },
                {
                    "sent": "WK here is just estimate using the neighbor data point from case class.",
                    "label": 0
                },
                {
                    "sent": "But in some applications on the scales on the neighbor data, men need to the Anvers an accurate estimation of WK here.",
                    "label": 1
                },
                {
                    "sent": "So our solution is just using unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And the second limitation of our model is just the dimension negative WK plans.",
                    "label": 0
                },
                {
                    "sent": "Important rule in the performance of our model.",
                    "label": 1
                },
                {
                    "sent": "So we hope to determine automatically one solution is just using the automatic relevance determination method.",
                    "label": 1
                },
                {
                    "sent": "This is just a ID method in used in Beijing.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Then we introduce the second Model, Square HPD.",
                    "label": 0
                },
                {
                    "sent": "Let's just say misplaced an extension or HP or D. So different from the settings in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "HBO DM in the transfer training data state.",
                    "label": 0
                },
                {
                    "sent": "We can we contains.",
                    "label": 0
                },
                {
                    "sent": "Condense you unlabeled data point, as well as labeled data point.",
                    "label": 0
                },
                {
                    "sent": "So the models for.",
                    "label": 0
                },
                {
                    "sent": "Four X squared.",
                    "label": 0
                },
                {
                    "sent": "There is similar to that of 4S and HP, or there's a difference is in this one.",
                    "label": 0
                },
                {
                    "sent": "This is just for unlabeled data point.",
                    "label": 0
                },
                {
                    "sent": "We model the model data density as the mixture model and each component in mixture model is just the class probability of the training data set an.",
                    "label": 0
                },
                {
                    "sent": "We also place a LD player on each column of WK that is, the blue cage is from normal distribution and.",
                    "label": 0
                },
                {
                    "sent": "UK here UK J He's just determine the importance of the GS.",
                    "label": 0
                },
                {
                    "sent": "The importance of the future.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The graphic model for square HP.",
                    "label": 0
                },
                {
                    "sent": "Oh dear.",
                    "label": 0
                },
                {
                    "sent": "So in this part this is the generative process for labeled data, and in this part this is for unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for Pam with learning the model parameters are.",
                    "label": 1
                },
                {
                    "sent": "UK The settlement K The test set of Turkey and Pie KUKJ&WK here.",
                    "label": 1
                },
                {
                    "sent": "So we first introduce the the I hear the I is just the hidden indicator vector for each unlabeled data point XI.",
                    "label": 1
                },
                {
                    "sent": "So ZI K = 2 one If XI belongs to case class an otherwise just zero.",
                    "label": 0
                },
                {
                    "sent": "Because the number of model parameters is not much larger, so we introduce A2 fold EM to speed up the convergence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the out folder in the hidden variable is just the I and the model parameters is spiky and Milky.",
                    "label": 0
                },
                {
                    "sent": "So in his step we just estimate the probability of ZI K = 2 one.",
                    "label": 0
                },
                {
                    "sent": "This is just the posterior of XI belongs to case class.",
                    "label": 0
                },
                {
                    "sent": "And in M step we just update the pikey and Milky as defined in this equations.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dinner for the year.",
                    "label": 0
                },
                {
                    "sent": "The Hidden Valley boys.",
                    "label": 0
                },
                {
                    "sent": "Just TK here T here and model parameters is just WK to K&UKJ here.",
                    "label": 0
                },
                {
                    "sent": "On the Internet, instead, we want to we just estimate the posterior of T even given XI.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In M step we update the WK to K&MUK.",
                    "label": 0
                },
                {
                    "sent": "Jack here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When will introduce the experiments in all experiments, we just invested get two applications.",
                    "label": 0
                },
                {
                    "sent": "First one is fast recognition.",
                    "label": 0
                },
                {
                    "sent": "We use the oil and pie data set and fall on.",
                    "label": 0
                },
                {
                    "sent": "The second application is object recognition.",
                    "label": 0
                },
                {
                    "sent": "We use the coil data set.",
                    "label": 0
                },
                {
                    "sent": "So for each data set we just random select P datapoint as labeled data, and Q -- P as unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So the training data set we have Q data point in each class and then remaining data from the test set.",
                    "label": 0
                },
                {
                    "sent": "So we just performance 10 random splits and reported mean and the standard operation.",
                    "label": 0
                },
                {
                    "sent": "As a result.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we compare with some dimensionality reduction method.",
                    "label": 1
                },
                {
                    "sent": "The first one is the probabilistic LDA, the second one is supervised probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "The one on the third one is just the same spiced PPC.",
                    "label": 0
                },
                {
                    "sent": "So we can analyze PLD RZA home, Homoscedastic supervised learning method and SPP, says Homoscedastic.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning method.",
                    "label": 0
                },
                {
                    "sent": "Square pizza is just how majestic servicewise method and our method is, just the heteroscedastic semi supervised learning method.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just the recognition error rates on oil.",
                    "label": 1
                },
                {
                    "sent": "So here the queue is equals 2 seven and for the first table the P = 2 two and the second table P = 2 three.",
                    "label": 0
                },
                {
                    "sent": "This is just this is a examine experiment result.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um Pi data set with P = 2, two and three and four and this is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "P = 2, five and six.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a experiments on coil data set.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from this experiment result we can find our method achieved best performance among the compared methods.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sewing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This paper we just proposed Historia distich probabilistic idea, an SMS pies extension which is which is M2.",
                    "label": 1
                },
                {
                    "sent": "Some anyway to some limitations of LDA?",
                    "label": 1
                },
                {
                    "sent": "In our future work around 2 weeks and investigate the kernel extension to deal with non any non linearity and also for Beijing extension to improve the performance further.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        }
    }
}