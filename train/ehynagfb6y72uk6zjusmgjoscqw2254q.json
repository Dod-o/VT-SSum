{
    "id": "ehynagfb6y72uk6zjusmgjoscqw2254q",
    "title": "Semi-supervised learning",
    "info": {
        "author": [
            "Guido Sanguinetti, Department of Molecular Biology and Biotechnology, University of Sheffield"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_sanguinetti_ssl/",
    "segmentation": [
        [
            "Thank you Tony.",
            "I need to turn this on.",
            "I press.",
            "I did it's fine."
        ],
        [
            "OK, so.",
            "Perhaps a better title would have been different types of learning, but I want to focus mostly on semi supervised learning, although I will give a pretty well sizable introduction also to two other types of learning which are closely related which are unsupervised learning.",
            "And supervised learning, and I'll give examples, and in particular I'll also introduce the EM algorithm when I talk about unsupervised learning, which is also fundamental.",
            "Tool in semi supervised learning, particularly generative models.",
            "And then I'll basically talk mostly about different types of semi supervised learning, so you can have generative models and you can have discriminative models for semi supervised learning.",
            "And both of them have got problems and they've got advantages and I'll briefly discuss these."
        ],
        [
            "So.",
            "First of all, the disclaimer, this is not going to be a technical talk.",
            "I will not present any new results and certainly not any results of mine.",
            "Most of the things that will tell you could be easily covered in Chris Bishop's book, and in Matias Igas very nice chapter in the semi supervised learning book which came out last year.",
            "A couple more things, slightly more advanced and I'll just mention them in passing.",
            "As I said.",
            "Probably half of the time will be talking about supervised and unsupervised learning, not just semi supervised, and I will be unashamedly probabilistic, if not Bayesian.",
            "Maybe becauses I will talk a lot about maximum likelihood and obviously Bayesians don't like maximum likelihood.",
            "Anne."
        ],
        [
            "So as I said, there are several different ways of learning.",
            "A very important chapter in machine learning is reinforcement learning, where you learn a strategy to optimize a reward.",
            "And it's a very active area of research has got very close links with control theory with decision theory, and that's all I'm going to tell.",
            "Tell you about enforcement learning.",
            "Yeah.",
            "The other two.",
            "Classical areas of learning are supervised learning where you want to learn a map.",
            "You're given inputs and you're given outputs and you want to learn the map and unsupervised learning where.",
            "Roughly speaking, you want to estimate the density that is behind your data.",
            "So let's look go into reverse order."
        ],
        [
            "Look at unsupervised learning first.",
            "So what's the scenario in unsupervised learning?",
            "OK, we're given.",
            "Data which I indicate as vectors X.",
            "Any general we're given more than one data point, of course, and we want to estimate the density that generates the data so.",
            "Broadly speaking, most of unsupervised learning can be classed as test density estimation.",
            "Although there are weaker forms such as you know, quantile estimation and so on.",
            "And I will make use of graphical models to describe the different types of learning.",
            "So how many of you know about graphical models?",
            "Yes.",
            "What you know about graphical models?",
            "Why didn't you raise your hand?",
            "Good so.",
            "So all you get a very good lecture tomorrow from zooming ghahremani who's?",
            "Certainly one of the world leading experts on graphical models, so I want to tell you.",
            "Virtually anything about graphical models, but I'll just explain.",
            "What they are, and I put them on the buffer board there.",
            "OK.",
            "Soap.",
            "Let's look at that graphical model.",
            "We got four quantities inside the circles.",
            "Which, broadly speaking.",
            "Random variable and some of them.",
            "We may also want to consider their parameters.",
            "Whatever that means.",
            "OK, so in this case we also have arrows within the circles.",
            "The arrows give us the structure of the conditional probability of the model.",
            "So what they're saying is that.",
            "The conditional probability of X, so the probability distribution of X depends on Y and on Theta OK.",
            "So if I wanted to write the whole joint probability.",
            "XY, Theta and \u03c0. OK, it basically tells me that X does not depend directly on.",
            "\u03a0 so I could factorize this as.",
            "P of X.",
            "Given Y and theater.",
            "And then P of Y which Y depends on \u03c0.",
            "And then I could.",
            "Either place a prior on Theta an on \u03a0, or I could leave them as parameters and estimated by maximum likelihood, but that's holding means it's just a very simple.",
            "Pictorial way of identifying who depends on who in our models.",
            "So in general, well in a lot of cases, although not in all cases, in unsupervised learning.",
            "We assume that there are some latent variables.",
            "That control the generative structure underlying probably underlie our data X.",
            "So in this case I'm saying that X depends on Y and for example, why could be a continuous variable which has got lower dimensionality than X?",
            "In that case, what I'm saying is that X.",
            "Given some parameters is determined by the value of Y, which lives in a lower dimensional space.",
            "And what I'm doing is dimensionality reduction and.",
            "You know everything about dimensionality reduction.",
            "Now that you've heard Neil.",
            "So.",
            "Continuous why is not going to be talked about in this lecture?",
            "What is the other option?",
            "The other option is that Y is a discrete variable.",
            "If Y is a discrete variable, the kind of case we are looking at this clustering.",
            "So why can take a few?",
            "Values say what could be 01 if we had only two classes and what we're saying is that the generative structure of X.",
            "Would be different according to whether Y.",
            "0 so it is in the zero cluster or wise one.",
            "It is in the one cluster.",
            "So let's look at this in detail in an example, OK. Oh, incidentally.",
            "Whenever wise discrete I'll call it see OK. Becausw stands for class."
        ],
        [
            "OK.",
            "In the example we look at is the daddy of all clustering is the mixture of Gaussians.",
            "We got vector measurements XI.",
            "I going from one to N. And the latent variable.",
            "Is well, we could think it's either AK dimensional binary vector class memberships or we could think it's a categorical variable.",
            "Want to K?",
            "OK, it's more convenient in this case to think of it as a K dimensional vector with zeros and one.",
            "And it only has one one which corresponds to the cluster in which the.",
            "Point needs to be classed OK.",
            "So.",
            "If we know the class.",
            "So if we know the value of CJ4 point J.",
            "Then if we know that point X is generated by the J class.",
            "Then what we are saying is that the probability distribution, the density underlying X is a Gaussian with a certain mean mu, J and a certain covariance matrix Sigma J.",
            "And we also have.",
            "Prior distribution over the class membership variables CJ.",
            "Which is given by this Pi parameters.",
            "Which have to sum to one OK?",
            "In order to be a distribution.",
            "So how many of you have seen mixture of Gaussians before?",
            "So a fair half of them, so please interrupt me anytime if you don't understand what I mean.",
            "What I'm saying.",
            "So in the case looking at the graphical model Now, X is what I called X.",
            "The theater parameters.",
            "The parameters of.",
            "The class conditional distribution.",
            "So there are the means and covariances of each component in the mixture.",
            "And the pie parameters prior on the class memberships, which are also called pie pie in here.",
            "So how do we estimate?"
        ],
        [
            "Such a model?",
            "Well, what do we want to do with this?",
            "OK, we want to estimate the various parameters Sigma, JMU J and I forgot to, Jayden and Paije.",
            "So we want to somehow learn them given data.",
            "And also will be interested of course in.",
            "Given a data point telling what of the clusters has generated it, so which clusters it belongs to, and this is conveniently expressed.",
            "By looking at the gamma IJS which are called the responsibilities, which are the probability the posterior probability that the class.",
            "Variable pertaining to the point I is JK, so it's called responsibility because it gives you the degree.",
            "By which the J clustering is glass cluster is responsible for having generated the fifth point.",
            "Now.",
            "We've got a marginal likelihood, So what we could do is just plug it into an optimizer like scaled conjugate gradient.",
            "And optimize it with respect to \u03c0 to mu Anta Sigma.",
            "But that's not very.",
            "Elegant, so the better method and very widely used method is the EM algorithm.",
            "OK.",
            "So again, how many of you know about the yam algorithm?",
            "Right, OK, so we'll describe it."
        ],
        [
            "In some detail, because it's quite important.",
            "So.",
            "The EM algorithm's M stands for expectation maximization, and it's an iterative procedure.",
            "That at each step increases the likelihood an if the likelihood is bounded.",
            "So if your model is convergent.",
            "You will be guaranteed to always increase the likelihood and find at least a local optimum.",
            "Now the log likelihood we want to maximize.",
            "Looks like this.",
            "It's a log of a sum of Gaussian bits, OK?",
            "Now.",
            "A log of a sum is not nice.",
            "Logs are very nice because a lot of the product is the sum of the logs and often we have probabilities that.",
            "I mean combine to be product, so taking a log likelihood is a sensible thing, But in this case we get a nasty.",
            "Expression.",
            "So basically what we want to do is to optimize that each step in the iterative procedure, something that does not contain this sum inside the log.",
            "OK, the key mathematical tools for doing this is Jensen's inequality.",
            "OK.",
            "Which I will now give."
        ],
        [
            "If.",
            "An intuitive proof of.",
            "OK, So what does Jensen's inequality tell you?",
            "Well, the formal statement is for every concave function.",
            "And for every random variable X and probability distribution Q of X.",
            "On variable X.",
            "The function of the expectation of X under Q of X.",
            "Is always greater or equal than the expectation of the function of X taken under the probability distribution.",
            "QX and concave is Neil likes to say means that the function makes a cave, so you see, this is a cave, so it's a concave function.",
            "Anne.",
            "And I generated it using a piece of a cubic function OK.",
            "So how do we go about proving this?",
            "Well, let's consider the case where X around a variable X only takes a finite number of points.",
            "OK, and in this case we have.",
            "Four instances of X.",
            "Now the.",
            "Abscissa or X coordinate of these points is my values of X.",
            "My function is this blue function OK.",
            "So.",
            "I can construct this figure.",
            "Basically, by joining the path XF of X.",
            "So I get there are four values I get.",
            "Quadrilateral figure is that the right English word, I don't know.",
            "So it's a quadrilateral.",
            "And the center of mass of the quadrilateral is obtained by taking the mean of the axis and the mean of the wise.",
            "So this point, which is the center of mass, has got X coordinate.",
            "The average under X.",
            "Under Q of X of X and it's got this Y coordinate.",
            "The average under Q of X of F of X. OK, are we all agreeing on this?",
            "OK. Now since.",
            "This is concave.",
            "Any?",
            "Poly Polygon subtended by concave.",
            "Curve is convex, so the center of mass lies within these quadrilateral always.",
            "Whatever four points are taken.",
            "The center of Mass would lie inside, which is pretty intuitive.",
            "So we've seen the center of Mass has as Y coordinate the expectation of F under Q of X.",
            "What would be the F of the expectation of X while the FD expectation of X is taken this point and look very crosses are function.",
            "And as you see, it's above.",
            "OK. You can generalize these two as many points you want to have, and you can also generalize it to the whole of a continuous interval, in which you basically take just the curve.",
            "And that tends to that.",
            "The expectation of F. Is always below.",
            "F of the expectation.",
            "OK, so this is a cartoon proof, but.",
            "I think it's a fairly compelling argument and I read it in David Mackay's book.",
            "OK, so everyone happy with the Jensen's inequality.",
            "That's a mathematical statement.",
            "Not the proof."
        ],
        [
            "Now, why are we interested in Jensen's inequality?"
        ],
        [
            "Well, if you recall the thing we wanted to.",
            "Look at was the log.",
            "Of.",
            "This quantity.",
            "OK.",
            "Which is effectively an expectation.",
            "OK, it's the expectation of the conditional likelihood under the prime."
        ],
        [
            "So the log is a concave function.",
            "It doesn't quite look like this.",
            "It goes up, but it's always concave, so the curvature points downwards."
        ],
        [
            "So we can use Jensen's inequality.",
            "And before we do that will.",
            "Introduce one of the best used tricks.",
            "So we want to optimize this quantity here.",
            "That's exactly the same as what we had before.",
            "Now the famous trick is to introduce surreptitiously a quantity Q of C, which will be a probability distribution, and we multiply and divide by it.",
            "So we're not doing anything.",
            "But this basically tells us that we are taking the log of the expectation of this ratio.",
            "And the probability distribution Q of C. OK now key of C is a probability distribution with.",
            "Use it as a condition and we're taking the log of an expectation.",
            "Now we can use Jensen's.",
            "And Jensen tells us that the log of the expectation.",
            "Is greater or equal?",
            "Then the expectation of the log.",
            "Now this is a log of a ratio, so logs of ratios are nice, so we can say it's the.",
            "Difference between the expectation of log of P of X.",
            "And the expectation of log of Q on the Q which is also called the entropy of the distribution.",
            "So.",
            "The further thing to notice.",
            "Is that?",
            "I got a greater equal here, so the first question people should ask me.",
            "Well, whenever you have a bound, is it a tight bound?",
            "Is it you know I'll be bounding something with something that is completely unrelated is always much bigger than the reality and the truth is that it is not much bigger.",
            "But actually we can also tell that when Q of C so are arbitrary distribution that we introduced.",
            "Surreptitiously in this mathematical step, when QFC is the posterior, so is QFC given X.",
            "This inequality actually saturated, so it becomes an equality.",
            "And the reason for that is that if Q of C is the posterior then.",
            "P of X given CP of X&C can also be written as.",
            "P of.",
            "Write it over here P. Of XC can be factorized using the fundamental rule of probability.",
            "OK. OK.",
            "So if Q of C is the posterior.",
            "P of X C / P of C given X.",
            "Is equal to P of X.",
            "Which does not depend on C. And therefore when we take the expectation under.",
            "A probability distribution, or see it doesn't do anything to it.",
            "So if Q of C is the posterior, this ratio does not depend on C and the log of the expectation is the same thing as the expectation of the log, yes?",
            "No, not necessarily.",
            "I mean, that could be QFC is just a completely arbitrary probability distribution.",
            "So for now, between probability distribution we have this inequality.",
            "But if Q of C is the posterior over the class.",
            "Then this inequality is an equality, so it becomes equal.",
            "Not greater equal.",
            "That's the key point.",
            "So the idea?"
        ],
        [
            "Behind them, and this is a cartoon.",
            "It is often seen that.",
            "OK, if you know the value of the parameters then."
        ],
        [
            "You can compute.",
            "The posterior using this formula OK if you know the value of the parameters, you can just Pi Sigma mu.",
            "You can plug them in and you find the posteriors for each point."
        ],
        [
            "The problem is you don't know the value."
        ],
        [
            "Parameters, but for a fixed value of the parameters you can compute the posterior in Jensen's inequality now.",
            "Gives you a bound that is tight.",
            "Is equal.",
            "So.",
            "Obviously though, when you vary the the value of the parameters, then that bound will not be tight and you will get a function that is consistently below your.",
            "Marginal likelihood OK, your log likelihood.",
            "So.",
            "Initialize with a certain value.",
            "The parameters you find the posterior.",
            "Then you look at.",
            "What is the maximum value of that bound?",
            "In the space of parameters and, the hope is that it would be much easier to compute.",
            "So you do an EM step, you find the maximum you go up.",
            "Which always increases the likelihood.",
            "And then you've got a new value of the parameters, and you can recompute your posterior and that will bring you up to the bound, which is always above because it's a bound.",
            "OK, so in this way you iterate between EM steps where you optimize parameters and E steps.",
            "When you compute posterior distributions and you're guaranteed to climb up your log likelihood, which may be very nasty, but the bound is generally much more tractable.",
            "So.",
            "I said here exercise M for mixtures of Gaussians.",
            "And that should.",
            "I think it's worthwhile to do this exercise.",
            "Yeah.",
            "And so first of all."
        ],
        [
            "Let's rewrite the bound OK.",
            "So the bound which I've got is that.",
            "Log off the expectation of P of.",
            "XC under.",
            "QFC greater equal.",
            "Then the expectation of the log of the joint.",
            "Plus the entropy OK?",
            "Now notice that the entropy of Q does not depend on the value of the parameters and the entropy distribution does not depend on and.",
            "My joint."
        ],
        [
            "Distribution was.",
            "P of X, C. Is.",
            "P of X given C * P of C. And it's.",
            "A normal distribution with mu J.",
            "And.",
            "Sigma J.",
            "Times \u03c0 J which is the prior on the class OK.",
            "So using this bound, forget about entropy of the of the discrete distribution.",
            "If I got a value, the parameters I can compute.",
            "The posteriors, which were the responsibilities which are given by.",
            "This is the responsibility that point high is generated by class J, so it's Pi J times.",
            "On XI.",
            "MU J. Sigma J.",
            "Of son of a kalaty of Pikey.",
            "OK.",
            "So I want to take the expectation of the joint probability so my joint probability that the point XI that we observe a point XI and that it comes from Class J is given by this expression.",
            "So it's one of a sqrt 2\u03c0 determinant of Sigma J. Expo minus 1/2.",
            "XI minus mu J. Transpose Sigma to the minus one.",
            "XI minus mu J Sigma J 2 -- 1.",
            "OK times.",
            "Paije I take the log of this.",
            "Slog paije plus.",
            "Actually, minus 1/2.",
            "Log of 2\u03c0 -- 1/2.",
            "Log of the determinant of Sigma J.",
            "Minus 1/2.",
            "XI minus mu J. Transposed Sigma to the minus one.",
            "XI minus mu J. OK. Now if I want to take the expectation under the posterior distribution, well I have to use the gamma IJS.",
            "So the expectation.",
            "Under the posterior that I have observed, the point XI coming from class CJ is.",
            ", I J.",
            "Times.",
            "This thing OK?",
            "OK, and.",
            "Since I'm taking the expected, the log of the joint probability overall points, the joint probability of all points is the product of the probability.",
            "Of each point.",
            "So this becomes some.",
            "Over I. OK, that's my bound.",
            "And now I can maximize it by just doing computing the derivatives if I do.",
            "The first thing I could look at is to find out what by Jay is.",
            "So now I call this.",
            "Call this thing L. That's my bound.",
            "So I may want to do DL.",
            "Indie pie.",
            "Jay and I get.",
            "Sum over I of gamma IJ times.",
            "1 / \u03c0 J.",
            "Equals 0.",
            "Is that correct?",
            "Thank you very much, OK?",
            "Yes, as you pointed out, the Pie JS are not independent.",
            "I mean if we said they were prior probabilities over the classes, so they have to sum to one and if I just optimize like that, I run into trouble as I was showing you.",
            "So I add the LaGrange multiplier times.",
            "The constraint, the sum of a J of Pi J.",
            "Must be equal to 1.",
            "So that's the thing I need to optimize, thank you.",
            "And.",
            "So if I look at that, I get an extra term.",
            "Plus Lambda.",
            "Equals you.",
            "OK, now I can rearrange this.",
            "Now how do we find Lambda?",
            "Well, I'll say the solution here.",
            "Otherwise we run out of time.",
            "We sum if we sum the whole thing both left inside and right inside.",
            "Over Jay.",
            "Now the writer side remains 0.",
            "This they sum to one, so this becomes.",
            "Lambda and the sum of the.",
            "Responsibilities?",
            "Over both high and J is equal to the total number of points.",
            "OK, because I can swap the sum over inj.",
            "Gamma AJ is a posterior probability.",
            "But the point I was generated by class J.",
            "So if I sum over all JS it's one or something over I I get N. She tells me that.",
            "Lambda has to be equal to minus N and then from here I get that Pi J.",
            "Is.",
            "The sum over I of gamma IJ.",
            "Over N. OK, and this is often.",
            "Called also Angie over N 'cause basically.",
            "If Gamma IJS was 01.",
            "This would be the number of points that came from Class J. Hi Jay, I'm not 0 ones, but it's the total responsibility for class J so effectively what it's telling us is that the prior over the class membership is the fraction of points you get in that class.",
            "Now that was the hard bit because it had the LaGrange multiplier.",
            "I can now plug that in and start computing.",
            "Derivatives of.",
            "I have sorry or my bound, for example, with respect to mu J.",
            "And this will involve a little bit of.",
            "Matrix differentiation, which I don't have time to cover, but if you rewrite this as a trace then this becomes.",
            "Sigma minus one.",
            "Sum over I gamma IJ Sigma minus one times XI minus mu J.",
            "Which is equal to 0, which tells you that.",
            "Mudrey is equal to.",
            "Some of her I. , I J. Xcite over Angie.",
            "Again, it's a nice result.",
            "It tells you that basically it's the weighted.",
            "Mean of all the points weighted by the probability posterior probability that that point is in that cluster in cluster J.",
            "And similarly you get the Sigma J.",
            "Is.",
            "OK, Sigma J is the maximum likelihood estimate.",
            "Of the covariance for the cluster J. OK.",
            "But the crucial thing to bring home is that by using Jensen's inequality, the M step equations that each step is is trivial.",
            "In this case you know it's just.",
            "Computing, plugging the parameters and computed the M step equations.",
            "I could solve them analytically OK.",
            "So it's a very effective.",
            "System.",
            "I don't have to do gradient descent or anything like that."
        ],
        [
            "So."
        ],
        [
            "That's all I'm going to say about unsupervised learning.",
            "What about supervised learning?",
            "As I said before, that's the other traditional.",
            "Piece of.",
            "Learning.",
            "And the data in this case consists of vector input X and.",
            "Output target values Y.",
            "So I've been told that you got an input X, which is a certain vector and the value corresponding to that vector is, why?",
            "What we want to learn is the map between X&Y.",
            "Which would also we could also equivalently call as the conditional probability of observing Y given X.",
            "It's evaluated using the error, the reconstruction error, or the classification accuracy.",
            "And again it can be divided into 2.",
            "Subgroups, if you have a Y which is a continuous variable.",
            "What you're doing is basically learning a map between continuous variables, and that's called regression.",
            "If you got a discrete Y.",
            "Then that's classification.",
            "Classification is what we're going to talk about because almost all of semi supervised learning is on."
        ],
        [
            "There are two types of classification methods.",
            "First one is the generative paradigm.",
            "Which.",
            "Start by saying well.",
            "We want to model the class conditional probabilities.",
            "So we got some classes we want to model.",
            "What's the probability of observing X given?",
            "The Class C. We estimate the parameters.",
            "For example, using maximum likelihood.",
            "And we assign.",
            "Then we compute.",
            "Once we have the parameters and we have the prior model of the class conditional probabilities, we can compute posteriors.",
            "For C / X given X, which is what we want, that's the map.",
            "OK, if I'm given X, where am I?",
            "Where is my see likely to be?",
            "Yes.",
            "Yeah.",
            "Function.",
            "Yep.",
            "Well you you, you're getting a.",
            "You can look at the probabilities.",
            "As I said in the disclaimer, we basically look at everything in terms of probabilistic models.",
            "You could also look at that point estimate OK.",
            "So you could tell me what's the maximum the most likely see given that we've observed a certain X and that would give you a map.",
            "OK.",
            "So the generalization of a map.",
            "So as I said, we normally want a functional relation, which is what you do with your Nets.",
            "But the more the more general outlook is to say we want to know what's the probability of having a certain class.",
            "Given.",
            "A certain input OK, certain target given a certain input.",
            "Dot Dot might give you an explicit function if you for example, then if you have two classes then you can say at each point you can take the class to be the one with the greatest posterior probability.",
            "That would give you an explicit function which would obviously have a step somewhere.",
            "OK."
        ],
        [
            "So the problem with generative methods is that it requires the estimation of lots of parameters.",
            "An example of this is.",
            "Discriminant analysis OK, which is closely related to the mixtures of Gaussians we've seen before.",
            "'cause we exactly use mixtures of Gaussians as our class conditional model.",
            "We postulate that.",
            "Given the class, the probability of observing X is given by a Gaussian OK with a certain covariance and certain mean.",
            "Now we will have to estimate both the means and the covariances for each class.",
            "Now if the data is D dimensional, this is a symmetric matrix, so is ordered.",
            "The squared was D * D + + 1 / 2.",
            "And we have to do it for each class, so we have a large number of parameters to estimate with maximum likelihood.",
            "Which may not be a very wise thing to do.",
            "And then we can classify.",
            "Once we have found the parameters again, we look at the posterior probabilities and then if we're given a new test data, we plug X into this formula and we get what's the posterior probability of X coming from a certain class OK?",
            "Now.",
            "I left this as an exercise and I think we might just about have time to do it.",
            "So it's.",
            "Possibly instructive for what we're going to see in the next 10 minutes.",
            "To rewrite these in terms of sigmoids.",
            "OK, so in the case when we have only two classes.",
            "I'm getting better.",
            "So let's remove all these mixtures of Gaussians.",
            "So our criterion.",
            "Suppose we have K = 2 Now.",
            "Criteria for assigning.",
            "New inputs to a certain class is to look at the posterior probabilities.",
            "OK, so we look at P of.",
            "Say C1.",
            "Given a certain X. OK. And then we also obviously we have.",
            "Pfc 2 given a certain X.",
            "IS 1 -- P or C1 because we only have?",
            "Two classes.",
            "So what we want to look is which one is bigger.",
            "Is this one bigger or this one bigger, so convenient way to look at that is to consider the ratio?",
            "Of the likelihoods.",
            "So we have say a function we call it D. Of a piece of.",
            "The ratio of the posteriors OK but the posterior.",
            "Is.",
            "OK.",
            "I'm just using Bayes theorem to express the posterior.",
            "So these terms go away.",
            "And it becomes.",
            "A ratio of likelihoods.",
            "OK, so the ratio of likelihoods is the.",
            "Discrimina criteria.",
            "OK, what I want to do is to rewrite this as.",
            "Sigma of a.",
            "Where Sigma.",
            "Is the.",
            "Logistic sigmoid function.",
            "1 + X -- A. OK. And.",
            "I look up my notes so that I don't make silly mistakes because I'm prone to silly mistakes.",
            "Completely trivial.",
            "Thing to do, if we take a.",
            "Hey.",
            "To be.",
            "The log of.",
            "P of X / C one.",
            "Times P of C1.",
            "Then we can rewrite this in this form exploiting this.",
            "Constraint OK.",
            "The interesting thing is that a if.",
            "Sigma one is equal to Sigma 2A becomes.",
            "Proportional to.",
            "New one minus Mewtwo.",
            "Transpose times X.",
            "Basically, what is standing you you measure where you are in the distance between the two means and the mean you're closer to is the class you get assigned to.",
            "OK. Now the reason why I wanted to do."
        ],
        [
            "This exercise is that.",
            "Is about what's coming up now.",
            "There is another paradigm for classification, which is the discriminative paradigm.",
            "Oskol Diagnostic and the idea.",
            "Is that model in the class conditional distributions involve significant computational overheads, but not only you could also get the model wrong.",
            "Perhaps what is more parsimonious?",
            "And is also more invariant under changes in the model is to look directly at the posterior probabilities.",
            "So directly model what the probability of CJ is given X. OK.",
            "This is closely related to the concept of transductive learning, which vapnik introduced which is.",
            "Formulated in a way that my grandmother would understand that say if you want to solve a certain problem, don't solve a harder problem first, OK?",
            "So."
        ],
        [
            "If we are generative.",
            "We got to model P of X given C and then we have to find marginal likelihoods for X and then we get the procedure problem."
        ],
        [
            "It is.",
            "If we are discriminative.",
            "We just go straight for the posterior probabilities."
        ],
        [
            "And an example of this is logistic regression.",
            "Which is effectively the discriminative analogue of discriminant analysis we've just seen because what they say is.",
            "Well, OK, forget about the generative process underlying the X.",
            "Just take.",
            "As a fact that the probability of being in a certain Class C one is given by.",
            "A sigmoid.",
            "So we've seen that the likelihood ratio is a sigmoid.",
            "If we assume that the class conditional probabilities are Gaussians.",
            "With the same.",
            "Covariance say well forget about the generative mechanism.",
            "Just write these conditional probabilities.",
            "Obviously they have to be normalized and.",
            "Sigma is this function here.",
            "Now the number of parameters.",
            "If we assume.",
            "Equal covariances, then the total number of parameters is just.",
            "The values encoded in this vector W. Which is just D instead of K * D squared, so it's a significant.",
            "Sam savings in terms of computational time."
        ],
        [
            "How do we estimate this?",
            "Well, it's actually quite easy.",
            "Again, we use maximum likelihood and.",
            "For ease of notation, we called why I the probability that XI comes from C1?",
            "OK, see I is 01 and likely it can be written in this form.",
            "OK, so it's the posterior probability.",
            "That's why I came from comes from C1.",
            "Times the posterior probability that it comes from C2.",
            "OK, this is just a clever way.",
            "Because you see C-01.",
            "So if C is 0.",
            "That means that it is in class one and we're looking at this probability if C is 1, then this goes way.",
            "We're looking at this probability OK?",
            "So we can take the log of that.",
            "We can compute.",
            "The gradient of the likelihood and we get the gradient of the likelihood looks like this.",
            "Which is nice because basically.",
            "You want the gradient of the likelihood to be 0, and you see this is zero if your posterior probability is very close to the label.",
            "So if your posterior probability of being in class one is very close to.",
            "One then this would be a small contribution to the gradient.",
            "However.",
            "It's a general problem with the maximum likelihood based methods that can be overfitting, because if the two classes are well separated and there are several possible values of the vector W. Which will separate exactly both classes.",
            "OK, so you may want to do Bayesian logistic regression or things like that."
        ],
        [
            "OK, so.",
            "Let's get to the title after.",
            "About 50 minutes.",
            "Semi supervised learning what is semi supervised learning about what's the problem?",
            "Well, as we've seen.",
            "Supervised learning assumes that you know the label for each data point for each input point.",
            "You also know the target.",
            "But in practical applications.",
            "That's not always feasible or sensible.",
            "For example, if you're interested in a classification problem where you have proteins, you have the sequence of a protein you want to predict the function.",
            "Now for some proteins.",
            "The function is known, so you have the label, but finding the function of a protein is an extremely expensive.",
            "And time consuming process requires lots of lab testing, so for the great majority of proteins you just know the sequence not.",
            "The function.",
            "Other examples, for example, are text classification, image classification.",
            "At the moment all the labeling has to be done by hand, so you need a person to say oh this text is about politics or this email is about sports.",
            "If you were to just gather emails or texts, obviously you would have a much larger corpus, so the idea is.",
            "Since you've got plenty of examples of inputs X, but few of the corresponding target.",
            "Maybe we could also make use of the inputs which don't have a target and improve our predictive power.",
            "What is the goal of semi supervised learning or the goal is still to predict?",
            "What is?",
            "Label given the input.",
            "And it's evaluated again by classification accuracy.",
            "So in the strict sense.",
            "In this sense, Semi supervised learning is a special instance of supervised learning, but.",
            "Sink because of its practical importance in a lot of examples, it has become a field on its own, basically.",
            "Semi supervised learning is.",
            "The way of aiding supervised algorithms by the use of unlabeled data."
        ],
        [
            "Now, a little bit of notation so.",
            "The label set.",
            "We also called the labeled data set.",
            "We also called the complete data and these are data set called DL.",
            "Which comprises of NL sets of pairs X&C, so we have NL instances of an input and of a target.",
            "We also have an unlabeled or incomplete data set.",
            "Did you?",
            "We just got NU.",
            "Examples OK, and they are only the vectors X.",
            "The common and the interesting case.",
            "Is when we have.",
            "The number of unlabeled data is much greater than the number of labeled data, OK?",
            "The idea being that you would be discarding an awful lot of information if you just used a supervised algorithm."
        ],
        [
            "Now there are two obvious baselines for semi supervised learning.",
            "Simple attack is.",
            "To ignore the labels.",
            "Use an unsupervised learning method.",
            "For example, a clustering approach approach.",
            "And then.",
            "Validated against the labels and find your accuracy from that.",
            "Obviously, the complementary attack is just, well, ignore all the unlabeled data and just to, say discriminant analysis or logistic regression or any supervised technique on the complete data.",
            "And then use the unlabeled data that says test data.",
            "Now there's an interesting statement which goes as a no free lunch statement that it is possible always to construct data distributions.",
            "For which either of the baselines outperforms.",
            "The given SSL method.",
            "So you can construct a data distribution.",
            "Which.",
            "Leads to better performance.",
            "If you don't know the labels affect if you don't use the labels or you can construct the data distribution where if you ignore the unlabeled data, you get better performance.",
            "Think for example, if the data distribution was such that the unlabeled data was completely unhelpful and sampled with an extreme bias, which completely screws your classifier.",
            "However.",
            "There is a generalized hope that.",
            "The no free lunch won't happen very often, and.",
            "We should be able to tailor SSL algorithms in order to match a specific application so that we always improve the situation by going semi supervised."
        ],
        [
            "No.",
            "OK, as I said.",
            "SSL is a special case of supervised learning, and again we can.",
            "Classify Semi supervised learning algorithms as generative algorithms and discriminative algorithms.",
            "OK, so.",
            "Perhaps the generative method is most intuitive.",
            "The graphical model is like this, which is exactly the same as the graphical model of unsupervised learning.",
            "And.",
            "We also have some.",
            "Unlabeled data, so maybe we would think of, you know, combining the likelihoods, we can write down the likelihood for the incomplete data, which comes from a clustering algorithm.",
            "Basically, is the clustering likely, and then we can write the likelihood for the label data.",
            "Not necessarily, but yeah you could do.",
            "OK, you just you're just in the general case where you have a latent class variable and you know there is a certain class conditional which need not be Gaussian.",
            "It could be, you know.",
            "Student T could be gamma, could be whatever.",
            "I mean you can do a mixture of any distributions OK, but yeah, the Gaussians is a good example to keep in mind because it's very clear what's going on.",
            "But in any case, you know.",
            "We can write and unlabeled.",
            "Data likelihood and we can write a complete data likelihood.",
            "We can sum them together.",
            "And then we can just say Oh well.",
            "For the unlabeled data, sees a latent variable, and then we have a very general powerful technique, which is am, which we've gone into some detail which allows you to estimate.",
            "Find maximum likelihood when you have missing.",
            "Latent variables.",
            "OK, so we could use them and estimate the parameters and then we would get our posterior distributions OK. Notice that this is truly semi supervised because we would use them over the whole thing.",
            "So we use both incomplete and complete data to estimate the parameters.",
            "Which would be our thetas."
        ],
        [
            "Toys.",
            "So this obviously has been done quite a long time ago and.",
            "It probably is the one of the very first papers in semi supervised learning which is by Jeff McLaughlin in 1977.",
            "Which considered a semi supervised learning problem for generative model with two Gaussian classes with known covariance.",
            "OK, now you may think this is pretty trivial, but it's actually not that trivial.",
            "And.",
            "We also assume the covariance to be known, and we only need to estimate mu one and MU two from the data and we want to use both the labels label data and the unlabeled data to estimate mu one MU 2.",
            "So we make a very strong assumption.",
            "Which is that we got?",
            "The unlabeled the size of the unlabeled data set.",
            "Is much smaller than the size of the label data set, so in this early days they were looking at.",
            "You know, when you have a little bit of unlabeled information and how do you have to modify an?",
            "Whether you can get guarantees of improvements in performance.",
            "We also suppose that the unlabeled data set comes from M1 points in the true truly are from the first class and M2 points are truly are from the second class.",
            "Further assumption, we assume that.",
            "We have an even split between the two classes amongst the labels, so we have little endpoints in Class One classified as Class 1 little endpoints in Class 2."
        ],
        [
            "So McLaughlin, being a proper statistician, the first thing he said is, well, we can.",
            "Consider the expected misclassification error, OK?",
            "So you can say, well, I can construct a discriminate and I can find theoretically what the expected error is.",
            "Now you can consider that the expected error for the method that uses only the label data.",
            "So the supervisor label and we call that R. And we can also consider what the expected error is when you add the unlabeled information, we call that R1 OK. And we estimate the parameters using AM.",
            "Now the surprising result, which I found surprising at least, is that.",
            "If.",
            "The numbers in the unlabeled data between the two classes are unbalanced.",
            "OK, so if M1 is different from M2.",
            "Then you can expand to 1st order.",
            "Our one and R. And you find that so first ordering and you over analysis, so we're assuming that the few unlabeled cases you find that this difference is positive only.",
            "If the number of unlabeled examples is below a certain critical number.",
            "OK.",
            "So this difference being positive means that the 1st order you're guaranteed that using the unlabeled data you're doing better.",
            "You're getting a less expected error.",
            "No, that's true.",
            "So these are the true.",
            "You don't know them.",
            "You have to estimate them.",
            "But this is a theoretical analysis.",
            "OK, so it's a theoretical analysis in which you don't know the labels.",
            "But you know, you are presuming that there are M1 and M2 from.",
            "Those classes OK, you don't know which ones they are, but it's just.",
            "It's something you need to do to compute the expected error in the R1 case OK.",
            "So.",
            "This is this is surprising I think.",
            "'cause it's telling you that unlabeled data helped.",
            "Only up to a point.",
            "So if you got a little bit of unlabeled data, then you're sure that you're going to improve.",
            "But if you got a lot of unlabeled data where at least the 1st order.",
            "You can show that you may not improve at all.",
            "You could actually get an expected error which is greater than on the supervised case.",
            "You have.",
            "Number.",
            "Yeah, that's true.",
            "It's not so surprising, but it's true.",
            "Also, you know when the imbalance is fine.",
            "So obviously yes, Mstar as a function of.",
            "Basically the difference between M1 and M2.",
            "Is is decreasing, so if everyone is very close to M2 and M star is large, but what it is saying is since you don't know I'm one and M2.",
            "If you're just presented with label data and unlabeled data, you cannot rule out that.",
            "For some reason you know you've got your labels that are balanced, but actually in the real data process the mixtures are not evenly balanced.",
            "Then I think what happens is that in your EM algorithm, basically you become.",
            "The data at the label data drives it to think that it's more balanced, and so I mean, you do make an error in estimating how many you are having balanced or unbalanced.",
            "But yeah, I mean it, it may not be such a surprising result, but what it's sending you is that it's, you know, just adding the unlabeled data might not solve all your problems, yes.",
            "Very funny movie about.",
            "As I said, it's for very small imbalance.",
            "It's very high.",
            "For very large imbalance, it could be quite.",
            "Well, it's not.",
            "It's not, and it doesn't have an analytical form, so it just did the.",
            "So you.",
            "Yeah, you can't make an exponent.",
            "It does grow very fast.",
            "Yeah, it looks as if it's exponential, but can't be exponentially because you can't.",
            "You know the domain of possible differences is not.",
            "Unbounded."
        ],
        [
            "OK, So what McLaughlin also proposed the way out, which was well, don't trust the unlabeled data as much and re wait.",
            "So compute your estimates by basically.",
            "Down waiting, oh.",
            "In any case, changing the weighting of the unlabeled data with respect to the label data, and that's something that is done very much in semi supervised learning.",
            "And this is probably the first instance in which this happened, and he showed that for a suitable value of Lambda.",
            "So for a suitable reweighting.",
            "Re balancing between the labeled and unlabeled data he could find.",
            "Unexpected error that.",
            "Was always smaller to 1st order OK?",
            "And obviously this Lambda depends on the true.",
            "Numbers which you don't know you have to.",
            "To estimate, but yes.",
            "Well, if you have, you know it's a generative model, so you're you got the expectation you got the class conditional probabilities.",
            "You estimate the parameters of the class conditional probabilities and then that gives you basically unexpected error.",
            "So depending on how far.",
            "The means are you can, you know, say what's the probability that you know a point that you find over here that you would classify as being.",
            "This cluster actually comes from this cluster.",
            "That's just a simple probability.",
            "No, no, you're estimating the parameters using both labeled and unlabeled data.",
            "Once you estimate the parameters, your job is done.",
            "You can compute the posterior probabilities.",
            "OK, and once you have posterior probabilities, you can say what's the error rate, assuming that you're classifying them by what's the greatest posterior probability.",
            "So in.",
            "Nona cartoon example.",
            "Yeah he would have.",
            "Some of these are labeled.",
            "Some of these are labeled.",
            "You use them both to determine that.",
            "Whatever is to the left of this line is classed as this.",
            "Now, if you get an extra point here.",
            "You can compute what's the probability you would classify it as here, but you can compute what's the probability that it actually comes from here rather than from that, and that's the expected error.",
            "OK.",
            "So you can compute it like that.",
            "That's what he does affect.",
            "Is that?",
            "Does that solve your problems?",
            "Yeah you can.",
            "I mean, what you're doing is basically working out the parameters.",
            "You effectively optimizing this likelihood.",
            "Which is the sum of the two.",
            "And then or weighted combination of the two.",
            "And optimizing that contains both the labels information.",
            "You see that the functional form if you have the label is different, as if you don't have the label, because if you don't have the label, you have to marginalized.",
            "Over the unseen class, you have to sum over, see if you have the label.",
            "No, it's just a joint probability.",
            "So the labels help in finding the parameters.",
            "So anyway, so."
        ],
        [
            "What it came to was that.",
            "And it's a generally very used technique is that you shouldn't just.",
            "Consider your total likelihood as being the sum of the incomplete, incomplete, and complete likelihood, but you should have re weighted sum of the incomplete incomplete likelihood.",
            "Anne.",
            "Another argument for this is that if you're in the interesting case, when the number of labeled points is very small, then.",
            "The unlabeled points would effectively swamp the information contained in the labels, and you would lose it if you just.",
            "Would add them without rescaling.",
            "OK, so it might seem sensible to choose a certain value of Lambda.",
            "But obviously.",
            "The tricky question is how do you choose Lambda?",
            "Cross validation for example.",
            "Well, if you got a very small label data set, how are you going to do cross validation?",
            "If you got 10 labels?",
            "You have to set aside a subset to do cross validation.",
            "You're not going to get very good results."
        ],
        [
            "So an elegant solution, OK?",
            "It was presented reasonably recently by coding and Yakola in 2002 is to look for stability.",
            "And the way they looked at it.",
            "Was basically it's a very simple and nice idea.",
            "So if you have a clustering problem.",
            "OK. Then your likelihood.",
            "With two classes, will certainly be bimodal.",
            "OK, because you can swap.",
            "The class is OK. You can say the parameter that you gave to class one is actually the parameter of Class 2.",
            "And nothing changes.",
            "OK, it's perfectly symmetrical.",
            "If you got, if you have the class labels instead, you're likely this unimodal.",
            "OK, if you got the class labels, you determine the maximum or delighted by taking the empirical covariance and the empirical mean, and that's the maximum of the likelihood for each class."
        ],
        [
            "Now the observation is that.",
            "We have Lambda equals zero.",
            "We're not considering the labels, so we are looking at an unsupervised problem, and it's a clustering problem, so we will have a likelihood with many modes.",
            "If you take Lambda equals one.",
            "We are completely ignoring the unlabeled data.",
            "We're looking at the supervised.",
            "Date problem for which we have a unimodal likelihood.",
            "So when you vary Lambda.",
            "You must cross a point where the single mode splits OK.",
            "So presumably it becomes flatter and flatter and at a certain .2 modes emerge.",
            "And."
        ],
        [
            "The proposal of Aquila encode viano, which was supported with obviously.",
            "More us strict argumentation and the one that was presented is that when you go from Lambda equals one to land equals 0, so from supervised to unsupervised, you're bound to encounter a critical Lambda star, which is say the maximum value for which you only have one mode and that should be.",
            "The optimal choice.",
            "In which sense it is the optimal choice, it's, let's say, is the maximum.",
            "Involvement of the unlabeled data that guarantees a unique solution.",
            "OK, so it's a very simple but elegant idea which they call stability.",
            "OK.",
            "So let's press on.",
            "Now these were was what we were going to say for generative models, as we've seen the very intuitive, there may be a way of going from super unsupervised towards semi supervised."
        ],
        [
            "We could also consider discriminative models.",
            "And again we have the same graphical model As for discriminative classification.",
            "We are interested in modeling.",
            "C given X rather than vice versa.",
            "And.",
            "We could just plow on as we always do, and we can write the total likelihood.",
            "For the unlabeled data and the labeled data.",
            "Given the parameters, well, it will factorize as.",
            "The likelihood for.",
            "The labels given.",
            "The.",
            "Put for the labels of the label data given the input of the label data times the likelihood for the label data and the unlabeled data given the parameter of the distribution that generates X. OK.",
            "So this is a.",
            "Total likelihood of the data, which is always what we want to maximize to find them."
        ],
        [
            "Amateurs.",
            "Now if we use Bayes theorem we can just, you know, try to find out what the posterior of the parameters.",
            "Remember, we always what we want to do is always estimating the parameters because once we have the parameters.",
            "We can plug them in and find the posteriors of the class labels, and we're fine.",
            "So if we look at what's the procedure of the parameters, given unlabeled data and the unlabeled data, well, we just.",
            "Use Bayes theorem.",
            "And now we observe that.",
            "We are the denominator in Bayes theorem is the marginal where we marginalized the parameters.",
            "Now this term.",
            "Does not depend on Theta, so it comes out of the integral and it simplifies with this term above.",
            "So effectively, the posterior over Theta is proportional.",
            "To the likely probability.",
            "All the classes.",
            "Given Excel and theater times, P of theater.",
            "OK. And this is quite worrying.",
            "What, why is it wearing is is everyone else worried?",
            "Or is it just me worried?",
            "That's absolutely correct.",
            "So the unlabeled data in a in a discriminative.",
            "What it's telling you.",
            "If you are Bayesian."
        ],
        [
            "And you use that graphical model, which is the graphical model of discriminative classification, and you want to introduce unlabeled data well.",
            "You may as well, but it won't make any difference whatsoever at all."
        ],
        [
            "So Bayesian methods in a straight, discriminative SSL cannot be employed, so you just can't do semisupervised with the basic method.",
            "There is a little bit of success when you're not Bayesian.",
            "But it generally is just a little bit of success.",
            "So if you actually compare the results with using the labels or not using the labels, using the unlabeled data, or ignoring the unlabeled data, the improvement is very small.",
            "For example, Anderson in 78 came up with.",
            "Semi supervised method for logistic regression of the discrete variables and that seemed to work alright, but it was a very special case and it's not clear how much.",
            "Information you can get.",
            "So.",
            "Obviously, people have been aware of this problem and they've come up with a solution and the solution, which is probably the most promising Ave. Is to change the graphical model and it's called regularization OK so?",
            "The problem came from the fact that you know theater M. You are a priority independent and when we plug this into Bayes theorem there are priority.",
            "They become also a posteriori independent.",
            "So if we add an extra arrow.",
            "Then maybe we can solve the problem.",
            "And we can OK, but what we're saying is now, is that our estimates of the parameters will depend on some characteristics of the input distribution.",
            "OK."
        ],
        [
            "So.",
            "As I said, the idea is some of the information in the generative process underlying X must be included into the discriminative technique in order to.",
            "Make use of unlabeled data.",
            "So can we call it still a discriminative approach?",
            "Well, probably not.",
            "So you know, the discriminative idea was we don't care about the harder problem of estimating the generative process underlying X.",
            "We only want to classify and in this case we must care at least a little bit.",
            "So it's not as strict discriminative.",
            "Approach.",
            "However, in practice, the hypothesis used for regularization are much weaker, so we don't want to model the whole of the generative process.",
            "We may just want to model things.",
            "Which are very weak indications of what the generative process looks like.",
            "Possibly."
        ],
        [
            "The most successful.",
            "Way of doing this is to use what they call the cluster assumption.",
            "OK. What is the cluster assumption?",
            "Well, formal statement is that the boundary between classes should cross areas of low data density.",
            "So as you see.",
            "We are thinking by data means the input state.",
            "OK, so we we are using information about the.",
            "Density that generates the input data X the way we are using it though is very weak.",
            "We just say.",
            "The boundary, which is effectively what determines the value of the parameters.",
            "Must fall in an area where the density of the data is low.",
            "And it does make sense as being a reasonable case, OK?",
            "So it's a fairly reasonable weak assumption.",
            "And as I said, I mean, I did say before that if your Bayesian you can't do semisupervised learning in a discriminative way, but.",
            "Actually, this assumption took hold a lot into non Bayesian methods as well.",
            "So when you have low density separation as VM's and stuff like that.",
            "Another way, perhaps an interesting, more interesting way to understand it is in terms of smoothness of the discriminative function.",
            "OK, so.",
            "I draw another picture, maybe over there this time.",
            "Effectively.",
            "In our case, the discriminant function would be the ratio of the likelihoods, and.",
            "What we want to say is that.",
            "Obviously the discriminate function will have to change OK to discriminate function will be 0.",
            "For example in the areas of class one and will be one in the area class.",
            "Two so perhaps would be something that looks like this.",
            "Now what we want is that if we got a cluster here.",
            "Then the discriminant function must very, very little where you have lots of data.",
            "So the boundaries which are given by the areas where the slope of the discriminant function is steep, those boundaries must fall where you don't have much data.",
            "OK.",
            "So this would be kind of the situation we're looking at.",
            "We've got lots of data, very little data here, lots of data here, and the boundary, so the steep bit indiscriminant must fall where there is little data.",
            "Yes.",
            "I'm wondering what happens if you go to 1500 or boredom instead.",
            "Soon.",
            "So then you might have loaded the data everywhere.",
            "Yep.",
            "Yeah, I mean you're not the first person to think about these objections, so the next few slides effectively address the problem.",
            "So very closely related, which effectively answers exactly the question you just."
        ],
        [
            "Post is the manifold assumption.",
            "So the manifold assumption says that.",
            "The data lies on a low dimensional submanifold over high dimensional space and this high dimensional space might be the real input space.",
            "Or might be a feature space to which you map the data first.",
            "OK, so OK if you have a high dimensional space you got low data density everywhere, but then the first thing you do is effectively almost the dimensionality reduction step, in which you say, well, the data lies on a manifold.",
            "Upload dimensions.",
            "And.",
            "What you effectively doing?",
            "The manifold assumption is just an add on to the cluster assumption.",
            "At least that's the way I see it and.",
            "You apply the cluster dimension in the cluster assumption.",
            "On the manifold which is in the high dimensional space.",
            "The key concept behind manifold assumptions is the mathematical theory of graphs, and in particular.",
            "The constant graph Laplacian and a lot of these work is closely related to spectral clustering as being pioneered by.",
            "Misha Belkin and Pathenia Yogi."
        ],
        [
            "So.",
            "This is very, very brief.",
            "Just to give you a super quick idea.",
            "Also, because we're running out of time.",
            "So the idea again we want discriminate functions which very little in regions of high data density.",
            "Right now suppose.",
            "That the density of the data over the manifold.",
            "Is given by P of X.",
            "You don't know it, but suppose it is like that.",
            "Then effectively what we are looking we want.",
            "This integral to be very small because this is the norm.",
            "Squared norm of the gradient of the discriminant function.",
            "So we want to allow this gradient to be sizeable only when this is very.",
            "Small.",
            "And we want to.",
            "Conversely, when this is big, this greater to be very small.",
            "So effectively we want this integral to be very, very small.",
            "OK. Now.",
            "If we are in Euclidean space, this is just the Laplacian of the function.",
            "FF is my discriminants.",
            "If we are on a manifold then that's the Laplace Beltrami operator which is just you know, the generalization of flash.",
            "It's just a derivative.",
            "OK is the square norm of the gradient effect.",
            "Now the problem is a we don't know the probability distribution B.",
            "We don't know.",
            "The manifold OK.",
            "So the idea is to.",
            "Approximate the manifold with a graph and then you can prove that the finite sample approximation to the Laplace Beltrami operator.",
            "So if you restrict the graph would be a graph was.",
            "And notes lie on the manifold, so you basically do a triangulation of the manifold.",
            "Then the restriction of the Laplace Beltrami operator onto the graph becomes what is known as the graph Laplacian, which is given by the adjacency matrix.",
            "Of the.",
            "Of the graph basically, and this is just some of the diagonal matrix whose entries are the sum of the rows of the adjacency matrix.",
            "This is a very common tool.",
            "Did you talk about graph Laplacians, nealen, right?",
            "OK, but this is very common in spectral clustering and spectral.",
            "You know methods.",
            "So the idea is.",
            "If we want to do SSL well, we look effectively in an unsupervised way we look at.",
            "Functions which becomes obviously vectors evaluated at each point of the graph.",
            "And we look at things that have a small.",
            "Value of this integral, which then becomes the inner product of the discriminant function with the Laplace graph Laplacian.",
            "That's the unsupervised part, which is spectral clustering effectively.",
            "And then we add.",
            "Apart which contains.",
            "The error given by F on the label data, so that's the basic idea behind.",
            "Graph methods in one slide.",
            "So the.",
            "The key point is.",
            "You want to have a manifold low dimensional manifold in a high dimensional space.",
            "You want to have things that very only where there is low data density.",
            "You can express that.",
            "Discriminants are very only on the low.",
            "Data density are effectively in the.",
            "Colonel Sordar, they've got very little.",
            "Area when acted upon by the Laplace Beltrami operator, you take a finite sample approximation, which is a graph.",
            "The Laplace Beltrami becomes a graph Laplacian.",
            "And that's your unsupervised bit.",
            "Structure of the data.",
            "Dictates where.",
            "The discriminant should be steep.",
            "And then you add a term that contains the labels in order to.",
            "Guide that further and using the label information as well.",
            "So that's.",
            "That's about all I wanted to tell you about.",
            "Semi supervised learning and.",
            "It does mean there is a lot of active research in these areas and lot of interesting papers.",
            "And hopefully.",
            "You know I what I all I aim to do was to just convey the intuition, at least as I understand it and.",
            "For you to be able to go and look at the semi supervised learning paper and kind of understand what perspective, what paradigms are employing, what kind of key methods they are using and so on.",
            "OK, so I'll be happy to take questions if any.",
            "Yep.",
            "No no yeah.",
            "Yep.",
            "Showing.",
            "No, but what is true is that there are.",
            "But there are non Bayesian methods where they do use unlabeled it.",
            "It doesn't make any assumptions about the input distribution versus the cost, so any frequencies method has to do that.",
            "It's just a Bayesian framework showing you have to be explicit about how you're making that something probabilistic.",
            "Reducing supervised learning, Bayesian way bring this way.",
            "Don't look at what the problem is.",
            "Complications are.",
            "You may be making lots of implicit assumption still.",
            "Yeah, so it might be very hard to see where the assumption is coming in, that's that.",
            "I think some of the people that authored those papers would disagree with that, but.",
            "Right, OK, well my dear Sir.",
            "Yeah, mateas.",
            "Sega seems to believe that some numbers and.",
            "When is book it?",
            "Does he says the opposite.",
            "So it might be that he's changed his mind, but.",
            "Relationship.",
            "Yep.",
            "Yeah.",
            "Yeah.",
            "Yeah, yeah.",
            "Yeah, I mean that's the thing.",
            "I mean, I don't really know what some frequentist might be doing.",
            "You know it's.",
            "No, no no.",
            "I mean, if you got that graph, you can't do.",
            "You can't use the unlabeled data.",
            "Yeah, I mean, the thing that I'm saying is that they might have some tricks by which unlabeled data comes in without using that graphical.",
            "Sorry, of course, if you got that graph, then that's that's it.",
            "Yes.",
            "By creating an extra link graph where you're not really claiming that this reflects reality, you're not claiming that.",
            "I'm going to.",
            "Well, I think the cluster assumption.",
            "I I'm not sure about that.",
            "I think people believe in the cluster assumption, which is exactly a way of saying the input distribution.",
            "So say the means are far and the.",
            "You know, basically there's a big saying that mixture of Gaussian case.",
            "There's a big Mahalanobis distance between the two classes.",
            "That's concerned entirely.",
            "With this set of parameters.",
            "And we're seeing that the discriminate, that is where the boundary comes, which is this set of parameters.",
            "Is influenced by this set of parameters, so they actually when you put the cluster assumption.",
            "If you and you normally believe in it, you're actually making an assumption.",
            "With this arrow exists, it's not an approximation.",
            "But as I said, you know you still don't want to estimate all these parameters.",
            "So do you want to have?",
            "To have very weak forms in which incorporate this information in order for your computational complexity to remain reasonable.",
            "Yes.",
            "Last line.",
            "How do you?",
            "So the first thing you would do is you.",
            "Do you compute the adjacency matrix, which is basically say you take an RBF kernel for example, which is mapping you in feature space and you take the adjacency matrix is.",
            "Export minus XI minus XJ squared over Sigma squared, so that's the IJ entry of W. OK. Yeah, so this is an N by N matrix.",
            "This is just obtained by summing the rows and having a diagonal matrix.",
            "F is a vector, your discriminant function is a vector of zeros and ones.",
            "Who is 0 for class zero and one for Class 1?",
            "And you plug it in, you just literally do F transpose LF.",
            "Plus the misclassification error and you search for the best.",
            "No.",
            "You just mapped into a feature space.",
            "You're not really doing the dimensionality reduction.",
            "You're extracting a vector.",
            "Which, if you were doing you know it is using a spectral technique so it does fall into the spectrum.",
            "I mean it's got links for example with kernel PCA and things like that, but.",
            "But he's not a dimensionality reduction, you're just interested in evaluating the vector F, which is a vector of zeros and ones, and it's the one that classifieds your points.",
            "Yeah, it was a bit short on this slide.",
            "Probably Ukraine, right?",
            "Anymore questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you Tony.",
                    "label": 0
                },
                {
                    "sent": "I need to turn this on.",
                    "label": 0
                },
                {
                    "sent": "I press.",
                    "label": 0
                },
                {
                    "sent": "I did it's fine.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a better title would have been different types of learning, but I want to focus mostly on semi supervised learning, although I will give a pretty well sizable introduction also to two other types of learning which are closely related which are unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And supervised learning, and I'll give examples, and in particular I'll also introduce the EM algorithm when I talk about unsupervised learning, which is also fundamental.",
                    "label": 1
                },
                {
                    "sent": "Tool in semi supervised learning, particularly generative models.",
                    "label": 0
                },
                {
                    "sent": "And then I'll basically talk mostly about different types of semi supervised learning, so you can have generative models and you can have discriminative models for semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And both of them have got problems and they've got advantages and I'll briefly discuss these.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First of all, the disclaimer, this is not going to be a technical talk.",
                    "label": 1
                },
                {
                    "sent": "I will not present any new results and certainly not any results of mine.",
                    "label": 0
                },
                {
                    "sent": "Most of the things that will tell you could be easily covered in Chris Bishop's book, and in Matias Igas very nice chapter in the semi supervised learning book which came out last year.",
                    "label": 0
                },
                {
                    "sent": "A couple more things, slightly more advanced and I'll just mention them in passing.",
                    "label": 0
                },
                {
                    "sent": "As I said.",
                    "label": 0
                },
                {
                    "sent": "Probably half of the time will be talking about supervised and unsupervised learning, not just semi supervised, and I will be unashamedly probabilistic, if not Bayesian.",
                    "label": 1
                },
                {
                    "sent": "Maybe becauses I will talk a lot about maximum likelihood and obviously Bayesians don't like maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, there are several different ways of learning.",
                    "label": 0
                },
                {
                    "sent": "A very important chapter in machine learning is reinforcement learning, where you learn a strategy to optimize a reward.",
                    "label": 1
                },
                {
                    "sent": "And it's a very active area of research has got very close links with control theory with decision theory, and that's all I'm going to tell.",
                    "label": 0
                },
                {
                    "sent": "Tell you about enforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The other two.",
                    "label": 1
                },
                {
                    "sent": "Classical areas of learning are supervised learning where you want to learn a map.",
                    "label": 0
                },
                {
                    "sent": "You're given inputs and you're given outputs and you want to learn the map and unsupervised learning where.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, you want to estimate the density that is behind your data.",
                    "label": 0
                },
                {
                    "sent": "So let's look go into reverse order.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at unsupervised learning first.",
                    "label": 1
                },
                {
                    "sent": "So what's the scenario in unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "OK, we're given.",
                    "label": 0
                },
                {
                    "sent": "Data which I indicate as vectors X.",
                    "label": 0
                },
                {
                    "sent": "Any general we're given more than one data point, of course, and we want to estimate the density that generates the data so.",
                    "label": 1
                },
                {
                    "sent": "Broadly speaking, most of unsupervised learning can be classed as test density estimation.",
                    "label": 0
                },
                {
                    "sent": "Although there are weaker forms such as you know, quantile estimation and so on.",
                    "label": 0
                },
                {
                    "sent": "And I will make use of graphical models to describe the different types of learning.",
                    "label": 0
                },
                {
                    "sent": "So how many of you know about graphical models?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What you know about graphical models?",
                    "label": 0
                },
                {
                    "sent": "Why didn't you raise your hand?",
                    "label": 0
                },
                {
                    "sent": "Good so.",
                    "label": 0
                },
                {
                    "sent": "So all you get a very good lecture tomorrow from zooming ghahremani who's?",
                    "label": 0
                },
                {
                    "sent": "Certainly one of the world leading experts on graphical models, so I want to tell you.",
                    "label": 0
                },
                {
                    "sent": "Virtually anything about graphical models, but I'll just explain.",
                    "label": 0
                },
                {
                    "sent": "What they are, and I put them on the buffer board there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "Soap.",
                    "label": 0
                },
                {
                    "sent": "Let's look at that graphical model.",
                    "label": 0
                },
                {
                    "sent": "We got four quantities inside the circles.",
                    "label": 0
                },
                {
                    "sent": "Which, broadly speaking.",
                    "label": 0
                },
                {
                    "sent": "Random variable and some of them.",
                    "label": 0
                },
                {
                    "sent": "We may also want to consider their parameters.",
                    "label": 0
                },
                {
                    "sent": "Whatever that means.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case we also have arrows within the circles.",
                    "label": 0
                },
                {
                    "sent": "The arrows give us the structure of the conditional probability of the model.",
                    "label": 0
                },
                {
                    "sent": "So what they're saying is that.",
                    "label": 0
                },
                {
                    "sent": "The conditional probability of X, so the probability distribution of X depends on Y and on Theta OK.",
                    "label": 0
                },
                {
                    "sent": "So if I wanted to write the whole joint probability.",
                    "label": 0
                },
                {
                    "sent": "XY, Theta and \u03c0. OK, it basically tells me that X does not depend directly on.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 so I could factorize this as.",
                    "label": 0
                },
                {
                    "sent": "P of X.",
                    "label": 0
                },
                {
                    "sent": "Given Y and theater.",
                    "label": 0
                },
                {
                    "sent": "And then P of Y which Y depends on \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And then I could.",
                    "label": 0
                },
                {
                    "sent": "Either place a prior on Theta an on \u03a0, or I could leave them as parameters and estimated by maximum likelihood, but that's holding means it's just a very simple.",
                    "label": 0
                },
                {
                    "sent": "Pictorial way of identifying who depends on who in our models.",
                    "label": 0
                },
                {
                    "sent": "So in general, well in a lot of cases, although not in all cases, in unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "We assume that there are some latent variables.",
                    "label": 0
                },
                {
                    "sent": "That control the generative structure underlying probably underlie our data X.",
                    "label": 0
                },
                {
                    "sent": "So in this case I'm saying that X depends on Y and for example, why could be a continuous variable which has got lower dimensionality than X?",
                    "label": 0
                },
                {
                    "sent": "In that case, what I'm saying is that X.",
                    "label": 1
                },
                {
                    "sent": "Given some parameters is determined by the value of Y, which lives in a lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And what I'm doing is dimensionality reduction and.",
                    "label": 0
                },
                {
                    "sent": "You know everything about dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Now that you've heard Neil.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Continuous why is not going to be talked about in this lecture?",
                    "label": 0
                },
                {
                    "sent": "What is the other option?",
                    "label": 0
                },
                {
                    "sent": "The other option is that Y is a discrete variable.",
                    "label": 0
                },
                {
                    "sent": "If Y is a discrete variable, the kind of case we are looking at this clustering.",
                    "label": 0
                },
                {
                    "sent": "So why can take a few?",
                    "label": 0
                },
                {
                    "sent": "Values say what could be 01 if we had only two classes and what we're saying is that the generative structure of X.",
                    "label": 0
                },
                {
                    "sent": "Would be different according to whether Y.",
                    "label": 0
                },
                {
                    "sent": "0 so it is in the zero cluster or wise one.",
                    "label": 0
                },
                {
                    "sent": "It is in the one cluster.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this in detail in an example, OK. Oh, incidentally.",
                    "label": 0
                },
                {
                    "sent": "Whenever wise discrete I'll call it see OK. Becausw stands for class.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In the example we look at is the daddy of all clustering is the mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "We got vector measurements XI.",
                    "label": 1
                },
                {
                    "sent": "I going from one to N. And the latent variable.",
                    "label": 0
                },
                {
                    "sent": "Is well, we could think it's either AK dimensional binary vector class memberships or we could think it's a categorical variable.",
                    "label": 0
                },
                {
                    "sent": "Want to K?",
                    "label": 0
                },
                {
                    "sent": "OK, it's more convenient in this case to think of it as a K dimensional vector with zeros and one.",
                    "label": 0
                },
                {
                    "sent": "And it only has one one which corresponds to the cluster in which the.",
                    "label": 0
                },
                {
                    "sent": "Point needs to be classed OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we know the class.",
                    "label": 0
                },
                {
                    "sent": "So if we know the value of CJ4 point J.",
                    "label": 0
                },
                {
                    "sent": "Then if we know that point X is generated by the J class.",
                    "label": 0
                },
                {
                    "sent": "Then what we are saying is that the probability distribution, the density underlying X is a Gaussian with a certain mean mu, J and a certain covariance matrix Sigma J.",
                    "label": 0
                },
                {
                    "sent": "And we also have.",
                    "label": 0
                },
                {
                    "sent": "Prior distribution over the class membership variables CJ.",
                    "label": 0
                },
                {
                    "sent": "Which is given by this Pi parameters.",
                    "label": 0
                },
                {
                    "sent": "Which have to sum to one OK?",
                    "label": 1
                },
                {
                    "sent": "In order to be a distribution.",
                    "label": 0
                },
                {
                    "sent": "So how many of you have seen mixture of Gaussians before?",
                    "label": 0
                },
                {
                    "sent": "So a fair half of them, so please interrupt me anytime if you don't understand what I mean.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying.",
                    "label": 0
                },
                {
                    "sent": "So in the case looking at the graphical model Now, X is what I called X.",
                    "label": 0
                },
                {
                    "sent": "The theater parameters.",
                    "label": 0
                },
                {
                    "sent": "The parameters of.",
                    "label": 0
                },
                {
                    "sent": "The class conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So there are the means and covariances of each component in the mixture.",
                    "label": 1
                },
                {
                    "sent": "And the pie parameters prior on the class memberships, which are also called pie pie in here.",
                    "label": 0
                },
                {
                    "sent": "So how do we estimate?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such a model?",
                    "label": 0
                },
                {
                    "sent": "Well, what do we want to do with this?",
                    "label": 0
                },
                {
                    "sent": "OK, we want to estimate the various parameters Sigma, JMU J and I forgot to, Jayden and Paije.",
                    "label": 1
                },
                {
                    "sent": "So we want to somehow learn them given data.",
                    "label": 0
                },
                {
                    "sent": "And also will be interested of course in.",
                    "label": 0
                },
                {
                    "sent": "Given a data point telling what of the clusters has generated it, so which clusters it belongs to, and this is conveniently expressed.",
                    "label": 0
                },
                {
                    "sent": "By looking at the gamma IJS which are called the responsibilities, which are the probability the posterior probability that the class.",
                    "label": 1
                },
                {
                    "sent": "Variable pertaining to the point I is JK, so it's called responsibility because it gives you the degree.",
                    "label": 0
                },
                {
                    "sent": "By which the J clustering is glass cluster is responsible for having generated the fifth point.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We've got a marginal likelihood, So what we could do is just plug it into an optimizer like scaled conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "And optimize it with respect to \u03c0 to mu Anta Sigma.",
                    "label": 0
                },
                {
                    "sent": "But that's not very.",
                    "label": 0
                },
                {
                    "sent": "Elegant, so the better method and very widely used method is the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, how many of you know about the yam algorithm?",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so we'll describe it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some detail, because it's quite important.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The EM algorithm's M stands for expectation maximization, and it's an iterative procedure.",
                    "label": 0
                },
                {
                    "sent": "That at each step increases the likelihood an if the likelihood is bounded.",
                    "label": 0
                },
                {
                    "sent": "So if your model is convergent.",
                    "label": 0
                },
                {
                    "sent": "You will be guaranteed to always increase the likelihood and find at least a local optimum.",
                    "label": 0
                },
                {
                    "sent": "Now the log likelihood we want to maximize.",
                    "label": 0
                },
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's a log of a sum of Gaussian bits, OK?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "A log of a sum is not nice.",
                    "label": 1
                },
                {
                    "sent": "Logs are very nice because a lot of the product is the sum of the logs and often we have probabilities that.",
                    "label": 0
                },
                {
                    "sent": "I mean combine to be product, so taking a log likelihood is a sensible thing, But in this case we get a nasty.",
                    "label": 0
                },
                {
                    "sent": "Expression.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want to do is to optimize that each step in the iterative procedure, something that does not contain this sum inside the log.",
                    "label": 1
                },
                {
                    "sent": "OK, the key mathematical tools for doing this is Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Which I will now give.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "An intuitive proof of.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does Jensen's inequality tell you?",
                    "label": 0
                },
                {
                    "sent": "Well, the formal statement is for every concave function.",
                    "label": 0
                },
                {
                    "sent": "And for every random variable X and probability distribution Q of X.",
                    "label": 1
                },
                {
                    "sent": "On variable X.",
                    "label": 0
                },
                {
                    "sent": "The function of the expectation of X under Q of X.",
                    "label": 0
                },
                {
                    "sent": "Is always greater or equal than the expectation of the function of X taken under the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "QX and concave is Neil likes to say means that the function makes a cave, so you see, this is a cave, so it's a concave function.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And I generated it using a piece of a cubic function OK.",
                    "label": 0
                },
                {
                    "sent": "So how do we go about proving this?",
                    "label": 0
                },
                {
                    "sent": "Well, let's consider the case where X around a variable X only takes a finite number of points.",
                    "label": 0
                },
                {
                    "sent": "OK, and in this case we have.",
                    "label": 0
                },
                {
                    "sent": "Four instances of X.",
                    "label": 0
                },
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "Abscissa or X coordinate of these points is my values of X.",
                    "label": 0
                },
                {
                    "sent": "My function is this blue function OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I can construct this figure.",
                    "label": 0
                },
                {
                    "sent": "Basically, by joining the path XF of X.",
                    "label": 0
                },
                {
                    "sent": "So I get there are four values I get.",
                    "label": 0
                },
                {
                    "sent": "Quadrilateral figure is that the right English word, I don't know.",
                    "label": 0
                },
                {
                    "sent": "So it's a quadrilateral.",
                    "label": 0
                },
                {
                    "sent": "And the center of mass of the quadrilateral is obtained by taking the mean of the axis and the mean of the wise.",
                    "label": 0
                },
                {
                    "sent": "So this point, which is the center of mass, has got X coordinate.",
                    "label": 0
                },
                {
                    "sent": "The average under X.",
                    "label": 0
                },
                {
                    "sent": "Under Q of X of X and it's got this Y coordinate.",
                    "label": 0
                },
                {
                    "sent": "The average under Q of X of F of X. OK, are we all agreeing on this?",
                    "label": 0
                },
                {
                    "sent": "OK. Now since.",
                    "label": 0
                },
                {
                    "sent": "This is concave.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "Poly Polygon subtended by concave.",
                    "label": 0
                },
                {
                    "sent": "Curve is convex, so the center of mass lies within these quadrilateral always.",
                    "label": 0
                },
                {
                    "sent": "Whatever four points are taken.",
                    "label": 0
                },
                {
                    "sent": "The center of Mass would lie inside, which is pretty intuitive.",
                    "label": 0
                },
                {
                    "sent": "So we've seen the center of Mass has as Y coordinate the expectation of F under Q of X.",
                    "label": 0
                },
                {
                    "sent": "What would be the F of the expectation of X while the FD expectation of X is taken this point and look very crosses are function.",
                    "label": 0
                },
                {
                    "sent": "And as you see, it's above.",
                    "label": 0
                },
                {
                    "sent": "OK. You can generalize these two as many points you want to have, and you can also generalize it to the whole of a continuous interval, in which you basically take just the curve.",
                    "label": 0
                },
                {
                    "sent": "And that tends to that.",
                    "label": 0
                },
                {
                    "sent": "The expectation of F. Is always below.",
                    "label": 0
                },
                {
                    "sent": "F of the expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a cartoon proof, but.",
                    "label": 0
                },
                {
                    "sent": "I think it's a fairly compelling argument and I read it in David Mackay's book.",
                    "label": 0
                },
                {
                    "sent": "OK, so everyone happy with the Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "That's a mathematical statement.",
                    "label": 0
                },
                {
                    "sent": "Not the proof.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, why are we interested in Jensen's inequality?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, if you recall the thing we wanted to.",
                    "label": 0
                },
                {
                    "sent": "Look at was the log.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "This quantity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Which is effectively an expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the expectation of the conditional likelihood under the prime.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the log is a concave function.",
                    "label": 0
                },
                {
                    "sent": "It doesn't quite look like this.",
                    "label": 0
                },
                {
                    "sent": "It goes up, but it's always concave, so the curvature points downwards.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can use Jensen's inequality.",
                    "label": 1
                },
                {
                    "sent": "And before we do that will.",
                    "label": 0
                },
                {
                    "sent": "Introduce one of the best used tricks.",
                    "label": 0
                },
                {
                    "sent": "So we want to optimize this quantity here.",
                    "label": 0
                },
                {
                    "sent": "That's exactly the same as what we had before.",
                    "label": 0
                },
                {
                    "sent": "Now the famous trick is to introduce surreptitiously a quantity Q of C, which will be a probability distribution, and we multiply and divide by it.",
                    "label": 0
                },
                {
                    "sent": "So we're not doing anything.",
                    "label": 0
                },
                {
                    "sent": "But this basically tells us that we are taking the log of the expectation of this ratio.",
                    "label": 0
                },
                {
                    "sent": "And the probability distribution Q of C. OK now key of C is a probability distribution with.",
                    "label": 0
                },
                {
                    "sent": "Use it as a condition and we're taking the log of an expectation.",
                    "label": 0
                },
                {
                    "sent": "Now we can use Jensen's.",
                    "label": 0
                },
                {
                    "sent": "And Jensen tells us that the log of the expectation.",
                    "label": 1
                },
                {
                    "sent": "Is greater or equal?",
                    "label": 0
                },
                {
                    "sent": "Then the expectation of the log.",
                    "label": 0
                },
                {
                    "sent": "Now this is a log of a ratio, so logs of ratios are nice, so we can say it's the.",
                    "label": 0
                },
                {
                    "sent": "Difference between the expectation of log of P of X.",
                    "label": 0
                },
                {
                    "sent": "And the expectation of log of Q on the Q which is also called the entropy of the distribution.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The further thing to notice.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "I got a greater equal here, so the first question people should ask me.",
                    "label": 0
                },
                {
                    "sent": "Well, whenever you have a bound, is it a tight bound?",
                    "label": 0
                },
                {
                    "sent": "Is it you know I'll be bounding something with something that is completely unrelated is always much bigger than the reality and the truth is that it is not much bigger.",
                    "label": 0
                },
                {
                    "sent": "But actually we can also tell that when Q of C so are arbitrary distribution that we introduced.",
                    "label": 0
                },
                {
                    "sent": "Surreptitiously in this mathematical step, when QFC is the posterior, so is QFC given X.",
                    "label": 0
                },
                {
                    "sent": "This inequality actually saturated, so it becomes an equality.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is that if Q of C is the posterior then.",
                    "label": 0
                },
                {
                    "sent": "P of X given CP of X&C can also be written as.",
                    "label": 0
                },
                {
                    "sent": "P of.",
                    "label": 0
                },
                {
                    "sent": "Write it over here P. Of XC can be factorized using the fundamental rule of probability.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 1
                },
                {
                    "sent": "So if Q of C is the posterior.",
                    "label": 0
                },
                {
                    "sent": "P of X C / P of C given X.",
                    "label": 0
                },
                {
                    "sent": "Is equal to P of X.",
                    "label": 0
                },
                {
                    "sent": "Which does not depend on C. And therefore when we take the expectation under.",
                    "label": 0
                },
                {
                    "sent": "A probability distribution, or see it doesn't do anything to it.",
                    "label": 0
                },
                {
                    "sent": "So if Q of C is the posterior, this ratio does not depend on C and the log of the expectation is the same thing as the expectation of the log, yes?",
                    "label": 0
                },
                {
                    "sent": "No, not necessarily.",
                    "label": 0
                },
                {
                    "sent": "I mean, that could be QFC is just a completely arbitrary probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So for now, between probability distribution we have this inequality.",
                    "label": 0
                },
                {
                    "sent": "But if Q of C is the posterior over the class.",
                    "label": 0
                },
                {
                    "sent": "Then this inequality is an equality, so it becomes equal.",
                    "label": 0
                },
                {
                    "sent": "Not greater equal.",
                    "label": 0
                },
                {
                    "sent": "That's the key point.",
                    "label": 0
                },
                {
                    "sent": "So the idea?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Behind them, and this is a cartoon.",
                    "label": 0
                },
                {
                    "sent": "It is often seen that.",
                    "label": 0
                },
                {
                    "sent": "OK, if you know the value of the parameters then.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can compute.",
                    "label": 0
                },
                {
                    "sent": "The posterior using this formula OK if you know the value of the parameters, you can just Pi Sigma mu.",
                    "label": 1
                },
                {
                    "sent": "You can plug them in and you find the posteriors for each point.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is you don't know the value.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameters, but for a fixed value of the parameters you can compute the posterior in Jensen's inequality now.",
                    "label": 1
                },
                {
                    "sent": "Gives you a bound that is tight.",
                    "label": 0
                },
                {
                    "sent": "Is equal.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Obviously though, when you vary the the value of the parameters, then that bound will not be tight and you will get a function that is consistently below your.",
                    "label": 0
                },
                {
                    "sent": "Marginal likelihood OK, your log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Initialize with a certain value.",
                    "label": 1
                },
                {
                    "sent": "The parameters you find the posterior.",
                    "label": 0
                },
                {
                    "sent": "Then you look at.",
                    "label": 0
                },
                {
                    "sent": "What is the maximum value of that bound?",
                    "label": 0
                },
                {
                    "sent": "In the space of parameters and, the hope is that it would be much easier to compute.",
                    "label": 0
                },
                {
                    "sent": "So you do an EM step, you find the maximum you go up.",
                    "label": 0
                },
                {
                    "sent": "Which always increases the likelihood.",
                    "label": 1
                },
                {
                    "sent": "And then you've got a new value of the parameters, and you can recompute your posterior and that will bring you up to the bound, which is always above because it's a bound.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this way you iterate between EM steps where you optimize parameters and E steps.",
                    "label": 0
                },
                {
                    "sent": "When you compute posterior distributions and you're guaranteed to climb up your log likelihood, which may be very nasty, but the bound is generally much more tractable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I said here exercise M for mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And that should.",
                    "label": 0
                },
                {
                    "sent": "I think it's worthwhile to do this exercise.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And so first of all.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's rewrite the bound OK.",
                    "label": 1
                },
                {
                    "sent": "So the bound which I've got is that.",
                    "label": 0
                },
                {
                    "sent": "Log off the expectation of P of.",
                    "label": 0
                },
                {
                    "sent": "XC under.",
                    "label": 0
                },
                {
                    "sent": "QFC greater equal.",
                    "label": 0
                },
                {
                    "sent": "Then the expectation of the log of the joint.",
                    "label": 1
                },
                {
                    "sent": "Plus the entropy OK?",
                    "label": 0
                },
                {
                    "sent": "Now notice that the entropy of Q does not depend on the value of the parameters and the entropy distribution does not depend on and.",
                    "label": 1
                },
                {
                    "sent": "My joint.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution was.",
                    "label": 0
                },
                {
                    "sent": "P of X, C. Is.",
                    "label": 0
                },
                {
                    "sent": "P of X given C * P of C. And it's.",
                    "label": 0
                },
                {
                    "sent": "A normal distribution with mu J.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Sigma J.",
                    "label": 0
                },
                {
                    "sent": "Times \u03c0 J which is the prior on the class OK.",
                    "label": 0
                },
                {
                    "sent": "So using this bound, forget about entropy of the of the discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "If I got a value, the parameters I can compute.",
                    "label": 0
                },
                {
                    "sent": "The posteriors, which were the responsibilities which are given by.",
                    "label": 0
                },
                {
                    "sent": "This is the responsibility that point high is generated by class J, so it's Pi J times.",
                    "label": 0
                },
                {
                    "sent": "On XI.",
                    "label": 0
                },
                {
                    "sent": "MU J. Sigma J.",
                    "label": 0
                },
                {
                    "sent": "Of son of a kalaty of Pikey.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I want to take the expectation of the joint probability so my joint probability that the point XI that we observe a point XI and that it comes from Class J is given by this expression.",
                    "label": 0
                },
                {
                    "sent": "So it's one of a sqrt 2\u03c0 determinant of Sigma J. Expo minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "XI minus mu J. Transpose Sigma to the minus one.",
                    "label": 0
                },
                {
                    "sent": "XI minus mu J Sigma J 2 -- 1.",
                    "label": 0
                },
                {
                    "sent": "OK times.",
                    "label": 0
                },
                {
                    "sent": "Paije I take the log of this.",
                    "label": 0
                },
                {
                    "sent": "Slog paije plus.",
                    "label": 0
                },
                {
                    "sent": "Actually, minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "Log of 2\u03c0 -- 1/2.",
                    "label": 0
                },
                {
                    "sent": "Log of the determinant of Sigma J.",
                    "label": 1
                },
                {
                    "sent": "Minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "XI minus mu J. Transposed Sigma to the minus one.",
                    "label": 0
                },
                {
                    "sent": "XI minus mu J. OK. Now if I want to take the expectation under the posterior distribution, well I have to use the gamma IJS.",
                    "label": 0
                },
                {
                    "sent": "So the expectation.",
                    "label": 0
                },
                {
                    "sent": "Under the posterior that I have observed, the point XI coming from class CJ is.",
                    "label": 0
                },
                {
                    "sent": ", I J.",
                    "label": 0
                },
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "This thing OK?",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Since I'm taking the expected, the log of the joint probability overall points, the joint probability of all points is the product of the probability.",
                    "label": 0
                },
                {
                    "sent": "Of each point.",
                    "label": 0
                },
                {
                    "sent": "So this becomes some.",
                    "label": 0
                },
                {
                    "sent": "Over I. OK, that's my bound.",
                    "label": 0
                },
                {
                    "sent": "And now I can maximize it by just doing computing the derivatives if I do.",
                    "label": 0
                },
                {
                    "sent": "The first thing I could look at is to find out what by Jay is.",
                    "label": 0
                },
                {
                    "sent": "So now I call this.",
                    "label": 0
                },
                {
                    "sent": "Call this thing L. That's my bound.",
                    "label": 0
                },
                {
                    "sent": "So I may want to do DL.",
                    "label": 0
                },
                {
                    "sent": "Indie pie.",
                    "label": 0
                },
                {
                    "sent": "Jay and I get.",
                    "label": 0
                },
                {
                    "sent": "Sum over I of gamma IJ times.",
                    "label": 0
                },
                {
                    "sent": "1 / \u03c0 J.",
                    "label": 0
                },
                {
                    "sent": "Equals 0.",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much, OK?",
                    "label": 0
                },
                {
                    "sent": "Yes, as you pointed out, the Pie JS are not independent.",
                    "label": 0
                },
                {
                    "sent": "I mean if we said they were prior probabilities over the classes, so they have to sum to one and if I just optimize like that, I run into trouble as I was showing you.",
                    "label": 0
                },
                {
                    "sent": "So I add the LaGrange multiplier times.",
                    "label": 1
                },
                {
                    "sent": "The constraint, the sum of a J of Pi J.",
                    "label": 0
                },
                {
                    "sent": "Must be equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So that's the thing I need to optimize, thank you.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So if I look at that, I get an extra term.",
                    "label": 0
                },
                {
                    "sent": "Plus Lambda.",
                    "label": 0
                },
                {
                    "sent": "Equals you.",
                    "label": 0
                },
                {
                    "sent": "OK, now I can rearrange this.",
                    "label": 0
                },
                {
                    "sent": "Now how do we find Lambda?",
                    "label": 0
                },
                {
                    "sent": "Well, I'll say the solution here.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we run out of time.",
                    "label": 0
                },
                {
                    "sent": "We sum if we sum the whole thing both left inside and right inside.",
                    "label": 0
                },
                {
                    "sent": "Over Jay.",
                    "label": 0
                },
                {
                    "sent": "Now the writer side remains 0.",
                    "label": 0
                },
                {
                    "sent": "This they sum to one, so this becomes.",
                    "label": 0
                },
                {
                    "sent": "Lambda and the sum of the.",
                    "label": 0
                },
                {
                    "sent": "Responsibilities?",
                    "label": 0
                },
                {
                    "sent": "Over both high and J is equal to the total number of points.",
                    "label": 0
                },
                {
                    "sent": "OK, because I can swap the sum over inj.",
                    "label": 0
                },
                {
                    "sent": "Gamma AJ is a posterior probability.",
                    "label": 0
                },
                {
                    "sent": "But the point I was generated by class J.",
                    "label": 0
                },
                {
                    "sent": "So if I sum over all JS it's one or something over I I get N. She tells me that.",
                    "label": 0
                },
                {
                    "sent": "Lambda has to be equal to minus N and then from here I get that Pi J.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "The sum over I of gamma IJ.",
                    "label": 0
                },
                {
                    "sent": "Over N. OK, and this is often.",
                    "label": 0
                },
                {
                    "sent": "Called also Angie over N 'cause basically.",
                    "label": 0
                },
                {
                    "sent": "If Gamma IJS was 01.",
                    "label": 0
                },
                {
                    "sent": "This would be the number of points that came from Class J. Hi Jay, I'm not 0 ones, but it's the total responsibility for class J so effectively what it's telling us is that the prior over the class membership is the fraction of points you get in that class.",
                    "label": 0
                },
                {
                    "sent": "Now that was the hard bit because it had the LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "I can now plug that in and start computing.",
                    "label": 0
                },
                {
                    "sent": "Derivatives of.",
                    "label": 0
                },
                {
                    "sent": "I have sorry or my bound, for example, with respect to mu J.",
                    "label": 0
                },
                {
                    "sent": "And this will involve a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Matrix differentiation, which I don't have time to cover, but if you rewrite this as a trace then this becomes.",
                    "label": 0
                },
                {
                    "sent": "Sigma minus one.",
                    "label": 0
                },
                {
                    "sent": "Sum over I gamma IJ Sigma minus one times XI minus mu J.",
                    "label": 0
                },
                {
                    "sent": "Which is equal to 0, which tells you that.",
                    "label": 0
                },
                {
                    "sent": "Mudrey is equal to.",
                    "label": 0
                },
                {
                    "sent": "Some of her I. , I J. Xcite over Angie.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a nice result.",
                    "label": 0
                },
                {
                    "sent": "It tells you that basically it's the weighted.",
                    "label": 0
                },
                {
                    "sent": "Mean of all the points weighted by the probability posterior probability that that point is in that cluster in cluster J.",
                    "label": 0
                },
                {
                    "sent": "And similarly you get the Sigma J.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 1
                },
                {
                    "sent": "OK, Sigma J is the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "Of the covariance for the cluster J. OK.",
                    "label": 0
                },
                {
                    "sent": "But the crucial thing to bring home is that by using Jensen's inequality, the M step equations that each step is is trivial.",
                    "label": 0
                },
                {
                    "sent": "In this case you know it's just.",
                    "label": 0
                },
                {
                    "sent": "Computing, plugging the parameters and computed the M step equations.",
                    "label": 0
                },
                {
                    "sent": "I could solve them analytically OK.",
                    "label": 0
                },
                {
                    "sent": "So it's a very effective.",
                    "label": 0
                },
                {
                    "sent": "System.",
                    "label": 0
                },
                {
                    "sent": "I don't have to do gradient descent or anything like that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's all I'm going to say about unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "What about supervised learning?",
                    "label": 0
                },
                {
                    "sent": "As I said before, that's the other traditional.",
                    "label": 0
                },
                {
                    "sent": "Piece of.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "And the data in this case consists of vector input X and.",
                    "label": 1
                },
                {
                    "sent": "Output target values Y.",
                    "label": 0
                },
                {
                    "sent": "So I've been told that you got an input X, which is a certain vector and the value corresponding to that vector is, why?",
                    "label": 1
                },
                {
                    "sent": "What we want to learn is the map between X&Y.",
                    "label": 0
                },
                {
                    "sent": "Which would also we could also equivalently call as the conditional probability of observing Y given X.",
                    "label": 1
                },
                {
                    "sent": "It's evaluated using the error, the reconstruction error, or the classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "And again it can be divided into 2.",
                    "label": 0
                },
                {
                    "sent": "Subgroups, if you have a Y which is a continuous variable.",
                    "label": 0
                },
                {
                    "sent": "What you're doing is basically learning a map between continuous variables, and that's called regression.",
                    "label": 0
                },
                {
                    "sent": "If you got a discrete Y.",
                    "label": 0
                },
                {
                    "sent": "Then that's classification.",
                    "label": 0
                },
                {
                    "sent": "Classification is what we're going to talk about because almost all of semi supervised learning is on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are two types of classification methods.",
                    "label": 0
                },
                {
                    "sent": "First one is the generative paradigm.",
                    "label": 1
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Start by saying well.",
                    "label": 0
                },
                {
                    "sent": "We want to model the class conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "So we got some classes we want to model.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of observing X given?",
                    "label": 0
                },
                {
                    "sent": "The Class C. We estimate the parameters.",
                    "label": 0
                },
                {
                    "sent": "For example, using maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "And we assign.",
                    "label": 0
                },
                {
                    "sent": "Then we compute.",
                    "label": 0
                },
                {
                    "sent": "Once we have the parameters and we have the prior model of the class conditional probabilities, we can compute posteriors.",
                    "label": 0
                },
                {
                    "sent": "For C / X given X, which is what we want, that's the map.",
                    "label": 0
                },
                {
                    "sent": "OK, if I'm given X, where am I?",
                    "label": 0
                },
                {
                    "sent": "Where is my see likely to be?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Well you you, you're getting a.",
                    "label": 0
                },
                {
                    "sent": "You can look at the probabilities.",
                    "label": 0
                },
                {
                    "sent": "As I said in the disclaimer, we basically look at everything in terms of probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "You could also look at that point estimate OK.",
                    "label": 0
                },
                {
                    "sent": "So you could tell me what's the maximum the most likely see given that we've observed a certain X and that would give you a map.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the generalization of a map.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we normally want a functional relation, which is what you do with your Nets.",
                    "label": 0
                },
                {
                    "sent": "But the more the more general outlook is to say we want to know what's the probability of having a certain class.",
                    "label": 0
                },
                {
                    "sent": "Given.",
                    "label": 0
                },
                {
                    "sent": "A certain input OK, certain target given a certain input.",
                    "label": 0
                },
                {
                    "sent": "Dot Dot might give you an explicit function if you for example, then if you have two classes then you can say at each point you can take the class to be the one with the greatest posterior probability.",
                    "label": 0
                },
                {
                    "sent": "That would give you an explicit function which would obviously have a step somewhere.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem with generative methods is that it requires the estimation of lots of parameters.",
                    "label": 0
                },
                {
                    "sent": "An example of this is.",
                    "label": 0
                },
                {
                    "sent": "Discriminant analysis OK, which is closely related to the mixtures of Gaussians we've seen before.",
                    "label": 0
                },
                {
                    "sent": "'cause we exactly use mixtures of Gaussians as our class conditional model.",
                    "label": 1
                },
                {
                    "sent": "We postulate that.",
                    "label": 0
                },
                {
                    "sent": "Given the class, the probability of observing X is given by a Gaussian OK with a certain covariance and certain mean.",
                    "label": 0
                },
                {
                    "sent": "Now we will have to estimate both the means and the covariances for each class.",
                    "label": 0
                },
                {
                    "sent": "Now if the data is D dimensional, this is a symmetric matrix, so is ordered.",
                    "label": 0
                },
                {
                    "sent": "The squared was D * D + + 1 / 2.",
                    "label": 0
                },
                {
                    "sent": "And we have to do it for each class, so we have a large number of parameters to estimate with maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Which may not be a very wise thing to do.",
                    "label": 0
                },
                {
                    "sent": "And then we can classify.",
                    "label": 0
                },
                {
                    "sent": "Once we have found the parameters again, we look at the posterior probabilities and then if we're given a new test data, we plug X into this formula and we get what's the posterior probability of X coming from a certain class OK?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I left this as an exercise and I think we might just about have time to do it.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "Possibly instructive for what we're going to see in the next 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "To rewrite these in terms of sigmoids.",
                    "label": 1
                },
                {
                    "sent": "OK, so in the case when we have only two classes.",
                    "label": 0
                },
                {
                    "sent": "I'm getting better.",
                    "label": 0
                },
                {
                    "sent": "So let's remove all these mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So our criterion.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have K = 2 Now.",
                    "label": 0
                },
                {
                    "sent": "Criteria for assigning.",
                    "label": 0
                },
                {
                    "sent": "New inputs to a certain class is to look at the posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so we look at P of.",
                    "label": 0
                },
                {
                    "sent": "Say C1.",
                    "label": 0
                },
                {
                    "sent": "Given a certain X. OK. And then we also obviously we have.",
                    "label": 0
                },
                {
                    "sent": "Pfc 2 given a certain X.",
                    "label": 0
                },
                {
                    "sent": "IS 1 -- P or C1 because we only have?",
                    "label": 0
                },
                {
                    "sent": "Two classes.",
                    "label": 0
                },
                {
                    "sent": "So what we want to look is which one is bigger.",
                    "label": 0
                },
                {
                    "sent": "Is this one bigger or this one bigger, so convenient way to look at that is to consider the ratio?",
                    "label": 0
                },
                {
                    "sent": "Of the likelihoods.",
                    "label": 0
                },
                {
                    "sent": "So we have say a function we call it D. Of a piece of.",
                    "label": 0
                },
                {
                    "sent": "The ratio of the posteriors OK but the posterior.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm just using Bayes theorem to express the posterior.",
                    "label": 0
                },
                {
                    "sent": "So these terms go away.",
                    "label": 0
                },
                {
                    "sent": "And it becomes.",
                    "label": 0
                },
                {
                    "sent": "A ratio of likelihoods.",
                    "label": 0
                },
                {
                    "sent": "OK, so the ratio of likelihoods is the.",
                    "label": 0
                },
                {
                    "sent": "Discrimina criteria.",
                    "label": 0
                },
                {
                    "sent": "OK, what I want to do is to rewrite this as.",
                    "label": 0
                },
                {
                    "sent": "Sigma of a.",
                    "label": 0
                },
                {
                    "sent": "Where Sigma.",
                    "label": 0
                },
                {
                    "sent": "Is the.",
                    "label": 0
                },
                {
                    "sent": "Logistic sigmoid function.",
                    "label": 0
                },
                {
                    "sent": "1 + X -- A. OK. And.",
                    "label": 0
                },
                {
                    "sent": "I look up my notes so that I don't make silly mistakes because I'm prone to silly mistakes.",
                    "label": 0
                },
                {
                    "sent": "Completely trivial.",
                    "label": 0
                },
                {
                    "sent": "Thing to do, if we take a.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "To be.",
                    "label": 0
                },
                {
                    "sent": "The log of.",
                    "label": 0
                },
                {
                    "sent": "P of X / C one.",
                    "label": 0
                },
                {
                    "sent": "Times P of C1.",
                    "label": 0
                },
                {
                    "sent": "Then we can rewrite this in this form exploiting this.",
                    "label": 0
                },
                {
                    "sent": "Constraint OK.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that a if.",
                    "label": 0
                },
                {
                    "sent": "Sigma one is equal to Sigma 2A becomes.",
                    "label": 0
                },
                {
                    "sent": "Proportional to.",
                    "label": 0
                },
                {
                    "sent": "New one minus Mewtwo.",
                    "label": 0
                },
                {
                    "sent": "Transpose times X.",
                    "label": 0
                },
                {
                    "sent": "Basically, what is standing you you measure where you are in the distance between the two means and the mean you're closer to is the class you get assigned to.",
                    "label": 0
                },
                {
                    "sent": "OK. Now the reason why I wanted to do.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This exercise is that.",
                    "label": 0
                },
                {
                    "sent": "Is about what's coming up now.",
                    "label": 0
                },
                {
                    "sent": "There is another paradigm for classification, which is the discriminative paradigm.",
                    "label": 0
                },
                {
                    "sent": "Oskol Diagnostic and the idea.",
                    "label": 0
                },
                {
                    "sent": "Is that model in the class conditional distributions involve significant computational overheads, but not only you could also get the model wrong.",
                    "label": 0
                },
                {
                    "sent": "Perhaps what is more parsimonious?",
                    "label": 0
                },
                {
                    "sent": "And is also more invariant under changes in the model is to look directly at the posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "So directly model what the probability of CJ is given X. OK.",
                    "label": 0
                },
                {
                    "sent": "This is closely related to the concept of transductive learning, which vapnik introduced which is.",
                    "label": 1
                },
                {
                    "sent": "Formulated in a way that my grandmother would understand that say if you want to solve a certain problem, don't solve a harder problem first, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we are generative.",
                    "label": 0
                },
                {
                    "sent": "We got to model P of X given C and then we have to find marginal likelihoods for X and then we get the procedure problem.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "If we are discriminative.",
                    "label": 0
                },
                {
                    "sent": "We just go straight for the posterior probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And an example of this is logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Which is effectively the discriminative analogue of discriminant analysis we've just seen because what they say is.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, forget about the generative process underlying the X.",
                    "label": 0
                },
                {
                    "sent": "Just take.",
                    "label": 0
                },
                {
                    "sent": "As a fact that the probability of being in a certain Class C one is given by.",
                    "label": 0
                },
                {
                    "sent": "A sigmoid.",
                    "label": 0
                },
                {
                    "sent": "So we've seen that the likelihood ratio is a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "If we assume that the class conditional probabilities are Gaussians.",
                    "label": 0
                },
                {
                    "sent": "With the same.",
                    "label": 0
                },
                {
                    "sent": "Covariance say well forget about the generative mechanism.",
                    "label": 0
                },
                {
                    "sent": "Just write these conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "Obviously they have to be normalized and.",
                    "label": 1
                },
                {
                    "sent": "Sigma is this function here.",
                    "label": 0
                },
                {
                    "sent": "Now the number of parameters.",
                    "label": 1
                },
                {
                    "sent": "If we assume.",
                    "label": 0
                },
                {
                    "sent": "Equal covariances, then the total number of parameters is just.",
                    "label": 0
                },
                {
                    "sent": "The values encoded in this vector W. Which is just D instead of K * D squared, so it's a significant.",
                    "label": 0
                },
                {
                    "sent": "Sam savings in terms of computational time.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we estimate this?",
                    "label": 0
                },
                {
                    "sent": "Well, it's actually quite easy.",
                    "label": 0
                },
                {
                    "sent": "Again, we use maximum likelihood and.",
                    "label": 0
                },
                {
                    "sent": "For ease of notation, we called why I the probability that XI comes from C1?",
                    "label": 0
                },
                {
                    "sent": "OK, see I is 01 and likely it can be written in this form.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the posterior probability.",
                    "label": 0
                },
                {
                    "sent": "That's why I came from comes from C1.",
                    "label": 0
                },
                {
                    "sent": "Times the posterior probability that it comes from C2.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just a clever way.",
                    "label": 0
                },
                {
                    "sent": "Because you see C-01.",
                    "label": 0
                },
                {
                    "sent": "So if C is 0.",
                    "label": 0
                },
                {
                    "sent": "That means that it is in class one and we're looking at this probability if C is 1, then this goes way.",
                    "label": 0
                },
                {
                    "sent": "We're looking at this probability OK?",
                    "label": 0
                },
                {
                    "sent": "So we can take the log of that.",
                    "label": 0
                },
                {
                    "sent": "We can compute.",
                    "label": 0
                },
                {
                    "sent": "The gradient of the likelihood and we get the gradient of the likelihood looks like this.",
                    "label": 0
                },
                {
                    "sent": "Which is nice because basically.",
                    "label": 0
                },
                {
                    "sent": "You want the gradient of the likelihood to be 0, and you see this is zero if your posterior probability is very close to the label.",
                    "label": 1
                },
                {
                    "sent": "So if your posterior probability of being in class one is very close to.",
                    "label": 0
                },
                {
                    "sent": "One then this would be a small contribution to the gradient.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "It's a general problem with the maximum likelihood based methods that can be overfitting, because if the two classes are well separated and there are several possible values of the vector W. Which will separate exactly both classes.",
                    "label": 0
                },
                {
                    "sent": "OK, so you may want to do Bayesian logistic regression or things like that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's get to the title after.",
                    "label": 0
                },
                {
                    "sent": "About 50 minutes.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised learning what is semi supervised learning about what's the problem?",
                    "label": 0
                },
                {
                    "sent": "Well, as we've seen.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning assumes that you know the label for each data point for each input point.",
                    "label": 0
                },
                {
                    "sent": "You also know the target.",
                    "label": 0
                },
                {
                    "sent": "But in practical applications.",
                    "label": 0
                },
                {
                    "sent": "That's not always feasible or sensible.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're interested in a classification problem where you have proteins, you have the sequence of a protein you want to predict the function.",
                    "label": 0
                },
                {
                    "sent": "Now for some proteins.",
                    "label": 0
                },
                {
                    "sent": "The function is known, so you have the label, but finding the function of a protein is an extremely expensive.",
                    "label": 0
                },
                {
                    "sent": "And time consuming process requires lots of lab testing, so for the great majority of proteins you just know the sequence not.",
                    "label": 0
                },
                {
                    "sent": "The function.",
                    "label": 0
                },
                {
                    "sent": "Other examples, for example, are text classification, image classification.",
                    "label": 0
                },
                {
                    "sent": "At the moment all the labeling has to be done by hand, so you need a person to say oh this text is about politics or this email is about sports.",
                    "label": 0
                },
                {
                    "sent": "If you were to just gather emails or texts, obviously you would have a much larger corpus, so the idea is.",
                    "label": 0
                },
                {
                    "sent": "Since you've got plenty of examples of inputs X, but few of the corresponding target.",
                    "label": 1
                },
                {
                    "sent": "Maybe we could also make use of the inputs which don't have a target and improve our predictive power.",
                    "label": 1
                },
                {
                    "sent": "What is the goal of semi supervised learning or the goal is still to predict?",
                    "label": 0
                },
                {
                    "sent": "What is?",
                    "label": 0
                },
                {
                    "sent": "Label given the input.",
                    "label": 0
                },
                {
                    "sent": "And it's evaluated again by classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "So in the strict sense.",
                    "label": 1
                },
                {
                    "sent": "In this sense, Semi supervised learning is a special instance of supervised learning, but.",
                    "label": 0
                },
                {
                    "sent": "Sink because of its practical importance in a lot of examples, it has become a field on its own, basically.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised learning is.",
                    "label": 0
                },
                {
                    "sent": "The way of aiding supervised algorithms by the use of unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, a little bit of notation so.",
                    "label": 0
                },
                {
                    "sent": "The label set.",
                    "label": 0
                },
                {
                    "sent": "We also called the labeled data set.",
                    "label": 0
                },
                {
                    "sent": "We also called the complete data and these are data set called DL.",
                    "label": 0
                },
                {
                    "sent": "Which comprises of NL sets of pairs X&C, so we have NL instances of an input and of a target.",
                    "label": 1
                },
                {
                    "sent": "We also have an unlabeled or incomplete data set.",
                    "label": 1
                },
                {
                    "sent": "Did you?",
                    "label": 0
                },
                {
                    "sent": "We just got NU.",
                    "label": 0
                },
                {
                    "sent": "Examples OK, and they are only the vectors X.",
                    "label": 0
                },
                {
                    "sent": "The common and the interesting case.",
                    "label": 0
                },
                {
                    "sent": "Is when we have.",
                    "label": 0
                },
                {
                    "sent": "The number of unlabeled data is much greater than the number of labeled data, OK?",
                    "label": 0
                },
                {
                    "sent": "The idea being that you would be discarding an awful lot of information if you just used a supervised algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are two obvious baselines for semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Simple attack is.",
                    "label": 0
                },
                {
                    "sent": "To ignore the labels.",
                    "label": 0
                },
                {
                    "sent": "Use an unsupervised learning method.",
                    "label": 1
                },
                {
                    "sent": "For example, a clustering approach approach.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Validated against the labels and find your accuracy from that.",
                    "label": 1
                },
                {
                    "sent": "Obviously, the complementary attack is just, well, ignore all the unlabeled data and just to, say discriminant analysis or logistic regression or any supervised technique on the complete data.",
                    "label": 1
                },
                {
                    "sent": "And then use the unlabeled data that says test data.",
                    "label": 0
                },
                {
                    "sent": "Now there's an interesting statement which goes as a no free lunch statement that it is possible always to construct data distributions.",
                    "label": 1
                },
                {
                    "sent": "For which either of the baselines outperforms.",
                    "label": 1
                },
                {
                    "sent": "The given SSL method.",
                    "label": 0
                },
                {
                    "sent": "So you can construct a data distribution.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Leads to better performance.",
                    "label": 0
                },
                {
                    "sent": "If you don't know the labels affect if you don't use the labels or you can construct the data distribution where if you ignore the unlabeled data, you get better performance.",
                    "label": 0
                },
                {
                    "sent": "Think for example, if the data distribution was such that the unlabeled data was completely unhelpful and sampled with an extreme bias, which completely screws your classifier.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "There is a generalized hope that.",
                    "label": 0
                },
                {
                    "sent": "The no free lunch won't happen very often, and.",
                    "label": 0
                },
                {
                    "sent": "We should be able to tailor SSL algorithms in order to match a specific application so that we always improve the situation by going semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, as I said.",
                    "label": 0
                },
                {
                    "sent": "SSL is a special case of supervised learning, and again we can.",
                    "label": 0
                },
                {
                    "sent": "Classify Semi supervised learning algorithms as generative algorithms and discriminative algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the generative method is most intuitive.",
                    "label": 1
                },
                {
                    "sent": "The graphical model is like this, which is exactly the same as the graphical model of unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We also have some.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data, so maybe we would think of, you know, combining the likelihoods, we can write down the likelihood for the incomplete data, which comes from a clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically, is the clustering likely, and then we can write the likelihood for the label data.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily, but yeah you could do.",
                    "label": 0
                },
                {
                    "sent": "OK, you just you're just in the general case where you have a latent class variable and you know there is a certain class conditional which need not be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It could be, you know.",
                    "label": 0
                },
                {
                    "sent": "Student T could be gamma, could be whatever.",
                    "label": 0
                },
                {
                    "sent": "I mean you can do a mixture of any distributions OK, but yeah, the Gaussians is a good example to keep in mind because it's very clear what's going on.",
                    "label": 0
                },
                {
                    "sent": "But in any case, you know.",
                    "label": 0
                },
                {
                    "sent": "We can write and unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Data likelihood and we can write a complete data likelihood.",
                    "label": 0
                },
                {
                    "sent": "We can sum them together.",
                    "label": 0
                },
                {
                    "sent": "And then we can just say Oh well.",
                    "label": 0
                },
                {
                    "sent": "For the unlabeled data, sees a latent variable, and then we have a very general powerful technique, which is am, which we've gone into some detail which allows you to estimate.",
                    "label": 0
                },
                {
                    "sent": "Find maximum likelihood when you have missing.",
                    "label": 0
                },
                {
                    "sent": "Latent variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so we could use them and estimate the parameters and then we would get our posterior distributions OK. Notice that this is truly semi supervised because we would use them over the whole thing.",
                    "label": 0
                },
                {
                    "sent": "So we use both incomplete and complete data to estimate the parameters.",
                    "label": 0
                },
                {
                    "sent": "Which would be our thetas.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Toys.",
                    "label": 0
                },
                {
                    "sent": "So this obviously has been done quite a long time ago and.",
                    "label": 0
                },
                {
                    "sent": "It probably is the one of the very first papers in semi supervised learning which is by Jeff McLaughlin in 1977.",
                    "label": 0
                },
                {
                    "sent": "Which considered a semi supervised learning problem for generative model with two Gaussian classes with known covariance.",
                    "label": 1
                },
                {
                    "sent": "OK, now you may think this is pretty trivial, but it's actually not that trivial.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We also assume the covariance to be known, and we only need to estimate mu one and MU two from the data and we want to use both the labels label data and the unlabeled data to estimate mu one MU 2.",
                    "label": 1
                },
                {
                    "sent": "So we make a very strong assumption.",
                    "label": 0
                },
                {
                    "sent": "Which is that we got?",
                    "label": 0
                },
                {
                    "sent": "The unlabeled the size of the unlabeled data set.",
                    "label": 0
                },
                {
                    "sent": "Is much smaller than the size of the label data set, so in this early days they were looking at.",
                    "label": 0
                },
                {
                    "sent": "You know, when you have a little bit of unlabeled information and how do you have to modify an?",
                    "label": 0
                },
                {
                    "sent": "Whether you can get guarantees of improvements in performance.",
                    "label": 0
                },
                {
                    "sent": "We also suppose that the unlabeled data set comes from M1 points in the true truly are from the first class and M2 points are truly are from the second class.",
                    "label": 0
                },
                {
                    "sent": "Further assumption, we assume that.",
                    "label": 0
                },
                {
                    "sent": "We have an even split between the two classes amongst the labels, so we have little endpoints in Class One classified as Class 1 little endpoints in Class 2.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So McLaughlin, being a proper statistician, the first thing he said is, well, we can.",
                    "label": 0
                },
                {
                    "sent": "Consider the expected misclassification error, OK?",
                    "label": 1
                },
                {
                    "sent": "So you can say, well, I can construct a discriminate and I can find theoretically what the expected error is.",
                    "label": 0
                },
                {
                    "sent": "Now you can consider that the expected error for the method that uses only the label data.",
                    "label": 1
                },
                {
                    "sent": "So the supervisor label and we call that R. And we can also consider what the expected error is when you add the unlabeled information, we call that R1 OK. And we estimate the parameters using AM.",
                    "label": 0
                },
                {
                    "sent": "Now the surprising result, which I found surprising at least, is that.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "The numbers in the unlabeled data between the two classes are unbalanced.",
                    "label": 0
                },
                {
                    "sent": "OK, so if M1 is different from M2.",
                    "label": 0
                },
                {
                    "sent": "Then you can expand to 1st order.",
                    "label": 0
                },
                {
                    "sent": "Our one and R. And you find that so first ordering and you over analysis, so we're assuming that the few unlabeled cases you find that this difference is positive only.",
                    "label": 0
                },
                {
                    "sent": "If the number of unlabeled examples is below a certain critical number.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this difference being positive means that the 1st order you're guaranteed that using the unlabeled data you're doing better.",
                    "label": 0
                },
                {
                    "sent": "You're getting a less expected error.",
                    "label": 0
                },
                {
                    "sent": "No, that's true.",
                    "label": 0
                },
                {
                    "sent": "So these are the true.",
                    "label": 0
                },
                {
                    "sent": "You don't know them.",
                    "label": 0
                },
                {
                    "sent": "You have to estimate them.",
                    "label": 0
                },
                {
                    "sent": "But this is a theoretical analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a theoretical analysis in which you don't know the labels.",
                    "label": 0
                },
                {
                    "sent": "But you know, you are presuming that there are M1 and M2 from.",
                    "label": 0
                },
                {
                    "sent": "Those classes OK, you don't know which ones they are, but it's just.",
                    "label": 0
                },
                {
                    "sent": "It's something you need to do to compute the expected error in the R1 case OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is this is surprising I think.",
                    "label": 0
                },
                {
                    "sent": "'cause it's telling you that unlabeled data helped.",
                    "label": 0
                },
                {
                    "sent": "Only up to a point.",
                    "label": 1
                },
                {
                    "sent": "So if you got a little bit of unlabeled data, then you're sure that you're going to improve.",
                    "label": 0
                },
                {
                    "sent": "But if you got a lot of unlabeled data where at least the 1st order.",
                    "label": 0
                },
                {
                    "sent": "You can show that you may not improve at all.",
                    "label": 0
                },
                {
                    "sent": "You could actually get an expected error which is greater than on the supervised case.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "It's not so surprising, but it's true.",
                    "label": 0
                },
                {
                    "sent": "Also, you know when the imbalance is fine.",
                    "label": 0
                },
                {
                    "sent": "So obviously yes, Mstar as a function of.",
                    "label": 0
                },
                {
                    "sent": "Basically the difference between M1 and M2.",
                    "label": 0
                },
                {
                    "sent": "Is is decreasing, so if everyone is very close to M2 and M star is large, but what it is saying is since you don't know I'm one and M2.",
                    "label": 0
                },
                {
                    "sent": "If you're just presented with label data and unlabeled data, you cannot rule out that.",
                    "label": 0
                },
                {
                    "sent": "For some reason you know you've got your labels that are balanced, but actually in the real data process the mixtures are not evenly balanced.",
                    "label": 0
                },
                {
                    "sent": "Then I think what happens is that in your EM algorithm, basically you become.",
                    "label": 0
                },
                {
                    "sent": "The data at the label data drives it to think that it's more balanced, and so I mean, you do make an error in estimating how many you are having balanced or unbalanced.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I mean it, it may not be such a surprising result, but what it's sending you is that it's, you know, just adding the unlabeled data might not solve all your problems, yes.",
                    "label": 0
                },
                {
                    "sent": "Very funny movie about.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's for very small imbalance.",
                    "label": 0
                },
                {
                    "sent": "It's very high.",
                    "label": 0
                },
                {
                    "sent": "For very large imbalance, it could be quite.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not, and it doesn't have an analytical form, so it just did the.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can't make an exponent.",
                    "label": 0
                },
                {
                    "sent": "It does grow very fast.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it looks as if it's exponential, but can't be exponentially because you can't.",
                    "label": 0
                },
                {
                    "sent": "You know the domain of possible differences is not.",
                    "label": 0
                },
                {
                    "sent": "Unbounded.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what McLaughlin also proposed the way out, which was well, don't trust the unlabeled data as much and re wait.",
                    "label": 0
                },
                {
                    "sent": "So compute your estimates by basically.",
                    "label": 0
                },
                {
                    "sent": "Down waiting, oh.",
                    "label": 0
                },
                {
                    "sent": "In any case, changing the weighting of the unlabeled data with respect to the label data, and that's something that is done very much in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And this is probably the first instance in which this happened, and he showed that for a suitable value of Lambda.",
                    "label": 1
                },
                {
                    "sent": "So for a suitable reweighting.",
                    "label": 0
                },
                {
                    "sent": "Re balancing between the labeled and unlabeled data he could find.",
                    "label": 0
                },
                {
                    "sent": "Unexpected error that.",
                    "label": 0
                },
                {
                    "sent": "Was always smaller to 1st order OK?",
                    "label": 0
                },
                {
                    "sent": "And obviously this Lambda depends on the true.",
                    "label": 1
                },
                {
                    "sent": "Numbers which you don't know you have to.",
                    "label": 0
                },
                {
                    "sent": "To estimate, but yes.",
                    "label": 0
                },
                {
                    "sent": "Well, if you have, you know it's a generative model, so you're you got the expectation you got the class conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "You estimate the parameters of the class conditional probabilities and then that gives you basically unexpected error.",
                    "label": 1
                },
                {
                    "sent": "So depending on how far.",
                    "label": 0
                },
                {
                    "sent": "The means are you can, you know, say what's the probability that you know a point that you find over here that you would classify as being.",
                    "label": 0
                },
                {
                    "sent": "This cluster actually comes from this cluster.",
                    "label": 0
                },
                {
                    "sent": "That's just a simple probability.",
                    "label": 0
                },
                {
                    "sent": "No, no, you're estimating the parameters using both labeled and unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Once you estimate the parameters, your job is done.",
                    "label": 0
                },
                {
                    "sent": "You can compute the posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, and once you have posterior probabilities, you can say what's the error rate, assuming that you're classifying them by what's the greatest posterior probability.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                },
                {
                    "sent": "Nona cartoon example.",
                    "label": 0
                },
                {
                    "sent": "Yeah he would have.",
                    "label": 0
                },
                {
                    "sent": "Some of these are labeled.",
                    "label": 0
                },
                {
                    "sent": "Some of these are labeled.",
                    "label": 1
                },
                {
                    "sent": "You use them both to determine that.",
                    "label": 0
                },
                {
                    "sent": "Whatever is to the left of this line is classed as this.",
                    "label": 0
                },
                {
                    "sent": "Now, if you get an extra point here.",
                    "label": 1
                },
                {
                    "sent": "You can compute what's the probability you would classify it as here, but you can compute what's the probability that it actually comes from here rather than from that, and that's the expected error.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you can compute it like that.",
                    "label": 0
                },
                {
                    "sent": "That's what he does affect.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "Does that solve your problems?",
                    "label": 0
                },
                {
                    "sent": "Yeah you can.",
                    "label": 0
                },
                {
                    "sent": "I mean, what you're doing is basically working out the parameters.",
                    "label": 0
                },
                {
                    "sent": "You effectively optimizing this likelihood.",
                    "label": 0
                },
                {
                    "sent": "Which is the sum of the two.",
                    "label": 0
                },
                {
                    "sent": "And then or weighted combination of the two.",
                    "label": 0
                },
                {
                    "sent": "And optimizing that contains both the labels information.",
                    "label": 0
                },
                {
                    "sent": "You see that the functional form if you have the label is different, as if you don't have the label, because if you don't have the label, you have to marginalized.",
                    "label": 0
                },
                {
                    "sent": "Over the unseen class, you have to sum over, see if you have the label.",
                    "label": 0
                },
                {
                    "sent": "No, it's just a joint probability.",
                    "label": 0
                },
                {
                    "sent": "So the labels help in finding the parameters.",
                    "label": 0
                },
                {
                    "sent": "So anyway, so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What it came to was that.",
                    "label": 0
                },
                {
                    "sent": "And it's a generally very used technique is that you shouldn't just.",
                    "label": 0
                },
                {
                    "sent": "Consider your total likelihood as being the sum of the incomplete, incomplete, and complete likelihood, but you should have re weighted sum of the incomplete incomplete likelihood.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Another argument for this is that if you're in the interesting case, when the number of labeled points is very small, then.",
                    "label": 1
                },
                {
                    "sent": "The unlabeled points would effectively swamp the information contained in the labels, and you would lose it if you just.",
                    "label": 0
                },
                {
                    "sent": "Would add them without rescaling.",
                    "label": 1
                },
                {
                    "sent": "OK, so it might seem sensible to choose a certain value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "But obviously.",
                    "label": 0
                },
                {
                    "sent": "The tricky question is how do you choose Lambda?",
                    "label": 1
                },
                {
                    "sent": "Cross validation for example.",
                    "label": 0
                },
                {
                    "sent": "Well, if you got a very small label data set, how are you going to do cross validation?",
                    "label": 1
                },
                {
                    "sent": "If you got 10 labels?",
                    "label": 0
                },
                {
                    "sent": "You have to set aside a subset to do cross validation.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get very good results.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an elegant solution, OK?",
                    "label": 1
                },
                {
                    "sent": "It was presented reasonably recently by coding and Yakola in 2002 is to look for stability.",
                    "label": 0
                },
                {
                    "sent": "And the way they looked at it.",
                    "label": 0
                },
                {
                    "sent": "Was basically it's a very simple and nice idea.",
                    "label": 0
                },
                {
                    "sent": "So if you have a clustering problem.",
                    "label": 0
                },
                {
                    "sent": "OK. Then your likelihood.",
                    "label": 0
                },
                {
                    "sent": "With two classes, will certainly be bimodal.",
                    "label": 0
                },
                {
                    "sent": "OK, because you can swap.",
                    "label": 0
                },
                {
                    "sent": "The class is OK. You can say the parameter that you gave to class one is actually the parameter of Class 2.",
                    "label": 0
                },
                {
                    "sent": "And nothing changes.",
                    "label": 0
                },
                {
                    "sent": "OK, it's perfectly symmetrical.",
                    "label": 0
                },
                {
                    "sent": "If you got, if you have the class labels instead, you're likely this unimodal.",
                    "label": 1
                },
                {
                    "sent": "OK, if you got the class labels, you determine the maximum or delighted by taking the empirical covariance and the empirical mean, and that's the maximum of the likelihood for each class.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the observation is that.",
                    "label": 0
                },
                {
                    "sent": "We have Lambda equals zero.",
                    "label": 0
                },
                {
                    "sent": "We're not considering the labels, so we are looking at an unsupervised problem, and it's a clustering problem, so we will have a likelihood with many modes.",
                    "label": 0
                },
                {
                    "sent": "If you take Lambda equals one.",
                    "label": 0
                },
                {
                    "sent": "We are completely ignoring the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the supervised.",
                    "label": 0
                },
                {
                    "sent": "Date problem for which we have a unimodal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So when you vary Lambda.",
                    "label": 0
                },
                {
                    "sent": "You must cross a point where the single mode splits OK.",
                    "label": 0
                },
                {
                    "sent": "So presumably it becomes flatter and flatter and at a certain .2 modes emerge.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The proposal of Aquila encode viano, which was supported with obviously.",
                    "label": 0
                },
                {
                    "sent": "More us strict argumentation and the one that was presented is that when you go from Lambda equals one to land equals 0, so from supervised to unsupervised, you're bound to encounter a critical Lambda star, which is say the maximum value for which you only have one mode and that should be.",
                    "label": 0
                },
                {
                    "sent": "The optimal choice.",
                    "label": 0
                },
                {
                    "sent": "In which sense it is the optimal choice, it's, let's say, is the maximum.",
                    "label": 1
                },
                {
                    "sent": "Involvement of the unlabeled data that guarantees a unique solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very simple but elegant idea which they call stability.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's press on.",
                    "label": 0
                },
                {
                    "sent": "Now these were was what we were going to say for generative models, as we've seen the very intuitive, there may be a way of going from super unsupervised towards semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could also consider discriminative models.",
                    "label": 0
                },
                {
                    "sent": "And again we have the same graphical model As for discriminative classification.",
                    "label": 1
                },
                {
                    "sent": "We are interested in modeling.",
                    "label": 0
                },
                {
                    "sent": "C given X rather than vice versa.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We could just plow on as we always do, and we can write the total likelihood.",
                    "label": 1
                },
                {
                    "sent": "For the unlabeled data and the labeled data.",
                    "label": 0
                },
                {
                    "sent": "Given the parameters, well, it will factorize as.",
                    "label": 0
                },
                {
                    "sent": "The likelihood for.",
                    "label": 0
                },
                {
                    "sent": "The labels given.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Put for the labels of the label data given the input of the label data times the likelihood for the label data and the unlabeled data given the parameter of the distribution that generates X. OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Total likelihood of the data, which is always what we want to maximize to find them.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Amateurs.",
                    "label": 0
                },
                {
                    "sent": "Now if we use Bayes theorem we can just, you know, try to find out what the posterior of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Remember, we always what we want to do is always estimating the parameters because once we have the parameters.",
                    "label": 0
                },
                {
                    "sent": "We can plug them in and find the posteriors of the class labels, and we're fine.",
                    "label": 0
                },
                {
                    "sent": "So if we look at what's the procedure of the parameters, given unlabeled data and the unlabeled data, well, we just.",
                    "label": 0
                },
                {
                    "sent": "Use Bayes theorem.",
                    "label": 0
                },
                {
                    "sent": "And now we observe that.",
                    "label": 0
                },
                {
                    "sent": "We are the denominator in Bayes theorem is the marginal where we marginalized the parameters.",
                    "label": 0
                },
                {
                    "sent": "Now this term.",
                    "label": 0
                },
                {
                    "sent": "Does not depend on Theta, so it comes out of the integral and it simplifies with this term above.",
                    "label": 0
                },
                {
                    "sent": "So effectively, the posterior over Theta is proportional.",
                    "label": 1
                },
                {
                    "sent": "To the likely probability.",
                    "label": 0
                },
                {
                    "sent": "All the classes.",
                    "label": 0
                },
                {
                    "sent": "Given Excel and theater times, P of theater.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is quite worrying.",
                    "label": 0
                },
                {
                    "sent": "What, why is it wearing is is everyone else worried?",
                    "label": 0
                },
                {
                    "sent": "Or is it just me worried?",
                    "label": 0
                },
                {
                    "sent": "That's absolutely correct.",
                    "label": 0
                },
                {
                    "sent": "So the unlabeled data in a in a discriminative.",
                    "label": 0
                },
                {
                    "sent": "What it's telling you.",
                    "label": 0
                },
                {
                    "sent": "If you are Bayesian.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you use that graphical model, which is the graphical model of discriminative classification, and you want to introduce unlabeled data well.",
                    "label": 0
                },
                {
                    "sent": "You may as well, but it won't make any difference whatsoever at all.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Bayesian methods in a straight, discriminative SSL cannot be employed, so you just can't do semisupervised with the basic method.",
                    "label": 1
                },
                {
                    "sent": "There is a little bit of success when you're not Bayesian.",
                    "label": 0
                },
                {
                    "sent": "But it generally is just a little bit of success.",
                    "label": 0
                },
                {
                    "sent": "So if you actually compare the results with using the labels or not using the labels, using the unlabeled data, or ignoring the unlabeled data, the improvement is very small.",
                    "label": 0
                },
                {
                    "sent": "For example, Anderson in 78 came up with.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised method for logistic regression of the discrete variables and that seemed to work alright, but it was a very special case and it's not clear how much.",
                    "label": 0
                },
                {
                    "sent": "Information you can get.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Obviously, people have been aware of this problem and they've come up with a solution and the solution, which is probably the most promising Ave. Is to change the graphical model and it's called regularization OK so?",
                    "label": 0
                },
                {
                    "sent": "The problem came from the fact that you know theater M. You are a priority independent and when we plug this into Bayes theorem there are priority.",
                    "label": 0
                },
                {
                    "sent": "They become also a posteriori independent.",
                    "label": 0
                },
                {
                    "sent": "So if we add an extra arrow.",
                    "label": 0
                },
                {
                    "sent": "Then maybe we can solve the problem.",
                    "label": 0
                },
                {
                    "sent": "And we can OK, but what we're saying is now, is that our estimates of the parameters will depend on some characteristics of the input distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As I said, the idea is some of the information in the generative process underlying X must be included into the discriminative technique in order to.",
                    "label": 0
                },
                {
                    "sent": "Make use of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So can we call it still a discriminative approach?",
                    "label": 1
                },
                {
                    "sent": "Well, probably not.",
                    "label": 0
                },
                {
                    "sent": "So you know, the discriminative idea was we don't care about the harder problem of estimating the generative process underlying X.",
                    "label": 0
                },
                {
                    "sent": "We only want to classify and in this case we must care at least a little bit.",
                    "label": 0
                },
                {
                    "sent": "So it's not as strict discriminative.",
                    "label": 0
                },
                {
                    "sent": "Approach.",
                    "label": 0
                },
                {
                    "sent": "However, in practice, the hypothesis used for regularization are much weaker, so we don't want to model the whole of the generative process.",
                    "label": 1
                },
                {
                    "sent": "We may just want to model things.",
                    "label": 0
                },
                {
                    "sent": "Which are very weak indications of what the generative process looks like.",
                    "label": 0
                },
                {
                    "sent": "Possibly.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most successful.",
                    "label": 0
                },
                {
                    "sent": "Way of doing this is to use what they call the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "OK. What is the cluster assumption?",
                    "label": 0
                },
                {
                    "sent": "Well, formal statement is that the boundary between classes should cross areas of low data density.",
                    "label": 1
                },
                {
                    "sent": "So as you see.",
                    "label": 0
                },
                {
                    "sent": "We are thinking by data means the input state.",
                    "label": 0
                },
                {
                    "sent": "OK, so we we are using information about the.",
                    "label": 0
                },
                {
                    "sent": "Density that generates the input data X the way we are using it though is very weak.",
                    "label": 0
                },
                {
                    "sent": "We just say.",
                    "label": 0
                },
                {
                    "sent": "The boundary, which is effectively what determines the value of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Must fall in an area where the density of the data is low.",
                    "label": 0
                },
                {
                    "sent": "And it does make sense as being a reasonable case, OK?",
                    "label": 0
                },
                {
                    "sent": "So it's a fairly reasonable weak assumption.",
                    "label": 0
                },
                {
                    "sent": "And as I said, I mean, I did say before that if your Bayesian you can't do semisupervised learning in a discriminative way, but.",
                    "label": 0
                },
                {
                    "sent": "Actually, this assumption took hold a lot into non Bayesian methods as well.",
                    "label": 0
                },
                {
                    "sent": "So when you have low density separation as VM's and stuff like that.",
                    "label": 1
                },
                {
                    "sent": "Another way, perhaps an interesting, more interesting way to understand it is in terms of smoothness of the discriminative function.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I draw another picture, maybe over there this time.",
                    "label": 0
                },
                {
                    "sent": "Effectively.",
                    "label": 0
                },
                {
                    "sent": "In our case, the discriminant function would be the ratio of the likelihoods, and.",
                    "label": 0
                },
                {
                    "sent": "What we want to say is that.",
                    "label": 0
                },
                {
                    "sent": "Obviously the discriminate function will have to change OK to discriminate function will be 0.",
                    "label": 0
                },
                {
                    "sent": "For example in the areas of class one and will be one in the area class.",
                    "label": 0
                },
                {
                    "sent": "Two so perhaps would be something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Now what we want is that if we got a cluster here.",
                    "label": 0
                },
                {
                    "sent": "Then the discriminant function must very, very little where you have lots of data.",
                    "label": 0
                },
                {
                    "sent": "So the boundaries which are given by the areas where the slope of the discriminant function is steep, those boundaries must fall where you don't have much data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this would be kind of the situation we're looking at.",
                    "label": 0
                },
                {
                    "sent": "We've got lots of data, very little data here, lots of data here, and the boundary, so the steep bit indiscriminant must fall where there is little data.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering what happens if you go to 1500 or boredom instead.",
                    "label": 0
                },
                {
                    "sent": "Soon.",
                    "label": 0
                },
                {
                    "sent": "So then you might have loaded the data everywhere.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean you're not the first person to think about these objections, so the next few slides effectively address the problem.",
                    "label": 0
                },
                {
                    "sent": "So very closely related, which effectively answers exactly the question you just.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Post is the manifold assumption.",
                    "label": 1
                },
                {
                    "sent": "So the manifold assumption says that.",
                    "label": 0
                },
                {
                    "sent": "The data lies on a low dimensional submanifold over high dimensional space and this high dimensional space might be the real input space.",
                    "label": 1
                },
                {
                    "sent": "Or might be a feature space to which you map the data first.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK if you have a high dimensional space you got low data density everywhere, but then the first thing you do is effectively almost the dimensionality reduction step, in which you say, well, the data lies on a manifold.",
                    "label": 0
                },
                {
                    "sent": "Upload dimensions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What you effectively doing?",
                    "label": 1
                },
                {
                    "sent": "The manifold assumption is just an add on to the cluster assumption.",
                    "label": 1
                },
                {
                    "sent": "At least that's the way I see it and.",
                    "label": 1
                },
                {
                    "sent": "You apply the cluster dimension in the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "On the manifold which is in the high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The key concept behind manifold assumptions is the mathematical theory of graphs, and in particular.",
                    "label": 0
                },
                {
                    "sent": "The constant graph Laplacian and a lot of these work is closely related to spectral clustering as being pioneered by.",
                    "label": 0
                },
                {
                    "sent": "Misha Belkin and Pathenia Yogi.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is very, very brief.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a super quick idea.",
                    "label": 0
                },
                {
                    "sent": "Also, because we're running out of time.",
                    "label": 0
                },
                {
                    "sent": "So the idea again we want discriminate functions which very little in regions of high data density.",
                    "label": 1
                },
                {
                    "sent": "Right now suppose.",
                    "label": 0
                },
                {
                    "sent": "That the density of the data over the manifold.",
                    "label": 0
                },
                {
                    "sent": "Is given by P of X.",
                    "label": 0
                },
                {
                    "sent": "You don't know it, but suppose it is like that.",
                    "label": 0
                },
                {
                    "sent": "Then effectively what we are looking we want.",
                    "label": 0
                },
                {
                    "sent": "This integral to be very small because this is the norm.",
                    "label": 0
                },
                {
                    "sent": "Squared norm of the gradient of the discriminant function.",
                    "label": 0
                },
                {
                    "sent": "So we want to allow this gradient to be sizeable only when this is very.",
                    "label": 0
                },
                {
                    "sent": "Small.",
                    "label": 0
                },
                {
                    "sent": "And we want to.",
                    "label": 0
                },
                {
                    "sent": "Conversely, when this is big, this greater to be very small.",
                    "label": 0
                },
                {
                    "sent": "So effectively we want this integral to be very, very small.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "If we are in Euclidean space, this is just the Laplacian of the function.",
                    "label": 0
                },
                {
                    "sent": "FF is my discriminants.",
                    "label": 0
                },
                {
                    "sent": "If we are on a manifold then that's the Laplace Beltrami operator which is just you know, the generalization of flash.",
                    "label": 0
                },
                {
                    "sent": "It's just a derivative.",
                    "label": 0
                },
                {
                    "sent": "OK is the square norm of the gradient effect.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is a we don't know the probability distribution B.",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "The manifold OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to.",
                    "label": 1
                },
                {
                    "sent": "Approximate the manifold with a graph and then you can prove that the finite sample approximation to the Laplace Beltrami operator.",
                    "label": 0
                },
                {
                    "sent": "So if you restrict the graph would be a graph was.",
                    "label": 0
                },
                {
                    "sent": "And notes lie on the manifold, so you basically do a triangulation of the manifold.",
                    "label": 0
                },
                {
                    "sent": "Then the restriction of the Laplace Beltrami operator onto the graph becomes what is known as the graph Laplacian, which is given by the adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "Of the graph basically, and this is just some of the diagonal matrix whose entries are the sum of the rows of the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "This is a very common tool.",
                    "label": 0
                },
                {
                    "sent": "Did you talk about graph Laplacians, nealen, right?",
                    "label": 0
                },
                {
                    "sent": "OK, but this is very common in spectral clustering and spectral.",
                    "label": 0
                },
                {
                    "sent": "You know methods.",
                    "label": 0
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "If we want to do SSL well, we look effectively in an unsupervised way we look at.",
                    "label": 0
                },
                {
                    "sent": "Functions which becomes obviously vectors evaluated at each point of the graph.",
                    "label": 0
                },
                {
                    "sent": "And we look at things that have a small.",
                    "label": 0
                },
                {
                    "sent": "Value of this integral, which then becomes the inner product of the discriminant function with the Laplace graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "That's the unsupervised part, which is spectral clustering effectively.",
                    "label": 0
                },
                {
                    "sent": "And then we add.",
                    "label": 0
                },
                {
                    "sent": "Apart which contains.",
                    "label": 0
                },
                {
                    "sent": "The error given by F on the label data, so that's the basic idea behind.",
                    "label": 0
                },
                {
                    "sent": "Graph methods in one slide.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The key point is.",
                    "label": 0
                },
                {
                    "sent": "You want to have a manifold low dimensional manifold in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You want to have things that very only where there is low data density.",
                    "label": 0
                },
                {
                    "sent": "You can express that.",
                    "label": 0
                },
                {
                    "sent": "Discriminants are very only on the low.",
                    "label": 0
                },
                {
                    "sent": "Data density are effectively in the.",
                    "label": 0
                },
                {
                    "sent": "Colonel Sordar, they've got very little.",
                    "label": 0
                },
                {
                    "sent": "Area when acted upon by the Laplace Beltrami operator, you take a finite sample approximation, which is a graph.",
                    "label": 0
                },
                {
                    "sent": "The Laplace Beltrami becomes a graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And that's your unsupervised bit.",
                    "label": 0
                },
                {
                    "sent": "Structure of the data.",
                    "label": 0
                },
                {
                    "sent": "Dictates where.",
                    "label": 0
                },
                {
                    "sent": "The discriminant should be steep.",
                    "label": 0
                },
                {
                    "sent": "And then you add a term that contains the labels in order to.",
                    "label": 0
                },
                {
                    "sent": "Guide that further and using the label information as well.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "That's about all I wanted to tell you about.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised learning and.",
                    "label": 0
                },
                {
                    "sent": "It does mean there is a lot of active research in these areas and lot of interesting papers.",
                    "label": 0
                },
                {
                    "sent": "And hopefully.",
                    "label": 0
                },
                {
                    "sent": "You know I what I all I aim to do was to just convey the intuition, at least as I understand it and.",
                    "label": 0
                },
                {
                    "sent": "For you to be able to go and look at the semi supervised learning paper and kind of understand what perspective, what paradigms are employing, what kind of key methods they are using and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll be happy to take questions if any.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "No no yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Showing.",
                    "label": 0
                },
                {
                    "sent": "No, but what is true is that there are.",
                    "label": 0
                },
                {
                    "sent": "But there are non Bayesian methods where they do use unlabeled it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't make any assumptions about the input distribution versus the cost, so any frequencies method has to do that.",
                    "label": 0
                },
                {
                    "sent": "It's just a Bayesian framework showing you have to be explicit about how you're making that something probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Reducing supervised learning, Bayesian way bring this way.",
                    "label": 0
                },
                {
                    "sent": "Don't look at what the problem is.",
                    "label": 0
                },
                {
                    "sent": "Complications are.",
                    "label": 0
                },
                {
                    "sent": "You may be making lots of implicit assumption still.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it might be very hard to see where the assumption is coming in, that's that.",
                    "label": 0
                },
                {
                    "sent": "I think some of the people that authored those papers would disagree with that, but.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, well my dear Sir.",
                    "label": 0
                },
                {
                    "sent": "Yeah, mateas.",
                    "label": 0
                },
                {
                    "sent": "Sega seems to believe that some numbers and.",
                    "label": 0
                },
                {
                    "sent": "When is book it?",
                    "label": 0
                },
                {
                    "sent": "Does he says the opposite.",
                    "label": 0
                },
                {
                    "sent": "So it might be that he's changed his mind, but.",
                    "label": 0
                },
                {
                    "sent": "Relationship.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean that's the thing.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't really know what some frequentist might be doing.",
                    "label": 0
                },
                {
                    "sent": "You know it's.",
                    "label": 0
                },
                {
                    "sent": "No, no no.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you got that graph, you can't do.",
                    "label": 0
                },
                {
                    "sent": "You can't use the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, the thing that I'm saying is that they might have some tricks by which unlabeled data comes in without using that graphical.",
                    "label": 0
                },
                {
                    "sent": "Sorry, of course, if you got that graph, then that's that's it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "By creating an extra link graph where you're not really claiming that this reflects reality, you're not claiming that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Well, I think the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "I I'm not sure about that.",
                    "label": 0
                },
                {
                    "sent": "I think people believe in the cluster assumption, which is exactly a way of saying the input distribution.",
                    "label": 0
                },
                {
                    "sent": "So say the means are far and the.",
                    "label": 0
                },
                {
                    "sent": "You know, basically there's a big saying that mixture of Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "There's a big Mahalanobis distance between the two classes.",
                    "label": 0
                },
                {
                    "sent": "That's concerned entirely.",
                    "label": 0
                },
                {
                    "sent": "With this set of parameters.",
                    "label": 0
                },
                {
                    "sent": "And we're seeing that the discriminate, that is where the boundary comes, which is this set of parameters.",
                    "label": 0
                },
                {
                    "sent": "Is influenced by this set of parameters, so they actually when you put the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "If you and you normally believe in it, you're actually making an assumption.",
                    "label": 0
                },
                {
                    "sent": "With this arrow exists, it's not an approximation.",
                    "label": 0
                },
                {
                    "sent": "But as I said, you know you still don't want to estimate all these parameters.",
                    "label": 0
                },
                {
                    "sent": "So do you want to have?",
                    "label": 0
                },
                {
                    "sent": "To have very weak forms in which incorporate this information in order for your computational complexity to remain reasonable.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Last line.",
                    "label": 0
                },
                {
                    "sent": "How do you?",
                    "label": 0
                },
                {
                    "sent": "So the first thing you would do is you.",
                    "label": 0
                },
                {
                    "sent": "Do you compute the adjacency matrix, which is basically say you take an RBF kernel for example, which is mapping you in feature space and you take the adjacency matrix is.",
                    "label": 0
                },
                {
                    "sent": "Export minus XI minus XJ squared over Sigma squared, so that's the IJ entry of W. OK. Yeah, so this is an N by N matrix.",
                    "label": 0
                },
                {
                    "sent": "This is just obtained by summing the rows and having a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "F is a vector, your discriminant function is a vector of zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "Who is 0 for class zero and one for Class 1?",
                    "label": 0
                },
                {
                    "sent": "And you plug it in, you just literally do F transpose LF.",
                    "label": 0
                },
                {
                    "sent": "Plus the misclassification error and you search for the best.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "You just mapped into a feature space.",
                    "label": 0
                },
                {
                    "sent": "You're not really doing the dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "You're extracting a vector.",
                    "label": 0
                },
                {
                    "sent": "Which, if you were doing you know it is using a spectral technique so it does fall into the spectrum.",
                    "label": 0
                },
                {
                    "sent": "I mean it's got links for example with kernel PCA and things like that, but.",
                    "label": 0
                },
                {
                    "sent": "But he's not a dimensionality reduction, you're just interested in evaluating the vector F, which is a vector of zeros and ones, and it's the one that classifieds your points.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it was a bit short on this slide.",
                    "label": 0
                },
                {
                    "sent": "Probably Ukraine, right?",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                }
            ]
        }
    }
}