{
    "id": "za34xykj2ebjrrhf3v5jvb37ip5w2pkm",
    "title": "Graph-based Semi-supervised Learning",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_ghahramani_applications_bayesian/",
    "segmentation": [
        [
            "So I'm going to talk about graph based, semi supervised learning 'cause this was voted, you know by audience selection.",
            "And this is a it's a fun topic is interesting.",
            "I think it's actually reasonably practical as a refresher.",
            "You'll get to hear me talk about fairly non Bayesian stuff, although I can't resist the temptation right at the end to have a little bit of discussion of how we might possibly be able to do this stuff in a basic primer.",
            "But almost the whole."
        ],
        [
            "In Semi supervised learning we are motivated by the opportunity that comes from the fact that in a lot of applications we have vast amounts of unlabeled data but small amounts of labeled data.",
            "And in particular, the process of getting good reliable labels or annotating data tends to be expensive.",
            "And we want to be able to have supervised learning methods in general.",
            "I'm going to talk about essentially discriminative learning methods that can somehow make use of the information about the input distribution that's given by large amounts of unlabeled data.",
            "So to motivate a particular example, let's say think about images.",
            "On the web."
        ],
        [
            "There are lots and lots of images of cats, let's say.",
            "And you know you could crawl the web for images, but only very small number of them might actually have proper reliable labels for what you're looking for.",
            "So can we use information from large amounts of unlabeled data to help us with classification in particular?",
            "But we can obviously talk about generalizations to other problems like regression and so on.",
            "Alright, so let's think about this.",
            "The basic approach that.",
            "That we're going to be taking is to make some assumptions about how that unlabeled data influences the class."
        ],
        [
            "Abilities OK, so the basic assumption is that there is some information in the data distribution.",
            "Alright, so consider the following problem.",
            "We have two labeled points in the minus class we have three labeled points in the plus class and all these black dots are unlabeled points.",
            "OK, now here is a quiz.",
            "This?",
            "This point at this location given by the question mark.",
            "Raise your hand if you think it's in the negative class.",
            "Come on somebody.",
            "Somebody's a diehard.",
            "Yeah alright thank you.",
            "OK we need somebody to pick on.",
            "OK, raise your hand if you think it's in the positive class alright, why?",
            "Why do we think that?",
            "Well, I don't need to, you know belabor this, but clearly we are using the fact that we have a lot of unlabeled data to sort of.",
            "Tell us that this point, although in Euclidean space, may be more close to the negative points and clearly a linear classifier, would put it well.",
            "On the side of the negative class, when we take into account the unlabeled points, we see that.",
            "There are many, many unlabeled points that sort of connect this to a positive class, but there is a big gap that disconnects it from the negative class.",
            "That's the basic intuition we are going to try to capture with our semi supervised learning methods.",
            "And there are various ways of thinking about this, but essentially the idea is it can be formulated in several ways.",
            "The idea is somehow that label information should propagate through the unlabeled data.",
            "And another way to think about this, perhaps is that when we are going to measure similarity between data points, we shouldn't ignore the vast amounts of unlabeled data, because perhaps they tell us what a natural way of measuring similarity is.",
            "OK, any questions about that?"
        ],
        [
            "Alright, so the outline of the talk is going to be.",
            "I'm going to focus mostly on graph based semi supervised learning.",
            "Then I'm going to talk about active learning in this framework, which I think is also very natural and very interesting.",
            "I don't know whether you've had any any lectures on active learning.",
            "So I think that will be interesting.",
            "And then in the last couple of slides I'll say alright everything was completely non Bayesian.",
            "You know, as a Bayesian can we do something that approaches the nice simplicity and generality of these methods?"
        ],
        [
            "OK, let's go to graph based semi supervised learning.",
            "The basic idea is we take our data both the labeled and unlabeled data, and we construct a graph.",
            "Connecting similar data points.",
            "So look at this.",
            "The data are handwritten images.",
            "The things in blue, if you can see them, are labels.",
            "A few of the data points are labeled this.",
            "This image is labeled two that images labeled 8 etc.",
            "But there are a whole bunch of images that are not labeled in this tiny toy example, but we can take a some sort of similarity metric that operates in this case on the raw pixel values of these.",
            "Little images and says that that image is similar to that image, so we connected in the graph.",
            "Perhaps that image is similar to that image and we connected in the graph, etc OK.",
            "So now we have a graph, some labeled nodes and a lot of unlabeled nodes.",
            "And what we're going to do with this graph is we're going to put a.",
            "Hidden variable put a random variable at each node in the graph corresponding to the label of that data point.",
            "Some of those random variables will be observed.",
            "The labeled ones will be observed and all the rest will be hidden.",
            "And in fact, we can actually interpret this graph in what I'm going to say in a couple of minutes as an undirected graphical model or Markov random field that I've created over my data after observing the data points so as to do the semi supervised classification problem.",
            "OK, the intuition that the graph captures is similar.",
            "Data points should have similar labels.",
            "Alright, so if I know that's got the label 8 and that's similar to that, then perhaps that should have the label 8 as well.",
            "And that information should propagate essentially from the labeled data through to the unlabeled data.",
            "And clearly it can go wrong.",
            "You can see here that you might then guess that that's an 8, and that's an 8, etc.",
            "So we have to make sure that that propagation of information doesn't go completely crazy.",
            "The graph encodes the intuition that similar data points have similar labels.",
            "So the work that I'm going to talk about on graph based semi supervised learning is work done with Jerry Zhu while he was at CMU and John Lafferty.",
            "And at the end of these slides I have a whole bunch of references, including for example a book by Jerry Zhu on the introduction to semi supervised learning etc.",
            "So.",
            "There you can look out for the references.",
            "I'll show him to you at the end.",
            "Alright, so here's the graph labels, unlabeled nodes.",
            "What do we do with it?",
            "OK, so let's consider the by."
        ],
        [
            "In every case I know I was using digits which have 10 classes, but for now we consider the binary case.",
            "What I'm going to say is going to generalize in a fairly straightforward manner to other numbers of classes to discrete class labels.",
            "So in terms of notation I'm going to use L to denote the set of labeled points, anuta the note, the set of unlabeled points, anmi.",
            "I'm going to have a total of N points.",
            "And the binary labels are just going to be vectors and dimensional vectors of zeros and ones.",
            "That's a full labeling of my end data points.",
            "The graph.",
            "I'm going to represent by an N by N symmetric weight matrix W, so it's going to be assumed to be a weighted graph where more similar pairs of nodes would have a higher weight connecting them.",
            "But we also generally want the graph to be sparse.",
            "We don't want a fully connected graph because it won't scale very well to large datasets, so all the mathematics doesn't assume sparsity.",
            "Of the connectivity matrix W. Algorithmically, we're almost always going to end up with this sparse graph.",
            "OK. And this is a big assumption.",
            "We're going to assume that that.",
            "Graph that weighted graph is given to us.",
            "OK, so somehow you need to be able to measure the similarity between pairs of data points.",
            "It's a bit like knowing what the kernel is or something like that.",
            "I will talk a bit later about how we can learn that graph and the similarity function from data, but that's actually quite thorny.",
            "Alright, so here's a very simple idea for the binary case.",
            "Come up with an energy function.",
            "The energy is a function of that binary vector Y and the energy is just one half sum over IJWIJY I -- Y J squared.",
            "OK, now YI&YJ are both binary, so why I minus YJ squared?",
            "I could equivalently write that as an indicator function for weather why I is the same as YJ.",
            "Or why I is different from YJ?",
            "And what's going to happen is that if all of these weights are positive, we're going to assume all of them are positive.",
            "On a non negative.",
            "I.",
            "Then this energy function is going to prefer configurations where connected nodes have the same label.",
            "So if you get a 110000, you're happy you're in a low energy configuration.",
            "If you have disagreement at 1 zero or then 01, you're unhappy you have a higher energy configuration.",
            "OK, so naturally this energy function captures the concept that.",
            "Pairs of data points that have high weights between them should tend to have the same label.",
            "Alright.",
            "So how far can we go?"
        ],
        [
            "With this, let's think about this.",
            "So we have an energy function.",
            "I've rewritten it here if you had no label data in a completely unsupervised setting, then you could find a very happy minimum energy configuration simply by setting all the nodes to either zero or one.",
            "And the energy will be 0 in that case clearly.",
            "And that's the minimum energy configuration that you can get in the completely unsupervised case, and that's not interesting to us because you know.",
            "That just says alright.",
            "The best thing is everybody belongs to the same class, right?",
            "What we're going to look at is what happens if we take this energy function, and now we assume that we have a small number of labeled points.",
            "Where those labeled points are clamped at either the values zero or one.",
            "So we fix those label points at zero or one depending on their label.",
            "In this case we have this data point here.",
            "If you can see it clamped at a one and this data point over here is clamped at zero, OK, it's a bit hard to see.",
            "Maybe from the back.",
            "Right now we're going to say, well, given that I fixed the labels of two of these points.",
            "What is the minimum energy labeling of all the other nodes in the graph?",
            "So for example, if I label everything in the graph in the Class 0.",
            "Except for that one which is clamped, then four edges are unhappy.",
            "All the other edges are happy if all the edges have weight one, then the energy of this configuration would be 4.",
            "OK.",
            "If I use this configuration.",
            "The energy is too.",
            "Because only two we have only two unhappy edges, and if I do this, labeling of the graph, the energy is 1 'cause I only have one unhappy edge.",
            "OK, so in fact in this case the lowest energy configuration is this labeling.",
            "If I clamp these two points, yes question.",
            "However, you could take if the matriks graphics based on similarity matrix.",
            "Basically you would have a complete graph because everything is similar to everything with a certain way 120.",
            "Do you make up ruling or yes yes, so you could have OK.",
            "Conceptually you could have a complete graph, everything connected to everything else, but just the weights getting smaller and smaller for points that are far away.",
            "Everything I'm going to say holds for the complete graph.",
            "You can still find the minimum energy configuration for that graph.",
            "But algorithmically, it's a really bad idea because we're trying to use large amounts of unlabeled data.",
            "We don't want to create an end by end graph if we have a million unlabeled points, OK?",
            "So we prune.",
            "There are various ways of pruning the graph, or even just forming the graph without ever computing those N by N things by finding the nearest neighbors somehow, efficiently and using a nearest neighbor graph, let's say.",
            "Yes.",
            "Setting for all equal right in this particular simple example at the bottom I assumed all the weights were one, yeah.",
            "Yep, is there any information theory kind of connection and there is a lot of interesting connections.",
            "I'll tell you about in a couple of minutes, yes.",
            "Any other questions?",
            "Yep.",
            "It looks like spectral clustering with constraints.",
            "Yes, in fact, one way to solve this problem is this particular problem.",
            "This isn't the final answer that we're going to do.",
            "One way to solve this particular problem is through form of graph cuts, but with constraints.",
            "Yeah.",
            "OK, so this is this is the intuitive idea here."
        ],
        [
            "And.",
            "And we can formalize this energy function minimization.",
            "As being.",
            "Identical or isomorphic to the problem of finding a maximum at posteriori configuration of these bunch of hidden variables.",
            "In a Markov random field given by this graph Markov random field.",
            "Is an undirected graphical model, where in this case with binary variables and the edges correspond to the factors in the factor graph of the undirected graphical model.",
            "So anytime you see an energy.",
            "That is bounded below, you know, so it's a reasonable energy.",
            "You can turn it into a probability distribution, assuming that you can normalize this thing by taking E to the minus the energy.",
            "This is, I mean, if anybody has a physics background.",
            "You know you often see in statistical physics energy functions and then Gibbs distributions that are obtained by taking E to the minus the energy function at some temperature here.",
            "Without loss of generality we can just set the temperature equal to 1 OK.",
            "So minimizing the energy is equivalent to finding the maximum posteriori configuration of the following undirected graphical model over my variables given by this probability distribution, and that can be solved with graph min cuts.",
            "As somebody pointed out.",
            "OK, now there are some problems with just following this, and this actually is.",
            "An approach that was suggested by Bloom and Charla, an that we actually followed up on, somewhat unsuccessfully in terms of what we were interested in doing.",
            "One of the problems is if we go, if we want to go beyond the energies and actually find the probability.",
            "Maybe instead of doing a hard labeling, we want to somehow come up with a measure of the probability that unlabeled node belongs to one or the other class.",
            "Then computing those probabilities is expensive because in general the graph is going to be multiply connected.",
            "So if we're going to do something like message passing on the, that's you know the exact message passing is expensive.",
            "You could do sampling.",
            "You could do Junction tree algorithm etc etc.",
            "Anyway, it's a hard problem in the sense that it's a combinatorial problem.",
            "You have two to the N. In general labelings of nodes.",
            "And although you can find the map configuration efficiently, doing any sort of manipulations with that probability distribution is hard because it's a space of two to the N possible configurations.",
            "The multi class generalization is also a bit messy in this framework and learning that similarity matrix is very hard because the normalizing constant of this probability distribution depends on that similarity matrix and computing that or taking gradients of that is difficult.",
            "So for a variety of reasons which I'm not going to go into in any more detail, this.",
            "Is a very nice idea, but not simple enough.",
            "And we're going to actually simplify this even further.",
            "Alright."
        ],
        [
            "So the way we simplify it is to go from binary variables to Gaussian variables.",
            "So we're going to now think of a Gaussian Markov random field over the same nodes in the graph.",
            "So remember."
        ],
        [
            "This was what I just described.",
            "And what we're going to do is replace.",
            "The bind."
        ],
        [
            "Three variables.",
            "With real variables.",
            "OK, it's a relaxation of the problem in a sense.",
            "Now the variables can live on the reels.",
            "In fact, all they're going to do is live on the interval 01 for some interesting reasons.",
            "Well, the means are going to live on the interval 01.",
            "And this energy function remember was a quadratic in the wise.",
            "So when we hav E to the minus a quadratic that ends up being a Gaussian.",
            "OK, so let's look at that in a little more deep."
        ],
        [
            "Tell.",
            "So the probability distribution is E to the minus a quadratic in Yi rewrite, that quadratic in Y.",
            "Here.",
            "This notation here just says the probability of the wise is proportional to this.",
            "But we are clamping the labeled points.",
            "OK, so we're conditioning on the labels of the labeled points, taking on the values zero or one.",
            "And we have here a Gaussian distribution condition on that.",
            "And now if we take this weight matrix an, we rewrite this Gaussian distribution in a quadratic form in the wise.",
            "We can rewrite this with this matrix Delta in here.",
            "OK, so we just take this and through 2 lines of algebra we rewrite it as Y transpose.",
            "Which is an end by one vector.",
            "Sorry Y which is an M by one vector transpose an N by N matrix Delta and that vector Y again OK. Any questions about that?",
            "Yes.",
            "But if you don't have any observations, then this is an improper distribution, right?",
            "That's true, this is not a Gaussian.",
            "Is not a proper Gaussian if you have no labeled observations.",
            "The way you can see that there's a very nice way to see that is if I have no labeled data, I can add a constant.",
            "To all the wise, add 17 1/2 to all the wise.",
            "That doesn't change the energy.",
            "The energy only cares about the differences in the wise.",
            "Adding the same constant all the guys doesn't change the energy.",
            "That means it doesn't change the probability.",
            "And that's very weird because that says that there is an improper distribution with a sort of Ridge where the constant vector for any constant the constant vector added to the wise, doesn't change the probability.",
            "OK, and the other way to see that is when we rewrite this expression in a matrix form this matrix Delta.",
            "Has an eigen vector proportional to the constant vector with zero eigenvalue.",
            "OK, I mean in a sense, that's the nullspace of this matrix.",
            "Delta is not invertible.",
            "All right, but as soon as you have one or more data points, that degree of freedom that are labeled that degree of freedom disappears and it becomes a proper Gaussian.",
            "So it's a funny Gaussian.",
            "Alright, any other questions?",
            "OK, now this matrix Delta in here is.",
            "Just through the algebra in here, it's obtained by taking your weight matrix.",
            "And a diagonal matrix which is simply either the row or column sums of the weight matrix.",
            "This is what the diagonal matrix looks like.",
            "The off diagonals are zero and the diagonal elements are the corresponding take row sums.",
            "For example, of the W matrix, you take D -- W, and that's the matrix Delta that appears in here.",
            "Just bialgebra OK. And that matrix Delta has a name that's called the graph Laplacian, and I'm going to talk about that in a minute now.",
            "In particular, we can take that matrix Delta and we can decompose it into.",
            "If you can see that at the bottom.",
            "Sorry, the labeled by labeled part the labeled by unlabeled part and the unlabeled by unlabeled part of the matrix.",
            "So it's take a block decomposition.",
            "Just reorder the points so that all your label points come first, and all your unlabeled points come later.",
            "And these sub matrices of this block matrix I'm going to be using in a minute.",
            "OK questions about that."
        ],
        [
            "So let's look at that.",
            "That matrix Delta is called either the combinatorial or graph Laplacian I've described.",
            "What it is it, it takes a weighted graph and it computes A&M by N matrix on that weighted graph that you can think of as.",
            "The Matrix you can think of as an operator that can act on functions on the nodes of the graph.",
            "So what do I mean by that?",
            "So first of all, I just want to say this graph Laplacian plays the same role on graphs as the Laplace operator place in other spaces.",
            "So for example, in a Cartesian coordinate system, the Laplacian is given by the sum of 2nd partial derivatives of the function.",
            "So here's just sort of the Laplacian operator acting on a function.",
            "Imagine a function of X&Y with the Laplacian operator does is.",
            "It takes the second derivatives of that function with respect to X at any point, and the second derivatives with respect to Y at any point, and it sums those two up.",
            "OK, so this operator Axon functions in general on a space.",
            "In a graph, we can think of the nodes of the graph.",
            "As being able to have some function values associated with each of 'em, so a graph with N nodes, you can think of a function, an end dimensional function living on that.",
            "The nodes of that graph an M by N matrix just acts.",
            "They said operator that acts on that function and will see what it does in a minute, but essentially it enforces.",
            "We can use this to enforce a sort of smoothness of that function on the graph.",
            "OK.",
            "If we somehow minimize the Laplacian operator acting on functions in a Cartesian coordinate system, then what we get are smooth functions in X&Y.",
            "If we do the same thing on a graph, we get functions that are smooth with respect to the graph.",
            "Is that clear?",
            "'cause that could be cut?",
            "Kind of confusing.",
            "When I say a function on the graph, just think of it as a vector.",
            "Where each node takes on, it has holds one of the values of that vector.",
            "OK."
        ],
        [
            "So, um.",
            "So that's just connection to graph Laplacian.",
            "So this joint distribution.",
            "Of all of the Y values is given by this as before, and now as I said, the distribution of Y given the labeled points is Gaussian, and that's great because we heard from the Gaussian process lecture we can do all these interesting conditioning operations on a Gaussian very easily.",
            "So in fact we can exactly compute what the distribution on the unlabeled points is.",
            "Distribution of Y unlabeled given Y labeled given the label points and that's Gaussian with a particular mean FU.",
            "And a particular covariance matrix.",
            "OK. And the mean of that function is given by this equation, it's.",
            "Minus Delta UU.",
            "Inverse Delta UU is that you buy you unlabeled part of the graph, Laplacian inverted.",
            "Delta UL&YL is the.",
            "The labeled points.",
            "Right?",
            "OK everybody happy.",
            "Alright.",
            "So the mean of that function is what we're going to be paying special."
        ],
        [
            "Session 2.",
            "So the mean, on the unlabeled points, because everything the wiser, all Gaussian, the mean of a Gaussian is also the maximum of a Gaussian, right?",
            "It's also the mode of a Gaussian that is, and it's also the minimum energy configuration of that Gaussian Markov random field, because the maximum probability configuration is also the minimum energy configuration.",
            "And it's unique.",
            "OK, once you have at least one labeled point, it becomes unique.",
            "And this function, this mean function.",
            "Satisfies Laplace's equation, so in fact it's a.",
            "It's called a harmonic function and this is a very very nice property.",
            "So what does that mean?",
            "That this means that Delta F = 0, The Laplace operator acting on the end by one vector F. Equals zero for the solution to this minimization problem.",
            "And if I translate if I if I unpack the definition of that may."
        ],
        [
            "Tricks Delta in terms of the weights, it's only purely defined in terms of the weights."
        ],
        [
            "What that means?",
            "What it means for a function to be harmonic on the graph is the following property.",
            "It means that FI, for an unlabeled node.",
            "Is the weighted average of its neighbors.",
            "OK, that's what this expression says, summing over JJ being neighbors of iwi JFJ normalized.",
            "And this is really cool.",
            "Really simple.",
            "It just says if I'm a node on the graph and I don't know where to sit on this 01 interval for my probability being in Class 1.",
            "I look at my neighbors.",
            "And I just end up being a weighted average of my neighbors, weighted by these W's.",
            "Alright, of course, if I'm a labeled node, I'm stuck at either zero or one.",
            "But now my neighbors will be influenced by my label.",
            "And this is a, you know, this gives you very smooth functions on the graph and the smoothness is completely determined by this weight matrix W. So any questions about that?",
            "Yes.",
            "We also define the the mean.",
            "Yes, the the you know given any constraints.",
            "There is a unique solution to this problem and the solution is a harmonic function and that is the mean, so that's sorry that's why I'm overloading the notation here.",
            "Any function that satisfies this satisfies that property.",
            "And I'm also saying this mean function satisfies that property.",
            "OK. OK, and now the other interesting thing is then if I clamp a few points at zero or one.",
            "That everywhere, assuming the graph is connected OK, then all the unlabeled nodes will have a mean that somewhere between zero and one.",
            "Alright.",
            "So I mean, the key of the idea here is very simple.",
            "Take your graph.",
            "Take your labeled points, solve for the harmonic function for the mean of this thing, which is solving a linear system of equations and then interpret.",
            "The F value at each node as the probability of belonging to one or the other class, and then do some thresholding, classification or something like that on those values if you want.",
            "OK. Now we can.",
            "I think it's important to have additional intuitions about what's going on and the nice thing is that graph laplacians and harmonic functions have been widely studied in a whole bunch of different fields, so let me just give you a couple of interesting intuition."
        ],
        [
            "Is.",
            "That relate to this, so the mean.",
            "Of that that mean function F. Has a random walk interpretation in the following way.",
            "Take my graph.",
            "Take the weights in my graph and for every node.",
            "Normalize the outgoing weights in that node.",
            "And think of those as probabilities of transitioning in a random walk on the nodes on this graph transitioning from I to J.",
            "So the probability of transitioning from I to J is WI J normalized?",
            "OK, now all the weights were assumed to be not negative.",
            "Just to be clear.",
            "Now the harmonic function on this graph can be interpreted.",
            "As the harmonic function evaluated at node I, let's say take nodai, this yellow node here can be interpreted as the probability of reaching a node labeled one.",
            "From node I.",
            "Following this random walk transition process, so I started node I, I randomly transition to other unlabeled nodes.",
            "And sometimes I hit a node labeled zero and I stop.",
            "But other times when I start from this node I I hit a node labeled one and I stop.",
            "And the probability of reaching a node labeled one is that function F5 is solved by linear system of equations.",
            "OK. And again, this gives you an intuition you're thinking about propagating labels on this graph.",
            "Essentially, through the neighborhood connectivity.",
            "Questions about that.",
            "Yep.",
            "Uniqueness of the software solution."
        ],
        [
            "So uniqueness, yeah, yes it's well it's unique in the same because.",
            "Sorry.",
            "Because we relax it to a Gaussian and therefore we now have it's, you know, as a minimization problem.",
            "It's a quadratic very convex minimization problem, and the unique solution is given by linear system of equations, which is the mode of the Gaussian.",
            "Ah, if this is assuming you want to go back to the discrete Markov random field problem, then you would have to take those F values and discretizes somehow, but I'm not going to take that step.",
            "I'm going to stop at the F values and the way I'm going to discretize those F values will then depend on for example the relative losses that I have for different class labels.",
            "Or I can combine those probabilities with other source of information to do my classification.",
            "So I'm not thinking about this as an approximate way of solving.",
            "The discrete problem I'm just thinking about it as a reformulation of the original problem.",
            "Yep.",
            "To think of as it like logistic regression problem and then try to model the Canonical parameter of exponential family as dysfunction at.",
            "It would be very interesting to do that.",
            "I haven't.",
            "I haven't really thought much along those lines, but in the original paper what we do is we do talk about how we would combine this graph based semi supervised learning with other discriminative learning methods based on exogenous features that we might have of the individual data points.",
            "So we have some procedures for combining graph based than other discriminative methods.",
            "OK, so."
        ],
        [
            "This is the interpretation is a random walk.",
            "I like it.",
            "I mean it gives me some intuition.",
            "I also IRI."
        ],
        [
            "Really like this hardware implementation.",
            "For some reason this this this interpretation as an electric network.",
            "So essentially what you now have is take a network of resistors.",
            "Connecting these nodes.",
            "Where the resistance between I&J is one over WI J.",
            "So essentially, if if the weight is 0, then there is no wire connecting those pair of nodes.",
            "This is called a resistive electrical network and now what we're going to do is we're going to clamp the voltage of some of the nodes at one, some subset of nodes at one clamp.",
            "The voltage of some of the other subset of.",
            "Nodes at zero in the other class and then measure the voltage at each one of these nodes in the network, and the solution to our harmonic problem is also solution to this electric network problem.",
            "OK.",
            "So here is a hardware implementation of the semi supervised learning algorithm which is completely useless of course but.",
            "I don't know what a is doing here."
        ],
        [
            "OK, alright, so now let's take our labels F and let's think about how we actually classify those labels.",
            "So one naive thing is just take these function values and threshold am at .5 OK. And what we found in practice is that that leads to often at least two unbalanced classification.",
            "That doesn't give very good performance honestly.",
            "So in particular, if you have situations where you know something about the relative frequencies of the different classes, then.",
            "We can actually.",
            "Constrain our original problem very simply by adding an additional linear constraint that says my labels on the unlabeled points should conform with the known frequencies of the classes from some test data or something like that.",
            "And that gets rid.",
            "This is what we call class mass normalization that gets rid of a site kind of instability of the algorithm in terms of these unbalanced datasets.",
            "Let me also mention how to generalize this to multiple classes.",
            "Essentially, the generalization to multiple classes is very straightforward.",
            "For every class you consider whether you're in that the labeled point is either in that class or not in that class.",
            "So it's one versus all kind of generalization an you can separately solve all of those linear systems of equations.",
            "Actually, you want to solve them simultaneously for efficiency, and then what you get is.",
            "Vector at every node of values that sum to one that actually happens to come from the harmonic property, and you interpret that vector as your class probabilities for your K different classes.",
            "It's a bit like thinking of the random walk."
        ],
        [
            "Interpretation, but now each class is a different color and you just ask what's the probability that if I start at node I I end up.",
            "Hitting this color versus this color versus this color this color OK."
        ],
        [
            "Alright, so now how does this actually do?",
            "I'll show you some examples.",
            "So I actually think whenever you know, whenever you show up lots of results, it should always be taken anecdotally OK, because the space of possible problems one could try things on is absolutely huge and any paper will only try a small number of those problems, right?",
            "So yeah."
        ],
        [
            "Is it like or aggression problem by using custom processing?",
            "It's a very, very good question, because I actually when I first started thinking about this, I wanted the Gaussian process solution.",
            "Or a Markov random field solution, or something that was based in, you know, discrete Markov random field solution or something like that.",
            "This is a Gaussian random field rather than a Gaussian process.",
            "And the reason for that subtle difference is that the Gaussian process.",
            "Parametrizes the covariance matrix.",
            "And here what we're doing is through our weights W. We're parameterising the inverse covariance matrix.",
            "OK, and there are important differences between writing down a model in terms of the inverse covariance matrix versus writing down a model in terms of the covariance matrix.",
            "Maybe this is worth explaining.",
            "Actually a model in terms of the covariance matrix that would look almost identical to this.",
            "I just write down code back to this slide.",
            "Let's say I wrote down something in terms of."
        ],
        [
            "Covariance matrix that look almost identical to this.",
            "It has the property that it's coherent under marginalization of nodes.",
            "In other words, if I add a new unlabeled point and then I integrate it out, the label of that unlabeled point, it doesn't affect any of my other points.",
            "And that's exactly what we don't want.",
            "OK, we want the presence of unlabeled points to influence the labeling on the other points.",
            "So, somewhat perversely, if you actually try to solve this with the Gaussian process, you actually are unable.",
            "In the simplest form, to get any influence of the unlabeled data.",
            "I'll come to that point later.",
            "Yes.",
            "Correct?",
            "This problem in the end for.",
            "We've got the calcium shake.",
            "Gaussian what sorry.",
            "Say if you interpret it as.",
            "Clustering images, right?",
            "No, no, no.",
            "This is almost, you know, in a sense, almost forget about the Gaussian involved.",
            "Right, what we have is a procedure where you take any objects you want, images, documents, whatever.",
            "You form a graph, you say which ones are labeled one and zero, and then for all the other nodes you get values between one and zero.",
            "That's what this solution here will be.",
            "Values between one and zero.",
            "Then you interpret those values as probabilities of belonging to the class one and 0.",
            "And the distributions of features of those images have.",
            "Don't have to have anything to do with the Gaussian distribution.",
            "In particular, maybe I show this example from the very."
        ],
        [
            "Beginning.",
            "The property this will have is that on data that looks like this, the graph that I form will have lots and lots of connections going from this point to this point and so with very high confidence you'll say that that point belongs to the blue positive class.",
            "OK, but it doesn't assume Gaussian gaussianity in feature space or anything like that.",
            "Anything?",
            "The distributions, yeah, you're not modeling the input density at all.",
            "You take the input data points you form a graph.",
            "And then you try to label.",
            "OK, so sorry I'm going forward.",
            "Alright."
        ],
        [
            "So here are some results, so this is.",
            "Classifying 10 digits.",
            "So it's a 10 way classification on this axis.",
            "What we have is the size of the labeled data set.",
            "The total data set is 4000.",
            "So for example, if we have, say 20 labeled points.",
            "What the method that I just described does is?",
            "It gives you about 70% accuracy in classification with 20 labeled points and 3980 unlabeled points OK. And as you get more and more labeled data, obviously the classification improves and plateaus in some way, and then these different curves are just for comparison.",
            "Here is what one nearest neighbor would do.",
            "An here is what the method would do if we didn't do the class balancing, so that actually does quite poorly unless we do the class balancing which is over here.",
            "OK, and incidentally one nearest neighbors is within the family of nearest neighbors method methods.",
            "On this data set is the best value of the number of nearest neighbors and is quite close to what most discriminative methods would do.",
            "With that, ignore the unlabeled data, but let me show you some more examples."
        ],
        [
            "Like this, so here is 20 newsgroups data set where we're trying to classify posts about PC versus Mac.",
            "This is sort of a classic text data set based on the bag of words representation for each post.",
            "To this newsgroup we form a graph with 1943 nodes in it.",
            "On this axis.",
            "What we have is the size of the labeled data set again.",
            "In the accuracy on this axis.",
            "So again with about let's say here 40 labeled documents.",
            "But you know almost a couple thousand unlabeled ones.",
            "We're getting over 90% accuracy.",
            "Whereas if you did voted Perceptron or SVM has actually very similar performance, in this case purely on the labeled data, you would be around 75% accuracy.",
            "This should, I mean, this is the basic thing that we want out of these methods.",
            "We want to be able to show that using the unlabeled data we can do better than methods state of the art methods that just use the labeled data OK.",
            "Yes.",
            "Is there any?",
            "Dip at the beginning.",
            "I think that's just the that's just a finite, very small data set effect.",
            "Also, the we've shown error bars here, which are absolutely huge, so that can be ignored I think.",
            "Alright.",
            "So where does the weight matrix come from?",
            "Can we learn the weight matrix in some way from the data and this has turned out to be extremely difficult, because if we assume we have a very small amount of labeled data and large amounts of unlabeled data, then if we try to learn the weight matrix somehow to do really well on the labeled data that we already have.",
            "Then almost any method that we try will overfit.",
            "OK, we're in the regime where we're assuming the amount of labeled data is small.",
            "So we need kind of a different principle then kind of fit to the labeled data to learn the weight matrix.",
            "So so just."
        ],
        [
            "To be clear, what do I mean by the weight matrix?",
            "So imagine for example the weights between two data points I&J are given by E to the minus some distance.",
            "Between the input features for data Point I and data point J.",
            "And so this is very similar to the parameters of a kernel that you might be trying to learn from the data.",
            "So these parameters here in blue could be lens scales that weight different features of your objects in order to come up with a weight matrix between your data points.",
            "So I don't know if this is clear.",
            "This is 1 example of learning parameters of weight matrix.",
            "Here is another simpler example.",
            "Let's say you use K nearest neighbor unweighted graph.",
            "So every data point you connect to is K nearest neighbors.",
            "What value of K should you use?",
            "OK. Or if for every data point I connected to all the other data points that are in an epsilon ball, what should the size of that epsilon ball be?",
            "Alright.",
            "These are the sorts of parameters what we're calling hyperparameters here of the graph that we want to learn from data.",
            "Yep.",
            "Do you consider to have a just directed rapport?",
            "It may, or it in directed graph.",
            "In our work, we assumed a completely undirected graph, so symmetric waits.",
            "There has been follow up work that considered generalizing it to directed graphs and you have an analog of the graph Laplacian for directed graphs as well.",
            "OK so here."
        ],
        [
            "Is what we're going to do you know what one thing we tried was evidenced maximization or marginal likelihood maximization within a Gaussian process type framework, but that ended up in a tech report because that just did didn't actually work very well right in this context of semi supervised learning.",
            "The method that we described.",
            "Is based on this funny idea of minimizing the entropy on the unlabeled points.",
            "OK, So what does that mean?",
            "That means we we try to find hyperparameters of these weights.",
            "Not so that they predict the label points well, but somehow so that they.",
            "Are most confident.",
            "On the unlabeled points.",
            "I think with with some care that that can actually do interesting things.",
            "So."
        ],
        [
            "Here is an example in handwritten digit classification.",
            "In this particular instance, ones versus twos.",
            "And here is you know the mean image of a one.",
            "Here's the mean image of a two.",
            "We start with the weights.",
            "Now that we're trying to learn our importance of every feature.",
            "Every pixel, sorry.",
            "In computing how similar 2 images are.",
            "So for every pixel we're going to learn a parameter, which is how important is that pixel Ann before learning, all pixels are equally important.",
            "After learning, we learn some pattern that seems to actually emphasize areas where the ones in the twos are different from each other, and in particular the entropy.",
            "Goes down after learning on the unlabeled data and the accuracy goes up fairly significantly from 94 to 98%.",
            "So here again, is anecdotal evidence.",
            "I would say that this entropy minimization idea or heuristic can be kind of useful for figuring out good hyperparameters of this weight.",
            "But I say that with great trepidation and caution."
        ],
        [
            "Alright.",
            "Now what I want to show you is a very cool application of semi supervised learning.",
            "I'll do that, then we can take a very short break and then I'll try to finish off, OK?",
            "So here is an application I wasn't actually involved in, but some of the Jerry Zhu was kind of John Lafferty were involved in this is from a paper called person identification in webcam images."
        ],
        [
            "So here's what they did.",
            "This was all done at Carnegie Mellon University and at CMU there was this lounge that was the grad student lounge, the computer science grad student lounge with the Coke machine.",
            "The Coke machine has a webcam built into it and looks out onto a table, and this webcam is called the free food Cam, so grad students can check this webcam to see if anybody has left.",
            "Food on this table and then they can run and eat the food OK.",
            "This is what graduate student life is like.",
            "As a side effect of this, we have fascinating data that we've collected on people coming into this lounge at different times of day and night, of course.",
            "And here are just for typical example images.",
            "Here is Avrim Blum trying to grab some free food.",
            "He's not a grad student by any means, but he seems to be interested in the free food as well.",
            "OK, so here are some typical example images.",
            "What we're trying to do is we're trying to identify the people in the room OK, and the way we're going to do that.",
            "I say we, although I wasn't involved at all.",
            "The way they're going to do that, is through a couple of steps, which I'll explain right now.",
            "So first of all, the camera is always pointing in the same direction, so we can subtract this sort of average background image."
        ],
        [
            "Which looks like this.",
            "And that is going to be ignored.",
            "You know, it's going to be subtracted out from our images because we care about person identification, not share identification.",
            "Then here is just an example of some of the data that they have.",
            "They have.",
            "They have 10 people that they're trying to identify.",
            "And these people come into the room on different days over some period of time and in total there are about 5000 images that are obtained from video sequences of people coming into the room.",
            "Whatever grabbing food and leaving.",
            "Now the interesting thing about this data is that it's quite varied, right?",
            "This is the typical thing in vision.",
            "The illumination obviously varies, but also people wear hopefully different clothes on different days.",
            "Right?"
        ],
        [
            "So.",
            "The data was preprocessed to do a foreground extraction, so this is John Lafferty and that's Jerry Zhu.",
            "An then from these foreground images you could also do face detection and just extract the face images."
        ],
        [
            "Now.",
            "What we're going to do is we're going to connect images to other images in a graph.",
            "Alright, the way we're going to do that is we're going to have several different kinds of neighborhood relationships between images.",
            "So one type of neighborhood is a time edge, so this is 1 example image.",
            "This is Nina Balcan, the first author of this paper, Ann.",
            "This is a neighbor of image 2910 and it's connected to that Biotime edge.",
            "Because this webcam image was taken very near in time to this webcam image.",
            "This webcam image is a neighbor of 2910 through a color edge.",
            "Because the foreground that was extracted here Nina's shirt has the same color as the foreground here, so through some color feature extraction you form edges between images that have the same foreground colors.",
            "This is another image that's a neighbor of this through color edge, another color edge, and now this image.",
            "Where she's wearing different clothes is actually a neighbor of 2910 through a face edge, so that's a different kind of relationship that says these two faces are similar enough to put an edge between them.",
            "OK, so we have time edges, color edges and face edges.",
            "We have 5000 images which are 5000 nodes on a graph.",
            "We connect them together.",
            "And then we can consider what happens if you walk around on this graph, starting from an unlabeled image to see where you end up with the labeled image.",
            "So in this particular case, this last image is labeled Jerry."
        ],
        [
            "This image here, which is actually a very difficult image to identify, right?",
            "It's Jerry's back.",
            "OK.",
            "This image here is connected to this image through a color edge.",
            "This image is connected to this image through a time edge.",
            "These happen nearby in time.",
            "This image is connected to this image through a face edge and this image is connected to this image through a color edge.",
            "OK, now we know that this image has the label Jerry and this sort of label propagation idea, or the graph Laplacian.",
            "When you solve for the harmonic function where it will give you is a high probability that that's an image of Jerry.",
            "OK."
        ],
        [
            "That's the basic intuition.",
            "You can turn that into plots in an experiment.",
            "So here we have the harmonic function accuracy as a function of number of labeled images.",
            "We have 10 people.",
            "20 labeled images means 2 images per person.",
            "OK, that's pretty tough.",
            "I think if you try to do this naively, you can get 70% accuracy with two images per person.",
            "It only goes up to about 80% with.",
            "You know?",
            "More images like.",
            "20 images per person question.",
            "Yeah.",
            "Ye good question.",
            "The weights on the different kinds of edges are.",
            "In this case, I'm not sure whether they learn them or they set them by hand.",
            "But they could have tried to learn the weights on the different kinds of edges through this entropy minimization framework.",
            "I would have to look at the paper to see what exactly they did.",
            "Yeah, yes.",
            "How do we combine combine these three different metrics for the?",
            "So you can't actually different graphs and then pointer.",
            "You can.",
            "I believe what they did, but I would have to check in more detail is they?",
            "Constructed three different graphs and then they combine them into one graph.",
            "The simplest form would be just put an edge if one of these or one of these or one of these is close enough, but a more sophisticated and this is also related to the previous question, more sophisticated thing to do would be to actually learn the relative importance or weights of these three kinds of similarity.",
            "I think that would be, and I think they did some of that as well.",
            "Questions, Yep.",
            "OK cool, I have some other I think interesting stuff to talk about, but I want you guys to have a chance to get up and take a break.",
            "So why don't you take a very short 5 minute break and then I'll talk about computation and active learning."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about graph based, semi supervised learning 'cause this was voted, you know by audience selection.",
                    "label": 0
                },
                {
                    "sent": "And this is a it's a fun topic is interesting.",
                    "label": 0
                },
                {
                    "sent": "I think it's actually reasonably practical as a refresher.",
                    "label": 0
                },
                {
                    "sent": "You'll get to hear me talk about fairly non Bayesian stuff, although I can't resist the temptation right at the end to have a little bit of discussion of how we might possibly be able to do this stuff in a basic primer.",
                    "label": 0
                },
                {
                    "sent": "But almost the whole.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In Semi supervised learning we are motivated by the opportunity that comes from the fact that in a lot of applications we have vast amounts of unlabeled data but small amounts of labeled data.",
                    "label": 0
                },
                {
                    "sent": "And in particular, the process of getting good reliable labels or annotating data tends to be expensive.",
                    "label": 0
                },
                {
                    "sent": "And we want to be able to have supervised learning methods in general.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about essentially discriminative learning methods that can somehow make use of the information about the input distribution that's given by large amounts of unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So to motivate a particular example, let's say think about images.",
                    "label": 0
                },
                {
                    "sent": "On the web.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are lots and lots of images of cats, let's say.",
                    "label": 0
                },
                {
                    "sent": "And you know you could crawl the web for images, but only very small number of them might actually have proper reliable labels for what you're looking for.",
                    "label": 0
                },
                {
                    "sent": "So can we use information from large amounts of unlabeled data to help us with classification in particular?",
                    "label": 0
                },
                {
                    "sent": "But we can obviously talk about generalizations to other problems like regression and so on.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's think about this.",
                    "label": 0
                },
                {
                    "sent": "The basic approach that.",
                    "label": 0
                },
                {
                    "sent": "That we're going to be taking is to make some assumptions about how that unlabeled data influences the class.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Abilities OK, so the basic assumption is that there is some information in the data distribution.",
                    "label": 1
                },
                {
                    "sent": "Alright, so consider the following problem.",
                    "label": 0
                },
                {
                    "sent": "We have two labeled points in the minus class we have three labeled points in the plus class and all these black dots are unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "OK, now here is a quiz.",
                    "label": 0
                },
                {
                    "sent": "This?",
                    "label": 0
                },
                {
                    "sent": "This point at this location given by the question mark.",
                    "label": 0
                },
                {
                    "sent": "Raise your hand if you think it's in the negative class.",
                    "label": 0
                },
                {
                    "sent": "Come on somebody.",
                    "label": 0
                },
                {
                    "sent": "Somebody's a diehard.",
                    "label": 0
                },
                {
                    "sent": "Yeah alright thank you.",
                    "label": 0
                },
                {
                    "sent": "OK we need somebody to pick on.",
                    "label": 0
                },
                {
                    "sent": "OK, raise your hand if you think it's in the positive class alright, why?",
                    "label": 0
                },
                {
                    "sent": "Why do we think that?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't need to, you know belabor this, but clearly we are using the fact that we have a lot of unlabeled data to sort of.",
                    "label": 0
                },
                {
                    "sent": "Tell us that this point, although in Euclidean space, may be more close to the negative points and clearly a linear classifier, would put it well.",
                    "label": 0
                },
                {
                    "sent": "On the side of the negative class, when we take into account the unlabeled points, we see that.",
                    "label": 0
                },
                {
                    "sent": "There are many, many unlabeled points that sort of connect this to a positive class, but there is a big gap that disconnects it from the negative class.",
                    "label": 0
                },
                {
                    "sent": "That's the basic intuition we are going to try to capture with our semi supervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "And there are various ways of thinking about this, but essentially the idea is it can be formulated in several ways.",
                    "label": 0
                },
                {
                    "sent": "The idea is somehow that label information should propagate through the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And another way to think about this, perhaps is that when we are going to measure similarity between data points, we shouldn't ignore the vast amounts of unlabeled data, because perhaps they tell us what a natural way of measuring similarity is.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about that?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the outline of the talk is going to be.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus mostly on graph based semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about active learning in this framework, which I think is also very natural and very interesting.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you've had any any lectures on active learning.",
                    "label": 0
                },
                {
                    "sent": "So I think that will be interesting.",
                    "label": 0
                },
                {
                    "sent": "And then in the last couple of slides I'll say alright everything was completely non Bayesian.",
                    "label": 0
                },
                {
                    "sent": "You know, as a Bayesian can we do something that approaches the nice simplicity and generality of these methods?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's go to graph based semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is we take our data both the labeled and unlabeled data, and we construct a graph.",
                    "label": 1
                },
                {
                    "sent": "Connecting similar data points.",
                    "label": 0
                },
                {
                    "sent": "So look at this.",
                    "label": 0
                },
                {
                    "sent": "The data are handwritten images.",
                    "label": 0
                },
                {
                    "sent": "The things in blue, if you can see them, are labels.",
                    "label": 0
                },
                {
                    "sent": "A few of the data points are labeled this.",
                    "label": 0
                },
                {
                    "sent": "This image is labeled two that images labeled 8 etc.",
                    "label": 0
                },
                {
                    "sent": "But there are a whole bunch of images that are not labeled in this tiny toy example, but we can take a some sort of similarity metric that operates in this case on the raw pixel values of these.",
                    "label": 0
                },
                {
                    "sent": "Little images and says that that image is similar to that image, so we connected in the graph.",
                    "label": 0
                },
                {
                    "sent": "Perhaps that image is similar to that image and we connected in the graph, etc OK.",
                    "label": 0
                },
                {
                    "sent": "So now we have a graph, some labeled nodes and a lot of unlabeled nodes.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do with this graph is we're going to put a.",
                    "label": 0
                },
                {
                    "sent": "Hidden variable put a random variable at each node in the graph corresponding to the label of that data point.",
                    "label": 0
                },
                {
                    "sent": "Some of those random variables will be observed.",
                    "label": 0
                },
                {
                    "sent": "The labeled ones will be observed and all the rest will be hidden.",
                    "label": 0
                },
                {
                    "sent": "And in fact, we can actually interpret this graph in what I'm going to say in a couple of minutes as an undirected graphical model or Markov random field that I've created over my data after observing the data points so as to do the semi supervised classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK, the intuition that the graph captures is similar.",
                    "label": 1
                },
                {
                    "sent": "Data points should have similar labels.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if I know that's got the label 8 and that's similar to that, then perhaps that should have the label 8 as well.",
                    "label": 0
                },
                {
                    "sent": "And that information should propagate essentially from the labeled data through to the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And clearly it can go wrong.",
                    "label": 0
                },
                {
                    "sent": "You can see here that you might then guess that that's an 8, and that's an 8, etc.",
                    "label": 0
                },
                {
                    "sent": "So we have to make sure that that propagation of information doesn't go completely crazy.",
                    "label": 0
                },
                {
                    "sent": "The graph encodes the intuition that similar data points have similar labels.",
                    "label": 1
                },
                {
                    "sent": "So the work that I'm going to talk about on graph based semi supervised learning is work done with Jerry Zhu while he was at CMU and John Lafferty.",
                    "label": 0
                },
                {
                    "sent": "And at the end of these slides I have a whole bunch of references, including for example a book by Jerry Zhu on the introduction to semi supervised learning etc.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There you can look out for the references.",
                    "label": 0
                },
                {
                    "sent": "I'll show him to you at the end.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's the graph labels, unlabeled nodes.",
                    "label": 0
                },
                {
                    "sent": "What do we do with it?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's consider the by.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In every case I know I was using digits which have 10 classes, but for now we consider the binary case.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to say is going to generalize in a fairly straightforward manner to other numbers of classes to discrete class labels.",
                    "label": 0
                },
                {
                    "sent": "So in terms of notation I'm going to use L to denote the set of labeled points, anuta the note, the set of unlabeled points, anmi.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have a total of N points.",
                    "label": 0
                },
                {
                    "sent": "And the binary labels are just going to be vectors and dimensional vectors of zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "That's a full labeling of my end data points.",
                    "label": 0
                },
                {
                    "sent": "The graph.",
                    "label": 0
                },
                {
                    "sent": "I'm going to represent by an N by N symmetric weight matrix W, so it's going to be assumed to be a weighted graph where more similar pairs of nodes would have a higher weight connecting them.",
                    "label": 1
                },
                {
                    "sent": "But we also generally want the graph to be sparse.",
                    "label": 0
                },
                {
                    "sent": "We don't want a fully connected graph because it won't scale very well to large datasets, so all the mathematics doesn't assume sparsity.",
                    "label": 0
                },
                {
                    "sent": "Of the connectivity matrix W. Algorithmically, we're almost always going to end up with this sparse graph.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is a big assumption.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that that.",
                    "label": 0
                },
                {
                    "sent": "Graph that weighted graph is given to us.",
                    "label": 0
                },
                {
                    "sent": "OK, so somehow you need to be able to measure the similarity between pairs of data points.",
                    "label": 0
                },
                {
                    "sent": "It's a bit like knowing what the kernel is or something like that.",
                    "label": 0
                },
                {
                    "sent": "I will talk a bit later about how we can learn that graph and the similarity function from data, but that's actually quite thorny.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's a very simple idea for the binary case.",
                    "label": 0
                },
                {
                    "sent": "Come up with an energy function.",
                    "label": 0
                },
                {
                    "sent": "The energy is a function of that binary vector Y and the energy is just one half sum over IJWIJY I -- Y J squared.",
                    "label": 0
                },
                {
                    "sent": "OK, now YI&YJ are both binary, so why I minus YJ squared?",
                    "label": 0
                },
                {
                    "sent": "I could equivalently write that as an indicator function for weather why I is the same as YJ.",
                    "label": 0
                },
                {
                    "sent": "Or why I is different from YJ?",
                    "label": 0
                },
                {
                    "sent": "And what's going to happen is that if all of these weights are positive, we're going to assume all of them are positive.",
                    "label": 0
                },
                {
                    "sent": "On a non negative.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Then this energy function is going to prefer configurations where connected nodes have the same label.",
                    "label": 0
                },
                {
                    "sent": "So if you get a 110000, you're happy you're in a low energy configuration.",
                    "label": 0
                },
                {
                    "sent": "If you have disagreement at 1 zero or then 01, you're unhappy you have a higher energy configuration.",
                    "label": 0
                },
                {
                    "sent": "OK, so naturally this energy function captures the concept that.",
                    "label": 0
                },
                {
                    "sent": "Pairs of data points that have high weights between them should tend to have the same label.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So how far can we go?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this, let's think about this.",
                    "label": 0
                },
                {
                    "sent": "So we have an energy function.",
                    "label": 0
                },
                {
                    "sent": "I've rewritten it here if you had no label data in a completely unsupervised setting, then you could find a very happy minimum energy configuration simply by setting all the nodes to either zero or one.",
                    "label": 0
                },
                {
                    "sent": "And the energy will be 0 in that case clearly.",
                    "label": 0
                },
                {
                    "sent": "And that's the minimum energy configuration that you can get in the completely unsupervised case, and that's not interesting to us because you know.",
                    "label": 0
                },
                {
                    "sent": "That just says alright.",
                    "label": 0
                },
                {
                    "sent": "The best thing is everybody belongs to the same class, right?",
                    "label": 0
                },
                {
                    "sent": "What we're going to look at is what happens if we take this energy function, and now we assume that we have a small number of labeled points.",
                    "label": 0
                },
                {
                    "sent": "Where those labeled points are clamped at either the values zero or one.",
                    "label": 0
                },
                {
                    "sent": "So we fix those label points at zero or one depending on their label.",
                    "label": 0
                },
                {
                    "sent": "In this case we have this data point here.",
                    "label": 0
                },
                {
                    "sent": "If you can see it clamped at a one and this data point over here is clamped at zero, OK, it's a bit hard to see.",
                    "label": 0
                },
                {
                    "sent": "Maybe from the back.",
                    "label": 0
                },
                {
                    "sent": "Right now we're going to say, well, given that I fixed the labels of two of these points.",
                    "label": 0
                },
                {
                    "sent": "What is the minimum energy labeling of all the other nodes in the graph?",
                    "label": 0
                },
                {
                    "sent": "So for example, if I label everything in the graph in the Class 0.",
                    "label": 0
                },
                {
                    "sent": "Except for that one which is clamped, then four edges are unhappy.",
                    "label": 0
                },
                {
                    "sent": "All the other edges are happy if all the edges have weight one, then the energy of this configuration would be 4.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If I use this configuration.",
                    "label": 0
                },
                {
                    "sent": "The energy is too.",
                    "label": 0
                },
                {
                    "sent": "Because only two we have only two unhappy edges, and if I do this, labeling of the graph, the energy is 1 'cause I only have one unhappy edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so in fact in this case the lowest energy configuration is this labeling.",
                    "label": 0
                },
                {
                    "sent": "If I clamp these two points, yes question.",
                    "label": 0
                },
                {
                    "sent": "However, you could take if the matriks graphics based on similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "Basically you would have a complete graph because everything is similar to everything with a certain way 120.",
                    "label": 0
                },
                {
                    "sent": "Do you make up ruling or yes yes, so you could have OK.",
                    "label": 0
                },
                {
                    "sent": "Conceptually you could have a complete graph, everything connected to everything else, but just the weights getting smaller and smaller for points that are far away.",
                    "label": 0
                },
                {
                    "sent": "Everything I'm going to say holds for the complete graph.",
                    "label": 0
                },
                {
                    "sent": "You can still find the minimum energy configuration for that graph.",
                    "label": 1
                },
                {
                    "sent": "But algorithmically, it's a really bad idea because we're trying to use large amounts of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "We don't want to create an end by end graph if we have a million unlabeled points, OK?",
                    "label": 0
                },
                {
                    "sent": "So we prune.",
                    "label": 0
                },
                {
                    "sent": "There are various ways of pruning the graph, or even just forming the graph without ever computing those N by N things by finding the nearest neighbors somehow, efficiently and using a nearest neighbor graph, let's say.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Setting for all equal right in this particular simple example at the bottom I assumed all the weights were one, yeah.",
                    "label": 1
                },
                {
                    "sent": "Yep, is there any information theory kind of connection and there is a lot of interesting connections.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you about in a couple of minutes, yes.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It looks like spectral clustering with constraints.",
                    "label": 0
                },
                {
                    "sent": "Yes, in fact, one way to solve this problem is this particular problem.",
                    "label": 0
                },
                {
                    "sent": "This isn't the final answer that we're going to do.",
                    "label": 0
                },
                {
                    "sent": "One way to solve this particular problem is through form of graph cuts, but with constraints.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the intuitive idea here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And we can formalize this energy function minimization.",
                    "label": 0
                },
                {
                    "sent": "As being.",
                    "label": 0
                },
                {
                    "sent": "Identical or isomorphic to the problem of finding a maximum at posteriori configuration of these bunch of hidden variables.",
                    "label": 0
                },
                {
                    "sent": "In a Markov random field given by this graph Markov random field.",
                    "label": 1
                },
                {
                    "sent": "Is an undirected graphical model, where in this case with binary variables and the edges correspond to the factors in the factor graph of the undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "So anytime you see an energy.",
                    "label": 0
                },
                {
                    "sent": "That is bounded below, you know, so it's a reasonable energy.",
                    "label": 0
                },
                {
                    "sent": "You can turn it into a probability distribution, assuming that you can normalize this thing by taking E to the minus the energy.",
                    "label": 0
                },
                {
                    "sent": "This is, I mean, if anybody has a physics background.",
                    "label": 0
                },
                {
                    "sent": "You know you often see in statistical physics energy functions and then Gibbs distributions that are obtained by taking E to the minus the energy function at some temperature here.",
                    "label": 0
                },
                {
                    "sent": "Without loss of generality we can just set the temperature equal to 1 OK.",
                    "label": 0
                },
                {
                    "sent": "So minimizing the energy is equivalent to finding the maximum posteriori configuration of the following undirected graphical model over my variables given by this probability distribution, and that can be solved with graph min cuts.",
                    "label": 0
                },
                {
                    "sent": "As somebody pointed out.",
                    "label": 0
                },
                {
                    "sent": "OK, now there are some problems with just following this, and this actually is.",
                    "label": 0
                },
                {
                    "sent": "An approach that was suggested by Bloom and Charla, an that we actually followed up on, somewhat unsuccessfully in terms of what we were interested in doing.",
                    "label": 0
                },
                {
                    "sent": "One of the problems is if we go, if we want to go beyond the energies and actually find the probability.",
                    "label": 0
                },
                {
                    "sent": "Maybe instead of doing a hard labeling, we want to somehow come up with a measure of the probability that unlabeled node belongs to one or the other class.",
                    "label": 0
                },
                {
                    "sent": "Then computing those probabilities is expensive because in general the graph is going to be multiply connected.",
                    "label": 1
                },
                {
                    "sent": "So if we're going to do something like message passing on the, that's you know the exact message passing is expensive.",
                    "label": 0
                },
                {
                    "sent": "You could do sampling.",
                    "label": 0
                },
                {
                    "sent": "You could do Junction tree algorithm etc etc.",
                    "label": 0
                },
                {
                    "sent": "Anyway, it's a hard problem in the sense that it's a combinatorial problem.",
                    "label": 0
                },
                {
                    "sent": "You have two to the N. In general labelings of nodes.",
                    "label": 0
                },
                {
                    "sent": "And although you can find the map configuration efficiently, doing any sort of manipulations with that probability distribution is hard because it's a space of two to the N possible configurations.",
                    "label": 1
                },
                {
                    "sent": "The multi class generalization is also a bit messy in this framework and learning that similarity matrix is very hard because the normalizing constant of this probability distribution depends on that similarity matrix and computing that or taking gradients of that is difficult.",
                    "label": 1
                },
                {
                    "sent": "So for a variety of reasons which I'm not going to go into in any more detail, this.",
                    "label": 0
                },
                {
                    "sent": "Is a very nice idea, but not simple enough.",
                    "label": 0
                },
                {
                    "sent": "And we're going to actually simplify this even further.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we simplify it is to go from binary variables to Gaussian variables.",
                    "label": 0
                },
                {
                    "sent": "So we're going to now think of a Gaussian Markov random field over the same nodes in the graph.",
                    "label": 1
                },
                {
                    "sent": "So remember.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was what I just described.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is replace.",
                    "label": 0
                },
                {
                    "sent": "The bind.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three variables.",
                    "label": 0
                },
                {
                    "sent": "With real variables.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a relaxation of the problem in a sense.",
                    "label": 0
                },
                {
                    "sent": "Now the variables can live on the reels.",
                    "label": 0
                },
                {
                    "sent": "In fact, all they're going to do is live on the interval 01 for some interesting reasons.",
                    "label": 0
                },
                {
                    "sent": "Well, the means are going to live on the interval 01.",
                    "label": 0
                },
                {
                    "sent": "And this energy function remember was a quadratic in the wise.",
                    "label": 0
                },
                {
                    "sent": "So when we hav E to the minus a quadratic that ends up being a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at that in a little more deep.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tell.",
                    "label": 0
                },
                {
                    "sent": "So the probability distribution is E to the minus a quadratic in Yi rewrite, that quadratic in Y.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This notation here just says the probability of the wise is proportional to this.",
                    "label": 0
                },
                {
                    "sent": "But we are clamping the labeled points.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're conditioning on the labels of the labeled points, taking on the values zero or one.",
                    "label": 0
                },
                {
                    "sent": "And we have here a Gaussian distribution condition on that.",
                    "label": 0
                },
                {
                    "sent": "And now if we take this weight matrix an, we rewrite this Gaussian distribution in a quadratic form in the wise.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this with this matrix Delta in here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just take this and through 2 lines of algebra we rewrite it as Y transpose.",
                    "label": 0
                },
                {
                    "sent": "Which is an end by one vector.",
                    "label": 0
                },
                {
                    "sent": "Sorry Y which is an M by one vector transpose an N by N matrix Delta and that vector Y again OK. Any questions about that?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "But if you don't have any observations, then this is an improper distribution, right?",
                    "label": 0
                },
                {
                    "sent": "That's true, this is not a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Is not a proper Gaussian if you have no labeled observations.",
                    "label": 0
                },
                {
                    "sent": "The way you can see that there's a very nice way to see that is if I have no labeled data, I can add a constant.",
                    "label": 0
                },
                {
                    "sent": "To all the wise, add 17 1/2 to all the wise.",
                    "label": 0
                },
                {
                    "sent": "That doesn't change the energy.",
                    "label": 0
                },
                {
                    "sent": "The energy only cares about the differences in the wise.",
                    "label": 0
                },
                {
                    "sent": "Adding the same constant all the guys doesn't change the energy.",
                    "label": 0
                },
                {
                    "sent": "That means it doesn't change the probability.",
                    "label": 0
                },
                {
                    "sent": "And that's very weird because that says that there is an improper distribution with a sort of Ridge where the constant vector for any constant the constant vector added to the wise, doesn't change the probability.",
                    "label": 0
                },
                {
                    "sent": "OK, and the other way to see that is when we rewrite this expression in a matrix form this matrix Delta.",
                    "label": 0
                },
                {
                    "sent": "Has an eigen vector proportional to the constant vector with zero eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean in a sense, that's the nullspace of this matrix.",
                    "label": 0
                },
                {
                    "sent": "Delta is not invertible.",
                    "label": 0
                },
                {
                    "sent": "All right, but as soon as you have one or more data points, that degree of freedom that are labeled that degree of freedom disappears and it becomes a proper Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So it's a funny Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Alright, any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, now this matrix Delta in here is.",
                    "label": 0
                },
                {
                    "sent": "Just through the algebra in here, it's obtained by taking your weight matrix.",
                    "label": 0
                },
                {
                    "sent": "And a diagonal matrix which is simply either the row or column sums of the weight matrix.",
                    "label": 0
                },
                {
                    "sent": "This is what the diagonal matrix looks like.",
                    "label": 0
                },
                {
                    "sent": "The off diagonals are zero and the diagonal elements are the corresponding take row sums.",
                    "label": 0
                },
                {
                    "sent": "For example, of the W matrix, you take D -- W, and that's the matrix Delta that appears in here.",
                    "label": 0
                },
                {
                    "sent": "Just bialgebra OK. And that matrix Delta has a name that's called the graph Laplacian, and I'm going to talk about that in a minute now.",
                    "label": 0
                },
                {
                    "sent": "In particular, we can take that matrix Delta and we can decompose it into.",
                    "label": 0
                },
                {
                    "sent": "If you can see that at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the labeled by labeled part the labeled by unlabeled part and the unlabeled by unlabeled part of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So it's take a block decomposition.",
                    "label": 0
                },
                {
                    "sent": "Just reorder the points so that all your label points come first, and all your unlabeled points come later.",
                    "label": 0
                },
                {
                    "sent": "And these sub matrices of this block matrix I'm going to be using in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK questions about that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at that.",
                    "label": 0
                },
                {
                    "sent": "That matrix Delta is called either the combinatorial or graph Laplacian I've described.",
                    "label": 0
                },
                {
                    "sent": "What it is it, it takes a weighted graph and it computes A&M by N matrix on that weighted graph that you can think of as.",
                    "label": 0
                },
                {
                    "sent": "The Matrix you can think of as an operator that can act on functions on the nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "So first of all, I just want to say this graph Laplacian plays the same role on graphs as the Laplace operator place in other spaces.",
                    "label": 1
                },
                {
                    "sent": "So for example, in a Cartesian coordinate system, the Laplacian is given by the sum of 2nd partial derivatives of the function.",
                    "label": 1
                },
                {
                    "sent": "So here's just sort of the Laplacian operator acting on a function.",
                    "label": 0
                },
                {
                    "sent": "Imagine a function of X&Y with the Laplacian operator does is.",
                    "label": 0
                },
                {
                    "sent": "It takes the second derivatives of that function with respect to X at any point, and the second derivatives with respect to Y at any point, and it sums those two up.",
                    "label": 0
                },
                {
                    "sent": "OK, so this operator Axon functions in general on a space.",
                    "label": 0
                },
                {
                    "sent": "In a graph, we can think of the nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "As being able to have some function values associated with each of 'em, so a graph with N nodes, you can think of a function, an end dimensional function living on that.",
                    "label": 0
                },
                {
                    "sent": "The nodes of that graph an M by N matrix just acts.",
                    "label": 0
                },
                {
                    "sent": "They said operator that acts on that function and will see what it does in a minute, but essentially it enforces.",
                    "label": 0
                },
                {
                    "sent": "We can use this to enforce a sort of smoothness of that function on the graph.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If we somehow minimize the Laplacian operator acting on functions in a Cartesian coordinate system, then what we get are smooth functions in X&Y.",
                    "label": 0
                },
                {
                    "sent": "If we do the same thing on a graph, we get functions that are smooth with respect to the graph.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "'cause that could be cut?",
                    "label": 0
                },
                {
                    "sent": "Kind of confusing.",
                    "label": 0
                },
                {
                    "sent": "When I say a function on the graph, just think of it as a vector.",
                    "label": 0
                },
                {
                    "sent": "Where each node takes on, it has holds one of the values of that vector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So that's just connection to graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Of all of the Y values is given by this as before, and now as I said, the distribution of Y given the labeled points is Gaussian, and that's great because we heard from the Gaussian process lecture we can do all these interesting conditioning operations on a Gaussian very easily.",
                    "label": 1
                },
                {
                    "sent": "So in fact we can exactly compute what the distribution on the unlabeled points is.",
                    "label": 0
                },
                {
                    "sent": "Distribution of Y unlabeled given Y labeled given the label points and that's Gaussian with a particular mean FU.",
                    "label": 0
                },
                {
                    "sent": "And a particular covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "OK. And the mean of that function is given by this equation, it's.",
                    "label": 0
                },
                {
                    "sent": "Minus Delta UU.",
                    "label": 0
                },
                {
                    "sent": "Inverse Delta UU is that you buy you unlabeled part of the graph, Laplacian inverted.",
                    "label": 0
                },
                {
                    "sent": "Delta UL&YL is the.",
                    "label": 0
                },
                {
                    "sent": "The labeled points.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK everybody happy.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So the mean of that function is what we're going to be paying special.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Session 2.",
                    "label": 0
                },
                {
                    "sent": "So the mean, on the unlabeled points, because everything the wiser, all Gaussian, the mean of a Gaussian is also the maximum of a Gaussian, right?",
                    "label": 1
                },
                {
                    "sent": "It's also the mode of a Gaussian that is, and it's also the minimum energy configuration of that Gaussian Markov random field, because the maximum probability configuration is also the minimum energy configuration.",
                    "label": 0
                },
                {
                    "sent": "And it's unique.",
                    "label": 0
                },
                {
                    "sent": "OK, once you have at least one labeled point, it becomes unique.",
                    "label": 0
                },
                {
                    "sent": "And this function, this mean function.",
                    "label": 0
                },
                {
                    "sent": "Satisfies Laplace's equation, so in fact it's a.",
                    "label": 0
                },
                {
                    "sent": "It's called a harmonic function and this is a very very nice property.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That this means that Delta F = 0, The Laplace operator acting on the end by one vector F. Equals zero for the solution to this minimization problem.",
                    "label": 0
                },
                {
                    "sent": "And if I translate if I if I unpack the definition of that may.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks Delta in terms of the weights, it's only purely defined in terms of the weights.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What that means?",
                    "label": 0
                },
                {
                    "sent": "What it means for a function to be harmonic on the graph is the following property.",
                    "label": 0
                },
                {
                    "sent": "It means that FI, for an unlabeled node.",
                    "label": 0
                },
                {
                    "sent": "Is the weighted average of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what this expression says, summing over JJ being neighbors of iwi JFJ normalized.",
                    "label": 0
                },
                {
                    "sent": "And this is really cool.",
                    "label": 0
                },
                {
                    "sent": "Really simple.",
                    "label": 0
                },
                {
                    "sent": "It just says if I'm a node on the graph and I don't know where to sit on this 01 interval for my probability being in Class 1.",
                    "label": 0
                },
                {
                    "sent": "I look at my neighbors.",
                    "label": 0
                },
                {
                    "sent": "And I just end up being a weighted average of my neighbors, weighted by these W's.",
                    "label": 0
                },
                {
                    "sent": "Alright, of course, if I'm a labeled node, I'm stuck at either zero or one.",
                    "label": 0
                },
                {
                    "sent": "But now my neighbors will be influenced by my label.",
                    "label": 0
                },
                {
                    "sent": "And this is a, you know, this gives you very smooth functions on the graph and the smoothness is completely determined by this weight matrix W. So any questions about that?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We also define the the mean.",
                    "label": 1
                },
                {
                    "sent": "Yes, the the you know given any constraints.",
                    "label": 0
                },
                {
                    "sent": "There is a unique solution to this problem and the solution is a harmonic function and that is the mean, so that's sorry that's why I'm overloading the notation here.",
                    "label": 0
                },
                {
                    "sent": "Any function that satisfies this satisfies that property.",
                    "label": 0
                },
                {
                    "sent": "And I'm also saying this mean function satisfies that property.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, and now the other interesting thing is then if I clamp a few points at zero or one.",
                    "label": 0
                },
                {
                    "sent": "That everywhere, assuming the graph is connected OK, then all the unlabeled nodes will have a mean that somewhere between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So I mean, the key of the idea here is very simple.",
                    "label": 0
                },
                {
                    "sent": "Take your graph.",
                    "label": 1
                },
                {
                    "sent": "Take your labeled points, solve for the harmonic function for the mean of this thing, which is solving a linear system of equations and then interpret.",
                    "label": 0
                },
                {
                    "sent": "The F value at each node as the probability of belonging to one or the other class, and then do some thresholding, classification or something like that on those values if you want.",
                    "label": 0
                },
                {
                    "sent": "OK. Now we can.",
                    "label": 0
                },
                {
                    "sent": "I think it's important to have additional intuitions about what's going on and the nice thing is that graph laplacians and harmonic functions have been widely studied in a whole bunch of different fields, so let me just give you a couple of interesting intuition.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "That relate to this, so the mean.",
                    "label": 0
                },
                {
                    "sent": "Of that that mean function F. Has a random walk interpretation in the following way.",
                    "label": 0
                },
                {
                    "sent": "Take my graph.",
                    "label": 0
                },
                {
                    "sent": "Take the weights in my graph and for every node.",
                    "label": 0
                },
                {
                    "sent": "Normalize the outgoing weights in that node.",
                    "label": 0
                },
                {
                    "sent": "And think of those as probabilities of transitioning in a random walk on the nodes on this graph transitioning from I to J.",
                    "label": 0
                },
                {
                    "sent": "So the probability of transitioning from I to J is WI J normalized?",
                    "label": 0
                },
                {
                    "sent": "OK, now all the weights were assumed to be not negative.",
                    "label": 0
                },
                {
                    "sent": "Just to be clear.",
                    "label": 0
                },
                {
                    "sent": "Now the harmonic function on this graph can be interpreted.",
                    "label": 0
                },
                {
                    "sent": "As the harmonic function evaluated at node I, let's say take nodai, this yellow node here can be interpreted as the probability of reaching a node labeled one.",
                    "label": 0
                },
                {
                    "sent": "From node I.",
                    "label": 0
                },
                {
                    "sent": "Following this random walk transition process, so I started node I, I randomly transition to other unlabeled nodes.",
                    "label": 0
                },
                {
                    "sent": "And sometimes I hit a node labeled zero and I stop.",
                    "label": 0
                },
                {
                    "sent": "But other times when I start from this node I I hit a node labeled one and I stop.",
                    "label": 0
                },
                {
                    "sent": "And the probability of reaching a node labeled one is that function F5 is solved by linear system of equations.",
                    "label": 0
                },
                {
                    "sent": "OK. And again, this gives you an intuition you're thinking about propagating labels on this graph.",
                    "label": 0
                },
                {
                    "sent": "Essentially, through the neighborhood connectivity.",
                    "label": 0
                },
                {
                    "sent": "Questions about that.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Uniqueness of the software solution.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So uniqueness, yeah, yes it's well it's unique in the same because.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Because we relax it to a Gaussian and therefore we now have it's, you know, as a minimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic very convex minimization problem, and the unique solution is given by linear system of equations, which is the mode of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Ah, if this is assuming you want to go back to the discrete Markov random field problem, then you would have to take those F values and discretizes somehow, but I'm not going to take that step.",
                    "label": 0
                },
                {
                    "sent": "I'm going to stop at the F values and the way I'm going to discretize those F values will then depend on for example the relative losses that I have for different class labels.",
                    "label": 0
                },
                {
                    "sent": "Or I can combine those probabilities with other source of information to do my classification.",
                    "label": 0
                },
                {
                    "sent": "So I'm not thinking about this as an approximate way of solving.",
                    "label": 0
                },
                {
                    "sent": "The discrete problem I'm just thinking about it as a reformulation of the original problem.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "To think of as it like logistic regression problem and then try to model the Canonical parameter of exponential family as dysfunction at.",
                    "label": 0
                },
                {
                    "sent": "It would be very interesting to do that.",
                    "label": 0
                },
                {
                    "sent": "I haven't.",
                    "label": 0
                },
                {
                    "sent": "I haven't really thought much along those lines, but in the original paper what we do is we do talk about how we would combine this graph based semi supervised learning with other discriminative learning methods based on exogenous features that we might have of the individual data points.",
                    "label": 0
                },
                {
                    "sent": "So we have some procedures for combining graph based than other discriminative methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the interpretation is a random walk.",
                    "label": 0
                },
                {
                    "sent": "I like it.",
                    "label": 0
                },
                {
                    "sent": "I mean it gives me some intuition.",
                    "label": 0
                },
                {
                    "sent": "I also IRI.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really like this hardware implementation.",
                    "label": 0
                },
                {
                    "sent": "For some reason this this this interpretation as an electric network.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you now have is take a network of resistors.",
                    "label": 0
                },
                {
                    "sent": "Connecting these nodes.",
                    "label": 0
                },
                {
                    "sent": "Where the resistance between I&J is one over WI J.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if if the weight is 0, then there is no wire connecting those pair of nodes.",
                    "label": 0
                },
                {
                    "sent": "This is called a resistive electrical network and now what we're going to do is we're going to clamp the voltage of some of the nodes at one, some subset of nodes at one clamp.",
                    "label": 0
                },
                {
                    "sent": "The voltage of some of the other subset of.",
                    "label": 0
                },
                {
                    "sent": "Nodes at zero in the other class and then measure the voltage at each one of these nodes in the network, and the solution to our harmonic problem is also solution to this electric network problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here is a hardware implementation of the semi supervised learning algorithm which is completely useless of course but.",
                    "label": 0
                },
                {
                    "sent": "I don't know what a is doing here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, alright, so now let's take our labels F and let's think about how we actually classify those labels.",
                    "label": 0
                },
                {
                    "sent": "So one naive thing is just take these function values and threshold am at .5 OK. And what we found in practice is that that leads to often at least two unbalanced classification.",
                    "label": 0
                },
                {
                    "sent": "That doesn't give very good performance honestly.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if you have situations where you know something about the relative frequencies of the different classes, then.",
                    "label": 0
                },
                {
                    "sent": "We can actually.",
                    "label": 0
                },
                {
                    "sent": "Constrain our original problem very simply by adding an additional linear constraint that says my labels on the unlabeled points should conform with the known frequencies of the classes from some test data or something like that.",
                    "label": 0
                },
                {
                    "sent": "And that gets rid.",
                    "label": 0
                },
                {
                    "sent": "This is what we call class mass normalization that gets rid of a site kind of instability of the algorithm in terms of these unbalanced datasets.",
                    "label": 0
                },
                {
                    "sent": "Let me also mention how to generalize this to multiple classes.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the generalization to multiple classes is very straightforward.",
                    "label": 0
                },
                {
                    "sent": "For every class you consider whether you're in that the labeled point is either in that class or not in that class.",
                    "label": 0
                },
                {
                    "sent": "So it's one versus all kind of generalization an you can separately solve all of those linear systems of equations.",
                    "label": 0
                },
                {
                    "sent": "Actually, you want to solve them simultaneously for efficiency, and then what you get is.",
                    "label": 0
                },
                {
                    "sent": "Vector at every node of values that sum to one that actually happens to come from the harmonic property, and you interpret that vector as your class probabilities for your K different classes.",
                    "label": 0
                },
                {
                    "sent": "It's a bit like thinking of the random walk.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpretation, but now each class is a different color and you just ask what's the probability that if I start at node I I end up.",
                    "label": 0
                },
                {
                    "sent": "Hitting this color versus this color versus this color this color OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now how does this actually do?",
                    "label": 0
                },
                {
                    "sent": "I'll show you some examples.",
                    "label": 0
                },
                {
                    "sent": "So I actually think whenever you know, whenever you show up lots of results, it should always be taken anecdotally OK, because the space of possible problems one could try things on is absolutely huge and any paper will only try a small number of those problems, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it like or aggression problem by using custom processing?",
                    "label": 0
                },
                {
                    "sent": "It's a very, very good question, because I actually when I first started thinking about this, I wanted the Gaussian process solution.",
                    "label": 0
                },
                {
                    "sent": "Or a Markov random field solution, or something that was based in, you know, discrete Markov random field solution or something like that.",
                    "label": 0
                },
                {
                    "sent": "This is a Gaussian random field rather than a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that subtle difference is that the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Parametrizes the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And here what we're doing is through our weights W. We're parameterising the inverse covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are important differences between writing down a model in terms of the inverse covariance matrix versus writing down a model in terms of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is worth explaining.",
                    "label": 0
                },
                {
                    "sent": "Actually a model in terms of the covariance matrix that would look almost identical to this.",
                    "label": 0
                },
                {
                    "sent": "I just write down code back to this slide.",
                    "label": 0
                },
                {
                    "sent": "Let's say I wrote down something in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covariance matrix that look almost identical to this.",
                    "label": 0
                },
                {
                    "sent": "It has the property that it's coherent under marginalization of nodes.",
                    "label": 0
                },
                {
                    "sent": "In other words, if I add a new unlabeled point and then I integrate it out, the label of that unlabeled point, it doesn't affect any of my other points.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what we don't want.",
                    "label": 0
                },
                {
                    "sent": "OK, we want the presence of unlabeled points to influence the labeling on the other points.",
                    "label": 0
                },
                {
                    "sent": "So, somewhat perversely, if you actually try to solve this with the Gaussian process, you actually are unable.",
                    "label": 0
                },
                {
                    "sent": "In the simplest form, to get any influence of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "I'll come to that point later.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Correct?",
                    "label": 0
                },
                {
                    "sent": "This problem in the end for.",
                    "label": 0
                },
                {
                    "sent": "We've got the calcium shake.",
                    "label": 0
                },
                {
                    "sent": "Gaussian what sorry.",
                    "label": 0
                },
                {
                    "sent": "Say if you interpret it as.",
                    "label": 0
                },
                {
                    "sent": "Clustering images, right?",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "This is almost, you know, in a sense, almost forget about the Gaussian involved.",
                    "label": 0
                },
                {
                    "sent": "Right, what we have is a procedure where you take any objects you want, images, documents, whatever.",
                    "label": 0
                },
                {
                    "sent": "You form a graph, you say which ones are labeled one and zero, and then for all the other nodes you get values between one and zero.",
                    "label": 0
                },
                {
                    "sent": "That's what this solution here will be.",
                    "label": 0
                },
                {
                    "sent": "Values between one and zero.",
                    "label": 0
                },
                {
                    "sent": "Then you interpret those values as probabilities of belonging to the class one and 0.",
                    "label": 0
                },
                {
                    "sent": "And the distributions of features of those images have.",
                    "label": 0
                },
                {
                    "sent": "Don't have to have anything to do with the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "In particular, maybe I show this example from the very.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beginning.",
                    "label": 0
                },
                {
                    "sent": "The property this will have is that on data that looks like this, the graph that I form will have lots and lots of connections going from this point to this point and so with very high confidence you'll say that that point belongs to the blue positive class.",
                    "label": 0
                },
                {
                    "sent": "OK, but it doesn't assume Gaussian gaussianity in feature space or anything like that.",
                    "label": 0
                },
                {
                    "sent": "Anything?",
                    "label": 0
                },
                {
                    "sent": "The distributions, yeah, you're not modeling the input density at all.",
                    "label": 0
                },
                {
                    "sent": "You take the input data points you form a graph.",
                    "label": 0
                },
                {
                    "sent": "And then you try to label.",
                    "label": 0
                },
                {
                    "sent": "OK, so sorry I'm going forward.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some results, so this is.",
                    "label": 0
                },
                {
                    "sent": "Classifying 10 digits.",
                    "label": 0
                },
                {
                    "sent": "So it's a 10 way classification on this axis.",
                    "label": 0
                },
                {
                    "sent": "What we have is the size of the labeled data set.",
                    "label": 0
                },
                {
                    "sent": "The total data set is 4000.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have, say 20 labeled points.",
                    "label": 0
                },
                {
                    "sent": "What the method that I just described does is?",
                    "label": 0
                },
                {
                    "sent": "It gives you about 70% accuracy in classification with 20 labeled points and 3980 unlabeled points OK. And as you get more and more labeled data, obviously the classification improves and plateaus in some way, and then these different curves are just for comparison.",
                    "label": 0
                },
                {
                    "sent": "Here is what one nearest neighbor would do.",
                    "label": 0
                },
                {
                    "sent": "An here is what the method would do if we didn't do the class balancing, so that actually does quite poorly unless we do the class balancing which is over here.",
                    "label": 0
                },
                {
                    "sent": "OK, and incidentally one nearest neighbors is within the family of nearest neighbors method methods.",
                    "label": 0
                },
                {
                    "sent": "On this data set is the best value of the number of nearest neighbors and is quite close to what most discriminative methods would do.",
                    "label": 0
                },
                {
                    "sent": "With that, ignore the unlabeled data, but let me show you some more examples.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, so here is 20 newsgroups data set where we're trying to classify posts about PC versus Mac.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a classic text data set based on the bag of words representation for each post.",
                    "label": 0
                },
                {
                    "sent": "To this newsgroup we form a graph with 1943 nodes in it.",
                    "label": 0
                },
                {
                    "sent": "On this axis.",
                    "label": 0
                },
                {
                    "sent": "What we have is the size of the labeled data set again.",
                    "label": 0
                },
                {
                    "sent": "In the accuracy on this axis.",
                    "label": 0
                },
                {
                    "sent": "So again with about let's say here 40 labeled documents.",
                    "label": 0
                },
                {
                    "sent": "But you know almost a couple thousand unlabeled ones.",
                    "label": 0
                },
                {
                    "sent": "We're getting over 90% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you did voted Perceptron or SVM has actually very similar performance, in this case purely on the labeled data, you would be around 75% accuracy.",
                    "label": 0
                },
                {
                    "sent": "This should, I mean, this is the basic thing that we want out of these methods.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to show that using the unlabeled data we can do better than methods state of the art methods that just use the labeled data OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is there any?",
                    "label": 0
                },
                {
                    "sent": "Dip at the beginning.",
                    "label": 0
                },
                {
                    "sent": "I think that's just the that's just a finite, very small data set effect.",
                    "label": 0
                },
                {
                    "sent": "Also, the we've shown error bars here, which are absolutely huge, so that can be ignored I think.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So where does the weight matrix come from?",
                    "label": 0
                },
                {
                    "sent": "Can we learn the weight matrix in some way from the data and this has turned out to be extremely difficult, because if we assume we have a very small amount of labeled data and large amounts of unlabeled data, then if we try to learn the weight matrix somehow to do really well on the labeled data that we already have.",
                    "label": 0
                },
                {
                    "sent": "Then almost any method that we try will overfit.",
                    "label": 0
                },
                {
                    "sent": "OK, we're in the regime where we're assuming the amount of labeled data is small.",
                    "label": 0
                },
                {
                    "sent": "So we need kind of a different principle then kind of fit to the labeled data to learn the weight matrix.",
                    "label": 0
                },
                {
                    "sent": "So so just.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To be clear, what do I mean by the weight matrix?",
                    "label": 0
                },
                {
                    "sent": "So imagine for example the weights between two data points I&J are given by E to the minus some distance.",
                    "label": 0
                },
                {
                    "sent": "Between the input features for data Point I and data point J.",
                    "label": 0
                },
                {
                    "sent": "And so this is very similar to the parameters of a kernel that you might be trying to learn from the data.",
                    "label": 0
                },
                {
                    "sent": "So these parameters here in blue could be lens scales that weight different features of your objects in order to come up with a weight matrix between your data points.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if this is clear.",
                    "label": 0
                },
                {
                    "sent": "This is 1 example of learning parameters of weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Here is another simpler example.",
                    "label": 0
                },
                {
                    "sent": "Let's say you use K nearest neighbor unweighted graph.",
                    "label": 1
                },
                {
                    "sent": "So every data point you connect to is K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "What value of K should you use?",
                    "label": 0
                },
                {
                    "sent": "OK. Or if for every data point I connected to all the other data points that are in an epsilon ball, what should the size of that epsilon ball be?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 1
                },
                {
                    "sent": "These are the sorts of parameters what we're calling hyperparameters here of the graph that we want to learn from data.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Do you consider to have a just directed rapport?",
                    "label": 0
                },
                {
                    "sent": "It may, or it in directed graph.",
                    "label": 0
                },
                {
                    "sent": "In our work, we assumed a completely undirected graph, so symmetric waits.",
                    "label": 0
                },
                {
                    "sent": "There has been follow up work that considered generalizing it to directed graphs and you have an analog of the graph Laplacian for directed graphs as well.",
                    "label": 0
                },
                {
                    "sent": "OK so here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is what we're going to do you know what one thing we tried was evidenced maximization or marginal likelihood maximization within a Gaussian process type framework, but that ended up in a tech report because that just did didn't actually work very well right in this context of semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "The method that we described.",
                    "label": 0
                },
                {
                    "sent": "Is based on this funny idea of minimizing the entropy on the unlabeled points.",
                    "label": 1
                },
                {
                    "sent": "OK, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means we we try to find hyperparameters of these weights.",
                    "label": 0
                },
                {
                    "sent": "Not so that they predict the label points well, but somehow so that they.",
                    "label": 0
                },
                {
                    "sent": "Are most confident.",
                    "label": 0
                },
                {
                    "sent": "On the unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "I think with with some care that that can actually do interesting things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is an example in handwritten digit classification.",
                    "label": 0
                },
                {
                    "sent": "In this particular instance, ones versus twos.",
                    "label": 0
                },
                {
                    "sent": "And here is you know the mean image of a one.",
                    "label": 0
                },
                {
                    "sent": "Here's the mean image of a two.",
                    "label": 0
                },
                {
                    "sent": "We start with the weights.",
                    "label": 0
                },
                {
                    "sent": "Now that we're trying to learn our importance of every feature.",
                    "label": 0
                },
                {
                    "sent": "Every pixel, sorry.",
                    "label": 0
                },
                {
                    "sent": "In computing how similar 2 images are.",
                    "label": 0
                },
                {
                    "sent": "So for every pixel we're going to learn a parameter, which is how important is that pixel Ann before learning, all pixels are equally important.",
                    "label": 0
                },
                {
                    "sent": "After learning, we learn some pattern that seems to actually emphasize areas where the ones in the twos are different from each other, and in particular the entropy.",
                    "label": 0
                },
                {
                    "sent": "Goes down after learning on the unlabeled data and the accuracy goes up fairly significantly from 94 to 98%.",
                    "label": 0
                },
                {
                    "sent": "So here again, is anecdotal evidence.",
                    "label": 0
                },
                {
                    "sent": "I would say that this entropy minimization idea or heuristic can be kind of useful for figuring out good hyperparameters of this weight.",
                    "label": 0
                },
                {
                    "sent": "But I say that with great trepidation and caution.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now what I want to show you is a very cool application of semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "I'll do that, then we can take a very short break and then I'll try to finish off, OK?",
                    "label": 0
                },
                {
                    "sent": "So here is an application I wasn't actually involved in, but some of the Jerry Zhu was kind of John Lafferty were involved in this is from a paper called person identification in webcam images.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's what they did.",
                    "label": 0
                },
                {
                    "sent": "This was all done at Carnegie Mellon University and at CMU there was this lounge that was the grad student lounge, the computer science grad student lounge with the Coke machine.",
                    "label": 0
                },
                {
                    "sent": "The Coke machine has a webcam built into it and looks out onto a table, and this webcam is called the free food Cam, so grad students can check this webcam to see if anybody has left.",
                    "label": 0
                },
                {
                    "sent": "Food on this table and then they can run and eat the food OK.",
                    "label": 0
                },
                {
                    "sent": "This is what graduate student life is like.",
                    "label": 0
                },
                {
                    "sent": "As a side effect of this, we have fascinating data that we've collected on people coming into this lounge at different times of day and night, of course.",
                    "label": 0
                },
                {
                    "sent": "And here are just for typical example images.",
                    "label": 0
                },
                {
                    "sent": "Here is Avrim Blum trying to grab some free food.",
                    "label": 0
                },
                {
                    "sent": "He's not a grad student by any means, but he seems to be interested in the free food as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are some typical example images.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do is we're trying to identify the people in the room OK, and the way we're going to do that.",
                    "label": 0
                },
                {
                    "sent": "I say we, although I wasn't involved at all.",
                    "label": 0
                },
                {
                    "sent": "The way they're going to do that, is through a couple of steps, which I'll explain right now.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the camera is always pointing in the same direction, so we can subtract this sort of average background image.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which looks like this.",
                    "label": 0
                },
                {
                    "sent": "And that is going to be ignored.",
                    "label": 0
                },
                {
                    "sent": "You know, it's going to be subtracted out from our images because we care about person identification, not share identification.",
                    "label": 0
                },
                {
                    "sent": "Then here is just an example of some of the data that they have.",
                    "label": 0
                },
                {
                    "sent": "They have.",
                    "label": 0
                },
                {
                    "sent": "They have 10 people that they're trying to identify.",
                    "label": 0
                },
                {
                    "sent": "And these people come into the room on different days over some period of time and in total there are about 5000 images that are obtained from video sequences of people coming into the room.",
                    "label": 0
                },
                {
                    "sent": "Whatever grabbing food and leaving.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing about this data is that it's quite varied, right?",
                    "label": 0
                },
                {
                    "sent": "This is the typical thing in vision.",
                    "label": 0
                },
                {
                    "sent": "The illumination obviously varies, but also people wear hopefully different clothes on different days.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The data was preprocessed to do a foreground extraction, so this is John Lafferty and that's Jerry Zhu.",
                    "label": 1
                },
                {
                    "sent": "An then from these foreground images you could also do face detection and just extract the face images.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to connect images to other images in a graph.",
                    "label": 0
                },
                {
                    "sent": "Alright, the way we're going to do that is we're going to have several different kinds of neighborhood relationships between images.",
                    "label": 0
                },
                {
                    "sent": "So one type of neighborhood is a time edge, so this is 1 example image.",
                    "label": 1
                },
                {
                    "sent": "This is Nina Balcan, the first author of this paper, Ann.",
                    "label": 0
                },
                {
                    "sent": "This is a neighbor of image 2910 and it's connected to that Biotime edge.",
                    "label": 1
                },
                {
                    "sent": "Because this webcam image was taken very near in time to this webcam image.",
                    "label": 1
                },
                {
                    "sent": "This webcam image is a neighbor of 2910 through a color edge.",
                    "label": 0
                },
                {
                    "sent": "Because the foreground that was extracted here Nina's shirt has the same color as the foreground here, so through some color feature extraction you form edges between images that have the same foreground colors.",
                    "label": 1
                },
                {
                    "sent": "This is another image that's a neighbor of this through color edge, another color edge, and now this image.",
                    "label": 1
                },
                {
                    "sent": "Where she's wearing different clothes is actually a neighbor of 2910 through a face edge, so that's a different kind of relationship that says these two faces are similar enough to put an edge between them.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have time edges, color edges and face edges.",
                    "label": 0
                },
                {
                    "sent": "We have 5000 images which are 5000 nodes on a graph.",
                    "label": 0
                },
                {
                    "sent": "We connect them together.",
                    "label": 0
                },
                {
                    "sent": "And then we can consider what happens if you walk around on this graph, starting from an unlabeled image to see where you end up with the labeled image.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, this last image is labeled Jerry.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This image here, which is actually a very difficult image to identify, right?",
                    "label": 0
                },
                {
                    "sent": "It's Jerry's back.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This image here is connected to this image through a color edge.",
                    "label": 0
                },
                {
                    "sent": "This image is connected to this image through a time edge.",
                    "label": 0
                },
                {
                    "sent": "These happen nearby in time.",
                    "label": 0
                },
                {
                    "sent": "This image is connected to this image through a face edge and this image is connected to this image through a color edge.",
                    "label": 0
                },
                {
                    "sent": "OK, now we know that this image has the label Jerry and this sort of label propagation idea, or the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "When you solve for the harmonic function where it will give you is a high probability that that's an image of Jerry.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the basic intuition.",
                    "label": 0
                },
                {
                    "sent": "You can turn that into plots in an experiment.",
                    "label": 0
                },
                {
                    "sent": "So here we have the harmonic function accuracy as a function of number of labeled images.",
                    "label": 0
                },
                {
                    "sent": "We have 10 people.",
                    "label": 0
                },
                {
                    "sent": "20 labeled images means 2 images per person.",
                    "label": 0
                },
                {
                    "sent": "OK, that's pretty tough.",
                    "label": 0
                },
                {
                    "sent": "I think if you try to do this naively, you can get 70% accuracy with two images per person.",
                    "label": 0
                },
                {
                    "sent": "It only goes up to about 80% with.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "More images like.",
                    "label": 0
                },
                {
                    "sent": "20 images per person question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Ye good question.",
                    "label": 0
                },
                {
                    "sent": "The weights on the different kinds of edges are.",
                    "label": 0
                },
                {
                    "sent": "In this case, I'm not sure whether they learn them or they set them by hand.",
                    "label": 0
                },
                {
                    "sent": "But they could have tried to learn the weights on the different kinds of edges through this entropy minimization framework.",
                    "label": 0
                },
                {
                    "sent": "I would have to look at the paper to see what exactly they did.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yes.",
                    "label": 0
                },
                {
                    "sent": "How do we combine combine these three different metrics for the?",
                    "label": 0
                },
                {
                    "sent": "So you can't actually different graphs and then pointer.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "I believe what they did, but I would have to check in more detail is they?",
                    "label": 0
                },
                {
                    "sent": "Constructed three different graphs and then they combine them into one graph.",
                    "label": 0
                },
                {
                    "sent": "The simplest form would be just put an edge if one of these or one of these or one of these is close enough, but a more sophisticated and this is also related to the previous question, more sophisticated thing to do would be to actually learn the relative importance or weights of these three kinds of similarity.",
                    "label": 0
                },
                {
                    "sent": "I think that would be, and I think they did some of that as well.",
                    "label": 0
                },
                {
                    "sent": "Questions, Yep.",
                    "label": 0
                },
                {
                    "sent": "OK cool, I have some other I think interesting stuff to talk about, but I want you guys to have a chance to get up and take a break.",
                    "label": 0
                },
                {
                    "sent": "So why don't you take a very short 5 minute break and then I'll talk about computation and active learning.",
                    "label": 0
                }
            ]
        }
    }
}