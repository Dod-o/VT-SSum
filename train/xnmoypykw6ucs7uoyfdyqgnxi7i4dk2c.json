{
    "id": "xnmoypykw6ucs7uoyfdyqgnxi7i4dk2c",
    "title": "Large Scale Learning Which Is Actually Useful",
    "info": {
        "author": [
            "Ronan Collobert, NEC Laboratories America, Inc."
        ],
        "published": "Sept. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_collobert_lsl/",
    "segmentation": [
        [
            "Always I will speak about it.",
            "My computer doesn't crash again.",
            "Um?",
            "Yes, so I've been asked through.",
            "To present you a very very large killer application of machine learning.",
            "And that's what I'm going to do.",
            "Whoops and that.",
            "Yeah, basically what I wanted to do here is.",
            "Like presents you what we started to do at NEC Labs two years ago we started to this to work on the natural language processing and we are company so we need to produce something.",
            "And as you know in natural language processing is really large scale.",
            "So it's really a real problem and it might be useful because we have to do some money with it.",
            "Anne, this is a.",
            "This is joint work with all my team.",
            "Oh, actually you cannot say sorry.",
            "I said you could see.",
            "Who?",
            "Are we OK?",
            "Kissing.",
            "Haha.",
            "What do I do in this case, I reboot.",
            "Compared to the wild track.",
            "Decision trees.",
            "OK, so as I was saying it's about large scale or natural language processing and this is joint work with all my colleagues, Jason Weston being by by very quick Skull and Leon Bottou."
        ],
        [
            "So yeah, two years ago with Jason, we thought all we are a bit bored about cure machine learning theory.",
            "We were working mainly on support vector machine.",
            "So we wanted to do something real for the company and we thought, well, it could be nice.",
            "Like in a few years we could have a conversation with our computer.",
            "Of course, if you say that to your boss is going to laugh at you and probably not give you a lot of."
        ],
        [
            "So instead we said to him, while we would like to do some opinion sentiment analysis like some automatic business profile from the text and some semantic search like some question answering on course I mean which is actually related to call centers.",
            "Also, like your minutes without her, but some machine translation system.",
            "So all these tasks are basically related to text and they are complex task and it's our very large scale."
        ],
        [
            "So well, basically, if we want to deal with text, there is already a community trying to deal with that.",
            "It's called natural language processing, so we had a look at what they were doing and.",
            "And we because we know that we have this kind of tasks that are producing, we could obtain like very good features for task and maybe do what we what we want so.",
            "What we saw in the natural language processing is that there is like interesting task like part of speech tagging, which is about trying to find if a word is a noun, a verb or noun, and so on.",
            "There's like chunking, which is about trying to find syntactic constituent in the sentence, like noun phrases, verb phrases, and so on.",
            "There is name, entity, organization which is about trying to find if world is like a name.",
            "Or for personal for Co location name or something else.",
            "And then there is also like a very interesting one which is called semantic or labeling which is quite hard but which tries to attribute like semantic words towards in a sentence.",
            "So for example if I have like Jaune ADR point in the garden, I first consider a very innocent tense like 8 and I try to to know who is the actor on this world.",
            "So John, on what it's acting on like the Apple when where?",
            "Canton so.",
            "If actually you saw my talk yesterday, you might get bored because a lot of slides of my talk from yesterday.",
            "All inside here.",
            "But yeah, I mean these are the natural language processing task we we were at first interest in.",
            "For example, if you want to do some automatic your business profile of companies, well what you can do is first use your good name, entity extraction algorithm, extract names of companies and then try to use semantic calling to see interactions between companies.",
            "So you really can obtain good features with this kind of task.",
            "So we're talking about large scale learning in this workshop.",
            "So actually our large scale all those tasks."
        ],
        [
            "Well.",
            "If we consider only label data, there are quite large scale already there, like about 1 million of words.",
            "If we consider a good data set like a Wall Street Journal.",
            "But if we consider on the unlabeled data or basically have the web, it's almost infinite.",
            "So I mean do we have like a machine machine learning algorithm which are able to handle this kind of data?",
            "Well, you know we were doing support vector machines since quite a while.",
            "You know in our group and actually we are also interested in in particular in support vector machine in a large scale."
        ],
        [
            "So yeah, we probably had something in hand.",
            "And then if we look at support Vector Machine which could handle 1 million of label example.",
            "Well, if I if I look on the linear as well like so much inside you know that?",
            "Limbo to her remind us like last year, I think that's a good order.",
            "Stochastic gradient is strictly applied to linear support vector machine works really well on really fast.",
            "But we also had in the domain like as in performed Austin, Pegasos, Liblinear, and and many others?",
            "And as we will see in this workshop, or maybe you saw it in reading the results, I don't know.",
            "It's it's, I mean you know also public domain really can handle large scale data and it works really, really well.",
            "Under nonlinear support vector machine side where it's less bright, I would say if I look at the workshop results, actually I saw I think only one guy who had the courage to try a normally are support vector machine on their data and I'm really happy about it.",
            "And yeah, so we also propose a in our group like some large scale support vector machine for only a support vector machine.",
            "I show you propose something online called lyzeum.",
            "And the biggest data set we ever trained it on was like on the 8 million of examples.",
            "It was actually like a modification of the MD's data set which is digit data set on.",
            "We extended it by adding like tweaks on the data.",
            "I mean like rotations, distortions and things like that.",
            "It took eight days training at a well, so it seems feasible.",
            "But drawback is even after this, eight days of training, well, we obtain like.",
            "150 oh sorry, 150,000 super vectors so well we can train it, but we cannot use it if we want to be large scale.",
            "So because of course as you know the support vector machine speed during testing phase depends on the number of public tools and we have.",
            "When you have like such a number of support vectors, you can basically forget it.",
            "So.",
            "And actually even worse, this task was a non noisy one.",
            "You get like a very very lower accuracy, like something like 0.6 so.",
            "For the noisy one we had like kind of hope because we proposal also like some non convex support vector machine which were able like to remove all these annoying over at least part of this ongoing support vectors which are bound.",
            "But still we never find out a way to combine this online algorithm with this non convex one never workout.",
            "So in the end, even if we can train only our support vector machine on large scale data well at during the testing phase, it's a bit of a problem."
        ],
        [
            "On the Super vector machine with like able to handle unlabeled examples, I mean it's a bit even worse, I mean most transductive support vector, machine algorithm which have been proposed, basically working only on three datasets and I resign in here only like 2 algorithm.",
            "One from I don't know how to put on Cinder seen Winnie and Katie.",
            "It's called SVM, Lin, and as the biggest data set I saw the trended on was like five millions of unlabeled examples, which is really good, and it's they obtain a pretty impressive timing results as well, which was 15 minutes.",
            "But on the nonlinear side, again.",
            "It's not done right, and the biggest super vector machine I saw was actually one we train a few years ago.",
            "It's what's called like CCP CCP, transductive support vector machine.",
            "It was using like a concave convex approach algorithm and we trained it on like 60,000 unlabeled examples in like 42 hours for like 2 days.",
            "Unfortunately, if we want to handle like infinite number of data, well it could be good.",
            "If we had like something online and once again we never found out how we could perform this one in an online manual.",
            "So.",
            "Yeah, I mean."
        ],
        [
            "As I've been asked to do a talk, I thought it would be nice if I had like sometimes some controversial slides.",
            "Just to add a little spice to this workshop.",
            "So don't be upset, just take it as spices.",
            "Yeah, I mean.",
            "I I think then when you talk about large scale, it means you're you want to handle a complex task.",
            "So because if you have a lot of data reporting means your your task is complicated.",
            "So if your task is complicated, you need probably a very complicated model as well.",
            "Anne."
        ],
        [
            "If you have this excuse of not using all the label, I mean all the examples over label.",
            "It probably means I mean under the pretext that your your algorithm is not like improving in generalization when you add examples, it probably means that your your model is probably under fitting.",
            "And I saw in the workshop some some people who had like this.",
            "I mean what I looked like the curve, like the timing with respect to numbers of example.",
            "I so like some flat curve at the end as if they decided to let stop after 100,000 training examples because my right is not improving anyways.",
            "Well, it's just means that your model is underfitting, so it's probably garbage.",
            "And actually when I look at it.",
            "The nonlinear support vector machine, for example, are doing much better than the linear one on the data set Alpha, so even if it takes maybe I don't remember our text to train it on Alpha, maybe a few hours or one hour.",
            "So I prefer to wait one hour or never very good accuracy.",
            "If I have a good application to do, then to wait like few seconds and having a bad accuracy."
        ],
        [
            "So in the end we have like 2 extreme choices to obtain a complex system.",
            "And both are interesting.",
            "On one case you you can like.",
            "Take a good linear support vector machine algorithm and we we show.",
            "I mean you will see in this workshop that it's indeed possible to have one which which scales really well, and then with this linear algorithm, which is actually very simple, you can design some very complex features you have to do it by yourself, but you can do it.",
            "On the other's outside, it's also very extreme.",
            "You can say, oh, I'm going to design a very complex algorithm, and I hope that this very complex algorithm is going to train implicitly all these good features for my task.",
            "Well, I think both approaches are bit extreme and poorly.",
            "The best is something in the middle.",
            "For example, if you have like some good a Prairie about your task, it would be too bad not to use to use it in your task, but I think limited all serve to linear support.",
            "Vector Machine is a bit too much.",
            "We should find something a bit more complex."
        ],
        [
            "So.",
            "We looked at what the natural language community was doing.",
            "And basically they were more on the extreme side of using a linear or support vector machine an using a lot of complex features.",
            "So what they do is that while they take a sentence.",
            "They they build their homemade like a lot of complex features.",
            "I mean, some are less complex, but so not complex.",
            "They they mix them together and they feed it to some good classifier like.",
            "Super Vector machine that we call like shallow here because it's quite simple.",
            "And it works actually pretty well.",
            "Well in a case like semantical labeling, which is a bit."
        ],
        [
            "Harder task they they actually cascade features so they for example compute part of speech, then with part of speech you can obtain like parse tree.",
            "You can build a bathroom tubs over that.",
            "Then from the past three you can extract some unbilled features and then from this homemade features where you can again like feed them to classify or like support vector machine.",
            "So it does work very well, but it's extremely slow.",
            "The reasons the main bottleneck here is actually the construction of the power stream, which is really time consuming.",
            "So basically we looked at the community and we saw, well, we cannot choose it for our application.",
            "I mean we need to handle a lot of data.",
            "We can really not use it during the testing Phase I mean, maybe we can train it, maybe it works.",
            "Well.",
            "Actually it does work well, but we cannot use it.",
            "For what we are interested in, so we have to find something else."
        ],
        [
            "So instead, well, we said let's try to do a complete end to end system.",
            "We will give text it out, put tags on this system is going to be a good old neural network so.",
            "It's a bit scary at first time, but what we what I'm going to present here and that's what I presented yesterday, it's.",
            "It's an empty end to end system when you like.",
            "Input text at the beginning just trade sentence straight walls.",
            "You, I mean basically, this system is going to embed words into, like some embedding space, some feature vector space.",
            "This feature vector are going to be combined and.",
            "Right?",
            "Like some local features in an automatic way, this local features are also going to be combined to obtain some global features an from this global features we obtain our tags.",
            "Everything is trained in in basically backpropagation, and it works really well as we will see.",
            "So we propose something which is a.",
            "A deep architecture we say, but actually it's just another network and it's like a unification of all the natural language processing task which involves tagging.",
            "The fact that we can unify all this natural language."
        ],
        [
            "Using task.",
            "Basically Oroya allow us to to perform joint training.",
            "In fact it's well known in the natural language community that all these tasks that I showed you at the beginning like part of speech ranking and so.",
            "Or this task are kind of correlated, so you know that if you have like good features 1141, it might be useful for the other one.",
            "So if you train like part of the network.",
            "I mean, if you if you if you're trying everything at the same time when you share part of the network, you basically share part of the features and you can hope that the training is going to be.",
            "I mean the testing performance are going to be better.",
            "So that's that.",
            "I will show here as well."
        ],
        [
            "Yeah, so it's not our networks and I've been told like few days ago actually that know how networks have been absolutely 20 years ago, so why should we use them?",
            "I've been quite the office created by this sentence.",
            "Because I mean, I think we should not take like so hard position like that.",
            "I mean, you know Super Vector machine have good sites nor networks are good size as well and they're actually pretty much.",
            "Would you say young?",
            "Complementary.",
            "So.",
            "So.",
            "Viable if I take like a, you know the stochastic gradient applied on the support vector machine, where it's just a good old stochastic gradient descent applied to margin perceptron with like away decades, nothing else.",
            "And also recently in our group we were interested, you know, in trying to go large scale with, for example, unlabeled examples.",
            "So what we did is take the support vector machine idea of like transaction and apply it to networks and it actually works really well.",
            "It gives very good performance.",
            "Is it scale extremely well?",
            "And why not using it?",
            "So well, before saying that no networks are really bad, let's see what they can do."
        ],
        [
            "So what we propose here.",
            "Is like Azaceta end to end system where we input text and we output tags well.",
            "Of course like computers cannot under text they have like to deal with numbers.",
            "So the first thing you have.",
            "Do you have to do it?",
            "It's like transform text into walls.",
            "So well in general, like when you have a tagging task, what you can do when you want to tag a world like this world set, it's like take a window around this world.",
            "So maybe my sentence with the cat sat on the mat and I took a window around the world set.",
            "These walls there are just indices for the computer.",
            "It's like an index in a dictionary, right?",
            "Well, this index I'm just going to map it with a lookup table to a feature vector in some particular.",
            "Space I have to choose the size of the space.",
            "So it's just like a look up table I ever want and I have my look up table.",
            "I just assign a feature vector on this feature vector is going to be trained by backpropagation.",
            "You can see it as like a very particular like matrix vector operation.",
            "If you consider you want as an index, well, you can replace them.",
            "This index as a big vector of 0 or the size of the dictionary with one bit in the middle at one which is at the position of the wording that you try.",
            "And you are going to multiply like a big matrix by this big matrix by this big vector of zero, except at one place.",
            "So it's exactly like.",
            "Taking one column of this matrix basically so it's really a lookup table.",
            "So you have once you move them to a feature vector space and then for each of those walls you.",
            "I mean you you have these vectors, you concatenate them and you can feed them to classical networks layer.",
            "It works really well for tasks like part of speech tagging or for chunking or even from name on TT organization.",
            "But when you have a task like semantic hauling it doesn't work anymore.",
            "Why it's because semantical labeling is like about labeling award?",
            "With respect to affirm that you chose before hand and the verb might be completely outside of the window here.",
            "So.",
            "You have to find a way to specify to the network the worm, and you have to find a way to handle the old sentence at the same time basically.",
            "So there is a generalization of this approach, which is called a convolutional neural."
        ],
        [
            "Networks as soon as you have to deal with sequences, think about convolutional neural networks.",
            "There are really well in case.",
            "I mean, for now it works at least.",
            "So it's almost the same approach.",
            "Again, you have like these walls, you map them to like some feature space at the same time.",
            "You have to indicate the position of the world.",
            "With respect I mean that you want to label so.",
            "For that we we give us input features like the distance of each one with respect to the world we want to label.",
            "We also give in the input the position of the each one with respect to the verb we chose before hand and with respect to which one we want to label whatever.",
            "So for each of this one, let's say features we obtain like.",
            "A feature vector by just applying a lookup table.",
            "Once again, you choose like a window size any but this time you are going to consider all possible Windows in this in this."
        ],
        [
            "Sentence so actually I have slides for that.",
            "So for example, if the window if outside three, I have this feature vectors.",
            "Well, I consider first this window of size 3.",
            "I apply my my matrixectomy depiction that I concatenate.",
            "I concatenate them, apply your matrix vector or multiplication.",
            "I open your feature vector.",
            "Then I shift I I take the next three once and I'm going to do the same again.",
            "I obtain a new feature writers and so on.",
            "This is basically just doing a convolution.",
            "It's I mean you can see it like I like sharing weights with time basically, but it's just a generalization of this window approach.",
            "Condition has been used with success in image along time ago and also in speech.",
            "So they do work really well.",
            "Anne.",
            "As we will see here, they also work with very well are intact."
        ],
        [
            "So yeah, you have this feature like dogs, you apply your convolution and basically you end up with a number of feature vectors which are like.",
            "Which can be seen like some local feature exhaust for around each one in the sentence.",
            "And still this number of local feature vectors.",
            "Is dependent on the number of words and sentence, so you have to get rid of this like time dimension.",
            "So what we do here is like take a Max overtime to try to force the network.",
            "To find the best feature for the world of I mean for labeling the world of interest."
        ],
        [
            "And if I look."
        ],
        [
            "One example.",
            "Like this very long sentence yesterday after Microsoft blah blah blah.",
            "The awarding ready, I mean the writing, already here is of Elmer I chose to.",
            "Before hand and the wording green here yesterday is like the world I chose to label.",
            "And this is our presentation of like the local features along time.",
            "And.",
            "In a red is basically which feature has been chosen.",
            "Which one was a maximum water, so which which has been chosen by the network and you can see that intensely interesting laser is always like 2 cluster one around the wall of interest and one around the verbal filters.",
            "And if I move the world of interest at the end while the cluster move."
        ],
        [
            "Basically.",
            "So.",
            "Once you you've done this Max overtime, you obtain like a fixed size.",
            "Feature vector which represents I mean which like is supposed to be good features for the task you're interested in, and for the sentence you presented.",
            "You can then feed this this feature vector to again classical layers.",
            "And you train the whole thing with like stochastic gradient descent and backpropagation."
        ],
        [
            "So.",
            "As I said, I mean this this architecture is really general and it's so general that we can apply it to any kind of natural language task which involved which involves tagging.",
            "So.",
            "That's what we tried him.",
            "We tried to train everything at the same time, ensuring like this, what we could look up tables.",
            "So this world feature weights.",
            "An and again train everything at the same time.",
            "Back propagation just sharing this this look up table and we will see that it does work very well.",
            "Yep.",
            "It it is, it is.",
            "Yeah, if if we will not then it would not work well.",
            "So you know you you really back back propagates."
        ],
        [
            "Up to the end here I mean up to the top, you really back propagate everything, so you update as well.",
            "That I mean, as I said, you can see this layer as a very particular layer.",
            "It's just a matrix vector multiplication as other layers in our network.",
            "But it's it's presented in the optimal, I mean in efficient way.",
            "Basically you can see it just as a look up table, but still you optimize it so."
        ],
        [
            "Yeah so.",
            "Yeah, millimeters cloning actually, which is about like your training things at the same time on sharing part of the network is absolutely absolutely are not new and it has been used quite a bit.",
            "Actually in a machine learning and you can find a very good overview in like a rich account, one basis if you want."
        ],
        [
            "So, Interestingly, in the case of natural language processing, 1,000,000 of what is actually not that large scale, unfortunately, and the reason is.",
            "That's well, I mean, if you consider all West regional, there's like about 36,000 of words in the dictionary and there is only 1 million of words.",
            "You know when I do a word count on the on the file.",
            "So.",
            "In fact, 15 of the most frequent walls, and that's I mean, I mean.",
            "How do you say come on problem in text 15 of the most frequent warnings that you join are seen only like 90% of the RC Nigeria 90% of the time.",
            "So many words are really well and are going to be seen like once or twice in the in the in the text.",
            "So if this was a rare well we are not going to train it, train them properly.",
            "So you have to find a way to deal with that.",
            "So what we first did and and that's a publication we we presented actually at the ACL conference last year.",
            "We did some world clustering according to Battle speech, but that's you know a bit bigger query.",
            "I mean why part of speech?",
            "Is there any other way to cluster?",
            "I don't know.",
            "So we didn't really like this approach, even if it was working OK.",
            "So then we sort all.",
            "Let's try to threshold the number of words in the dictionary so we all the other ones according to their frequency and we just cut back at some threshold we chose beforehand on that we have to optimize.",
            "Um?",
            "And all the words above the threshold, I'll just mapped to a particular special one like unknown.",
            "So why not?",
            "It does Walker tree?",
            "But it's still not sufficient.",
            "So."
        ],
        [
            "We can do better.",
            "And.",
            "When you think about it, if you have like a sentence like the cat sat on the mat, which actually might not be in Wall Street Journal because it's about cats.",
            "So I don't think they talk about cats in Wall Street Journal.",
            "But if you had that in the words visual cat is a very common one.",
            "Very common word.",
            "So this this sentence is probably going to be tagged properly anytime.",
            "But then if you consider the sentence, the fair is a feline sat on the mat.",
            "Feline is actually a very rare world, and even if the sentence looks the same, the network is pulling too, probably going to do something bad on this sentence.",
            "So in fact, what you want here, it's.",
            "Something which enforce the embedding vector of like 2 walls which are semantically related to be close to each other.",
            "So one thing we saw her first is all let's try to use Walnut because one it is a radio.",
            "Good label data set.",
            "So as I think like about 100,000 words.",
            "And it tells you about like semantic relations of walls so.",
            "Let's take Walnut and we if we consider like 2 words in Walnut.",
            "If they are linked in one at it.",
            "So if they have a semantical relationship, let's force the embedding to be close to each other, and if they are not, let's presume apart.",
            "So this can be viewed as an additional task that we can apply in this."
        ],
        [
            "Hello in this multi task learning way and it can basically regularize pretty well this lookup table.",
            "So."
        ],
        [
            "We tried that.",
            "It works pretty well, but we can do even better.",
            "We can use, well, you know, all the web."
        ],
        [
            "So if you have this, if you have the model like an algorithm.",
            "Which which could answer the question.",
            "If a sentence is actually good English or not, well, it would mean that your model underneath understand really well what is syntax of English.",
            "What is grammar?",
            "What is semantic?",
            "So that's what we try to do here.",
            "We decided or let's try to add an additional task which would understand implicitly this kind of things.",
            "So we designed the language model an.",
            "We we took I mean.",
            "The web is really large scale, so we limit ourselves because we we aren't like limited in time and we limited ourselves like to Wikipedia which is already 600.",
            "So to you, 1 million of worlds.",
            "After a few tweaks.",
            "And we decided to consider the task of flag discriminating English and English, so I mean.",
            "Wikipedia windows of text from Wikipedia windows of text, while the middle one has been replaced by some random world.",
            "So.",
            "We trained this task in actually ranking cost.",
            "I mean using a ranking cost, ranking cost and all network basically was out putting a score for window of text and we want the score for a true window of text to be larger with a margin than all other score for all other window of text wells and neither one has been replaced by something random.",
            "Once again, we train that with stochastic gradient backpropagation.",
            "When you can be sure that it takes weeks days.",
            "Add a. Yeah, actually before I continue.",
            "I mean I should mention that this is not completely new.",
            "Um?",
            "Yeah, you should.",
            "Banjo dealer already like some some language model like that in the past with no networks as well.",
            "It also, I mean they are also pretty good results.",
            "But yeah, yeah, it rained it only on one million of walls I think.",
            "Or maybe no, maybe 10 millions, isn't it?",
            "So I'm seeing between one and 10, I don't know.",
            "Some of the words just random words, sorry.",
            "When we train, we take everything stochastic, so we just like stochastic.",
            "Sorry, what does it take so long?",
            "Because it's really huge.",
            "That's all.",
            "How many parameters?",
            "I don't know exactly, but I consider a Dictionary of like 30,000 and 100,000 actually and times the size of the world vector and it's about 50 in general.",
            "And and then you add you have to add like this.",
            "You know hidden layers parameters, but that's maybe peanuts compared to this big lookup tables so.",
            "Do you have like 1.6 million parameters?",
            "You have like your.",
            "Let's say you're 50 * 30,000.",
            "It is quite location but but you know they are not used.",
            "Sorry yeah but they are not user.",
            "At each sentence, you know because you select only power matters which yeah exactly so it's hard to say actually what capacity it is.",
            "But steel is pretty huge.",
            "Sorry.",
            "Wait, it's this?",
            "This is really slow.",
            "I mean try it.",
            "Select character sizes yeah.",
            "What I mean, don't forget that at the beginning.",
            "Yeah, maybe I forgot to say that actually, you know we take a window of text again here and there were the size of the window is 11 one yeah, but no eleven wants yet.",
            "So you have 11 wants times 50 at the input, so it's quite big.",
            "And then I mean I can tell you actually the architecture I showed."
        ],
        [
            "Yeah so.",
            "Yeah, the ones each one about 50.",
            "The window 11 walls.",
            "Then like query two layers one maybe one onwards user 200 something at so you know it's it's pretty big.",
            "How many parameters for personal example?",
            "How many parameters are active?",
            "So all the power matters here, so it's like 11 * 50.",
            "Plus so I mean then, plus 11 * 50 times the number of hidden units which are like I don't remember 100 or 200.",
            "Plus 100 or two.",
            "So it's pretty big."
        ],
        [
            "Yeah.",
            "So."
        ],
        [
            "So Interestingly, I was very lazy at the beginning.",
            "And you know, I have Wikipedia.",
            "I just decided to let stupidly read Wikipedia with my network on training like that on the fly.",
            "It's online, right?",
            "Why you should actually shuffle the data.",
            "I had some Interestingly spiking the test and I was wondering what's happening here and it took me awhile actually to find out that in Wikipedia you have like a lot of data or related to cities and they say oh this city is about I don't know how many million of inhabitants you know and it's always the same sentence like Switch follow.",
            "You have like millions of cities.",
            "So basically each time you go over the cities they have this spike.",
            "Also takes data so so shuffling the data is a good thing, and it might be actually quite tricky when you have a large data set."
        ],
        [
            "Also, if you do it like straight like that, it's going to take you a very, very long time to train.",
            "And I mean it's going to work, but I mean it's going to be just really, really, really long.",
            "So what I did instead is like I thought, all let's train my normal network lights like if I was training a child, you know.",
            "A first consider a simple problem, so I just consider a small dictionary size.",
            "So dictionary size were ordered first words.",
            "You know I took the most frequent words and I trained my networks just resource for us and then I increase the now network.",
            "I mean the showing size and also increased only once the size of Sir input window."
        ],
        [
            "So.",
            "After like three weeks of praying and unlearning, I actually obtain her impressive embedding.",
            "Um, 05 minutes well.",
            "OK, so.",
            "OK Novia, impressive embedding and here it's like I pick up like a random word in the dictionary.",
            "And the number of underneath is like the ranking of the world in frequency.",
            "So a smaller number means very frequently large number of Israel.",
            "And this was like the 10 closest words according to the equation distance in the embedding space.",
            "And you can see when it learns a lot of semantic even for."
        ],
        [
            "Firewalls.",
            "Uh.",
            "So yeah, as I said at the beginning, we were interested in a multitask learning an we aren't easy or so in the semantical labeling a lot.",
            "So we first try to train our semantical labeling task alone.",
            "So this is a plot of the testing error with respect to the number of epochs just to show that it converges, converges quite quickly an.",
            "This is for two different embedding size, so 15 and 100.",
            "The behavior is pretty much the same, but it's more clear here.",
            "So on the top here you have like this semantical labeling task alone.",
            "All this like a bunch of curves like semantic whole labeling task, train with like drinking bottle, speech mix of drinking or parts of speech or name, entity organization and things like that.",
            "Then you gain quite a bit if you use like a Walnut.",
            "An you gain a lot if you use everything at the same time or just the language model with semantic Halloween.",
            "We actually obtain like state of the art performance, you want to be better and it's extremely fast during the testing phase."
        ],
        [
            "Considering the overall task is also improves in Chongqing, basically we have the the state of the art in part of speech when you use multitask learning.",
            "It doesn't improve much compared to the task alone, but it's not surprising because part of speech is a very easy task.",
            "Actually, in Chongqing we can even beat the state of the art if we add a bit of prior knowledge in the features.",
            "So if we just add part of speech features we like far away for what they have."
        ],
        [
            "So as I said also at the beginning I'm on, I am in a company so I'm interested in doing like you know, money for my company so.",
            "We we actually on this system, which is like extremely fast.",
            "We can obtain for free.",
            "Basically, a semantic search system, I mean almost for free.",
            "You have like all this stuff around, but in the middle we we build like this similarity ranking measure between a query and text sentence.",
            "Just by, you know using like the symbol ticker or labeling tags.",
            "And tweaking it."
        ],
        [
            "Iranda it works extremely well when the queries are about.",
            "About in Windsor.",
            "Quail semantic compared to other."
        ],
        [
            "Which is.",
            "And I don't know you can't see much like I'm afraid.",
            "This was an example about who bought compact, so with our system to show that if you add a bit of semantic here really gain a lot compared to classic."
        ],
        [
            "Call a system like Google.",
            "Which if you try it actually works pretty badly on this one.",
            "The 1st first answer is not even.",
            "I mean.",
            "Basically I it won't Google just walked by keywords or maybe some other idealistic.",
            "I don't know, but if you try you will see that the first sentence is about compact acquires digital blah blah blah.",
            "So I mean it's not about who both."
        ],
        [
            "So to summarize, I mean I showed you here like.",
            "A big non network architecture which can be applied for natural language processing task which involves tagging.",
            "It has kind of a lot of advantages.",
            "It's very general.",
            "You can obtain state of the art performance without designing any handmade features.",
            "You can perform joint training to obtain a very good performance using like massive label data and it's extremely fast during testing phase which order us to like apply it directly on large scale application like Semantic Search.",
            "It does some inconvenience and sonar network and no network.",
            "Our powerful tool so it's kind of hard to handle, but.",
            "It does already like some early impacts.",
            "I mean you can easily apply it, you know to some other languages like Japanese or other task.",
            "And as I said, yeah we developed this."
        ],
        [
            "So system, so I mean to conclude, I would say that's support vector machine are very good algorithm for studies.",
            "But you should not limit yourself to that, and because very large scale complex task in general require more ambitious models.",
            "Questions.",
            "In my head, I think you doing about 10 to 14 parameter updates menu training process.",
            "About 10 to 14 parameter updates.",
            "Yes, maybe I checked it at some point.",
            "OK, it's it's quite true.",
            "Yeah.",
            "Other problems that could be converted to.",
            "Good timing, right?",
            "Well, actually, I mean for us.",
            "Basically, it's almost like.",
            "Problem solved for that.",
            "I mean we we tried many task we have still few interns working on that to like clean up our experiments on updating like state of the art performance.",
            "But we're almost done and then we want to move on like to more complex tasks like.",
            "Questioning question, answering system or machine translation because.",
            "This is more challenging.",
            "Question.",
            "Training binary classifier or some structured classifier if you will find the specs.",
            "Yeah, so actually here.",
            "I mean it's multi class classifier.",
            "I mean for example part of speech is like about 50 classes or 45 material.",
            "So it's all multiclass, but it's true.",
            "Not too far for given work."
        ],
        [
            "Want to know what is the label or you want to also detect?",
            "Which would be some?",
            "No, you want to you want to put a label on each in each world.",
            "What we didn't do here is apply for example of.",
            "It'll be at the end because we could.",
            "Actually we have some grammar you know on the labels, so we could even.",
            "Improve the performance by applying some dynamic algorithm at the element we didn't yet.",
            "I have another question.",
            "If I remember well, when you use this first approach about nothing, the words yeah.",
            "Yeah.",
            "Which is the fact that the.",
            "The distance, yeah.",
            "Yeah it yeah I discussed with him with about that and I never understood why.",
            "I don't know.",
            "I wonder if he didn't have a bug.",
            "We looked into this.",
            "Yeah.",
            "OK. OK, was it the surprise or what did you expect that the actually the distance was kind of semantic in your mobile?",
            "Yeah, I mean when I did it I really wanted it.",
            "It took me 3 months to obtain it, so it's not like something you do like in one day, but.",
            "You can then how can you choose in order to enforce the distance to be to have good properties?",
            "Well, I.",
            "You know, I I don't play with much one or actually I play with a lot of things, but in the end what was my main problem was the cost function.",
            "Because I didn't want to use a cost function like your show, which was probabilistic because you know probabilities, it's about normalizing and you have to normalize all words and you sure made many tricks too.",
            "To accelerate it, but I didn't want that so I had to choose a good cost function.",
            "It took me a while, but other than that I mean.",
            "It's.",
            "I mean, the energy stuff is mobility produces, right?",
            "I mean you don't need to normalize.",
            "Yeah?",
            "I mean no one else.",
            "You know, it's just a one tool.",
            "To say many things but.",
            "I'm not sure.",
            "We did have to find ways to normalize in this.",
            "Yeah, because things are cases you know it's a. I mean, first I wanted to compare to other people and usually people use like this.",
            "Complexity or measure, which is about you know how it is and also.",
            "Yeah, often the language models are used like in speech decoding system and you need point is also.",
            "But me, I mean I didn't care, I don't need priorities, so.",
            "Other questions.",
            "Speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Always I will speak about it.",
                    "label": 0
                },
                {
                    "sent": "My computer doesn't crash again.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, so I've been asked through.",
                    "label": 0
                },
                {
                    "sent": "To present you a very very large killer application of machine learning.",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "Whoops and that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically what I wanted to do here is.",
                    "label": 0
                },
                {
                    "sent": "Like presents you what we started to do at NEC Labs two years ago we started to this to work on the natural language processing and we are company so we need to produce something.",
                    "label": 0
                },
                {
                    "sent": "And as you know in natural language processing is really large scale.",
                    "label": 0
                },
                {
                    "sent": "So it's really a real problem and it might be useful because we have to do some money with it.",
                    "label": 0
                },
                {
                    "sent": "Anne, this is a.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with all my team.",
                    "label": 0
                },
                {
                    "sent": "Oh, actually you cannot say sorry.",
                    "label": 0
                },
                {
                    "sent": "I said you could see.",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Are we OK?",
                    "label": 0
                },
                {
                    "sent": "Kissing.",
                    "label": 0
                },
                {
                    "sent": "Haha.",
                    "label": 0
                },
                {
                    "sent": "What do I do in this case, I reboot.",
                    "label": 0
                },
                {
                    "sent": "Compared to the wild track.",
                    "label": 0
                },
                {
                    "sent": "Decision trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I was saying it's about large scale or natural language processing and this is joint work with all my colleagues, Jason Weston being by by very quick Skull and Leon Bottou.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, two years ago with Jason, we thought all we are a bit bored about cure machine learning theory.",
                    "label": 0
                },
                {
                    "sent": "We were working mainly on support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to do something real for the company and we thought, well, it could be nice.",
                    "label": 0
                },
                {
                    "sent": "Like in a few years we could have a conversation with our computer.",
                    "label": 1
                },
                {
                    "sent": "Of course, if you say that to your boss is going to laugh at you and probably not give you a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So instead we said to him, while we would like to do some opinion sentiment analysis like some automatic business profile from the text and some semantic search like some question answering on course I mean which is actually related to call centers.",
                    "label": 1
                },
                {
                    "sent": "Also, like your minutes without her, but some machine translation system.",
                    "label": 0
                },
                {
                    "sent": "So all these tasks are basically related to text and they are complex task and it's our very large scale.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, basically, if we want to deal with text, there is already a community trying to deal with that.",
                    "label": 0
                },
                {
                    "sent": "It's called natural language processing, so we had a look at what they were doing and.",
                    "label": 0
                },
                {
                    "sent": "And we because we know that we have this kind of tasks that are producing, we could obtain like very good features for task and maybe do what we what we want so.",
                    "label": 0
                },
                {
                    "sent": "What we saw in the natural language processing is that there is like interesting task like part of speech tagging, which is about trying to find if a word is a noun, a verb or noun, and so on.",
                    "label": 1
                },
                {
                    "sent": "There's like chunking, which is about trying to find syntactic constituent in the sentence, like noun phrases, verb phrases, and so on.",
                    "label": 1
                },
                {
                    "sent": "There is name, entity, organization which is about trying to find if world is like a name.",
                    "label": 0
                },
                {
                    "sent": "Or for personal for Co location name or something else.",
                    "label": 0
                },
                {
                    "sent": "And then there is also like a very interesting one which is called semantic or labeling which is quite hard but which tries to attribute like semantic words towards in a sentence.",
                    "label": 0
                },
                {
                    "sent": "So for example if I have like Jaune ADR point in the garden, I first consider a very innocent tense like 8 and I try to to know who is the actor on this world.",
                    "label": 0
                },
                {
                    "sent": "So John, on what it's acting on like the Apple when where?",
                    "label": 0
                },
                {
                    "sent": "Canton so.",
                    "label": 0
                },
                {
                    "sent": "If actually you saw my talk yesterday, you might get bored because a lot of slides of my talk from yesterday.",
                    "label": 0
                },
                {
                    "sent": "All inside here.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I mean these are the natural language processing task we we were at first interest in.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to do some automatic your business profile of companies, well what you can do is first use your good name, entity extraction algorithm, extract names of companies and then try to use semantic calling to see interactions between companies.",
                    "label": 0
                },
                {
                    "sent": "So you really can obtain good features with this kind of task.",
                    "label": 0
                },
                {
                    "sent": "So we're talking about large scale learning in this workshop.",
                    "label": 0
                },
                {
                    "sent": "So actually our large scale all those tasks.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If we consider only label data, there are quite large scale already there, like about 1 million of words.",
                    "label": 0
                },
                {
                    "sent": "If we consider a good data set like a Wall Street Journal.",
                    "label": 1
                },
                {
                    "sent": "But if we consider on the unlabeled data or basically have the web, it's almost infinite.",
                    "label": 0
                },
                {
                    "sent": "So I mean do we have like a machine machine learning algorithm which are able to handle this kind of data?",
                    "label": 0
                },
                {
                    "sent": "Well, you know we were doing support vector machines since quite a while.",
                    "label": 0
                },
                {
                    "sent": "You know in our group and actually we are also interested in in particular in support vector machine in a large scale.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, we probably had something in hand.",
                    "label": 0
                },
                {
                    "sent": "And then if we look at support Vector Machine which could handle 1 million of label example.",
                    "label": 0
                },
                {
                    "sent": "Well, if I if I look on the linear as well like so much inside you know that?",
                    "label": 0
                },
                {
                    "sent": "Limbo to her remind us like last year, I think that's a good order.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient is strictly applied to linear support vector machine works really well on really fast.",
                    "label": 1
                },
                {
                    "sent": "But we also had in the domain like as in performed Austin, Pegasos, Liblinear, and and many others?",
                    "label": 0
                },
                {
                    "sent": "And as we will see in this workshop, or maybe you saw it in reading the results, I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's it's, I mean you know also public domain really can handle large scale data and it works really, really well.",
                    "label": 0
                },
                {
                    "sent": "Under nonlinear support vector machine side where it's less bright, I would say if I look at the workshop results, actually I saw I think only one guy who had the courage to try a normally are support vector machine on their data and I'm really happy about it.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so we also propose a in our group like some large scale support vector machine for only a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "I show you propose something online called lyzeum.",
                    "label": 0
                },
                {
                    "sent": "And the biggest data set we ever trained it on was like on the 8 million of examples.",
                    "label": 0
                },
                {
                    "sent": "It was actually like a modification of the MD's data set which is digit data set on.",
                    "label": 0
                },
                {
                    "sent": "We extended it by adding like tweaks on the data.",
                    "label": 0
                },
                {
                    "sent": "I mean like rotations, distortions and things like that.",
                    "label": 0
                },
                {
                    "sent": "It took eight days training at a well, so it seems feasible.",
                    "label": 0
                },
                {
                    "sent": "But drawback is even after this, eight days of training, well, we obtain like.",
                    "label": 0
                },
                {
                    "sent": "150 oh sorry, 150,000 super vectors so well we can train it, but we cannot use it if we want to be large scale.",
                    "label": 0
                },
                {
                    "sent": "So because of course as you know the support vector machine speed during testing phase depends on the number of public tools and we have.",
                    "label": 0
                },
                {
                    "sent": "When you have like such a number of support vectors, you can basically forget it.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And actually even worse, this task was a non noisy one.",
                    "label": 0
                },
                {
                    "sent": "You get like a very very lower accuracy, like something like 0.6 so.",
                    "label": 0
                },
                {
                    "sent": "For the noisy one we had like kind of hope because we proposal also like some non convex support vector machine which were able like to remove all these annoying over at least part of this ongoing support vectors which are bound.",
                    "label": 0
                },
                {
                    "sent": "But still we never find out a way to combine this online algorithm with this non convex one never workout.",
                    "label": 1
                },
                {
                    "sent": "So in the end, even if we can train only our support vector machine on large scale data well at during the testing phase, it's a bit of a problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the Super vector machine with like able to handle unlabeled examples, I mean it's a bit even worse, I mean most transductive support vector, machine algorithm which have been proposed, basically working only on three datasets and I resign in here only like 2 algorithm.",
                    "label": 1
                },
                {
                    "sent": "One from I don't know how to put on Cinder seen Winnie and Katie.",
                    "label": 0
                },
                {
                    "sent": "It's called SVM, Lin, and as the biggest data set I saw the trended on was like five millions of unlabeled examples, which is really good, and it's they obtain a pretty impressive timing results as well, which was 15 minutes.",
                    "label": 1
                },
                {
                    "sent": "But on the nonlinear side, again.",
                    "label": 0
                },
                {
                    "sent": "It's not done right, and the biggest super vector machine I saw was actually one we train a few years ago.",
                    "label": 0
                },
                {
                    "sent": "It's what's called like CCP CCP, transductive support vector machine.",
                    "label": 0
                },
                {
                    "sent": "It was using like a concave convex approach algorithm and we trained it on like 60,000 unlabeled examples in like 42 hours for like 2 days.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, if we want to handle like infinite number of data, well it could be good.",
                    "label": 0
                },
                {
                    "sent": "If we had like something online and once again we never found out how we could perform this one in an online manual.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I've been asked to do a talk, I thought it would be nice if I had like sometimes some controversial slides.",
                    "label": 0
                },
                {
                    "sent": "Just to add a little spice to this workshop.",
                    "label": 0
                },
                {
                    "sent": "So don't be upset, just take it as spices.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "I I think then when you talk about large scale, it means you're you want to handle a complex task.",
                    "label": 0
                },
                {
                    "sent": "So because if you have a lot of data reporting means your your task is complicated.",
                    "label": 1
                },
                {
                    "sent": "So if your task is complicated, you need probably a very complicated model as well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you have this excuse of not using all the label, I mean all the examples over label.",
                    "label": 1
                },
                {
                    "sent": "It probably means I mean under the pretext that your your algorithm is not like improving in generalization when you add examples, it probably means that your your model is probably under fitting.",
                    "label": 0
                },
                {
                    "sent": "And I saw in the workshop some some people who had like this.",
                    "label": 0
                },
                {
                    "sent": "I mean what I looked like the curve, like the timing with respect to numbers of example.",
                    "label": 0
                },
                {
                    "sent": "I so like some flat curve at the end as if they decided to let stop after 100,000 training examples because my right is not improving anyways.",
                    "label": 1
                },
                {
                    "sent": "Well, it's just means that your model is underfitting, so it's probably garbage.",
                    "label": 0
                },
                {
                    "sent": "And actually when I look at it.",
                    "label": 0
                },
                {
                    "sent": "The nonlinear support vector machine, for example, are doing much better than the linear one on the data set Alpha, so even if it takes maybe I don't remember our text to train it on Alpha, maybe a few hours or one hour.",
                    "label": 0
                },
                {
                    "sent": "So I prefer to wait one hour or never very good accuracy.",
                    "label": 0
                },
                {
                    "sent": "If I have a good application to do, then to wait like few seconds and having a bad accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the end we have like 2 extreme choices to obtain a complex system.",
                    "label": 1
                },
                {
                    "sent": "And both are interesting.",
                    "label": 0
                },
                {
                    "sent": "On one case you you can like.",
                    "label": 0
                },
                {
                    "sent": "Take a good linear support vector machine algorithm and we we show.",
                    "label": 0
                },
                {
                    "sent": "I mean you will see in this workshop that it's indeed possible to have one which which scales really well, and then with this linear algorithm, which is actually very simple, you can design some very complex features you have to do it by yourself, but you can do it.",
                    "label": 0
                },
                {
                    "sent": "On the other's outside, it's also very extreme.",
                    "label": 0
                },
                {
                    "sent": "You can say, oh, I'm going to design a very complex algorithm, and I hope that this very complex algorithm is going to train implicitly all these good features for my task.",
                    "label": 0
                },
                {
                    "sent": "Well, I think both approaches are bit extreme and poorly.",
                    "label": 1
                },
                {
                    "sent": "The best is something in the middle.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have like some good a Prairie about your task, it would be too bad not to use to use it in your task, but I think limited all serve to linear support.",
                    "label": 0
                },
                {
                    "sent": "Vector Machine is a bit too much.",
                    "label": 0
                },
                {
                    "sent": "We should find something a bit more complex.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We looked at what the natural language community was doing.",
                    "label": 0
                },
                {
                    "sent": "And basically they were more on the extreme side of using a linear or support vector machine an using a lot of complex features.",
                    "label": 0
                },
                {
                    "sent": "So what they do is that while they take a sentence.",
                    "label": 0
                },
                {
                    "sent": "They they build their homemade like a lot of complex features.",
                    "label": 0
                },
                {
                    "sent": "I mean, some are less complex, but so not complex.",
                    "label": 0
                },
                {
                    "sent": "They they mix them together and they feed it to some good classifier like.",
                    "label": 0
                },
                {
                    "sent": "Super Vector machine that we call like shallow here because it's quite simple.",
                    "label": 0
                },
                {
                    "sent": "And it works actually pretty well.",
                    "label": 0
                },
                {
                    "sent": "Well in a case like semantical labeling, which is a bit.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Harder task they they actually cascade features so they for example compute part of speech, then with part of speech you can obtain like parse tree.",
                    "label": 1
                },
                {
                    "sent": "You can build a bathroom tubs over that.",
                    "label": 0
                },
                {
                    "sent": "Then from the past three you can extract some unbilled features and then from this homemade features where you can again like feed them to classify or like support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So it does work very well, but it's extremely slow.",
                    "label": 0
                },
                {
                    "sent": "The reasons the main bottleneck here is actually the construction of the power stream, which is really time consuming.",
                    "label": 0
                },
                {
                    "sent": "So basically we looked at the community and we saw, well, we cannot choose it for our application.",
                    "label": 0
                },
                {
                    "sent": "I mean we need to handle a lot of data.",
                    "label": 0
                },
                {
                    "sent": "We can really not use it during the testing Phase I mean, maybe we can train it, maybe it works.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Actually it does work well, but we cannot use it.",
                    "label": 0
                },
                {
                    "sent": "For what we are interested in, so we have to find something else.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So instead, well, we said let's try to do a complete end to end system.",
                    "label": 0
                },
                {
                    "sent": "We will give text it out, put tags on this system is going to be a good old neural network so.",
                    "label": 0
                },
                {
                    "sent": "It's a bit scary at first time, but what we what I'm going to present here and that's what I presented yesterday, it's.",
                    "label": 0
                },
                {
                    "sent": "It's an empty end to end system when you like.",
                    "label": 0
                },
                {
                    "sent": "Input text at the beginning just trade sentence straight walls.",
                    "label": 0
                },
                {
                    "sent": "You, I mean basically, this system is going to embed words into, like some embedding space, some feature vector space.",
                    "label": 0
                },
                {
                    "sent": "This feature vector are going to be combined and.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Like some local features in an automatic way, this local features are also going to be combined to obtain some global features an from this global features we obtain our tags.",
                    "label": 1
                },
                {
                    "sent": "Everything is trained in in basically backpropagation, and it works really well as we will see.",
                    "label": 0
                },
                {
                    "sent": "So we propose something which is a.",
                    "label": 0
                },
                {
                    "sent": "A deep architecture we say, but actually it's just another network and it's like a unification of all the natural language processing task which involves tagging.",
                    "label": 0
                },
                {
                    "sent": "The fact that we can unify all this natural language.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using task.",
                    "label": 0
                },
                {
                    "sent": "Basically Oroya allow us to to perform joint training.",
                    "label": 0
                },
                {
                    "sent": "In fact it's well known in the natural language community that all these tasks that I showed you at the beginning like part of speech ranking and so.",
                    "label": 0
                },
                {
                    "sent": "Or this task are kind of correlated, so you know that if you have like good features 1141, it might be useful for the other one.",
                    "label": 0
                },
                {
                    "sent": "So if you train like part of the network.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you if you if you're trying everything at the same time when you share part of the network, you basically share part of the features and you can hope that the training is going to be.",
                    "label": 0
                },
                {
                    "sent": "I mean the testing performance are going to be better.",
                    "label": 0
                },
                {
                    "sent": "So that's that.",
                    "label": 0
                },
                {
                    "sent": "I will show here as well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so it's not our networks and I've been told like few days ago actually that know how networks have been absolutely 20 years ago, so why should we use them?",
                    "label": 1
                },
                {
                    "sent": "I've been quite the office created by this sentence.",
                    "label": 0
                },
                {
                    "sent": "Because I mean, I think we should not take like so hard position like that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know Super Vector machine have good sites nor networks are good size as well and they're actually pretty much.",
                    "label": 0
                },
                {
                    "sent": "Would you say young?",
                    "label": 0
                },
                {
                    "sent": "Complementary.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Viable if I take like a, you know the stochastic gradient applied on the support vector machine, where it's just a good old stochastic gradient descent applied to margin perceptron with like away decades, nothing else.",
                    "label": 0
                },
                {
                    "sent": "And also recently in our group we were interested, you know, in trying to go large scale with, for example, unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So what we did is take the support vector machine idea of like transaction and apply it to networks and it actually works really well.",
                    "label": 0
                },
                {
                    "sent": "It gives very good performance.",
                    "label": 0
                },
                {
                    "sent": "Is it scale extremely well?",
                    "label": 0
                },
                {
                    "sent": "And why not using it?",
                    "label": 0
                },
                {
                    "sent": "So well, before saying that no networks are really bad, let's see what they can do.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we propose here.",
                    "label": 0
                },
                {
                    "sent": "Is like Azaceta end to end system where we input text and we output tags well.",
                    "label": 0
                },
                {
                    "sent": "Of course like computers cannot under text they have like to deal with numbers.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you have.",
                    "label": 0
                },
                {
                    "sent": "Do you have to do it?",
                    "label": 0
                },
                {
                    "sent": "It's like transform text into walls.",
                    "label": 0
                },
                {
                    "sent": "So well in general, like when you have a tagging task, what you can do when you want to tag a world like this world set, it's like take a window around this world.",
                    "label": 0
                },
                {
                    "sent": "So maybe my sentence with the cat sat on the mat and I took a window around the world set.",
                    "label": 1
                },
                {
                    "sent": "These walls there are just indices for the computer.",
                    "label": 0
                },
                {
                    "sent": "It's like an index in a dictionary, right?",
                    "label": 0
                },
                {
                    "sent": "Well, this index I'm just going to map it with a lookup table to a feature vector in some particular.",
                    "label": 0
                },
                {
                    "sent": "Space I have to choose the size of the space.",
                    "label": 0
                },
                {
                    "sent": "So it's just like a look up table I ever want and I have my look up table.",
                    "label": 0
                },
                {
                    "sent": "I just assign a feature vector on this feature vector is going to be trained by backpropagation.",
                    "label": 0
                },
                {
                    "sent": "You can see it as like a very particular like matrix vector operation.",
                    "label": 0
                },
                {
                    "sent": "If you consider you want as an index, well, you can replace them.",
                    "label": 0
                },
                {
                    "sent": "This index as a big vector of 0 or the size of the dictionary with one bit in the middle at one which is at the position of the wording that you try.",
                    "label": 0
                },
                {
                    "sent": "And you are going to multiply like a big matrix by this big matrix by this big vector of zero, except at one place.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly like.",
                    "label": 1
                },
                {
                    "sent": "Taking one column of this matrix basically so it's really a lookup table.",
                    "label": 0
                },
                {
                    "sent": "So you have once you move them to a feature vector space and then for each of those walls you.",
                    "label": 0
                },
                {
                    "sent": "I mean you you have these vectors, you concatenate them and you can feed them to classical networks layer.",
                    "label": 0
                },
                {
                    "sent": "It works really well for tasks like part of speech tagging or for chunking or even from name on TT organization.",
                    "label": 0
                },
                {
                    "sent": "But when you have a task like semantic hauling it doesn't work anymore.",
                    "label": 0
                },
                {
                    "sent": "Why it's because semantical labeling is like about labeling award?",
                    "label": 0
                },
                {
                    "sent": "With respect to affirm that you chose before hand and the verb might be completely outside of the window here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You have to find a way to specify to the network the worm, and you have to find a way to handle the old sentence at the same time basically.",
                    "label": 0
                },
                {
                    "sent": "So there is a generalization of this approach, which is called a convolutional neural.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Networks as soon as you have to deal with sequences, think about convolutional neural networks.",
                    "label": 0
                },
                {
                    "sent": "There are really well in case.",
                    "label": 0
                },
                {
                    "sent": "I mean, for now it works at least.",
                    "label": 0
                },
                {
                    "sent": "So it's almost the same approach.",
                    "label": 0
                },
                {
                    "sent": "Again, you have like these walls, you map them to like some feature space at the same time.",
                    "label": 0
                },
                {
                    "sent": "You have to indicate the position of the world.",
                    "label": 0
                },
                {
                    "sent": "With respect I mean that you want to label so.",
                    "label": 0
                },
                {
                    "sent": "For that we we give us input features like the distance of each one with respect to the world we want to label.",
                    "label": 0
                },
                {
                    "sent": "We also give in the input the position of the each one with respect to the verb we chose before hand and with respect to which one we want to label whatever.",
                    "label": 0
                },
                {
                    "sent": "So for each of this one, let's say features we obtain like.",
                    "label": 0
                },
                {
                    "sent": "A feature vector by just applying a lookup table.",
                    "label": 0
                },
                {
                    "sent": "Once again, you choose like a window size any but this time you are going to consider all possible Windows in this in this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sentence so actually I have slides for that.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the window if outside three, I have this feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Well, I consider first this window of size 3.",
                    "label": 0
                },
                {
                    "sent": "I apply my my matrixectomy depiction that I concatenate.",
                    "label": 0
                },
                {
                    "sent": "I concatenate them, apply your matrix vector or multiplication.",
                    "label": 0
                },
                {
                    "sent": "I open your feature vector.",
                    "label": 0
                },
                {
                    "sent": "Then I shift I I take the next three once and I'm going to do the same again.",
                    "label": 0
                },
                {
                    "sent": "I obtain a new feature writers and so on.",
                    "label": 0
                },
                {
                    "sent": "This is basically just doing a convolution.",
                    "label": 0
                },
                {
                    "sent": "It's I mean you can see it like I like sharing weights with time basically, but it's just a generalization of this window approach.",
                    "label": 0
                },
                {
                    "sent": "Condition has been used with success in image along time ago and also in speech.",
                    "label": 1
                },
                {
                    "sent": "So they do work really well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "As we will see here, they also work with very well are intact.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, you have this feature like dogs, you apply your convolution and basically you end up with a number of feature vectors which are like.",
                    "label": 0
                },
                {
                    "sent": "Which can be seen like some local feature exhaust for around each one in the sentence.",
                    "label": 0
                },
                {
                    "sent": "And still this number of local feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Is dependent on the number of words and sentence, so you have to get rid of this like time dimension.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is like take a Max overtime to try to force the network.",
                    "label": 0
                },
                {
                    "sent": "To find the best feature for the world of I mean for labeling the world of interest.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if I look.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One example.",
                    "label": 0
                },
                {
                    "sent": "Like this very long sentence yesterday after Microsoft blah blah blah.",
                    "label": 1
                },
                {
                    "sent": "The awarding ready, I mean the writing, already here is of Elmer I chose to.",
                    "label": 0
                },
                {
                    "sent": "Before hand and the wording green here yesterday is like the world I chose to label.",
                    "label": 0
                },
                {
                    "sent": "And this is our presentation of like the local features along time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In a red is basically which feature has been chosen.",
                    "label": 0
                },
                {
                    "sent": "Which one was a maximum water, so which which has been chosen by the network and you can see that intensely interesting laser is always like 2 cluster one around the wall of interest and one around the verbal filters.",
                    "label": 0
                },
                {
                    "sent": "And if I move the world of interest at the end while the cluster move.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Once you you've done this Max overtime, you obtain like a fixed size.",
                    "label": 0
                },
                {
                    "sent": "Feature vector which represents I mean which like is supposed to be good features for the task you're interested in, and for the sentence you presented.",
                    "label": 0
                },
                {
                    "sent": "You can then feed this this feature vector to again classical layers.",
                    "label": 0
                },
                {
                    "sent": "And you train the whole thing with like stochastic gradient descent and backpropagation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As I said, I mean this this architecture is really general and it's so general that we can apply it to any kind of natural language task which involved which involves tagging.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's what we tried him.",
                    "label": 0
                },
                {
                    "sent": "We tried to train everything at the same time, ensuring like this, what we could look up tables.",
                    "label": 0
                },
                {
                    "sent": "So this world feature weights.",
                    "label": 0
                },
                {
                    "sent": "An and again train everything at the same time.",
                    "label": 0
                },
                {
                    "sent": "Back propagation just sharing this this look up table and we will see that it does work very well.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It it is, it is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if if we will not then it would not work well.",
                    "label": 0
                },
                {
                    "sent": "So you know you you really back back propagates.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up to the end here I mean up to the top, you really back propagate everything, so you update as well.",
                    "label": 0
                },
                {
                    "sent": "That I mean, as I said, you can see this layer as a very particular layer.",
                    "label": 0
                },
                {
                    "sent": "It's just a matrix vector multiplication as other layers in our network.",
                    "label": 0
                },
                {
                    "sent": "But it's it's presented in the optimal, I mean in efficient way.",
                    "label": 0
                },
                {
                    "sent": "Basically you can see it just as a look up table, but still you optimize it so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, millimeters cloning actually, which is about like your training things at the same time on sharing part of the network is absolutely absolutely are not new and it has been used quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Actually in a machine learning and you can find a very good overview in like a rich account, one basis if you want.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, Interestingly, in the case of natural language processing, 1,000,000 of what is actually not that large scale, unfortunately, and the reason is.",
                    "label": 0
                },
                {
                    "sent": "That's well, I mean, if you consider all West regional, there's like about 36,000 of words in the dictionary and there is only 1 million of words.",
                    "label": 1
                },
                {
                    "sent": "You know when I do a word count on the on the file.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In fact, 15 of the most frequent walls, and that's I mean, I mean.",
                    "label": 0
                },
                {
                    "sent": "How do you say come on problem in text 15 of the most frequent warnings that you join are seen only like 90% of the RC Nigeria 90% of the time.",
                    "label": 1
                },
                {
                    "sent": "So many words are really well and are going to be seen like once or twice in the in the in the text.",
                    "label": 0
                },
                {
                    "sent": "So if this was a rare well we are not going to train it, train them properly.",
                    "label": 0
                },
                {
                    "sent": "So you have to find a way to deal with that.",
                    "label": 0
                },
                {
                    "sent": "So what we first did and and that's a publication we we presented actually at the ACL conference last year.",
                    "label": 0
                },
                {
                    "sent": "We did some world clustering according to Battle speech, but that's you know a bit bigger query.",
                    "label": 0
                },
                {
                    "sent": "I mean why part of speech?",
                    "label": 0
                },
                {
                    "sent": "Is there any other way to cluster?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "So we didn't really like this approach, even if it was working OK.",
                    "label": 1
                },
                {
                    "sent": "So then we sort all.",
                    "label": 0
                },
                {
                    "sent": "Let's try to threshold the number of words in the dictionary so we all the other ones according to their frequency and we just cut back at some threshold we chose beforehand on that we have to optimize.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And all the words above the threshold, I'll just mapped to a particular special one like unknown.",
                    "label": 0
                },
                {
                    "sent": "So why not?",
                    "label": 0
                },
                {
                    "sent": "It does Walker tree?",
                    "label": 0
                },
                {
                    "sent": "But it's still not sufficient.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do better.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When you think about it, if you have like a sentence like the cat sat on the mat, which actually might not be in Wall Street Journal because it's about cats.",
                    "label": 1
                },
                {
                    "sent": "So I don't think they talk about cats in Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "But if you had that in the words visual cat is a very common one.",
                    "label": 0
                },
                {
                    "sent": "Very common word.",
                    "label": 0
                },
                {
                    "sent": "So this this sentence is probably going to be tagged properly anytime.",
                    "label": 1
                },
                {
                    "sent": "But then if you consider the sentence, the fair is a feline sat on the mat.",
                    "label": 0
                },
                {
                    "sent": "Feline is actually a very rare world, and even if the sentence looks the same, the network is pulling too, probably going to do something bad on this sentence.",
                    "label": 0
                },
                {
                    "sent": "So in fact, what you want here, it's.",
                    "label": 0
                },
                {
                    "sent": "Something which enforce the embedding vector of like 2 walls which are semantically related to be close to each other.",
                    "label": 0
                },
                {
                    "sent": "So one thing we saw her first is all let's try to use Walnut because one it is a radio.",
                    "label": 0
                },
                {
                    "sent": "Good label data set.",
                    "label": 0
                },
                {
                    "sent": "So as I think like about 100,000 words.",
                    "label": 0
                },
                {
                    "sent": "And it tells you about like semantic relations of walls so.",
                    "label": 0
                },
                {
                    "sent": "Let's take Walnut and we if we consider like 2 words in Walnut.",
                    "label": 0
                },
                {
                    "sent": "If they are linked in one at it.",
                    "label": 0
                },
                {
                    "sent": "So if they have a semantical relationship, let's force the embedding to be close to each other, and if they are not, let's presume apart.",
                    "label": 0
                },
                {
                    "sent": "So this can be viewed as an additional task that we can apply in this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello in this multi task learning way and it can basically regularize pretty well this lookup table.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We tried that.",
                    "label": 0
                },
                {
                    "sent": "It works pretty well, but we can do even better.",
                    "label": 0
                },
                {
                    "sent": "We can use, well, you know, all the web.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you have this, if you have the model like an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which which could answer the question.",
                    "label": 0
                },
                {
                    "sent": "If a sentence is actually good English or not, well, it would mean that your model underneath understand really well what is syntax of English.",
                    "label": 1
                },
                {
                    "sent": "What is grammar?",
                    "label": 0
                },
                {
                    "sent": "What is semantic?",
                    "label": 0
                },
                {
                    "sent": "So that's what we try to do here.",
                    "label": 0
                },
                {
                    "sent": "We decided or let's try to add an additional task which would understand implicitly this kind of things.",
                    "label": 0
                },
                {
                    "sent": "So we designed the language model an.",
                    "label": 0
                },
                {
                    "sent": "We we took I mean.",
                    "label": 0
                },
                {
                    "sent": "The web is really large scale, so we limit ourselves because we we aren't like limited in time and we limited ourselves like to Wikipedia which is already 600.",
                    "label": 0
                },
                {
                    "sent": "So to you, 1 million of worlds.",
                    "label": 0
                },
                {
                    "sent": "After a few tweaks.",
                    "label": 0
                },
                {
                    "sent": "And we decided to consider the task of flag discriminating English and English, so I mean.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia windows of text from Wikipedia windows of text, while the middle one has been replaced by some random world.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We trained this task in actually ranking cost.",
                    "label": 0
                },
                {
                    "sent": "I mean using a ranking cost, ranking cost and all network basically was out putting a score for window of text and we want the score for a true window of text to be larger with a margin than all other score for all other window of text wells and neither one has been replaced by something random.",
                    "label": 0
                },
                {
                    "sent": "Once again, we train that with stochastic gradient backpropagation.",
                    "label": 0
                },
                {
                    "sent": "When you can be sure that it takes weeks days.",
                    "label": 0
                },
                {
                    "sent": "Add a. Yeah, actually before I continue.",
                    "label": 0
                },
                {
                    "sent": "I mean I should mention that this is not completely new.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you should.",
                    "label": 0
                },
                {
                    "sent": "Banjo dealer already like some some language model like that in the past with no networks as well.",
                    "label": 0
                },
                {
                    "sent": "It also, I mean they are also pretty good results.",
                    "label": 0
                },
                {
                    "sent": "But yeah, yeah, it rained it only on one million of walls I think.",
                    "label": 0
                },
                {
                    "sent": "Or maybe no, maybe 10 millions, isn't it?",
                    "label": 0
                },
                {
                    "sent": "So I'm seeing between one and 10, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Some of the words just random words, sorry.",
                    "label": 0
                },
                {
                    "sent": "When we train, we take everything stochastic, so we just like stochastic.",
                    "label": 0
                },
                {
                    "sent": "Sorry, what does it take so long?",
                    "label": 0
                },
                {
                    "sent": "Because it's really huge.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "How many parameters?",
                    "label": 0
                },
                {
                    "sent": "I don't know exactly, but I consider a Dictionary of like 30,000 and 100,000 actually and times the size of the world vector and it's about 50 in general.",
                    "label": 0
                },
                {
                    "sent": "And and then you add you have to add like this.",
                    "label": 0
                },
                {
                    "sent": "You know hidden layers parameters, but that's maybe peanuts compared to this big lookup tables so.",
                    "label": 0
                },
                {
                    "sent": "Do you have like 1.6 million parameters?",
                    "label": 0
                },
                {
                    "sent": "You have like your.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're 50 * 30,000.",
                    "label": 0
                },
                {
                    "sent": "It is quite location but but you know they are not used.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah but they are not user.",
                    "label": 0
                },
                {
                    "sent": "At each sentence, you know because you select only power matters which yeah exactly so it's hard to say actually what capacity it is.",
                    "label": 0
                },
                {
                    "sent": "But steel is pretty huge.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Wait, it's this?",
                    "label": 0
                },
                {
                    "sent": "This is really slow.",
                    "label": 0
                },
                {
                    "sent": "I mean try it.",
                    "label": 0
                },
                {
                    "sent": "Select character sizes yeah.",
                    "label": 0
                },
                {
                    "sent": "What I mean, don't forget that at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe I forgot to say that actually, you know we take a window of text again here and there were the size of the window is 11 one yeah, but no eleven wants yet.",
                    "label": 0
                },
                {
                    "sent": "So you have 11 wants times 50 at the input, so it's quite big.",
                    "label": 0
                },
                {
                    "sent": "And then I mean I can tell you actually the architecture I showed.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the ones each one about 50.",
                    "label": 0
                },
                {
                    "sent": "The window 11 walls.",
                    "label": 0
                },
                {
                    "sent": "Then like query two layers one maybe one onwards user 200 something at so you know it's it's pretty big.",
                    "label": 0
                },
                {
                    "sent": "How many parameters for personal example?",
                    "label": 0
                },
                {
                    "sent": "How many parameters are active?",
                    "label": 0
                },
                {
                    "sent": "So all the power matters here, so it's like 11 * 50.",
                    "label": 0
                },
                {
                    "sent": "Plus so I mean then, plus 11 * 50 times the number of hidden units which are like I don't remember 100 or 200.",
                    "label": 0
                },
                {
                    "sent": "Plus 100 or two.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty big.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Interestingly, I was very lazy at the beginning.",
                    "label": 0
                },
                {
                    "sent": "And you know, I have Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "I just decided to let stupidly read Wikipedia with my network on training like that on the fly.",
                    "label": 0
                },
                {
                    "sent": "It's online, right?",
                    "label": 0
                },
                {
                    "sent": "Why you should actually shuffle the data.",
                    "label": 0
                },
                {
                    "sent": "I had some Interestingly spiking the test and I was wondering what's happening here and it took me awhile actually to find out that in Wikipedia you have like a lot of data or related to cities and they say oh this city is about I don't know how many million of inhabitants you know and it's always the same sentence like Switch follow.",
                    "label": 0
                },
                {
                    "sent": "You have like millions of cities.",
                    "label": 0
                },
                {
                    "sent": "So basically each time you go over the cities they have this spike.",
                    "label": 0
                },
                {
                    "sent": "Also takes data so so shuffling the data is a good thing, and it might be actually quite tricky when you have a large data set.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, if you do it like straight like that, it's going to take you a very, very long time to train.",
                    "label": 0
                },
                {
                    "sent": "And I mean it's going to work, but I mean it's going to be just really, really, really long.",
                    "label": 0
                },
                {
                    "sent": "So what I did instead is like I thought, all let's train my normal network lights like if I was training a child, you know.",
                    "label": 0
                },
                {
                    "sent": "A first consider a simple problem, so I just consider a small dictionary size.",
                    "label": 1
                },
                {
                    "sent": "So dictionary size were ordered first words.",
                    "label": 0
                },
                {
                    "sent": "You know I took the most frequent words and I trained my networks just resource for us and then I increase the now network.",
                    "label": 0
                },
                {
                    "sent": "I mean the showing size and also increased only once the size of Sir input window.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "After like three weeks of praying and unlearning, I actually obtain her impressive embedding.",
                    "label": 0
                },
                {
                    "sent": "Um, 05 minutes well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK Novia, impressive embedding and here it's like I pick up like a random word in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "And the number of underneath is like the ranking of the world in frequency.",
                    "label": 0
                },
                {
                    "sent": "So a smaller number means very frequently large number of Israel.",
                    "label": 0
                },
                {
                    "sent": "And this was like the 10 closest words according to the equation distance in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "And you can see when it learns a lot of semantic even for.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Firewalls.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So yeah, as I said at the beginning, we were interested in a multitask learning an we aren't easy or so in the semantical labeling a lot.",
                    "label": 0
                },
                {
                    "sent": "So we first try to train our semantical labeling task alone.",
                    "label": 0
                },
                {
                    "sent": "So this is a plot of the testing error with respect to the number of epochs just to show that it converges, converges quite quickly an.",
                    "label": 0
                },
                {
                    "sent": "This is for two different embedding size, so 15 and 100.",
                    "label": 0
                },
                {
                    "sent": "The behavior is pretty much the same, but it's more clear here.",
                    "label": 0
                },
                {
                    "sent": "So on the top here you have like this semantical labeling task alone.",
                    "label": 0
                },
                {
                    "sent": "All this like a bunch of curves like semantic whole labeling task, train with like drinking bottle, speech mix of drinking or parts of speech or name, entity organization and things like that.",
                    "label": 0
                },
                {
                    "sent": "Then you gain quite a bit if you use like a Walnut.",
                    "label": 0
                },
                {
                    "sent": "An you gain a lot if you use everything at the same time or just the language model with semantic Halloween.",
                    "label": 0
                },
                {
                    "sent": "We actually obtain like state of the art performance, you want to be better and it's extremely fast during the testing phase.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Considering the overall task is also improves in Chongqing, basically we have the the state of the art in part of speech when you use multitask learning.",
                    "label": 0
                },
                {
                    "sent": "It doesn't improve much compared to the task alone, but it's not surprising because part of speech is a very easy task.",
                    "label": 0
                },
                {
                    "sent": "Actually, in Chongqing we can even beat the state of the art if we add a bit of prior knowledge in the features.",
                    "label": 0
                },
                {
                    "sent": "So if we just add part of speech features we like far away for what they have.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said also at the beginning I'm on, I am in a company so I'm interested in doing like you know, money for my company so.",
                    "label": 0
                },
                {
                    "sent": "We we actually on this system, which is like extremely fast.",
                    "label": 0
                },
                {
                    "sent": "We can obtain for free.",
                    "label": 0
                },
                {
                    "sent": "Basically, a semantic search system, I mean almost for free.",
                    "label": 0
                },
                {
                    "sent": "You have like all this stuff around, but in the middle we we build like this similarity ranking measure between a query and text sentence.",
                    "label": 0
                },
                {
                    "sent": "Just by, you know using like the symbol ticker or labeling tags.",
                    "label": 0
                },
                {
                    "sent": "And tweaking it.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Iranda it works extremely well when the queries are about.",
                    "label": 0
                },
                {
                    "sent": "About in Windsor.",
                    "label": 0
                },
                {
                    "sent": "Quail semantic compared to other.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "And I don't know you can't see much like I'm afraid.",
                    "label": 0
                },
                {
                    "sent": "This was an example about who bought compact, so with our system to show that if you add a bit of semantic here really gain a lot compared to classic.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call a system like Google.",
                    "label": 0
                },
                {
                    "sent": "Which if you try it actually works pretty badly on this one.",
                    "label": 0
                },
                {
                    "sent": "The 1st first answer is not even.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Basically I it won't Google just walked by keywords or maybe some other idealistic.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but if you try you will see that the first sentence is about compact acquires digital blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "So I mean it's not about who both.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, I mean I showed you here like.",
                    "label": 0
                },
                {
                    "sent": "A big non network architecture which can be applied for natural language processing task which involves tagging.",
                    "label": 0
                },
                {
                    "sent": "It has kind of a lot of advantages.",
                    "label": 0
                },
                {
                    "sent": "It's very general.",
                    "label": 0
                },
                {
                    "sent": "You can obtain state of the art performance without designing any handmade features.",
                    "label": 0
                },
                {
                    "sent": "You can perform joint training to obtain a very good performance using like massive label data and it's extremely fast during testing phase which order us to like apply it directly on large scale application like Semantic Search.",
                    "label": 1
                },
                {
                    "sent": "It does some inconvenience and sonar network and no network.",
                    "label": 0
                },
                {
                    "sent": "Our powerful tool so it's kind of hard to handle, but.",
                    "label": 1
                },
                {
                    "sent": "It does already like some early impacts.",
                    "label": 0
                },
                {
                    "sent": "I mean you can easily apply it, you know to some other languages like Japanese or other task.",
                    "label": 0
                },
                {
                    "sent": "And as I said, yeah we developed this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So system, so I mean to conclude, I would say that's support vector machine are very good algorithm for studies.",
                    "label": 0
                },
                {
                    "sent": "But you should not limit yourself to that, and because very large scale complex task in general require more ambitious models.",
                    "label": 1
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "In my head, I think you doing about 10 to 14 parameter updates menu training process.",
                    "label": 0
                },
                {
                    "sent": "About 10 to 14 parameter updates.",
                    "label": 0
                },
                {
                    "sent": "Yes, maybe I checked it at some point.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's quite true.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Other problems that could be converted to.",
                    "label": 0
                },
                {
                    "sent": "Good timing, right?",
                    "label": 0
                },
                {
                    "sent": "Well, actually, I mean for us.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's almost like.",
                    "label": 0
                },
                {
                    "sent": "Problem solved for that.",
                    "label": 0
                },
                {
                    "sent": "I mean we we tried many task we have still few interns working on that to like clean up our experiments on updating like state of the art performance.",
                    "label": 0
                },
                {
                    "sent": "But we're almost done and then we want to move on like to more complex tasks like.",
                    "label": 0
                },
                {
                    "sent": "Questioning question, answering system or machine translation because.",
                    "label": 0
                },
                {
                    "sent": "This is more challenging.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Training binary classifier or some structured classifier if you will find the specs.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so actually here.",
                    "label": 0
                },
                {
                    "sent": "I mean it's multi class classifier.",
                    "label": 0
                },
                {
                    "sent": "I mean for example part of speech is like about 50 classes or 45 material.",
                    "label": 0
                },
                {
                    "sent": "So it's all multiclass, but it's true.",
                    "label": 0
                },
                {
                    "sent": "Not too far for given work.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to know what is the label or you want to also detect?",
                    "label": 0
                },
                {
                    "sent": "Which would be some?",
                    "label": 0
                },
                {
                    "sent": "No, you want to you want to put a label on each in each world.",
                    "label": 0
                },
                {
                    "sent": "What we didn't do here is apply for example of.",
                    "label": 0
                },
                {
                    "sent": "It'll be at the end because we could.",
                    "label": 0
                },
                {
                    "sent": "Actually we have some grammar you know on the labels, so we could even.",
                    "label": 0
                },
                {
                    "sent": "Improve the performance by applying some dynamic algorithm at the element we didn't yet.",
                    "label": 0
                },
                {
                    "sent": "I have another question.",
                    "label": 0
                },
                {
                    "sent": "If I remember well, when you use this first approach about nothing, the words yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Which is the fact that the.",
                    "label": 0
                },
                {
                    "sent": "The distance, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah it yeah I discussed with him with about that and I never understood why.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I wonder if he didn't have a bug.",
                    "label": 0
                },
                {
                    "sent": "We looked into this.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, was it the surprise or what did you expect that the actually the distance was kind of semantic in your mobile?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean when I did it I really wanted it.",
                    "label": 0
                },
                {
                    "sent": "It took me 3 months to obtain it, so it's not like something you do like in one day, but.",
                    "label": 0
                },
                {
                    "sent": "You can then how can you choose in order to enforce the distance to be to have good properties?",
                    "label": 0
                },
                {
                    "sent": "Well, I.",
                    "label": 0
                },
                {
                    "sent": "You know, I I don't play with much one or actually I play with a lot of things, but in the end what was my main problem was the cost function.",
                    "label": 0
                },
                {
                    "sent": "Because I didn't want to use a cost function like your show, which was probabilistic because you know probabilities, it's about normalizing and you have to normalize all words and you sure made many tricks too.",
                    "label": 0
                },
                {
                    "sent": "To accelerate it, but I didn't want that so I had to choose a good cost function.",
                    "label": 0
                },
                {
                    "sent": "It took me a while, but other than that I mean.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "I mean, the energy stuff is mobility produces, right?",
                    "label": 0
                },
                {
                    "sent": "I mean you don't need to normalize.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "I mean no one else.",
                    "label": 0
                },
                {
                    "sent": "You know, it's just a one tool.",
                    "label": 0
                },
                {
                    "sent": "To say many things but.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "We did have to find ways to normalize in this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because things are cases you know it's a. I mean, first I wanted to compare to other people and usually people use like this.",
                    "label": 0
                },
                {
                    "sent": "Complexity or measure, which is about you know how it is and also.",
                    "label": 0
                },
                {
                    "sent": "Yeah, often the language models are used like in speech decoding system and you need point is also.",
                    "label": 0
                },
                {
                    "sent": "But me, I mean I didn't care, I don't need priorities, so.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Speaker.",
                    "label": 0
                }
            ]
        }
    }
}