{
    "id": "l2jz2l6qsgeyed4tqvvnbgm3p3auajdj",
    "title": "Stability Selection for High-Dimensional Data",
    "info": {
        "author": [
            "Peter B\u00fchlmann, ETH Zurich"
        ],
        "published": "Dec. 18, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Social Sciences->Economics->Econometrics"
        ]
    },
    "url": "http://videolectures.net/sip08_buhlmann_ssfhd/",
    "segmentation": [
        [
            "I would like to thank the organizers also for having invited me to this wonderful workshop.",
            "I really enjoy being here.",
            "And my talk is about stability selection and I think it's a good idea to start recent example.",
            "What I mean by Stew?"
        ],
        [
            "And so I think this is indeed one example of stability.",
            "This is a highly stable object.",
            "I think he's not flipping his opinion."
        ],
        [
            "He will not so easily change his mind.",
            "Jahnke just told me he's actually waiting here in London for his next airplane.",
            "Going to Tel Aviv.",
            "And so this is indeed what we're achieving for."
        ],
        [
            "So there's actually a little how can I make the screen larger control shift?",
            "P. OK, so congratulations to the Home book prize.",
            "I think that's the right place to say this and I'm really pleased to hear that you got this prestigious prize.",
            "So let's put Joe."
        ],
        [
            "Listen Yankee aside, and let's talk about my talk.",
            "So what I want to explain you and show you is that we want to do some kind of better detection, better sparse recovery of a pattern of variables are off components or factors in a model.",
            "And the talk will also address kind of the Earth.",
            "Burning question, I mean, how many variables should be actually having our model?",
            "How many components?",
            "How many edges in a graph?",
            "And so roughly speaking, what I will do today is I will talk about regularised estimation for discrete structures in a high dimensional setting.",
            "And I do think this is very relevant in many practical applications, and it is often also substantially more difficult in prediction."
        ],
        [
            "So here is another example, not Yankees example.",
            "This is a biology example.",
            "I'm aware that this is more linked to econometrics, but let's just go with this little example.",
            "It's an example where we actually found quite a lot of success by applying this lawsuit type.",
            "L1 penalization methods example is about finding a transcription factor binding site in the DNA sequence.",
            "So roughly speaking, this is just a site where a protein can bind to the DNA, and I am omitting many details here.",
            "I just tell you.",
            "This is a regression type of problem.",
            "We will have unique varied response variables Y.",
            "They measure some kind of binding intensity on a course DNA statement.",
            "We have fairly high dimensional exists and the JS component JS covariate measures an abundance score of a candidate binding site.",
            "So you put together a lot of candidates and at the end of the day you want to select the goodness and So what you do is or what we did is in run a variable selection in an additive model as Joe explained us yesterday we Model Y as an additive function of the excess plus error.",
            "Sample size is 287, P is not super high dimensional, but I think in many real good data problems, at least in high dimension.",
            "In biology these P = 1 million rarely occur I think.",
            "So member list is fairly high dimensional, so if you run this variable selection in this additive model, I think Sarah will say a bit more in the next talk.",
            "How we do this, and I think we have quite a reasonable algorithm and it selects 26 covariance, 26 variables Oracle.",
            "So then you would say OK, these 26 variables.",
            "These are the interesting candidates.",
            "The interesting binding sites on the DNA sequence and hence giving report these findings to the biologist."
        ],
        [
            "Right or really would you do it?"
        ],
        [
            "Do we trust this selection algorithm?",
            "And the other burning question is how stable are these findings?",
            "And to be honest, I mean I have worked on quite many biology problems.",
            "I would never report these 26.",
            "You really want to make sure that your findings are kind of more stable, more robust, and only then you report it to the biologist.",
            "Because it's quite a laborious work to do biological validation and so on and so forth.",
            "OK, so how can we address this question?",
            "Do we trust the algorithm?",
            "How stable are defined?"
        ],
        [
            "And I mean.",
            "I don't have so many great ideas.",
            "Let's just stick to the simple idea and let's use subsampling or bootstrapping.",
            "And we applied this to this biology problem.",
            "So if you run this sparse additive modeling which includes a variable selection scheme in is 287, P is almost 200.",
            "So in the first sub sample data set maybe select 15 variables in another 10.",
            "Some of the variables are selected over and over again, and of course at the end of the day we keep the variables or we declared him to be stable if they have a curd in say more than 80% of the subsampling run or 90%.",
            "Or in general Pi threshold time Times 100% among the subsampling grammars.",
            "So you would report these variables and in this particular data set.",
            "Only two stable findings remained, so initially you have 26.",
            "After this subsampling stability procedure only two stable findings remain, and actually these two stable findings there highly interesting that was kind of lucky.",
            "In this project one of the covariates.",
            "This is its corresponding estimated additive function.",
            "It's slightly nonlinear.",
            "This corresponds to the true known binding site, which is indicated by the other stable coherent corresponds to this additive function, which is almost linear, and this isn't.",
            "Interesting candidate, we have good additional support for relevance.",
            "Also for this covariate and we do biological validation because I told them hey this is stable variable.",
            "OK, so biology aside, this is kind of my introductory example."
        ],
        [
            "Promise statistical point of view.",
            "We want to answer the question how should we choose this threshold?",
            "You do this subsampling and then you need to ask yourself, should I keep the variables which occur in more than 80% of the time or 90 or what?"
        ],
        [
            "OK, so this is again a bit and outline what I want to show you today.",
            "In our experience, and I actually do this since quite a while, this subsampling, bootstrapping in the context of selection, yields results.",
            "And of course this is nothing else than the bagging procedure on the selection part.",
            "So layer Brian, we've heard about him yesterday, he did it on the prediction part.",
            "We just do it on this election part and it really works.",
            "So it worked in our many empirical examples about is fairly new is that we can now mathematically justify this procedure.",
            "It's a very simple procedure and it has very interesting properties.",
            "I think what we will be able to achieve and I will show you we can show finite sample control of the number of false positives when you select the variables list this subsampling scheme.",
            "And so in a sense, what we're doing is really a marriage of high dimensional selection algorithms like the lawsuit, Adamczyk selector, graphical modeling and so on, and finite sample error control."
        ],
        [
            "OK so here is what it is.",
            "Let's just be simple and consider a linear model.",
            "So here's my standard linear model.",
            "The only unusual thing is that potentially P is much larger than in and P can be even in the million something Sasha mentioned people 1000 people do compute this pina million.",
            "This is not a big issue anymore.",
            "Of course you compute a bit, so this is the only unusual thing about this linear model.",
            "Then we have to set of active variables.",
            "This is the set S, so these are the covariates with corresponding regression coefficients different from zero.",
            "They're not an ingredient.",
            "You have a variable selection procedure, and we denoted by S hat Lambda, so it's a selector, so it selects just a subset among my pico very as a subset of this index.",
            "Said want to pee and land as a tuning parameter.",
            "I want to be very general here, but of course a prime example his class so so beta hat, Lambda hysta, lasso estimator, you minimize residual sum of squares you penalized by the L1 norm.",
            "I think we all know that this shrink.",
            "Some of the coefficients exactly to 0.",
            "So we have a selection property and an S headlander is just the covariance with estimated regression coefficients different from zero, so there's no significance testing involved.",
            "This is just a plain optimization problem.",
            "OK, so the ingredient base we have a variable selection procedure, for example the LASSO and then."
        ],
        [
            "You move into this subsampling idea.",
            "You draw a subsample of size in half.",
            "Of course you can take other subsampling sizes, but that's not so crucial.",
            "I think you do sub sampling without replacement.",
            "We denote such a random sample by I star.",
            "Then we run this selection algorithm on this subsample I star.",
            "I do this many times and of course at the end of the day I can calculate relative selection frequencies, just the percentages that certain variables have been selected.",
            "During this subsampling runs.",
            "So mathematically, this is Pete's or that the coherent J has been selected by F. Hadland I star and the probability P star is with respect to the subsampling mechanism.",
            "Interestingly, here I think is we actually prove something just for any kind of finite number of subsampling which you actually do on the computer.",
            "It's not that you go over all in choosing their aim is to subsample size."
        ],
        [
            "So an obvious question could also use the Bootstrap sampling these replacement.",
            "Indeed, that's true.",
            "Subsampling this subsample size in half is very similar to bootstrap resampling, and I think this is also a tribute to David Friedman who recently passed away.",
            "He wrote a paper in the 60s I think about the total variation distance between Bootstrap sampling.",
            "I think the bootstrap didn't exist at that time, it was just sampling with replacement and subsampling the sample size in half.",
            "And the variation total variation distance is small.",
            "Listing away when you pick this up sampling sizing half OK.",
            "So Bootstrap sampling and subsampling is subsample size in half or pretty soon."
        ],
        [
            "OK, so now I need to say what is the stability selection?",
            "So typically of course think about Lasso.",
            "You can see there are many Landers, right?",
            "So you look at the whole path.",
            "Because you don't know what a good lamb days so you look at many regularization parameters over a whole state capital land and its capital and there can be discreet.",
            "This is what we compute.",
            "It can be a Singleton.",
            "Then it's not many lambdas or it can be continuous and then we have the stability selection definition S have stable.",
            "These are the covariates such that and then we take the maximum.",
            "Overall these lambdas in this capital and such that the selection frequencies.",
            "The maximum overall land that you're considering is figure in a certain threshold titration.",
            "So that's the definition of this stability selection.",
            "And the question is, how should we choose this paper?"
        ],
        [
            "OK, so this is the next slide.",
            "How should we choose this?",
            "And I think the theorem here will answer how we choose it in why we choose it in this way, but we need a bit of notation, so we denote by ishap capital Lambda the Union over its head, Lambda.",
            "Over all the little Landers.",
            "So typically you have almost a monotonicity, so if you make you Lambda larger and larger, you select fewer and fewer variables.",
            "So typically this is had capital, and that is something like this.",
            "Ace had landerman.",
            "The smaller slander you're considering, you're set.",
            "OK, this is 1 quantity, then the other.",
            "Is this Q capital Lambda and this is the expected value with respect to your bootstrapping or sub sampling procedure.",
            "Of this at Lambda High Star.",
            "So this is something you can compute automatically.",
            "You just take the average of your subsampling sizes.",
            "OK, then you denote by V the number of false positives.",
            "And then there is a theorem and a theorem.",
            "Joint work with Nicolai Meinshausen is there's an assumption which we call the exchangeability assumption.",
            "It will appear in the next slide.",
            "There's another assumption about your variable selection procedure, and this is a very mild assumption.",
            "It only has to be better than random guessing.",
            "I think this is again a Leo Bryman terminology, but so you just need to be a bit better than just uniformly drawing a covariant.",
            "OK and then.",
            "We can prove that the expected number of false positives is lower bounded by quantity.",
            "Now, this quantity involves the pie threshold in most P. So in the linear model, the number of covariates and what you see here also from the theorem is P. Large helps, that's great OK, and the Q Lambda is just the size of these subsampling selectors.",
            "Now, for example, in the last, so there's a very crude bound Q. Lambda is always list in in.",
            "And so Q Lambda squared is at most in squared, so this is at most N ^2 / / P. And if P is much larger than in, this is a pretty good pound.",
            "So now we're there.",
            "Of course, it's a conservative found, but it seems to work quite well in numerical examples, which I will show you.",
            "It's finite sample control.",
            "It's not Adam tatik.",
            "Even if P is huge and now we just choose the threshold.",
            "Of course you invert this formula so that you can control, for example, for the expected number of false positives list in one, you can control the familywise error rate by this argument.",
            "You can also control for other values, so of course now you have to fix your expected number of false positives, but I do think this is more objective and we kind of have a feeling how we want to control this.",
            "For example, familywise error rate.",
            "It's very conservative.",
            "But maybe you want to be on the conservative side."
        ],
        [
            "OK, so note the general idea of the theorem.",
            "It works for any method which is better than random guessing.",
            "I think that's kind of great.",
            "It works not only in regression.",
            "But in any.",
            "Quote discrete structure estimation problems.",
            "Whenever there is an include, exclude relation of course it can do exactly the same thing.",
            "You just count how many times you have included the same.",
            "So this includes variable selection graphical modeling I mean also.",
            "I mean graphical modeling index with directions clustering and so on and so forth.",
            "So this is really super general."
        ],
        [
            "Of course, if it's so super general, there must be a fairly strong condition.",
            "Otherwise I think it's fooling ourselves.",
            "And the condition is here.",
            "And what's nice about this condition?",
            "It's easy to say what it is.",
            "It's maybe not so easy to hold, but it's very easy to say what it is.",
            "It's what we call the exchangeability condition, and it just requires that the distribution of this thing here.",
            "So we take the indicator.",
            "We look better.",
            "Choraria J is selected by the selector is had Lambda and we only look at Jays being non active.",
            "The noise covariance for the distribution of among these noise cover it has to be exchanged.",
            "So this is maybe a pretty strong assumption.",
            "What is kind of nice about this assumption away.",
            "I think there is only this is only a requirement for the noise covariance.",
            "Now you may wonder, come on.",
            "I mean if you have active queries which are perfectly correlated or almost perfectly correlated, what's happening here?",
            "This is allowed, but if you have very strongly correlated variables saying a linear model with Lasso, so about 50% of the time, the one is chosen about the other 50% of the time the other, so they cannot be stable.",
            "Highly correlated variables cannot be stable.",
            "So for specific problems, of course we can give kind of weaker assumptions, but I think this kind of nice.",
            "Just the single exchangeability assumption, and it works in this great generality."
        ],
        [
            "OK, so this is a toy example.",
            "Very condition holds.",
            "Look at the linear model with a random design.",
            "And then you random design covariates you require that they are exchangeable, right?",
            "And an easy example there is hold this, for example, E correlation among the noise covariance.",
            "So just to convince you there are examples which fulfill our conditions.",
            "It's not an empty condition."
        ],
        [
            "So how does this work?",
            "So I will have quite many pictures.",
            "I learned this from Yankee two weeks ago, so he gave some wonderful talk about many pictures and figures, and I tried to convince you of it with pictures that this is kind of an interesting method.",
            "So here is a range of scenarios for variable selection in linear models using the lasso.",
            "I think we've seen that now many times.",
            "P is 660.",
            "What I think is interesting here.",
            "This is from a real design data matrix, so this is about this multiplication problem.",
            "We take this X design matrix 660 sixty in is up to 7:50.",
            "We choose two different ends for 5750 and then we make artificial wise by just creating some kind of simulated betas and we arranged the sparsity the number of active variables in steps up A4, four 812 up to 40.",
            "Ann Singleton noise ratio.",
            "This is something which is never reported by people giving talks on how it performs, or it's rarely reported.",
            "Many people look at scenarios where signal to noise ratio is 5 or 10 and we were really interested in this kind of thing like signal to noise ratio, .25 one and four.",
            "So in total we have 60 scenarios with this P with these two ends with a couple of sparsity degrees and different signal to noise ratios."
        ],
        [
            "There's a picture, and here's the picture again.",
            "So what does the picture mean?",
            "So you see, black dots and red triangles and you see a line, and so each of these pairs Vista line.",
            "This is 1 scenario, so there are 60 scenarios.",
            "So each of these pair with a line.",
            "This is 1 scenario.",
            "Now the black dot man maybe the axis.",
            "So the X axis are the number of false positives.",
            "OK, this is bad if it's large and the Y axis is the proportion of true positives up is good.",
            "Now the black dots.",
            "This is the solution.",
            "Meditate cross validation tuning for the last so of course we know these pics too large models, but just for a comparison I think this is a good way to look at it and the Red triangle system stability selection.",
            "This this Lambda OK and in this scenario and So what we see from this picture is we have accurate control.",
            "So here's the line we control at the expected number of false positives are 2.5.",
            "This is the vertical line and this is very accurate control.",
            "As proved by our theory, so this works.",
            "Maybe this exchange ability condition doesn't hold, but it seems to hold up pretty well in this practical example.",
            "Make think there is a drastic reduction of false positives so has validated.",
            "Tune solutions to.",
            "Can you reduce a lot of false positives?",
            "Obviously there is a bit of power loss, but if you look at it, there's not much loss in terms of power.",
            "So if the line would be very steep upwards, there would be a lot of loss, but the lines are pretty flat, so I would claim there is not much loss in terms of power."
        ],
        [
            "I want to convince you really, this is a very general purpose tool and this is always dangerous general purpose tools, but I think in this setting I'm quite convinced this is an interesting tool which can be applied in very many signals.",
            "So this is graphical modeling using the Gmail app, so I think Peter Bickel calls it Spice algorithm and I'll sounds a bit better spicy stuff, so OK.",
            "So what it is if you don't know much about graphical modeling, it's an L1 penalty type technique to infer the so called conditional independence graph.",
            "So it's kind of a generally that isation of lasso for regression to graphical models.",
            "There's that Lambda parameter in your spice orgy lasso algorithm, and that's here.",
            "So if you vary your Lambda, you get denser graphs or kind of listing scraps.",
            "Obviously.",
            "Now if you compute the stability selection solution of this with this land, that here is the stability selection for that landed here and there, and so the graph is pretty stable, right?",
            "This is what I want to achieve.",
            "And the conclusion is the stability selection.",
            "The choice of this initial Lambda tuning does really not matter much as we can actually prove in our theory and what you need to fix.",
            "Of course, is your finite sample control.",
            "If you make a more stringent finite sample control, you get as far as a graph and vice versa.",
            "But I do think this is a more objective way to judge how big we should take the graph."
        ],
        [
            "So this is the null situation.",
            "This is actually a true underlying day."
        ],
        [
            "To say we don't know what the true graph."
        ],
        [
            "But you can of course.",
            "Mute your data, set the structure and indeed through graph is the empty graph.",
            "And then with the Lambda is just really terrible.",
            "In this example, is the empty graph.",
            "Of course, if you make Lambda larger and larger at some point, you could also an empty graph, but it's very hard to know if you don't know that the through graph is the empty graph.",
            "I think you would not find it.",
            "The stability selection just called here a couple false positives, but in general we really can control the number the expected number of false positives as proof by our theory."
        ],
        [
            "OK, it holds in greater generality clustering.",
            "So of course this is also a discrete structure estimation problem.",
            "You would just want to know whether two points are in the same class.",
            "Three yes or no, and this can be also written as a graph, so the vertices or the sample points and the sample points are in the same cluster.",
            "Then you draw an edge between the vertices so it's again a graph type problem.",
            "This is how we implemented.",
            "So this is a toy example, but it's kind of a neat example.",
            "I think it's only two dimensional and the true clustering structure is down here, so there's a cluster hearing rate, the clustering black and kind of two Singleton clusters, just two single points, 1 green guy here, 1 blue guy here.",
            "This is kind of the model from Gaussians into outliers.",
            "So now just let's apply K means for example to this problem.",
            "So this is K means K equal 2.",
            "Of course we cannot find the four clusters K = 3.",
            "We cannot find the full cost of K equals.",
            "This is the right number of clusters, but we don't get the structure because these are so strange points here we don't get these structures K equals six K = 8.",
            "Now let's do stability selection on K = 2.",
            "Get the structure K = 2 doesn't mean after stability selection you only have two clusters.",
            "This is not true, we just get a fundamentally different solution Kafel 3 same class through Class 4 through cluster six through cluster and at some point it deteriorates as well.",
            "If you regularization parameter, the initial parameter is too bad.",
            "Stability selection cannot recover it."
        ],
        [
            "Another example, the lasso and its corresponding stability regularization path.",
            "This is again a real example.",
            "It's about vitamin production visit.",
            "Bacillus P is more than 4000.",
            "We have 115 samples.",
            "And.",
            "So this in collaboration with Dutch state mining company and so they say these six clear, it's pretty sure that they they are here D6 jeans.",
            "So we take these six jeans and be.",
            "So suppose there true.",
            "And we permute all the other stuff.",
            "So this is kind of this kind of semi simulated data set.",
            "So we know or we hope that the six ones are the true guys there indicated in red the path and the others are for sure noise clearance.",
            "Because we have renewed it.",
            "So this is lost, so if you're not familiar with it, you just compute your lasso coefficients as a function of your Lambda, and then if you land at large, you have nothing selected in the first variable comes up, and so on.",
            "So the true variables they indeed come up, but then it mingles Vista noise and becomes very hard to choose a good Lambda.",
            "The true variables do not stick out that greatly from the noise coherence.",
            "This is stability selection on the lasso and just from the picture it's clear that the true rate curves they stick out much more clearly from the noise coherence, and this is a further improvement, which actually I think has some relations to Sasha's talk, and then I will come to that this is stability selection with an additional randomization with stability selection, Mr randomized LASSO, and this is even a bit better.",
            "I think these six lines here even.",
            "Stick out a bit further from the noise covariance.",
            "The black lines in here.",
            "So the conclusion here be stability selection.",
            "At least four up to six through variables are sticking out much more clear."
        ],
        [
            "I think this I said it already, but I think it's important to say stability selection cannot be represented by simply selecting a good land."
        ],
        [
            "Course not if you look at this path, you cannot click the Lambda."
        ],
        [
            "And it gives you the same solution.",
            "It really provides a fundamentally different solution."
        ],
        [
            "OK, So what about this?",
            "Further improvement, and this randomized lasso and it has a lot to do with the stability selection and the way we detect this was really the first invented.",
            "What I just told by now and then we saw that additional randomization can help.",
            "So let's discuss this issue with linear models and glass.",
            "So we have a linear model, high dimensional and we consider the last so.",
            "And variable selection consistency, as we've heard, I think many times means the probability that you selected set of variables equals to through one converges to 1.",
            "Sample size tends to Infinity and I think it's alone by now that variable selection is the last, so it needs in necessary in the sufficient condition.",
            "We called it the neighborhood stability condition.",
            "Later Ping John been you?",
            "They kind of reformulated our ugly condition and made it to a nicer understandable condition.",
            "And this is what's called nowadays the representable condition.",
            "So it is a condition on your design.",
            "And I'm meeting the details here.",
            "I think Sasha already hinted at some of the stuff here.",
            "I just want to say it is restrictive.",
            "It's a restrictive condition on the design, so lots of works if and only if their restrictive condition on the design holds."
        ],
        [
            "And while this is no surprise, assuming this strong in representable condition, we can also prove the variable selection Vista stability.",
            "Issue is the stability subsampling is consistent as well.",
            "No big surprise, so you don't make it worse.",
            "From a practical point of view, what you gain is this insensitivity disrespect to the choice of Lambda and a more objective way how you should choose the number of variables."
        ],
        [
            "OK, but that's not the end of the story.",
            "From a theory and practical perspective, we can improve by additional randomization.",
            "I think Sasha calls it also perturbation and I was not aware of his working until three weeks.",
            "It's not the same, but I think it bears quite a lot of similarities here."
        ],
        [
            "OK, so this is the procedure randomized lasso.",
            "So what it is, it's kind of a weighted lasso, and we have weights and the weights are totally dumb weights.",
            "In a way it's just IID random variables for every coherent and it just take values in the range Alpha to one, OK?",
            "And Alpha is what we call the weakness parameter.",
            "And I would propose actually how we implement it.",
            "We make each two values for this W. Either it is equal to Alpha or one with corresponding probabilities PW or 1 -- P W. So you have these weights and then you have this randomized lasso.",
            "It's of course the comics optimization problem.",
            "You minimize residual sum of squares and you penalized by these weighted penalty.",
            "So if WJ is 1, it's the ordinary penalty.",
            "And if WJ is Alpha small you increase the penalty.",
            "So this is a weighted lasso, AKA the adaptive lasso, which has been proposed by Joe.",
            "But so of course took a Daydream and good.",
            "WJ there is.",
            "We take it totally stupid WJ reaches take them totally random.",
            "And of course, as a standalone procedure, this is absolutely nonsensical.",
            "This is pretty stupid like this."
        ],
        [
            "But you can recover the stupidness Mr Stability selection and you gain something with it.",
            "OK, in connection with stability selection."
        ],
        [
            "This will improve.",
            "I'm here is in an example.",
            "Artificielle two active variables which satisfy the strong is representable representable condition, one bat covariate which violates the representable condition.",
            "This is in blue 197.",
            "Noise covariance in is 200.",
            "This is the stability selection pass with the lasso and this is the stability selection path with the randomized lasso.",
            "And what happens here?",
            "Stability selection missed the last, so you cannot get rid of the blue guy.",
            "It violates the representable condition and even stability selection doesn't help here.",
            "But if you do the randomized lasso, you can kind of decrease this effect.",
            "So if you choose a large threshold set .7, the only recover the two through active variables of course."
        ],
        [
            "This is now official example.",
            "We ran quite a few of simulated models and.",
            "I think this is kind of the fair figure.",
            "Sometimes you gain something like 70% in reducing the number of false positives.",
            "Sometimes you gain nothing, essentially 3%.",
            "Let's say a 0% but.",
            "At least what we've seen so far.",
            "We never lost style."
        ],
        [
            "OK, so here is the theorem.",
            "This is variable selection consistency with the randomized lasso.",
            "And so we look at these randomized lasso it past his weakness parameter Alpha.",
            "And of course we need a couple assumptions.",
            "The assumptions as we take a fixed design in our linear model.",
            "We work with this sparse eigenvalue condition because Nikolai this is Nicholas kind of fruit.",
            "We are aware your condition is a bit weaker, but nevertheless I think that is not such a restrictive condition.",
            "It seems that this is much less restrictive than, say, this neighborhood stability.",
            "Or your representable condition.",
            "OK, then the state of Lambda, how so?",
            "You only need to make an assumption on the minimal and either you take very large lambdas, doesn't matter.",
            "Because you you take the you know all these things.",
            "So the Lambda mean should be up for certain form.",
            "Then there is this issue that the minimal non 0 beta doesn't have to be too small and this is certainly not tight.",
            "I think we wouldn't want to get rid of these S to the power three half.",
            "P much larger than a nice of course allowed and then the theorem states for suitable range of thresholds pie threshold you have variable selection consistency.",
            "You can recover the true set.",
            "I should say I mean the assumptions are maybe not minimal, but the difficulty here is view analyzing an algorithm.",
            "So we have this last so and this subsampling and I think at least.",
            "With my skills I find it much harder than analyzing, say, a lasso, which is clearly defined as an optimization procedure."
        ],
        [
            "OK, so why does it work?",
            "And I think Sasha gave in a lady answered.",
            "I kind of minion here.",
            "I've heard him again.",
            "They said OK, this is part of the answer.",
            "Somehow we perturb also our design matrix in.",
            "Obviously the randomized lasso.",
            "It's a way that lasts.",
            "Oh, so this can always be rewritten.",
            "Just re scale your covariates and then you have a standard lasso problem, right?",
            "And so we kind of have a randomized perturbed design now.",
            "So you have WJ XJ is our new covert XWJ and now what's happening mathematically is as follows.",
            "If you would make this weakness parameter lower and lower, the design gets verse and verse in terms of.",
            "Say the condition number, the ratio between the maximal and minimal sparse eigenvalue gets worse, so a smaller Alpha is bad, so don't make it too bad there is a lower limit and don't go below this lower meaning Alpha low of course is lower limit as N gets large, it tends to 0, but there is a lower limit.",
            "You shouldn't make it 2 week.",
            "OK, so I think it's kind of plausible that if you don't make it too bad, you still can catch the true guys because they're still strong enough.",
            "So the question is really, how can we get rid of this blue guy?",
            "This one here.",
            "So how can we make it like this?",
            "And.",
            "Part of the answer is as follows.",
            "What you also can prove, and I think this matches perfectly what you are seeing.",
            "In this perturbed design, if you perturb.",
            "Kind of surprisingly, the representable condition holds sometimes.",
            "And this sometimes can be characterized.",
            "It's a certain set script B, so sometimes the representability condition holds.",
            "It holds you don't pick up annoys, coherent, OK. Then you're doing the right thing even when it is not fulfilled for the original data.",
            "And so the noise clear it's what's happening.",
            "Is there sometimes not select?",
            "It's correctly, sometimes, not always.",
            "So far we have only sometimes, but this sometimes doesn't have two small probabilities, so it happens sometimes.",
            "If you do stability selection many times, this sometimes means that if you sometimes do not select it, you cannot stabilize select it so.",
            "It said it's not a chicken and egg sentence, which I'm saying here.",
            "It's kind of maybe needs 3 minutes.",
            "Looking at this in a quiet time, but we don't have to quiet time, but the issue is Alpha small.",
            "Makes it possible that the represent ability starts to hold even if it doesn't hold for the original data."
        ],
        [
            "Conclusions stability selection.",
            "I think it really did races in a way.",
            "Also the need in particularly high dimensional problems to say something about how many inches, how many variables are?",
            "How are the clusters looking like?",
            "We need to stabilize this and I think this is kind of an interesting, very easy to do and very easy to.",
            "To do procedure, I mean I think people have done this since years and I think our contribution is here to have a more solid mathematical foundation for this.",
            "So it's very generic.",
            "We have finite sample error control when estimating any kind of discrete structures.",
            "When I say variable selection, graphical modeling, clustering and so on so forth.",
            "And the other part is this additional randomization.",
            "This perturbation of the design matrix really helps.",
            "We did it in terms of these randomized lasso.",
            "I think Sasha is this is model on this kind of noisy on observable covariates.",
            "What we call also a randomized weakening.",
            "That's why we call it a weakness parameter.",
            "It's really randomly giving a high penalty and that kind of helps a lot.",
            "Maybe there is a connection to random forests.",
            "I put that in yesterday night after you talk because random forests also has this randomization step on the covariates and not only under sampling points.",
            "Thank you.",
            "Well, give me.",
            "Distance from escape from Mississippi.",
            "This multiplicative noise solution for the same role by role, yes, so.",
            "Maybe one can have better schemes, but yeah, it's right.",
            "It's the same.",
            "Yeah.",
            "It is the same as total Delta in that like I, I think no, it's not your Dell, that's your other stuff, which is the easy stuff really.",
            "So in our experience take Alpha, .5 or .2.",
            "Don't make it so.",
            "We have not had that much experience.",
            "I think we ran.",
            "Maybe run quite a few simulations, but it seems like.",
            "The big issue, of course, is in the simulation study how you choose your finance sample control.",
            "That's that's it.",
            "Into weather how you choose you Alpha and also your land.",
            "I agree there are two parameters involves an Alpha and Lambda.",
            "Lambda seems really very insensitive the Alpha.",
            "Just take .2 I think but as a wakes suggestion in our experience so far, it doesn't matter very much.",
            "But of course you are.",
            "I mean it's a bit maybe."
        ],
        [
            "Maybe I'm a bit fast and paying this.",
            "Of course this thing is happening so there are two things.",
            "Low Alpha is good with regard to the representable condition, but it's bad in terms of this maximal minimal eigenvalue eigen values in.",
            "So mathematically, you need to be careful how to choose Alpha.",
            "Next I'm not sure, but then the central theorem, but there's something is bothering me.",
            "What could win the provincial from maybe as many valuable as you like and make me as loud as you can?",
            "Just whatever be believable you randomize.",
            "Music random generator.",
            "So I think good point, never thought in terms of the finite sample error control, no problem because there we only talk about false positives.",
            "But of course I guess there's a power loss.",
            "But finite sample control.",
            "This is just.",
            "You can do it.",
            "The designs that help you.",
            "The interesting really sign suspect, I mean, are there some that really screw up?",
            "And if you think the multiple select the wrong ones or is it more gradual?",
            "I mean if you block something like.",
            "I suppose you know the truth.",
            "Second, you got something like OK with this Rosa PS.",
            "Then I select this phone variable.",
            "I don't quite understand, maybe your remark, but I mean first of all, this is real.",
            "The real example."
        ],
        [
            "But of course, in order to understand what is good or not, we permute lots of this stuff, so I know the black things here.",
            "There, noise coherence.",
            "This is just kind of more to understand.",
            "The property for these real procedure and I do not really take out rows or columns in my design matrix, I just.",
            "Randomly give higher or ordinary penalties to these variables.",
            "You know why I think this smears out the range of.",
            "Individual penalty parameters for individual covariates, but of course you do it randomly and do not cover everything.",
            "Ideally would like to maybe a penalize every covariate in a different manner, but of course you cannot do that and this randomized.",
            "Method kind of smears it out to a range of different values.",
            "So maybe we should talk in a break.",
            "Do you have in mind that there's some serious pre processing preselection process before you start the whole thing?",
            "Or are you just saying well all of them do selection?",
            "Process this data within this destination.",
            "Now, I would argue the other way around.",
            "I think I would take this as my pre processor.",
            "Now you have to use select variables and now you do re estimation.",
            "However you would like to do you have a very sparse set of variables.",
            "Other peoples are saying something.",
            "I mean this stability selection you could put in anything such that the procedure is better in random guessing, no problem, but.",
            "Yeah, yeah no, no this is just ensuring that the theorem holds and it's again.",
            "I think the power question.",
            "I mean whether you did a good job or not in terms of false positives, you cannot control it.",
            "But in terms of power you may lose something in so of course the power question comes in by the theory.",
            "I mean for variable selection consistency you need power and.",
            "So here is a saying."
        ],
        [
            "What's happening if I would do some kind of pre processing first?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would like to thank the organizers also for having invited me to this wonderful workshop.",
                    "label": 0
                },
                {
                    "sent": "I really enjoy being here.",
                    "label": 0
                },
                {
                    "sent": "And my talk is about stability selection and I think it's a good idea to start recent example.",
                    "label": 1
                },
                {
                    "sent": "What I mean by Stew?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I think this is indeed one example of stability.",
                    "label": 0
                },
                {
                    "sent": "This is a highly stable object.",
                    "label": 1
                },
                {
                    "sent": "I think he's not flipping his opinion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He will not so easily change his mind.",
                    "label": 1
                },
                {
                    "sent": "Jahnke just told me he's actually waiting here in London for his next airplane.",
                    "label": 0
                },
                {
                    "sent": "Going to Tel Aviv.",
                    "label": 0
                },
                {
                    "sent": "And so this is indeed what we're achieving for.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's actually a little how can I make the screen larger control shift?",
                    "label": 0
                },
                {
                    "sent": "P. OK, so congratulations to the Home book prize.",
                    "label": 1
                },
                {
                    "sent": "I think that's the right place to say this and I'm really pleased to hear that you got this prestigious prize.",
                    "label": 0
                },
                {
                    "sent": "So let's put Joe.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Listen Yankee aside, and let's talk about my talk.",
                    "label": 0
                },
                {
                    "sent": "So what I want to explain you and show you is that we want to do some kind of better detection, better sparse recovery of a pattern of variables are off components or factors in a model.",
                    "label": 0
                },
                {
                    "sent": "And the talk will also address kind of the Earth.",
                    "label": 0
                },
                {
                    "sent": "Burning question, I mean, how many variables should be actually having our model?",
                    "label": 0
                },
                {
                    "sent": "How many components?",
                    "label": 0
                },
                {
                    "sent": "How many edges in a graph?",
                    "label": 1
                },
                {
                    "sent": "And so roughly speaking, what I will do today is I will talk about regularised estimation for discrete structures in a high dimensional setting.",
                    "label": 1
                },
                {
                    "sent": "And I do think this is very relevant in many practical applications, and it is often also substantially more difficult in prediction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is another example, not Yankees example.",
                    "label": 0
                },
                {
                    "sent": "This is a biology example.",
                    "label": 0
                },
                {
                    "sent": "I'm aware that this is more linked to econometrics, but let's just go with this little example.",
                    "label": 0
                },
                {
                    "sent": "It's an example where we actually found quite a lot of success by applying this lawsuit type.",
                    "label": 0
                },
                {
                    "sent": "L1 penalization methods example is about finding a transcription factor binding site in the DNA sequence.",
                    "label": 1
                },
                {
                    "sent": "So roughly speaking, this is just a site where a protein can bind to the DNA, and I am omitting many details here.",
                    "label": 0
                },
                {
                    "sent": "I just tell you.",
                    "label": 0
                },
                {
                    "sent": "This is a regression type of problem.",
                    "label": 0
                },
                {
                    "sent": "We will have unique varied response variables Y.",
                    "label": 1
                },
                {
                    "sent": "They measure some kind of binding intensity on a course DNA statement.",
                    "label": 1
                },
                {
                    "sent": "We have fairly high dimensional exists and the JS component JS covariate measures an abundance score of a candidate binding site.",
                    "label": 0
                },
                {
                    "sent": "So you put together a lot of candidates and at the end of the day you want to select the goodness and So what you do is or what we did is in run a variable selection in an additive model as Joe explained us yesterday we Model Y as an additive function of the excess plus error.",
                    "label": 0
                },
                {
                    "sent": "Sample size is 287, P is not super high dimensional, but I think in many real good data problems, at least in high dimension.",
                    "label": 1
                },
                {
                    "sent": "In biology these P = 1 million rarely occur I think.",
                    "label": 0
                },
                {
                    "sent": "So member list is fairly high dimensional, so if you run this variable selection in this additive model, I think Sarah will say a bit more in the next talk.",
                    "label": 1
                },
                {
                    "sent": "How we do this, and I think we have quite a reasonable algorithm and it selects 26 covariance, 26 variables Oracle.",
                    "label": 1
                },
                {
                    "sent": "So then you would say OK, these 26 variables.",
                    "label": 0
                },
                {
                    "sent": "These are the interesting candidates.",
                    "label": 0
                },
                {
                    "sent": "The interesting binding sites on the DNA sequence and hence giving report these findings to the biologist.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right or really would you do it?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do we trust this selection algorithm?",
                    "label": 1
                },
                {
                    "sent": "And the other burning question is how stable are these findings?",
                    "label": 0
                },
                {
                    "sent": "And to be honest, I mean I have worked on quite many biology problems.",
                    "label": 0
                },
                {
                    "sent": "I would never report these 26.",
                    "label": 0
                },
                {
                    "sent": "You really want to make sure that your findings are kind of more stable, more robust, and only then you report it to the biologist.",
                    "label": 0
                },
                {
                    "sent": "Because it's quite a laborious work to do biological validation and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "OK, so how can we address this question?",
                    "label": 0
                },
                {
                    "sent": "Do we trust the algorithm?",
                    "label": 0
                },
                {
                    "sent": "How stable are defined?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I mean.",
                    "label": 0
                },
                {
                    "sent": "I don't have so many great ideas.",
                    "label": 0
                },
                {
                    "sent": "Let's just stick to the simple idea and let's use subsampling or bootstrapping.",
                    "label": 1
                },
                {
                    "sent": "And we applied this to this biology problem.",
                    "label": 1
                },
                {
                    "sent": "So if you run this sparse additive modeling which includes a variable selection scheme in is 287, P is almost 200.",
                    "label": 1
                },
                {
                    "sent": "So in the first sub sample data set maybe select 15 variables in another 10.",
                    "label": 1
                },
                {
                    "sent": "Some of the variables are selected over and over again, and of course at the end of the day we keep the variables or we declared him to be stable if they have a curd in say more than 80% of the subsampling run or 90%.",
                    "label": 1
                },
                {
                    "sent": "Or in general Pi threshold time Times 100% among the subsampling grammars.",
                    "label": 0
                },
                {
                    "sent": "So you would report these variables and in this particular data set.",
                    "label": 0
                },
                {
                    "sent": "Only two stable findings remained, so initially you have 26.",
                    "label": 0
                },
                {
                    "sent": "After this subsampling stability procedure only two stable findings remain, and actually these two stable findings there highly interesting that was kind of lucky.",
                    "label": 0
                },
                {
                    "sent": "In this project one of the covariates.",
                    "label": 1
                },
                {
                    "sent": "This is its corresponding estimated additive function.",
                    "label": 0
                },
                {
                    "sent": "It's slightly nonlinear.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to the true known binding site, which is indicated by the other stable coherent corresponds to this additive function, which is almost linear, and this isn't.",
                    "label": 0
                },
                {
                    "sent": "Interesting candidate, we have good additional support for relevance.",
                    "label": 1
                },
                {
                    "sent": "Also for this covariate and we do biological validation because I told them hey this is stable variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so biology aside, this is kind of my introductory example.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Promise statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "We want to answer the question how should we choose this threshold?",
                    "label": 0
                },
                {
                    "sent": "You do this subsampling and then you need to ask yourself, should I keep the variables which occur in more than 80% of the time or 90 or what?",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is again a bit and outline what I want to show you today.",
                    "label": 0
                },
                {
                    "sent": "In our experience, and I actually do this since quite a while, this subsampling, bootstrapping in the context of selection, yields results.",
                    "label": 1
                },
                {
                    "sent": "And of course this is nothing else than the bagging procedure on the selection part.",
                    "label": 0
                },
                {
                    "sent": "So layer Brian, we've heard about him yesterday, he did it on the prediction part.",
                    "label": 0
                },
                {
                    "sent": "We just do it on this election part and it really works.",
                    "label": 0
                },
                {
                    "sent": "So it worked in our many empirical examples about is fairly new is that we can now mathematically justify this procedure.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple procedure and it has very interesting properties.",
                    "label": 0
                },
                {
                    "sent": "I think what we will be able to achieve and I will show you we can show finite sample control of the number of false positives when you select the variables list this subsampling scheme.",
                    "label": 1
                },
                {
                    "sent": "And so in a sense, what we're doing is really a marriage of high dimensional selection algorithms like the lawsuit, Adamczyk selector, graphical modeling and so on, and finite sample error control.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so here is what it is.",
                    "label": 0
                },
                {
                    "sent": "Let's just be simple and consider a linear model.",
                    "label": 0
                },
                {
                    "sent": "So here's my standard linear model.",
                    "label": 1
                },
                {
                    "sent": "The only unusual thing is that potentially P is much larger than in and P can be even in the million something Sasha mentioned people 1000 people do compute this pina million.",
                    "label": 0
                },
                {
                    "sent": "This is not a big issue anymore.",
                    "label": 0
                },
                {
                    "sent": "Of course you compute a bit, so this is the only unusual thing about this linear model.",
                    "label": 0
                },
                {
                    "sent": "Then we have to set of active variables.",
                    "label": 1
                },
                {
                    "sent": "This is the set S, so these are the covariates with corresponding regression coefficients different from zero.",
                    "label": 0
                },
                {
                    "sent": "They're not an ingredient.",
                    "label": 0
                },
                {
                    "sent": "You have a variable selection procedure, and we denoted by S hat Lambda, so it's a selector, so it selects just a subset among my pico very as a subset of this index.",
                    "label": 1
                },
                {
                    "sent": "Said want to pee and land as a tuning parameter.",
                    "label": 0
                },
                {
                    "sent": "I want to be very general here, but of course a prime example his class so so beta hat, Lambda hysta, lasso estimator, you minimize residual sum of squares you penalized by the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "I think we all know that this shrink.",
                    "label": 0
                },
                {
                    "sent": "Some of the coefficients exactly to 0.",
                    "label": 0
                },
                {
                    "sent": "So we have a selection property and an S headlander is just the covariance with estimated regression coefficients different from zero, so there's no significance testing involved.",
                    "label": 1
                },
                {
                    "sent": "This is just a plain optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the ingredient base we have a variable selection procedure, for example the LASSO and then.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You move into this subsampling idea.",
                    "label": 0
                },
                {
                    "sent": "You draw a subsample of size in half.",
                    "label": 1
                },
                {
                    "sent": "Of course you can take other subsampling sizes, but that's not so crucial.",
                    "label": 1
                },
                {
                    "sent": "I think you do sub sampling without replacement.",
                    "label": 0
                },
                {
                    "sent": "We denote such a random sample by I star.",
                    "label": 0
                },
                {
                    "sent": "Then we run this selection algorithm on this subsample I star.",
                    "label": 0
                },
                {
                    "sent": "I do this many times and of course at the end of the day I can calculate relative selection frequencies, just the percentages that certain variables have been selected.",
                    "label": 1
                },
                {
                    "sent": "During this subsampling runs.",
                    "label": 1
                },
                {
                    "sent": "So mathematically, this is Pete's or that the coherent J has been selected by F. Hadland I star and the probability P star is with respect to the subsampling mechanism.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, here I think is we actually prove something just for any kind of finite number of subsampling which you actually do on the computer.",
                    "label": 0
                },
                {
                    "sent": "It's not that you go over all in choosing their aim is to subsample size.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an obvious question could also use the Bootstrap sampling these replacement.",
                    "label": 1
                },
                {
                    "sent": "Indeed, that's true.",
                    "label": 0
                },
                {
                    "sent": "Subsampling this subsample size in half is very similar to bootstrap resampling, and I think this is also a tribute to David Friedman who recently passed away.",
                    "label": 0
                },
                {
                    "sent": "He wrote a paper in the 60s I think about the total variation distance between Bootstrap sampling.",
                    "label": 1
                },
                {
                    "sent": "I think the bootstrap didn't exist at that time, it was just sampling with replacement and subsampling the sample size in half.",
                    "label": 0
                },
                {
                    "sent": "And the variation total variation distance is small.",
                    "label": 1
                },
                {
                    "sent": "Listing away when you pick this up sampling sizing half OK.",
                    "label": 0
                },
                {
                    "sent": "So Bootstrap sampling and subsampling is subsample size in half or pretty soon.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I need to say what is the stability selection?",
                    "label": 0
                },
                {
                    "sent": "So typically of course think about Lasso.",
                    "label": 0
                },
                {
                    "sent": "You can see there are many Landers, right?",
                    "label": 0
                },
                {
                    "sent": "So you look at the whole path.",
                    "label": 0
                },
                {
                    "sent": "Because you don't know what a good lamb days so you look at many regularization parameters over a whole state capital land and its capital and there can be discreet.",
                    "label": 0
                },
                {
                    "sent": "This is what we compute.",
                    "label": 1
                },
                {
                    "sent": "It can be a Singleton.",
                    "label": 0
                },
                {
                    "sent": "Then it's not many lambdas or it can be continuous and then we have the stability selection definition S have stable.",
                    "label": 0
                },
                {
                    "sent": "These are the covariates such that and then we take the maximum.",
                    "label": 0
                },
                {
                    "sent": "Overall these lambdas in this capital and such that the selection frequencies.",
                    "label": 0
                },
                {
                    "sent": "The maximum overall land that you're considering is figure in a certain threshold titration.",
                    "label": 0
                },
                {
                    "sent": "So that's the definition of this stability selection.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how should we choose this paper?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the next slide.",
                    "label": 0
                },
                {
                    "sent": "How should we choose this?",
                    "label": 0
                },
                {
                    "sent": "And I think the theorem here will answer how we choose it in why we choose it in this way, but we need a bit of notation, so we denote by ishap capital Lambda the Union over its head, Lambda.",
                    "label": 0
                },
                {
                    "sent": "Over all the little Landers.",
                    "label": 0
                },
                {
                    "sent": "So typically you have almost a monotonicity, so if you make you Lambda larger and larger, you select fewer and fewer variables.",
                    "label": 0
                },
                {
                    "sent": "So typically this is had capital, and that is something like this.",
                    "label": 0
                },
                {
                    "sent": "Ace had landerman.",
                    "label": 0
                },
                {
                    "sent": "The smaller slander you're considering, you're set.",
                    "label": 0
                },
                {
                    "sent": "OK, this is 1 quantity, then the other.",
                    "label": 0
                },
                {
                    "sent": "Is this Q capital Lambda and this is the expected value with respect to your bootstrapping or sub sampling procedure.",
                    "label": 0
                },
                {
                    "sent": "Of this at Lambda High Star.",
                    "label": 0
                },
                {
                    "sent": "So this is something you can compute automatically.",
                    "label": 0
                },
                {
                    "sent": "You just take the average of your subsampling sizes.",
                    "label": 0
                },
                {
                    "sent": "OK, then you denote by V the number of false positives.",
                    "label": 1
                },
                {
                    "sent": "And then there is a theorem and a theorem.",
                    "label": 0
                },
                {
                    "sent": "Joint work with Nicolai Meinshausen is there's an assumption which we call the exchangeability assumption.",
                    "label": 0
                },
                {
                    "sent": "It will appear in the next slide.",
                    "label": 0
                },
                {
                    "sent": "There's another assumption about your variable selection procedure, and this is a very mild assumption.",
                    "label": 0
                },
                {
                    "sent": "It only has to be better than random guessing.",
                    "label": 1
                },
                {
                    "sent": "I think this is again a Leo Bryman terminology, but so you just need to be a bit better than just uniformly drawing a covariant.",
                    "label": 0
                },
                {
                    "sent": "OK and then.",
                    "label": 0
                },
                {
                    "sent": "We can prove that the expected number of false positives is lower bounded by quantity.",
                    "label": 0
                },
                {
                    "sent": "Now, this quantity involves the pie threshold in most P. So in the linear model, the number of covariates and what you see here also from the theorem is P. Large helps, that's great OK, and the Q Lambda is just the size of these subsampling selectors.",
                    "label": 0
                },
                {
                    "sent": "Now, for example, in the last, so there's a very crude bound Q. Lambda is always list in in.",
                    "label": 0
                },
                {
                    "sent": "And so Q Lambda squared is at most in squared, so this is at most N ^2 / / P. And if P is much larger than in, this is a pretty good pound.",
                    "label": 0
                },
                {
                    "sent": "So now we're there.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's a conservative found, but it seems to work quite well in numerical examples, which I will show you.",
                    "label": 0
                },
                {
                    "sent": "It's finite sample control.",
                    "label": 1
                },
                {
                    "sent": "It's not Adam tatik.",
                    "label": 0
                },
                {
                    "sent": "Even if P is huge and now we just choose the threshold.",
                    "label": 0
                },
                {
                    "sent": "Of course you invert this formula so that you can control, for example, for the expected number of false positives list in one, you can control the familywise error rate by this argument.",
                    "label": 0
                },
                {
                    "sent": "You can also control for other values, so of course now you have to fix your expected number of false positives, but I do think this is more objective and we kind of have a feeling how we want to control this.",
                    "label": 0
                },
                {
                    "sent": "For example, familywise error rate.",
                    "label": 0
                },
                {
                    "sent": "It's very conservative.",
                    "label": 0
                },
                {
                    "sent": "But maybe you want to be on the conservative side.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so note the general idea of the theorem.",
                    "label": 0
                },
                {
                    "sent": "It works for any method which is better than random guessing.",
                    "label": 1
                },
                {
                    "sent": "I think that's kind of great.",
                    "label": 1
                },
                {
                    "sent": "It works not only in regression.",
                    "label": 0
                },
                {
                    "sent": "But in any.",
                    "label": 0
                },
                {
                    "sent": "Quote discrete structure estimation problems.",
                    "label": 0
                },
                {
                    "sent": "Whenever there is an include, exclude relation of course it can do exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "You just count how many times you have included the same.",
                    "label": 0
                },
                {
                    "sent": "So this includes variable selection graphical modeling I mean also.",
                    "label": 0
                },
                {
                    "sent": "I mean graphical modeling index with directions clustering and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is really super general.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, if it's so super general, there must be a fairly strong condition.",
                    "label": 1
                },
                {
                    "sent": "Otherwise I think it's fooling ourselves.",
                    "label": 0
                },
                {
                    "sent": "And the condition is here.",
                    "label": 0
                },
                {
                    "sent": "And what's nice about this condition?",
                    "label": 0
                },
                {
                    "sent": "It's easy to say what it is.",
                    "label": 0
                },
                {
                    "sent": "It's maybe not so easy to hold, but it's very easy to say what it is.",
                    "label": 1
                },
                {
                    "sent": "It's what we call the exchangeability condition, and it just requires that the distribution of this thing here.",
                    "label": 0
                },
                {
                    "sent": "So we take the indicator.",
                    "label": 0
                },
                {
                    "sent": "We look better.",
                    "label": 0
                },
                {
                    "sent": "Choraria J is selected by the selector is had Lambda and we only look at Jays being non active.",
                    "label": 0
                },
                {
                    "sent": "The noise covariance for the distribution of among these noise cover it has to be exchanged.",
                    "label": 0
                },
                {
                    "sent": "So this is maybe a pretty strong assumption.",
                    "label": 1
                },
                {
                    "sent": "What is kind of nice about this assumption away.",
                    "label": 0
                },
                {
                    "sent": "I think there is only this is only a requirement for the noise covariance.",
                    "label": 0
                },
                {
                    "sent": "Now you may wonder, come on.",
                    "label": 0
                },
                {
                    "sent": "I mean if you have active queries which are perfectly correlated or almost perfectly correlated, what's happening here?",
                    "label": 0
                },
                {
                    "sent": "This is allowed, but if you have very strongly correlated variables saying a linear model with Lasso, so about 50% of the time, the one is chosen about the other 50% of the time the other, so they cannot be stable.",
                    "label": 0
                },
                {
                    "sent": "Highly correlated variables cannot be stable.",
                    "label": 1
                },
                {
                    "sent": "So for specific problems, of course we can give kind of weaker assumptions, but I think this kind of nice.",
                    "label": 0
                },
                {
                    "sent": "Just the single exchangeability assumption, and it works in this great generality.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a toy example.",
                    "label": 0
                },
                {
                    "sent": "Very condition holds.",
                    "label": 0
                },
                {
                    "sent": "Look at the linear model with a random design.",
                    "label": 1
                },
                {
                    "sent": "And then you random design covariates you require that they are exchangeable, right?",
                    "label": 0
                },
                {
                    "sent": "And an easy example there is hold this, for example, E correlation among the noise covariance.",
                    "label": 0
                },
                {
                    "sent": "So just to convince you there are examples which fulfill our conditions.",
                    "label": 0
                },
                {
                    "sent": "It's not an empty condition.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "So I will have quite many pictures.",
                    "label": 0
                },
                {
                    "sent": "I learned this from Yankee two weeks ago, so he gave some wonderful talk about many pictures and figures, and I tried to convince you of it with pictures that this is kind of an interesting method.",
                    "label": 0
                },
                {
                    "sent": "So here is a range of scenarios for variable selection in linear models using the lasso.",
                    "label": 1
                },
                {
                    "sent": "I think we've seen that now many times.",
                    "label": 0
                },
                {
                    "sent": "P is 660.",
                    "label": 0
                },
                {
                    "sent": "What I think is interesting here.",
                    "label": 1
                },
                {
                    "sent": "This is from a real design data matrix, so this is about this multiplication problem.",
                    "label": 0
                },
                {
                    "sent": "We take this X design matrix 660 sixty in is up to 7:50.",
                    "label": 0
                },
                {
                    "sent": "We choose two different ends for 5750 and then we make artificial wise by just creating some kind of simulated betas and we arranged the sparsity the number of active variables in steps up A4, four 812 up to 40.",
                    "label": 0
                },
                {
                    "sent": "Ann Singleton noise ratio.",
                    "label": 0
                },
                {
                    "sent": "This is something which is never reported by people giving talks on how it performs, or it's rarely reported.",
                    "label": 0
                },
                {
                    "sent": "Many people look at scenarios where signal to noise ratio is 5 or 10 and we were really interested in this kind of thing like signal to noise ratio, .25 one and four.",
                    "label": 0
                },
                {
                    "sent": "So in total we have 60 scenarios with this P with these two ends with a couple of sparsity degrees and different signal to noise ratios.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a picture, and here's the picture again.",
                    "label": 0
                },
                {
                    "sent": "So what does the picture mean?",
                    "label": 0
                },
                {
                    "sent": "So you see, black dots and red triangles and you see a line, and so each of these pairs Vista line.",
                    "label": 0
                },
                {
                    "sent": "This is 1 scenario, so there are 60 scenarios.",
                    "label": 0
                },
                {
                    "sent": "So each of these pair with a line.",
                    "label": 0
                },
                {
                    "sent": "This is 1 scenario.",
                    "label": 0
                },
                {
                    "sent": "Now the black dot man maybe the axis.",
                    "label": 0
                },
                {
                    "sent": "So the X axis are the number of false positives.",
                    "label": 0
                },
                {
                    "sent": "OK, this is bad if it's large and the Y axis is the proportion of true positives up is good.",
                    "label": 0
                },
                {
                    "sent": "Now the black dots.",
                    "label": 0
                },
                {
                    "sent": "This is the solution.",
                    "label": 0
                },
                {
                    "sent": "Meditate cross validation tuning for the last so of course we know these pics too large models, but just for a comparison I think this is a good way to look at it and the Red triangle system stability selection.",
                    "label": 0
                },
                {
                    "sent": "This this Lambda OK and in this scenario and So what we see from this picture is we have accurate control.",
                    "label": 0
                },
                {
                    "sent": "So here's the line we control at the expected number of false positives are 2.5.",
                    "label": 0
                },
                {
                    "sent": "This is the vertical line and this is very accurate control.",
                    "label": 0
                },
                {
                    "sent": "As proved by our theory, so this works.",
                    "label": 1
                },
                {
                    "sent": "Maybe this exchange ability condition doesn't hold, but it seems to hold up pretty well in this practical example.",
                    "label": 0
                },
                {
                    "sent": "Make think there is a drastic reduction of false positives so has validated.",
                    "label": 1
                },
                {
                    "sent": "Tune solutions to.",
                    "label": 0
                },
                {
                    "sent": "Can you reduce a lot of false positives?",
                    "label": 0
                },
                {
                    "sent": "Obviously there is a bit of power loss, but if you look at it, there's not much loss in terms of power.",
                    "label": 1
                },
                {
                    "sent": "So if the line would be very steep upwards, there would be a lot of loss, but the lines are pretty flat, so I would claim there is not much loss in terms of power.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to convince you really, this is a very general purpose tool and this is always dangerous general purpose tools, but I think in this setting I'm quite convinced this is an interesting tool which can be applied in very many signals.",
                    "label": 0
                },
                {
                    "sent": "So this is graphical modeling using the Gmail app, so I think Peter Bickel calls it Spice algorithm and I'll sounds a bit better spicy stuff, so OK.",
                    "label": 0
                },
                {
                    "sent": "So what it is if you don't know much about graphical modeling, it's an L1 penalty type technique to infer the so called conditional independence graph.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a generally that isation of lasso for regression to graphical models.",
                    "label": 0
                },
                {
                    "sent": "There's that Lambda parameter in your spice orgy lasso algorithm, and that's here.",
                    "label": 0
                },
                {
                    "sent": "So if you vary your Lambda, you get denser graphs or kind of listing scraps.",
                    "label": 0
                },
                {
                    "sent": "Obviously.",
                    "label": 0
                },
                {
                    "sent": "Now if you compute the stability selection solution of this with this land, that here is the stability selection for that landed here and there, and so the graph is pretty stable, right?",
                    "label": 0
                },
                {
                    "sent": "This is what I want to achieve.",
                    "label": 0
                },
                {
                    "sent": "And the conclusion is the stability selection.",
                    "label": 0
                },
                {
                    "sent": "The choice of this initial Lambda tuning does really not matter much as we can actually prove in our theory and what you need to fix.",
                    "label": 0
                },
                {
                    "sent": "Of course, is your finite sample control.",
                    "label": 0
                },
                {
                    "sent": "If you make a more stringent finite sample control, you get as far as a graph and vice versa.",
                    "label": 0
                },
                {
                    "sent": "But I do think this is a more objective way to judge how big we should take the graph.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the null situation.",
                    "label": 0
                },
                {
                    "sent": "This is actually a true underlying day.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say we don't know what the true graph.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can of course.",
                    "label": 0
                },
                {
                    "sent": "Mute your data, set the structure and indeed through graph is the empty graph.",
                    "label": 0
                },
                {
                    "sent": "And then with the Lambda is just really terrible.",
                    "label": 0
                },
                {
                    "sent": "In this example, is the empty graph.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you make Lambda larger and larger at some point, you could also an empty graph, but it's very hard to know if you don't know that the through graph is the empty graph.",
                    "label": 0
                },
                {
                    "sent": "I think you would not find it.",
                    "label": 0
                },
                {
                    "sent": "The stability selection just called here a couple false positives, but in general we really can control the number the expected number of false positives as proof by our theory.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, it holds in greater generality clustering.",
                    "label": 0
                },
                {
                    "sent": "So of course this is also a discrete structure estimation problem.",
                    "label": 0
                },
                {
                    "sent": "You would just want to know whether two points are in the same class.",
                    "label": 0
                },
                {
                    "sent": "Three yes or no, and this can be also written as a graph, so the vertices or the sample points and the sample points are in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "Then you draw an edge between the vertices so it's again a graph type problem.",
                    "label": 0
                },
                {
                    "sent": "This is how we implemented.",
                    "label": 0
                },
                {
                    "sent": "So this is a toy example, but it's kind of a neat example.",
                    "label": 0
                },
                {
                    "sent": "I think it's only two dimensional and the true clustering structure is down here, so there's a cluster hearing rate, the clustering black and kind of two Singleton clusters, just two single points, 1 green guy here, 1 blue guy here.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the model from Gaussians into outliers.",
                    "label": 0
                },
                {
                    "sent": "So now just let's apply K means for example to this problem.",
                    "label": 0
                },
                {
                    "sent": "So this is K means K equal 2.",
                    "label": 0
                },
                {
                    "sent": "Of course we cannot find the four clusters K = 3.",
                    "label": 0
                },
                {
                    "sent": "We cannot find the full cost of K equals.",
                    "label": 0
                },
                {
                    "sent": "This is the right number of clusters, but we don't get the structure because these are so strange points here we don't get these structures K equals six K = 8.",
                    "label": 0
                },
                {
                    "sent": "Now let's do stability selection on K = 2.",
                    "label": 0
                },
                {
                    "sent": "Get the structure K = 2 doesn't mean after stability selection you only have two clusters.",
                    "label": 0
                },
                {
                    "sent": "This is not true, we just get a fundamentally different solution Kafel 3 same class through Class 4 through cluster six through cluster and at some point it deteriorates as well.",
                    "label": 0
                },
                {
                    "sent": "If you regularization parameter, the initial parameter is too bad.",
                    "label": 0
                },
                {
                    "sent": "Stability selection cannot recover it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example, the lasso and its corresponding stability regularization path.",
                    "label": 1
                },
                {
                    "sent": "This is again a real example.",
                    "label": 0
                },
                {
                    "sent": "It's about vitamin production visit.",
                    "label": 0
                },
                {
                    "sent": "Bacillus P is more than 4000.",
                    "label": 0
                },
                {
                    "sent": "We have 115 samples.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this in collaboration with Dutch state mining company and so they say these six clear, it's pretty sure that they they are here D6 jeans.",
                    "label": 0
                },
                {
                    "sent": "So we take these six jeans and be.",
                    "label": 0
                },
                {
                    "sent": "So suppose there true.",
                    "label": 0
                },
                {
                    "sent": "And we permute all the other stuff.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of this kind of semi simulated data set.",
                    "label": 0
                },
                {
                    "sent": "So we know or we hope that the six ones are the true guys there indicated in red the path and the others are for sure noise clearance.",
                    "label": 0
                },
                {
                    "sent": "Because we have renewed it.",
                    "label": 0
                },
                {
                    "sent": "So this is lost, so if you're not familiar with it, you just compute your lasso coefficients as a function of your Lambda, and then if you land at large, you have nothing selected in the first variable comes up, and so on.",
                    "label": 0
                },
                {
                    "sent": "So the true variables they indeed come up, but then it mingles Vista noise and becomes very hard to choose a good Lambda.",
                    "label": 0
                },
                {
                    "sent": "The true variables do not stick out that greatly from the noise coherence.",
                    "label": 0
                },
                {
                    "sent": "This is stability selection on the lasso and just from the picture it's clear that the true rate curves they stick out much more clearly from the noise coherence, and this is a further improvement, which actually I think has some relations to Sasha's talk, and then I will come to that this is stability selection with an additional randomization with stability selection, Mr randomized LASSO, and this is even a bit better.",
                    "label": 0
                },
                {
                    "sent": "I think these six lines here even.",
                    "label": 0
                },
                {
                    "sent": "Stick out a bit further from the noise covariance.",
                    "label": 0
                },
                {
                    "sent": "The black lines in here.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion here be stability selection.",
                    "label": 1
                },
                {
                    "sent": "At least four up to six through variables are sticking out much more clear.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think this I said it already, but I think it's important to say stability selection cannot be represented by simply selecting a good land.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Course not if you look at this path, you cannot click the Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it gives you the same solution.",
                    "label": 0
                },
                {
                    "sent": "It really provides a fundamentally different solution.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what about this?",
                    "label": 0
                },
                {
                    "sent": "Further improvement, and this randomized lasso and it has a lot to do with the stability selection and the way we detect this was really the first invented.",
                    "label": 0
                },
                {
                    "sent": "What I just told by now and then we saw that additional randomization can help.",
                    "label": 1
                },
                {
                    "sent": "So let's discuss this issue with linear models and glass.",
                    "label": 1
                },
                {
                    "sent": "So we have a linear model, high dimensional and we consider the last so.",
                    "label": 1
                },
                {
                    "sent": "And variable selection consistency, as we've heard, I think many times means the probability that you selected set of variables equals to through one converges to 1.",
                    "label": 0
                },
                {
                    "sent": "Sample size tends to Infinity and I think it's alone by now that variable selection is the last, so it needs in necessary in the sufficient condition.",
                    "label": 0
                },
                {
                    "sent": "We called it the neighborhood stability condition.",
                    "label": 1
                },
                {
                    "sent": "Later Ping John been you?",
                    "label": 0
                },
                {
                    "sent": "They kind of reformulated our ugly condition and made it to a nicer understandable condition.",
                    "label": 0
                },
                {
                    "sent": "And this is what's called nowadays the representable condition.",
                    "label": 0
                },
                {
                    "sent": "So it is a condition on your design.",
                    "label": 0
                },
                {
                    "sent": "And I'm meeting the details here.",
                    "label": 0
                },
                {
                    "sent": "I think Sasha already hinted at some of the stuff here.",
                    "label": 0
                },
                {
                    "sent": "I just want to say it is restrictive.",
                    "label": 0
                },
                {
                    "sent": "It's a restrictive condition on the design, so lots of works if and only if their restrictive condition on the design holds.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And while this is no surprise, assuming this strong in representable condition, we can also prove the variable selection Vista stability.",
                    "label": 1
                },
                {
                    "sent": "Issue is the stability subsampling is consistent as well.",
                    "label": 0
                },
                {
                    "sent": "No big surprise, so you don't make it worse.",
                    "label": 0
                },
                {
                    "sent": "From a practical point of view, what you gain is this insensitivity disrespect to the choice of Lambda and a more objective way how you should choose the number of variables.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but that's not the end of the story.",
                    "label": 0
                },
                {
                    "sent": "From a theory and practical perspective, we can improve by additional randomization.",
                    "label": 0
                },
                {
                    "sent": "I think Sasha calls it also perturbation and I was not aware of his working until three weeks.",
                    "label": 0
                },
                {
                    "sent": "It's not the same, but I think it bears quite a lot of similarities here.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the procedure randomized lasso.",
                    "label": 0
                },
                {
                    "sent": "So what it is, it's kind of a weighted lasso, and we have weights and the weights are totally dumb weights.",
                    "label": 0
                },
                {
                    "sent": "In a way it's just IID random variables for every coherent and it just take values in the range Alpha to one, OK?",
                    "label": 0
                },
                {
                    "sent": "And Alpha is what we call the weakness parameter.",
                    "label": 0
                },
                {
                    "sent": "And I would propose actually how we implement it.",
                    "label": 0
                },
                {
                    "sent": "We make each two values for this W. Either it is equal to Alpha or one with corresponding probabilities PW or 1 -- P W. So you have these weights and then you have this randomized lasso.",
                    "label": 0
                },
                {
                    "sent": "It's of course the comics optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You minimize residual sum of squares and you penalized by these weighted penalty.",
                    "label": 0
                },
                {
                    "sent": "So if WJ is 1, it's the ordinary penalty.",
                    "label": 0
                },
                {
                    "sent": "And if WJ is Alpha small you increase the penalty.",
                    "label": 0
                },
                {
                    "sent": "So this is a weighted lasso, AKA the adaptive lasso, which has been proposed by Joe.",
                    "label": 0
                },
                {
                    "sent": "But so of course took a Daydream and good.",
                    "label": 0
                },
                {
                    "sent": "WJ there is.",
                    "label": 0
                },
                {
                    "sent": "We take it totally stupid WJ reaches take them totally random.",
                    "label": 0
                },
                {
                    "sent": "And of course, as a standalone procedure, this is absolutely nonsensical.",
                    "label": 0
                },
                {
                    "sent": "This is pretty stupid like this.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can recover the stupidness Mr Stability selection and you gain something with it.",
                    "label": 0
                },
                {
                    "sent": "OK, in connection with stability selection.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This will improve.",
                    "label": 0
                },
                {
                    "sent": "I'm here is in an example.",
                    "label": 0
                },
                {
                    "sent": "Artificielle two active variables which satisfy the strong is representable representable condition, one bat covariate which violates the representable condition.",
                    "label": 0
                },
                {
                    "sent": "This is in blue 197.",
                    "label": 0
                },
                {
                    "sent": "Noise covariance in is 200.",
                    "label": 0
                },
                {
                    "sent": "This is the stability selection pass with the lasso and this is the stability selection path with the randomized lasso.",
                    "label": 0
                },
                {
                    "sent": "And what happens here?",
                    "label": 0
                },
                {
                    "sent": "Stability selection missed the last, so you cannot get rid of the blue guy.",
                    "label": 0
                },
                {
                    "sent": "It violates the representable condition and even stability selection doesn't help here.",
                    "label": 0
                },
                {
                    "sent": "But if you do the randomized lasso, you can kind of decrease this effect.",
                    "label": 0
                },
                {
                    "sent": "So if you choose a large threshold set .7, the only recover the two through active variables of course.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is now official example.",
                    "label": 0
                },
                {
                    "sent": "We ran quite a few of simulated models and.",
                    "label": 0
                },
                {
                    "sent": "I think this is kind of the fair figure.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you gain something like 70% in reducing the number of false positives.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you gain nothing, essentially 3%.",
                    "label": 0
                },
                {
                    "sent": "Let's say a 0% but.",
                    "label": 0
                },
                {
                    "sent": "At least what we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "We never lost style.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is the theorem.",
                    "label": 0
                },
                {
                    "sent": "This is variable selection consistency with the randomized lasso.",
                    "label": 1
                },
                {
                    "sent": "And so we look at these randomized lasso it past his weakness parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "And of course we need a couple assumptions.",
                    "label": 0
                },
                {
                    "sent": "The assumptions as we take a fixed design in our linear model.",
                    "label": 0
                },
                {
                    "sent": "We work with this sparse eigenvalue condition because Nikolai this is Nicholas kind of fruit.",
                    "label": 0
                },
                {
                    "sent": "We are aware your condition is a bit weaker, but nevertheless I think that is not such a restrictive condition.",
                    "label": 0
                },
                {
                    "sent": "It seems that this is much less restrictive than, say, this neighborhood stability.",
                    "label": 0
                },
                {
                    "sent": "Or your representable condition.",
                    "label": 0
                },
                {
                    "sent": "OK, then the state of Lambda, how so?",
                    "label": 0
                },
                {
                    "sent": "You only need to make an assumption on the minimal and either you take very large lambdas, doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Because you you take the you know all these things.",
                    "label": 0
                },
                {
                    "sent": "So the Lambda mean should be up for certain form.",
                    "label": 0
                },
                {
                    "sent": "Then there is this issue that the minimal non 0 beta doesn't have to be too small and this is certainly not tight.",
                    "label": 0
                },
                {
                    "sent": "I think we wouldn't want to get rid of these S to the power three half.",
                    "label": 0
                },
                {
                    "sent": "P much larger than a nice of course allowed and then the theorem states for suitable range of thresholds pie threshold you have variable selection consistency.",
                    "label": 0
                },
                {
                    "sent": "You can recover the true set.",
                    "label": 0
                },
                {
                    "sent": "I should say I mean the assumptions are maybe not minimal, but the difficulty here is view analyzing an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have this last so and this subsampling and I think at least.",
                    "label": 0
                },
                {
                    "sent": "With my skills I find it much harder than analyzing, say, a lasso, which is clearly defined as an optimization procedure.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so why does it work?",
                    "label": 0
                },
                {
                    "sent": "And I think Sasha gave in a lady answered.",
                    "label": 0
                },
                {
                    "sent": "I kind of minion here.",
                    "label": 0
                },
                {
                    "sent": "I've heard him again.",
                    "label": 0
                },
                {
                    "sent": "They said OK, this is part of the answer.",
                    "label": 0
                },
                {
                    "sent": "Somehow we perturb also our design matrix in.",
                    "label": 0
                },
                {
                    "sent": "Obviously the randomized lasso.",
                    "label": 0
                },
                {
                    "sent": "It's a way that lasts.",
                    "label": 0
                },
                {
                    "sent": "Oh, so this can always be rewritten.",
                    "label": 0
                },
                {
                    "sent": "Just re scale your covariates and then you have a standard lasso problem, right?",
                    "label": 0
                },
                {
                    "sent": "And so we kind of have a randomized perturbed design now.",
                    "label": 0
                },
                {
                    "sent": "So you have WJ XJ is our new covert XWJ and now what's happening mathematically is as follows.",
                    "label": 0
                },
                {
                    "sent": "If you would make this weakness parameter lower and lower, the design gets verse and verse in terms of.",
                    "label": 0
                },
                {
                    "sent": "Say the condition number, the ratio between the maximal and minimal sparse eigenvalue gets worse, so a smaller Alpha is bad, so don't make it too bad there is a lower limit and don't go below this lower meaning Alpha low of course is lower limit as N gets large, it tends to 0, but there is a lower limit.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't make it 2 week.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think it's kind of plausible that if you don't make it too bad, you still can catch the true guys because they're still strong enough.",
                    "label": 0
                },
                {
                    "sent": "So the question is really, how can we get rid of this blue guy?",
                    "label": 0
                },
                {
                    "sent": "This one here.",
                    "label": 0
                },
                {
                    "sent": "So how can we make it like this?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Part of the answer is as follows.",
                    "label": 0
                },
                {
                    "sent": "What you also can prove, and I think this matches perfectly what you are seeing.",
                    "label": 0
                },
                {
                    "sent": "In this perturbed design, if you perturb.",
                    "label": 0
                },
                {
                    "sent": "Kind of surprisingly, the representable condition holds sometimes.",
                    "label": 0
                },
                {
                    "sent": "And this sometimes can be characterized.",
                    "label": 0
                },
                {
                    "sent": "It's a certain set script B, so sometimes the representability condition holds.",
                    "label": 0
                },
                {
                    "sent": "It holds you don't pick up annoys, coherent, OK. Then you're doing the right thing even when it is not fulfilled for the original data.",
                    "label": 0
                },
                {
                    "sent": "And so the noise clear it's what's happening.",
                    "label": 0
                },
                {
                    "sent": "Is there sometimes not select?",
                    "label": 0
                },
                {
                    "sent": "It's correctly, sometimes, not always.",
                    "label": 0
                },
                {
                    "sent": "So far we have only sometimes, but this sometimes doesn't have two small probabilities, so it happens sometimes.",
                    "label": 0
                },
                {
                    "sent": "If you do stability selection many times, this sometimes means that if you sometimes do not select it, you cannot stabilize select it so.",
                    "label": 0
                },
                {
                    "sent": "It said it's not a chicken and egg sentence, which I'm saying here.",
                    "label": 0
                },
                {
                    "sent": "It's kind of maybe needs 3 minutes.",
                    "label": 0
                },
                {
                    "sent": "Looking at this in a quiet time, but we don't have to quiet time, but the issue is Alpha small.",
                    "label": 0
                },
                {
                    "sent": "Makes it possible that the represent ability starts to hold even if it doesn't hold for the original data.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusions stability selection.",
                    "label": 0
                },
                {
                    "sent": "I think it really did races in a way.",
                    "label": 1
                },
                {
                    "sent": "Also the need in particularly high dimensional problems to say something about how many inches, how many variables are?",
                    "label": 0
                },
                {
                    "sent": "How are the clusters looking like?",
                    "label": 0
                },
                {
                    "sent": "We need to stabilize this and I think this is kind of an interesting, very easy to do and very easy to.",
                    "label": 0
                },
                {
                    "sent": "To do procedure, I mean I think people have done this since years and I think our contribution is here to have a more solid mathematical foundation for this.",
                    "label": 0
                },
                {
                    "sent": "So it's very generic.",
                    "label": 0
                },
                {
                    "sent": "We have finite sample error control when estimating any kind of discrete structures.",
                    "label": 0
                },
                {
                    "sent": "When I say variable selection, graphical modeling, clustering and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And the other part is this additional randomization.",
                    "label": 0
                },
                {
                    "sent": "This perturbation of the design matrix really helps.",
                    "label": 1
                },
                {
                    "sent": "We did it in terms of these randomized lasso.",
                    "label": 0
                },
                {
                    "sent": "I think Sasha is this is model on this kind of noisy on observable covariates.",
                    "label": 0
                },
                {
                    "sent": "What we call also a randomized weakening.",
                    "label": 0
                },
                {
                    "sent": "That's why we call it a weakness parameter.",
                    "label": 0
                },
                {
                    "sent": "It's really randomly giving a high penalty and that kind of helps a lot.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a connection to random forests.",
                    "label": 0
                },
                {
                    "sent": "I put that in yesterday night after you talk because random forests also has this randomization step on the covariates and not only under sampling points.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Well, give me.",
                    "label": 1
                },
                {
                    "sent": "Distance from escape from Mississippi.",
                    "label": 0
                },
                {
                    "sent": "This multiplicative noise solution for the same role by role, yes, so.",
                    "label": 1
                },
                {
                    "sent": "Maybe one can have better schemes, but yeah, it's right.",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It is the same as total Delta in that like I, I think no, it's not your Dell, that's your other stuff, which is the easy stuff really.",
                    "label": 0
                },
                {
                    "sent": "So in our experience take Alpha, .5 or .2.",
                    "label": 0
                },
                {
                    "sent": "Don't make it so.",
                    "label": 0
                },
                {
                    "sent": "We have not had that much experience.",
                    "label": 0
                },
                {
                    "sent": "I think we ran.",
                    "label": 0
                },
                {
                    "sent": "Maybe run quite a few simulations, but it seems like.",
                    "label": 0
                },
                {
                    "sent": "The big issue, of course, is in the simulation study how you choose your finance sample control.",
                    "label": 0
                },
                {
                    "sent": "That's that's it.",
                    "label": 0
                },
                {
                    "sent": "Into weather how you choose you Alpha and also your land.",
                    "label": 0
                },
                {
                    "sent": "I agree there are two parameters involves an Alpha and Lambda.",
                    "label": 0
                },
                {
                    "sent": "Lambda seems really very insensitive the Alpha.",
                    "label": 0
                },
                {
                    "sent": "Just take .2 I think but as a wakes suggestion in our experience so far, it doesn't matter very much.",
                    "label": 0
                },
                {
                    "sent": "But of course you are.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a bit maybe.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe I'm a bit fast and paying this.",
                    "label": 0
                },
                {
                    "sent": "Of course this thing is happening so there are two things.",
                    "label": 0
                },
                {
                    "sent": "Low Alpha is good with regard to the representable condition, but it's bad in terms of this maximal minimal eigenvalue eigen values in.",
                    "label": 0
                },
                {
                    "sent": "So mathematically, you need to be careful how to choose Alpha.",
                    "label": 0
                },
                {
                    "sent": "Next I'm not sure, but then the central theorem, but there's something is bothering me.",
                    "label": 0
                },
                {
                    "sent": "What could win the provincial from maybe as many valuable as you like and make me as loud as you can?",
                    "label": 0
                },
                {
                    "sent": "Just whatever be believable you randomize.",
                    "label": 0
                },
                {
                    "sent": "Music random generator.",
                    "label": 0
                },
                {
                    "sent": "So I think good point, never thought in terms of the finite sample error control, no problem because there we only talk about false positives.",
                    "label": 0
                },
                {
                    "sent": "But of course I guess there's a power loss.",
                    "label": 0
                },
                {
                    "sent": "But finite sample control.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "The designs that help you.",
                    "label": 0
                },
                {
                    "sent": "The interesting really sign suspect, I mean, are there some that really screw up?",
                    "label": 0
                },
                {
                    "sent": "And if you think the multiple select the wrong ones or is it more gradual?",
                    "label": 0
                },
                {
                    "sent": "I mean if you block something like.",
                    "label": 0
                },
                {
                    "sent": "I suppose you know the truth.",
                    "label": 0
                },
                {
                    "sent": "Second, you got something like OK with this Rosa PS.",
                    "label": 0
                },
                {
                    "sent": "Then I select this phone variable.",
                    "label": 0
                },
                {
                    "sent": "I don't quite understand, maybe your remark, but I mean first of all, this is real.",
                    "label": 0
                },
                {
                    "sent": "The real example.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But of course, in order to understand what is good or not, we permute lots of this stuff, so I know the black things here.",
                    "label": 0
                },
                {
                    "sent": "There, noise coherence.",
                    "label": 0
                },
                {
                    "sent": "This is just kind of more to understand.",
                    "label": 0
                },
                {
                    "sent": "The property for these real procedure and I do not really take out rows or columns in my design matrix, I just.",
                    "label": 0
                },
                {
                    "sent": "Randomly give higher or ordinary penalties to these variables.",
                    "label": 0
                },
                {
                    "sent": "You know why I think this smears out the range of.",
                    "label": 1
                },
                {
                    "sent": "Individual penalty parameters for individual covariates, but of course you do it randomly and do not cover everything.",
                    "label": 0
                },
                {
                    "sent": "Ideally would like to maybe a penalize every covariate in a different manner, but of course you cannot do that and this randomized.",
                    "label": 0
                },
                {
                    "sent": "Method kind of smears it out to a range of different values.",
                    "label": 0
                },
                {
                    "sent": "So maybe we should talk in a break.",
                    "label": 1
                },
                {
                    "sent": "Do you have in mind that there's some serious pre processing preselection process before you start the whole thing?",
                    "label": 0
                },
                {
                    "sent": "Or are you just saying well all of them do selection?",
                    "label": 0
                },
                {
                    "sent": "Process this data within this destination.",
                    "label": 0
                },
                {
                    "sent": "Now, I would argue the other way around.",
                    "label": 0
                },
                {
                    "sent": "I think I would take this as my pre processor.",
                    "label": 0
                },
                {
                    "sent": "Now you have to use select variables and now you do re estimation.",
                    "label": 0
                },
                {
                    "sent": "However you would like to do you have a very sparse set of variables.",
                    "label": 1
                },
                {
                    "sent": "Other peoples are saying something.",
                    "label": 0
                },
                {
                    "sent": "I mean this stability selection you could put in anything such that the procedure is better in random guessing, no problem, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah no, no this is just ensuring that the theorem holds and it's again.",
                    "label": 0
                },
                {
                    "sent": "I think the power question.",
                    "label": 0
                },
                {
                    "sent": "I mean whether you did a good job or not in terms of false positives, you cannot control it.",
                    "label": 0
                },
                {
                    "sent": "But in terms of power you may lose something in so of course the power question comes in by the theory.",
                    "label": 1
                },
                {
                    "sent": "I mean for variable selection consistency you need power and.",
                    "label": 0
                },
                {
                    "sent": "So here is a saying.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's happening if I would do some kind of pre processing first?",
                    "label": 0
                }
            ]
        }
    }
}