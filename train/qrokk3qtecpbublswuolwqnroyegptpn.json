{
    "id": "qrokk3qtecpbublswuolwqnroyegptpn",
    "title": "Convex Relaxation and Estimation of High-Dimensional Matrices",
    "info": {
        "author": [
            "Martin J. Wainwright, UC Berkeley"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/aistats2011_wainwright_convex/",
    "segmentation": [
        [
            "OK, so this is joint work with some students, Alec Agarwal, Sahan, Negahban, a former postdoc, Pradeep Ravikumar and colleague been you who I suspect you heard of heard speak on Monday.",
            "So."
        ],
        [
            "As Joshua said, it's about convex methods and it's about high dimensional matrices.",
            "So let me just sort of set the stage a little bit great.",
            "So this area is about high dimensional datasets.",
            "I think that's familiar to an audience like this.",
            "So basically we're getting more and more data, both more and more samples.",
            "But I think also Interestingly, we're getting richer and richer kinds of data.",
            "As technology evolves, we are able to measure and collect interesting kinds of phenomena.",
            "So things like social networks.",
            "We have massive amounts of data, things like recommender systems, Netflix, Amazon, other kinds of users, huge amounts of data on all of us in computer vision.",
            "We of course huge image databases, video databases.",
            "I do some work in astronomy and the astronomers collect so much data that essentially what they're asking us to do is figure out very fast ways to throw out 99% of the data quickly 'cause they don't have enough disk space to actually store it.",
            "Right, so this sort of modern data delusion is certainly going on and getting worse.",
            "This talk sort of falls.",
            "You can be motivated by this kind of question.",
            "This is the question that many people are asking when you're confronted with high dimensional data.",
            "So they're sort of saying, suppose that I have N = 100 samples, but I'm measuring things that take place in some ambient space.",
            "I'll call D with 1000 dimensions.",
            "Do we really expect theory?",
            "So classical theory things like law of large numbers or consistency of things like maximum likelihood?"
        ],
        [
            "That would be based on saying let's let's end the number of samples get really, really big and think of the model is fixed and then good stuff happens, right?",
            "You have law of large numbers.",
            "You have central limit 3 so on.",
            "But the question is if really the regime in which we're working is N is 100 and D is 1000, is that kind of theory going to be useful?",
            "So I think you can answer this question in two ways.",
            "It's possible that you would have a data set where actually the classical theory would be useful for this sort of depends on how quickly the asymptotic's kick in, but what various people are interested in doing is not doing asymptotically."
        ],
        [
            "Actually getting statements that are non asymptotic and they hold for all sample sizes and all sizes of models and other kinds of parameters.",
            "So this is sort of the area of high dimensional statistics or high dimensional learning theory.",
            "You want to study estimators, algorithms.",
            "You want to allow for scalings in which the sample size might be much smaller than the ambient dimension, or maybe roughly the same order.",
            "And you'd like to have results that aren't asymptotically like to say for fixed ND with some probability, good stuff happens.",
            "So that's sort of the general picture, but the same time, I guess you're probably wondering if you really have 1000 parameters and only 100 samples, then there's probably various kinds of impossibility theorems.",
            "You can't do much right if you have so many 10 times more degrees of freedom, So what?",
            "Lots of people are doing is investigating various types of low dimensional structure and sort of saying typical datasets might lie in 1000 or 10,000 dimensions.",
            "But probably there's a lot of low dimensional structure that if we understood it.",
            "We could exploit it so then the name of the game is what kinds of low dimensional structure are interesting and practically relevant?",
            "An what kind of techniques can we efficient ones computationally efficient ones?",
            "Can we develop to exploit it?",
            "OK, so that's very broad overview.",
            "This is sort of a broad area.",
            "Many people are working in it.",
            "In this talk I want to talk focus on just one particular kind of low."
        ],
        [
            "Dimensional structure, and that's one which probably many of you are familiar with.",
            "When you have a high dimensional matrix, I'll be thinking about matrices or call Theta star dimension D1 by D2 and this is going to be quite large typically, but you want to think about the case where if you computed in SVD.",
            "So if you computed the left singular vectors, the singular values and the right singular vectors.",
            "It might be low rank, so you might have a rank R that's much less than the minimum of D1D2.",
            "Or maybe this holds only in an approximate sense that the sense in which the matrix is well approximated by a low rank.",
            "That's probably much more realistic.",
            "In practice, you don't in practice have exactly low rank things, but you have things that if you sort of just took the top singular vectors and threw away the rest, you get a very good approximation.",
            "So let me give you some examples just to start up where these kinds of low rank structures arise.",
            "One that you'd be familiar with.",
            "I won't talk about so much methods like principal component analysis, Canonical correlations.",
            "These of course are looking for low rank or near low rank structure, but there are many others, so let me begin with some examples."
        ],
        [
            "One problem that comes up in imaging systems.",
            "Specially these days what I'm showing you here is I'm showing you 2 pictures.",
            "From two different cameras of a famous cathedral in Germany.",
            "And the point is that the cameras are slightly offset from one another, so the same image but one camera is like this.",
            "The others like this, it's kind of mimicking a binocular human vision system, right?",
            "We get depth actually because your eyes are slightly separated.",
            "You're actually getting a fair bit of disparity or depth information from images.",
            "So if you're clever, what you could do, or if you can cross your eyes, you can actually fuse these images, make them overlap on top of one another, and then you'll see a very nice sort of cathedral and you'll see pillars behind one another.",
            "You'll see a nice sense of depth.",
            "Right, so in practice, modern vision systems.",
            "We don't just have two cameras.",
            "You might have, let's say 50 cameras, right?",
            "So modern sporting events these days would have 50 or 100 cameras, and they're trying to do all sorts of things quite quickly, like reconstruct depth or do rotations of the scene.",
            "One way to model this is that it's actually a prediction problem, but it's actually a whole family of simultanea."
        ],
        [
            "Prediction problems.",
            "Right, so you can think that what you're trying to do is you have an unknown matrix in each column of this matrix is an image or some function of an image, something like a depth map that you're trying to reconstruct.",
            "So you're assuming that you have D2 images and each image is D1 dimensional.",
            "Alright, so D1 might be well depends how you represent the image, but D1 could be in the 10s of thousands easily and in the simplest case you would sort of imagine that what you actually have is a whole coupled set of linear predictors.",
            "So you have X that's known to you and X times the images.",
            "Plus noise is generating what you see, right?",
            "So you have a linear inverse problem and it's high dimensional.",
            "You're thinking about the number of samples is going to be quite a bit smaller than D 1 * D Two the number of degrees of freedom.",
            "But what you expect is that the predictors this matrix have a lot of structure.",
            "One kind of structure that does arise is low rank structure that not that the predictors are exactly low rank, but somehow that if you got an appropriate rotation of the matrix, there's a low rank space which they lie close to.",
            "So the idea is that if you can exploit the low rank structure in this problem, then you can generate better predictors using this observation model.",
            "So that's an instance of what's called low rank multitask learning, multitask regression.",
            "This is regression discuss.",
            "I'm thinking about it as a linear model, but you can think of classification problems, all sorts of other problems that have this kind of low rank structure.",
            "Another example that probably many of you are familiar with, particularly if you use something like Netflix."
        ],
        [
            "Would be the problem of recommender systems or collaborative filtering?",
            "This was first studied, I think in machine learning community.",
            "By Nati Srebro, Alana Nacla back in 2004.",
            "So this is a nice problem, again involving matrices.",
            "So the problem is that what Netflix has or other kinds of companies what they have is a large database that's think of as a matrix with individuals in the columns and movies across the individuals in the rows and movies across the columns.",
            "This matrix is partially populated by numbers, numerical values that are ratings of movies that people have watched right.",
            "So here Roger Federer has watched Woody Allen movie and because he's European and he likes Woody Allen movies, so he gives it a four out of five.",
            "He hasn't watched Jaws, but oddly enough Rafael Nadal has right.",
            "So there's somehow this kind of sharing that's going on.",
            "You got a very sparse matrix, lots of probably the majority of the matrix is stars, realistically.",
            "Right, I mean I watch a lot of movies personally, but I think, well, unless you sort of quit your professional life and just watch movies all the time, the number of non stars that you could, you know movies.",
            "You could realistically rates going to be a banishing fraction of the total Netflix database.",
            "Right, so it's a problem with tons of missing data.",
            "And the collaborative filtering problem is essentially you'd like to fill in the missing entries.",
            "You'd like to recommend to people who haven't seen a movie you'd like to find things that have four and five, so you'd like to fill in predictions for them.",
            "Now, of course, again, it would be an underdetermined problem, but what you sort of expect in practice is that what drives people's ratings of movies.",
            "There's probably some kind of hidden meta factors you could query people about.",
            "Things like do you like horror movies?",
            "Do you like Arnold Schwarzenegger appearing naked?",
            "Flexing his biceps in movies.",
            "There's probably some factors like this and a relatively small number of them that would be quite predictive of people's ratings.",
            "And so if you have that kind of factor model in mind, what it translates to is that the real full unknown matrix, although it's very large dimensional, you know hundreds of 1000 dimensional if not million.",
            "There's probably a very small number of.",
            "Components that are actually have high variance.",
            "So when people do experiments they find well depends on the experiment.",
            "But you might find that 30 singular vectors give a reasonable approximation.",
            "It's not exactly low rank, but it's well approximated by low rank.",
            "So another kind of problem.",
            "These are interesting to security."
        ],
        [
            "Robustness issues arise in recommender systems.",
            "Probably some of you are familiar with this.",
            "There's a very famous breakdown of Amazon.",
            "the Amazon recommender system.",
            "Of course, they have the same kind of problem.",
            "Write your rating things like, well your rating by buying books or whatever you buy on Amazon and based on that they're trying to suggest other things you'd like to buy.",
            "So there's a case in 2002 where it happened to be the case that if you bought this spiritual guide by Pat Robertson.",
            "Then the recommend."
        ],
        [
            "System would also recommend a certain sex manual to you if you're interested.",
            "You can Google and quite quickly fill in these question marks.",
            "Other people give talks about this and they are brave enough to actually tell you both the title and give you the cover, but I just can't do that.",
            "'cause the old sort of dictate that would you do something that if your mother were in the audience you would feel proud about so I can't do that because of my mother.",
            "But you're welcome to to Google.",
            "I can guarantee that.",
            "It's entertaining.",
            "But the point here is right that this was, you know, this was actually pretty simple manipulation.",
            "It was people going into Amazon and spiking the database with love, spiritual guide and love sex manual.",
            "So you had a lot of Co occurrences and then the naive system wasn't able to filter them.",
            "And it's not just Amazon.",
            "Of course, people looking at academic recommendation systems, you know people that informs where you're allowed to rate other people's books or write your own book.",
            "You of course, see authors doing nasty things like having their friends go and rate the competitors books very low and then spike their own books with high ratings.",
            "So all sorts of systems like this can be manipulated, and it's important to understand how we can."
        ],
        [
            "Mitigate those kinds of effects.",
            "So one way in which some people have been studying this is instead of looking just at a low rank problem, you allow the low rank problem to have an additional noise matrix where this noise matrix can be adversarial.",
            "But again it has structure one kind of structure that's been studied is when this matrix is sparse.",
            "That means when a subset of users are manipulating sort of adding nastiness to the entries in the sort of true low rank matrix that you're trying to recover.",
            "So various people have studied this.",
            "I think the first people were Chandrasekaran Parillo and will ski and then other people have studied this or variants of this.",
            "Another variant is when you have columns that are corrupted or rose that are corrupted right?",
            "That would be when you have a user, let's say the row is corrupted, 1 user is being completely insane so you want to exclude that particular user.",
            "I'm."
        ],
        [
            "So there's some other examples."
        ],
        [
            "I can talk about, but I think."
        ],
        [
            "Just."
        ],
        [
            "Interest of time.",
            "I'll sort of move on from this point.",
            "Actually, how long should the talk last for?",
            "OK, and so we began around 20 after so.",
            "OK, so 9:15 should be the end OK?",
            "So what I've done so far is at least I hope I've convinced you that there's many interesting problems that involve high dimensional matrices, right?",
            "So high dimensional means the dimensions of the matrices are large, but in many of these problems were also interested in is that the number of samples is often less than the product of the dimensions of the matrix, so these problems in some sense are challenging because you really have fewer samples than the effective number, or the total number of degrees of freedom.",
            "So what we'd like to understand is how do we solve prediction problems in this kind of setting?",
            "So what I'd like to talk about is a standard method, nothing new in the method, but a standard method is that in order to enforce rank constraints in a convex way, you use something called a nuclear norm.",
            "It's basically just a convex relaxation of a rank constraint.",
            "So I'll talk about just a class of estimators that are based on solving a convex program, and they apply to many different models they apply to all of the models that I mentioned here, and what I'd like to show is really.",
            "There's just a couple of simple ideas that allow you to understand why these methods.",
            "Well, when these methods work in practice they don't always work, of course, but under what conditions will they work and exactly how many samples do you need as a function of the matrix and other parameters of the problem to be sure that you're getting good estimates out of your model?",
            "And then I'll give you a couple of examples."
        ],
        [
            "Mainly I think about gum completion and decomposition.",
            "OK, so let's talk about matrix regression problems.",
            "Pretty much all the problems I mentioned before can be thought of as this way.",
            "So it's really just a generalization of linear regression.",
            "You want to think that you have some unknown matrix, some high dimensional thing.",
            "And then you have some operator.",
            "Just a fancy way for fancy way to say something that takes the matrix and it takes N samples of the matrix in some way and gives you an N vector of N samples.",
            "Depending on the model, the sampling could be sampling is different if it's matrix completion, the sampling is very simple.",
            "In matrix completion, each time what this operator does is it chooses random positions within the matrix and then X of Theta is just the end vector of those randomly chosen positions.",
            "That's a very simple kind of sampling operator.",
            "But if you have something like multi task learning then this X is more complicated.",
            "What it looks like is you're sort of taking your original.",
            "Regression vectors, your covariates, and you're doing kinds of projections between your covariance and parts of your matrix, maybe with a particular column or a subset of the matrix.",
            "Right so, but for the purpose of this talk will just sort of stay high level.",
            "You just want to think there's some kind of operator that Maps matrices to N vectors, and your problem is you observe why is some of these unknown numbers plus some noise, and what you'd like to do is you'd like to recover a good estimate of Theta star.",
            "OK.",
            "Right, and so we're assuming also that they to store has some kind of low rank structure.",
            "So."
        ],
        [
            "Natural family of estimators or what any person machine learning would do, or one thing you could do, you would say I'm going to give you a loss function that measures for a given matrix data.",
            "It measures how well you fit the data.",
            "Right, so the data you observe the outputs and you observe X, that's your operator of covariates or predictors, and so you have a loss function that fits that.",
            "And what you'd like to do, of course, is you'd like to impose a rank constraint, but as we all know, rank constraints are nasty and combinatorial, so a standard thing to do is you relax the rank constraint to the nearest convex norm.",
            "And so that's something called the nuclear norm.",
            "That's just a fancy way for saying, take the matrix, compute its singular values.",
            "Those are non negative quantities and then add them up.",
            "Right, so it's also sometimes called the trace norm.",
            "It would be the trace if it was a positive semi definite matrix.",
            "So if you think about it, just the L1 norm on the vector of singular values.",
            "Right, many losses are possible, but often people use least squares loss just partially for reasons of computational tractability.",
            "Right, so this is a convex program.",
            "It's actually what's known as an instance of what's known as a semidefinite program, and so it can be solved quite efficiently.",
            "There are sort of fast algorithms for solving it, and there's a whole nice line of work many people in both machine learning and optimization that when you have particular structure typically structure, let's say on X or structure on the loss, can you get sort of very fast?",
            "Specifically tailored algorithms, and so there's been a nice line of work on doing that for different models.",
            "So here I won't be talking so much about the computational issues.",
            "I'm sort of just going to assume that you have a way of solving this, and there are ways I'm not sort of punting on that issue.",
            "There are fast ways of solving it.",
            "But what I'd like to understand is I'd like to get intuition so you're solving this program.",
            "This is a random semidefinite program because the data is random.",
            "So you're getting the random matrix Theta hat, and I'd like to understand under what conditions is that random matrix close to the unknown matrix Theta star, right?",
            "So I'd like to analyze consistency of the estimator.",
            "But we'd like to do it in a regime in the high dimensional regime.",
            "I'd like it to be the case that my sample size N is allowed to be potentially much less than D1 by D2.",
            "Right, so the problem in some sense is ill posed.",
            "But with the rank constraints, we'll see that no.",
            "In general these problems become well posed, but sort of understanding exactly how that works.",
            "That's what we would like to understand in the next couple of slides.",
            "OK, so just to convince you these methods work right so.",
            "Well, to make the method work, there's a Lambda Anna regularization parameter.",
            "You have to choose that in a certain way.",
            "And our theory is going to give you a particular way in which that should be chosen in order for good things to happen."
        ],
        [
            "But just to sort of demonstrate that good things do happen, let's look at how this works for noisy matrix completion, right?",
            "So this is the matrix completion model you're observing randomly chosen entries.",
            "These entries are a lot of work, is looked at noiseless matrix completion, but this is the more realistic setting where the entries are perturbed by some noise.",
            "Um?",
            "And you can do it either for exactly low rank or you can do it for approximately low rank matrices.",
            "It doesn't really matter, but the kinds of curves you get what I'm plotting here is I'm plotting the Frobenius norm.",
            "That's just the squared error of what the estimator returns compared to the truth.",
            "And I'm plotting that versus sample size.",
            "And so you see, if I do a matrix that's 40 by 40, so 1600 entries, then it drops down quite quickly and it's getting quite low.",
            "Let's say the .1 you know by 1000 samples.",
            "If you have 3600 entries again drops.",
            "It's quite low by about 2000.",
            "And all of these curves are dropping.",
            "You can see they're all consistent as N gets big, you're getting, you're going to recover the matrix up to a.",
            "An error here that's just in our simulations below .1.",
            "I think we are scaling our matrices to have the original unhappiness norm One South.",
            "I guess you'd call that relative error, right?",
            "Yeah, you have to do some scaling 'cause you have to define the signal to noise ratio, so that's the one that we chose.",
            "So what you see here is that these curves all shift to the right right as the matrices get bigger.",
            "Then naturally, you expect you need more data to get to the same level of error, and just so you sort of understand what the point I'm going to be driving theory a bit later, but what's the point of that theory?",
            "Why should I care about the kinds of results that this Community this kind of community is developing?",
            "Well, the reason is because if the people get their results right, then what you can do is re scale the axis and make all of those curves align up on top of one another.",
            "Right, So what I've done here is I've divided the axis by a certain function.",
            "And that function is not what my graduate student obtained by tweaking and running with some parameters.",
            "That function is what the theory told me to do.",
            "The theory says there's going to be a function of the dimension of the rank.",
            "Other parameters of the problem, that if you re scale it, all of these curves should look roughly the same.",
            "So this is what's called a high dimensional scaling laws showing you that somehow the behavior of this model is very predictable in some sense that if you scale in the right way and you define a rescaled sample size, all of the curves look the same.",
            "So that's sort of the goal.",
            "I guess I sort of operating two communities.",
            "I sometimes deal with more theoretical types, who unfortunately I think among theorists, are sometimes this opinion that, oh, if you did a simulation, or you actually implemented your method, then you're not serious.",
            "I think that's completely false.",
            "I think that you know people are proving things.",
            "What is the point of proving things?",
            "The point of proving things is trying to try and explain what actually happens in practice, right?",
            "I mean, personally, I think that's the goal of the field, so I think it's more convincing if someone gives you a result and says look, this is what the result predicts and this is how the method behaves.",
            "Admittedly this is for simulated data, so you can criticize.",
            "That's fine, but at least it's somehow bringing us closer between what theory says and what's actually happening in practice.",
            "So I think would be good if theorists did not denigrate simulation or implementation, particularly implementation.",
            "Actually, it's one thing to say you have a polynomial time method.",
            "It's another thing to actually have something that runs so.",
            "OK, so what's going on here?",
            "Why is this somehow?",
            "I mean, at one level I'm showing you something that's not surprising, but another level is surprising because."
        ],
        [
            "If you think about, let's think about this loss function.",
            "If you have only N terms and you compute the Hessian of this function.",
            "You're going to get a kind of thing that basically has rank N. There's going to be there's only end terms.",
            "This is an N dimensional thing.",
            "So you can have a Hessian of a function that's in a space that's D1 by D2 that has at most rank N. And in all the cases were interested in an is much less than D 1 * D Two right, so the normal way if you go back, let's say even just to classical statistics, the normal way of proving consistency results is you take the loss function and you do a Taylor series expansion and you get the second the Hessian, the second derivative, and you invert that.",
            "But in this case the Hessian will never be invertible, so you can't do those kinds of tricks, so something else is required.",
            "Right the picture this geometrically of what I'm trying to say here is that if you think about the loss function, if you think about what you're trying to minimize, it typically looks like this in high dimensions, it's got some directions and."
        ],
        [
            "Which it's very strongly curved, but when the matrix dimensions are larger than N, it's got a whole number of directions in which it's totally flat.",
            "There's a huge nullspace to this problem.",
            "Right, so that's a serious issue.",
            "If you didn't have any regularization, what it means is that this model is actually ill posed.",
            "It's an identifiable right?",
            "Because it means that I can construct matrices that just move along in these directions, and your Y, which is X times Theta.",
            "You'll never be able to see the difference is there's a whole subspace of matrices that you can't tell me anything at all about, right?",
            "So that's the challenge.",
            "These models generically or an identifiable.",
            "But of course, the whole point is."
        ],
        [
            "I have an extra picture.",
            "We have an extra bit of structure.",
            "We've got a regularizer.",
            "And we have the assumption that Theta stars either low rank or near low rank, and that's going to help us right?",
            "So the name of the game is that the loss function is somehow degenerate, but the regularizer can help us out.",
            "The regularizer can kill off some directions and that's what you need generically to prove these kinds of results.",
            "What you need is that the regularizer somehow kills these flat directions and restricts you only to nice curved erections.",
            "Right, so there's sort of a nice geometric picture here.",
            "You need flat directions and you need the regularizer to kill off the flat directions.",
            "OK, so that's the picture, let me."
        ],
        [
            "Plane how you can kill off flat directions.",
            "So a couple years back we defined a notion of restricted strong convexity.",
            "It's a notion that holds more generally, not just for quadratic loss functions, but I'll describe it in the case where you have a quadratic loss function.",
            "So what we're requiring is the following, requiring that you've got this observation operator.",
            "It's taking your matrices and it's mapping them to an end vector, so you do that, and then you look at the L2 norm over N of your set of observations.",
            "What we want to have happen is we'd like there to be a constant.",
            "We call this a curvature parameter gamma, such that for any matrix Theta, whatever you choose, I want this quantity to be at least as large as gamma times the Frobenius norm of the matrix.",
            "Frobenius Norm Assist a fancy name for just the ordinary L2 norm on the elements of the matrix.",
            "Right now, if I just had that if I didn't have this extra term, this will never hold, right?",
            "'cause this may this.",
            "This operator has a null space, so anything in the null space.",
            "This is 0, so the regularizer comes into play 'cause it gives you slack.",
            "It gives you some slack parameter we call Kappa and then you take the square of the nuclear norm.",
            "Right?",
            "So what you're guaranteeing here, if you think about it?"
        ],
        [
            "Is, well, a couple of comments.",
            "If you didn't have a slack parameter.",
            "This is just one way of writing.",
            "One is the least squares loss strongly convex, strongly convex.",
            "This means that you're curved in all directions no matter what direction you look.",
            "You're curved, but we know that never happens in high dimensions, so the slack parameter really is essential in this definition.",
            "Now, probably many of you have heard of things like restricted isometry properties.",
            "This is related, but it's much milder.",
            "There's no sense of isometry, and there's no sense in which you need any kind of upper bound.",
            "The only thing that you need for statistical problems is a lower bound right upper bounds if X, Theta inflates certain directions of Theta.",
            "That's great for you because it just increases signal to noise ratio, so the only thing that's needed is lower bounds, upper bounds, or isometries.",
            "These are all just.",
            "Ways of actually guaranteeing that this holds, but this is the essential condition that needs to hold.",
            "Right, and So what?",
            "It's sort of saying is saying that if Theta is in the null space of this operator right, then it must be the case that somehow the nuclear norm is quite big relative to the Frobenius norm.",
            "Right?",
            "'cause once the nuclear norm is big then it will kill this term and you'll get something 0 or negative here and then it's OK.",
            "Right, So what?",
            "It's sort of saying is, as long as your method restricts to matrices where the nuclear norm is smallish relative to the Frobenius norm, then it's possible that this will hold right, and so it's saying that you will have curvature if you have matrices where the ratio of the nuclear to Frobenius norm is not too big and those are exactly matrices that are near low rank, right?",
            "There may not be exactly low rank, but they have to have fast decaying singular vectors are singular values.",
            "Right, So what this is doing?"
        ],
        [
            "And it's sort of saying if you have this tolerance."
        ],
        [
            "Term it's saying I'm going to kill off these directions.",
            "Right when that restricted strong convexity term holds, it means all of these directions are matrices that are not close to low rank matrices.",
            "They're all sort of generically quite high rank, and these directions which are good directions."
        ],
        [
            "These curved erections there you have matrices that look lower rank.",
            "OK, so that's a condition."
        ],
        [
            "Well, you might say, when does that condition hold?",
            "But it holds for many models.",
            "It holds for matrix completion in a slightly modified form, but essentially this condition it holds for multi task regression problems.",
            "It holds when you're trying to do system identification for autoregressive processes.",
            "Things like this hold for matrix decomposition problems, right?",
            "So there's a whole class of models for which this kind of condition holds.",
            "There's some work to showing that it holds in different cases.",
            "But if this condition holds, then you can say interesting things about the estimator.",
            "So let me do that now.",
            "Right, so just to refresh what we're doing is we're given these noisy observation model.",
            "We're estimating the unknown matr."
        ],
        [
            "By solving an SDP.",
            "And we're getting this Theta hat and we want to understand how close that is to Theta star.",
            "And what we're assuming is the first assumption is what I just said that you have a."
        ],
        [
            "Strong convexity condition.",
            "Here I'm doing it for quadratic loss, but you could do this more generally if you were in a classification problem for logistic loss or other kinds of log, linear losses are fine too.",
            "And what I'm assuming is that you choose your regularization parameter Lambda N to be bigger than two times a certain quantity.",
            "This is the ad joint of your operator.",
            "It's something that Maps the noise vector and vector to a matrix, and then you need to compute the maximum singular value of that matrix.",
            "So if you choose Lambda N in that way, by the way, there is an issue hidden in that choice.",
            "You think about it.",
            "If you like, we'll discuss it in a moment.",
            "Then the guarantee is the following that no matter what matrix you take, it doesn't have to be a low rank matrix.",
            "Any matrix you want any solution to the SDP?",
            "There might be more than one, but anyone that you find or your algorithm finds its Frobenius norm error will be less than a minimum minimum over R. You want to think of."
        ],
        [
            "RR is a parameter you choose.",
            "It's like what rank would you like to choose to estimate?",
            "And then you get 2 natural terms.",
            "You get one term which is proportional to R and the other which depends on the sum over the tail of the singular values.",
            "So the sum over J larger than R + 1 of the singular values of the unknown matrix.",
            "Right, so it's very natural if your matrix were exactly low rank, so you were rank R, then you set R to be the rank of the matrix here.",
            "And then all of these are zero.",
            "The approximation error would completely vanish and you'd get an error that's proportional to the regularization parameter.",
            "It scales inversely with the curvature, right?",
            "That's what you'd expect if the curvature becomes low, then your problem is harder.",
            "The curvature is high, your problems easier.",
            "And it scales proportionately to the rank that you chose, so that sort of makes intuitive sense.",
            "But in general, if you don't have a matrix that exactly low rank, then you have a bound and what you need to do is you need to play a game where you try and figure out what's the right choice of R to make this term equal to that, so you get the best possible upper bound.",
            "Alright, so that's the usual game that we play.",
            "You're trading off well, can call it bias or variance, or I'm calling it here.",
            "Estimation error in approximation error.",
            "That's the usual game we play.",
            "Yep.",
            "It is essentially a corollary of the more general framework, yes, so I think Ben you on Monday spoke a little bit about a general framework with restricted strong convexity, indecomposable regularizers.",
            "The nuclear norm is a decomposable regularizer, and so it falls within that framework.",
            "So yes, for those of you who know that that is true.",
            "So what's not satisfying about this result?",
            "Well, the form of it.",
            "Sort of.",
            "You know, when you prove things, it's nice to be reassured that they make sense, right?",
            "That's the whole point.",
            "This makes sense.",
            "The form of it, but what's not satisfying is that I'm playing games here as a user, can you actually choose Lambda end to be bigger than this quantity?",
            "That's what you need to do right when you implement it to have this guarantee, you need to be sure that holds.",
            "You can't actually, because this depends on the noise W you know X, so you know X star, but you don't know W, so we're cheating slightly there, but this result actually holds deterministically, like there's no probability in the result yet.",
            "So where does the probability come in?",
            "It comes in because you have to make some assumptions about your noise or some assumptions about maybe you have a random X, like in matrix completion.",
            "So this becomes a random variable and you have to bound that so there's a little bit of work in doing that, But that's something that you can do.",
            "And if you do that for different models then you'll get different choices of the regularization and then you'll plug these in here and you'll get various rates.",
            "Right, so that's sort of the last step.",
            "The last step is you need to use probability theory to ensure that you make some choice of this, and this will hold with high probability.",
            "Store.",
            "So W here is just the noise vector X stars the ad joint of X.",
            "So what it does is it takes an N vector and it Maps it back to the space of matrices.",
            "If yeah, it's it's like the transpose except it's sort of an operator on D1 by D2 to N, right?",
            "So you know X star because in your observation model we know X&Y, but we don't know W. But if you assume that was something like Gaussian or more generally, just had some kind of tail behavior.",
            "Then you can bound how big that maximum singular value is, so there's."
        ],
        [
            "Work there, but somehow conceptually the work is clear.",
            "The path is clear what you need to do.",
            "OK, so in the last sort of 10 minutes or so, let me talk a little bit about specific models, right?",
            "That's a fairly generic result, but if you want to show that it has teeth and you'd like to say, OK, let's look at some models that are interesting and people are studying or used in practice.",
            "And let's see what we can say.",
            "So let's look at matrix completion to start.",
            "Matrix completion what you're doing is the element of your observation operator.",
            "You're choosing a random index from your matrix AI by some random index, and basically what you're doing is you're taking the trace inner product of your matrix in a mask matrix, something that's all zeros except for one in one position.",
            "So this is a fairly challenging model because even in the noiseless setting various people dating back to rechten kandas have recognized that it's an identifiable.",
            "It's very easy to see why it's an identifiable even if data stars low rank, right?",
            "So the reason it's under identifiable as you can always construct an annoying matrix that's rank one but concentrates all its mass in the upper left entry.",
            "Right, so if you start applying this operator to it, you're randomly sampling entries, but this is just one entry.",
            "When N is much less than D1D2, you'll almost never see that entry with very high probability will never see it.",
            "Right, so you cannot distinguish between this matrix and the all zeros matrix.",
            "So what past work is done is they've imposed what are known as eigen incoherence conditions.",
            "What this just means, roughly speaking, it means that I want to be sure that the singular vectors are the eigenvectors in this case of Theta star do not align with things that look like this with things that look like the sparse basis vectors.",
            "Right, if you do that, then you're getting rid of the identifiable part.",
            "But if you think about noisy problems, it's not really quite severe restriction.",
            "It's not actually necessary.",
            "If we don't care about exactly getting the matrix right.",
            "So the way to think about it is, suppose I gave you a matrix that consisted of a good part that had good eigen incoherence.",
            "But then I added in a bad part.",
            "I added in a small Delta times this bad poisonous matrix.",
            "Right I can incoherence would immediately rule out this matrix right?",
            "Because it aligns if I choose things properly.",
            "You can make an eigen a singular vector aligned with this guy.",
            "But somehow, if we're in a noisy setting, which in most learning problems is the case, I don't care about getting the matrix exactly.",
            "So I should only care about this component if Delta is somehow large.",
            "If Delta was relatively small in some sense, then I'll just ignore that part and not estimate it.",
            "I won't find it, but I shouldn't care about it because it's not big anyway."
        ],
        [
            "Alright, so the milder thing to do if you are only interested in approximate matrix recovery is what you should do is compute the spikiness ratio.",
            "You should compute the ratio of the Infinity norm of the matrix times D over the sum of the squares of all its entries."
        ],
        [
            "Right, why is that?",
            "An interesting quantity?",
            "Well, it's a quantity that's between one and D. It's going to be one if your matrix is perfectly uniform, all its entries are equal.",
            "Then the ratio of the Infinity norm Times D to the Frobenius is exactly 1.",
            "If your matrix is nasty, like this guy here, then the ratio will be D. I'm looking at square matrices for the moment here.",
            "Right, so our results are going to be stated in terms of this ratio.",
            "If that ratio is out of control, then you can't say much, but in practice you expect that ratio for a reasonable matrix wouldn't be too large, and then you can."
        ],
        [
            "Say something interesting.",
            "Um?",
            "So here's a a result about matrix completion noisy matrix completion.",
            "So just to make things concrete, let me give you a class of not low rank matrices but near low rank matrices.",
            "So we'll look at all matrices that if you take their singular values and you raise them to the cute power.",
            "Then that sum is bounded less than some RQ.",
            "So this is a fancy way if Q is 0, this is just a fancy way of saying that you have a low rank matrix.",
            "If Q is bigger than zero, you're matrices can be full rank, but you're saying that the singular values are going to drop off fast, right?",
            "So this is one way of characterizing what it means for a matrix to be approximately low rank.",
            "So what I fear it guarantees is that the Frobenius error will be less than or equal to the radius of the ball, so exactly low rank case.",
            "That would be the rank.",
            "Times some terms, the Alpha is the spikiness ratio and you get D log D. So matrix dimensions log D / N to the 1 -- Q on 2.",
            "So that result is has the right flavor.",
            "The log D is kind of superfluous, but the ratio of the dimension to N. That's sort of correct.",
            "And if you think about just think about the case Q = 0, That's the easiest case to understand in Q0 what you're getting is the rank times the dimension.",
            "So you're saying that the rates roughly rank times dimension over sample size.",
            "That makes sense because a matrix in D dimensions with rank R has on the order Rd degrees of freedom.",
            "So this results optimal.",
            "You can prove that you cannot improve this result up to that annoying logarithmically factor, but I think there are ways of getting rid of that log so it's sort of the correct rate.",
            "No method can actually estimate matrices at a better rate than this.",
            "About this stuff.",
            "Right so sub exponential tail.",
            "This means that something like Bernstein's condition holds.",
            "It means that the moment generating function exists in a neighborhood around zero.",
            "Any bounded noise is certainly sub or sub exponential.",
            "Heavy tailed noise, one could prove results like this, but it would present you have to use a different argument.",
            "The rates possibly could change just because you're asking where the noise conditions are coming in is you need to bound this quantity.",
            "So we're using subexponential properties in the noise to use a random matrix theory results to control that.",
            "Um?",
            "Let me just wrap up by comparing to some other results.",
            "There's been some."
        ],
        [
            "This past work, if we look at the special case of rank exactly low rank matrices.",
            "As I said, our result is giving something like Rd times log D / N."
        ],
        [
            "Candace and plan looked at this same estimator.",
            "And they tried to sort of extrapolate from exact recovery results.",
            "And the result that they get in some ways is better than ours because it's a result that if the noise variance went to zero, they would get exact recovery.",
            "But it has some funny scalings.",
            "If you let the dimension scale, it actually diverges, and it also diverges with the sample size, so it's not really capturing for the noisy case what's going on, it's the right result for the noiseless case.",
            "Keshavan Montanari Anno had a very nice paper, varied."
        ],
        [
            "Print method quite simple based on trimming the matrix and applying SVD.",
            "What they proved is a result that removes the log D from RR scaling.",
            "So somehow it's right there.",
            "But it also involves the matrix condition, number matrix condition number is the ratio of the maximum to minimum singular value, so that could cause some bad scaling, particularly if you think of a matrix whose got some decaying singular values.",
            "So our results not strictly better, but you can see that there's some sense and it improves certain aspects of."
        ],
        [
            "Of that result.",
            "Yep."
        ],
        [
            "Patient Superman is also restricted.",
            "Sorry which.",
            "This right?",
            "So they're looking exactly rank are matrices.",
            "No, it's not the restricted minimum, it's the ratio of the maximum to minimum singular values of the true unknown matrix, so that could diverge.",
            "I could play games and make that diverge.",
            "You think about their result is optimal if the matrix sort of has homogeneous singular values in is exactly rank R, but none of this past work also applied to general matrices, which are approximately low rank.",
            "That I think is actually important, 'cause I personally don't believe that any matrix and practices exactly.",
            "Low rank.",
            "OK, so let me just wrap up there's other things that I could say but just just to wrap up.",
            "Um, so I guess high level.",
            "I hope I've convinced you that high dimensional problem matrix problems occur in different."
        ],
        [
            "Settings and they're interesting.",
            "Many people are looking at estimators based on the nuclear norm and other kinds of convex relaxations.",
            "It's quite an active area of work, lots of new papers appearing so exciting time to be in the area.",
            "What I've hoped to describe today is that there's sort of one single result that has basically two ideas and notion of strong convexity and a certain property that the regularizer has to satisfy.",
            "Nuclear norm does that can be used to compute bounds in a fairly wide range of settings, and I didn't talk so much about this, but often the bounds are actually optimal, meaning that even if you used a better estimator, even if you did an exponential time search, let's say in the matrix completion problem.",
            "You wouldn't be able to get an algorithm that has much better accuracy than these kinds of simple relaxations, so in those cases there's some sense that the algorithms are essentially the best that we could hope for.",
            "They're both computationally efficient, and they are statistically optimal, right?",
            "So it's sort of a nice meeting of the two worlds in those cases, so I've listed a couple references if you're interested in more details.",
            "This is the more general viewpoint that CJ was mentioning.",
            "This kind of unified framework.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is joint work with some students, Alec Agarwal, Sahan, Negahban, a former postdoc, Pradeep Ravikumar and colleague been you who I suspect you heard of heard speak on Monday.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As Joshua said, it's about convex methods and it's about high dimensional matrices.",
                    "label": 0
                },
                {
                    "sent": "So let me just sort of set the stage a little bit great.",
                    "label": 0
                },
                {
                    "sent": "So this area is about high dimensional datasets.",
                    "label": 0
                },
                {
                    "sent": "I think that's familiar to an audience like this.",
                    "label": 0
                },
                {
                    "sent": "So basically we're getting more and more data, both more and more samples.",
                    "label": 0
                },
                {
                    "sent": "But I think also Interestingly, we're getting richer and richer kinds of data.",
                    "label": 0
                },
                {
                    "sent": "As technology evolves, we are able to measure and collect interesting kinds of phenomena.",
                    "label": 0
                },
                {
                    "sent": "So things like social networks.",
                    "label": 1
                },
                {
                    "sent": "We have massive amounts of data, things like recommender systems, Netflix, Amazon, other kinds of users, huge amounts of data on all of us in computer vision.",
                    "label": 1
                },
                {
                    "sent": "We of course huge image databases, video databases.",
                    "label": 0
                },
                {
                    "sent": "I do some work in astronomy and the astronomers collect so much data that essentially what they're asking us to do is figure out very fast ways to throw out 99% of the data quickly 'cause they don't have enough disk space to actually store it.",
                    "label": 0
                },
                {
                    "sent": "Right, so this sort of modern data delusion is certainly going on and getting worse.",
                    "label": 0
                },
                {
                    "sent": "This talk sort of falls.",
                    "label": 0
                },
                {
                    "sent": "You can be motivated by this kind of question.",
                    "label": 0
                },
                {
                    "sent": "This is the question that many people are asking when you're confronted with high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So they're sort of saying, suppose that I have N = 100 samples, but I'm measuring things that take place in some ambient space.",
                    "label": 0
                },
                {
                    "sent": "I'll call D with 1000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Do we really expect theory?",
                    "label": 0
                },
                {
                    "sent": "So classical theory things like law of large numbers or consistency of things like maximum likelihood?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That would be based on saying let's let's end the number of samples get really, really big and think of the model is fixed and then good stuff happens, right?",
                    "label": 0
                },
                {
                    "sent": "You have law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "You have central limit 3 so on.",
                    "label": 0
                },
                {
                    "sent": "But the question is if really the regime in which we're working is N is 100 and D is 1000, is that kind of theory going to be useful?",
                    "label": 1
                },
                {
                    "sent": "So I think you can answer this question in two ways.",
                    "label": 0
                },
                {
                    "sent": "It's possible that you would have a data set where actually the classical theory would be useful for this sort of depends on how quickly the asymptotic's kick in, but what various people are interested in doing is not doing asymptotically.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually getting statements that are non asymptotic and they hold for all sample sizes and all sizes of models and other kinds of parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the area of high dimensional statistics or high dimensional learning theory.",
                    "label": 0
                },
                {
                    "sent": "You want to study estimators, algorithms.",
                    "label": 0
                },
                {
                    "sent": "You want to allow for scalings in which the sample size might be much smaller than the ambient dimension, or maybe roughly the same order.",
                    "label": 0
                },
                {
                    "sent": "And you'd like to have results that aren't asymptotically like to say for fixed ND with some probability, good stuff happens.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the general picture, but the same time, I guess you're probably wondering if you really have 1000 parameters and only 100 samples, then there's probably various kinds of impossibility theorems.",
                    "label": 0
                },
                {
                    "sent": "You can't do much right if you have so many 10 times more degrees of freedom, So what?",
                    "label": 0
                },
                {
                    "sent": "Lots of people are doing is investigating various types of low dimensional structure and sort of saying typical datasets might lie in 1000 or 10,000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "But probably there's a lot of low dimensional structure that if we understood it.",
                    "label": 0
                },
                {
                    "sent": "We could exploit it so then the name of the game is what kinds of low dimensional structure are interesting and practically relevant?",
                    "label": 0
                },
                {
                    "sent": "An what kind of techniques can we efficient ones computationally efficient ones?",
                    "label": 0
                },
                {
                    "sent": "Can we develop to exploit it?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's very broad overview.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a broad area.",
                    "label": 0
                },
                {
                    "sent": "Many people are working in it.",
                    "label": 0
                },
                {
                    "sent": "In this talk I want to talk focus on just one particular kind of low.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensional structure, and that's one which probably many of you are familiar with.",
                    "label": 0
                },
                {
                    "sent": "When you have a high dimensional matrix, I'll be thinking about matrices or call Theta star dimension D1 by D2 and this is going to be quite large typically, but you want to think about the case where if you computed in SVD.",
                    "label": 0
                },
                {
                    "sent": "So if you computed the left singular vectors, the singular values and the right singular vectors.",
                    "label": 0
                },
                {
                    "sent": "It might be low rank, so you might have a rank R that's much less than the minimum of D1D2.",
                    "label": 0
                },
                {
                    "sent": "Or maybe this holds only in an approximate sense that the sense in which the matrix is well approximated by a low rank.",
                    "label": 0
                },
                {
                    "sent": "That's probably much more realistic.",
                    "label": 0
                },
                {
                    "sent": "In practice, you don't in practice have exactly low rank things, but you have things that if you sort of just took the top singular vectors and threw away the rest, you get a very good approximation.",
                    "label": 0
                },
                {
                    "sent": "So let me give you some examples just to start up where these kinds of low rank structures arise.",
                    "label": 0
                },
                {
                    "sent": "One that you'd be familiar with.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about so much methods like principal component analysis, Canonical correlations.",
                    "label": 0
                },
                {
                    "sent": "These of course are looking for low rank or near low rank structure, but there are many others, so let me begin with some examples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One problem that comes up in imaging systems.",
                    "label": 0
                },
                {
                    "sent": "Specially these days what I'm showing you here is I'm showing you 2 pictures.",
                    "label": 0
                },
                {
                    "sent": "From two different cameras of a famous cathedral in Germany.",
                    "label": 0
                },
                {
                    "sent": "And the point is that the cameras are slightly offset from one another, so the same image but one camera is like this.",
                    "label": 0
                },
                {
                    "sent": "The others like this, it's kind of mimicking a binocular human vision system, right?",
                    "label": 0
                },
                {
                    "sent": "We get depth actually because your eyes are slightly separated.",
                    "label": 0
                },
                {
                    "sent": "You're actually getting a fair bit of disparity or depth information from images.",
                    "label": 0
                },
                {
                    "sent": "So if you're clever, what you could do, or if you can cross your eyes, you can actually fuse these images, make them overlap on top of one another, and then you'll see a very nice sort of cathedral and you'll see pillars behind one another.",
                    "label": 0
                },
                {
                    "sent": "You'll see a nice sense of depth.",
                    "label": 0
                },
                {
                    "sent": "Right, so in practice, modern vision systems.",
                    "label": 0
                },
                {
                    "sent": "We don't just have two cameras.",
                    "label": 0
                },
                {
                    "sent": "You might have, let's say 50 cameras, right?",
                    "label": 0
                },
                {
                    "sent": "So modern sporting events these days would have 50 or 100 cameras, and they're trying to do all sorts of things quite quickly, like reconstruct depth or do rotations of the scene.",
                    "label": 0
                },
                {
                    "sent": "One way to model this is that it's actually a prediction problem, but it's actually a whole family of simultanea.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction problems.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can think that what you're trying to do is you have an unknown matrix in each column of this matrix is an image or some function of an image, something like a depth map that you're trying to reconstruct.",
                    "label": 0
                },
                {
                    "sent": "So you're assuming that you have D2 images and each image is D1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Alright, so D1 might be well depends how you represent the image, but D1 could be in the 10s of thousands easily and in the simplest case you would sort of imagine that what you actually have is a whole coupled set of linear predictors.",
                    "label": 0
                },
                {
                    "sent": "So you have X that's known to you and X times the images.",
                    "label": 0
                },
                {
                    "sent": "Plus noise is generating what you see, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a linear inverse problem and it's high dimensional.",
                    "label": 0
                },
                {
                    "sent": "You're thinking about the number of samples is going to be quite a bit smaller than D 1 * D Two the number of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "But what you expect is that the predictors this matrix have a lot of structure.",
                    "label": 0
                },
                {
                    "sent": "One kind of structure that does arise is low rank structure that not that the predictors are exactly low rank, but somehow that if you got an appropriate rotation of the matrix, there's a low rank space which they lie close to.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if you can exploit the low rank structure in this problem, then you can generate better predictors using this observation model.",
                    "label": 0
                },
                {
                    "sent": "So that's an instance of what's called low rank multitask learning, multitask regression.",
                    "label": 0
                },
                {
                    "sent": "This is regression discuss.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking about it as a linear model, but you can think of classification problems, all sorts of other problems that have this kind of low rank structure.",
                    "label": 0
                },
                {
                    "sent": "Another example that probably many of you are familiar with, particularly if you use something like Netflix.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would be the problem of recommender systems or collaborative filtering?",
                    "label": 1
                },
                {
                    "sent": "This was first studied, I think in machine learning community.",
                    "label": 0
                },
                {
                    "sent": "By Nati Srebro, Alana Nacla back in 2004.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice problem, again involving matrices.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that what Netflix has or other kinds of companies what they have is a large database that's think of as a matrix with individuals in the columns and movies across the individuals in the rows and movies across the columns.",
                    "label": 0
                },
                {
                    "sent": "This matrix is partially populated by numbers, numerical values that are ratings of movies that people have watched right.",
                    "label": 0
                },
                {
                    "sent": "So here Roger Federer has watched Woody Allen movie and because he's European and he likes Woody Allen movies, so he gives it a four out of five.",
                    "label": 0
                },
                {
                    "sent": "He hasn't watched Jaws, but oddly enough Rafael Nadal has right.",
                    "label": 0
                },
                {
                    "sent": "So there's somehow this kind of sharing that's going on.",
                    "label": 0
                },
                {
                    "sent": "You got a very sparse matrix, lots of probably the majority of the matrix is stars, realistically.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean I watch a lot of movies personally, but I think, well, unless you sort of quit your professional life and just watch movies all the time, the number of non stars that you could, you know movies.",
                    "label": 0
                },
                {
                    "sent": "You could realistically rates going to be a banishing fraction of the total Netflix database.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's a problem with tons of missing data.",
                    "label": 0
                },
                {
                    "sent": "And the collaborative filtering problem is essentially you'd like to fill in the missing entries.",
                    "label": 0
                },
                {
                    "sent": "You'd like to recommend to people who haven't seen a movie you'd like to find things that have four and five, so you'd like to fill in predictions for them.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, again, it would be an underdetermined problem, but what you sort of expect in practice is that what drives people's ratings of movies.",
                    "label": 0
                },
                {
                    "sent": "There's probably some kind of hidden meta factors you could query people about.",
                    "label": 0
                },
                {
                    "sent": "Things like do you like horror movies?",
                    "label": 0
                },
                {
                    "sent": "Do you like Arnold Schwarzenegger appearing naked?",
                    "label": 0
                },
                {
                    "sent": "Flexing his biceps in movies.",
                    "label": 0
                },
                {
                    "sent": "There's probably some factors like this and a relatively small number of them that would be quite predictive of people's ratings.",
                    "label": 0
                },
                {
                    "sent": "And so if you have that kind of factor model in mind, what it translates to is that the real full unknown matrix, although it's very large dimensional, you know hundreds of 1000 dimensional if not million.",
                    "label": 0
                },
                {
                    "sent": "There's probably a very small number of.",
                    "label": 0
                },
                {
                    "sent": "Components that are actually have high variance.",
                    "label": 0
                },
                {
                    "sent": "So when people do experiments they find well depends on the experiment.",
                    "label": 0
                },
                {
                    "sent": "But you might find that 30 singular vectors give a reasonable approximation.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly low rank, but it's well approximated by low rank.",
                    "label": 0
                },
                {
                    "sent": "So another kind of problem.",
                    "label": 0
                },
                {
                    "sent": "These are interesting to security.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Robustness issues arise in recommender systems.",
                    "label": 0
                },
                {
                    "sent": "Probably some of you are familiar with this.",
                    "label": 0
                },
                {
                    "sent": "There's a very famous breakdown of Amazon.",
                    "label": 1
                },
                {
                    "sent": "the Amazon recommender system.",
                    "label": 0
                },
                {
                    "sent": "Of course, they have the same kind of problem.",
                    "label": 0
                },
                {
                    "sent": "Write your rating things like, well your rating by buying books or whatever you buy on Amazon and based on that they're trying to suggest other things you'd like to buy.",
                    "label": 0
                },
                {
                    "sent": "So there's a case in 2002 where it happened to be the case that if you bought this spiritual guide by Pat Robertson.",
                    "label": 0
                },
                {
                    "sent": "Then the recommend.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System would also recommend a certain sex manual to you if you're interested.",
                    "label": 0
                },
                {
                    "sent": "You can Google and quite quickly fill in these question marks.",
                    "label": 0
                },
                {
                    "sent": "Other people give talks about this and they are brave enough to actually tell you both the title and give you the cover, but I just can't do that.",
                    "label": 0
                },
                {
                    "sent": "'cause the old sort of dictate that would you do something that if your mother were in the audience you would feel proud about so I can't do that because of my mother.",
                    "label": 0
                },
                {
                    "sent": "But you're welcome to to Google.",
                    "label": 0
                },
                {
                    "sent": "I can guarantee that.",
                    "label": 0
                },
                {
                    "sent": "It's entertaining.",
                    "label": 0
                },
                {
                    "sent": "But the point here is right that this was, you know, this was actually pretty simple manipulation.",
                    "label": 0
                },
                {
                    "sent": "It was people going into Amazon and spiking the database with love, spiritual guide and love sex manual.",
                    "label": 1
                },
                {
                    "sent": "So you had a lot of Co occurrences and then the naive system wasn't able to filter them.",
                    "label": 0
                },
                {
                    "sent": "And it's not just Amazon.",
                    "label": 0
                },
                {
                    "sent": "Of course, people looking at academic recommendation systems, you know people that informs where you're allowed to rate other people's books or write your own book.",
                    "label": 0
                },
                {
                    "sent": "You of course, see authors doing nasty things like having their friends go and rate the competitors books very low and then spike their own books with high ratings.",
                    "label": 0
                },
                {
                    "sent": "So all sorts of systems like this can be manipulated, and it's important to understand how we can.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mitigate those kinds of effects.",
                    "label": 0
                },
                {
                    "sent": "So one way in which some people have been studying this is instead of looking just at a low rank problem, you allow the low rank problem to have an additional noise matrix where this noise matrix can be adversarial.",
                    "label": 0
                },
                {
                    "sent": "But again it has structure one kind of structure that's been studied is when this matrix is sparse.",
                    "label": 0
                },
                {
                    "sent": "That means when a subset of users are manipulating sort of adding nastiness to the entries in the sort of true low rank matrix that you're trying to recover.",
                    "label": 0
                },
                {
                    "sent": "So various people have studied this.",
                    "label": 0
                },
                {
                    "sent": "I think the first people were Chandrasekaran Parillo and will ski and then other people have studied this or variants of this.",
                    "label": 0
                },
                {
                    "sent": "Another variant is when you have columns that are corrupted or rose that are corrupted right?",
                    "label": 0
                },
                {
                    "sent": "That would be when you have a user, let's say the row is corrupted, 1 user is being completely insane so you want to exclude that particular user.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's some other examples.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can talk about, but I think.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interest of time.",
                    "label": 0
                },
                {
                    "sent": "I'll sort of move on from this point.",
                    "label": 0
                },
                {
                    "sent": "Actually, how long should the talk last for?",
                    "label": 0
                },
                {
                    "sent": "OK, and so we began around 20 after so.",
                    "label": 0
                },
                {
                    "sent": "OK, so 9:15 should be the end OK?",
                    "label": 0
                },
                {
                    "sent": "So what I've done so far is at least I hope I've convinced you that there's many interesting problems that involve high dimensional matrices, right?",
                    "label": 0
                },
                {
                    "sent": "So high dimensional means the dimensions of the matrices are large, but in many of these problems were also interested in is that the number of samples is often less than the product of the dimensions of the matrix, so these problems in some sense are challenging because you really have fewer samples than the effective number, or the total number of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to understand is how do we solve prediction problems in this kind of setting?",
                    "label": 0
                },
                {
                    "sent": "So what I'd like to talk about is a standard method, nothing new in the method, but a standard method is that in order to enforce rank constraints in a convex way, you use something called a nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "It's basically just a convex relaxation of a rank constraint.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about just a class of estimators that are based on solving a convex program, and they apply to many different models they apply to all of the models that I mentioned here, and what I'd like to show is really.",
                    "label": 0
                },
                {
                    "sent": "There's just a couple of simple ideas that allow you to understand why these methods.",
                    "label": 0
                },
                {
                    "sent": "Well, when these methods work in practice they don't always work, of course, but under what conditions will they work and exactly how many samples do you need as a function of the matrix and other parameters of the problem to be sure that you're getting good estimates out of your model?",
                    "label": 0
                },
                {
                    "sent": "And then I'll give you a couple of examples.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mainly I think about gum completion and decomposition.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about matrix regression problems.",
                    "label": 1
                },
                {
                    "sent": "Pretty much all the problems I mentioned before can be thought of as this way.",
                    "label": 0
                },
                {
                    "sent": "So it's really just a generalization of linear regression.",
                    "label": 0
                },
                {
                    "sent": "You want to think that you have some unknown matrix, some high dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "And then you have some operator.",
                    "label": 0
                },
                {
                    "sent": "Just a fancy way for fancy way to say something that takes the matrix and it takes N samples of the matrix in some way and gives you an N vector of N samples.",
                    "label": 0
                },
                {
                    "sent": "Depending on the model, the sampling could be sampling is different if it's matrix completion, the sampling is very simple.",
                    "label": 0
                },
                {
                    "sent": "In matrix completion, each time what this operator does is it chooses random positions within the matrix and then X of Theta is just the end vector of those randomly chosen positions.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple kind of sampling operator.",
                    "label": 0
                },
                {
                    "sent": "But if you have something like multi task learning then this X is more complicated.",
                    "label": 0
                },
                {
                    "sent": "What it looks like is you're sort of taking your original.",
                    "label": 0
                },
                {
                    "sent": "Regression vectors, your covariates, and you're doing kinds of projections between your covariance and parts of your matrix, maybe with a particular column or a subset of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Right so, but for the purpose of this talk will just sort of stay high level.",
                    "label": 0
                },
                {
                    "sent": "You just want to think there's some kind of operator that Maps matrices to N vectors, and your problem is you observe why is some of these unknown numbers plus some noise, and what you'd like to do is you'd like to recover a good estimate of Theta star.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, and so we're assuming also that they to store has some kind of low rank structure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Natural family of estimators or what any person machine learning would do, or one thing you could do, you would say I'm going to give you a loss function that measures for a given matrix data.",
                    "label": 0
                },
                {
                    "sent": "It measures how well you fit the data.",
                    "label": 0
                },
                {
                    "sent": "Right, so the data you observe the outputs and you observe X, that's your operator of covariates or predictors, and so you have a loss function that fits that.",
                    "label": 0
                },
                {
                    "sent": "And what you'd like to do, of course, is you'd like to impose a rank constraint, but as we all know, rank constraints are nasty and combinatorial, so a standard thing to do is you relax the rank constraint to the nearest convex norm.",
                    "label": 0
                },
                {
                    "sent": "And so that's something called the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "That's just a fancy way for saying, take the matrix, compute its singular values.",
                    "label": 0
                },
                {
                    "sent": "Those are non negative quantities and then add them up.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's also sometimes called the trace norm.",
                    "label": 0
                },
                {
                    "sent": "It would be the trace if it was a positive semi definite matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it, just the L1 norm on the vector of singular values.",
                    "label": 0
                },
                {
                    "sent": "Right, many losses are possible, but often people use least squares loss just partially for reasons of computational tractability.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a convex program.",
                    "label": 0
                },
                {
                    "sent": "It's actually what's known as an instance of what's known as a semidefinite program, and so it can be solved quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "There are sort of fast algorithms for solving it, and there's a whole nice line of work many people in both machine learning and optimization that when you have particular structure typically structure, let's say on X or structure on the loss, can you get sort of very fast?",
                    "label": 0
                },
                {
                    "sent": "Specifically tailored algorithms, and so there's been a nice line of work on doing that for different models.",
                    "label": 0
                },
                {
                    "sent": "So here I won't be talking so much about the computational issues.",
                    "label": 0
                },
                {
                    "sent": "I'm sort of just going to assume that you have a way of solving this, and there are ways I'm not sort of punting on that issue.",
                    "label": 0
                },
                {
                    "sent": "There are fast ways of solving it.",
                    "label": 0
                },
                {
                    "sent": "But what I'd like to understand is I'd like to get intuition so you're solving this program.",
                    "label": 0
                },
                {
                    "sent": "This is a random semidefinite program because the data is random.",
                    "label": 0
                },
                {
                    "sent": "So you're getting the random matrix Theta hat, and I'd like to understand under what conditions is that random matrix close to the unknown matrix Theta star, right?",
                    "label": 0
                },
                {
                    "sent": "So I'd like to analyze consistency of the estimator.",
                    "label": 0
                },
                {
                    "sent": "But we'd like to do it in a regime in the high dimensional regime.",
                    "label": 0
                },
                {
                    "sent": "I'd like it to be the case that my sample size N is allowed to be potentially much less than D1 by D2.",
                    "label": 0
                },
                {
                    "sent": "Right, so the problem in some sense is ill posed.",
                    "label": 0
                },
                {
                    "sent": "But with the rank constraints, we'll see that no.",
                    "label": 0
                },
                {
                    "sent": "In general these problems become well posed, but sort of understanding exactly how that works.",
                    "label": 0
                },
                {
                    "sent": "That's what we would like to understand in the next couple of slides.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to convince you these methods work right so.",
                    "label": 0
                },
                {
                    "sent": "Well, to make the method work, there's a Lambda Anna regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "You have to choose that in a certain way.",
                    "label": 0
                },
                {
                    "sent": "And our theory is going to give you a particular way in which that should be chosen in order for good things to happen.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But just to sort of demonstrate that good things do happen, let's look at how this works for noisy matrix completion, right?",
                    "label": 1
                },
                {
                    "sent": "So this is the matrix completion model you're observing randomly chosen entries.",
                    "label": 0
                },
                {
                    "sent": "These entries are a lot of work, is looked at noiseless matrix completion, but this is the more realistic setting where the entries are perturbed by some noise.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And you can do it either for exactly low rank or you can do it for approximately low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter, but the kinds of curves you get what I'm plotting here is I'm plotting the Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "That's just the squared error of what the estimator returns compared to the truth.",
                    "label": 1
                },
                {
                    "sent": "And I'm plotting that versus sample size.",
                    "label": 0
                },
                {
                    "sent": "And so you see, if I do a matrix that's 40 by 40, so 1600 entries, then it drops down quite quickly and it's getting quite low.",
                    "label": 0
                },
                {
                    "sent": "Let's say the .1 you know by 1000 samples.",
                    "label": 0
                },
                {
                    "sent": "If you have 3600 entries again drops.",
                    "label": 0
                },
                {
                    "sent": "It's quite low by about 2000.",
                    "label": 0
                },
                {
                    "sent": "And all of these curves are dropping.",
                    "label": 0
                },
                {
                    "sent": "You can see they're all consistent as N gets big, you're getting, you're going to recover the matrix up to a.",
                    "label": 0
                },
                {
                    "sent": "An error here that's just in our simulations below .1.",
                    "label": 0
                },
                {
                    "sent": "I think we are scaling our matrices to have the original unhappiness norm One South.",
                    "label": 0
                },
                {
                    "sent": "I guess you'd call that relative error, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have to do some scaling 'cause you have to define the signal to noise ratio, so that's the one that we chose.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is that these curves all shift to the right right as the matrices get bigger.",
                    "label": 0
                },
                {
                    "sent": "Then naturally, you expect you need more data to get to the same level of error, and just so you sort of understand what the point I'm going to be driving theory a bit later, but what's the point of that theory?",
                    "label": 0
                },
                {
                    "sent": "Why should I care about the kinds of results that this Community this kind of community is developing?",
                    "label": 0
                },
                {
                    "sent": "Well, the reason is because if the people get their results right, then what you can do is re scale the axis and make all of those curves align up on top of one another.",
                    "label": 0
                },
                {
                    "sent": "Right, So what I've done here is I've divided the axis by a certain function.",
                    "label": 0
                },
                {
                    "sent": "And that function is not what my graduate student obtained by tweaking and running with some parameters.",
                    "label": 0
                },
                {
                    "sent": "That function is what the theory told me to do.",
                    "label": 0
                },
                {
                    "sent": "The theory says there's going to be a function of the dimension of the rank.",
                    "label": 0
                },
                {
                    "sent": "Other parameters of the problem, that if you re scale it, all of these curves should look roughly the same.",
                    "label": 0
                },
                {
                    "sent": "So this is what's called a high dimensional scaling laws showing you that somehow the behavior of this model is very predictable in some sense that if you scale in the right way and you define a rescaled sample size, all of the curves look the same.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the goal.",
                    "label": 0
                },
                {
                    "sent": "I guess I sort of operating two communities.",
                    "label": 0
                },
                {
                    "sent": "I sometimes deal with more theoretical types, who unfortunately I think among theorists, are sometimes this opinion that, oh, if you did a simulation, or you actually implemented your method, then you're not serious.",
                    "label": 0
                },
                {
                    "sent": "I think that's completely false.",
                    "label": 0
                },
                {
                    "sent": "I think that you know people are proving things.",
                    "label": 0
                },
                {
                    "sent": "What is the point of proving things?",
                    "label": 0
                },
                {
                    "sent": "The point of proving things is trying to try and explain what actually happens in practice, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, personally, I think that's the goal of the field, so I think it's more convincing if someone gives you a result and says look, this is what the result predicts and this is how the method behaves.",
                    "label": 0
                },
                {
                    "sent": "Admittedly this is for simulated data, so you can criticize.",
                    "label": 0
                },
                {
                    "sent": "That's fine, but at least it's somehow bringing us closer between what theory says and what's actually happening in practice.",
                    "label": 0
                },
                {
                    "sent": "So I think would be good if theorists did not denigrate simulation or implementation, particularly implementation.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's one thing to say you have a polynomial time method.",
                    "label": 0
                },
                {
                    "sent": "It's another thing to actually have something that runs so.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Why is this somehow?",
                    "label": 0
                },
                {
                    "sent": "I mean, at one level I'm showing you something that's not surprising, but another level is surprising because.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you think about, let's think about this loss function.",
                    "label": 0
                },
                {
                    "sent": "If you have only N terms and you compute the Hessian of this function.",
                    "label": 0
                },
                {
                    "sent": "You're going to get a kind of thing that basically has rank N. There's going to be there's only end terms.",
                    "label": 0
                },
                {
                    "sent": "This is an N dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "So you can have a Hessian of a function that's in a space that's D1 by D2 that has at most rank N. And in all the cases were interested in an is much less than D 1 * D Two right, so the normal way if you go back, let's say even just to classical statistics, the normal way of proving consistency results is you take the loss function and you do a Taylor series expansion and you get the second the Hessian, the second derivative, and you invert that.",
                    "label": 0
                },
                {
                    "sent": "But in this case the Hessian will never be invertible, so you can't do those kinds of tricks, so something else is required.",
                    "label": 0
                },
                {
                    "sent": "Right the picture this geometrically of what I'm trying to say here is that if you think about the loss function, if you think about what you're trying to minimize, it typically looks like this in high dimensions, it's got some directions and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which it's very strongly curved, but when the matrix dimensions are larger than N, it's got a whole number of directions in which it's totally flat.",
                    "label": 0
                },
                {
                    "sent": "There's a huge nullspace to this problem.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's a serious issue.",
                    "label": 0
                },
                {
                    "sent": "If you didn't have any regularization, what it means is that this model is actually ill posed.",
                    "label": 0
                },
                {
                    "sent": "It's an identifiable right?",
                    "label": 0
                },
                {
                    "sent": "Because it means that I can construct matrices that just move along in these directions, and your Y, which is X times Theta.",
                    "label": 0
                },
                {
                    "sent": "You'll never be able to see the difference is there's a whole subspace of matrices that you can't tell me anything at all about, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the challenge.",
                    "label": 0
                },
                {
                    "sent": "These models generically or an identifiable.",
                    "label": 0
                },
                {
                    "sent": "But of course, the whole point is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have an extra picture.",
                    "label": 0
                },
                {
                    "sent": "We have an extra bit of structure.",
                    "label": 0
                },
                {
                    "sent": "We've got a regularizer.",
                    "label": 0
                },
                {
                    "sent": "And we have the assumption that Theta stars either low rank or near low rank, and that's going to help us right?",
                    "label": 0
                },
                {
                    "sent": "So the name of the game is that the loss function is somehow degenerate, but the regularizer can help us out.",
                    "label": 0
                },
                {
                    "sent": "The regularizer can kill off some directions and that's what you need generically to prove these kinds of results.",
                    "label": 0
                },
                {
                    "sent": "What you need is that the regularizer somehow kills these flat directions and restricts you only to nice curved erections.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's sort of a nice geometric picture here.",
                    "label": 0
                },
                {
                    "sent": "You need flat directions and you need the regularizer to kill off the flat directions.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the picture, let me.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plane how you can kill off flat directions.",
                    "label": 0
                },
                {
                    "sent": "So a couple years back we defined a notion of restricted strong convexity.",
                    "label": 1
                },
                {
                    "sent": "It's a notion that holds more generally, not just for quadratic loss functions, but I'll describe it in the case where you have a quadratic loss function.",
                    "label": 0
                },
                {
                    "sent": "So what we're requiring is the following, requiring that you've got this observation operator.",
                    "label": 0
                },
                {
                    "sent": "It's taking your matrices and it's mapping them to an end vector, so you do that, and then you look at the L2 norm over N of your set of observations.",
                    "label": 0
                },
                {
                    "sent": "What we want to have happen is we'd like there to be a constant.",
                    "label": 0
                },
                {
                    "sent": "We call this a curvature parameter gamma, such that for any matrix Theta, whatever you choose, I want this quantity to be at least as large as gamma times the Frobenius norm of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Frobenius Norm Assist a fancy name for just the ordinary L2 norm on the elements of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Right now, if I just had that if I didn't have this extra term, this will never hold, right?",
                    "label": 0
                },
                {
                    "sent": "'cause this may this.",
                    "label": 0
                },
                {
                    "sent": "This operator has a null space, so anything in the null space.",
                    "label": 0
                },
                {
                    "sent": "This is 0, so the regularizer comes into play 'cause it gives you slack.",
                    "label": 1
                },
                {
                    "sent": "It gives you some slack parameter we call Kappa and then you take the square of the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So what you're guaranteeing here, if you think about it?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is, well, a couple of comments.",
                    "label": 0
                },
                {
                    "sent": "If you didn't have a slack parameter.",
                    "label": 0
                },
                {
                    "sent": "This is just one way of writing.",
                    "label": 0
                },
                {
                    "sent": "One is the least squares loss strongly convex, strongly convex.",
                    "label": 0
                },
                {
                    "sent": "This means that you're curved in all directions no matter what direction you look.",
                    "label": 0
                },
                {
                    "sent": "You're curved, but we know that never happens in high dimensions, so the slack parameter really is essential in this definition.",
                    "label": 0
                },
                {
                    "sent": "Now, probably many of you have heard of things like restricted isometry properties.",
                    "label": 0
                },
                {
                    "sent": "This is related, but it's much milder.",
                    "label": 0
                },
                {
                    "sent": "There's no sense of isometry, and there's no sense in which you need any kind of upper bound.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you need for statistical problems is a lower bound right upper bounds if X, Theta inflates certain directions of Theta.",
                    "label": 0
                },
                {
                    "sent": "That's great for you because it just increases signal to noise ratio, so the only thing that's needed is lower bounds, upper bounds, or isometries.",
                    "label": 0
                },
                {
                    "sent": "These are all just.",
                    "label": 0
                },
                {
                    "sent": "Ways of actually guaranteeing that this holds, but this is the essential condition that needs to hold.",
                    "label": 0
                },
                {
                    "sent": "Right, and So what?",
                    "label": 0
                },
                {
                    "sent": "It's sort of saying is saying that if Theta is in the null space of this operator right, then it must be the case that somehow the nuclear norm is quite big relative to the Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "'cause once the nuclear norm is big then it will kill this term and you'll get something 0 or negative here and then it's OK.",
                    "label": 0
                },
                {
                    "sent": "Right, So what?",
                    "label": 0
                },
                {
                    "sent": "It's sort of saying is, as long as your method restricts to matrices where the nuclear norm is smallish relative to the Frobenius norm, then it's possible that this will hold right, and so it's saying that you will have curvature if you have matrices where the ratio of the nuclear to Frobenius norm is not too big and those are exactly matrices that are near low rank, right?",
                    "label": 0
                },
                {
                    "sent": "There may not be exactly low rank, but they have to have fast decaying singular vectors are singular values.",
                    "label": 0
                },
                {
                    "sent": "Right, So what this is doing?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's sort of saying if you have this tolerance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Term it's saying I'm going to kill off these directions.",
                    "label": 0
                },
                {
                    "sent": "Right when that restricted strong convexity term holds, it means all of these directions are matrices that are not close to low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "They're all sort of generically quite high rank, and these directions which are good directions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These curved erections there you have matrices that look lower rank.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a condition.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you might say, when does that condition hold?",
                    "label": 0
                },
                {
                    "sent": "But it holds for many models.",
                    "label": 0
                },
                {
                    "sent": "It holds for matrix completion in a slightly modified form, but essentially this condition it holds for multi task regression problems.",
                    "label": 0
                },
                {
                    "sent": "It holds when you're trying to do system identification for autoregressive processes.",
                    "label": 0
                },
                {
                    "sent": "Things like this hold for matrix decomposition problems, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a whole class of models for which this kind of condition holds.",
                    "label": 0
                },
                {
                    "sent": "There's some work to showing that it holds in different cases.",
                    "label": 0
                },
                {
                    "sent": "But if this condition holds, then you can say interesting things about the estimator.",
                    "label": 0
                },
                {
                    "sent": "So let me do that now.",
                    "label": 0
                },
                {
                    "sent": "Right, so just to refresh what we're doing is we're given these noisy observation model.",
                    "label": 0
                },
                {
                    "sent": "We're estimating the unknown matr.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By solving an SDP.",
                    "label": 0
                },
                {
                    "sent": "And we're getting this Theta hat and we want to understand how close that is to Theta star.",
                    "label": 0
                },
                {
                    "sent": "And what we're assuming is the first assumption is what I just said that you have a.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strong convexity condition.",
                    "label": 0
                },
                {
                    "sent": "Here I'm doing it for quadratic loss, but you could do this more generally if you were in a classification problem for logistic loss or other kinds of log, linear losses are fine too.",
                    "label": 0
                },
                {
                    "sent": "And what I'm assuming is that you choose your regularization parameter Lambda N to be bigger than two times a certain quantity.",
                    "label": 0
                },
                {
                    "sent": "This is the ad joint of your operator.",
                    "label": 0
                },
                {
                    "sent": "It's something that Maps the noise vector and vector to a matrix, and then you need to compute the maximum singular value of that matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you choose Lambda N in that way, by the way, there is an issue hidden in that choice.",
                    "label": 0
                },
                {
                    "sent": "You think about it.",
                    "label": 0
                },
                {
                    "sent": "If you like, we'll discuss it in a moment.",
                    "label": 0
                },
                {
                    "sent": "Then the guarantee is the following that no matter what matrix you take, it doesn't have to be a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Any matrix you want any solution to the SDP?",
                    "label": 0
                },
                {
                    "sent": "There might be more than one, but anyone that you find or your algorithm finds its Frobenius norm error will be less than a minimum minimum over R. You want to think of.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "RR is a parameter you choose.",
                    "label": 0
                },
                {
                    "sent": "It's like what rank would you like to choose to estimate?",
                    "label": 0
                },
                {
                    "sent": "And then you get 2 natural terms.",
                    "label": 0
                },
                {
                    "sent": "You get one term which is proportional to R and the other which depends on the sum over the tail of the singular values.",
                    "label": 0
                },
                {
                    "sent": "So the sum over J larger than R + 1 of the singular values of the unknown matrix.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's very natural if your matrix were exactly low rank, so you were rank R, then you set R to be the rank of the matrix here.",
                    "label": 0
                },
                {
                    "sent": "And then all of these are zero.",
                    "label": 0
                },
                {
                    "sent": "The approximation error would completely vanish and you'd get an error that's proportional to the regularization parameter.",
                    "label": 1
                },
                {
                    "sent": "It scales inversely with the curvature, right?",
                    "label": 0
                },
                {
                    "sent": "That's what you'd expect if the curvature becomes low, then your problem is harder.",
                    "label": 0
                },
                {
                    "sent": "The curvature is high, your problems easier.",
                    "label": 0
                },
                {
                    "sent": "And it scales proportionately to the rank that you chose, so that sort of makes intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "But in general, if you don't have a matrix that exactly low rank, then you have a bound and what you need to do is you need to play a game where you try and figure out what's the right choice of R to make this term equal to that, so you get the best possible upper bound.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the usual game that we play.",
                    "label": 0
                },
                {
                    "sent": "You're trading off well, can call it bias or variance, or I'm calling it here.",
                    "label": 0
                },
                {
                    "sent": "Estimation error in approximation error.",
                    "label": 0
                },
                {
                    "sent": "That's the usual game we play.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It is essentially a corollary of the more general framework, yes, so I think Ben you on Monday spoke a little bit about a general framework with restricted strong convexity, indecomposable regularizers.",
                    "label": 1
                },
                {
                    "sent": "The nuclear norm is a decomposable regularizer, and so it falls within that framework.",
                    "label": 0
                },
                {
                    "sent": "So yes, for those of you who know that that is true.",
                    "label": 0
                },
                {
                    "sent": "So what's not satisfying about this result?",
                    "label": 0
                },
                {
                    "sent": "Well, the form of it.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, when you prove things, it's nice to be reassured that they make sense, right?",
                    "label": 0
                },
                {
                    "sent": "That's the whole point.",
                    "label": 0
                },
                {
                    "sent": "This makes sense.",
                    "label": 0
                },
                {
                    "sent": "The form of it, but what's not satisfying is that I'm playing games here as a user, can you actually choose Lambda end to be bigger than this quantity?",
                    "label": 0
                },
                {
                    "sent": "That's what you need to do right when you implement it to have this guarantee, you need to be sure that holds.",
                    "label": 0
                },
                {
                    "sent": "You can't actually, because this depends on the noise W you know X, so you know X star, but you don't know W, so we're cheating slightly there, but this result actually holds deterministically, like there's no probability in the result yet.",
                    "label": 0
                },
                {
                    "sent": "So where does the probability come in?",
                    "label": 0
                },
                {
                    "sent": "It comes in because you have to make some assumptions about your noise or some assumptions about maybe you have a random X, like in matrix completion.",
                    "label": 0
                },
                {
                    "sent": "So this becomes a random variable and you have to bound that so there's a little bit of work in doing that, But that's something that you can do.",
                    "label": 0
                },
                {
                    "sent": "And if you do that for different models then you'll get different choices of the regularization and then you'll plug these in here and you'll get various rates.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's sort of the last step.",
                    "label": 0
                },
                {
                    "sent": "The last step is you need to use probability theory to ensure that you make some choice of this, and this will hold with high probability.",
                    "label": 0
                },
                {
                    "sent": "Store.",
                    "label": 0
                },
                {
                    "sent": "So W here is just the noise vector X stars the ad joint of X.",
                    "label": 0
                },
                {
                    "sent": "So what it does is it takes an N vector and it Maps it back to the space of matrices.",
                    "label": 0
                },
                {
                    "sent": "If yeah, it's it's like the transpose except it's sort of an operator on D1 by D2 to N, right?",
                    "label": 0
                },
                {
                    "sent": "So you know X star because in your observation model we know X&Y, but we don't know W. But if you assume that was something like Gaussian or more generally, just had some kind of tail behavior.",
                    "label": 0
                },
                {
                    "sent": "Then you can bound how big that maximum singular value is, so there's.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work there, but somehow conceptually the work is clear.",
                    "label": 0
                },
                {
                    "sent": "The path is clear what you need to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the last sort of 10 minutes or so, let me talk a little bit about specific models, right?",
                    "label": 0
                },
                {
                    "sent": "That's a fairly generic result, but if you want to show that it has teeth and you'd like to say, OK, let's look at some models that are interesting and people are studying or used in practice.",
                    "label": 0
                },
                {
                    "sent": "And let's see what we can say.",
                    "label": 0
                },
                {
                    "sent": "So let's look at matrix completion to start.",
                    "label": 1
                },
                {
                    "sent": "Matrix completion what you're doing is the element of your observation operator.",
                    "label": 0
                },
                {
                    "sent": "You're choosing a random index from your matrix AI by some random index, and basically what you're doing is you're taking the trace inner product of your matrix in a mask matrix, something that's all zeros except for one in one position.",
                    "label": 0
                },
                {
                    "sent": "So this is a fairly challenging model because even in the noiseless setting various people dating back to rechten kandas have recognized that it's an identifiable.",
                    "label": 1
                },
                {
                    "sent": "It's very easy to see why it's an identifiable even if data stars low rank, right?",
                    "label": 0
                },
                {
                    "sent": "So the reason it's under identifiable as you can always construct an annoying matrix that's rank one but concentrates all its mass in the upper left entry.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you start applying this operator to it, you're randomly sampling entries, but this is just one entry.",
                    "label": 0
                },
                {
                    "sent": "When N is much less than D1D2, you'll almost never see that entry with very high probability will never see it.",
                    "label": 1
                },
                {
                    "sent": "Right, so you cannot distinguish between this matrix and the all zeros matrix.",
                    "label": 0
                },
                {
                    "sent": "So what past work is done is they've imposed what are known as eigen incoherence conditions.",
                    "label": 0
                },
                {
                    "sent": "What this just means, roughly speaking, it means that I want to be sure that the singular vectors are the eigenvectors in this case of Theta star do not align with things that look like this with things that look like the sparse basis vectors.",
                    "label": 0
                },
                {
                    "sent": "Right, if you do that, then you're getting rid of the identifiable part.",
                    "label": 0
                },
                {
                    "sent": "But if you think about noisy problems, it's not really quite severe restriction.",
                    "label": 1
                },
                {
                    "sent": "It's not actually necessary.",
                    "label": 0
                },
                {
                    "sent": "If we don't care about exactly getting the matrix right.",
                    "label": 0
                },
                {
                    "sent": "So the way to think about it is, suppose I gave you a matrix that consisted of a good part that had good eigen incoherence.",
                    "label": 0
                },
                {
                    "sent": "But then I added in a bad part.",
                    "label": 0
                },
                {
                    "sent": "I added in a small Delta times this bad poisonous matrix.",
                    "label": 0
                },
                {
                    "sent": "Right I can incoherence would immediately rule out this matrix right?",
                    "label": 0
                },
                {
                    "sent": "Because it aligns if I choose things properly.",
                    "label": 0
                },
                {
                    "sent": "You can make an eigen a singular vector aligned with this guy.",
                    "label": 0
                },
                {
                    "sent": "But somehow, if we're in a noisy setting, which in most learning problems is the case, I don't care about getting the matrix exactly.",
                    "label": 0
                },
                {
                    "sent": "So I should only care about this component if Delta is somehow large.",
                    "label": 0
                },
                {
                    "sent": "If Delta was relatively small in some sense, then I'll just ignore that part and not estimate it.",
                    "label": 0
                },
                {
                    "sent": "I won't find it, but I shouldn't care about it because it's not big anyway.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the milder thing to do if you are only interested in approximate matrix recovery is what you should do is compute the spikiness ratio.",
                    "label": 0
                },
                {
                    "sent": "You should compute the ratio of the Infinity norm of the matrix times D over the sum of the squares of all its entries.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, why is that?",
                    "label": 0
                },
                {
                    "sent": "An interesting quantity?",
                    "label": 0
                },
                {
                    "sent": "Well, it's a quantity that's between one and D. It's going to be one if your matrix is perfectly uniform, all its entries are equal.",
                    "label": 0
                },
                {
                    "sent": "Then the ratio of the Infinity norm Times D to the Frobenius is exactly 1.",
                    "label": 0
                },
                {
                    "sent": "If your matrix is nasty, like this guy here, then the ratio will be D. I'm looking at square matrices for the moment here.",
                    "label": 0
                },
                {
                    "sent": "Right, so our results are going to be stated in terms of this ratio.",
                    "label": 0
                },
                {
                    "sent": "If that ratio is out of control, then you can't say much, but in practice you expect that ratio for a reasonable matrix wouldn't be too large, and then you can.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say something interesting.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here's a a result about matrix completion noisy matrix completion.",
                    "label": 0
                },
                {
                    "sent": "So just to make things concrete, let me give you a class of not low rank matrices but near low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "So we'll look at all matrices that if you take their singular values and you raise them to the cute power.",
                    "label": 0
                },
                {
                    "sent": "Then that sum is bounded less than some RQ.",
                    "label": 0
                },
                {
                    "sent": "So this is a fancy way if Q is 0, this is just a fancy way of saying that you have a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "If Q is bigger than zero, you're matrices can be full rank, but you're saying that the singular values are going to drop off fast, right?",
                    "label": 0
                },
                {
                    "sent": "So this is one way of characterizing what it means for a matrix to be approximately low rank.",
                    "label": 0
                },
                {
                    "sent": "So what I fear it guarantees is that the Frobenius error will be less than or equal to the radius of the ball, so exactly low rank case.",
                    "label": 0
                },
                {
                    "sent": "That would be the rank.",
                    "label": 0
                },
                {
                    "sent": "Times some terms, the Alpha is the spikiness ratio and you get D log D. So matrix dimensions log D / N to the 1 -- Q on 2.",
                    "label": 1
                },
                {
                    "sent": "So that result is has the right flavor.",
                    "label": 0
                },
                {
                    "sent": "The log D is kind of superfluous, but the ratio of the dimension to N. That's sort of correct.",
                    "label": 0
                },
                {
                    "sent": "And if you think about just think about the case Q = 0, That's the easiest case to understand in Q0 what you're getting is the rank times the dimension.",
                    "label": 0
                },
                {
                    "sent": "So you're saying that the rates roughly rank times dimension over sample size.",
                    "label": 0
                },
                {
                    "sent": "That makes sense because a matrix in D dimensions with rank R has on the order Rd degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "So this results optimal.",
                    "label": 0
                },
                {
                    "sent": "You can prove that you cannot improve this result up to that annoying logarithmically factor, but I think there are ways of getting rid of that log so it's sort of the correct rate.",
                    "label": 0
                },
                {
                    "sent": "No method can actually estimate matrices at a better rate than this.",
                    "label": 0
                },
                {
                    "sent": "About this stuff.",
                    "label": 0
                },
                {
                    "sent": "Right so sub exponential tail.",
                    "label": 0
                },
                {
                    "sent": "This means that something like Bernstein's condition holds.",
                    "label": 0
                },
                {
                    "sent": "It means that the moment generating function exists in a neighborhood around zero.",
                    "label": 0
                },
                {
                    "sent": "Any bounded noise is certainly sub or sub exponential.",
                    "label": 0
                },
                {
                    "sent": "Heavy tailed noise, one could prove results like this, but it would present you have to use a different argument.",
                    "label": 0
                },
                {
                    "sent": "The rates possibly could change just because you're asking where the noise conditions are coming in is you need to bound this quantity.",
                    "label": 0
                },
                {
                    "sent": "So we're using subexponential properties in the noise to use a random matrix theory results to control that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let me just wrap up by comparing to some other results.",
                    "label": 0
                },
                {
                    "sent": "There's been some.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This past work, if we look at the special case of rank exactly low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "As I said, our result is giving something like Rd times log D / N.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Candace and plan looked at this same estimator.",
                    "label": 0
                },
                {
                    "sent": "And they tried to sort of extrapolate from exact recovery results.",
                    "label": 1
                },
                {
                    "sent": "And the result that they get in some ways is better than ours because it's a result that if the noise variance went to zero, they would get exact recovery.",
                    "label": 0
                },
                {
                    "sent": "But it has some funny scalings.",
                    "label": 0
                },
                {
                    "sent": "If you let the dimension scale, it actually diverges, and it also diverges with the sample size, so it's not really capturing for the noisy case what's going on, it's the right result for the noiseless case.",
                    "label": 0
                },
                {
                    "sent": "Keshavan Montanari Anno had a very nice paper, varied.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Print method quite simple based on trimming the matrix and applying SVD.",
                    "label": 1
                },
                {
                    "sent": "What they proved is a result that removes the log D from RR scaling.",
                    "label": 0
                },
                {
                    "sent": "So somehow it's right there.",
                    "label": 0
                },
                {
                    "sent": "But it also involves the matrix condition, number matrix condition number is the ratio of the maximum to minimum singular value, so that could cause some bad scaling, particularly if you think of a matrix whose got some decaying singular values.",
                    "label": 1
                },
                {
                    "sent": "So our results not strictly better, but you can see that there's some sense and it improves certain aspects of.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that result.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient Superman is also restricted.",
                    "label": 0
                },
                {
                    "sent": "Sorry which.",
                    "label": 0
                },
                {
                    "sent": "This right?",
                    "label": 0
                },
                {
                    "sent": "So they're looking exactly rank are matrices.",
                    "label": 0
                },
                {
                    "sent": "No, it's not the restricted minimum, it's the ratio of the maximum to minimum singular values of the true unknown matrix, so that could diverge.",
                    "label": 0
                },
                {
                    "sent": "I could play games and make that diverge.",
                    "label": 0
                },
                {
                    "sent": "You think about their result is optimal if the matrix sort of has homogeneous singular values in is exactly rank R, but none of this past work also applied to general matrices, which are approximately low rank.",
                    "label": 0
                },
                {
                    "sent": "That I think is actually important, 'cause I personally don't believe that any matrix and practices exactly.",
                    "label": 0
                },
                {
                    "sent": "Low rank.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just wrap up there's other things that I could say but just just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "Um, so I guess high level.",
                    "label": 0
                },
                {
                    "sent": "I hope I've convinced you that high dimensional problem matrix problems occur in different.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Settings and they're interesting.",
                    "label": 0
                },
                {
                    "sent": "Many people are looking at estimators based on the nuclear norm and other kinds of convex relaxations.",
                    "label": 1
                },
                {
                    "sent": "It's quite an active area of work, lots of new papers appearing so exciting time to be in the area.",
                    "label": 1
                },
                {
                    "sent": "What I've hoped to describe today is that there's sort of one single result that has basically two ideas and notion of strong convexity and a certain property that the regularizer has to satisfy.",
                    "label": 0
                },
                {
                    "sent": "Nuclear norm does that can be used to compute bounds in a fairly wide range of settings, and I didn't talk so much about this, but often the bounds are actually optimal, meaning that even if you used a better estimator, even if you did an exponential time search, let's say in the matrix completion problem.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't be able to get an algorithm that has much better accuracy than these kinds of simple relaxations, so in those cases there's some sense that the algorithms are essentially the best that we could hope for.",
                    "label": 0
                },
                {
                    "sent": "They're both computationally efficient, and they are statistically optimal, right?",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a nice meeting of the two worlds in those cases, so I've listed a couple references if you're interested in more details.",
                    "label": 0
                },
                {
                    "sent": "This is the more general viewpoint that CJ was mentioning.",
                    "label": 1
                },
                {
                    "sent": "This kind of unified framework.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}