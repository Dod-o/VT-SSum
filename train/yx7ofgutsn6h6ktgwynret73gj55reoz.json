{
    "id": "yx7ofgutsn6h6ktgwynret73gj55reoz",
    "title": "Combining near-optimal feature selection with gSpan",
    "info": {
        "author": [
            "Marisa Thoma, University of Munich"
        ],
        "published": "Aug. 25, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Mathematics->Graph Theory",
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/mlg08_thoma_cnofs/",
    "segmentation": [
        [
            "Oh no, thank you for the introduction.",
            "I will give you a short introduction into Nouvel.",
            "Feature selection approach which is already combining on the graph.",
            "The subgraph mining procedure of the spoon with the actual selection, so this is joint work with a lot of people, so they have Fortunately already been mentioned.",
            "I'd like to give my special thanks to Carson block Word who was the?",
            "Well, man, pushing uh this publication through.",
            "So."
        ],
        [
            "I'm first going to tell you why we need feature selection and frequent subgraphs.",
            "Then I'll give a short introduction in the benefits of submodular selection criteria.",
            "And then I'm going to introduce the new feature selection criterion, which we have developed, which is called Cork.",
            "And demonstrate how this criterion can be included into the giesemann process.",
            "We have a short experimental validation and, well, I'll give you some summary and outlook."
        ],
        [
            "Um?",
            "The purpose of our whole setting is graph classification.",
            "We have simplified it to a two class problem, so we have.",
            "A collection of graphs which has to be mapped to two to two class labels.",
            "Mean positive, negative or whatever and currently there are already a lot of solutions available.",
            "Most of them are pattern based.",
            "If you only learn it from the graph topology, you usually resort to subgraphs or special kinds of subgraphs like paths or circles.",
            "Or with more sophisticated methods like graph kernel approaches, you can also use a random walks or other similarity criteria.",
            "A third learning approach is the nested technique, meaning you only generate those features or those subgraphs.",
            "Let's stay with that from your data set that you actually need to classify.",
            "Also."
        ],
        [
            "Would we have been using?",
            "Is the frequent subgraph technique?",
            "Why?",
            "Because, well, it's these subgraphs of frequent.",
            "They occur in our data set, thus they must mean something to the data.",
            "And they can be efficiently enumerated by algorithms like G, span or.",
            "Other enumeration techniques.",
            "The problem with frequent subgraphs is that they can also contain insignificant features.",
            "Or redundant features, so the insignificant features they're just treatment.",
            "They may occur in do you want in one class or the other at the approximately same ratio or redundant features?",
            "Especially for subgraphs, it's usually the case that large reconsult growth has many smaller subgroups, as many smaller subgraphs, which is also a frequent.",
            "Thus it is hard to decide which of those subgraphs are actually meaningful for learning problem.",
            "And of course, there's potentially exponentially many frequent subgraphs, which is just just the nature of the problem, so we always have to tune your frequency threshold to get a reasonable number of features.",
            "So we.",
            "Definitely need feature selection to get a reasonable res."
        ],
        [
            "Walt let me give a short, formal definition.",
            "Simple one.",
            "We want to identify does identify a set out of our total feature set is subset of the total features that which maximizes a decision function or a decision.",
            "An information criterion F over all subsets out of our feature space.",
            "Basically that's it.",
            "So as you can see, it's a it's a set problem.",
            "There are also exponentially many combinations among those feature combinations which which would have to be tested in order to get an exact solution to this problem."
        ],
        [
            "So this is already the first feature selection approach, meaning you you simply enumerate all your possible feature combinations.",
            "However, this usually cannot be.",
            "Cannot be put through the processes.",
            "You'll just run out of time.",
            "The opposite approach is you evaluate each feature separately, meaning you.",
            "You do a sequential scan over all of your features.",
            "Assign an individual information criterion value to each feature and then just take the K. Best of those.",
            "So these are anchor selections and in between way between the Rancor selection on the complete wrapper approach would be greedy approaches, meaning you can start with your complete feature set and one by one eliminates all those features which are going to keep which decrease your information criterion the most.",
            "Or you take forward selection.",
            "You start with the empty feature set and iteratively add the most informative features to them.",
            "Alright, that was probably it some another way to do this is nested approaches, so this is the same point I wanted to point out in the beginning you can.",
            "You can perform feature selection while actually enumerating all of your features.",
            "This is what we're going to do, so we will try to only enumerate features.",
            "Which we act."
        ],
        [
            "Need.",
            "Alright, in our approach we use greedy forward selection since there has been approved by crossing guestrin that if you apply a submodular decision function on greedy forward selection, you'll have a guaranteed.",
            "Approximate feature set which is going to originate from this procedure.",
            "So you have a guarantee upper bound of the quality of the features you you are enumerating, so this is already quite useful on.",
            "So what are submodular disk?"
        ],
        [
            "And functions.",
            "A set function mapping set to real value is submodular if for any two sets out of the available feature sets.",
            "Let's stay with features for now.",
            "Um?",
            "So those sets for any two sets such that one set is a subset of the other.",
            "So S is a subset of T. If you add a new feature, so small S to both of them.",
            "The improvement for S. So for the subset.",
            "Has to be at least as large as the improvement for adding small as to T the Super set.",
            "So this can be visualized by some simple."
        ],
        [
            "Simple submodular criterion.",
            "Let's say we have some sets and the criterion just declares some area.",
            "So we have two sets, the blue one and the green one green.",
            "One is a subset of the of the blue."
        ],
        [
            "One if you know add and you feature so when you point to both of these sets.",
            "The improvement in the area of the blue one is only this Half Moon.",
            "So this little half movement, which hasn't been part of the blue one so far, whereas for the green one we gained the whole new information.",
            "Of the black circle.",
            "Basically that's it."
        ],
        [
            "So let's now introduce the new criterion.",
            "We first define a correspondence which some of you might already know as an inconsistency.",
            "It's basically.",
            "A pair of instances, all of our data set from opposite classes, so the class label from instance I is different from the class label instance J.",
            "They're called correspondence for a feature set S if I&J have the same feature pattern under us, so each feature value in S in both of these instances is the same.",
            "Then it's a correspondence.",
            "And our correspondence based quality criterion Cork is simply defined as minus one times the number of correspondence is in S. OK, I'll give an example in a minute.",
            "Let me just tell you that from now on we only use this criterion for binary values, meaning on.",
            "Is a subgraph or is not a subgraph one and 0?",
            "This can of course also be applied to other other data, but it's a lot easier for binary data from now.",
            "Um?"
        ],
        [
            "So let's remember we have minus one times the number of correspondences.",
            "This can be efficiently calculated by declaring two variables.",
            "I hope you can read them a as zero and a is 1.",
            "A zero is simply the number of instances for which feature S is zero in Class A.",
            "Write the number of instances of a which have no sub graph S. So the number of mismatches in A&AS one is simply the number of matches in a.",
            "The same kind of course, also be done for Class B.",
            "So.",
            "Our criterion up there simply is equivalent to the number of mismatches and a test times the number of mismatches and B plus the number of matches in a tense in the motor mismatches and be so it simply you multiply all of your equivalent instances from the one class with the other.",
            "And let me give a short example.",
            "Here we have a two class datasets with classes A&B.",
            "Each of them has three instances, instance 181283 and B1283 and we have two features available.",
            "Feature S1 and S2.",
            "So if we want to have the correspondence with the number of correspondences for features."
        ],
        [
            "This one.",
            "We simply count the number of mismatches.",
            "So AS 0, which is 1 times the number of mismatches in B, which is 2 alright plus the number of matches and a 1, two times the number of matches in B. Alright, and then we arrive at a total of four correspondances.",
            "Which makes the core value of minus 4."
        ],
        [
            "But the same can be done for feature as two just quickly put you through this we have.",
            "Two mismatches in a two mismatches and be one.",
            "Mention A and 1B, so we arrive at a quote value of minus 5."
        ],
        [
            "Alright, this can also be done on sets naturally, so.",
            "This is again the formula for simply one feature.",
            "Below we see the formula for 2 features.",
            "Um?",
            "You can imagine it as a tree.",
            "The first feature splits your data into two equivalence classes, meaning the matches and mismatches and the second can split those classes again, meaning in.",
            "Well, a double match or double mismatch or imagine mismatch or the opposite.",
            "So you have to sum over all of those equivalence classes to get your Clock score for 2 features.",
            "Alright, naturally this can also be done for plenty of features.",
            "So in M which gives you 2 to the power of M possible equivalence classes.",
            "Of course we try not to, not to always enumerate all of those, but try to restrict ourselves to.",
            "Too few equivalence classes.",
            "So I just skip this below.",
            "Here it's simply the quark score for the combination of those two features.",
            "Most of you already have read it through."
        ],
        [
            "Alright, now let me give a short proof approved for why Kroger submodular.",
            "Let's remember what we need for for submodel activity.",
            "Any improvement of the Super set T?",
            "Which is Lucia.",
            "Must also result in an improvement of the subset S. Alright, if we add another feature.",
            "Improving team must also improve as so how do we improve our sets?",
            "Well, we remove a correspondence, right?",
            "If you have fewer correspondences.",
            "Our clocks grew.",
            "Our called score grows."
        ],
        [
            "So let's look at this small example.",
            "We have a data set which is the red line above.",
            "Which is split by features, a set of feature S. Well, the set of features actually only defines two equivalence classes, the red and the blue one.",
            "In both label classes A&B.",
            "Alright.",
            "The next the Super set of SC which is here in the third line.",
            "It also has the same partitioning limit as S since it is super set must it must define the same equivalence equivalence class?",
            "And it also gives us a new partition.",
            "In what region would help which hasn't been partitioned before?",
            "It may, of course contain a lot of more than that, but let's keep it simple.",
            "So what happens now if we add an additional feature small as below here?",
            "We can see that a small is also only declares 2 new equivalence classes.",
            "The blue and the green one.",
            "And.",
            "If those are added to.",
            "The previous feature sets S&T."
        ],
        [
            "We see that as.",
            "Is split in this red equivalence class, and it's also split in the blue equivalence class.",
            "Well, as has now four equivalence classes.",
            "Meaning the features in in the red equivalence class have less feet.",
            "The sorry, the instances in the red equivalence class of a have less instances in B to match 2, so we have fewer correspondences.",
            "Same happens to T if we add S, so we also have a new split here, but if we look at this.",
            "By accident we only have.",
            "We simply keep the old partitioning.",
            "So it's not necessarily the case that the number in T increases as much as the phone number in this subset.",
            "So in fact, that's the submodularity criterion below there.",
            "Alright, that's very nice.",
            "We can use this in greedy forward selection and have a guaranteed approximation.",
            "However, we can do more with that.",
            "We can include Cork into G spin."
        ],
        [
            "I'm going to show you how that is so.",
            "Let's first give a short primer on G span.",
            "You probably all know how it works, so you have a search tree which contains of which consists of of DFS codes.",
            "Which are sequential encodings of a graph?",
            "Specifically, subgraphs which are being mined in our data set.",
            "So the first note is.",
            "Simply one vertex.",
            "This is corresponds to 1 edge and they are sequentially extended as long as they are still frequent in our data set.",
            "So once we arrive at a subgraph which is not frequent, it's being pruned.",
            "Another way of pruning happens if you arrive at a sub graph which has already been seen before, so if.",
            "This can be easily checked by the minimality test of the DFS code.",
            "It's kind of expensive, but it works.",
            "So in the worst case you have an exponential runtime, which of course also in the nature of the problem.",
            "However, it has been shown to work very well.",
            "On the future, let's remember that any parent or any parent node is a subgroup of all of its child nodes.",
            "We're gonna need this.",
            "OK, if you now want to add another bound into this."
        ],
        [
            "ISIS.",
            "We just look at this parent child relationship.",
            "And define the parent S. As a feature.",
            "And the child St also a feature we can sub graph.",
            "So in the translation from S to T, we can only lose matching subgraphs 40.",
            "So if we have a number of matches in S, this number of metrics can also occur in T, but it doesn't need to.",
            "We can.",
            "Yeah, in fact that's it.",
            "We can never gain any new matches since if if anything matches in T, it must also help matches matches matched in S. Alright, so the frequency of the matches in a is at least as large as the frequency of measures in a of T. Same accounts for B Class B, sorry.",
            "Alright, let's look at our criterion again.",
            "We have here a formula which includes these parameters.",
            "The frequency in A&B.",
            "And the.",
            "Yeah, we can observe that the maximal value of this criterion is reached when all matches in a or all matches and B are lost, while of course keeping some matches in the other class.",
            "So this can be visualized."
        ],
        [
            "Buy some schematic.",
            "Good search tree.",
            "Let's say this is a search tree.",
            "Here we have some sub graph which we shall call S. So this is the parent.",
            "Below here we have a child which has frequencies which do not exactly differ from the frequencies of the parent, so we probably cannot do anything here and we have to track the complete.",
            "The complete branch.",
            "However, if we lose all of the occurrences in a in Class A and still keep some in Class B.",
            "We know that any supergraph of this note here cannot give us any more information than we already have in this node.",
            "So we can simply prune.",
            "Everything below here the complete branch.",
            "Alright, this is a simplified pruning criterion.",
            "In fact, using this formula we can derive something a bit more."
        ],
        [
            "Restrictive.",
            "Which sums up to this year I'm I'm going to skip the derivation, but it can be easily done on a paper.",
            "And the.",
            "Well, with this we have an upper bound for the quality criterion of T. It's at least.",
            "It's at most as large as the quality criterion of the parent node.",
            "Plus some maximum criterion from here.",
            "So using this criterion, we can say that no super graph of T of the child node can exceed this upper bond naturally, since they are also super graphs of S. And if we have seen a better subgraph, we can prove it.",
            "If we so far have seen a better sub graph than this bound, we can prune the whole branch.",
            "That's the point.",
            "Alright this guy."
        ],
        [
            "Some rest as an algorithm.",
            "We're beginning with the empty feature set, so this is greedy forward selection.",
            "Then we run gisburn to give us the best sub graph according to the quality criterion off the sub graph joined with the feature set selected so far.",
            "So in the beginning that's empty, but it's going to fill up.",
            "While the quality criterion of this set is an improvement to the previous feature set.",
            "We just join our feature tool set and run gisburn again to get us the next best feature.",
            "Well and in the end we return our date our feature set."
        ],
        [
            "Right?",
            "We can show you smaller experimental validation.",
            "We've taken some part of the NCI data set which most of you probably know consists of chemical structures of.",
            "Molecules which have been tested about their efficiency against cancer.",
            "So you have a labeling of one and zero and.",
            "Well, we have compared our approach to two feature ranking methods mean with sequential cover which is using a confidence confidence score to evaluate the feature attempt and the Pearson correlation measuring the correlation between the data and the label.",
            "And we've also compared to wrapper approach called La La, so which has been published last year.",
            "And which?",
            "Users.",
            "Actually the learner to validate the feature you you want to you want to evaluate.",
            "So this is working extremely well."
        ],
        [
            "So.",
            "In our evaluation of 10 repetitions of tenfold cross validation on AGS penetration of frequency 10.",
            "So we have said that we want to test all frequency oil frequent subgraphs which occur at least 10% of our data.",
            "Actually, this wrapper approach always bet all of the of the other approaches, including ours, unfortunately.",
            "Um?",
            "However, there's still room for improvement.",
            "I'm going to get into this next slide.",
            "We have validated the data set on a linear SVM and well at least we could establish that.",
            "Cork always goes better than any of the other anchors, also including several other criterion which criteria which we have not pointed up here yet.",
            "So.",
            "Yes, we can see that there."
        ],
        [
            "Room for improvement.",
            "A big improvement must be made at runtime, since you see we we are always calling up.",
            "She's been forgetting the next best feature.",
            "Of course this can be set up by simply saving simply saving the minimal DFS code, so we have to remember our DFS search tree and which of those nodes are minimal or not.",
            "Since this is the.",
            "At the same time, essential step in G spoon.",
            "And further improvement must be made at the bound for later iterations.",
            "If we have a problem which is well separable that we don't have a problem, since this can be.",
            "Can be separated very easily and the algorithm converges pretty quickly.",
            "However, if in later iterations we only can add one more feature and we can, we can only resolve one more correspondence than one more correspondence.",
            "It's just iterating.",
            "Into eternity and never.",
            "Well, it stops sometimes, but it's it's pretty slow, so we have to to think about something too.",
            "About our algorithm in later iterations.",
            "A further possibility would be combination with the with other bounding criteria such as.",
            "Well the the very simple but effective only criteria of mouse.",
            "So far we haven't had any success here, but it's still ongoing research.",
            "So once we have solved this, we can use our algorithm on.",
            "On a data set, even without giving an explicit frequency bound.",
            "This is the fun part about it because we already have some some very strong bound and GS in Cork.",
            "We don't need to give cheeseburger frequency threshold.",
            "So this is a.",
            "Really interesting part about this.",
            "And well, another outlook would be to to explore this tree structure of our decision criterion.",
            "I've already told you you can imagine this to be a tree of splits into equivalence classes, and these this tree structure can simply be looked at as a decision tree.",
            "So in early tests we have.",
            "Determined that those results give almost as good as predictions as the SVM.",
            "So yeah, in principle, that was it."
        ],
        [
            "Thank you for your patience on any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh no, thank you for the introduction.",
                    "label": 0
                },
                {
                    "sent": "I will give you a short introduction into Nouvel.",
                    "label": 0
                },
                {
                    "sent": "Feature selection approach which is already combining on the graph.",
                    "label": 1
                },
                {
                    "sent": "The subgraph mining procedure of the spoon with the actual selection, so this is joint work with a lot of people, so they have Fortunately already been mentioned.",
                    "label": 1
                },
                {
                    "sent": "I'd like to give my special thanks to Carson block Word who was the?",
                    "label": 0
                },
                {
                    "sent": "Well, man, pushing uh this publication through.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm first going to tell you why we need feature selection and frequent subgraphs.",
                    "label": 0
                },
                {
                    "sent": "Then I'll give a short introduction in the benefits of submodular selection criteria.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to introduce the new feature selection criterion, which we have developed, which is called Cork.",
                    "label": 0
                },
                {
                    "sent": "And demonstrate how this criterion can be included into the giesemann process.",
                    "label": 0
                },
                {
                    "sent": "We have a short experimental validation and, well, I'll give you some summary and outlook.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The purpose of our whole setting is graph classification.",
                    "label": 0
                },
                {
                    "sent": "We have simplified it to a two class problem, so we have.",
                    "label": 0
                },
                {
                    "sent": "A collection of graphs which has to be mapped to two to two class labels.",
                    "label": 1
                },
                {
                    "sent": "Mean positive, negative or whatever and currently there are already a lot of solutions available.",
                    "label": 1
                },
                {
                    "sent": "Most of them are pattern based.",
                    "label": 0
                },
                {
                    "sent": "If you only learn it from the graph topology, you usually resort to subgraphs or special kinds of subgraphs like paths or circles.",
                    "label": 1
                },
                {
                    "sent": "Or with more sophisticated methods like graph kernel approaches, you can also use a random walks or other similarity criteria.",
                    "label": 0
                },
                {
                    "sent": "A third learning approach is the nested technique, meaning you only generate those features or those subgraphs.",
                    "label": 0
                },
                {
                    "sent": "Let's stay with that from your data set that you actually need to classify.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would we have been using?",
                    "label": 0
                },
                {
                    "sent": "Is the frequent subgraph technique?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because, well, it's these subgraphs of frequent.",
                    "label": 0
                },
                {
                    "sent": "They occur in our data set, thus they must mean something to the data.",
                    "label": 0
                },
                {
                    "sent": "And they can be efficiently enumerated by algorithms like G, span or.",
                    "label": 1
                },
                {
                    "sent": "Other enumeration techniques.",
                    "label": 0
                },
                {
                    "sent": "The problem with frequent subgraphs is that they can also contain insignificant features.",
                    "label": 1
                },
                {
                    "sent": "Or redundant features, so the insignificant features they're just treatment.",
                    "label": 0
                },
                {
                    "sent": "They may occur in do you want in one class or the other at the approximately same ratio or redundant features?",
                    "label": 0
                },
                {
                    "sent": "Especially for subgraphs, it's usually the case that large reconsult growth has many smaller subgroups, as many smaller subgraphs, which is also a frequent.",
                    "label": 0
                },
                {
                    "sent": "Thus it is hard to decide which of those subgraphs are actually meaningful for learning problem.",
                    "label": 0
                },
                {
                    "sent": "And of course, there's potentially exponentially many frequent subgraphs, which is just just the nature of the problem, so we always have to tune your frequency threshold to get a reasonable number of features.",
                    "label": 1
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Definitely need feature selection to get a reasonable res.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Walt let me give a short, formal definition.",
                    "label": 0
                },
                {
                    "sent": "Simple one.",
                    "label": 0
                },
                {
                    "sent": "We want to identify does identify a set out of our total feature set is subset of the total features that which maximizes a decision function or a decision.",
                    "label": 0
                },
                {
                    "sent": "An information criterion F over all subsets out of our feature space.",
                    "label": 1
                },
                {
                    "sent": "Basically that's it.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, it's a it's a set problem.",
                    "label": 0
                },
                {
                    "sent": "There are also exponentially many combinations among those feature combinations which which would have to be tested in order to get an exact solution to this problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is already the first feature selection approach, meaning you you simply enumerate all your possible feature combinations.",
                    "label": 1
                },
                {
                    "sent": "However, this usually cannot be.",
                    "label": 0
                },
                {
                    "sent": "Cannot be put through the processes.",
                    "label": 0
                },
                {
                    "sent": "You'll just run out of time.",
                    "label": 0
                },
                {
                    "sent": "The opposite approach is you evaluate each feature separately, meaning you.",
                    "label": 0
                },
                {
                    "sent": "You do a sequential scan over all of your features.",
                    "label": 0
                },
                {
                    "sent": "Assign an individual information criterion value to each feature and then just take the K. Best of those.",
                    "label": 0
                },
                {
                    "sent": "So these are anchor selections and in between way between the Rancor selection on the complete wrapper approach would be greedy approaches, meaning you can start with your complete feature set and one by one eliminates all those features which are going to keep which decrease your information criterion the most.",
                    "label": 0
                },
                {
                    "sent": "Or you take forward selection.",
                    "label": 1
                },
                {
                    "sent": "You start with the empty feature set and iteratively add the most informative features to them.",
                    "label": 0
                },
                {
                    "sent": "Alright, that was probably it some another way to do this is nested approaches, so this is the same point I wanted to point out in the beginning you can.",
                    "label": 1
                },
                {
                    "sent": "You can perform feature selection while actually enumerating all of your features.",
                    "label": 0
                },
                {
                    "sent": "This is what we're going to do, so we will try to only enumerate features.",
                    "label": 0
                },
                {
                    "sent": "Which we act.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need.",
                    "label": 0
                },
                {
                    "sent": "Alright, in our approach we use greedy forward selection since there has been approved by crossing guestrin that if you apply a submodular decision function on greedy forward selection, you'll have a guaranteed.",
                    "label": 1
                },
                {
                    "sent": "Approximate feature set which is going to originate from this procedure.",
                    "label": 0
                },
                {
                    "sent": "So you have a guarantee upper bound of the quality of the features you you are enumerating, so this is already quite useful on.",
                    "label": 0
                },
                {
                    "sent": "So what are submodular disk?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And functions.",
                    "label": 0
                },
                {
                    "sent": "A set function mapping set to real value is submodular if for any two sets out of the available feature sets.",
                    "label": 0
                },
                {
                    "sent": "Let's stay with features for now.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So those sets for any two sets such that one set is a subset of the other.",
                    "label": 0
                },
                {
                    "sent": "So S is a subset of T. If you add a new feature, so small S to both of them.",
                    "label": 0
                },
                {
                    "sent": "The improvement for S. So for the subset.",
                    "label": 0
                },
                {
                    "sent": "Has to be at least as large as the improvement for adding small as to T the Super set.",
                    "label": 0
                },
                {
                    "sent": "So this can be visualized by some simple.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple submodular criterion.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have some sets and the criterion just declares some area.",
                    "label": 0
                },
                {
                    "sent": "So we have two sets, the blue one and the green one green.",
                    "label": 0
                },
                {
                    "sent": "One is a subset of the of the blue.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One if you know add and you feature so when you point to both of these sets.",
                    "label": 0
                },
                {
                    "sent": "The improvement in the area of the blue one is only this Half Moon.",
                    "label": 0
                },
                {
                    "sent": "So this little half movement, which hasn't been part of the blue one so far, whereas for the green one we gained the whole new information.",
                    "label": 0
                },
                {
                    "sent": "Of the black circle.",
                    "label": 0
                },
                {
                    "sent": "Basically that's it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's now introduce the new criterion.",
                    "label": 0
                },
                {
                    "sent": "We first define a correspondence which some of you might already know as an inconsistency.",
                    "label": 0
                },
                {
                    "sent": "It's basically.",
                    "label": 0
                },
                {
                    "sent": "A pair of instances, all of our data set from opposite classes, so the class label from instance I is different from the class label instance J.",
                    "label": 0
                },
                {
                    "sent": "They're called correspondence for a feature set S if I&J have the same feature pattern under us, so each feature value in S in both of these instances is the same.",
                    "label": 1
                },
                {
                    "sent": "Then it's a correspondence.",
                    "label": 0
                },
                {
                    "sent": "And our correspondence based quality criterion Cork is simply defined as minus one times the number of correspondence is in S. OK, I'll give an example in a minute.",
                    "label": 0
                },
                {
                    "sent": "Let me just tell you that from now on we only use this criterion for binary values, meaning on.",
                    "label": 0
                },
                {
                    "sent": "Is a subgraph or is not a subgraph one and 0?",
                    "label": 0
                },
                {
                    "sent": "This can of course also be applied to other other data, but it's a lot easier for binary data from now.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's remember we have minus one times the number of correspondences.",
                    "label": 0
                },
                {
                    "sent": "This can be efficiently calculated by declaring two variables.",
                    "label": 0
                },
                {
                    "sent": "I hope you can read them a as zero and a is 1.",
                    "label": 0
                },
                {
                    "sent": "A zero is simply the number of instances for which feature S is zero in Class A.",
                    "label": 1
                },
                {
                    "sent": "Write the number of instances of a which have no sub graph S. So the number of mismatches in A&AS one is simply the number of matches in a.",
                    "label": 0
                },
                {
                    "sent": "The same kind of course, also be done for Class B.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our criterion up there simply is equivalent to the number of mismatches and a test times the number of mismatches and B plus the number of matches in a tense in the motor mismatches and be so it simply you multiply all of your equivalent instances from the one class with the other.",
                    "label": 0
                },
                {
                    "sent": "And let me give a short example.",
                    "label": 0
                },
                {
                    "sent": "Here we have a two class datasets with classes A&B.",
                    "label": 0
                },
                {
                    "sent": "Each of them has three instances, instance 181283 and B1283 and we have two features available.",
                    "label": 0
                },
                {
                    "sent": "Feature S1 and S2.",
                    "label": 0
                },
                {
                    "sent": "So if we want to have the correspondence with the number of correspondences for features.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "We simply count the number of mismatches.",
                    "label": 0
                },
                {
                    "sent": "So AS 0, which is 1 times the number of mismatches in B, which is 2 alright plus the number of matches and a 1, two times the number of matches in B. Alright, and then we arrive at a total of four correspondances.",
                    "label": 0
                },
                {
                    "sent": "Which makes the core value of minus 4.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the same can be done for feature as two just quickly put you through this we have.",
                    "label": 0
                },
                {
                    "sent": "Two mismatches in a two mismatches and be one.",
                    "label": 0
                },
                {
                    "sent": "Mention A and 1B, so we arrive at a quote value of minus 5.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, this can also be done on sets naturally, so.",
                    "label": 0
                },
                {
                    "sent": "This is again the formula for simply one feature.",
                    "label": 0
                },
                {
                    "sent": "Below we see the formula for 2 features.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can imagine it as a tree.",
                    "label": 0
                },
                {
                    "sent": "The first feature splits your data into two equivalence classes, meaning the matches and mismatches and the second can split those classes again, meaning in.",
                    "label": 0
                },
                {
                    "sent": "Well, a double match or double mismatch or imagine mismatch or the opposite.",
                    "label": 0
                },
                {
                    "sent": "So you have to sum over all of those equivalence classes to get your Clock score for 2 features.",
                    "label": 1
                },
                {
                    "sent": "Alright, naturally this can also be done for plenty of features.",
                    "label": 0
                },
                {
                    "sent": "So in M which gives you 2 to the power of M possible equivalence classes.",
                    "label": 1
                },
                {
                    "sent": "Of course we try not to, not to always enumerate all of those, but try to restrict ourselves to.",
                    "label": 0
                },
                {
                    "sent": "Too few equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "So I just skip this below.",
                    "label": 1
                },
                {
                    "sent": "Here it's simply the quark score for the combination of those two features.",
                    "label": 0
                },
                {
                    "sent": "Most of you already have read it through.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, now let me give a short proof approved for why Kroger submodular.",
                    "label": 0
                },
                {
                    "sent": "Let's remember what we need for for submodel activity.",
                    "label": 0
                },
                {
                    "sent": "Any improvement of the Super set T?",
                    "label": 1
                },
                {
                    "sent": "Which is Lucia.",
                    "label": 0
                },
                {
                    "sent": "Must also result in an improvement of the subset S. Alright, if we add another feature.",
                    "label": 0
                },
                {
                    "sent": "Improving team must also improve as so how do we improve our sets?",
                    "label": 1
                },
                {
                    "sent": "Well, we remove a correspondence, right?",
                    "label": 1
                },
                {
                    "sent": "If you have fewer correspondences.",
                    "label": 0
                },
                {
                    "sent": "Our clocks grew.",
                    "label": 0
                },
                {
                    "sent": "Our called score grows.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at this small example.",
                    "label": 0
                },
                {
                    "sent": "We have a data set which is the red line above.",
                    "label": 0
                },
                {
                    "sent": "Which is split by features, a set of feature S. Well, the set of features actually only defines two equivalence classes, the red and the blue one.",
                    "label": 0
                },
                {
                    "sent": "In both label classes A&B.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "The next the Super set of SC which is here in the third line.",
                    "label": 0
                },
                {
                    "sent": "It also has the same partitioning limit as S since it is super set must it must define the same equivalence equivalence class?",
                    "label": 0
                },
                {
                    "sent": "And it also gives us a new partition.",
                    "label": 0
                },
                {
                    "sent": "In what region would help which hasn't been partitioned before?",
                    "label": 0
                },
                {
                    "sent": "It may, of course contain a lot of more than that, but let's keep it simple.",
                    "label": 0
                },
                {
                    "sent": "So what happens now if we add an additional feature small as below here?",
                    "label": 0
                },
                {
                    "sent": "We can see that a small is also only declares 2 new equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "The blue and the green one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If those are added to.",
                    "label": 0
                },
                {
                    "sent": "The previous feature sets S&T.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see that as.",
                    "label": 0
                },
                {
                    "sent": "Is split in this red equivalence class, and it's also split in the blue equivalence class.",
                    "label": 0
                },
                {
                    "sent": "Well, as has now four equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "Meaning the features in in the red equivalence class have less feet.",
                    "label": 0
                },
                {
                    "sent": "The sorry, the instances in the red equivalence class of a have less instances in B to match 2, so we have fewer correspondences.",
                    "label": 0
                },
                {
                    "sent": "Same happens to T if we add S, so we also have a new split here, but if we look at this.",
                    "label": 0
                },
                {
                    "sent": "By accident we only have.",
                    "label": 0
                },
                {
                    "sent": "We simply keep the old partitioning.",
                    "label": 0
                },
                {
                    "sent": "So it's not necessarily the case that the number in T increases as much as the phone number in this subset.",
                    "label": 0
                },
                {
                    "sent": "So in fact, that's the submodularity criterion below there.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's very nice.",
                    "label": 0
                },
                {
                    "sent": "We can use this in greedy forward selection and have a guaranteed approximation.",
                    "label": 0
                },
                {
                    "sent": "However, we can do more with that.",
                    "label": 0
                },
                {
                    "sent": "We can include Cork into G spin.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to show you how that is so.",
                    "label": 0
                },
                {
                    "sent": "Let's first give a short primer on G span.",
                    "label": 0
                },
                {
                    "sent": "You probably all know how it works, so you have a search tree which contains of which consists of of DFS codes.",
                    "label": 0
                },
                {
                    "sent": "Which are sequential encodings of a graph?",
                    "label": 0
                },
                {
                    "sent": "Specifically, subgraphs which are being mined in our data set.",
                    "label": 0
                },
                {
                    "sent": "So the first note is.",
                    "label": 0
                },
                {
                    "sent": "Simply one vertex.",
                    "label": 0
                },
                {
                    "sent": "This is corresponds to 1 edge and they are sequentially extended as long as they are still frequent in our data set.",
                    "label": 0
                },
                {
                    "sent": "So once we arrive at a subgraph which is not frequent, it's being pruned.",
                    "label": 0
                },
                {
                    "sent": "Another way of pruning happens if you arrive at a sub graph which has already been seen before, so if.",
                    "label": 0
                },
                {
                    "sent": "This can be easily checked by the minimality test of the DFS code.",
                    "label": 0
                },
                {
                    "sent": "It's kind of expensive, but it works.",
                    "label": 0
                },
                {
                    "sent": "So in the worst case you have an exponential runtime, which of course also in the nature of the problem.",
                    "label": 0
                },
                {
                    "sent": "However, it has been shown to work very well.",
                    "label": 0
                },
                {
                    "sent": "On the future, let's remember that any parent or any parent node is a subgroup of all of its child nodes.",
                    "label": 0
                },
                {
                    "sent": "We're gonna need this.",
                    "label": 0
                },
                {
                    "sent": "OK, if you now want to add another bound into this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "ISIS.",
                    "label": 0
                },
                {
                    "sent": "We just look at this parent child relationship.",
                    "label": 0
                },
                {
                    "sent": "And define the parent S. As a feature.",
                    "label": 0
                },
                {
                    "sent": "And the child St also a feature we can sub graph.",
                    "label": 0
                },
                {
                    "sent": "So in the translation from S to T, we can only lose matching subgraphs 40.",
                    "label": 1
                },
                {
                    "sent": "So if we have a number of matches in S, this number of metrics can also occur in T, but it doesn't need to.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact that's it.",
                    "label": 0
                },
                {
                    "sent": "We can never gain any new matches since if if anything matches in T, it must also help matches matches matched in S. Alright, so the frequency of the matches in a is at least as large as the frequency of measures in a of T. Same accounts for B Class B, sorry.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's look at our criterion again.",
                    "label": 0
                },
                {
                    "sent": "We have here a formula which includes these parameters.",
                    "label": 0
                },
                {
                    "sent": "The frequency in A&B.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can observe that the maximal value of this criterion is reached when all matches in a or all matches and B are lost, while of course keeping some matches in the other class.",
                    "label": 1
                },
                {
                    "sent": "So this can be visualized.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buy some schematic.",
                    "label": 0
                },
                {
                    "sent": "Good search tree.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is a search tree.",
                    "label": 0
                },
                {
                    "sent": "Here we have some sub graph which we shall call S. So this is the parent.",
                    "label": 0
                },
                {
                    "sent": "Below here we have a child which has frequencies which do not exactly differ from the frequencies of the parent, so we probably cannot do anything here and we have to track the complete.",
                    "label": 0
                },
                {
                    "sent": "The complete branch.",
                    "label": 0
                },
                {
                    "sent": "However, if we lose all of the occurrences in a in Class A and still keep some in Class B.",
                    "label": 0
                },
                {
                    "sent": "We know that any supergraph of this note here cannot give us any more information than we already have in this node.",
                    "label": 0
                },
                {
                    "sent": "So we can simply prune.",
                    "label": 0
                },
                {
                    "sent": "Everything below here the complete branch.",
                    "label": 0
                },
                {
                    "sent": "Alright, this is a simplified pruning criterion.",
                    "label": 0
                },
                {
                    "sent": "In fact, using this formula we can derive something a bit more.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Restrictive.",
                    "label": 0
                },
                {
                    "sent": "Which sums up to this year I'm I'm going to skip the derivation, but it can be easily done on a paper.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Well, with this we have an upper bound for the quality criterion of T. It's at least.",
                    "label": 0
                },
                {
                    "sent": "It's at most as large as the quality criterion of the parent node.",
                    "label": 0
                },
                {
                    "sent": "Plus some maximum criterion from here.",
                    "label": 0
                },
                {
                    "sent": "So using this criterion, we can say that no super graph of T of the child node can exceed this upper bond naturally, since they are also super graphs of S. And if we have seen a better subgraph, we can prove it.",
                    "label": 1
                },
                {
                    "sent": "If we so far have seen a better sub graph than this bound, we can prune the whole branch.",
                    "label": 0
                },
                {
                    "sent": "That's the point.",
                    "label": 0
                },
                {
                    "sent": "Alright this guy.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some rest as an algorithm.",
                    "label": 0
                },
                {
                    "sent": "We're beginning with the empty feature set, so this is greedy forward selection.",
                    "label": 0
                },
                {
                    "sent": "Then we run gisburn to give us the best sub graph according to the quality criterion off the sub graph joined with the feature set selected so far.",
                    "label": 0
                },
                {
                    "sent": "So in the beginning that's empty, but it's going to fill up.",
                    "label": 0
                },
                {
                    "sent": "While the quality criterion of this set is an improvement to the previous feature set.",
                    "label": 0
                },
                {
                    "sent": "We just join our feature tool set and run gisburn again to get us the next best feature.",
                    "label": 0
                },
                {
                    "sent": "Well and in the end we return our date our feature set.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "We can show you smaller experimental validation.",
                    "label": 1
                },
                {
                    "sent": "We've taken some part of the NCI data set which most of you probably know consists of chemical structures of.",
                    "label": 1
                },
                {
                    "sent": "Molecules which have been tested about their efficiency against cancer.",
                    "label": 0
                },
                {
                    "sent": "So you have a labeling of one and zero and.",
                    "label": 0
                },
                {
                    "sent": "Well, we have compared our approach to two feature ranking methods mean with sequential cover which is using a confidence confidence score to evaluate the feature attempt and the Pearson correlation measuring the correlation between the data and the label.",
                    "label": 1
                },
                {
                    "sent": "And we've also compared to wrapper approach called La La, so which has been published last year.",
                    "label": 0
                },
                {
                    "sent": "And which?",
                    "label": 0
                },
                {
                    "sent": "Users.",
                    "label": 0
                },
                {
                    "sent": "Actually the learner to validate the feature you you want to you want to evaluate.",
                    "label": 0
                },
                {
                    "sent": "So this is working extremely well.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In our evaluation of 10 repetitions of tenfold cross validation on AGS penetration of frequency 10.",
                    "label": 1
                },
                {
                    "sent": "So we have said that we want to test all frequency oil frequent subgraphs which occur at least 10% of our data.",
                    "label": 0
                },
                {
                    "sent": "Actually, this wrapper approach always bet all of the of the other approaches, including ours, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "However, there's still room for improvement.",
                    "label": 0
                },
                {
                    "sent": "I'm going to get into this next slide.",
                    "label": 1
                },
                {
                    "sent": "We have validated the data set on a linear SVM and well at least we could establish that.",
                    "label": 0
                },
                {
                    "sent": "Cork always goes better than any of the other anchors, also including several other criterion which criteria which we have not pointed up here yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yes, we can see that there.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Room for improvement.",
                    "label": 0
                },
                {
                    "sent": "A big improvement must be made at runtime, since you see we we are always calling up.",
                    "label": 0
                },
                {
                    "sent": "She's been forgetting the next best feature.",
                    "label": 0
                },
                {
                    "sent": "Of course this can be set up by simply saving simply saving the minimal DFS code, so we have to remember our DFS search tree and which of those nodes are minimal or not.",
                    "label": 0
                },
                {
                    "sent": "Since this is the.",
                    "label": 0
                },
                {
                    "sent": "At the same time, essential step in G spoon.",
                    "label": 0
                },
                {
                    "sent": "And further improvement must be made at the bound for later iterations.",
                    "label": 1
                },
                {
                    "sent": "If we have a problem which is well separable that we don't have a problem, since this can be.",
                    "label": 0
                },
                {
                    "sent": "Can be separated very easily and the algorithm converges pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "However, if in later iterations we only can add one more feature and we can, we can only resolve one more correspondence than one more correspondence.",
                    "label": 0
                },
                {
                    "sent": "It's just iterating.",
                    "label": 0
                },
                {
                    "sent": "Into eternity and never.",
                    "label": 0
                },
                {
                    "sent": "Well, it stops sometimes, but it's it's pretty slow, so we have to to think about something too.",
                    "label": 0
                },
                {
                    "sent": "About our algorithm in later iterations.",
                    "label": 1
                },
                {
                    "sent": "A further possibility would be combination with the with other bounding criteria such as.",
                    "label": 0
                },
                {
                    "sent": "Well the the very simple but effective only criteria of mouse.",
                    "label": 0
                },
                {
                    "sent": "So far we haven't had any success here, but it's still ongoing research.",
                    "label": 0
                },
                {
                    "sent": "So once we have solved this, we can use our algorithm on.",
                    "label": 0
                },
                {
                    "sent": "On a data set, even without giving an explicit frequency bound.",
                    "label": 1
                },
                {
                    "sent": "This is the fun part about it because we already have some some very strong bound and GS in Cork.",
                    "label": 0
                },
                {
                    "sent": "We don't need to give cheeseburger frequency threshold.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Really interesting part about this.",
                    "label": 0
                },
                {
                    "sent": "And well, another outlook would be to to explore this tree structure of our decision criterion.",
                    "label": 0
                },
                {
                    "sent": "I've already told you you can imagine this to be a tree of splits into equivalence classes, and these this tree structure can simply be looked at as a decision tree.",
                    "label": 0
                },
                {
                    "sent": "So in early tests we have.",
                    "label": 0
                },
                {
                    "sent": "Determined that those results give almost as good as predictions as the SVM.",
                    "label": 0
                },
                {
                    "sent": "So yeah, in principle, that was it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your patience on any questions.",
                    "label": 0
                }
            ]
        }
    }
}