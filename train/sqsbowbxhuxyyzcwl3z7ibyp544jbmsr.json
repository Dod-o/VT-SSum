{
    "id": "sqsbowbxhuxyyzcwl3z7ibyp544jbmsr",
    "title": "A Personal Journey: From Signals and Systems to Graphical Models",
    "info": {
        "author": [
            "Alan Willsky, Stochastic Systems Group, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_willsky_pjf/",
    "segmentation": [
        [
            "OK, it's pleasure to be here.",
            "But I just got it pretty much quite right.",
            "Little bit of personal history.",
            "My PhD is actually in systems and control, which I worked for.",
            "A number of years, although I think it's fair to say I've been out of control for the last three decades and migrated from there into doing things in signal processing.",
            "Statistical signal processing and then into things which overlap with NIPS.",
            "Now the reason I switch fields is is conjectural.",
            "Even to me, part of it might be that I have a short attention span and each change fields part of it might be that I get ticked off at each of the fields I'm in because of the narrow perspectives.",
            "They have an when I end up giving lectures in those fields to tell him how ticked off they are.",
            "So you can infer from that that maybe I'm ticked off at machine learning.",
            "At this point.",
            "But my big message is the fact that each of the fields offers something to the other.",
            "So just as I give talks here where I may be talking bout his distance and control statistical signal processing in numerical linear algebra have something to do for machine learning.",
            "I also give talks to point out where machine learning has an awful lot to save for systems and control in signal processing."
        ],
        [
            "So so I I really ended up getting into graphical models, but by complete accident I had a Sabbatical 1988 at INRIA in France.",
            "And if you don't know Albert Benvenisti should he's one of the greatest signal processors, control theorists and computer scientists I know working in a wide variety of fields, and in 1988 it sabbatical there.",
            "And this is just about the time wavelets.",
            "We're becoming hot and wavelets were being proposed as something that could talk through things such as create better malt whiskies and possibly solve the structural debt problem in the United States.",
            "The claims were outrageous and what albair poses a problem was could we come up with?",
            "A principled way of saying why they were good and where they were useful?",
            "For now, one of the problems I had with signal processing, which is no longer true to be fair, is the fact that it's primarily analysis based.",
            "That is, I mean, I hate the word signal processing.",
            "Signal processor sounds like something cuisinarts should make my rather think about a signal, then then then process it.",
            "So they were basically analyzing signals they weren't modeling them.",
            "So something that's common to systems and control.",
            "Anti Machine Learning is the fact that you think about using models and using those is the basis for deducing things and so wavelets, if you're familiar with them or something that do of course fine to coarse decomposition of signals.",
            "They also had the analysis form, that's the way they were being mostly applied, but there's a synthesis form which says how you create a fine scale signal by taking coarser versions and adding detail.",
            "And we took the picture of single G. Let's think about that as the basis for statistical models.",
            "Signals and this led us to thinking about something we didn't realize.",
            "We were doing graphical models which were models multiresolution models on."
        ],
        [
            "Trees and so this is a slide that really was aimed at signal processors.",
            "And a lot of it will resonate through the bottom, gives a picture of the applications that drove our work in the early part they were all what are typically referred to as Markov random field problems.",
            "So the graphical model problems, but they're huge.",
            "For example, the running over there talking about the ocean.",
            "Oceanographers have a problem that you can think of as being a 300 million dimensional time series that you're trying to solve over a period of 1000 years, and that's a typical signal processing problem.",
            "They have their gargantuan.",
            "And the things that we that we want to try to capture was the fact that in lots of the applications that specially ones that we were aware of at that point in interested in the phenomenon, might be multi resolution.",
            "The ocean, for example, has scales, spatial scales ranging from kilometers to thousands of kilometers.",
            "It is time scales.",
            "This is not.",
            "This is taking out waves and tides that have time scales from the order of days to the order of 1000 years.",
            "And so there are a huge number of timescales built in.",
            "And the idea of using a flat model, which is what Navier Stokes equation gives you, which is what the physicists would want, is something that any of us who think about building machine learning models would say with much better try to use use hierarchical models, whether or not the phenomenon is multi resolution today to maybe so in the oceanography context.",
            "And I'll talk about another one later.",
            "You have data, those those are satellite tracks you see over there which provide variations measurements of sea height variations.",
            "They're trying to estimate sea level C. See, the sea is not level dynamic range of sea levels about a meter over the Earth.",
            "That's why we have currents.",
            "If it were, if in fact it changed by by only 20 centimeters, Gulfstream would stop.",
            "So in fact understanding these things is incredibly important, but they have data like this and they also have ships that go around and then they have ocean acoustic tomography.",
            "Very different scales.",
            "Another thing is whether or not the phenomena or data are multi resolution.",
            "The objectives maybe so thinking about building models for discrimination purposes rather than generative models.",
            "Something physicists, for example don't think about, but those of us in estimation and machine learning do.",
            "And finally, whether any of those are true.",
            "The algorithms you may want to be multi resolution now we've got a couple of talks and I'll talk a little bit about it using the idea that coarser scale versions of a problem solving them maybe easier can guide finer scale versions.",
            "This is a picture of some applications.",
            "Summerworks been picked up by people who work in all sorts of other fields.",
            "I have no idea what helioseismology is, but I know that that our software is being used."
        ],
        [
            "There I should say that I'm probably going to missed mentioning the cadre of students who've worked on these things as they go along, just because the fact that forgotten most of their names.",
            "But there are great many students who contributed to all of these things, so we started to begin working on multi resolution models on directly trees leading leading to models that, for example, a linear model might look like this where.",
            "A finer scale version is some interpolation of a coarser scale clarinet plus white noise.",
            "So if those of us come from control, this is something we think about as as a very natural model aside point there, for those of you familiar with the French School in particular, things such as seminar Bourbaki where results are fairly mystical, we want to introduce something that was equivalent to shifts like in time series like Z transforms, and we put a gamma here for the shift going up from a from a fine too of course or level.",
            "I went away in a trip to Alsace and came back and Albair had changed it to putting a bar over the gamma and I asked him why he was working at the computer and he turned to me and said because it is the only choice and then went back to working so it was a typical something I've learned from my interactions with the inscrutable French.",
            "There are variety of other ones.",
            "This one I want to mention just for a minute 'cause it's kind of interesting.",
            "This is an approach that using computer graphics, but there's a precise way of thinking about constructing set things such as Brownian motion.",
            "So Brownian motions, a Markov process.",
            "And suppose I have the values at two endpoints of an interval.",
            "Well, if I draw the straight line between them, that gives me the best estimate at every point given those those two points.",
            "In addition, I can compute the variance of the error in that and so G at the midpoint.",
            "I can add an independent random variable that deflects that, so they now have three points that correspond to samples on.",
            "That Brownian motion, well, Gee, now they've got three.",
            "Since Marco process, I can separately think about doing each of the half domains separately.",
            "I'm going to create a tree.",
            "An important point think point here, which is something you don't see that much of machine learning is, even though the field that you want to get the fine level is scalar.",
            "The values I'm going to put it coarser nodes, maybe vectors, and a critical thing is understanding what the dimension of them is that something that people in control spend a lot of time on, and I'll make that point a couple of times."
        ],
        [
            "So let me talk a bit about the control theorist view of things so you know we've got a tree, so machine learning people not that easy to think about building algorithms to solve these the way that that control theorists would think about it, is one that brings in things that we know we algorithm that does fine to coarse sweep of your data.",
            "It's basically at every point computing the best estimate given all the data at that node and in the subtree below it.",
            "It's a generalization of the Kalman filter.",
            "It leads to generalization.",
            "Of of Ricardi equations.",
            "For those of you familiar with how you calculate error variances in state space models, and I want to mention a word here about stability results, 'cause this will come in early 'cause this is something you don't see discussed in machine machine learning something that people in control worry about a lot in terms of error propagation are the error.",
            "Suppose I'm making error, I have my models a little bit off.",
            "I round off or something like that.",
            "Are those errors going to propagate so that my estimate diverges?",
            "And there are two concepts that are used in control to show that won't happen.",
            "One of them is the notion of control ability, which is usually thought of as G. Can I take my system from some point by the controls I apply, go to another point.",
            "Now that may be something where I can't do it instantaneously.",
            "That is, when you drive your car, you don't instantaneously get from your home to your office.",
            "All you're doing is controlling acceleration and using the fact that the dynamics, the linkage to other variables propagates that into position differences.",
            "So the idea that so control ability here is the fact that in this context is the fact that there's enough uncertainty as they propagate forward, so that hopefully the errors that I made earlier are insignificant to their diffused out by the effects of additional noise difficulty there is, if the system is unstable, those initial errors can grow something people machine learning don't typically talk about, but there are graphical models that if you think of them, is getting arbitrarily large, in fact, are unstable, very good.",
            "One to point out.",
            "In the context of Markov random fields, something known as the thin plate model.",
            "So in fact understanding how errors can be bound is important and will come back to that a little bit later, so that's one place where control brings something that I have not seen in this field, and there's a downward, by the way, from this perspective, as many of you know, all your doing is solving a set of linear equations by doing Gaussian elimination back substitution.",
            "That's what the Kalman filter in the rough country will smoother are, and because in fact it's a tree, there's no fill when you do the Gaussian elimination, and that's the.",
            "Important fact.",
            "OK, one of the things we spend a lot of time on in control theory.",
            "It's not really a machine learning problem, it's more of an algebraic approach.",
            "So actually if you go back to you to pearls book, you'll see some things that are quite algebraic as well, and the idea here is if I give you the 2nd order statistics of the process, you might have gotten those from data, but if I give you the 2nd order statistics, can I build a model of that state space model, white noise driving a dynamic system in the big problem then is a can I be?",
            "What is the dimension of that state?",
            "Focuses on something very different in typical typical control problems, the graph structure is fixed, it's time.",
            "And what the real problem is, is what are the dimensions of the variables are going to put here?",
            "Here we're going to talk about something our first work in this area is where again the tree structure specified.",
            "So we've given you the nodal structure on this, but the challenge is to understand the dimensions.",
            "There's an interesting open question here, which is one that we're spending time thinking about.",
            "What if I don't know the structure of the dimensions?",
            "It's actually very ill posed problem, But they're interesting questions.",
            "Try to understand how to solve that.",
            "OK."
        ],
        [
            "Alright, so so this is a concept you also do not see in machine learning, and it comes from a very deep set of results in systems and control that stretch their way back to Norbert Wiener and two spectral factorization.",
            "And the picture is the following.",
            "Suppose I give you the 2nd order statistics of a stochastic process in time.",
            "I want to build a state space model and I want to get one that's at the lowest dimension possible.",
            "What's pretty interesting is you can build a model that is invertible that is the white noise that's driving it is actually a function of the observed variables that you get out of this.",
            "Something called the innovations representation, which is related to the Kalman filter.",
            "It does not build in any extrinsic uncertainty into it, so the state variables aren't hidden there.",
            "In fact, functionals of your observations are very deep fact and there are interesting things that come out of this in terms of building models that have that property.",
            "That have the minimal covariance that have have it, and those are models that are typically defined going forward in time and ones that have the largest covariance going backwards in time and that.",
            "Interesting, interesting context for those, those are often referred to there.",
            "Often a class of models referred to as.",
            "Conjugate models, ones whose covariances are inverses of each other, and something that in control, is studied a lot and turns the end of my talk.",
            "I'll get back to that in the context of machine learning, so it's an internal model in this context, so we've got a fixed rate.",
            "Let's look at the simplest problems with thinking that all the data.",
            "That we have data or the variables of primary interest with the finals finals scales what we're going to do at each node of the tree is we want that variable to be internal.",
            "We want it to be a deterministic function of the variables at the finest scale.",
            "So this is a very different notion that you typically see in machine learning.",
            "Some of those functions might be given, they might correspond to measurements.",
            "You take.",
            "Some of them correspond to variables you want to estimate.",
            "I'll give you an example of that.",
            "The rest are to be chosen or designed.",
            "And the methods that you end up using.",
            "Typically in this context are linear algebraic.",
            "They involve things such as Canonical correlations, basically doing singular value decomposition to variety of problems, and so the idea here is typically you're not going to be able to get exact models, you want to get ones that are approximate, and you do this by used like basically clipping off the small singular values in these in something that these models what you want to think of is the state is nothing more than the projection of the future on the past.",
            "There's an analog here, and all you're trying to do is to find the variables that provide basically a low dimensional approximation to that projection."
        ],
        [
            "OK, there's a nice scale recursive algebraic design for these who actually build these models from fine to coarse, and there's an alternative using wavelets, which we came back to will mention a bit.",
            "One of the things that that when I first saw it drove me nuts, I mean.",
            "The kind series case what is known, and so all the nodes are are not on the same scale and none of them are hidden.",
            "What's known is if you can find a state space model, the minimal one will in fact be internal ones that are minimal.",
            "Not true for my hidden models, not true.",
            "Here's a model which basically says look my my my my child is going to be equal to my parent plus noise.",
            "So think about two points down here that have the same parent.",
            "They have independent noise is given to him so I have X here.",
            "Here I have X + W on X + W two give you X + W one and if I give you X + W two you cannot exactly construct X.",
            "So you have something where minimal models aren't aren't necessarily internal.",
            "What's interesting is if in fact you only have measurements at that finescale.",
            "Your estimate is only of the internal part, so this is something that there's some actually very deep connections to.",
            "The fact that you have these hidden variables in the structure of these problems."
        ],
        [
            "OK, so this is something before we knew anything about graphical models was trees were great, but we knew that there were limitations to them.",
            "So again question what to do.",
            "The first thing we did was to flaunt it.",
            "We looked at problems in which the real objective was at a much coarser scale where in fact the problem here for us.",
            "If we were thinking about spatial models or temporal models with fine scale variables have physical distances, you have two nodes that are close together, but they're far apart on the tree.",
            "And so the variable that at their common at least common ancestor has to do the job of completely decorrelating these guys, and that's the problem with this.",
            "Well, if your objectives at a high scale, who cares if you've got it right, and that's something that in some fields, such as physics there horrified by we built models where we don't conserve mass at horrify.",
            "Some, but they aren't they from the data they have.",
            "You don't need to do that for the objectives that are interested in.",
            "Oftentimes I kid my oceanography friends is saying that they are only interested in one bit.",
            "Is it getting warmer or isn't it?",
            "The second thing we did, and I'll talk a little bit about one aspect of this is is to try to beat the dealer.",
            "This is cheating, just moving the trees around to get that.",
            "There's a theory theoretically precise way of doing it, which is to say look.",
            "I'm going to build a tree where each node corresponds to a region in space, but those regions overlap, so when I get to the finest level, I actually have multiple nodes that correspond to the same physical location that you can think of generative model, something that generates this way, and then you project by taking a convex combination of those values.",
            "To get a value point which is very interesting is you get an exact estimation problem that says take your measurements.",
            "If they're only the finest scale, put those same measurement values at all the nodes that are replicas of this thing, but put them in so the total information content of them is equal to the original information content.",
            "Basically, you do something that says that if I take a look at the inverses of noise variances, I'm going to put on these some of the inverses of those noise variances equals the original noise variance.",
            "You then run something on this overlap tree as we call it, and then you project back down.",
            "And it's a way to reconstruct the optimal estimate.",
            "A third one I'll talk about briefly is wavelet San finally, and this is about the point in time where I started interacting with Mike Jordan when he was still at MIT.",
            "We said, OK, let's start putting some loops and I'll talk, and that's where most of the talk will end up going.",
            "OK, so this was a really interesting example there.",
            "Actually some interesting openings by the way.",
            "Control does a lot more than linear things.",
            "Ann and Kalman filtering.",
            "But I tended Zupan Ghahramani stock at the Sam Roy session is open.",
            "Took a course for me.",
            "I think if you listen to him control consisted of doing factor analysis, changed overtime and with Kalman filter.",
            "And over the extended Kalman filter.",
            "Well that's true.",
            "We did the extended Kalman filter in 1970.",
            "There's a lot more to the field, so this is an interesting problem.",
            "There's an interesting problem, there's sort of a couple of context for it.",
            "We were motivated here by problem related, somethings place called Yucca Mountain, and so I'm not sure if you're familiar with Yucca Mountain.",
            "It's where there's all these thoughts about bout storing nuclear waste.",
            "There's a huge body of salt there.",
            "Absolutely huge, very stable, and so it's actually a perfect place to put this.",
            "But the question was things.",
            "Things do sleep and what you want to make sure the question was.",
            "Did the United States have to buy a piece of land?",
            "The size of Rhode Island or they have to buy a piece of land.",
            "The size of Georgia so that the expected time by which any of the radioactive material would escape from that region was greater than the half life of the radioactive material.",
            "And So what you're interested in doing is estimating a very global property.",
            "What's the escape time from a region?",
            "What kind of measurements do they have?",
            "Well, the drill drill and number of holes and what are they measuring their their measuring hydraulic conductivity?",
            "It's the analog of electrical connectivity, and then they'll do something that if your electrical engineer you think it's a 2 port tester in multi port test, they'll pump at one and watch the drawdown of water on the other.",
            "What's that doing?",
            "It's giving him a nonlinear measurement of some average value.",
            "Over a region.",
            "OK, so they have very different scale measurements and their objective is very coarse scales.",
            "One thing they want to know what the expected time to 222 part is.",
            "So what what do we do?",
            "We said we were going to build a model where at the finest level are going to be the hydraulic conductivities.",
            "We're actually not really interested in those were going to higher level that those nonlinear those measurements of these more aggregate things and at the top of the tree we're going to have our variable corresponding to this escape time well.",
            "I should tell you the way people do this in practice, because this problem also comes up in oil exploration and there will this take these measurements and they'll do conditional simulation.",
            "The pride of open entire conductivity field of huge area, the finest scale.",
            "It's too big for their simulators of water and oil flow, so they have to scale that up.",
            "That takes them several days to do and then they have to run their simulator by the end of this that exhausted, they've done 1 sample run.",
            "They go to the bank and say see there's oil here.",
            "Can you give us $50,000,000 to infect adults field?",
            "It's almost like that these problems are huge problems.",
            "In our case, we're going to run out and was up and down.",
            "The tree gives you estimates and variances and you're done.",
            "And what this shows is a comparison in two cases and whether this is the case where linearization works, you have small variations.",
            "This is something you get trivially.",
            "It's very easy to get.",
            "This is running their algorithms, it does pretty good fit now when things aren't linear, you don't get nearly as good if it.",
            "What does this say?",
            "Gee, we probably want to use nonlinear models, not only your functionals that sit on these things.",
            "And maybe we want to start using things for algorithms we're not going to use extended Kalman filters, but we may use particle filters and may end up using things like nonparametric belief propagation that Eric and Alex Tyler developed in in their work.",
            "So that's what this says is is we're certainly can do it and what we were talking with, Steve.",
            "We want the the the people who work in this field to use their forward models to give us samples so that we can learn from those samples what the right models are for our objective."
        ],
        [
            "OK, so let me talk a bit about wavelets and so I'll talk initially about the simplest way, which is the hard wavelet and the hard way that you want to think of it.",
            "Each resolution I have a signal, and from that I'm going to create a course and find version.",
            "I'm going to take pairs of values, nonoverlapping pairs of values.",
            "I'm going to take their difference that's going to give me the detail at that scale.",
            "Then I'm going to take their sums because I'm taking pairwise, just not overly pairwise.",
            "I've gotta signals got half as many points.",
            "That goes up.",
            "The synthesis then is G. If I've got a core scale variable, you tell me what the detail is.",
            "The difference of the fine scale values at this scale I'm going to add it to one side and subtract it from the other so it's a dynamic system.",
            "This is what it looks like.",
            "Their two children in each of those children are going to be equal to the parent value, apparent average ones.",
            "Adding the difference in the other, subtracting it, it's not a Markov model, because those two WSR negatives of each other.",
            "What would what would a control person do in this case?",
            "Well, Gee, if I've got a model where I've got position as a state variable.",
            "I might want to augment it with velocities of random variable to higher order model.",
            "So here you end up doing state augmentation.",
            "You actually have a model which says the state at each node consists of both the.",
            "Average value and that difference in values.",
            "They want to think about doing the dynamics.",
            "Of course, define this.",
            "Now is a deterministic relationship because both of these are in the state at that finer node.",
            "The stochastic part is I have to predict what the detail is going to be the next place next level based on what the statistics are of my fine process.",
            "So one of the things this brings up is the fact in lots of models that come up in these contexts, the potentials you'll get between your graphical model where the variables that the nodes are vectors will be degenerate.",
            "They won't be full rank.",
            "Something you typically don't see in a lot of the things that people work on in machine learning.",
            "So, so the question is, the Heart Heart is very choppy.",
            "Wavelet, there's a whole class of wavelets that in fact are much smoother than this, but but they overlap in terms of the values of the finescale signal that they've got.",
            "So if you think about it, if I want to get some the terministic going coarse to fine, I'm going to need to have more wavelet coefficients sitting up here.",
            "But then we want the model to be internal.",
            "We want the variable at their core scale.",
            "Note to be the wavelet coefficient or the scaling coefficient finescale signal that actually means adding additional coefficients to the state.",
            "Now you might say, well, Gee, that means doesn't go on forever, they need to start adding more and more things.",
            "The answer is you don't.",
            "So you end up with.",
            "With a finite dimensional model, the variables are in fact finite."
        ],
        [
            "And you can get pretty good estimates.",
            "So this is a simple toy example to model of process that that is multi scale fractional Brownian motion.",
            "These are noisy measurements of fractional Brownian motion where there was a gap in the data and what you're seeing on the right hand side are our estimates which come out of ours, which is the solid line and the optimal estimates which you can barely see.",
            "You can see a little bit of here and the one Sigma bars for the optimal estimates, and since I think all in machine learning.",
            "I agree that truth is a relative content concept.",
            "Purely truly, generative models are not things we necessarily want.",
            "I would argue that in fact this is something where our model is just as good as anybody who does.",
            "Fractional Brownian motion come up with."
        ],
        [
            "OK, so let me talk about ideas from numerical solution of partial differential equation that we started to use and the first one is something called nested dissection.",
            "Graph you want to think about, sort of the prototypical one here that I'm talking about need not be.",
            "This is one that's regular 2D grid.",
            "And there are class of methods there that have been dealt for solving PDE, so I've got a PDA that basically is ax equals B&A has a structure with a non zero elements, corresponds to the edges in that graph is just take all the values along one column and March.",
            "This way, in this way machine learning where people say fine you're creating a junction tree of some type out of these things, dimension is huge.",
            "There are other approaches would save with nested dissection says.",
            "Let me divide the region up so if I if I take the mid mid line condition on that these two are independent.",
            "Well Gee, then I can do divide and conquer.",
            "It's still the case that the variables at at the courses scale there.",
            "The ones that sit at the course so I can build a tree or high dimensional very high dimensional, and so you're not going to be able to keep a mean.",
            "You'll be able to keep it mean, but not a covariance matrix there, but you might be able to keep an approximation to the inverse covariance.",
            "So for Gaussian models the inverse covariance is most here.",
            "Assume no gives you the structure of the graphical model, you have an edge between nodes if and only if the corresponding element of the inverse covariance matrix is non 0.",
            "And So what we looked at was something saying, let's start at the bottom rather than the top and do variable elimination.",
            "And by the way, I was told I'm not allowed to talk about optimization here because there is optimization in another section.",
            "So if after this you go and say, well, she sent some things about optimization, I will disavow it.",
            "But here's a problem where there is optimization is actually part of an inference algorithm, and so better rather than go through this, let me let me do it."
        ],
        [
            "Pictures so the picture here is the following.",
            "I'm going to start at the bottom scale.",
            "I'm going to start doing variable elimination, growing region so so I have something here and when I grow it because in fact it's not a tree.",
            "I end up with fill in the model.",
            "LG let me get this thing big enough so I can do.",
            "Still do exacts smaller so I can still do exact statistical modeling on it and let me TH in that model.",
            "Use your favorite method.",
            "You could use a sparsity method Max entropy method to infect.",
            "Try to thin it.",
            "Typical, I projection in exponential families to come up with a thinned model.",
            "This is the inverse of the covariance.",
            "Then I'm going to do this at a whole bunch of different nodes and eventually these nodes are going to collide.",
            "And then I have to do what I think of as inverse mitosis.",
            "I've got to join them and eliminate those internal variables and thin and I get up to the top.",
            "Now I've got something that's a blanket for the rest and I can do back substitution again with thinning as I come down.",
            "So so this is."
        ],
        [
            "Thing that we applied to reasonably large problems to get both both estimates and error variances.",
            "You can see a little bit of Gulf Stream in the Atlantic.",
            "It's not nearly as powerful as some of the currents that you see in the Pacific.",
            "By the way, which also has things like the Gulf Stream, just the Pacific is very large.",
            "And on the right you see the error variances, which is something they're quite interested in, and you can see the pattern you'd expect where the error variance is smaller where the satellite had happened to flying tape measures of these things, so this is I don't know a couple of million variables.",
            "So what I've just described you, if I hadn't done thinning.",
            "That would be a tree model on this nested dissected thing that goes up and down the tree.",
            "So it's exactly an optimal model on this, but I'm making approximations.",
            "I'm putting in errors, so do those errors grow or not?",
            "That's a control question, so the question is, is this graphical model controllable?",
            "Well, what does that mean is, I think about going from a region to a somewhat larger region.",
            "What it means is the fact that the potentials that connect them.",
            "Over whatever that annulus is are weak enough so that in fact there's some diffusion that goes on.",
            "Now, what what is observe ability mean?",
            "Observe ability means that there are enough place, and so you want where you want to think about this is that if I put noise in an acceleration that will propagate into noise in position and that's the picture that we have here for observe ability, it's the opposite way.",
            "If I have a measurement position, I confer overtime something about about acceleration and what does that correspond to these models?",
            "That means I have measurements at enough nodes.",
            "The single node potentials are strong enough at enough locations.",
            "And the potentials that connect them to the variables of where I don't have have individual node node potentials.",
            "No potentials here correspond to.",
            "Measurements are strong enough so that I can get some inference from them.",
            "And under this you can get a precise picture of the fact that fact that you can control the errors, even if you're in the thin plate model, which means that region gets larger the errors if you did not have observe ability measurements would grow.",
            "It's also possible that you can iterate these things.",
            "Use this as something called the preconditioner, and now we're going to talk about preconditioners, and I'll say a few words about those in something called the Richardson iteration."
        ],
        [
            "OK, so now let me turn to another topic related to walk sums and many of you may have seen some of these things about walks arms and this is work of Jason Johnson and Dmitry Malamute off and the latter part of think at Chandrasekharan.",
            "So here we've got a large Gaussian process.",
            "The way these are specified for graphical models isn't Interment to meaning.",
            "The covariance.",
            "What you have are the graphical model structure which gives you potentials in the form of the precision matrix, the inverse of the covariance J.",
            "And you have what control we would call the information state, which is the inverse of covariance times the mean.",
            "That's basically what you get from data, and so the problem you want to be able to solve is given J&H and I sorry I switched from U to X hat.",
            "You want to be able to solve these equations if you want to get the estimate.",
            "If you want to get the error variance is I want to get the diagonal elements of the inverse of the coherence.",
            "Alright, so let's do something that is fairly simple minded, but it is something that gives us a fair amount of intuition is.",
            "I'm going to assume that that I've normalized my model and it's not difficult to do so that the diagonal elements of inverse covariance, or one that's just for simplicity and let me write J as I -- R. So R captures all of the off diagonal terms.",
            "RIJ has very precise interpretation, is partial correlation coefficient.",
            "It's the correlation between XI and XJ, conditioned on all of the other variables in the graph.",
            "So in fact, if if there's not an edge between those at 0.",
            "But it gives you an idea of the strength of things.",
            "So let's suppose we did something not even trying to do this in verse, and this corresponds to doing some kind of Gauss Jacobi iteration in solving the equations just to power series.",
            "Not saying this is the way you want to do it.",
            "Let's think about what these matrices are.",
            "Think about the IG element of R-squared.",
            "Taking are multiplying by itself.",
            "Well, that IG element is only going to be 990 if there is a path that is linked to that starts at.",
            "I goes to some other node K and then goes to J.",
            "And I'm going to get the sum of all of those, so I'm going to get a sum of weighted walks around this graph.",
            "OK, so this is starting to look a little bit of thinking like message passing.",
            "There's actually information that's propagating inside of these things and so you can get sums over these weighted walks corresponding to these and."
        ],
        [
            "And it's fairly simple just looking at the equation to see if I wanted to get the Estee element of the covariance that corresponds to collecting all the walks that go from noticed and OT.",
            "And if I want to get the mean, I wait.",
            "I think the sum of those works waited by the corresponding potential vector potential value measurement.",
            "The weighted measurement value at each of the nodes.",
            "Alright, well different inference algorithms may compute walks, maybe not all of them, and we'll see that BP does not compute all of them.",
            "At least there are loops in the graph, different message schedules.",
            "What that means is, there's one class of models where you came with impunity.",
            "Think about using any message schedule, and those are ones where these series are absolutely summable, doesn't matter the order in which I add these things up, and that corresponds take that our matrix and take the absolute value of every element.",
            "In it.",
            "And as we'll see that he builds the case that if you think about a -- R, the eigenvalues of our have to be less than one because I -- R has to be positive definite.",
            "The eigenvalues of our bar might not be less than one.",
            "But if the spectral radius if the largest eigenvalue of this model is less than one, you have walk some ability.",
            "You can add up these things in any order that that you want.",
            "Because of that, for walk some ability.",
            "BP converges 'cause it calculates will see a calculation a number of these walks, but if it converges, it turns out it calculates all the walks for the calculating the means, and that's well known result.",
            "BP convergence for Gaussian models get the means right, but it doesn't collect them all for the variances.",
            "It misses some.",
            "And that is easy as seen if you just look at the computation tree."
        ],
        [
            "So here's a simple graph an what I've written out is what the computation tree for three iterations of BP would look like from the perspective of node one.",
            "So the first time no one is going to get measurements from these three guys.",
            "The second time, well, if you think about the two iterations, well, no one would have sent the message to node 2, but this guy is smart enough not to send that back.",
            "He's just going to take a message from node three, send it back.",
            "Another time, and similarly from from no three, he's going to get 3 messages his first iteration, but he's just going to take these two and send it back.",
            "And So what BP does after three iterations from the perspective of node one, is to complete inference on that tree.",
            "That means in apparently stuck on something here.",
            "That means that it is calculated the full walk sums corresponding to that tree.",
            "That means that it's calculated for calculating the means from all of these different ones.",
            "It messages propagate up from those appear.",
            "This gives a very simple picture of why captures all the.",
            "Ones for calculating the means but interesting.",
            "Remember it's only the node at the top.",
            "This is valid for in terms of where the capital should go or not and look at the end of the self return walks.",
            "So if I want to get the diagonal elements PII, those who walks that started I and come back.",
            "Alright, so if I start at the top node and want to go, I can go 12321.",
            "I can go 12132321 variety of things here and those are all in here but I cannot get the walk 1231.",
            "It doesn't appear there.",
            "It's not a self backtracking walk.",
            "So BP only calculates backtracking walks.",
            "Interestingly, it goes through every one of these things twice even number of times.",
            "What does that mean?",
            "The sign of the of the element of our doesn't matter 'cause you're going to square it or quadruple it, so BP doesn't know if in fact the.",
            "You have a attractive model, but all those things are positive or a frustrated model where some of them are negative and that's where it runs into trouble.",
            "So that's what all of this says.",
            "Alright, so for walks on mobile models things converge, but if it's not work, some of OBP may not converge and it's pretty easy to see can produce absolute nonsense.",
            "And the reason is that the model that it can create on the right may correspond to a information matrix that has negative eigenvalues, which is nuts doesn't mean anything."
        ],
        [
            "So this is a simple example where I put values on these things.",
            "Or Dimitri put values on these things value row and it has one frustrated potential.",
            "So these things would like to be positively correlated.",
            "But somehow these won't be negatively correlated.",
            "And look at where rows equal to .39.",
            "This has to be less than one for work, some ability.",
            "It is everything.",
            "Bro Infinity here is is a theoretical calculation of our all of the computation trees.",
            "Valid, valid graphical models.",
            "If all of them are, BP typically will converge for the variances that may not converge for the means, but I'm not going through all of 'em well, I've just got to .4, and in fact this is a picture of what the variances are that BP computes this iteration.",
            "It says the variance is minus 150.",
            "Which is an example of the kind of catastrophic failure that BP has been.",
            "'cause of the way it's trying to collect these works."
        ],
        [
            "OK, so this is some work that was done by.",
            "A couple of the students in my group, Eric Sudderth and Martin Wainwright.",
            "Not sure whatever happened to either of them.",
            "But it was a very nice piece of work in which we've built, and it's fairly simple idea, and it's related to the notion of Richardson iterations, and so the idea I've got J and it's got this graphical structure.",
            "And So what Eric Martin did first to say, let me think about this is one of one case.",
            "You can do more general things that you can find in the paper that they wrote.",
            "And in Eric's Masters thesis let me just cut some of those edges.",
            "Where does that mean?",
            "I'm going to take this metrics and divide it up into part that corresponds to a tractable sub graph when somebody, somewhere.",
            "Willing to do exact inference and another piece I'm going to put that - there.",
            "Remember I want to solve Jax hat is equal to H. What I'm going to do is bring this over to the other side and I've got an equation that looks like this and now I'm going to make it.",
            "This is what's called Richardson iteration.",
            "Going to solve this kind of problem, we're going to plug in my last estimate over here.",
            "I'm going to solve this problem.",
            "Then do this iteratively if it converges.",
            "In fact, you're going to get the right answer, and that corresponds to looking at the eigenvalues, DS, inverse times, KS for example.",
            "More generally, and this is something that Eric and Martin found their work, you end up with with much better performance in many cases.",
            "If you switch among different trees during the different iterations, you cut different edges, it can significantly outperform these things.",
            "What Venkat did was to come in and take a look at why this case and how these things worked, when in fact you had walked summable models."
        ],
        [
            "There are variations on these things, So what I've just described here is it is a full Richardson iteration.",
            "Their Gauss Seidel iterations, in which in each iteration you're only going to update some of the variables.",
            "So typically people do in solving PDE's on a 2 dimensional grid still give us a checkerboard red, black where I'm at one point I update the red, keeping the black fixed, and then I iterate so only update some of the variables, so some of them stay fixed and I need to deal with the other deal with the others and update those.",
            "And of course when I want.",
            "Update just that a subset of variables.",
            "There's a graph that sits there and I could do something iterative on that so you can miss these things.",
            "If it works on mobile models, you're going to get the exact answers.",
            "If you collect all of the walks and then why you want to think about this is at one iteration.",
            "I've done all of the walks in one of these trees.",
            "Then I'm going to do a calculation which hops across one of the edges.",
            "I just that I cut.",
            "And then I'm going to do something in a different tree, and so there's a very simple way of trying to see whether or not at some point every walk is in fact captured.",
            "Very simple tests that can effect show that these algorithms will give you the right answer, but of course there's a question of what tree do you use?",
            "An Venkat did some.",
            "Did something sort of cute.",
            "Done this and so.",
            "Let's take a look at the error in our estimate.",
            "So if we have an estimate at an EN, is the error in that?",
            "Now obviously we'd like to do is to drive that error as make it as small as possible in the next iteration.",
            "Unfortunately, we don't know what X had is.",
            "That's the whole point, so we can't get the error.",
            "So we're going to look at what in system identification and control would be.",
            "Look at equation error.",
            "What we do know is we know what H is, and we know that.",
            "This thing is supposed to be equal to that little.",
            "Take a look at the difference in that.",
            "That's the difference in the right hand side that we can get our hands on.",
            "And it's fairly simple bound.",
            "It says in fact the L1 error on the state can be can be there's a bar here 'cause there's a slight calculation you have to do in addition, But basically you're doing walk sums on the entire graph, something you don't want to touch minus the walk sums on whatever graph you choose to use next.",
            "Now obviously you want to make this as small as possible.",
            "This is whatever it is.",
            "It's just in terms of full graph.",
            "You want to make this this as large as you possibly can, so you have a problem which says you want to get the Max.",
            "Walk some tree.",
            "And that's a problem that starting to make things look tractable.",
            "The problem is that that's not an easy problem, it's an empty hard problem to solve.",
            "So Venkat did something similar, said look I want to put weights on the edges and say what I'm going to do is look at each edge.",
            "I'm going to look at what the work some contribution is going to be to producing that error.",
            "Just if I had that edge, so those are walks that go back and forth.",
            "It is pretty simple to figure out what that is now.",
            "I have an edge on each of.",
            "Weight on each edge of the graph.",
            "I do Max weight spanning tree to figure out what the best graph uses."
        ],
        [
            "And this shows the kind of performance you can get.",
            "This is this is a log log scale here for how the error goes down.",
            "This is using one tree.",
            "This is using tools.",
            "You get an exponential speedup and both of these ones Gauss Seidel, one to Max spanning tree, one or the types of speedups you can get by adaptively choosing these things.",
            "So it's actually quite fast."
        ],
        [
            "And it may have implications in waste applying it in the PD context as well.",
            "Where people do these kinds of things.",
            "So graph decompositions in solving PZ is a very active area.",
            "There's an awful lot of work if you read Siam Journal on numerical linear algebra.",
            "We see lots of things about that.",
            "Alright, there's an alternative approach that being glue just investigated in his Masters thesis, which this is the simple picture.",
            "So we're looking at the notion of feedback, vertex sets and somebody some of you may know what feedback vertex set is.",
            "Feedback vertex Set is a set of nodes that if I eliminate them, I would have tree.",
            "OK. And here's a picture where there's one node that in fact creates a tree.",
            "Suppose I have a feedback vertex and will come back to the fact that for graphs like 2D grids, feedback for assessor, very large, you can't deal with them.",
            "What I can imagine wanting to try to do is to do Gaussian elimination on this remaining tree so the feedback vertex set, either one note or several, can then solve its own problem, and then when it's solved then I can spread things back and do tree calculations so you have a different message passing algorithm.",
            "It's one where initially the feedback vertex nodes send messages here are basically say hey help me provide think about doing Gaussian elimination.",
            "You're doing things where you're doing operations on the left hand side, ax equals B and on the right hand side.",
            "And what the feedback vertex notice that needs both of those and each doesn't know something about what's happening to the things it needs to solve its problem.",
            "It also needs to know something about what's going on in the rest.",
            "The rest of the tree thinks so.",
            "In fact, this note the BP messages here will be vector messages, 'cause some of them correspond using their own potentials.",
            "Their own H is to calculate what they think are estimates the wrong because they haven't taken.",
            "In fact, the fact that there are loops.",
            "And also to calculate quantities that they will then feedback to this guy so or to the set of feedback vertex node since they can solve their problem exactly once that's done, there's another set of messages that get sent back out to the other nodes to do corrections and come up with the optimal estimates.",
            "So this yields the optimal answer in this.",
            "In fact, you've got a feedback vertex set for the graph.",
            "Now there are obvious issues which I think.",
            "Most of you are probably already thinking about, which has to do with the fact that feedback Vertex set can be large.",
            "Complexity here is order K squared an where K is the cardinality of that feedback vertex set.",
            "So it better be smaller.",
            "K is on the order of nor the square root of N. You're in trouble 'cause this is N squared or N ^3.",
            "So if K is too large, the idea is look.",
            "I'm only going to choose a subset of the nodes that correspond to the break some of loops, and so I'm going to run this thing on the rest of the graph.",
            "I could just do BP.",
            "It's not going to give me exact answers if it converges, will give me the right means and it will in fact give me the right information for the feedback, vertex nodes.",
            "Or maybe I run embedded trees or some other algorithm that does those things exactly."
        ],
        [
            "And assuming you get convergence, and in fact now because of the fact you're limiting some loops, you can get convergence for non walk summable models fairly easilly.",
            "You know give a picture of that, you always get even if you're doing approximate BP.",
            "On the rest you always get the exact means and variances on those feedback nodes, you get the exact means everywhere and if you're doing BP on the remaining graph you get obvious approximations to the variances.",
            "Basically, this approximate variances collect more walks than because they're getting some of the things that involve the loops that go through the feedback vertex nodes which are exactly the things that BP misses.",
            "If you have attractive models, so everything you're adding is positive, you're getting better and better bounds pending on how many walks here.",
            "In fact, collecting this how large the feedback vertex it is.",
            "What you can also came with.",
            "How do you choose which nodes, and there are two things you want to do is you'd like to be able to enhance convergence on the remaining graph, so it's not working, but we would like to get.",
            "That that row bar that I put in the rope to be as small as possible.",
            "You also want to be able to collect the most important walk, so there's a local calculation.",
            "Come up with something that gives you an idea of which nodes to choose."
        ],
        [
            "And you get pretty good performance with on the order of log notes, so getting looked at at 80 by 80 grids with randomly chosen potentials, so it was not a regular process.",
            "An example here where phenomenon walk summable model, so loopy BP is awful.",
            "This is basically if you use something that corresponds to separators, but this is using on the order of nine nodes and this again is the log of the variance.",
            "You're getting dramatic performance here.",
            "We don't have proofs.",
            "Of this sum results on on correlation decay.",
            "I think will allow us to get proofs on the fact that in fact we can get complexity on the order of log squared N and log squared N. For these types of models.",
            "And that's for calculating both means and the variance is.",
            "OK, I'm going to change gears a piece of work of Dmitri Melut offs and it's."
        ],
        [
            "When I give you the part that makes it sound outrageous, I'm going to do, I'm going to solve the problem by making a low rank approximation to the identity matrix.",
            "So that's what I'm going to do.",
            "Alright, so we got this inverse covariance and we'd like to get its inverse.",
            "We'd like to get paid well, obviously, one way to do it is to solve this set of equations.",
            "And I can think about doing that column.",
            "Why so I can get the first column of P by solving J times the first, P is equal to Y.",
            "One can do for all of that, so in fact.",
            "Jay is sparse.",
            "That's an order and calculation, but I got 10 of them so I have an N squared problem out of this, so it's infeasible for large graphs.",
            "So what actually Jason Johnson and Dimitri said was, you know, maybe I can do something here where I'm going to do a low rank approximation of the identity.",
            "So rather than having the identity here, I'm going to put a matrix that's low rank so it's going to be be transposed where B is going to be long and skinny, be transposed, therefore short and fat.",
            "And that means the rows of B are redundant there.",
            "They are not linearly independent there over complete.",
            "And so I would like to be able to solve this problem.",
            "I'm going to claim that that gives me something good.",
            "I mean this is not problem with the identity.",
            "Would you actually solve is for solve this equation and then post multiply it.",
            "Complexity is on the order of M * N where M is is basically the width of this matrix be that column dimension so that you'd like to be small.",
            "Why does this make any sense?"
        ],
        [
            "And the reason is, if you take a look at the approximate answer you've gotten compared to the exact one, it's equal to the exact one plus some interference terms which you can think of, either as doing the fact that we're splicing some things or aliasing some things out of this.",
            "And if you take a look at those those terms, you can see how you might want to do things well.",
            "Gee, suppose that.",
            "And I don't know the value of PNG, but what if I assume that PIJ is reasonably large?",
            "Well, I would like to take BI&BJ orthogonal to each other, so I get something that zero if this is near 0.",
            "I don't care if they're orthogonal or not.",
            "OK, but what I will do is I will in fact do random sign flips.",
            "I'm actually had the rows of this be matrix be identical except that the orthogonal ones and then will be some identical ones, but with a random sign sign flip.",
            "What that means is that for those by transpose BJ is either zero 'cause there are Fogle or it's random variable with zero mean and variance one 'cause it either takes a value plus one or minus one and that means this gives me an unbiased estimate of covariance.",
            "You can think of this as a graph coloring problem.",
            "If I have correlation decay, you know if there's a point that's far enough out where the correlation with the point I'm interested in here is small.",
            "I'm going to repeat the same vector but with the random sign flip, so I can think about taking a bunch of points and figuring out how to color it to in fact get good performance.",
            "But it doesn't work well if there's low correlation delay, and that says you don't want to think about taking the values at the individual nodes, and they want to think about taking linear combinations of them in particular, bases that do a fair amount of compression, such as wavelet bases."
        ],
        [
            "And so here's the picture that goes on.",
            "Here is the identity matrix.",
            "Here's my spliced matrix, and so the different the two different colors, Gray or black, correspond to whether A plus one or minus one goes on.",
            "So what I've done is I've repeated some of these things.",
            "And for things that have correlation, decay mean you may be able to see there's a little bit of difference between the approximation in the true.",
            "However, that does really badly.",
            "In fact, if in fact you have long tails in your correlation, so maybe we use wavelet basis and we splice the wavelet basis and you can get dramatic improvements and performance that is even better.",
            "You can't really see a difference between those two, and we don't have."
        ],
        [
            "But you can get on using correlation decay.",
            "We applied it to to a smaller version of that ocean data problem, so we had only a million variables in it an where our approximation to a million by million covariance matrix had ranked 448.",
            "You got answers, they're indistinguishable crunch through to the exact answer."
        ],
        [
            "OK, so let me talk now about things where I know we're going to have other talks of multigrid, and I'll talk a bit about these.",
            "So these are models that the number of people talked about looked at, which are pyramidal structures of 2D.",
            "Grades are typical models we would like to get a student join or Masters thesis developed optimization methods to solve these.",
            "I didn't say that.",
            "But you can capture things with long range correlations, and you can also get things which motivated us to do something else which have a very nice property which actually people without piddies use for a different method.",
            "I'll talk about in a minute, which is the fact that conditioned on neighboring scales, the residual correlations within a scale tend to be sparse and local correlations do.",
            "And so now you can think about doing algorithms.",
            "Typical multigrid algorithms would do iterations at each iteration, starting at the coarser ones, and then propagate down and do additional additional iterations in calculations as they go down.",
            "What Jim did was to use those adaptive embedded sub graph algorithms to figure out each iteration which was the best thing to do."
        ],
        [
            "And here is an application anyone want to guess what?",
            "That's a picture of?",
            "It is the surface of a soft salt Dome several miles below the floor of the Gulf of Mexico.",
            "As BP has unfortunately shown us to be true, there are huge deposits of oil very deep under the floor of the Gulf of Mexico.",
            "One of the reasons that they are there, they're trapped there, oils, very salt is very impermanent, permeable so so that the oil will trap.",
            "Against it and underneath it.",
            "A big problem for the seismic processing is salt doesn't have layers, so in fact the things they do to get the layering structure doesn't work very well unless they know where the top of the salt is.",
            "So they have these guys who sit looking at the seismic data and they pick out points to create a surface, and what we were told we have an algorithm.",
            "Pick out a bunch of points, they go off and they play a game of squash.",
            "They call their mother, they have lunch and then come back in the algorithms.",
            "Given an answer that's not right, you're going to pick some more points.",
            "And I'm going to change the stiffness in a few places so they go out.",
            "They have dinner.",
            "They have a good night's sleep.",
            "They have long breakfast and lunch time just before lunch time.",
            "They come in and I didn't answer and the question was so now this is a re estimation problem.",
            "I've got an estimate and now I'm going to do changes to some of the potentials.",
            "A subset of the nodes.",
            "Can I do something iteratively to solve that fast and so we use these adaptive methods that we developed to be able to solve these problems and this was worked on with some folks at Shell Oil in.",
            "In Houston"
        ],
        [
            "Alright, so now let me talk a bit about multipole moments.",
            "And these come from partial differential equation mostly from solving things like Maxwell's equations and the idea behind these is Gee, if I want to get a good approximation electric field at a point here and there are a bunch of charges up here, I probably can get a pretty good first order approximation by aggregating those into a course of one big charge.",
            "And then I probably gotta do some local smoothing.",
            "OK, now that's the preconditioner that use, but that let us to think about the fact that G conditioned on the score scale picture the remaining correlations were in fact local.",
            "And of course people want to Maxwell's equations exactly, believe Maxwell equations or truth, and therefore will just use this as an approximation as people who do machine learning like we said, look, we're going to build a model that has this structure exactly.",
            "And so the idea here is I want to build things which which can think of if I just look at the graph of between scales, it looks like a tree pyramidal structure in scale.",
            "However, when conditioned on neighboring scales, the correlations correlations within the scale are sparse and local.",
            "OK, that doesn't mean the graphical model is."
        ],
        [
            "So you have to rethink how you think about things.",
            "So in fact you know graphical model.",
            "It's the inverse of the covariance, that's sparse.",
            "I mentioned earlier in the in the in the control context we often think about things where we have dual things which have the inverses of the covariances being the thing.",
            "So we use the word conjugate models.",
            "This is a graph where you're showing showing the correlation structure.",
            "So for example, in this model, one in three or uncorrelated.",
            "Just showing the correlations.",
            "If I took the inverse of this to get the graphical model before.",
            "But multiplying by this is really fast, and that corresponds to finite impulse response filtering, something that people in image processing do all of the time.",
            "It's a finite finite with."
        ],
        [
            "Wiener filter.",
            "So we developed the title.",
            "I can't remember why we came up with that complicated tile, so it's sparse.",
            "So we've got a multi resolution model and it's sparse in scale, conditional covariance and So what this says is condition on scales one and three variables.",
            "Two and four are independent.",
            "And So what this means, if I look at the inverse covariance, I'm trying to come up with a model of it as something that lives on a tree.",
            "Or maybe slightly more complicated graph.",
            "I'd like that that is that is sparse.",
            "Plus something whose inverse is sparse and concentrated within scales.",
            "So this guy is not going to be sparse."
        ],
        [
            "'cause he's got that so so this is J X = H and I'm trying to solve solve for X.",
            "If I do a Richardson iteration and just keep the three part on to go up and down, I'll bring the thing over to the other side.",
            "So what happens in that step in that step?",
            "The calculations I have to do our looks like this.",
            "I'm going to get a new estimate so this is a tree and H minus this and of course this is not sparse.",
            "I don't calculate that exactly, this is sparse, so I use whatever sparse method.",
            "I've got to solve for Z, which I plug in here, very fast to be able to do that.",
            "And then when I want to solve within each scale that's multiplying by a sparse matrix, it's basically what's used in multiple methods, it's dramatic."
        ],
        [
            "Thing I will show 1 result from solving the optimization problem.",
            "This is on the state reports was 100 index but but Jin had data over that 17 year.",
            "For only 84 of the companies and she took it as that.",
            "Restructure the way SMP divides things into markets, divisions in industries and companies into this and so it remodel there and then I want to understand where there are strong residual correlations.",
            "So you find things such as Schlumberger which is an oil services company.",
            "In a different industry, has high residual correlation with Shell Oil companies.",
            "You find that companies like Microsoft are are going to have high high residual correlation with things like Dell sit in different places.",
            "You also find out that Walmart is negatively correlated to everything."
        ],
        [
            "OK, a few other things that that that we're doing that that just to advertise a bid on this.",
            "We've done some work of that quite information theoretic and has some interest to the to the community.",
            "This work done by student Vincent, an anonymous on Kumar who is a postdoc in our group of trying to learn the error exponent.",
            "So as I get more and more data, let's think about Chow Liu algorithm for learning a tree.",
            "What errors does it make?",
            "And what's the rate at which those errors decay to zero as I get more and more data?",
            "More than that, what are the graphs that are hardest to learn, easiest to learn so it turns out that that change Markov chains are the easiest to learn.",
            "Star graphs are the hardest because of the fact that it's very easy to.",
            "2 interchange edges.",
            "Some work that actually you can find.",
            "If you look at your pearls book, something that as a control theorist I really liked it was an algebraic characterization of how for graphical models, if they were hidden trees, how you could, if you had the exact statistics, discover exactly the hidden structure.",
            "There's some algebraic conditions, and Jin Choi develop this into a precise algorithm for turning this on, and then of course you know exact statistics to relax the algorithms that allow you to do robust identification of those hidden trees.",
            "So this is a problem we're trying to discover the tree structure.",
            "We haven't looked at the problem yet trying to look discover the tree structure and the dimensions of the variables at the hidden nodes.",
            "It's very interesting problem that we're quite interested in it.",
            "And this is something that a number of people think Emmanuel Candes presented something on this type of problem here.",
            "This problem I proposed to Venkat Chandrasekharan.",
            "Roughly speaking, the.",
            "There are two ways that we often talk about doing complexity reduction of complex data.",
            "One is dimensionality reduction.",
            "OK, things like PCA, the other is let's keep the full dimension, but let's get a sparse model.",
            "OK, well, why don't you just one of those?",
            "Why put all the heavy lifting on one of them?",
            "Why not do a combination of them?",
            "So rather than do PCA, PCA says look, I want to get all of the statistical structure there.",
            "I'm going to identify a low dimensional set of variables that when conditioned on them, all of my original variables have a sparse graphical model.",
            "And what that corresponds to is taking an inverse covariance in writing, is some of something that sparse and something that's low rank.",
            "And you can formulate very precise problems on this and then Cat is his drag.",
            "Me kicking and string screaming trying to relearn all the algebraic geometry I've forgotten the conditions under which you can see how the errors behave in this by looking at the curvature of algebraic varieties, but are very beautiful set of results that lead to very very efficient algorithms."
        ],
        [
            "So what's on the horizon?",
            "So this is one that I think about because of sensor networks.",
            "We talk about message passing, but in machine learning you really not talking bout messages or if it's a sensor network, their messages, and if you think about BP as a distributed algorithm.",
            "It's a really dumb algorithm with very simple protocol.",
            "What does each node need to know?",
            "It needs to know whose neighbors are and it needs to know which of the messages are coming from which of its neighbors.",
            "Because if you want to send a message to this guy, it's going to bind the messages from all of its other neighbors to send it there.",
            "So it's going to computation routing.",
            "That's what that's basically what it's doing.",
            "But of course, they so the only header bit it gets is which node is sending it, something it then immediately forgets everything.",
            "It just saw.",
            "It gets a new set of messages.",
            "That's what BP does.",
            "G. Suppose I don't try to keep data that tells me something the way the way cellphone multihop messages work.",
            "Wireless networks that that give you an idea of where things have gone.",
            "Are there ways that notes can in fact, use those to understand not just where to route things, but what computations to perform on these?",
            "The feedback vertex set out to my describe you is a is a global algorithm that is all of the nodes need to know the structure and know who the feedback vertex nodes are.",
            "There ways of getting things that are completely distributed so it looks like BP and each node on the way figures out if in fact it's Leonardo DiCaprio as King of the mountain is one of the feedback vertex nodes, or Leonardo DiCaprio is down in steerage.",
            "As one of the nodes in the leftover tree.",
            "This is something that we're really quite interested in understanding, by the way, you can think of these distributors that these are distributed dynamic systems, and we know what we're trying to realize.",
            "We're trying to get calculate all the walks.",
            "For example, alright, can we come up with distributed approximate ones by building more complex memory into these things?",
            "And so this is actually really quite interesting.",
            "Is supposed to really our messages, but the statistical structure of the process you're looking at is not identical to the communication network structure.",
            "So you're going to send messages to different places where you send.",
            "Why do you have to send them along along nodes of the graphical structure?",
            "And we're doing a variety of things on non Gaussian models.",
            "Most of what I've talked about here we're trying to generalize van cats doing some really wild things on be posting graphs instead of doing sparse Postal rank compositions.",
            "And with that, I mean, if I recall correctly, you gave Me 2 1/2 hours to talk and I think I finished early.",
            "OK, thank you.",
            "We have a question.",
            "Yeah, metric.",
            "Um, so you know the work that Erica Martin did on that trees and things like that?",
            "Is it closely related to this?",
            "Subgraph preconditioners work?",
            "That is, indeed, indeed, I mean, this is stuff that I think I think it's correct.",
            "We started developing these without really having looked that much at the linear algebra literature.",
            "But then you find those things in there and they don't have some of the things that we've started to do.",
            "For example, these venkat's algorithm is trying to figure out the best trees to use and like on.",
            "But indeed there are lots of things in there.",
            "Typically there, but I didn't talk about is how we might be able to calculate the variance is in these things and Eric's work.",
            "He showed that Gee, you called him keynotes.",
            "If there is a small number of keynotes which turns out to be a feedback vertex set, you got a really cute way of getting variances.",
            "Well, that's not something you'll find in the medical literature 'cause they don't.",
            "They're solving ax equals B, whereas an elliptic operator they're just trying to solve that they're not trying to get the diagonal elements of the inverse of a, which is what you want to get.",
            "Having maximum weighted trees is very scattered that.",
            "They choose those trees in that way, but but it's not in sort of this walk some kind of picture of things.",
            "They have simpler out.",
            "We've talked to some of the people in that field.",
            "There are ways they tried to Max weight with weights there usually taking or the potential weights on things multiplied by whatever the equation error is at each node.",
            "So this is a bit more bit more machine learning BP oriented in terms of thinking about walk sums so, but is it?",
            "Is it a very numerical linear algebra?",
            "So that's neat.",
            "I can just plug that into my algorithm.",
            "By changing 3 lines of code, yeah.",
            "Good.",
            "Maybe I have one so you mentioned a few times so.",
            "Control people do.",
            "Don't know about control.",
            "Control people so.",
            "So what do you think are the main reasons for that?",
            "I mean, it's tractable to get into this.",
            "Well, in my opinion, the barriers are breaking down their variety of us, Constantine, me and other people have come from sort of more systems and control background optimization background or finding interesting things here.",
            "And this is indeed the case if you now take a look at the control field, you're finding the.",
            "The nature of things that people are looking at it, especially if we started thinking about distributed systems or network control boundaries, are.",
            "Thankfully, shattering early on I would say the major barrier that existed for a long time was continuous mathematics versus discrete mathematics, and the fact that for lots of the problems control people used to spend and still spend a lot of time on the model they were looking typically given was based on physics, so there was a physical model rather than something they completely had to infer.",
            "On the other hand, they do a lot of work.",
            "There's a lot of work in model reduction, which you can think of is approximate modeling, so there are places where their ties.",
            "And you know another reason is just the fact of how many meetings can you attend the IEEE Conference on Decision Control.",
            "Starts day after tomorrow, so so in fact there are issues that come up just because of the timing of meetings.",
            "My personal view is and it's something that MIT we're now.",
            "Whether you like our new status, enter if you know it or not.",
            "One of the nice things that didn't put the control and optimization people in the same building with all of the computer scientists so that boundary is being shattered.",
            "And I think that's exceedingly important because their imaginary."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, it's pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "But I just got it pretty much quite right.",
                    "label": 0
                },
                {
                    "sent": "Little bit of personal history.",
                    "label": 0
                },
                {
                    "sent": "My PhD is actually in systems and control, which I worked for.",
                    "label": 0
                },
                {
                    "sent": "A number of years, although I think it's fair to say I've been out of control for the last three decades and migrated from there into doing things in signal processing.",
                    "label": 0
                },
                {
                    "sent": "Statistical signal processing and then into things which overlap with NIPS.",
                    "label": 0
                },
                {
                    "sent": "Now the reason I switch fields is is conjectural.",
                    "label": 0
                },
                {
                    "sent": "Even to me, part of it might be that I have a short attention span and each change fields part of it might be that I get ticked off at each of the fields I'm in because of the narrow perspectives.",
                    "label": 0
                },
                {
                    "sent": "They have an when I end up giving lectures in those fields to tell him how ticked off they are.",
                    "label": 0
                },
                {
                    "sent": "So you can infer from that that maybe I'm ticked off at machine learning.",
                    "label": 1
                },
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "But my big message is the fact that each of the fields offers something to the other.",
                    "label": 0
                },
                {
                    "sent": "So just as I give talks here where I may be talking bout his distance and control statistical signal processing in numerical linear algebra have something to do for machine learning.",
                    "label": 0
                },
                {
                    "sent": "I also give talks to point out where machine learning has an awful lot to save for systems and control in signal processing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so I I really ended up getting into graphical models, but by complete accident I had a Sabbatical 1988 at INRIA in France.",
                    "label": 0
                },
                {
                    "sent": "And if you don't know Albert Benvenisti should he's one of the greatest signal processors, control theorists and computer scientists I know working in a wide variety of fields, and in 1988 it sabbatical there.",
                    "label": 0
                },
                {
                    "sent": "And this is just about the time wavelets.",
                    "label": 0
                },
                {
                    "sent": "We're becoming hot and wavelets were being proposed as something that could talk through things such as create better malt whiskies and possibly solve the structural debt problem in the United States.",
                    "label": 0
                },
                {
                    "sent": "The claims were outrageous and what albair poses a problem was could we come up with?",
                    "label": 0
                },
                {
                    "sent": "A principled way of saying why they were good and where they were useful?",
                    "label": 0
                },
                {
                    "sent": "For now, one of the problems I had with signal processing, which is no longer true to be fair, is the fact that it's primarily analysis based.",
                    "label": 0
                },
                {
                    "sent": "That is, I mean, I hate the word signal processing.",
                    "label": 0
                },
                {
                    "sent": "Signal processor sounds like something cuisinarts should make my rather think about a signal, then then then process it.",
                    "label": 1
                },
                {
                    "sent": "So they were basically analyzing signals they weren't modeling them.",
                    "label": 0
                },
                {
                    "sent": "So something that's common to systems and control.",
                    "label": 0
                },
                {
                    "sent": "Anti Machine Learning is the fact that you think about using models and using those is the basis for deducing things and so wavelets, if you're familiar with them or something that do of course fine to coarse decomposition of signals.",
                    "label": 0
                },
                {
                    "sent": "They also had the analysis form, that's the way they were being mostly applied, but there's a synthesis form which says how you create a fine scale signal by taking coarser versions and adding detail.",
                    "label": 0
                },
                {
                    "sent": "And we took the picture of single G. Let's think about that as the basis for statistical models.",
                    "label": 0
                },
                {
                    "sent": "Signals and this led us to thinking about something we didn't realize.",
                    "label": 1
                },
                {
                    "sent": "We were doing graphical models which were models multiresolution models on.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trees and so this is a slide that really was aimed at signal processors.",
                    "label": 0
                },
                {
                    "sent": "And a lot of it will resonate through the bottom, gives a picture of the applications that drove our work in the early part they were all what are typically referred to as Markov random field problems.",
                    "label": 0
                },
                {
                    "sent": "So the graphical model problems, but they're huge.",
                    "label": 0
                },
                {
                    "sent": "For example, the running over there talking about the ocean.",
                    "label": 0
                },
                {
                    "sent": "Oceanographers have a problem that you can think of as being a 300 million dimensional time series that you're trying to solve over a period of 1000 years, and that's a typical signal processing problem.",
                    "label": 0
                },
                {
                    "sent": "They have their gargantuan.",
                    "label": 0
                },
                {
                    "sent": "And the things that we that we want to try to capture was the fact that in lots of the applications that specially ones that we were aware of at that point in interested in the phenomenon, might be multi resolution.",
                    "label": 1
                },
                {
                    "sent": "The ocean, for example, has scales, spatial scales ranging from kilometers to thousands of kilometers.",
                    "label": 0
                },
                {
                    "sent": "It is time scales.",
                    "label": 0
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "This is taking out waves and tides that have time scales from the order of days to the order of 1000 years.",
                    "label": 0
                },
                {
                    "sent": "And so there are a huge number of timescales built in.",
                    "label": 0
                },
                {
                    "sent": "And the idea of using a flat model, which is what Navier Stokes equation gives you, which is what the physicists would want, is something that any of us who think about building machine learning models would say with much better try to use use hierarchical models, whether or not the phenomenon is multi resolution today to maybe so in the oceanography context.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about another one later.",
                    "label": 0
                },
                {
                    "sent": "You have data, those those are satellite tracks you see over there which provide variations measurements of sea height variations.",
                    "label": 0
                },
                {
                    "sent": "They're trying to estimate sea level C. See, the sea is not level dynamic range of sea levels about a meter over the Earth.",
                    "label": 0
                },
                {
                    "sent": "That's why we have currents.",
                    "label": 0
                },
                {
                    "sent": "If it were, if in fact it changed by by only 20 centimeters, Gulfstream would stop.",
                    "label": 0
                },
                {
                    "sent": "So in fact understanding these things is incredibly important, but they have data like this and they also have ships that go around and then they have ocean acoustic tomography.",
                    "label": 0
                },
                {
                    "sent": "Very different scales.",
                    "label": 0
                },
                {
                    "sent": "Another thing is whether or not the phenomena or data are multi resolution.",
                    "label": 1
                },
                {
                    "sent": "The objectives maybe so thinking about building models for discrimination purposes rather than generative models.",
                    "label": 0
                },
                {
                    "sent": "Something physicists, for example don't think about, but those of us in estimation and machine learning do.",
                    "label": 0
                },
                {
                    "sent": "And finally, whether any of those are true.",
                    "label": 0
                },
                {
                    "sent": "The algorithms you may want to be multi resolution now we've got a couple of talks and I'll talk a little bit about it using the idea that coarser scale versions of a problem solving them maybe easier can guide finer scale versions.",
                    "label": 1
                },
                {
                    "sent": "This is a picture of some applications.",
                    "label": 0
                },
                {
                    "sent": "Summerworks been picked up by people who work in all sorts of other fields.",
                    "label": 0
                },
                {
                    "sent": "I have no idea what helioseismology is, but I know that that our software is being used.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There I should say that I'm probably going to missed mentioning the cadre of students who've worked on these things as they go along, just because the fact that forgotten most of their names.",
                    "label": 0
                },
                {
                    "sent": "But there are great many students who contributed to all of these things, so we started to begin working on multi resolution models on directly trees leading leading to models that, for example, a linear model might look like this where.",
                    "label": 0
                },
                {
                    "sent": "A finer scale version is some interpolation of a coarser scale clarinet plus white noise.",
                    "label": 0
                },
                {
                    "sent": "So if those of us come from control, this is something we think about as as a very natural model aside point there, for those of you familiar with the French School in particular, things such as seminar Bourbaki where results are fairly mystical, we want to introduce something that was equivalent to shifts like in time series like Z transforms, and we put a gamma here for the shift going up from a from a fine too of course or level.",
                    "label": 0
                },
                {
                    "sent": "I went away in a trip to Alsace and came back and Albair had changed it to putting a bar over the gamma and I asked him why he was working at the computer and he turned to me and said because it is the only choice and then went back to working so it was a typical something I've learned from my interactions with the inscrutable French.",
                    "label": 0
                },
                {
                    "sent": "There are variety of other ones.",
                    "label": 0
                },
                {
                    "sent": "This one I want to mention just for a minute 'cause it's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "This is an approach that using computer graphics, but there's a precise way of thinking about constructing set things such as Brownian motion.",
                    "label": 1
                },
                {
                    "sent": "So Brownian motions, a Markov process.",
                    "label": 0
                },
                {
                    "sent": "And suppose I have the values at two endpoints of an interval.",
                    "label": 0
                },
                {
                    "sent": "Well, if I draw the straight line between them, that gives me the best estimate at every point given those those two points.",
                    "label": 0
                },
                {
                    "sent": "In addition, I can compute the variance of the error in that and so G at the midpoint.",
                    "label": 0
                },
                {
                    "sent": "I can add an independent random variable that deflects that, so they now have three points that correspond to samples on.",
                    "label": 0
                },
                {
                    "sent": "That Brownian motion, well, Gee, now they've got three.",
                    "label": 0
                },
                {
                    "sent": "Since Marco process, I can separately think about doing each of the half domains separately.",
                    "label": 1
                },
                {
                    "sent": "I'm going to create a tree.",
                    "label": 0
                },
                {
                    "sent": "An important point think point here, which is something you don't see that much of machine learning is, even though the field that you want to get the fine level is scalar.",
                    "label": 0
                },
                {
                    "sent": "The values I'm going to put it coarser nodes, maybe vectors, and a critical thing is understanding what the dimension of them is that something that people in control spend a lot of time on, and I'll make that point a couple of times.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me talk a bit about the control theorist view of things so you know we've got a tree, so machine learning people not that easy to think about building algorithms to solve these the way that that control theorists would think about it, is one that brings in things that we know we algorithm that does fine to coarse sweep of your data.",
                    "label": 1
                },
                {
                    "sent": "It's basically at every point computing the best estimate given all the data at that node and in the subtree below it.",
                    "label": 0
                },
                {
                    "sent": "It's a generalization of the Kalman filter.",
                    "label": 1
                },
                {
                    "sent": "It leads to generalization.",
                    "label": 0
                },
                {
                    "sent": "Of of Ricardi equations.",
                    "label": 0
                },
                {
                    "sent": "For those of you familiar with how you calculate error variances in state space models, and I want to mention a word here about stability results, 'cause this will come in early 'cause this is something you don't see discussed in machine machine learning something that people in control worry about a lot in terms of error propagation are the error.",
                    "label": 0
                },
                {
                    "sent": "Suppose I'm making error, I have my models a little bit off.",
                    "label": 0
                },
                {
                    "sent": "I round off or something like that.",
                    "label": 0
                },
                {
                    "sent": "Are those errors going to propagate so that my estimate diverges?",
                    "label": 0
                },
                {
                    "sent": "And there are two concepts that are used in control to show that won't happen.",
                    "label": 0
                },
                {
                    "sent": "One of them is the notion of control ability, which is usually thought of as G. Can I take my system from some point by the controls I apply, go to another point.",
                    "label": 0
                },
                {
                    "sent": "Now that may be something where I can't do it instantaneously.",
                    "label": 0
                },
                {
                    "sent": "That is, when you drive your car, you don't instantaneously get from your home to your office.",
                    "label": 0
                },
                {
                    "sent": "All you're doing is controlling acceleration and using the fact that the dynamics, the linkage to other variables propagates that into position differences.",
                    "label": 0
                },
                {
                    "sent": "So the idea that so control ability here is the fact that in this context is the fact that there's enough uncertainty as they propagate forward, so that hopefully the errors that I made earlier are insignificant to their diffused out by the effects of additional noise difficulty there is, if the system is unstable, those initial errors can grow something people machine learning don't typically talk about, but there are graphical models that if you think of them, is getting arbitrarily large, in fact, are unstable, very good.",
                    "label": 0
                },
                {
                    "sent": "One to point out.",
                    "label": 0
                },
                {
                    "sent": "In the context of Markov random fields, something known as the thin plate model.",
                    "label": 1
                },
                {
                    "sent": "So in fact understanding how errors can be bound is important and will come back to that a little bit later, so that's one place where control brings something that I have not seen in this field, and there's a downward, by the way, from this perspective, as many of you know, all your doing is solving a set of linear equations by doing Gaussian elimination back substitution.",
                    "label": 0
                },
                {
                    "sent": "That's what the Kalman filter in the rough country will smoother are, and because in fact it's a tree, there's no fill when you do the Gaussian elimination, and that's the.",
                    "label": 0
                },
                {
                    "sent": "Important fact.",
                    "label": 0
                },
                {
                    "sent": "OK, one of the things we spend a lot of time on in control theory.",
                    "label": 0
                },
                {
                    "sent": "It's not really a machine learning problem, it's more of an algebraic approach.",
                    "label": 0
                },
                {
                    "sent": "So actually if you go back to you to pearls book, you'll see some things that are quite algebraic as well, and the idea here is if I give you the 2nd order statistics of the process, you might have gotten those from data, but if I give you the 2nd order statistics, can I build a model of that state space model, white noise driving a dynamic system in the big problem then is a can I be?",
                    "label": 0
                },
                {
                    "sent": "What is the dimension of that state?",
                    "label": 0
                },
                {
                    "sent": "Focuses on something very different in typical typical control problems, the graph structure is fixed, it's time.",
                    "label": 1
                },
                {
                    "sent": "And what the real problem is, is what are the dimensions of the variables are going to put here?",
                    "label": 0
                },
                {
                    "sent": "Here we're going to talk about something our first work in this area is where again the tree structure specified.",
                    "label": 0
                },
                {
                    "sent": "So we've given you the nodal structure on this, but the challenge is to understand the dimensions.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting open question here, which is one that we're spending time thinking about.",
                    "label": 0
                },
                {
                    "sent": "What if I don't know the structure of the dimensions?",
                    "label": 0
                },
                {
                    "sent": "It's actually very ill posed problem, But they're interesting questions.",
                    "label": 0
                },
                {
                    "sent": "Try to understand how to solve that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so so this is a concept you also do not see in machine learning, and it comes from a very deep set of results in systems and control that stretch their way back to Norbert Wiener and two spectral factorization.",
                    "label": 0
                },
                {
                    "sent": "And the picture is the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose I give you the 2nd order statistics of a stochastic process in time.",
                    "label": 0
                },
                {
                    "sent": "I want to build a state space model and I want to get one that's at the lowest dimension possible.",
                    "label": 0
                },
                {
                    "sent": "What's pretty interesting is you can build a model that is invertible that is the white noise that's driving it is actually a function of the observed variables that you get out of this.",
                    "label": 0
                },
                {
                    "sent": "Something called the innovations representation, which is related to the Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "It does not build in any extrinsic uncertainty into it, so the state variables aren't hidden there.",
                    "label": 0
                },
                {
                    "sent": "In fact, functionals of your observations are very deep fact and there are interesting things that come out of this in terms of building models that have that property.",
                    "label": 0
                },
                {
                    "sent": "That have the minimal covariance that have have it, and those are models that are typically defined going forward in time and ones that have the largest covariance going backwards in time and that.",
                    "label": 0
                },
                {
                    "sent": "Interesting, interesting context for those, those are often referred to there.",
                    "label": 0
                },
                {
                    "sent": "Often a class of models referred to as.",
                    "label": 0
                },
                {
                    "sent": "Conjugate models, ones whose covariances are inverses of each other, and something that in control, is studied a lot and turns the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'll get back to that in the context of machine learning, so it's an internal model in this context, so we've got a fixed rate.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the simplest problems with thinking that all the data.",
                    "label": 0
                },
                {
                    "sent": "That we have data or the variables of primary interest with the finals finals scales what we're going to do at each node of the tree is we want that variable to be internal.",
                    "label": 0
                },
                {
                    "sent": "We want it to be a deterministic function of the variables at the finest scale.",
                    "label": 1
                },
                {
                    "sent": "So this is a very different notion that you typically see in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Some of those functions might be given, they might correspond to measurements.",
                    "label": 0
                },
                {
                    "sent": "You take.",
                    "label": 1
                },
                {
                    "sent": "Some of them correspond to variables you want to estimate.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example of that.",
                    "label": 0
                },
                {
                    "sent": "The rest are to be chosen or designed.",
                    "label": 1
                },
                {
                    "sent": "And the methods that you end up using.",
                    "label": 0
                },
                {
                    "sent": "Typically in this context are linear algebraic.",
                    "label": 0
                },
                {
                    "sent": "They involve things such as Canonical correlations, basically doing singular value decomposition to variety of problems, and so the idea here is typically you're not going to be able to get exact models, you want to get ones that are approximate, and you do this by used like basically clipping off the small singular values in these in something that these models what you want to think of is the state is nothing more than the projection of the future on the past.",
                    "label": 0
                },
                {
                    "sent": "There's an analog here, and all you're trying to do is to find the variables that provide basically a low dimensional approximation to that projection.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, there's a nice scale recursive algebraic design for these who actually build these models from fine to coarse, and there's an alternative using wavelets, which we came back to will mention a bit.",
                    "label": 0
                },
                {
                    "sent": "One of the things that that when I first saw it drove me nuts, I mean.",
                    "label": 0
                },
                {
                    "sent": "The kind series case what is known, and so all the nodes are are not on the same scale and none of them are hidden.",
                    "label": 0
                },
                {
                    "sent": "What's known is if you can find a state space model, the minimal one will in fact be internal ones that are minimal.",
                    "label": 0
                },
                {
                    "sent": "Not true for my hidden models, not true.",
                    "label": 0
                },
                {
                    "sent": "Here's a model which basically says look my my my my child is going to be equal to my parent plus noise.",
                    "label": 0
                },
                {
                    "sent": "So think about two points down here that have the same parent.",
                    "label": 0
                },
                {
                    "sent": "They have independent noise is given to him so I have X here.",
                    "label": 0
                },
                {
                    "sent": "Here I have X + W on X + W two give you X + W one and if I give you X + W two you cannot exactly construct X.",
                    "label": 0
                },
                {
                    "sent": "So you have something where minimal models aren't aren't necessarily internal.",
                    "label": 0
                },
                {
                    "sent": "What's interesting is if in fact you only have measurements at that finescale.",
                    "label": 0
                },
                {
                    "sent": "Your estimate is only of the internal part, so this is something that there's some actually very deep connections to.",
                    "label": 0
                },
                {
                    "sent": "The fact that you have these hidden variables in the structure of these problems.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is something before we knew anything about graphical models was trees were great, but we knew that there were limitations to them.",
                    "label": 0
                },
                {
                    "sent": "So again question what to do.",
                    "label": 0
                },
                {
                    "sent": "The first thing we did was to flaunt it.",
                    "label": 0
                },
                {
                    "sent": "We looked at problems in which the real objective was at a much coarser scale where in fact the problem here for us.",
                    "label": 0
                },
                {
                    "sent": "If we were thinking about spatial models or temporal models with fine scale variables have physical distances, you have two nodes that are close together, but they're far apart on the tree.",
                    "label": 0
                },
                {
                    "sent": "And so the variable that at their common at least common ancestor has to do the job of completely decorrelating these guys, and that's the problem with this.",
                    "label": 0
                },
                {
                    "sent": "Well, if your objectives at a high scale, who cares if you've got it right, and that's something that in some fields, such as physics there horrified by we built models where we don't conserve mass at horrify.",
                    "label": 0
                },
                {
                    "sent": "Some, but they aren't they from the data they have.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do that for the objectives that are interested in.",
                    "label": 0
                },
                {
                    "sent": "Oftentimes I kid my oceanography friends is saying that they are only interested in one bit.",
                    "label": 0
                },
                {
                    "sent": "Is it getting warmer or isn't it?",
                    "label": 0
                },
                {
                    "sent": "The second thing we did, and I'll talk a little bit about one aspect of this is is to try to beat the dealer.",
                    "label": 0
                },
                {
                    "sent": "This is cheating, just moving the trees around to get that.",
                    "label": 0
                },
                {
                    "sent": "There's a theory theoretically precise way of doing it, which is to say look.",
                    "label": 0
                },
                {
                    "sent": "I'm going to build a tree where each node corresponds to a region in space, but those regions overlap, so when I get to the finest level, I actually have multiple nodes that correspond to the same physical location that you can think of generative model, something that generates this way, and then you project by taking a convex combination of those values.",
                    "label": 0
                },
                {
                    "sent": "To get a value point which is very interesting is you get an exact estimation problem that says take your measurements.",
                    "label": 0
                },
                {
                    "sent": "If they're only the finest scale, put those same measurement values at all the nodes that are replicas of this thing, but put them in so the total information content of them is equal to the original information content.",
                    "label": 0
                },
                {
                    "sent": "Basically, you do something that says that if I take a look at the inverses of noise variances, I'm going to put on these some of the inverses of those noise variances equals the original noise variance.",
                    "label": 0
                },
                {
                    "sent": "You then run something on this overlap tree as we call it, and then you project back down.",
                    "label": 0
                },
                {
                    "sent": "And it's a way to reconstruct the optimal estimate.",
                    "label": 0
                },
                {
                    "sent": "A third one I'll talk about briefly is wavelet San finally, and this is about the point in time where I started interacting with Mike Jordan when he was still at MIT.",
                    "label": 0
                },
                {
                    "sent": "We said, OK, let's start putting some loops and I'll talk, and that's where most of the talk will end up going.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was a really interesting example there.",
                    "label": 0
                },
                {
                    "sent": "Actually some interesting openings by the way.",
                    "label": 0
                },
                {
                    "sent": "Control does a lot more than linear things.",
                    "label": 0
                },
                {
                    "sent": "Ann and Kalman filtering.",
                    "label": 0
                },
                {
                    "sent": "But I tended Zupan Ghahramani stock at the Sam Roy session is open.",
                    "label": 0
                },
                {
                    "sent": "Took a course for me.",
                    "label": 0
                },
                {
                    "sent": "I think if you listen to him control consisted of doing factor analysis, changed overtime and with Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "And over the extended Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "Well that's true.",
                    "label": 0
                },
                {
                    "sent": "We did the extended Kalman filter in 1970.",
                    "label": 0
                },
                {
                    "sent": "There's a lot more to the field, so this is an interesting problem.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting problem, there's sort of a couple of context for it.",
                    "label": 0
                },
                {
                    "sent": "We were motivated here by problem related, somethings place called Yucca Mountain, and so I'm not sure if you're familiar with Yucca Mountain.",
                    "label": 0
                },
                {
                    "sent": "It's where there's all these thoughts about bout storing nuclear waste.",
                    "label": 0
                },
                {
                    "sent": "There's a huge body of salt there.",
                    "label": 0
                },
                {
                    "sent": "Absolutely huge, very stable, and so it's actually a perfect place to put this.",
                    "label": 0
                },
                {
                    "sent": "But the question was things.",
                    "label": 0
                },
                {
                    "sent": "Things do sleep and what you want to make sure the question was.",
                    "label": 0
                },
                {
                    "sent": "Did the United States have to buy a piece of land?",
                    "label": 0
                },
                {
                    "sent": "The size of Rhode Island or they have to buy a piece of land.",
                    "label": 0
                },
                {
                    "sent": "The size of Georgia so that the expected time by which any of the radioactive material would escape from that region was greater than the half life of the radioactive material.",
                    "label": 0
                },
                {
                    "sent": "And So what you're interested in doing is estimating a very global property.",
                    "label": 0
                },
                {
                    "sent": "What's the escape time from a region?",
                    "label": 0
                },
                {
                    "sent": "What kind of measurements do they have?",
                    "label": 0
                },
                {
                    "sent": "Well, the drill drill and number of holes and what are they measuring their their measuring hydraulic conductivity?",
                    "label": 0
                },
                {
                    "sent": "It's the analog of electrical connectivity, and then they'll do something that if your electrical engineer you think it's a 2 port tester in multi port test, they'll pump at one and watch the drawdown of water on the other.",
                    "label": 0
                },
                {
                    "sent": "What's that doing?",
                    "label": 0
                },
                {
                    "sent": "It's giving him a nonlinear measurement of some average value.",
                    "label": 0
                },
                {
                    "sent": "Over a region.",
                    "label": 0
                },
                {
                    "sent": "OK, so they have very different scale measurements and their objective is very coarse scales.",
                    "label": 0
                },
                {
                    "sent": "One thing they want to know what the expected time to 222 part is.",
                    "label": 0
                },
                {
                    "sent": "So what what do we do?",
                    "label": 1
                },
                {
                    "sent": "We said we were going to build a model where at the finest level are going to be the hydraulic conductivities.",
                    "label": 0
                },
                {
                    "sent": "We're actually not really interested in those were going to higher level that those nonlinear those measurements of these more aggregate things and at the top of the tree we're going to have our variable corresponding to this escape time well.",
                    "label": 0
                },
                {
                    "sent": "I should tell you the way people do this in practice, because this problem also comes up in oil exploration and there will this take these measurements and they'll do conditional simulation.",
                    "label": 0
                },
                {
                    "sent": "The pride of open entire conductivity field of huge area, the finest scale.",
                    "label": 0
                },
                {
                    "sent": "It's too big for their simulators of water and oil flow, so they have to scale that up.",
                    "label": 0
                },
                {
                    "sent": "That takes them several days to do and then they have to run their simulator by the end of this that exhausted, they've done 1 sample run.",
                    "label": 0
                },
                {
                    "sent": "They go to the bank and say see there's oil here.",
                    "label": 0
                },
                {
                    "sent": "Can you give us $50,000,000 to infect adults field?",
                    "label": 0
                },
                {
                    "sent": "It's almost like that these problems are huge problems.",
                    "label": 0
                },
                {
                    "sent": "In our case, we're going to run out and was up and down.",
                    "label": 0
                },
                {
                    "sent": "The tree gives you estimates and variances and you're done.",
                    "label": 0
                },
                {
                    "sent": "And what this shows is a comparison in two cases and whether this is the case where linearization works, you have small variations.",
                    "label": 0
                },
                {
                    "sent": "This is something you get trivially.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to get.",
                    "label": 0
                },
                {
                    "sent": "This is running their algorithms, it does pretty good fit now when things aren't linear, you don't get nearly as good if it.",
                    "label": 0
                },
                {
                    "sent": "What does this say?",
                    "label": 0
                },
                {
                    "sent": "Gee, we probably want to use nonlinear models, not only your functionals that sit on these things.",
                    "label": 0
                },
                {
                    "sent": "And maybe we want to start using things for algorithms we're not going to use extended Kalman filters, but we may use particle filters and may end up using things like nonparametric belief propagation that Eric and Alex Tyler developed in in their work.",
                    "label": 0
                },
                {
                    "sent": "So that's what this says is is we're certainly can do it and what we were talking with, Steve.",
                    "label": 0
                },
                {
                    "sent": "We want the the the people who work in this field to use their forward models to give us samples so that we can learn from those samples what the right models are for our objective.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk a bit about wavelets and so I'll talk initially about the simplest way, which is the hard wavelet and the hard way that you want to think of it.",
                    "label": 0
                },
                {
                    "sent": "Each resolution I have a signal, and from that I'm going to create a course and find version.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take pairs of values, nonoverlapping pairs of values.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take their difference that's going to give me the detail at that scale.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to take their sums because I'm taking pairwise, just not overly pairwise.",
                    "label": 0
                },
                {
                    "sent": "I've gotta signals got half as many points.",
                    "label": 0
                },
                {
                    "sent": "That goes up.",
                    "label": 0
                },
                {
                    "sent": "The synthesis then is G. If I've got a core scale variable, you tell me what the detail is.",
                    "label": 0
                },
                {
                    "sent": "The difference of the fine scale values at this scale I'm going to add it to one side and subtract it from the other so it's a dynamic system.",
                    "label": 0
                },
                {
                    "sent": "This is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "Their two children in each of those children are going to be equal to the parent value, apparent average ones.",
                    "label": 0
                },
                {
                    "sent": "Adding the difference in the other, subtracting it, it's not a Markov model, because those two WSR negatives of each other.",
                    "label": 1
                },
                {
                    "sent": "What would what would a control person do in this case?",
                    "label": 0
                },
                {
                    "sent": "Well, Gee, if I've got a model where I've got position as a state variable.",
                    "label": 0
                },
                {
                    "sent": "I might want to augment it with velocities of random variable to higher order model.",
                    "label": 0
                },
                {
                    "sent": "So here you end up doing state augmentation.",
                    "label": 0
                },
                {
                    "sent": "You actually have a model which says the state at each node consists of both the.",
                    "label": 1
                },
                {
                    "sent": "Average value and that difference in values.",
                    "label": 0
                },
                {
                    "sent": "They want to think about doing the dynamics.",
                    "label": 0
                },
                {
                    "sent": "Of course, define this.",
                    "label": 0
                },
                {
                    "sent": "Now is a deterministic relationship because both of these are in the state at that finer node.",
                    "label": 0
                },
                {
                    "sent": "The stochastic part is I have to predict what the detail is going to be the next place next level based on what the statistics are of my fine process.",
                    "label": 0
                },
                {
                    "sent": "So one of the things this brings up is the fact in lots of models that come up in these contexts, the potentials you'll get between your graphical model where the variables that the nodes are vectors will be degenerate.",
                    "label": 0
                },
                {
                    "sent": "They won't be full rank.",
                    "label": 0
                },
                {
                    "sent": "Something you typically don't see in a lot of the things that people work on in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So, so the question is, the Heart Heart is very choppy.",
                    "label": 1
                },
                {
                    "sent": "Wavelet, there's a whole class of wavelets that in fact are much smoother than this, but but they overlap in terms of the values of the finescale signal that they've got.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it, if I want to get some the terministic going coarse to fine, I'm going to need to have more wavelet coefficients sitting up here.",
                    "label": 0
                },
                {
                    "sent": "But then we want the model to be internal.",
                    "label": 1
                },
                {
                    "sent": "We want the variable at their core scale.",
                    "label": 0
                },
                {
                    "sent": "Note to be the wavelet coefficient or the scaling coefficient finescale signal that actually means adding additional coefficients to the state.",
                    "label": 0
                },
                {
                    "sent": "Now you might say, well, Gee, that means doesn't go on forever, they need to start adding more and more things.",
                    "label": 0
                },
                {
                    "sent": "The answer is you don't.",
                    "label": 0
                },
                {
                    "sent": "So you end up with.",
                    "label": 0
                },
                {
                    "sent": "With a finite dimensional model, the variables are in fact finite.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can get pretty good estimates.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple toy example to model of process that that is multi scale fractional Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "These are noisy measurements of fractional Brownian motion where there was a gap in the data and what you're seeing on the right hand side are our estimates which come out of ours, which is the solid line and the optimal estimates which you can barely see.",
                    "label": 0
                },
                {
                    "sent": "You can see a little bit of here and the one Sigma bars for the optimal estimates, and since I think all in machine learning.",
                    "label": 0
                },
                {
                    "sent": "I agree that truth is a relative content concept.",
                    "label": 0
                },
                {
                    "sent": "Purely truly, generative models are not things we necessarily want.",
                    "label": 0
                },
                {
                    "sent": "I would argue that in fact this is something where our model is just as good as anybody who does.",
                    "label": 0
                },
                {
                    "sent": "Fractional Brownian motion come up with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk about ideas from numerical solution of partial differential equation that we started to use and the first one is something called nested dissection.",
                    "label": 1
                },
                {
                    "sent": "Graph you want to think about, sort of the prototypical one here that I'm talking about need not be.",
                    "label": 0
                },
                {
                    "sent": "This is one that's regular 2D grid.",
                    "label": 0
                },
                {
                    "sent": "And there are class of methods there that have been dealt for solving PDE, so I've got a PDA that basically is ax equals B&A has a structure with a non zero elements, corresponds to the edges in that graph is just take all the values along one column and March.",
                    "label": 0
                },
                {
                    "sent": "This way, in this way machine learning where people say fine you're creating a junction tree of some type out of these things, dimension is huge.",
                    "label": 0
                },
                {
                    "sent": "There are other approaches would save with nested dissection says.",
                    "label": 0
                },
                {
                    "sent": "Let me divide the region up so if I if I take the mid mid line condition on that these two are independent.",
                    "label": 0
                },
                {
                    "sent": "Well Gee, then I can do divide and conquer.",
                    "label": 1
                },
                {
                    "sent": "It's still the case that the variables at at the courses scale there.",
                    "label": 0
                },
                {
                    "sent": "The ones that sit at the course so I can build a tree or high dimensional very high dimensional, and so you're not going to be able to keep a mean.",
                    "label": 0
                },
                {
                    "sent": "You'll be able to keep it mean, but not a covariance matrix there, but you might be able to keep an approximation to the inverse covariance.",
                    "label": 0
                },
                {
                    "sent": "So for Gaussian models the inverse covariance is most here.",
                    "label": 0
                },
                {
                    "sent": "Assume no gives you the structure of the graphical model, you have an edge between nodes if and only if the corresponding element of the inverse covariance matrix is non 0.",
                    "label": 0
                },
                {
                    "sent": "And So what we looked at was something saying, let's start at the bottom rather than the top and do variable elimination.",
                    "label": 1
                },
                {
                    "sent": "And by the way, I was told I'm not allowed to talk about optimization here because there is optimization in another section.",
                    "label": 0
                },
                {
                    "sent": "So if after this you go and say, well, she sent some things about optimization, I will disavow it.",
                    "label": 0
                },
                {
                    "sent": "But here's a problem where there is optimization is actually part of an inference algorithm, and so better rather than go through this, let me let me do it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pictures so the picture here is the following.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start at the bottom scale.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start doing variable elimination, growing region so so I have something here and when I grow it because in fact it's not a tree.",
                    "label": 0
                },
                {
                    "sent": "I end up with fill in the model.",
                    "label": 0
                },
                {
                    "sent": "LG let me get this thing big enough so I can do.",
                    "label": 0
                },
                {
                    "sent": "Still do exacts smaller so I can still do exact statistical modeling on it and let me TH in that model.",
                    "label": 0
                },
                {
                    "sent": "Use your favorite method.",
                    "label": 0
                },
                {
                    "sent": "You could use a sparsity method Max entropy method to infect.",
                    "label": 0
                },
                {
                    "sent": "Try to thin it.",
                    "label": 0
                },
                {
                    "sent": "Typical, I projection in exponential families to come up with a thinned model.",
                    "label": 0
                },
                {
                    "sent": "This is the inverse of the covariance.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to do this at a whole bunch of different nodes and eventually these nodes are going to collide.",
                    "label": 0
                },
                {
                    "sent": "And then I have to do what I think of as inverse mitosis.",
                    "label": 0
                },
                {
                    "sent": "I've got to join them and eliminate those internal variables and thin and I get up to the top.",
                    "label": 0
                },
                {
                    "sent": "Now I've got something that's a blanket for the rest and I can do back substitution again with thinning as I come down.",
                    "label": 0
                },
                {
                    "sent": "So so this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing that we applied to reasonably large problems to get both both estimates and error variances.",
                    "label": 0
                },
                {
                    "sent": "You can see a little bit of Gulf Stream in the Atlantic.",
                    "label": 0
                },
                {
                    "sent": "It's not nearly as powerful as some of the currents that you see in the Pacific.",
                    "label": 0
                },
                {
                    "sent": "By the way, which also has things like the Gulf Stream, just the Pacific is very large.",
                    "label": 0
                },
                {
                    "sent": "And on the right you see the error variances, which is something they're quite interested in, and you can see the pattern you'd expect where the error variance is smaller where the satellite had happened to flying tape measures of these things, so this is I don't know a couple of million variables.",
                    "label": 0
                },
                {
                    "sent": "So what I've just described you, if I hadn't done thinning.",
                    "label": 0
                },
                {
                    "sent": "That would be a tree model on this nested dissected thing that goes up and down the tree.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly an optimal model on this, but I'm making approximations.",
                    "label": 0
                },
                {
                    "sent": "I'm putting in errors, so do those errors grow or not?",
                    "label": 0
                },
                {
                    "sent": "That's a control question, so the question is, is this graphical model controllable?",
                    "label": 0
                },
                {
                    "sent": "Well, what does that mean is, I think about going from a region to a somewhat larger region.",
                    "label": 0
                },
                {
                    "sent": "What it means is the fact that the potentials that connect them.",
                    "label": 0
                },
                {
                    "sent": "Over whatever that annulus is are weak enough so that in fact there's some diffusion that goes on.",
                    "label": 0
                },
                {
                    "sent": "Now, what what is observe ability mean?",
                    "label": 0
                },
                {
                    "sent": "Observe ability means that there are enough place, and so you want where you want to think about this is that if I put noise in an acceleration that will propagate into noise in position and that's the picture that we have here for observe ability, it's the opposite way.",
                    "label": 0
                },
                {
                    "sent": "If I have a measurement position, I confer overtime something about about acceleration and what does that correspond to these models?",
                    "label": 0
                },
                {
                    "sent": "That means I have measurements at enough nodes.",
                    "label": 0
                },
                {
                    "sent": "The single node potentials are strong enough at enough locations.",
                    "label": 0
                },
                {
                    "sent": "And the potentials that connect them to the variables of where I don't have have individual node node potentials.",
                    "label": 0
                },
                {
                    "sent": "No potentials here correspond to.",
                    "label": 0
                },
                {
                    "sent": "Measurements are strong enough so that I can get some inference from them.",
                    "label": 0
                },
                {
                    "sent": "And under this you can get a precise picture of the fact that fact that you can control the errors, even if you're in the thin plate model, which means that region gets larger the errors if you did not have observe ability measurements would grow.",
                    "label": 0
                },
                {
                    "sent": "It's also possible that you can iterate these things.",
                    "label": 0
                },
                {
                    "sent": "Use this as something called the preconditioner, and now we're going to talk about preconditioners, and I'll say a few words about those in something called the Richardson iteration.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let me turn to another topic related to walk sums and many of you may have seen some of these things about walks arms and this is work of Jason Johnson and Dmitry Malamute off and the latter part of think at Chandrasekharan.",
                    "label": 0
                },
                {
                    "sent": "So here we've got a large Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "The way these are specified for graphical models isn't Interment to meaning.",
                    "label": 0
                },
                {
                    "sent": "The covariance.",
                    "label": 0
                },
                {
                    "sent": "What you have are the graphical model structure which gives you potentials in the form of the precision matrix, the inverse of the covariance J.",
                    "label": 0
                },
                {
                    "sent": "And you have what control we would call the information state, which is the inverse of covariance times the mean.",
                    "label": 0
                },
                {
                    "sent": "That's basically what you get from data, and so the problem you want to be able to solve is given J&H and I sorry I switched from U to X hat.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to solve these equations if you want to get the estimate.",
                    "label": 0
                },
                {
                    "sent": "If you want to get the error variance is I want to get the diagonal elements of the inverse of the coherence.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's do something that is fairly simple minded, but it is something that gives us a fair amount of intuition is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume that that I've normalized my model and it's not difficult to do so that the diagonal elements of inverse covariance, or one that's just for simplicity and let me write J as I -- R. So R captures all of the off diagonal terms.",
                    "label": 0
                },
                {
                    "sent": "RIJ has very precise interpretation, is partial correlation coefficient.",
                    "label": 1
                },
                {
                    "sent": "It's the correlation between XI and XJ, conditioned on all of the other variables in the graph.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if if there's not an edge between those at 0.",
                    "label": 0
                },
                {
                    "sent": "But it gives you an idea of the strength of things.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose we did something not even trying to do this in verse, and this corresponds to doing some kind of Gauss Jacobi iteration in solving the equations just to power series.",
                    "label": 1
                },
                {
                    "sent": "Not saying this is the way you want to do it.",
                    "label": 0
                },
                {
                    "sent": "Let's think about what these matrices are.",
                    "label": 0
                },
                {
                    "sent": "Think about the IG element of R-squared.",
                    "label": 0
                },
                {
                    "sent": "Taking are multiplying by itself.",
                    "label": 1
                },
                {
                    "sent": "Well, that IG element is only going to be 990 if there is a path that is linked to that starts at.",
                    "label": 0
                },
                {
                    "sent": "I goes to some other node K and then goes to J.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to get the sum of all of those, so I'm going to get a sum of weighted walks around this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is starting to look a little bit of thinking like message passing.",
                    "label": 0
                },
                {
                    "sent": "There's actually information that's propagating inside of these things and so you can get sums over these weighted walks corresponding to these and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's fairly simple just looking at the equation to see if I wanted to get the Estee element of the covariance that corresponds to collecting all the walks that go from noticed and OT.",
                    "label": 0
                },
                {
                    "sent": "And if I want to get the mean, I wait.",
                    "label": 0
                },
                {
                    "sent": "I think the sum of those works waited by the corresponding potential vector potential value measurement.",
                    "label": 0
                },
                {
                    "sent": "The weighted measurement value at each of the nodes.",
                    "label": 1
                },
                {
                    "sent": "Alright, well different inference algorithms may compute walks, maybe not all of them, and we'll see that BP does not compute all of them.",
                    "label": 1
                },
                {
                    "sent": "At least there are loops in the graph, different message schedules.",
                    "label": 0
                },
                {
                    "sent": "What that means is, there's one class of models where you came with impunity.",
                    "label": 0
                },
                {
                    "sent": "Think about using any message schedule, and those are ones where these series are absolutely summable, doesn't matter the order in which I add these things up, and that corresponds take that our matrix and take the absolute value of every element.",
                    "label": 0
                },
                {
                    "sent": "In it.",
                    "label": 0
                },
                {
                    "sent": "And as we'll see that he builds the case that if you think about a -- R, the eigenvalues of our have to be less than one because I -- R has to be positive definite.",
                    "label": 0
                },
                {
                    "sent": "The eigenvalues of our bar might not be less than one.",
                    "label": 0
                },
                {
                    "sent": "But if the spectral radius if the largest eigenvalue of this model is less than one, you have walk some ability.",
                    "label": 1
                },
                {
                    "sent": "You can add up these things in any order that that you want.",
                    "label": 0
                },
                {
                    "sent": "Because of that, for walk some ability.",
                    "label": 0
                },
                {
                    "sent": "BP converges 'cause it calculates will see a calculation a number of these walks, but if it converges, it turns out it calculates all the walks for the calculating the means, and that's well known result.",
                    "label": 0
                },
                {
                    "sent": "BP convergence for Gaussian models get the means right, but it doesn't collect them all for the variances.",
                    "label": 1
                },
                {
                    "sent": "It misses some.",
                    "label": 0
                },
                {
                    "sent": "And that is easy as seen if you just look at the computation tree.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a simple graph an what I've written out is what the computation tree for three iterations of BP would look like from the perspective of node one.",
                    "label": 0
                },
                {
                    "sent": "So the first time no one is going to get measurements from these three guys.",
                    "label": 0
                },
                {
                    "sent": "The second time, well, if you think about the two iterations, well, no one would have sent the message to node 2, but this guy is smart enough not to send that back.",
                    "label": 0
                },
                {
                    "sent": "He's just going to take a message from node three, send it back.",
                    "label": 0
                },
                {
                    "sent": "Another time, and similarly from from no three, he's going to get 3 messages his first iteration, but he's just going to take these two and send it back.",
                    "label": 0
                },
                {
                    "sent": "And So what BP does after three iterations from the perspective of node one, is to complete inference on that tree.",
                    "label": 0
                },
                {
                    "sent": "That means in apparently stuck on something here.",
                    "label": 0
                },
                {
                    "sent": "That means that it is calculated the full walk sums corresponding to that tree.",
                    "label": 0
                },
                {
                    "sent": "That means that it's calculated for calculating the means from all of these different ones.",
                    "label": 0
                },
                {
                    "sent": "It messages propagate up from those appear.",
                    "label": 0
                },
                {
                    "sent": "This gives a very simple picture of why captures all the.",
                    "label": 0
                },
                {
                    "sent": "Ones for calculating the means but interesting.",
                    "label": 0
                },
                {
                    "sent": "Remember it's only the node at the top.",
                    "label": 0
                },
                {
                    "sent": "This is valid for in terms of where the capital should go or not and look at the end of the self return walks.",
                    "label": 0
                },
                {
                    "sent": "So if I want to get the diagonal elements PII, those who walks that started I and come back.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if I start at the top node and want to go, I can go 12321.",
                    "label": 0
                },
                {
                    "sent": "I can go 12132321 variety of things here and those are all in here but I cannot get the walk 1231.",
                    "label": 0
                },
                {
                    "sent": "It doesn't appear there.",
                    "label": 0
                },
                {
                    "sent": "It's not a self backtracking walk.",
                    "label": 0
                },
                {
                    "sent": "So BP only calculates backtracking walks.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, it goes through every one of these things twice even number of times.",
                    "label": 1
                },
                {
                    "sent": "What does that mean?",
                    "label": 1
                },
                {
                    "sent": "The sign of the of the element of our doesn't matter 'cause you're going to square it or quadruple it, so BP doesn't know if in fact the.",
                    "label": 0
                },
                {
                    "sent": "You have a attractive model, but all those things are positive or a frustrated model where some of them are negative and that's where it runs into trouble.",
                    "label": 0
                },
                {
                    "sent": "So that's what all of this says.",
                    "label": 0
                },
                {
                    "sent": "Alright, so for walks on mobile models things converge, but if it's not work, some of OBP may not converge and it's pretty easy to see can produce absolute nonsense.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that the model that it can create on the right may correspond to a information matrix that has negative eigenvalues, which is nuts doesn't mean anything.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a simple example where I put values on these things.",
                    "label": 0
                },
                {
                    "sent": "Or Dimitri put values on these things value row and it has one frustrated potential.",
                    "label": 0
                },
                {
                    "sent": "So these things would like to be positively correlated.",
                    "label": 0
                },
                {
                    "sent": "But somehow these won't be negatively correlated.",
                    "label": 0
                },
                {
                    "sent": "And look at where rows equal to .39.",
                    "label": 0
                },
                {
                    "sent": "This has to be less than one for work, some ability.",
                    "label": 0
                },
                {
                    "sent": "It is everything.",
                    "label": 0
                },
                {
                    "sent": "Bro Infinity here is is a theoretical calculation of our all of the computation trees.",
                    "label": 0
                },
                {
                    "sent": "Valid, valid graphical models.",
                    "label": 0
                },
                {
                    "sent": "If all of them are, BP typically will converge for the variances that may not converge for the means, but I'm not going through all of 'em well, I've just got to .4, and in fact this is a picture of what the variances are that BP computes this iteration.",
                    "label": 0
                },
                {
                    "sent": "It says the variance is minus 150.",
                    "label": 0
                },
                {
                    "sent": "Which is an example of the kind of catastrophic failure that BP has been.",
                    "label": 0
                },
                {
                    "sent": "'cause of the way it's trying to collect these works.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is some work that was done by.",
                    "label": 0
                },
                {
                    "sent": "A couple of the students in my group, Eric Sudderth and Martin Wainwright.",
                    "label": 0
                },
                {
                    "sent": "Not sure whatever happened to either of them.",
                    "label": 0
                },
                {
                    "sent": "But it was a very nice piece of work in which we've built, and it's fairly simple idea, and it's related to the notion of Richardson iterations, and so the idea I've got J and it's got this graphical structure.",
                    "label": 0
                },
                {
                    "sent": "And So what Eric Martin did first to say, let me think about this is one of one case.",
                    "label": 0
                },
                {
                    "sent": "You can do more general things that you can find in the paper that they wrote.",
                    "label": 0
                },
                {
                    "sent": "And in Eric's Masters thesis let me just cut some of those edges.",
                    "label": 0
                },
                {
                    "sent": "Where does that mean?",
                    "label": 0
                },
                {
                    "sent": "I'm going to take this metrics and divide it up into part that corresponds to a tractable sub graph when somebody, somewhere.",
                    "label": 0
                },
                {
                    "sent": "Willing to do exact inference and another piece I'm going to put that - there.",
                    "label": 0
                },
                {
                    "sent": "Remember I want to solve Jax hat is equal to H. What I'm going to do is bring this over to the other side and I've got an equation that looks like this and now I'm going to make it.",
                    "label": 0
                },
                {
                    "sent": "This is what's called Richardson iteration.",
                    "label": 1
                },
                {
                    "sent": "Going to solve this kind of problem, we're going to plug in my last estimate over here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Then do this iteratively if it converges.",
                    "label": 0
                },
                {
                    "sent": "In fact, you're going to get the right answer, and that corresponds to looking at the eigenvalues, DS, inverse times, KS for example.",
                    "label": 0
                },
                {
                    "sent": "More generally, and this is something that Eric and Martin found their work, you end up with with much better performance in many cases.",
                    "label": 0
                },
                {
                    "sent": "If you switch among different trees during the different iterations, you cut different edges, it can significantly outperform these things.",
                    "label": 1
                },
                {
                    "sent": "What Venkat did was to come in and take a look at why this case and how these things worked, when in fact you had walked summable models.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are variations on these things, So what I've just described here is it is a full Richardson iteration.",
                    "label": 0
                },
                {
                    "sent": "Their Gauss Seidel iterations, in which in each iteration you're only going to update some of the variables.",
                    "label": 0
                },
                {
                    "sent": "So typically people do in solving PDE's on a 2 dimensional grid still give us a checkerboard red, black where I'm at one point I update the red, keeping the black fixed, and then I iterate so only update some of the variables, so some of them stay fixed and I need to deal with the other deal with the others and update those.",
                    "label": 0
                },
                {
                    "sent": "And of course when I want.",
                    "label": 0
                },
                {
                    "sent": "Update just that a subset of variables.",
                    "label": 0
                },
                {
                    "sent": "There's a graph that sits there and I could do something iterative on that so you can miss these things.",
                    "label": 0
                },
                {
                    "sent": "If it works on mobile models, you're going to get the exact answers.",
                    "label": 0
                },
                {
                    "sent": "If you collect all of the walks and then why you want to think about this is at one iteration.",
                    "label": 0
                },
                {
                    "sent": "I've done all of the walks in one of these trees.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to do a calculation which hops across one of the edges.",
                    "label": 0
                },
                {
                    "sent": "I just that I cut.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to do something in a different tree, and so there's a very simple way of trying to see whether or not at some point every walk is in fact captured.",
                    "label": 0
                },
                {
                    "sent": "Very simple tests that can effect show that these algorithms will give you the right answer, but of course there's a question of what tree do you use?",
                    "label": 0
                },
                {
                    "sent": "An Venkat did some.",
                    "label": 0
                },
                {
                    "sent": "Did something sort of cute.",
                    "label": 0
                },
                {
                    "sent": "Done this and so.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at the error in our estimate.",
                    "label": 0
                },
                {
                    "sent": "So if we have an estimate at an EN, is the error in that?",
                    "label": 0
                },
                {
                    "sent": "Now obviously we'd like to do is to drive that error as make it as small as possible in the next iteration.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we don't know what X had is.",
                    "label": 0
                },
                {
                    "sent": "That's the whole point, so we can't get the error.",
                    "label": 0
                },
                {
                    "sent": "So we're going to look at what in system identification and control would be.",
                    "label": 0
                },
                {
                    "sent": "Look at equation error.",
                    "label": 0
                },
                {
                    "sent": "What we do know is we know what H is, and we know that.",
                    "label": 0
                },
                {
                    "sent": "This thing is supposed to be equal to that little.",
                    "label": 0
                },
                {
                    "sent": "Take a look at the difference in that.",
                    "label": 0
                },
                {
                    "sent": "That's the difference in the right hand side that we can get our hands on.",
                    "label": 0
                },
                {
                    "sent": "And it's fairly simple bound.",
                    "label": 0
                },
                {
                    "sent": "It says in fact the L1 error on the state can be can be there's a bar here 'cause there's a slight calculation you have to do in addition, But basically you're doing walk sums on the entire graph, something you don't want to touch minus the walk sums on whatever graph you choose to use next.",
                    "label": 0
                },
                {
                    "sent": "Now obviously you want to make this as small as possible.",
                    "label": 0
                },
                {
                    "sent": "This is whatever it is.",
                    "label": 0
                },
                {
                    "sent": "It's just in terms of full graph.",
                    "label": 0
                },
                {
                    "sent": "You want to make this this as large as you possibly can, so you have a problem which says you want to get the Max.",
                    "label": 1
                },
                {
                    "sent": "Walk some tree.",
                    "label": 0
                },
                {
                    "sent": "And that's a problem that starting to make things look tractable.",
                    "label": 0
                },
                {
                    "sent": "The problem is that that's not an easy problem, it's an empty hard problem to solve.",
                    "label": 0
                },
                {
                    "sent": "So Venkat did something similar, said look I want to put weights on the edges and say what I'm going to do is look at each edge.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at what the work some contribution is going to be to producing that error.",
                    "label": 0
                },
                {
                    "sent": "Just if I had that edge, so those are walks that go back and forth.",
                    "label": 0
                },
                {
                    "sent": "It is pretty simple to figure out what that is now.",
                    "label": 0
                },
                {
                    "sent": "I have an edge on each of.",
                    "label": 0
                },
                {
                    "sent": "Weight on each edge of the graph.",
                    "label": 0
                },
                {
                    "sent": "I do Max weight spanning tree to figure out what the best graph uses.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this shows the kind of performance you can get.",
                    "label": 0
                },
                {
                    "sent": "This is this is a log log scale here for how the error goes down.",
                    "label": 0
                },
                {
                    "sent": "This is using one tree.",
                    "label": 0
                },
                {
                    "sent": "This is using tools.",
                    "label": 0
                },
                {
                    "sent": "You get an exponential speedup and both of these ones Gauss Seidel, one to Max spanning tree, one or the types of speedups you can get by adaptively choosing these things.",
                    "label": 0
                },
                {
                    "sent": "So it's actually quite fast.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it may have implications in waste applying it in the PD context as well.",
                    "label": 0
                },
                {
                    "sent": "Where people do these kinds of things.",
                    "label": 0
                },
                {
                    "sent": "So graph decompositions in solving PZ is a very active area.",
                    "label": 0
                },
                {
                    "sent": "There's an awful lot of work if you read Siam Journal on numerical linear algebra.",
                    "label": 0
                },
                {
                    "sent": "We see lots of things about that.",
                    "label": 0
                },
                {
                    "sent": "Alright, there's an alternative approach that being glue just investigated in his Masters thesis, which this is the simple picture.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at the notion of feedback, vertex sets and somebody some of you may know what feedback vertex set is.",
                    "label": 1
                },
                {
                    "sent": "Feedback vertex Set is a set of nodes that if I eliminate them, I would have tree.",
                    "label": 0
                },
                {
                    "sent": "OK. And here's a picture where there's one node that in fact creates a tree.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have a feedback vertex and will come back to the fact that for graphs like 2D grids, feedback for assessor, very large, you can't deal with them.",
                    "label": 0
                },
                {
                    "sent": "What I can imagine wanting to try to do is to do Gaussian elimination on this remaining tree so the feedback vertex set, either one note or several, can then solve its own problem, and then when it's solved then I can spread things back and do tree calculations so you have a different message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's one where initially the feedback vertex nodes send messages here are basically say hey help me provide think about doing Gaussian elimination.",
                    "label": 0
                },
                {
                    "sent": "You're doing things where you're doing operations on the left hand side, ax equals B and on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And what the feedback vertex notice that needs both of those and each doesn't know something about what's happening to the things it needs to solve its problem.",
                    "label": 1
                },
                {
                    "sent": "It also needs to know something about what's going on in the rest.",
                    "label": 0
                },
                {
                    "sent": "The rest of the tree thinks so.",
                    "label": 0
                },
                {
                    "sent": "In fact, this note the BP messages here will be vector messages, 'cause some of them correspond using their own potentials.",
                    "label": 0
                },
                {
                    "sent": "Their own H is to calculate what they think are estimates the wrong because they haven't taken.",
                    "label": 0
                },
                {
                    "sent": "In fact, the fact that there are loops.",
                    "label": 0
                },
                {
                    "sent": "And also to calculate quantities that they will then feedback to this guy so or to the set of feedback vertex node since they can solve their problem exactly once that's done, there's another set of messages that get sent back out to the other nodes to do corrections and come up with the optimal estimates.",
                    "label": 0
                },
                {
                    "sent": "So this yields the optimal answer in this.",
                    "label": 0
                },
                {
                    "sent": "In fact, you've got a feedback vertex set for the graph.",
                    "label": 0
                },
                {
                    "sent": "Now there are obvious issues which I think.",
                    "label": 0
                },
                {
                    "sent": "Most of you are probably already thinking about, which has to do with the fact that feedback Vertex set can be large.",
                    "label": 0
                },
                {
                    "sent": "Complexity here is order K squared an where K is the cardinality of that feedback vertex set.",
                    "label": 0
                },
                {
                    "sent": "So it better be smaller.",
                    "label": 0
                },
                {
                    "sent": "K is on the order of nor the square root of N. You're in trouble 'cause this is N squared or N ^3.",
                    "label": 0
                },
                {
                    "sent": "So if K is too large, the idea is look.",
                    "label": 0
                },
                {
                    "sent": "I'm only going to choose a subset of the nodes that correspond to the break some of loops, and so I'm going to run this thing on the rest of the graph.",
                    "label": 0
                },
                {
                    "sent": "I could just do BP.",
                    "label": 0
                },
                {
                    "sent": "It's not going to give me exact answers if it converges, will give me the right means and it will in fact give me the right information for the feedback, vertex nodes.",
                    "label": 0
                },
                {
                    "sent": "Or maybe I run embedded trees or some other algorithm that does those things exactly.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And assuming you get convergence, and in fact now because of the fact you're limiting some loops, you can get convergence for non walk summable models fairly easilly.",
                    "label": 0
                },
                {
                    "sent": "You know give a picture of that, you always get even if you're doing approximate BP.",
                    "label": 0
                },
                {
                    "sent": "On the rest you always get the exact means and variances on those feedback nodes, you get the exact means everywhere and if you're doing BP on the remaining graph you get obvious approximations to the variances.",
                    "label": 1
                },
                {
                    "sent": "Basically, this approximate variances collect more walks than because they're getting some of the things that involve the loops that go through the feedback vertex nodes which are exactly the things that BP misses.",
                    "label": 0
                },
                {
                    "sent": "If you have attractive models, so everything you're adding is positive, you're getting better and better bounds pending on how many walks here.",
                    "label": 0
                },
                {
                    "sent": "In fact, collecting this how large the feedback vertex it is.",
                    "label": 0
                },
                {
                    "sent": "What you can also came with.",
                    "label": 0
                },
                {
                    "sent": "How do you choose which nodes, and there are two things you want to do is you'd like to be able to enhance convergence on the remaining graph, so it's not working, but we would like to get.",
                    "label": 0
                },
                {
                    "sent": "That that row bar that I put in the rope to be as small as possible.",
                    "label": 0
                },
                {
                    "sent": "You also want to be able to collect the most important walk, so there's a local calculation.",
                    "label": 0
                },
                {
                    "sent": "Come up with something that gives you an idea of which nodes to choose.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you get pretty good performance with on the order of log notes, so getting looked at at 80 by 80 grids with randomly chosen potentials, so it was not a regular process.",
                    "label": 0
                },
                {
                    "sent": "An example here where phenomenon walk summable model, so loopy BP is awful.",
                    "label": 0
                },
                {
                    "sent": "This is basically if you use something that corresponds to separators, but this is using on the order of nine nodes and this again is the log of the variance.",
                    "label": 0
                },
                {
                    "sent": "You're getting dramatic performance here.",
                    "label": 0
                },
                {
                    "sent": "We don't have proofs.",
                    "label": 0
                },
                {
                    "sent": "Of this sum results on on correlation decay.",
                    "label": 0
                },
                {
                    "sent": "I think will allow us to get proofs on the fact that in fact we can get complexity on the order of log squared N and log squared N. For these types of models.",
                    "label": 0
                },
                {
                    "sent": "And that's for calculating both means and the variance is.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to change gears a piece of work of Dmitri Melut offs and it's.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When I give you the part that makes it sound outrageous, I'm going to do, I'm going to solve the problem by making a low rank approximation to the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we got this inverse covariance and we'd like to get its inverse.",
                    "label": 0
                },
                {
                    "sent": "We'd like to get paid well, obviously, one way to do it is to solve this set of equations.",
                    "label": 0
                },
                {
                    "sent": "And I can think about doing that column.",
                    "label": 0
                },
                {
                    "sent": "Why so I can get the first column of P by solving J times the first, P is equal to Y.",
                    "label": 0
                },
                {
                    "sent": "One can do for all of that, so in fact.",
                    "label": 0
                },
                {
                    "sent": "Jay is sparse.",
                    "label": 0
                },
                {
                    "sent": "That's an order and calculation, but I got 10 of them so I have an N squared problem out of this, so it's infeasible for large graphs.",
                    "label": 1
                },
                {
                    "sent": "So what actually Jason Johnson and Dimitri said was, you know, maybe I can do something here where I'm going to do a low rank approximation of the identity.",
                    "label": 0
                },
                {
                    "sent": "So rather than having the identity here, I'm going to put a matrix that's low rank so it's going to be be transposed where B is going to be long and skinny, be transposed, therefore short and fat.",
                    "label": 0
                },
                {
                    "sent": "And that means the rows of B are redundant there.",
                    "label": 0
                },
                {
                    "sent": "They are not linearly independent there over complete.",
                    "label": 0
                },
                {
                    "sent": "And so I would like to be able to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "I'm going to claim that that gives me something good.",
                    "label": 0
                },
                {
                    "sent": "I mean this is not problem with the identity.",
                    "label": 0
                },
                {
                    "sent": "Would you actually solve is for solve this equation and then post multiply it.",
                    "label": 1
                },
                {
                    "sent": "Complexity is on the order of M * N where M is is basically the width of this matrix be that column dimension so that you'd like to be small.",
                    "label": 0
                },
                {
                    "sent": "Why does this make any sense?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the reason is, if you take a look at the approximate answer you've gotten compared to the exact one, it's equal to the exact one plus some interference terms which you can think of, either as doing the fact that we're splicing some things or aliasing some things out of this.",
                    "label": 0
                },
                {
                    "sent": "And if you take a look at those those terms, you can see how you might want to do things well.",
                    "label": 0
                },
                {
                    "sent": "Gee, suppose that.",
                    "label": 0
                },
                {
                    "sent": "And I don't know the value of PNG, but what if I assume that PIJ is reasonably large?",
                    "label": 0
                },
                {
                    "sent": "Well, I would like to take BI&BJ orthogonal to each other, so I get something that zero if this is near 0.",
                    "label": 0
                },
                {
                    "sent": "I don't care if they're orthogonal or not.",
                    "label": 1
                },
                {
                    "sent": "OK, but what I will do is I will in fact do random sign flips.",
                    "label": 1
                },
                {
                    "sent": "I'm actually had the rows of this be matrix be identical except that the orthogonal ones and then will be some identical ones, but with a random sign sign flip.",
                    "label": 0
                },
                {
                    "sent": "What that means is that for those by transpose BJ is either zero 'cause there are Fogle or it's random variable with zero mean and variance one 'cause it either takes a value plus one or minus one and that means this gives me an unbiased estimate of covariance.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as a graph coloring problem.",
                    "label": 1
                },
                {
                    "sent": "If I have correlation decay, you know if there's a point that's far enough out where the correlation with the point I'm interested in here is small.",
                    "label": 0
                },
                {
                    "sent": "I'm going to repeat the same vector but with the random sign flip, so I can think about taking a bunch of points and figuring out how to color it to in fact get good performance.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't work well if there's low correlation delay, and that says you don't want to think about taking the values at the individual nodes, and they want to think about taking linear combinations of them in particular, bases that do a fair amount of compression, such as wavelet bases.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here's the picture that goes on.",
                    "label": 0
                },
                {
                    "sent": "Here is the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Here's my spliced matrix, and so the different the two different colors, Gray or black, correspond to whether A plus one or minus one goes on.",
                    "label": 0
                },
                {
                    "sent": "So what I've done is I've repeated some of these things.",
                    "label": 0
                },
                {
                    "sent": "And for things that have correlation, decay mean you may be able to see there's a little bit of difference between the approximation in the true.",
                    "label": 0
                },
                {
                    "sent": "However, that does really badly.",
                    "label": 0
                },
                {
                    "sent": "In fact, if in fact you have long tails in your correlation, so maybe we use wavelet basis and we splice the wavelet basis and you can get dramatic improvements and performance that is even better.",
                    "label": 0
                },
                {
                    "sent": "You can't really see a difference between those two, and we don't have.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can get on using correlation decay.",
                    "label": 0
                },
                {
                    "sent": "We applied it to to a smaller version of that ocean data problem, so we had only a million variables in it an where our approximation to a million by million covariance matrix had ranked 448.",
                    "label": 0
                },
                {
                    "sent": "You got answers, they're indistinguishable crunch through to the exact answer.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk now about things where I know we're going to have other talks of multigrid, and I'll talk a bit about these.",
                    "label": 0
                },
                {
                    "sent": "So these are models that the number of people talked about looked at, which are pyramidal structures of 2D.",
                    "label": 0
                },
                {
                    "sent": "Grades are typical models we would like to get a student join or Masters thesis developed optimization methods to solve these.",
                    "label": 0
                },
                {
                    "sent": "I didn't say that.",
                    "label": 0
                },
                {
                    "sent": "But you can capture things with long range correlations, and you can also get things which motivated us to do something else which have a very nice property which actually people without piddies use for a different method.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about in a minute, which is the fact that conditioned on neighboring scales, the residual correlations within a scale tend to be sparse and local correlations do.",
                    "label": 1
                },
                {
                    "sent": "And so now you can think about doing algorithms.",
                    "label": 0
                },
                {
                    "sent": "Typical multigrid algorithms would do iterations at each iteration, starting at the coarser ones, and then propagate down and do additional additional iterations in calculations as they go down.",
                    "label": 1
                },
                {
                    "sent": "What Jim did was to use those adaptive embedded sub graph algorithms to figure out each iteration which was the best thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is an application anyone want to guess what?",
                    "label": 0
                },
                {
                    "sent": "That's a picture of?",
                    "label": 0
                },
                {
                    "sent": "It is the surface of a soft salt Dome several miles below the floor of the Gulf of Mexico.",
                    "label": 0
                },
                {
                    "sent": "As BP has unfortunately shown us to be true, there are huge deposits of oil very deep under the floor of the Gulf of Mexico.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons that they are there, they're trapped there, oils, very salt is very impermanent, permeable so so that the oil will trap.",
                    "label": 0
                },
                {
                    "sent": "Against it and underneath it.",
                    "label": 0
                },
                {
                    "sent": "A big problem for the seismic processing is salt doesn't have layers, so in fact the things they do to get the layering structure doesn't work very well unless they know where the top of the salt is.",
                    "label": 0
                },
                {
                    "sent": "So they have these guys who sit looking at the seismic data and they pick out points to create a surface, and what we were told we have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Pick out a bunch of points, they go off and they play a game of squash.",
                    "label": 0
                },
                {
                    "sent": "They call their mother, they have lunch and then come back in the algorithms.",
                    "label": 1
                },
                {
                    "sent": "Given an answer that's not right, you're going to pick some more points.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to change the stiffness in a few places so they go out.",
                    "label": 0
                },
                {
                    "sent": "They have dinner.",
                    "label": 0
                },
                {
                    "sent": "They have a good night's sleep.",
                    "label": 0
                },
                {
                    "sent": "They have long breakfast and lunch time just before lunch time.",
                    "label": 0
                },
                {
                    "sent": "They come in and I didn't answer and the question was so now this is a re estimation problem.",
                    "label": 0
                },
                {
                    "sent": "I've got an estimate and now I'm going to do changes to some of the potentials.",
                    "label": 0
                },
                {
                    "sent": "A subset of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Can I do something iteratively to solve that fast and so we use these adaptive methods that we developed to be able to solve these problems and this was worked on with some folks at Shell Oil in.",
                    "label": 0
                },
                {
                    "sent": "In Houston",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now let me talk a bit about multipole moments.",
                    "label": 0
                },
                {
                    "sent": "And these come from partial differential equation mostly from solving things like Maxwell's equations and the idea behind these is Gee, if I want to get a good approximation electric field at a point here and there are a bunch of charges up here, I probably can get a pretty good first order approximation by aggregating those into a course of one big charge.",
                    "label": 0
                },
                {
                    "sent": "And then I probably gotta do some local smoothing.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's the preconditioner that use, but that let us to think about the fact that G conditioned on the score scale picture the remaining correlations were in fact local.",
                    "label": 0
                },
                {
                    "sent": "And of course people want to Maxwell's equations exactly, believe Maxwell equations or truth, and therefore will just use this as an approximation as people who do machine learning like we said, look, we're going to build a model that has this structure exactly.",
                    "label": 0
                },
                {
                    "sent": "And so the idea here is I want to build things which which can think of if I just look at the graph of between scales, it looks like a tree pyramidal structure in scale.",
                    "label": 0
                },
                {
                    "sent": "However, when conditioned on neighboring scales, the correlations correlations within the scale are sparse and local.",
                    "label": 1
                },
                {
                    "sent": "OK, that doesn't mean the graphical model is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you have to rethink how you think about things.",
                    "label": 0
                },
                {
                    "sent": "So in fact you know graphical model.",
                    "label": 1
                },
                {
                    "sent": "It's the inverse of the covariance, that's sparse.",
                    "label": 0
                },
                {
                    "sent": "I mentioned earlier in the in the in the control context we often think about things where we have dual things which have the inverses of the covariances being the thing.",
                    "label": 1
                },
                {
                    "sent": "So we use the word conjugate models.",
                    "label": 0
                },
                {
                    "sent": "This is a graph where you're showing showing the correlation structure.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this model, one in three or uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "Just showing the correlations.",
                    "label": 0
                },
                {
                    "sent": "If I took the inverse of this to get the graphical model before.",
                    "label": 0
                },
                {
                    "sent": "But multiplying by this is really fast, and that corresponds to finite impulse response filtering, something that people in image processing do all of the time.",
                    "label": 0
                },
                {
                    "sent": "It's a finite finite with.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wiener filter.",
                    "label": 0
                },
                {
                    "sent": "So we developed the title.",
                    "label": 0
                },
                {
                    "sent": "I can't remember why we came up with that complicated tile, so it's sparse.",
                    "label": 0
                },
                {
                    "sent": "So we've got a multi resolution model and it's sparse in scale, conditional covariance and So what this says is condition on scales one and three variables.",
                    "label": 0
                },
                {
                    "sent": "Two and four are independent.",
                    "label": 0
                },
                {
                    "sent": "And So what this means, if I look at the inverse covariance, I'm trying to come up with a model of it as something that lives on a tree.",
                    "label": 0
                },
                {
                    "sent": "Or maybe slightly more complicated graph.",
                    "label": 0
                },
                {
                    "sent": "I'd like that that is that is sparse.",
                    "label": 0
                },
                {
                    "sent": "Plus something whose inverse is sparse and concentrated within scales.",
                    "label": 0
                },
                {
                    "sent": "So this guy is not going to be sparse.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause he's got that so so this is J X = H and I'm trying to solve solve for X.",
                    "label": 0
                },
                {
                    "sent": "If I do a Richardson iteration and just keep the three part on to go up and down, I'll bring the thing over to the other side.",
                    "label": 1
                },
                {
                    "sent": "So what happens in that step in that step?",
                    "label": 0
                },
                {
                    "sent": "The calculations I have to do our looks like this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to get a new estimate so this is a tree and H minus this and of course this is not sparse.",
                    "label": 0
                },
                {
                    "sent": "I don't calculate that exactly, this is sparse, so I use whatever sparse method.",
                    "label": 0
                },
                {
                    "sent": "I've got to solve for Z, which I plug in here, very fast to be able to do that.",
                    "label": 0
                },
                {
                    "sent": "And then when I want to solve within each scale that's multiplying by a sparse matrix, it's basically what's used in multiple methods, it's dramatic.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing I will show 1 result from solving the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This is on the state reports was 100 index but but Jin had data over that 17 year.",
                    "label": 1
                },
                {
                    "sent": "For only 84 of the companies and she took it as that.",
                    "label": 0
                },
                {
                    "sent": "Restructure the way SMP divides things into markets, divisions in industries and companies into this and so it remodel there and then I want to understand where there are strong residual correlations.",
                    "label": 1
                },
                {
                    "sent": "So you find things such as Schlumberger which is an oil services company.",
                    "label": 0
                },
                {
                    "sent": "In a different industry, has high residual correlation with Shell Oil companies.",
                    "label": 0
                },
                {
                    "sent": "You find that companies like Microsoft are are going to have high high residual correlation with things like Dell sit in different places.",
                    "label": 0
                },
                {
                    "sent": "You also find out that Walmart is negatively correlated to everything.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, a few other things that that that we're doing that that just to advertise a bid on this.",
                    "label": 1
                },
                {
                    "sent": "We've done some work of that quite information theoretic and has some interest to the to the community.",
                    "label": 0
                },
                {
                    "sent": "This work done by student Vincent, an anonymous on Kumar who is a postdoc in our group of trying to learn the error exponent.",
                    "label": 0
                },
                {
                    "sent": "So as I get more and more data, let's think about Chow Liu algorithm for learning a tree.",
                    "label": 0
                },
                {
                    "sent": "What errors does it make?",
                    "label": 0
                },
                {
                    "sent": "And what's the rate at which those errors decay to zero as I get more and more data?",
                    "label": 1
                },
                {
                    "sent": "More than that, what are the graphs that are hardest to learn, easiest to learn so it turns out that that change Markov chains are the easiest to learn.",
                    "label": 1
                },
                {
                    "sent": "Star graphs are the hardest because of the fact that it's very easy to.",
                    "label": 0
                },
                {
                    "sent": "2 interchange edges.",
                    "label": 0
                },
                {
                    "sent": "Some work that actually you can find.",
                    "label": 0
                },
                {
                    "sent": "If you look at your pearls book, something that as a control theorist I really liked it was an algebraic characterization of how for graphical models, if they were hidden trees, how you could, if you had the exact statistics, discover exactly the hidden structure.",
                    "label": 0
                },
                {
                    "sent": "There's some algebraic conditions, and Jin Choi develop this into a precise algorithm for turning this on, and then of course you know exact statistics to relax the algorithms that allow you to do robust identification of those hidden trees.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem we're trying to discover the tree structure.",
                    "label": 0
                },
                {
                    "sent": "We haven't looked at the problem yet trying to look discover the tree structure and the dimensions of the variables at the hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting problem that we're quite interested in it.",
                    "label": 0
                },
                {
                    "sent": "And this is something that a number of people think Emmanuel Candes presented something on this type of problem here.",
                    "label": 0
                },
                {
                    "sent": "This problem I proposed to Venkat Chandrasekharan.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, the.",
                    "label": 0
                },
                {
                    "sent": "There are two ways that we often talk about doing complexity reduction of complex data.",
                    "label": 0
                },
                {
                    "sent": "One is dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "OK, things like PCA, the other is let's keep the full dimension, but let's get a sparse model.",
                    "label": 0
                },
                {
                    "sent": "OK, well, why don't you just one of those?",
                    "label": 0
                },
                {
                    "sent": "Why put all the heavy lifting on one of them?",
                    "label": 0
                },
                {
                    "sent": "Why not do a combination of them?",
                    "label": 0
                },
                {
                    "sent": "So rather than do PCA, PCA says look, I want to get all of the statistical structure there.",
                    "label": 0
                },
                {
                    "sent": "I'm going to identify a low dimensional set of variables that when conditioned on them, all of my original variables have a sparse graphical model.",
                    "label": 0
                },
                {
                    "sent": "And what that corresponds to is taking an inverse covariance in writing, is some of something that sparse and something that's low rank.",
                    "label": 0
                },
                {
                    "sent": "And you can formulate very precise problems on this and then Cat is his drag.",
                    "label": 0
                },
                {
                    "sent": "Me kicking and string screaming trying to relearn all the algebraic geometry I've forgotten the conditions under which you can see how the errors behave in this by looking at the curvature of algebraic varieties, but are very beautiful set of results that lead to very very efficient algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's on the horizon?",
                    "label": 1
                },
                {
                    "sent": "So this is one that I think about because of sensor networks.",
                    "label": 0
                },
                {
                    "sent": "We talk about message passing, but in machine learning you really not talking bout messages or if it's a sensor network, their messages, and if you think about BP as a distributed algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a really dumb algorithm with very simple protocol.",
                    "label": 0
                },
                {
                    "sent": "What does each node need to know?",
                    "label": 0
                },
                {
                    "sent": "It needs to know whose neighbors are and it needs to know which of the messages are coming from which of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "Because if you want to send a message to this guy, it's going to bind the messages from all of its other neighbors to send it there.",
                    "label": 0
                },
                {
                    "sent": "So it's going to computation routing.",
                    "label": 0
                },
                {
                    "sent": "That's what that's basically what it's doing.",
                    "label": 0
                },
                {
                    "sent": "But of course, they so the only header bit it gets is which node is sending it, something it then immediately forgets everything.",
                    "label": 0
                },
                {
                    "sent": "It just saw.",
                    "label": 0
                },
                {
                    "sent": "It gets a new set of messages.",
                    "label": 0
                },
                {
                    "sent": "That's what BP does.",
                    "label": 0
                },
                {
                    "sent": "G. Suppose I don't try to keep data that tells me something the way the way cellphone multihop messages work.",
                    "label": 0
                },
                {
                    "sent": "Wireless networks that that give you an idea of where things have gone.",
                    "label": 0
                },
                {
                    "sent": "Are there ways that notes can in fact, use those to understand not just where to route things, but what computations to perform on these?",
                    "label": 0
                },
                {
                    "sent": "The feedback vertex set out to my describe you is a is a global algorithm that is all of the nodes need to know the structure and know who the feedback vertex nodes are.",
                    "label": 0
                },
                {
                    "sent": "There ways of getting things that are completely distributed so it looks like BP and each node on the way figures out if in fact it's Leonardo DiCaprio as King of the mountain is one of the feedback vertex nodes, or Leonardo DiCaprio is down in steerage.",
                    "label": 0
                },
                {
                    "sent": "As one of the nodes in the leftover tree.",
                    "label": 0
                },
                {
                    "sent": "This is something that we're really quite interested in understanding, by the way, you can think of these distributors that these are distributed dynamic systems, and we know what we're trying to realize.",
                    "label": 0
                },
                {
                    "sent": "We're trying to get calculate all the walks.",
                    "label": 0
                },
                {
                    "sent": "For example, alright, can we come up with distributed approximate ones by building more complex memory into these things?",
                    "label": 0
                },
                {
                    "sent": "And so this is actually really quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Is supposed to really our messages, but the statistical structure of the process you're looking at is not identical to the communication network structure.",
                    "label": 0
                },
                {
                    "sent": "So you're going to send messages to different places where you send.",
                    "label": 0
                },
                {
                    "sent": "Why do you have to send them along along nodes of the graphical structure?",
                    "label": 1
                },
                {
                    "sent": "And we're doing a variety of things on non Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "Most of what I've talked about here we're trying to generalize van cats doing some really wild things on be posting graphs instead of doing sparse Postal rank compositions.",
                    "label": 0
                },
                {
                    "sent": "And with that, I mean, if I recall correctly, you gave Me 2 1/2 hours to talk and I think I finished early.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "We have a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, metric.",
                    "label": 0
                },
                {
                    "sent": "Um, so you know the work that Erica Martin did on that trees and things like that?",
                    "label": 0
                },
                {
                    "sent": "Is it closely related to this?",
                    "label": 0
                },
                {
                    "sent": "Subgraph preconditioners work?",
                    "label": 0
                },
                {
                    "sent": "That is, indeed, indeed, I mean, this is stuff that I think I think it's correct.",
                    "label": 0
                },
                {
                    "sent": "We started developing these without really having looked that much at the linear algebra literature.",
                    "label": 0
                },
                {
                    "sent": "But then you find those things in there and they don't have some of the things that we've started to do.",
                    "label": 0
                },
                {
                    "sent": "For example, these venkat's algorithm is trying to figure out the best trees to use and like on.",
                    "label": 0
                },
                {
                    "sent": "But indeed there are lots of things in there.",
                    "label": 0
                },
                {
                    "sent": "Typically there, but I didn't talk about is how we might be able to calculate the variance is in these things and Eric's work.",
                    "label": 0
                },
                {
                    "sent": "He showed that Gee, you called him keynotes.",
                    "label": 0
                },
                {
                    "sent": "If there is a small number of keynotes which turns out to be a feedback vertex set, you got a really cute way of getting variances.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not something you'll find in the medical literature 'cause they don't.",
                    "label": 0
                },
                {
                    "sent": "They're solving ax equals B, whereas an elliptic operator they're just trying to solve that they're not trying to get the diagonal elements of the inverse of a, which is what you want to get.",
                    "label": 0
                },
                {
                    "sent": "Having maximum weighted trees is very scattered that.",
                    "label": 0
                },
                {
                    "sent": "They choose those trees in that way, but but it's not in sort of this walk some kind of picture of things.",
                    "label": 0
                },
                {
                    "sent": "They have simpler out.",
                    "label": 0
                },
                {
                    "sent": "We've talked to some of the people in that field.",
                    "label": 0
                },
                {
                    "sent": "There are ways they tried to Max weight with weights there usually taking or the potential weights on things multiplied by whatever the equation error is at each node.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit more bit more machine learning BP oriented in terms of thinking about walk sums so, but is it?",
                    "label": 0
                },
                {
                    "sent": "Is it a very numerical linear algebra?",
                    "label": 0
                },
                {
                    "sent": "So that's neat.",
                    "label": 0
                },
                {
                    "sent": "I can just plug that into my algorithm.",
                    "label": 0
                },
                {
                    "sent": "By changing 3 lines of code, yeah.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Maybe I have one so you mentioned a few times so.",
                    "label": 0
                },
                {
                    "sent": "Control people do.",
                    "label": 0
                },
                {
                    "sent": "Don't know about control.",
                    "label": 0
                },
                {
                    "sent": "Control people so.",
                    "label": 0
                },
                {
                    "sent": "So what do you think are the main reasons for that?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's tractable to get into this.",
                    "label": 0
                },
                {
                    "sent": "Well, in my opinion, the barriers are breaking down their variety of us, Constantine, me and other people have come from sort of more systems and control background optimization background or finding interesting things here.",
                    "label": 0
                },
                {
                    "sent": "And this is indeed the case if you now take a look at the control field, you're finding the.",
                    "label": 0
                },
                {
                    "sent": "The nature of things that people are looking at it, especially if we started thinking about distributed systems or network control boundaries, are.",
                    "label": 0
                },
                {
                    "sent": "Thankfully, shattering early on I would say the major barrier that existed for a long time was continuous mathematics versus discrete mathematics, and the fact that for lots of the problems control people used to spend and still spend a lot of time on the model they were looking typically given was based on physics, so there was a physical model rather than something they completely had to infer.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, they do a lot of work.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of work in model reduction, which you can think of is approximate modeling, so there are places where their ties.",
                    "label": 0
                },
                {
                    "sent": "And you know another reason is just the fact of how many meetings can you attend the IEEE Conference on Decision Control.",
                    "label": 0
                },
                {
                    "sent": "Starts day after tomorrow, so so in fact there are issues that come up just because of the timing of meetings.",
                    "label": 0
                },
                {
                    "sent": "My personal view is and it's something that MIT we're now.",
                    "label": 0
                },
                {
                    "sent": "Whether you like our new status, enter if you know it or not.",
                    "label": 0
                },
                {
                    "sent": "One of the nice things that didn't put the control and optimization people in the same building with all of the computer scientists so that boundary is being shattered.",
                    "label": 0
                },
                {
                    "sent": "And I think that's exceedingly important because their imaginary.",
                    "label": 0
                }
            ]
        }
    }
}