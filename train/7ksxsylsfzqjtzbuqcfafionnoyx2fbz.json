{
    "id": "7ksxsylsfzqjtzbuqcfafionnoyx2fbz",
    "title": "Learning to Compare using Operator-Valued Large-Margin Classifiers",
    "info": {
        "author": [
            "Andreas Maurer, Stemmer Imaging"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/lce06_maurer_lcuov/",
    "segmentation": [
        [
            "Probably very well known to all of you, and assuming that I can embed the input space, which is where the pattern that we work with live again embedded in a Euclidean space, high dimensional Euclidean space with some kind of kernel trick or some other preprocessing device, and I'm just identifying the input space the so there would be a pixel vectors or whatever, but identifying them with their image under this kernel map.",
            "Preprocessing map so I don't have to talk about the kernel at all anymore, so everything that's going to be that I'm going to be talking off will be linear.",
            "That's happening there.",
            "The assumption that the diameter of the embedded input is less than one or equal to 1 is suggested.",
            "Technical convenience for the theoretical results that I present.",
            "So we want to compare inputs.",
            "That means in a way deciding if they are similar or dissimilar or measuring distances.",
            "Now I'll concentrate on the question of similarity, dissimilarity or.",
            "So I'm going to try to put everything in the form of a classification problem that was already mentioned.",
            "I think before in a question that that a lot of the comparing problem can be viewed as a as a classification."
        ],
        [
            "Bing.",
            "Yeah.",
            "So the underlying reality is that I'm assuming that there is a pair Oracle which every time I ask that, I call on the pair or after it returns a pair of inputs.",
            "And a label which is going to be one if they are homonymous or have the same label or are similar when we have a large number of words that we can use for it or minus one if they are heteronomous and this is a probability measure on the set of the labeled pairs so.",
            "If you can look at it as a model for the stochastic generation of equivalence constraints.",
            "Also, that's also a name that's being used for these these triplets really.",
            "And what you're looking for or what I'm looking for at first is a pair classifier which just classifiers pairs and predict if they're going to be or monomers or heteronomous.",
            "So it just it's a classifier to predict the third argument.",
            "Off this, that pair Oracle produces."
        ],
        [
            "Now the we don't know the ways of the Oracle, so we will have to use the data that comes from a finite sample that we generate in independent draws of this probability measure, or in independent calls of the Oracle.",
            "So the."
        ],
        [
            "The object is to use this training sample, which just consists of a finite sequence of triplets of pairs of inputs and the label, which can be one or minus one, depending if they have the same.",
            "If they are the same type or similar or whatever, or not and we just want to use this to find a pair classifier which has a low error probability, this probability is going to be again measured with the pair or actual.",
            "So that's the framework that I'm starting out."
        ],
        [
            "With here.",
            "And the.",
            "Now the next step would be to commit to a hypothesis space because there's so many possible candidates of classifiers.",
            "Of course, to make a search possible, I'm going to look to look at a particular model for the classifiers that I will use, and I will look at the set of classifiers that can be parameterized that actually use this.",
            "I just bought this so I'm not used to it.",
            "The parameterized by linear transformations on this space on the Hilbert space.",
            "And the way such a linear transformation generates a classifier is very simple.",
            "I just I applied to.",
            "The inputs will be by linearity.",
            "I could just put a parenthesis here and just write it on.",
            "I apply it to the two inputs and if the distance is less than one, then the sign here is going to be positive.",
            "So I'm going to say there are monomers and if the distance is larger than one, then I say they're hit around in my state and so it's a very intuitive thing.",
            "I just measured the mapped.",
            "The distance between the map points and if the closer together than one, then I say that they belong to the same kind and if they are further apart than one, I say that belong to different kinds and the threshold.",
            "The reason why I can use the threshold one is because linearity.",
            "If I had any other threshold I could just absorb it in the transformation.",
            "So, so this is the model of classifiers today.",
            "You say to find a class."
        ],
        [
            "So I have to find a linear transformation and so this fits in with many of the frameworks that are going to be talked about.",
            "Also today, because this choice of the linear transformation.",
            "Apart from just defining this classifier, it also defines a pseudo metric on the this mapped input space or the Mahalanobis.",
            "Tongue Twister distance that would just be the square of the the pseudometric.",
            "The one induced by the operator T, started.",
            "This is the joint or transpose of the operator.",
            "Or a positive semidefinite kernel that is induced also by this, this this operator this product so that I get all this if I get if I have the transformation T."
        ],
        [
            "And.",
            "Now I can risk that I associate with an operator.",
            "Is just the error probability of the induced classifier and despair or actually so the risk of the transformation T is the probability under calls from the Oracle that the classifier will just give the wrong make the wrong decision on the pair that I have, so it will if it says there if the Oracle says they belong to the same kind of says belongs to different kinds.",
            "Or the other way around, I can rewrite this in this form?",
            "Here is this is a standard kind of thing because this is can be one or minus one.",
            "So if you think about a little bit, it's easy.",
            "You see this a lot.",
            "I can also put in the square because I'm using one as a threshold so it doesn't make any difference on the probability here instead of events is the same.",
            "And.",
            "This square here.",
            "This expression.",
            "Is the same as this this Mahalanobis distance here?",
            "If you just take the definition of the inner product of the norm and the relation to the inner product and bring the transpose over, then you can see that in this expression is linear in the T star T linear in the.",
            "In this operator here, this is also which you can also see.",
            "So this is really a linear classifier that I'm looking for.",
            "And the intuition behind everything that I'm doing is that we know how to find linear classifiers with the support vector machines, say and.",
            "So we're looking for a support vector machine where the weight vector is.",
            "Is an operator is a positive operator.",
            "So that's beginning to, I hope, to clarify why the title of the talk is talks about large margin operator valued large margin classifiers.",
            "So this is the intuition and the way."
        ],
        [
            "So I proceed is.",
            "Bye.",
            "So what I want to do is, I since I don't know the probability itself, I don't know the distribution.",
            "I only know the sample, so I have to define a sample Bates based estimator on the error and I'm going to minimize this estimator in some regularization terms on it and.",
            "So if I have a Lipschitz function which bounds the indicator function of the negative numbers like, this could be the hinge loss, which is what I'm using.",
            "But it could also be the square square loss that have been talked about before.",
            "And and the training sample.",
            "Then I define this risk functional, the empirical and empirical risk estimate.",
            "So.",
            "The reason why this is going to work is because this is an empirical average.",
            "If I have a Lipschitz function, I can control the difference between the empirical average and the.",
            "Spectation now the If I take the expectation of this, the lipsticks function is going to upper bound the characteristic function of the negatives.",
            "So this this is going to be with the expectation that it's going to be an upper bound for the for the oops.",
            "This just goes to pass the upper bound for the risk that I have here.",
            "This is like I could write this as an expectation with the characteristic function of the negative numbers, so it would be a.",
            "It's a reason why this is going to work as an estimator, So what?"
        ],
        [
            "It is the theorem which I will not prove because I don't have anywhere near the time, but it's actually straightforward.",
            "It uses the standard theorems for linear for SVM's really.",
            "And for the linear case and so that.",
            "But the theorem says is that.",
            "For any kind of confidence parameter with overwhelming probability or high probability depending on the confidence parameter, I will draw a sample from the pyroraptor such that for every operator the risk.",
            "This is just a technical that will not cause any problems because most of the operators are going to induce a norm that's larger than one.",
            "I'm going to for every operator have this bound.",
            "The risk will be bounded by the empirical estimator plus complexity penalty.",
            "Which decreases, and this is also very usual in the in the square root of the number of of the size of my sample.",
            "Also, these there's a lot of.",
            "This logarithmic term containing the confidence parameter is something very usual, so it's not.",
            "It shouldn't be too strange to look at this and this norm here is the Hilbert normally infinite dimensional case.",
            "It is the Frobenius norm of a transformation in the Matrix case.",
            "And what I'm going to do is just minimize this bound, really.",
            "So that's basically the way that I proceed.",
            "I'm going to make some practical modifications.",
            "One of it is that I'm going to just negate this part because it's too complicated for me because I just regard this as being dominant.",
            "Here and the other one is that these bounds.",
            "Notoriously overestimate the complexity penalties.",
            "They overestimate the.",
            "The estimation error, and for this reason I'm going to substitute the 8 L here.",
            "The Ellis Lipschitz constant and we're going to substitute that with the regularization constant regularization parameter, which I can can adjust, but what I'm going to do is I'm going to use the same one for all the experiments.",
            "So.",
            "What I end up with is."
        ],
        [
            "An objective function.",
            "That I have to minimize.",
            "Is composed of an empirical termina regularizer here as opposed to other, you often see this without the root M. Here I put that in there.",
            "I let it.",
            "I tried to modify the bound of the theorem as little as possible to get the objective function, and so you could use other regular.",
            "This I've seen other regularizers in papers for in similar cases.",
            "For instance you have you have this bound, so you could use this as a regularizer.",
            "That would be the trace of this operator, but it's it's a stronger regular.",
            "I said it makes everything more efficient, but it's going to.",
            "The results weren't quite as good if I used it, so I just continued using the things that worked well because I didn't have many resources for the experiments.",
            "And so efficiency is not really a concern of anything really.",
            "In my talk here.",
            "Pardon the start.",
            "Yeah, it's transposition.",
            "Here I join into a transposition of it."
        ],
        [
            "So for the the N for the loss I just take the hinge loss which.",
            "Graphically depicted here with a margin.",
            "Of course, the margin is going to give me the Lipschitz constant, and so it will.",
            "It will work in the pound.",
            "And he had his loss has some practical properties, some nice properties that it is in a way the smallest one that has this Lipschitz constant and realises this.",
            "This margin.",
            "But I could have used the square square loss 2.",
            "In fact, it might might be very interesting, might actually improve something, but as I said, I just know this works so well that I didn't really think or have time to try anything else.",
            "If I can rewrite this up there, just substituting this this equality here this identity.",
            "The UPS nor there was the wrong button.",
            "Here it is, and so you can see that's linear in the operator T star T The positive square of T. So this whole thing is going to be.",
            "It's going to be convex if because the Lipschitz the function F, the hinge loss is convex and this is then going to be affine in TS 30 this is also convex because it's a norm, so the whole objective function is convex in the.",
            "In the operator T starchie.",
            "OK. And."
        ],
        [
            "So you've written it again, so I have two 2 possibilities for the algorithm.",
            "One is just two.",
            "To minimize this and take the square root, the make the spectral decomposition and take the op."
        ],
        [
            "Wait, which is the positive square root of it.",
            "I didn't do that because I found that I can.",
            "I can just minimize this by gradient descent and it will.",
            "I will still not.",
            "I will not have any problems with the with the local minima and that theorem.",
            "I found the proof so nice that I just continued working with it and that's why I discarded this technique.",
            "But it may actually."
        ],
        [
            "More efficient.",
            "The other possibility is that, yeah, that's what I do.",
            "I just do gradient descent and it's in the paper on the on the web page, so there's a very simple proof that I'm not going to have this going to be no stable local minimum of this functional in the function of T groups.",
            "I.",
            "If there isn't one of the other one so."
        ],
        [
            "Here's the algorithm.",
            "I only show it to you to demonstrate that you can write it on one page.",
            "Of course, there's a lot of shorthand letter using, so if you do all the kernel stuff in it, then it's quite a bit of programming that has been done to be done so.",
            "And."
        ],
        [
            "So I made some experiments and they were all done with image recognition.",
            "But so I use the invariant character recognition.",
            "I work with spatial rotations like in this coil database in face recognition.",
            "And what I always did is I took a train, the operator T from one group of tasks and then apply it to as in the previous talk to.",
            "OK and then applied it to another task.",
            "This is the kernel that I used.",
            "OK, it's just I didn't do any image specific preprocessing here other than normalization, so the if there would be an arbitrary permutation of pixels, the results would be exactly the same and on it."
        ],
        [
            "And I use the same parameters for all.",
            "I'm just going to show you the results of 1 experiment that I did that was with arbitrarily scaled and rotated characters, and I did so that when I trained the operator, these two are the same and these two are the same.",
            "It gets told that these are different and these are the same ends on four 4000 examples and I only use the Alpha characters to train it."
        ],
        [
            "And applied it to digits to test it with a single example per class.",
            "So this could be a complete training set.",
            "And then I."
        ],
        [
            "Applied it to this kind of data and just see what the single example nearest neighbor classifier gives me in terms of."
        ],
        [
            "For.",
            "Results there are the results.",
            "This is the RC curve for the metric as a detector of upper class inequality.",
            "You can see that the original like if I just use the input data, the Euclidean metric on the input data it has.",
            "I have cannot recognize anything and this is using the T operator.",
            "You can see it's doing quite well.",
            "It reduces the error from 82% which is close to random.",
            "Guessing too little bit under 10%.",
            "OK, so I guess I should stop now because I'm already over the time limit.",
            "Is that correct?",
            "If you want to have some questions, yeah, sure, yeah, so I've done.",
            "So.",
            "Compute T. It has to have a nice representation from finite tractor representation.",
            "Yeah, talk about women.",
            "How T inherits some kind of funny representation from the data with a finite representation is a consequence of the objective because it becomes clear if you look at it that it has to live on the space the subspace that is spanned by the examples, just like with the SVM's.",
            "It's the same thing.",
            "And the complexity is quadratic, right?",
            "The you know it's super quadratic.",
            "It's on every step.",
            "I have have quadratic complexity, so it would be better to use the the thing at all algorithm to enhance the efficiency.",
            "But as I said it still worked.",
            "You know if I put it in a closet or when I it and the next day then I eventually get a result.",
            "So this is the kind of efficiency that I'm talking off, it's not, it's.",
            "That's with the 4000 examples in the training examples.",
            "Yes, please.",
            "Because we are losing.",
            "How do you compute dissent?",
            "Well, it's almost everywhere differentiable.",
            "The hinge loss.",
            "There's only one point, so at that point it is arbitrary.",
            "Existing yeah this is closing a little bit of this mountain crash ability and assumption.",
            "What is going to be active constraints where the hinge loss assumes a value that is larger than zero and an active one so and so as long as they are still active.",
            "Of course there still is still a nonvanishing gradient, so.",
            "In the trials that I make and we can like a random stochastic gradient descent there.",
            "So it's only one point where it's not different.",
            "We have measured zero, it's.",
            "Fed to pick a configuration is going to be.",
            "It's going to be differentiable at that point.",
            "Yes.",
            "Yes, I had to.",
            "I was racing through that part.",
            "Maybe I can quickly.",
            "Here's the kernel, so these are the pixel vectors and normalize them, and I used the same width parameter for all so the coil and eight AT&T database.",
            "The other things you can read up on the paper.",
            "The other tests that I made I just took the just picked the most difficult one that was combined.",
            "Rotation and scale invariants.",
            "Looks like so.",
            "I've done it with rotation invariants and linear kernels, and not surprisingly, the components of T are all rotation invariant images.",
            "Then it's clear that it has to be like that.",
            "With rotation invariants.",
            "But of course with combined rotation and scale invariants, the linear kernel will not work anymore, so I did most of most most of the work with a Gaussian RBF kernel.",
            "There are other questions.",
            "Best time for fishing.",
            "OK, So what it says the speaker again thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably very well known to all of you, and assuming that I can embed the input space, which is where the pattern that we work with live again embedded in a Euclidean space, high dimensional Euclidean space with some kind of kernel trick or some other preprocessing device, and I'm just identifying the input space the so there would be a pixel vectors or whatever, but identifying them with their image under this kernel map.",
                    "label": 1
                },
                {
                    "sent": "Preprocessing map so I don't have to talk about the kernel at all anymore, so everything that's going to be that I'm going to be talking off will be linear.",
                    "label": 0
                },
                {
                    "sent": "That's happening there.",
                    "label": 0
                },
                {
                    "sent": "The assumption that the diameter of the embedded input is less than one or equal to 1 is suggested.",
                    "label": 0
                },
                {
                    "sent": "Technical convenience for the theoretical results that I present.",
                    "label": 0
                },
                {
                    "sent": "So we want to compare inputs.",
                    "label": 0
                },
                {
                    "sent": "That means in a way deciding if they are similar or dissimilar or measuring distances.",
                    "label": 0
                },
                {
                    "sent": "Now I'll concentrate on the question of similarity, dissimilarity or.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try to put everything in the form of a classification problem that was already mentioned.",
                    "label": 0
                },
                {
                    "sent": "I think before in a question that that a lot of the comparing problem can be viewed as a as a classification.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bing.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the underlying reality is that I'm assuming that there is a pair Oracle which every time I ask that, I call on the pair or after it returns a pair of inputs.",
                    "label": 1
                },
                {
                    "sent": "And a label which is going to be one if they are homonymous or have the same label or are similar when we have a large number of words that we can use for it or minus one if they are heteronomous and this is a probability measure on the set of the labeled pairs so.",
                    "label": 0
                },
                {
                    "sent": "If you can look at it as a model for the stochastic generation of equivalence constraints.",
                    "label": 0
                },
                {
                    "sent": "Also, that's also a name that's being used for these these triplets really.",
                    "label": 0
                },
                {
                    "sent": "And what you're looking for or what I'm looking for at first is a pair classifier which just classifiers pairs and predict if they're going to be or monomers or heteronomous.",
                    "label": 0
                },
                {
                    "sent": "So it just it's a classifier to predict the third argument.",
                    "label": 1
                },
                {
                    "sent": "Off this, that pair Oracle produces.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the we don't know the ways of the Oracle, so we will have to use the data that comes from a finite sample that we generate in independent draws of this probability measure, or in independent calls of the Oracle.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The object is to use this training sample, which just consists of a finite sequence of triplets of pairs of inputs and the label, which can be one or minus one, depending if they have the same.",
                    "label": 0
                },
                {
                    "sent": "If they are the same type or similar or whatever, or not and we just want to use this to find a pair classifier which has a low error probability, this probability is going to be again measured with the pair or actual.",
                    "label": 0
                },
                {
                    "sent": "So that's the framework that I'm starting out.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With here.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Now the next step would be to commit to a hypothesis space because there's so many possible candidates of classifiers.",
                    "label": 0
                },
                {
                    "sent": "Of course, to make a search possible, I'm going to look to look at a particular model for the classifiers that I will use, and I will look at the set of classifiers that can be parameterized that actually use this.",
                    "label": 0
                },
                {
                    "sent": "I just bought this so I'm not used to it.",
                    "label": 0
                },
                {
                    "sent": "The parameterized by linear transformations on this space on the Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "And the way such a linear transformation generates a classifier is very simple.",
                    "label": 0
                },
                {
                    "sent": "I just I applied to.",
                    "label": 0
                },
                {
                    "sent": "The inputs will be by linearity.",
                    "label": 0
                },
                {
                    "sent": "I could just put a parenthesis here and just write it on.",
                    "label": 0
                },
                {
                    "sent": "I apply it to the two inputs and if the distance is less than one, then the sign here is going to be positive.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say there are monomers and if the distance is larger than one, then I say they're hit around in my state and so it's a very intuitive thing.",
                    "label": 0
                },
                {
                    "sent": "I just measured the mapped.",
                    "label": 0
                },
                {
                    "sent": "The distance between the map points and if the closer together than one, then I say that they belong to the same kind and if they are further apart than one, I say that belong to different kinds and the threshold.",
                    "label": 0
                },
                {
                    "sent": "The reason why I can use the threshold one is because linearity.",
                    "label": 0
                },
                {
                    "sent": "If I had any other threshold I could just absorb it in the transformation.",
                    "label": 0
                },
                {
                    "sent": "So, so this is the model of classifiers today.",
                    "label": 0
                },
                {
                    "sent": "You say to find a class.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have to find a linear transformation and so this fits in with many of the frameworks that are going to be talked about.",
                    "label": 0
                },
                {
                    "sent": "Also today, because this choice of the linear transformation.",
                    "label": 1
                },
                {
                    "sent": "Apart from just defining this classifier, it also defines a pseudo metric on the this mapped input space or the Mahalanobis.",
                    "label": 1
                },
                {
                    "sent": "Tongue Twister distance that would just be the square of the the pseudometric.",
                    "label": 1
                },
                {
                    "sent": "The one induced by the operator T, started.",
                    "label": 1
                },
                {
                    "sent": "This is the joint or transpose of the operator.",
                    "label": 0
                },
                {
                    "sent": "Or a positive semidefinite kernel that is induced also by this, this this operator this product so that I get all this if I get if I have the transformation T.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now I can risk that I associate with an operator.",
                    "label": 0
                },
                {
                    "sent": "Is just the error probability of the induced classifier and despair or actually so the risk of the transformation T is the probability under calls from the Oracle that the classifier will just give the wrong make the wrong decision on the pair that I have, so it will if it says there if the Oracle says they belong to the same kind of says belongs to different kinds.",
                    "label": 1
                },
                {
                    "sent": "Or the other way around, I can rewrite this in this form?",
                    "label": 0
                },
                {
                    "sent": "Here is this is a standard kind of thing because this is can be one or minus one.",
                    "label": 0
                },
                {
                    "sent": "So if you think about a little bit, it's easy.",
                    "label": 0
                },
                {
                    "sent": "You see this a lot.",
                    "label": 0
                },
                {
                    "sent": "I can also put in the square because I'm using one as a threshold so it doesn't make any difference on the probability here instead of events is the same.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This square here.",
                    "label": 0
                },
                {
                    "sent": "This expression.",
                    "label": 0
                },
                {
                    "sent": "Is the same as this this Mahalanobis distance here?",
                    "label": 0
                },
                {
                    "sent": "If you just take the definition of the inner product of the norm and the relation to the inner product and bring the transpose over, then you can see that in this expression is linear in the T star T linear in the.",
                    "label": 0
                },
                {
                    "sent": "In this operator here, this is also which you can also see.",
                    "label": 0
                },
                {
                    "sent": "So this is really a linear classifier that I'm looking for.",
                    "label": 0
                },
                {
                    "sent": "And the intuition behind everything that I'm doing is that we know how to find linear classifiers with the support vector machines, say and.",
                    "label": 0
                },
                {
                    "sent": "So we're looking for a support vector machine where the weight vector is.",
                    "label": 0
                },
                {
                    "sent": "Is an operator is a positive operator.",
                    "label": 0
                },
                {
                    "sent": "So that's beginning to, I hope, to clarify why the title of the talk is talks about large margin operator valued large margin classifiers.",
                    "label": 0
                },
                {
                    "sent": "So this is the intuition and the way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I proceed is.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is, I since I don't know the probability itself, I don't know the distribution.",
                    "label": 0
                },
                {
                    "sent": "I only know the sample, so I have to define a sample Bates based estimator on the error and I'm going to minimize this estimator in some regularization terms on it and.",
                    "label": 0
                },
                {
                    "sent": "So if I have a Lipschitz function which bounds the indicator function of the negative numbers like, this could be the hinge loss, which is what I'm using.",
                    "label": 0
                },
                {
                    "sent": "But it could also be the square square loss that have been talked about before.",
                    "label": 0
                },
                {
                    "sent": "And and the training sample.",
                    "label": 1
                },
                {
                    "sent": "Then I define this risk functional, the empirical and empirical risk estimate.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The reason why this is going to work is because this is an empirical average.",
                    "label": 0
                },
                {
                    "sent": "If I have a Lipschitz function, I can control the difference between the empirical average and the.",
                    "label": 0
                },
                {
                    "sent": "Spectation now the If I take the expectation of this, the lipsticks function is going to upper bound the characteristic function of the negatives.",
                    "label": 0
                },
                {
                    "sent": "So this this is going to be with the expectation that it's going to be an upper bound for the for the oops.",
                    "label": 0
                },
                {
                    "sent": "This just goes to pass the upper bound for the risk that I have here.",
                    "label": 0
                },
                {
                    "sent": "This is like I could write this as an expectation with the characteristic function of the negative numbers, so it would be a.",
                    "label": 0
                },
                {
                    "sent": "It's a reason why this is going to work as an estimator, So what?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is the theorem which I will not prove because I don't have anywhere near the time, but it's actually straightforward.",
                    "label": 0
                },
                {
                    "sent": "It uses the standard theorems for linear for SVM's really.",
                    "label": 0
                },
                {
                    "sent": "And for the linear case and so that.",
                    "label": 0
                },
                {
                    "sent": "But the theorem says is that.",
                    "label": 0
                },
                {
                    "sent": "For any kind of confidence parameter with overwhelming probability or high probability depending on the confidence parameter, I will draw a sample from the pyroraptor such that for every operator the risk.",
                    "label": 0
                },
                {
                    "sent": "This is just a technical that will not cause any problems because most of the operators are going to induce a norm that's larger than one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to for every operator have this bound.",
                    "label": 0
                },
                {
                    "sent": "The risk will be bounded by the empirical estimator plus complexity penalty.",
                    "label": 0
                },
                {
                    "sent": "Which decreases, and this is also very usual in the in the square root of the number of of the size of my sample.",
                    "label": 0
                },
                {
                    "sent": "Also, these there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "This logarithmic term containing the confidence parameter is something very usual, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It shouldn't be too strange to look at this and this norm here is the Hilbert normally infinite dimensional case.",
                    "label": 0
                },
                {
                    "sent": "It is the Frobenius norm of a transformation in the Matrix case.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to do is just minimize this bound, really.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the way that I proceed.",
                    "label": 0
                },
                {
                    "sent": "I'm going to make some practical modifications.",
                    "label": 0
                },
                {
                    "sent": "One of it is that I'm going to just negate this part because it's too complicated for me because I just regard this as being dominant.",
                    "label": 0
                },
                {
                    "sent": "Here and the other one is that these bounds.",
                    "label": 0
                },
                {
                    "sent": "Notoriously overestimate the complexity penalties.",
                    "label": 0
                },
                {
                    "sent": "They overestimate the.",
                    "label": 0
                },
                {
                    "sent": "The estimation error, and for this reason I'm going to substitute the 8 L here.",
                    "label": 0
                },
                {
                    "sent": "The Ellis Lipschitz constant and we're going to substitute that with the regularization constant regularization parameter, which I can can adjust, but what I'm going to do is I'm going to use the same one for all the experiments.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I end up with is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An objective function.",
                    "label": 0
                },
                {
                    "sent": "That I have to minimize.",
                    "label": 1
                },
                {
                    "sent": "Is composed of an empirical termina regularizer here as opposed to other, you often see this without the root M. Here I put that in there.",
                    "label": 0
                },
                {
                    "sent": "I let it.",
                    "label": 1
                },
                {
                    "sent": "I tried to modify the bound of the theorem as little as possible to get the objective function, and so you could use other regular.",
                    "label": 0
                },
                {
                    "sent": "This I've seen other regularizers in papers for in similar cases.",
                    "label": 1
                },
                {
                    "sent": "For instance you have you have this bound, so you could use this as a regularizer.",
                    "label": 1
                },
                {
                    "sent": "That would be the trace of this operator, but it's it's a stronger regular.",
                    "label": 0
                },
                {
                    "sent": "I said it makes everything more efficient, but it's going to.",
                    "label": 0
                },
                {
                    "sent": "The results weren't quite as good if I used it, so I just continued using the things that worked well because I didn't have many resources for the experiments.",
                    "label": 0
                },
                {
                    "sent": "And so efficiency is not really a concern of anything really.",
                    "label": 0
                },
                {
                    "sent": "In my talk here.",
                    "label": 0
                },
                {
                    "sent": "Pardon the start.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's transposition.",
                    "label": 0
                },
                {
                    "sent": "Here I join into a transposition of it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the the N for the loss I just take the hinge loss which.",
                    "label": 1
                },
                {
                    "sent": "Graphically depicted here with a margin.",
                    "label": 0
                },
                {
                    "sent": "Of course, the margin is going to give me the Lipschitz constant, and so it will.",
                    "label": 0
                },
                {
                    "sent": "It will work in the pound.",
                    "label": 0
                },
                {
                    "sent": "And he had his loss has some practical properties, some nice properties that it is in a way the smallest one that has this Lipschitz constant and realises this.",
                    "label": 0
                },
                {
                    "sent": "This margin.",
                    "label": 0
                },
                {
                    "sent": "But I could have used the square square loss 2.",
                    "label": 0
                },
                {
                    "sent": "In fact, it might might be very interesting, might actually improve something, but as I said, I just know this works so well that I didn't really think or have time to try anything else.",
                    "label": 0
                },
                {
                    "sent": "If I can rewrite this up there, just substituting this this equality here this identity.",
                    "label": 0
                },
                {
                    "sent": "The UPS nor there was the wrong button.",
                    "label": 1
                },
                {
                    "sent": "Here it is, and so you can see that's linear in the operator T star T The positive square of T. So this whole thing is going to be.",
                    "label": 0
                },
                {
                    "sent": "It's going to be convex if because the Lipschitz the function F, the hinge loss is convex and this is then going to be affine in TS 30 this is also convex because it's a norm, so the whole objective function is convex in the.",
                    "label": 0
                },
                {
                    "sent": "In the operator T starchie.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you've written it again, so I have two 2 possibilities for the algorithm.",
                    "label": 0
                },
                {
                    "sent": "One is just two.",
                    "label": 0
                },
                {
                    "sent": "To minimize this and take the square root, the make the spectral decomposition and take the op.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, which is the positive square root of it.",
                    "label": 0
                },
                {
                    "sent": "I didn't do that because I found that I can.",
                    "label": 0
                },
                {
                    "sent": "I can just minimize this by gradient descent and it will.",
                    "label": 0
                },
                {
                    "sent": "I will still not.",
                    "label": 0
                },
                {
                    "sent": "I will not have any problems with the with the local minima and that theorem.",
                    "label": 0
                },
                {
                    "sent": "I found the proof so nice that I just continued working with it and that's why I discarded this technique.",
                    "label": 0
                },
                {
                    "sent": "But it may actually.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More efficient.",
                    "label": 0
                },
                {
                    "sent": "The other possibility is that, yeah, that's what I do.",
                    "label": 0
                },
                {
                    "sent": "I just do gradient descent and it's in the paper on the on the web page, so there's a very simple proof that I'm not going to have this going to be no stable local minimum of this functional in the function of T groups.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "If there isn't one of the other one so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I only show it to you to demonstrate that you can write it on one page.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's a lot of shorthand letter using, so if you do all the kernel stuff in it, then it's quite a bit of programming that has been done to be done so.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I made some experiments and they were all done with image recognition.",
                    "label": 0
                },
                {
                    "sent": "But so I use the invariant character recognition.",
                    "label": 0
                },
                {
                    "sent": "I work with spatial rotations like in this coil database in face recognition.",
                    "label": 1
                },
                {
                    "sent": "And what I always did is I took a train, the operator T from one group of tasks and then apply it to as in the previous talk to.",
                    "label": 1
                },
                {
                    "sent": "OK and then applied it to another task.",
                    "label": 0
                },
                {
                    "sent": "This is the kernel that I used.",
                    "label": 0
                },
                {
                    "sent": "OK, it's just I didn't do any image specific preprocessing here other than normalization, so the if there would be an arbitrary permutation of pixels, the results would be exactly the same and on it.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I use the same parameters for all.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to show you the results of 1 experiment that I did that was with arbitrarily scaled and rotated characters, and I did so that when I trained the operator, these two are the same and these two are the same.",
                    "label": 0
                },
                {
                    "sent": "It gets told that these are different and these are the same ends on four 4000 examples and I only use the Alpha characters to train it.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And applied it to digits to test it with a single example per class.",
                    "label": 0
                },
                {
                    "sent": "So this could be a complete training set.",
                    "label": 0
                },
                {
                    "sent": "And then I.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applied it to this kind of data and just see what the single example nearest neighbor classifier gives me in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Results there are the results.",
                    "label": 0
                },
                {
                    "sent": "This is the RC curve for the metric as a detector of upper class inequality.",
                    "label": 0
                },
                {
                    "sent": "You can see that the original like if I just use the input data, the Euclidean metric on the input data it has.",
                    "label": 0
                },
                {
                    "sent": "I have cannot recognize anything and this is using the T operator.",
                    "label": 0
                },
                {
                    "sent": "You can see it's doing quite well.",
                    "label": 0
                },
                {
                    "sent": "It reduces the error from 82% which is close to random.",
                    "label": 0
                },
                {
                    "sent": "Guessing too little bit under 10%.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess I should stop now because I'm already over the time limit.",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "If you want to have some questions, yeah, sure, yeah, so I've done.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Compute T. It has to have a nice representation from finite tractor representation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, talk about women.",
                    "label": 0
                },
                {
                    "sent": "How T inherits some kind of funny representation from the data with a finite representation is a consequence of the objective because it becomes clear if you look at it that it has to live on the space the subspace that is spanned by the examples, just like with the SVM's.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing.",
                    "label": 0
                },
                {
                    "sent": "And the complexity is quadratic, right?",
                    "label": 0
                },
                {
                    "sent": "The you know it's super quadratic.",
                    "label": 0
                },
                {
                    "sent": "It's on every step.",
                    "label": 0
                },
                {
                    "sent": "I have have quadratic complexity, so it would be better to use the the thing at all algorithm to enhance the efficiency.",
                    "label": 0
                },
                {
                    "sent": "But as I said it still worked.",
                    "label": 0
                },
                {
                    "sent": "You know if I put it in a closet or when I it and the next day then I eventually get a result.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of efficiency that I'm talking off, it's not, it's.",
                    "label": 0
                },
                {
                    "sent": "That's with the 4000 examples in the training examples.",
                    "label": 0
                },
                {
                    "sent": "Yes, please.",
                    "label": 0
                },
                {
                    "sent": "Because we are losing.",
                    "label": 0
                },
                {
                    "sent": "How do you compute dissent?",
                    "label": 0
                },
                {
                    "sent": "Well, it's almost everywhere differentiable.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss.",
                    "label": 0
                },
                {
                    "sent": "There's only one point, so at that point it is arbitrary.",
                    "label": 0
                },
                {
                    "sent": "Existing yeah this is closing a little bit of this mountain crash ability and assumption.",
                    "label": 0
                },
                {
                    "sent": "What is going to be active constraints where the hinge loss assumes a value that is larger than zero and an active one so and so as long as they are still active.",
                    "label": 0
                },
                {
                    "sent": "Of course there still is still a nonvanishing gradient, so.",
                    "label": 0
                },
                {
                    "sent": "In the trials that I make and we can like a random stochastic gradient descent there.",
                    "label": 0
                },
                {
                    "sent": "So it's only one point where it's not different.",
                    "label": 0
                },
                {
                    "sent": "We have measured zero, it's.",
                    "label": 0
                },
                {
                    "sent": "Fed to pick a configuration is going to be.",
                    "label": 0
                },
                {
                    "sent": "It's going to be differentiable at that point.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, I had to.",
                    "label": 0
                },
                {
                    "sent": "I was racing through that part.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can quickly.",
                    "label": 0
                },
                {
                    "sent": "Here's the kernel, so these are the pixel vectors and normalize them, and I used the same width parameter for all so the coil and eight AT&T database.",
                    "label": 0
                },
                {
                    "sent": "The other things you can read up on the paper.",
                    "label": 0
                },
                {
                    "sent": "The other tests that I made I just took the just picked the most difficult one that was combined.",
                    "label": 0
                },
                {
                    "sent": "Rotation and scale invariants.",
                    "label": 0
                },
                {
                    "sent": "Looks like so.",
                    "label": 0
                },
                {
                    "sent": "I've done it with rotation invariants and linear kernels, and not surprisingly, the components of T are all rotation invariant images.",
                    "label": 0
                },
                {
                    "sent": "Then it's clear that it has to be like that.",
                    "label": 0
                },
                {
                    "sent": "With rotation invariants.",
                    "label": 0
                },
                {
                    "sent": "But of course with combined rotation and scale invariants, the linear kernel will not work anymore, so I did most of most most of the work with a Gaussian RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "There are other questions.",
                    "label": 0
                },
                {
                    "sent": "Best time for fishing.",
                    "label": 0
                },
                {
                    "sent": "OK, So what it says the speaker again thank you.",
                    "label": 0
                }
            ]
        }
    }
}