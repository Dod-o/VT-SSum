{
    "id": "tkv7ljbhnyrap6s2umlf3him7wpphjiq",
    "title": "Automated Character Annotation in Multimedia",
    "info": {
        "author": [
            "Andrew Zisserman, University of Oxford"
        ],
        "published": "Feb. 14, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/mcvc08_zisserman_acam/",
    "segmentation": [
        [
            "OK, so thank you for the invitation to speak here.",
            "I'm going to talk on multimedia application and what I've tried to do is to hit as many of the topics in muscle as I could.",
            "OK, so you can take them off as I go through.",
            "This is joint work with mark everything.",
            "Josef sivic."
        ],
        [
            "And what I want to do is work through a problem and the problem is.",
            "Given some digital material like video material.",
            "Like a team, something on TV or a DVD, I want to automatically label all the characters.",
            "That's that's my goal for this talk.",
            "So.",
            "Something like this?",
            "I want to be able to say."
        ],
        [
            "Hey.",
            "I'm that this this is in the film, one of the characters is Edward, the other character is Vivian.",
            "This is the film Pretty Woman, of course, and I want to do this automatically using readily available information.",
            "OK, that's that's the goal of this talk.",
            "And.",
            "That's really comes down to."
        ],
        [
            "2 problems.",
            "There's a visual problem which is.",
            "Being able to say that faces are using faces throughout the detected faces are of the same person.",
            "So here you see, I'm gonna point week here you see two.",
            "Examples of a person.",
            "It's clear to all of us that these are both Julia Roberts and I want to go through a visual technique.",
            "Let's do this for video.",
            "And so that's a way of establishing that detected faces of the same of person and then as well as establishing the same person.",
            "I want to be able to say who they are and do that automatically.",
            "OK, so this is the character Vivian in the film."
        ],
        [
            "You may be for this project.",
            "You don't need motivation but to give you some motivation why this is a useful thing to do.",
            "There's an amazing amount of video material available now as we know, so use you as far as I could tell, has about 70 million videos.",
            "There are lots of old film and video archives you have personal mpegs that you're generating, and they're not searchable on their visual content.",
            "The only way they're searchable, the moment is if there's text annotation provided, and most of these don't have any text annotation.",
            "So the idea here is if we can do visual matching on faces, at least then we can search on faces and humans are very important in videos.",
            "And more than that, once we start producing text associated with faces associated with characters, we can start moving onto generating summaries and narratives.",
            "Is the first step towards automation of generating some reason narratives for videos?",
            "OK, so that's the background motivation so I could do a pointer.",
            "Is Erica."
        ],
        [
            "Some sort of laser device would be good.",
            "Thank you again.",
            "How do I use it?",
            "Press this.",
            "Right, so I'm going to go through four stages in how to solve this problem automatically generating examinations.",
            "I'll show you.",
            "First of all, we're going to get the supervision from the text supervision, and that's going to be a combination of the subtitles that come with the video material and scripts.",
            "So that's the text part.",
            "Then the visual part.",
            "I'll describe our representation of faces in videos and how we match detected faces to establish whether the same person or not.",
            "Then that's the visual part.",
            "The text part.",
            "Then there's a learning part, and this is essentially going to be a semi supervised learning problem with noisy supervision.",
            "And then depending how long ago and for the piece of extensions OK."
        ],
        [
            "So first of all, what this is like many of you are familiar with the work that was done at Berkeley in David Foresights Group where they took web pages from news sites and on the web pages there were images and there was text and the problem they looked at was given images like this and detecting faces in these images and text which might contain the names of the people, establish the correspondence between here in this case George Bush and this face here.",
            "OK, there's an ambiguity, and they looked at taking a large number of.",
            "Web pages and because they have lots of examples they could solve this correspondence problem between names and faces.",
            "OK, So what I'm going to talk about is essentially a generalization of this video.",
            "So instead of just having static, images are going to video and we're going to have text to be similar problems, But what you'll see is that by using video actually we can simplify the problem.",
            "You'd expect in video you'd have more information, and indeed we do, and that lets us get a better solution."
        ],
        [
            "So.",
            "Part one, the text.",
            "So for this I'm going to have a running example and I've chosen it."
        ],
        [
            "The series here, which is going to be Buffy the Vampire Slayer which has.",
            "I finished now running on television, but it's available on DVD and it comes with subtitles.",
            "It satisfies what I need, so just as A to involve you, how many people have not seen Buffy the Vampire Slayer?",
            "Hands off of you not seen it.",
            "OK, so that means about half of you have seen it, which is good.",
            "Again, another missing out.",
            "So this is the goal then is that given this is these are some of the characters principal characters in Buffy the Vampire Slayer we want to when they appear on the screen, identify them.",
            "Anne and."
        ],
        [
            "Buffy is available on DVD and it comes with subtitles.",
            "OK, so that's part one of how we're going to get the supervision to label the people.",
            "And subtitles.",
            "This is typically what the subtitle might say.",
            "This is this character speaking, and it says you're entering a new world, one for which I myself am not entirely prepared base or pompous English speaking, but what this is saying is what's being said.",
            "It's not saying who is saying it.",
            "OK, so we're missing some information here.",
            "All we have is what's being said and what we're going to need for supervision is who is saying this."
        ],
        [
            "And the way we get that is to make use of fan websites so any television series which is popular and any feature film will have a fan website associated with it.",
            "So it's just a gift from the Internet.",
            "And Buffy has websites like this and on the websites people have the transcripts of every episode.",
            "Similarly for feature films would be transcripts of the scripts?",
            "OK, so this is always available.",
            "And then given the subtitles and given the transcripts, we can do a text alignment completely, standard text alignment using dynamic."
        ],
        [
            "Time warping like this, so we simply take all the script words from the transcript of the subtitle words and generate a path through and that doesn't alignment of the two."
        ],
        [
            "So here's an example of what we end up with.",
            "So on the left is the information from the subtitles.",
            "And this is an argument between two vampires.",
            "In fact, who hear the subtitles and we don't know who's saying what on the right is the script, and by aligning this we found that this was said by Harmony one of vampires.",
            "This is this said by two people.",
            "The first line by 1 Vampire II by another etc.",
            "OK, so now we can pull over this information from the script onto the subtitles and what we end up with is we know who is saying what and when.",
            "OK so this is the game.",
            "Also been automatically, so this is going to be text supervision.",
            "We're going to have OK, which is provided for us essentially."
        ],
        [
            "Tree.",
            "And this is just as an aside, this is a very useful source of information because the fans are obsessive and in the transcripts they tend to write.",
            "Where things are happening, the locations what's going on, the actions, what the characters are doing, the camera motion, whether it's panning.",
            "So there's lots of information available there.",
            "If you want to do automated supervision for vision for video."
        ],
        [
            "OK, so where we get to then is what I'm showing here.",
            "Is the detective faces in each of three frames and the box at the top is.",
            "What from this synchronized subtitles?",
            "The counter that's speaking.",
            "OK, so in this shot here there's a counter called Tara speaking.",
            "So this character called Star is speaking.",
            "And here the subtitles synchronized with the script save character called Buffy is speaking in here a character called Tara speaking OK, so this is the information we have from the text and the question is is it useful?",
            "So there are problems, so here there are two characters detected.",
            "In fact, the one the smaller one at the back is Tyra, and this character is not toilet.",
            "So we don't know that we don't.",
            "We haven't solved the problem.",
            "Likewise, here account called Buffy is speaking, and in fact this is Buffy and her face was not detected.",
            "So again we haven't solved the problem here and here.",
            "The character called Tyler is speaking.",
            "We detected two faces but neither Visa Tara because this is a reaction shot.",
            "She's off camera, so actually it appears that this.",
            "This sort of information hasn't solved our problem and it hasn't.",
            "It's going to help and I'll come to how it helps in a moment, but it hasn't done the whole deal.",
            "So what I'm going to do is introduce two more sources of information, so the first one is going to be.",
            "The visual matching faces, so here the annotation is no use what we need to do is learn in another shot who this character is and learn another shot through this countries as Buffy and then use visual matching or faces to annotate this.",
            "This shot, OK, that's what we're going to do.",
            "A matching faces and the second thing we're going to do is speaker detection to improve the level of supervision from the text.",
            "So I go through now the visual matching."
        ],
        [
            "Say official matches on faces here.",
            "It could be on other things that some phases."
        ],
        [
            "And.",
            "Working with this sort of material is difficult because it's very uncontrolled.",
            "This is typically called faces in the wild material you have changes in pose, you have changes in lighting have partial occlusion.",
            "You have changes in expression, so it's very difficult to much more difficult than standard databases, which often have a neutral background and well controlled expressions and impose.",
            "We want to be able to match faces like this."
        ],
        [
            "Now the idea we're going to use is something we can do in video.",
            "If I asked you whether these two people are the same, it would be quite difficult to decide if this person is the same as this person or not.",
            "But if I give you."
        ],
        [
            "This evidence, so I tell you that all the people in this row.",
            "Are the same person and all the people in this row are the same person and I think it's clear that these two people are not the same.",
            "OK, you just got much more information.",
            "So we compare this.",
            "So all these people say difficult to tell are these two people the same?",
            "They're not the same.",
            "OK, anybody know?",
            "If you want some audience interaction again, why this is more difficult than for people who haven't seen this before?",
            "Why this is a particularly difficult problem.",
            "Any film fans here that know about these things?",
            "OK, so this is Bill Murray and this is his half brother.",
            "OK, so these people actually related.",
            "They're both actors in the film Groundhog Day, but there is actually some family link between them, so it's really difficult anyway.",
            "But the point about this is that if you have sets of faces, it's easier to judge whether or not faces match."
        ],
        [
            "Now we've got video here, so it's.",
            "Get some master code.",
            "So if we have the video, we have many many frames.",
            "And we're in a position to detect faces and align them automatically.",
            "So here, so it's very easy for us to gather sets of faces, and that's what I'm going to go through.",
            "But from video, unlike in still images, we can gather these sets of phases, which in turn is going to make the matching of people easier, not solved."
        ],
        [
            "Yeah.",
            "So I'll go through three steps on how we do this.",
            "We're going to get sets of faces.",
            "Associate them by sort of tracking.",
            "Represent them by feature vectors and then match face set to a face set using these sets of feature vectors.",
            "That's the computer."
        ],
        [
            "And part of this talk."
        ],
        [
            "So first of all, on faces we run a face detector.",
            "Kisses Standard open CV face detector.",
            "So frontal face detector so when people turn away we don't get their face.",
            "OK this is running a face detector independently on each frame, and here you're seeing.",
            "Two faces detected and as a human you associate the faces, but at the moment there's no knowledge that these face detections belong to two people.",
            "They could belong to 10 people or one person.",
            "There's no knowledge of that.",
            "How we associate them is by."
        ],
        [
            "I. Tracking so, but we're not going to track the faces we're going to use some of the many techniques have been developed in computer vision, tracking tracking.",
            "Big thing in computer vision.",
            "Many years of developing methods.",
            "What we do is."
        ],
        [
            "Take come to here, we take a standard track of computer vision, which is called the KLT tracker, which simply tracks feature points.",
            "Doesn't know anything about faces, but it's been designed for many years.",
            "It's very fast.",
            "We can start tracking in this frame, see where the tracks go, and if the tracks link up with another detected face, then we can deem that these are two face two face detections belong to the same person.",
            "OK, so this is completely independent evidence.",
            "We detect faces in every frame and then we decide later by tracking independently whether these detections are the same person or."
        ],
        [
            "So what that means is we get to this.",
            "So here the faces sections are color coded and that means that we've, by tracking independently using these points, decided that there are two people here and that all of the ones colored red belong to Julia Roberts and all the ones colored other kind of Richard Gere, OK.",
            "So that's all being done by tracking the face detection."
        ],
        [
            "Faces so then we get to the situation where we have many face detections associated because we're detecting every frame.",
            "Say for example."
        ],
        [
            "For this shot.",
            "So again, the color coded and down the bottom.",
            "These are all these faces that have been associated for this character.",
            "All these faces.",
            "Each of these characters, so we get hundreds of faces in sets automatically associated.",
            "This straightforward thing to do, and now what we want to do is to build descriptors for these sets and use those to match the faces or see if the faces match."
        ],
        [
            "Alright, so our face representation.",
            "As should be clear, if he's not thinking about faces, you have pose variations.",
            "You have size variations that I said what you want to do is to some extent try and normalize out or normalize the faces to remove these pose variations, and we're going to do that by detecting facial features, eyes, nose, mouth, other features in the face which are always there and use that to transform the detected face to a standard position."
        ],
        [
            "So to show you what I mean, here's a a shot an you're seeing in red.",
            "The detective features and in each frame we fund this detective features and then map those onto a standard position to transform the face interest.",
            "Fairly standard position."
        ],
        [
            "And you can see I don't really go into how it's done.",
            "It's done.",
            "We do this by learning facial features and learning that."
        ],
        [
            "Decorations.",
            "What we call a consolation or.",
            "Capital structure model K."
        ],
        [
            "The effect is is impressive.",
            "So what I'm going to show you here is the original shot, then the detected phase will be here and this will be the face after normalization.",
            "So what you can see in the center one which is the role detection, is this sort of pulsing, and that's typical of these face detectors that their scale is discretized, so you get it switches, one scalar, another pyramid.",
            "You get this pulsing of scales, whereas that's removed after the after taking facial features, mappings and position, the detection is stabilized.",
            "Both impose and in scale, but you can see this alot of not perfect.",
            "You haven't completely removed all motion, but this is what we want to learn from.",
            "From this we want to learn the representation of this character."
        ],
        [
            "OK, and the way we do that then we've detected the facial features around the facial features and that several other points on the face.",
            "We build a feature vector.",
            "And for this it's not."
        ],
        [
            "Important details, but actually we measure what's called a SIFT descriptor, which is a array of orientations measured around each of these facial features that we record."
        ],
        [
            "And so for each phase we have some large feature vectors records the distribution of orientations.",
            "Maybe it's not just 360 dimensions for each phase?",
            "OK, so each phase is represented by 360 dimensional vector."
        ],
        [
            "And now we have sets of faces."
        ],
        [
            "And each of these faces we have these three 6 dimensional vectors and what we want to do now is to match a set of vectors to a set of vectors.",
            "And I saw that this afternoon is going to talk on this as well.",
            "That would be interesting to follow."
        ],
        [
            "We take a very simple measure, so imagine that these faces generate points in this 360 dimensional space.",
            "OK, so for this set of faces we have one set of vectors.",
            "This set of vectors.",
            "We want to judge whether these are the same person."
        ],
        [
            "And we do that simply by using the min, min distance.",
            "OK, so we find the points.",
            "If you look at the blue set in the green set, we find the points which are closest together and that gives the distance between these two sets.",
            "The red set and the blue set will be a much larger distance.",
            "OK, so within some threshold we can judge that these two are the same person and this is a different person.",
            "Using this min distance between the sets."
        ],
        [
            "Right, so I'll give you an example.",
            "Now the whole process, so I will show you a sequence from.",
            "Office Vampire Slayer and then work you through the processing.",
            "We do so here.",
            "There should be some sound.",
            "So there should be some sound.",
            "So you see what they're doing.",
            "Their video jerkiness is coming apart from the laptop that's not in original, but they're sort of dancing around each other while they have breakfast.",
            "OK, and we want to be able to establish whether the same people are not in there.",
            "Actually five shots here, so the camera is changing to profile views as well."
        ],
        [
            "So now I'm just show you the processing.",
            "So first of all, here's the raw face detection, so this is detecting faces on every frame and what you have here is X along the bottom.",
            "So X of the frame and then the vertical direction is time.",
            "So here, as the bar goes up, you're seeing the face detections on original video.",
            "At the moment these are colored white because we don't know whether the same people or not.",
            "This is just the raw detections.",
            "OK, and now we."
        ],
        [
            "Actor faces, so now they're color coded according to where they've been trapped together.",
            "But you see at this point.",
            "This character is Buffy turns away from the camera, so we no longer have any frontal detections, so that tracks going to break.",
            "We carry on.",
            "Then one person walks in front of another, so again the tracks going to break.",
            "They can't track through occlusions like this.",
            "So after the tracking, we've reduced the.",
            "Number of feet.",
            "They're not just hundreds of Detective faces now we've associated into a handful of tracks, but there are still several tracks for each person.",
            "That's the situation at this point."
        ],
        [
            "And."
        ],
        [
            "Now we're going to.",
            "For each of these, each of these are the set of phases of maybe 100 faces.",
            "Each of these tracks.",
            "Now we match them using motive.",
            "Clustering using this min, min distance and as we do that then it correctly.",
            "Comes from.",
            "I'm just a set of tracks."
        ],
        [
            "Here too.",
            "Correctly associating the face detections of the people here and we're down to just three.",
            "Associations which correspond to three people.",
            "And this is very typical of the sort of processing that happens that face the.",
            "We detect faces independently when we link them.",
            "We don't get false positives.",
            "This very robust when we.",
            "Then use the minimum distance to join up the sets of faces.",
            "That works very well indeed, like 99% accuracy.",
            "OK, so this is this is now the sort of data that we can generate automatically.",
            "Right?"
        ],
        [
            "Now I'm going to.",
            "Come back to this slide I had before on the ambiguity, so we've gone through the computer part.",
            "We've done the visual processing.",
            "And now I want to look at improving the annotation.",
            "So we had this problem that even though.",
            "The text was giving us suggestions for who people were these ambiguities?",
            "Now, how can we improve this?",
            "So you see, in this case there are two characters somebody speaking what we're going to look at is using speaker detection to decide if the person is really speaking."
        ],
        [
            "So here's an example.",
            "If the subtitles and align script are saying that the character Willow is speaking.",
            "So if we can determine in the shot that this character is actually speaking, then we can really believe that this characters Willow.",
            "Whereas if this character is not speaking then we would ignore the annotation."
        ],
        [
            "So how do we do that is quite simple.",
            "We take these facial features we detected more facial features and I showed you before.",
            "So we have many facial features we can pull out the mouth through a shot and that's what's being shown here.",
            "We put out the mouth and we can.",
            "Simply look at the amplitude of the change between one frame and the next.",
            "We do that using cross correlation between one frame in the next under translation.",
            "So we find the best cross correlation possible between here and here under small translations in between.",
            "Here and here, measure the sum of squared distances, and that's what you're.",
            "Sing down below and if this is above a certain threshold, then we deem that the person is speaking.",
            "I'm sure you can think of ways of improving this, but actually we found this was the best way so far in the ways we've tried.",
            "If it's below a threshold or not significant, then we don't classes, don't know.",
            "OK, so only when we're fairly sure do we deem this person."
        ],
        [
            "Speaking.",
            "And this works surprisingly well.",
            "Here's an example where the person is not speaking, but there supposed change.",
            "So it's quite a large change going through the shot, and we correctly classify that as not speaking.",
            "You might think that smiling would generate a false positive, so here this character smiling, but again we we correctly classified as not speaking."
        ],
        [
            "But it's not perfect.",
            "Here's a character opening her mouth in horror or surprise, and we incorrectly say that she's speaking.",
            "So it works.",
            "We choose an operating point.",
            "You'll see later where we can.",
            "But 99% of the tracks that we say are speaking are correct.",
            "Right?"
        ],
        [
            "What does that do?",
            "The effect of that is where this is correct.",
            "So here we had two characters.",
            "We find that this one is speaking, so we now have a label for this character, and we know this one doesn't have a label here.",
            "We had the character Buffy was labeled, she wasn't detected.",
            "This character is not detected speaking, so correctly, we don't have any information here, but it's not incorrect information here.",
            "We had this reaction shot.",
            "And we find that neither the speaking so we don't incorrectly label them with this label Tara.",
            "OK, so this is what this is done is really improved.",
            "The strength of our supervision.",
            "It's not complete, but it's now much stronger.",
            "Going from here."
        ],
        [
            "Now moving on to the third part of the talk, which is the learning part.",
            "OK, switch on the vision.",
            "We've got our text annotation for improved our text annotation.",
            "Now how do we combine all these things together?",
            "So what we want to do is we've got these tracks and essentially what we want to do is to label all these tracked faces so our granularity is a tracker faces a set of faces, maybe 100 faces linked together, and each one of them will be hundreds of these through a video.",
            "We want to label them with the character."
        ],
        [
            "Right now some of the tracks are labeled, so for the ones where they've been given a single label by the speaker detection, we're going to treat those as being correct and being exemplars.",
            "So we're going to the moment in true.",
            "Assume that that there are positive set we've labeled them.",
            "And then the other tracks which don't have a label, and so this now becomes like a semi supervised learning problem.",
            "You have some data which is labeled and some which is not.",
            "And as you'll see as well, it's noisy."
        ],
        [
            "It's hard and that and so we take a simple approach, which is to use the nearest neighbor classifier.",
            "So here's one track of this character, Buffy, which has been labeled.",
            "Here's another track which is being labeled, and these are some of the other characters which have been labeled.",
            "We have a new track which is unlabeled and what we do is associated with the nearest track.",
            "Using this min distance I showed before and assign it that label.",
            "OK, so it's a simple classifier, but we want to capture the fact that some of the classifications are better than others.",
            "So for example, if we had.",
            "I'm a track here.",
            "We could be very certain that it was this character Buffy as if it was here where it's close to two different characters were not sure which one it is so we can use nearest neighbor classifier, but we're going to associate a confidence with that."
        ],
        [
            "And we do that by making like a mock posterior.",
            "Where we normalize the minimum distance by the summer distances to all the other characters OK, and what happens then is if we're very sure that it's.",
            "One character will have a high confidence, probably near one, and if it's confused it will have a low confidence and we can use that for ranking.",
            "And we can also refuse to predict, so that if something has a low confidence, we can say, well, we're not going to predict what that track is."
        ],
        [
            "I am.",
            "So we've tested this on two episodes of Vampire Slayer.",
            "The statistics are here.",
            "We've got 130,000 frames, 50,000 faces detected 1000 tracks, so we want to label 1000 tracks on the right you see a Gallery of the people we want to label.",
            "These are the main characters from those episodes.",
            "And."
        ],
        [
            "His what we get out of it.",
            "So this is like plug and play.",
            "We Simply put in the DVD, supply the script and this is what comes out.",
            "We can see we label many of the characters, even though their faces might be small and the pose might be unusual and.",
            "Check."
        ],
        [
            "And here you can see the lighting.",
            "The expressions we can do as well.",
            "OK, so this is what can be done automatically as an overview."
        ],
        [
            "Some statistics were pressing position recall here, so I think most of you know president recall is but essentially a perfect graph would be a straight line up in the right hand corner here.",
            "Anne, and if you just concentrate on this graph is one episode.",
            "This is the other episode you can see that we do very well and then die off slowly ordering these in this confidence level.",
            "So remember for each of the tracks we had a probability associated with it and that gives us our ranking here.",
            "And so statistic would be.",
            "At around up to about 80% recall, we get 80% precision, which is obviously very good being done automatically."
        ],
        [
            "And I'm going to show you typical results, so I'm going to run a sequence and what you'll see is many shots.",
            "The labels of the characters all generated automatically, and there are two errors which will be in red.",
            "Get the sound on this as well."
        ],
        [
            "OK, so this is what we produced.",
            "Skip this to 6 bit for you.",
            "The important thing here on this slide is that if we just use the subtitles and didn't do the speaker detection, we only get 45% correct and by using factory by using the subtitles with the speaker detection we double the precision.",
            "I guess what the visual speaker detection gives us."
        ],
        [
            "One point map for learning.",
            "I guess you saw.",
            "We used a simple nearest neighbor classifier.",
            "But some of these labels are wrong.",
            "As I've said, the speaker detection isn't perfect, so we've really gotta noisy label problem, summer incorrect and support vector machines are designed to deal with a certain amount of noise in the training data.",
            "So we can take the nearest neighbor classifier, put it on one side, replace it by an SVM classifier.",
            "OK, so now it's still.",
            "Our units are tracks with training data.",
            "Each training data is a track and we have positive training data for the characters, which is labeled ones.",
            "We do a one versus all.",
            "SVM for each character in turn and look at this graph here that the dashed line here is the nearest neighbor classifier shows you before the blue is after running through an SVM.",
            "So you can see that the SVM does overcome some of the noise in that rate in the training data.",
            "And the rate is, if we had, if the if we had perfect classification from speaker detection, OK, that's the best we could do so using something which is tolerant to noise in the training data is better.",
            "Which back in SVM is better than the nearest neighbor classifier?"
        ],
        [
            "I'm so."
        ],
        [
            "I'm just going to."
        ],
        [
            "Show you one extension.",
            "I've got some just that's the main part of the problem.",
            "We can label DVD's, we can label television material with subtitles.",
            "Where would this go next?",
            "One thing."
        ],
        [
            "Do is deal with profiles OK so we can detect profiles.",
            "We can run exactly the same ideas.",
            "Again.",
            "We can do facial feature detection profile.",
            "We can do speaker detection in profile."
        ],
        [
            "And we can.",
            "Track between profiles and frontal.",
            "So here you see a sequence where we're switching between profile and frontal trackers using this detection independently each frame and then linking by KLT tracks as before.",
            "So now we can find profile matches another.",
            "Secret shop Switcher have track Switcher profiles, and they're labeled.",
            "We can pull them into shots which have frontal and profile.",
            "We can switch between the two.",
            "OK, so you can extend this in terms of coverage."
        ],
        [
            "Miss this come to the end.",
            "So to conclude, then.",
            "We've done what we set out to do this is we walk through this problem where we would go next in terms of improvements.",
            "Is building up a larger data set of videos so we can go between episodes go between movies once we've learned.",
            "Face models and classifiers for different places.",
            "Extend the visual description.",
            "Extend the classification so we're going to see this often.",
            "As I said, some classification of sets of vectors and also moved from just recognizing faces to recognizing other things, recognizing emotions, recognizing actions, and we're going to talk by Ivan leftover this afternoon on recognizing actions.",
            "OK, so this idea can just run and run.",
            "This is a sensible, sensible question.",
            "We didn't need to in terms of the video using the vision was sufficient for our needs, but yes we could.",
            "It could help us.",
            "It could help us determine the gender of the people.",
            "It could help us, but there's only so much you can do in one at one go, but it's a good thing to integrate as well.",
            "Yeah, so we've tried several features, So what we were using originally is sift descriptors around each of the facial features.",
            "So the vector would just be concatenated.",
            "Sift descriptors and there we were using usually shift as a four by four grid.",
            "We were using a three by three grid.",
            "We found that we looked at performance over different grid resolutions.",
            "We found a three by three grid with shift work better.",
            "We've also used raw intensities, and we've used gradients.",
            "We've compared all of these are current.",
            "Work finds out the safe descriptors are still best.",
            "Testing in.",
            "So the results I showed it is within an episode we we've got.",
            "The text we have.",
            "Say 500 tracks.",
            "Yes, why do you actually have to have results for that?",
            "So here I didn't show them, but this is going from training on one episode and testing on the other episode.",
            "And this is a sort of results we get, so it's obviously much more demanding task as we're just using visual models.",
            "Here is comparing three different types of.",
            "What is several things changing here that the blue one, which is our best one?",
            "That moment is using some extra facial feature normalization for intensities due to build trigs?",
            "And then a random firm classifier, but that's maybe not so important.",
            "So this is our results going from one episode to the other."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so thank you for the invitation to speak here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk on multimedia application and what I've tried to do is to hit as many of the topics in muscle as I could.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can take them off as I go through.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with mark everything.",
                    "label": 1
                },
                {
                    "sent": "Josef sivic.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I want to do is work through a problem and the problem is.",
                    "label": 0
                },
                {
                    "sent": "Given some digital material like video material.",
                    "label": 0
                },
                {
                    "sent": "Like a team, something on TV or a DVD, I want to automatically label all the characters.",
                    "label": 0
                },
                {
                    "sent": "That's that's my goal for this talk.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "I want to be able to say.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "I'm that this this is in the film, one of the characters is Edward, the other character is Vivian.",
                    "label": 0
                },
                {
                    "sent": "This is the film Pretty Woman, of course, and I want to do this automatically using readily available information.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's the goal of this talk.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's really comes down to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2 problems.",
                    "label": 0
                },
                {
                    "sent": "There's a visual problem which is.",
                    "label": 0
                },
                {
                    "sent": "Being able to say that faces are using faces throughout the detected faces are of the same person.",
                    "label": 0
                },
                {
                    "sent": "So here you see, I'm gonna point week here you see two.",
                    "label": 0
                },
                {
                    "sent": "Examples of a person.",
                    "label": 0
                },
                {
                    "sent": "It's clear to all of us that these are both Julia Roberts and I want to go through a visual technique.",
                    "label": 0
                },
                {
                    "sent": "Let's do this for video.",
                    "label": 0
                },
                {
                    "sent": "And so that's a way of establishing that detected faces of the same of person and then as well as establishing the same person.",
                    "label": 1
                },
                {
                    "sent": "I want to be able to say who they are and do that automatically.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the character Vivian in the film.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You may be for this project.",
                    "label": 0
                },
                {
                    "sent": "You don't need motivation but to give you some motivation why this is a useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "There's an amazing amount of video material available now as we know, so use you as far as I could tell, has about 70 million videos.",
                    "label": 1
                },
                {
                    "sent": "There are lots of old film and video archives you have personal mpegs that you're generating, and they're not searchable on their visual content.",
                    "label": 0
                },
                {
                    "sent": "The only way they're searchable, the moment is if there's text annotation provided, and most of these don't have any text annotation.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is if we can do visual matching on faces, at least then we can search on faces and humans are very important in videos.",
                    "label": 0
                },
                {
                    "sent": "And more than that, once we start producing text associated with faces associated with characters, we can start moving onto generating summaries and narratives.",
                    "label": 1
                },
                {
                    "sent": "Is the first step towards automation of generating some reason narratives for videos?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the background motivation so I could do a pointer.",
                    "label": 0
                },
                {
                    "sent": "Is Erica.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some sort of laser device would be good.",
                    "label": 0
                },
                {
                    "sent": "Thank you again.",
                    "label": 0
                },
                {
                    "sent": "How do I use it?",
                    "label": 0
                },
                {
                    "sent": "Press this.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'm going to go through four stages in how to solve this problem automatically generating examinations.",
                    "label": 0
                },
                {
                    "sent": "I'll show you.",
                    "label": 0
                },
                {
                    "sent": "First of all, we're going to get the supervision from the text supervision, and that's going to be a combination of the subtitles that come with the video material and scripts.",
                    "label": 0
                },
                {
                    "sent": "So that's the text part.",
                    "label": 0
                },
                {
                    "sent": "Then the visual part.",
                    "label": 0
                },
                {
                    "sent": "I'll describe our representation of faces in videos and how we match detected faces to establish whether the same person or not.",
                    "label": 0
                },
                {
                    "sent": "Then that's the visual part.",
                    "label": 0
                },
                {
                    "sent": "The text part.",
                    "label": 0
                },
                {
                    "sent": "Then there's a learning part, and this is essentially going to be a semi supervised learning problem with noisy supervision.",
                    "label": 0
                },
                {
                    "sent": "And then depending how long ago and for the piece of extensions OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, what this is like many of you are familiar with the work that was done at Berkeley in David Foresights Group where they took web pages from news sites and on the web pages there were images and there was text and the problem they looked at was given images like this and detecting faces in these images and text which might contain the names of the people, establish the correspondence between here in this case George Bush and this face here.",
                    "label": 0
                },
                {
                    "sent": "OK, there's an ambiguity, and they looked at taking a large number of.",
                    "label": 0
                },
                {
                    "sent": "Web pages and because they have lots of examples they could solve this correspondence problem between names and faces.",
                    "label": 1
                },
                {
                    "sent": "OK, So what I'm going to talk about is essentially a generalization of this video.",
                    "label": 0
                },
                {
                    "sent": "So instead of just having static, images are going to video and we're going to have text to be similar problems, But what you'll see is that by using video actually we can simplify the problem.",
                    "label": 0
                },
                {
                    "sent": "You'd expect in video you'd have more information, and indeed we do, and that lets us get a better solution.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Part one, the text.",
                    "label": 0
                },
                {
                    "sent": "So for this I'm going to have a running example and I've chosen it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The series here, which is going to be Buffy the Vampire Slayer which has.",
                    "label": 1
                },
                {
                    "sent": "I finished now running on television, but it's available on DVD and it comes with subtitles.",
                    "label": 0
                },
                {
                    "sent": "It satisfies what I need, so just as A to involve you, how many people have not seen Buffy the Vampire Slayer?",
                    "label": 0
                },
                {
                    "sent": "Hands off of you not seen it.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means about half of you have seen it, which is good.",
                    "label": 0
                },
                {
                    "sent": "Again, another missing out.",
                    "label": 0
                },
                {
                    "sent": "So this is the goal then is that given this is these are some of the characters principal characters in Buffy the Vampire Slayer we want to when they appear on the screen, identify them.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buffy is available on DVD and it comes with subtitles.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's part one of how we're going to get the supervision to label the people.",
                    "label": 0
                },
                {
                    "sent": "And subtitles.",
                    "label": 0
                },
                {
                    "sent": "This is typically what the subtitle might say.",
                    "label": 0
                },
                {
                    "sent": "This is this character speaking, and it says you're entering a new world, one for which I myself am not entirely prepared base or pompous English speaking, but what this is saying is what's being said.",
                    "label": 0
                },
                {
                    "sent": "It's not saying who is saying it.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're missing some information here.",
                    "label": 0
                },
                {
                    "sent": "All we have is what's being said and what we're going to need for supervision is who is saying this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way we get that is to make use of fan websites so any television series which is popular and any feature film will have a fan website associated with it.",
                    "label": 0
                },
                {
                    "sent": "So it's just a gift from the Internet.",
                    "label": 0
                },
                {
                    "sent": "And Buffy has websites like this and on the websites people have the transcripts of every episode.",
                    "label": 0
                },
                {
                    "sent": "Similarly for feature films would be transcripts of the scripts?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is always available.",
                    "label": 0
                },
                {
                    "sent": "And then given the subtitles and given the transcripts, we can do a text alignment completely, standard text alignment using dynamic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time warping like this, so we simply take all the script words from the transcript of the subtitle words and generate a path through and that doesn't alignment of the two.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example of what we end up with.",
                    "label": 1
                },
                {
                    "sent": "So on the left is the information from the subtitles.",
                    "label": 0
                },
                {
                    "sent": "And this is an argument between two vampires.",
                    "label": 0
                },
                {
                    "sent": "In fact, who hear the subtitles and we don't know who's saying what on the right is the script, and by aligning this we found that this was said by Harmony one of vampires.",
                    "label": 0
                },
                {
                    "sent": "This is this said by two people.",
                    "label": 0
                },
                {
                    "sent": "The first line by 1 Vampire II by another etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we can pull over this information from the script onto the subtitles and what we end up with is we know who is saying what and when.",
                    "label": 0
                },
                {
                    "sent": "OK so this is the game.",
                    "label": 1
                },
                {
                    "sent": "Also been automatically, so this is going to be text supervision.",
                    "label": 1
                },
                {
                    "sent": "We're going to have OK, which is provided for us essentially.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tree.",
                    "label": 0
                },
                {
                    "sent": "And this is just as an aside, this is a very useful source of information because the fans are obsessive and in the transcripts they tend to write.",
                    "label": 1
                },
                {
                    "sent": "Where things are happening, the locations what's going on, the actions, what the characters are doing, the camera motion, whether it's panning.",
                    "label": 1
                },
                {
                    "sent": "So there's lots of information available there.",
                    "label": 0
                },
                {
                    "sent": "If you want to do automated supervision for vision for video.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so where we get to then is what I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "Is the detective faces in each of three frames and the box at the top is.",
                    "label": 0
                },
                {
                    "sent": "What from this synchronized subtitles?",
                    "label": 0
                },
                {
                    "sent": "The counter that's speaking.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this shot here there's a counter called Tara speaking.",
                    "label": 0
                },
                {
                    "sent": "So this character called Star is speaking.",
                    "label": 0
                },
                {
                    "sent": "And here the subtitles synchronized with the script save character called Buffy is speaking in here a character called Tara speaking OK, so this is the information we have from the text and the question is is it useful?",
                    "label": 0
                },
                {
                    "sent": "So there are problems, so here there are two characters detected.",
                    "label": 0
                },
                {
                    "sent": "In fact, the one the smaller one at the back is Tyra, and this character is not toilet.",
                    "label": 0
                },
                {
                    "sent": "So we don't know that we don't.",
                    "label": 0
                },
                {
                    "sent": "We haven't solved the problem.",
                    "label": 0
                },
                {
                    "sent": "Likewise, here account called Buffy is speaking, and in fact this is Buffy and her face was not detected.",
                    "label": 0
                },
                {
                    "sent": "So again we haven't solved the problem here and here.",
                    "label": 0
                },
                {
                    "sent": "The character called Tyler is speaking.",
                    "label": 0
                },
                {
                    "sent": "We detected two faces but neither Visa Tara because this is a reaction shot.",
                    "label": 0
                },
                {
                    "sent": "She's off camera, so actually it appears that this.",
                    "label": 0
                },
                {
                    "sent": "This sort of information hasn't solved our problem and it hasn't.",
                    "label": 0
                },
                {
                    "sent": "It's going to help and I'll come to how it helps in a moment, but it hasn't done the whole deal.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is introduce two more sources of information, so the first one is going to be.",
                    "label": 0
                },
                {
                    "sent": "The visual matching faces, so here the annotation is no use what we need to do is learn in another shot who this character is and learn another shot through this countries as Buffy and then use visual matching or faces to annotate this.",
                    "label": 0
                },
                {
                    "sent": "This shot, OK, that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "A matching faces and the second thing we're going to do is speaker detection to improve the level of supervision from the text.",
                    "label": 0
                },
                {
                    "sent": "So I go through now the visual matching.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say official matches on faces here.",
                    "label": 0
                },
                {
                    "sent": "It could be on other things that some phases.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Working with this sort of material is difficult because it's very uncontrolled.",
                    "label": 0
                },
                {
                    "sent": "This is typically called faces in the wild material you have changes in pose, you have changes in lighting have partial occlusion.",
                    "label": 1
                },
                {
                    "sent": "You have changes in expression, so it's very difficult to much more difficult than standard databases, which often have a neutral background and well controlled expressions and impose.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to match faces like this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the idea we're going to use is something we can do in video.",
                    "label": 0
                },
                {
                    "sent": "If I asked you whether these two people are the same, it would be quite difficult to decide if this person is the same as this person or not.",
                    "label": 0
                },
                {
                    "sent": "But if I give you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This evidence, so I tell you that all the people in this row.",
                    "label": 0
                },
                {
                    "sent": "Are the same person and all the people in this row are the same person and I think it's clear that these two people are not the same.",
                    "label": 0
                },
                {
                    "sent": "OK, you just got much more information.",
                    "label": 0
                },
                {
                    "sent": "So we compare this.",
                    "label": 0
                },
                {
                    "sent": "So all these people say difficult to tell are these two people the same?",
                    "label": 1
                },
                {
                    "sent": "They're not the same.",
                    "label": 0
                },
                {
                    "sent": "OK, anybody know?",
                    "label": 0
                },
                {
                    "sent": "If you want some audience interaction again, why this is more difficult than for people who haven't seen this before?",
                    "label": 0
                },
                {
                    "sent": "Why this is a particularly difficult problem.",
                    "label": 0
                },
                {
                    "sent": "Any film fans here that know about these things?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is Bill Murray and this is his half brother.",
                    "label": 0
                },
                {
                    "sent": "OK, so these people actually related.",
                    "label": 0
                },
                {
                    "sent": "They're both actors in the film Groundhog Day, but there is actually some family link between them, so it's really difficult anyway.",
                    "label": 1
                },
                {
                    "sent": "But the point about this is that if you have sets of faces, it's easier to judge whether or not faces match.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we've got video here, so it's.",
                    "label": 0
                },
                {
                    "sent": "Get some master code.",
                    "label": 0
                },
                {
                    "sent": "So if we have the video, we have many many frames.",
                    "label": 0
                },
                {
                    "sent": "And we're in a position to detect faces and align them automatically.",
                    "label": 0
                },
                {
                    "sent": "So here, so it's very easy for us to gather sets of faces, and that's what I'm going to go through.",
                    "label": 0
                },
                {
                    "sent": "But from video, unlike in still images, we can gather these sets of phases, which in turn is going to make the matching of people easier, not solved.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I'll go through three steps on how we do this.",
                    "label": 1
                },
                {
                    "sent": "We're going to get sets of faces.",
                    "label": 1
                },
                {
                    "sent": "Associate them by sort of tracking.",
                    "label": 0
                },
                {
                    "sent": "Represent them by feature vectors and then match face set to a face set using these sets of feature vectors.",
                    "label": 1
                },
                {
                    "sent": "That's the computer.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And part of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, on faces we run a face detector.",
                    "label": 0
                },
                {
                    "sent": "Kisses Standard open CV face detector.",
                    "label": 0
                },
                {
                    "sent": "So frontal face detector so when people turn away we don't get their face.",
                    "label": 0
                },
                {
                    "sent": "OK this is running a face detector independently on each frame, and here you're seeing.",
                    "label": 0
                },
                {
                    "sent": "Two faces detected and as a human you associate the faces, but at the moment there's no knowledge that these face detections belong to two people.",
                    "label": 0
                },
                {
                    "sent": "They could belong to 10 people or one person.",
                    "label": 0
                },
                {
                    "sent": "There's no knowledge of that.",
                    "label": 0
                },
                {
                    "sent": "How we associate them is by.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I. Tracking so, but we're not going to track the faces we're going to use some of the many techniques have been developed in computer vision, tracking tracking.",
                    "label": 0
                },
                {
                    "sent": "Big thing in computer vision.",
                    "label": 0
                },
                {
                    "sent": "Many years of developing methods.",
                    "label": 0
                },
                {
                    "sent": "What we do is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Take come to here, we take a standard track of computer vision, which is called the KLT tracker, which simply tracks feature points.",
                    "label": 1
                },
                {
                    "sent": "Doesn't know anything about faces, but it's been designed for many years.",
                    "label": 0
                },
                {
                    "sent": "It's very fast.",
                    "label": 0
                },
                {
                    "sent": "We can start tracking in this frame, see where the tracks go, and if the tracks link up with another detected face, then we can deem that these are two face two face detections belong to the same person.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is completely independent evidence.",
                    "label": 1
                },
                {
                    "sent": "We detect faces in every frame and then we decide later by tracking independently whether these detections are the same person or.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what that means is we get to this.",
                    "label": 0
                },
                {
                    "sent": "So here the faces sections are color coded and that means that we've, by tracking independently using these points, decided that there are two people here and that all of the ones colored red belong to Julia Roberts and all the ones colored other kind of Richard Gere, OK.",
                    "label": 0
                },
                {
                    "sent": "So that's all being done by tracking the face detection.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faces so then we get to the situation where we have many face detections associated because we're detecting every frame.",
                    "label": 0
                },
                {
                    "sent": "Say for example.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this shot.",
                    "label": 0
                },
                {
                    "sent": "So again, the color coded and down the bottom.",
                    "label": 0
                },
                {
                    "sent": "These are all these faces that have been associated for this character.",
                    "label": 0
                },
                {
                    "sent": "All these faces.",
                    "label": 0
                },
                {
                    "sent": "Each of these characters, so we get hundreds of faces in sets automatically associated.",
                    "label": 0
                },
                {
                    "sent": "This straightforward thing to do, and now what we want to do is to build descriptors for these sets and use those to match the faces or see if the faces match.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so our face representation.",
                    "label": 0
                },
                {
                    "sent": "As should be clear, if he's not thinking about faces, you have pose variations.",
                    "label": 0
                },
                {
                    "sent": "You have size variations that I said what you want to do is to some extent try and normalize out or normalize the faces to remove these pose variations, and we're going to do that by detecting facial features, eyes, nose, mouth, other features in the face which are always there and use that to transform the detected face to a standard position.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to show you what I mean, here's a a shot an you're seeing in red.",
                    "label": 0
                },
                {
                    "sent": "The detective features and in each frame we fund this detective features and then map those onto a standard position to transform the face interest.",
                    "label": 0
                },
                {
                    "sent": "Fairly standard position.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can see I don't really go into how it's done.",
                    "label": 0
                },
                {
                    "sent": "It's done.",
                    "label": 0
                },
                {
                    "sent": "We do this by learning facial features and learning that.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Decorations.",
                    "label": 0
                },
                {
                    "sent": "What we call a consolation or.",
                    "label": 0
                },
                {
                    "sent": "Capital structure model K.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The effect is is impressive.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to show you here is the original shot, then the detected phase will be here and this will be the face after normalization.",
                    "label": 0
                },
                {
                    "sent": "So what you can see in the center one which is the role detection, is this sort of pulsing, and that's typical of these face detectors that their scale is discretized, so you get it switches, one scalar, another pyramid.",
                    "label": 0
                },
                {
                    "sent": "You get this pulsing of scales, whereas that's removed after the after taking facial features, mappings and position, the detection is stabilized.",
                    "label": 0
                },
                {
                    "sent": "Both impose and in scale, but you can see this alot of not perfect.",
                    "label": 0
                },
                {
                    "sent": "You haven't completely removed all motion, but this is what we want to learn from.",
                    "label": 0
                },
                {
                    "sent": "From this we want to learn the representation of this character.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the way we do that then we've detected the facial features around the facial features and that several other points on the face.",
                    "label": 0
                },
                {
                    "sent": "We build a feature vector.",
                    "label": 0
                },
                {
                    "sent": "And for this it's not.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Important details, but actually we measure what's called a SIFT descriptor, which is a array of orientations measured around each of these facial features that we record.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so for each phase we have some large feature vectors records the distribution of orientations.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not just 360 dimensions for each phase?",
                    "label": 0
                },
                {
                    "sent": "OK, so each phase is represented by 360 dimensional vector.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we have sets of faces.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And each of these faces we have these three 6 dimensional vectors and what we want to do now is to match a set of vectors to a set of vectors.",
                    "label": 0
                },
                {
                    "sent": "And I saw that this afternoon is going to talk on this as well.",
                    "label": 0
                },
                {
                    "sent": "That would be interesting to follow.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take a very simple measure, so imagine that these faces generate points in this 360 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this set of faces we have one set of vectors.",
                    "label": 0
                },
                {
                    "sent": "This set of vectors.",
                    "label": 0
                },
                {
                    "sent": "We want to judge whether these are the same person.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do that simply by using the min, min distance.",
                    "label": 0
                },
                {
                    "sent": "OK, so we find the points.",
                    "label": 0
                },
                {
                    "sent": "If you look at the blue set in the green set, we find the points which are closest together and that gives the distance between these two sets.",
                    "label": 0
                },
                {
                    "sent": "The red set and the blue set will be a much larger distance.",
                    "label": 0
                },
                {
                    "sent": "OK, so within some threshold we can judge that these two are the same person and this is a different person.",
                    "label": 0
                },
                {
                    "sent": "Using this min distance between the sets.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so I'll give you an example.",
                    "label": 0
                },
                {
                    "sent": "Now the whole process, so I will show you a sequence from.",
                    "label": 0
                },
                {
                    "sent": "Office Vampire Slayer and then work you through the processing.",
                    "label": 1
                },
                {
                    "sent": "We do so here.",
                    "label": 0
                },
                {
                    "sent": "There should be some sound.",
                    "label": 0
                },
                {
                    "sent": "So there should be some sound.",
                    "label": 0
                },
                {
                    "sent": "So you see what they're doing.",
                    "label": 0
                },
                {
                    "sent": "Their video jerkiness is coming apart from the laptop that's not in original, but they're sort of dancing around each other while they have breakfast.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want to be able to establish whether the same people are not in there.",
                    "label": 0
                },
                {
                    "sent": "Actually five shots here, so the camera is changing to profile views as well.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm just show you the processing.",
                    "label": 0
                },
                {
                    "sent": "So first of all, here's the raw face detection, so this is detecting faces on every frame and what you have here is X along the bottom.",
                    "label": 0
                },
                {
                    "sent": "So X of the frame and then the vertical direction is time.",
                    "label": 0
                },
                {
                    "sent": "So here, as the bar goes up, you're seeing the face detections on original video.",
                    "label": 0
                },
                {
                    "sent": "At the moment these are colored white because we don't know whether the same people or not.",
                    "label": 0
                },
                {
                    "sent": "This is just the raw detections.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actor faces, so now they're color coded according to where they've been trapped together.",
                    "label": 0
                },
                {
                    "sent": "But you see at this point.",
                    "label": 0
                },
                {
                    "sent": "This character is Buffy turns away from the camera, so we no longer have any frontal detections, so that tracks going to break.",
                    "label": 0
                },
                {
                    "sent": "We carry on.",
                    "label": 0
                },
                {
                    "sent": "Then one person walks in front of another, so again the tracks going to break.",
                    "label": 0
                },
                {
                    "sent": "They can't track through occlusions like this.",
                    "label": 0
                },
                {
                    "sent": "So after the tracking, we've reduced the.",
                    "label": 0
                },
                {
                    "sent": "Number of feet.",
                    "label": 0
                },
                {
                    "sent": "They're not just hundreds of Detective faces now we've associated into a handful of tracks, but there are still several tracks for each person.",
                    "label": 0
                },
                {
                    "sent": "That's the situation at this point.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to.",
                    "label": 0
                },
                {
                    "sent": "For each of these, each of these are the set of phases of maybe 100 faces.",
                    "label": 0
                },
                {
                    "sent": "Each of these tracks.",
                    "label": 0
                },
                {
                    "sent": "Now we match them using motive.",
                    "label": 0
                },
                {
                    "sent": "Clustering using this min, min distance and as we do that then it correctly.",
                    "label": 0
                },
                {
                    "sent": "Comes from.",
                    "label": 0
                },
                {
                    "sent": "I'm just a set of tracks.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here too.",
                    "label": 0
                },
                {
                    "sent": "Correctly associating the face detections of the people here and we're down to just three.",
                    "label": 0
                },
                {
                    "sent": "Associations which correspond to three people.",
                    "label": 0
                },
                {
                    "sent": "And this is very typical of the sort of processing that happens that face the.",
                    "label": 0
                },
                {
                    "sent": "We detect faces independently when we link them.",
                    "label": 0
                },
                {
                    "sent": "We don't get false positives.",
                    "label": 0
                },
                {
                    "sent": "This very robust when we.",
                    "label": 0
                },
                {
                    "sent": "Then use the minimum distance to join up the sets of faces.",
                    "label": 0
                },
                {
                    "sent": "That works very well indeed, like 99% accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is now the sort of data that we can generate automatically.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Come back to this slide I had before on the ambiguity, so we've gone through the computer part.",
                    "label": 0
                },
                {
                    "sent": "We've done the visual processing.",
                    "label": 0
                },
                {
                    "sent": "And now I want to look at improving the annotation.",
                    "label": 0
                },
                {
                    "sent": "So we had this problem that even though.",
                    "label": 0
                },
                {
                    "sent": "The text was giving us suggestions for who people were these ambiguities?",
                    "label": 0
                },
                {
                    "sent": "Now, how can we improve this?",
                    "label": 0
                },
                {
                    "sent": "So you see, in this case there are two characters somebody speaking what we're going to look at is using speaker detection to decide if the person is really speaking.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "If the subtitles and align script are saying that the character Willow is speaking.",
                    "label": 1
                },
                {
                    "sent": "So if we can determine in the shot that this character is actually speaking, then we can really believe that this characters Willow.",
                    "label": 0
                },
                {
                    "sent": "Whereas if this character is not speaking then we would ignore the annotation.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we do that is quite simple.",
                    "label": 0
                },
                {
                    "sent": "We take these facial features we detected more facial features and I showed you before.",
                    "label": 0
                },
                {
                    "sent": "So we have many facial features we can pull out the mouth through a shot and that's what's being shown here.",
                    "label": 0
                },
                {
                    "sent": "We put out the mouth and we can.",
                    "label": 1
                },
                {
                    "sent": "Simply look at the amplitude of the change between one frame and the next.",
                    "label": 0
                },
                {
                    "sent": "We do that using cross correlation between one frame in the next under translation.",
                    "label": 0
                },
                {
                    "sent": "So we find the best cross correlation possible between here and here under small translations in between.",
                    "label": 1
                },
                {
                    "sent": "Here and here, measure the sum of squared distances, and that's what you're.",
                    "label": 1
                },
                {
                    "sent": "Sing down below and if this is above a certain threshold, then we deem that the person is speaking.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you can think of ways of improving this, but actually we found this was the best way so far in the ways we've tried.",
                    "label": 0
                },
                {
                    "sent": "If it's below a threshold or not significant, then we don't classes, don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, so only when we're fairly sure do we deem this person.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Speaking.",
                    "label": 0
                },
                {
                    "sent": "And this works surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "Here's an example where the person is not speaking, but there supposed change.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a large change going through the shot, and we correctly classify that as not speaking.",
                    "label": 0
                },
                {
                    "sent": "You might think that smiling would generate a false positive, so here this character smiling, but again we we correctly classified as not speaking.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's not perfect.",
                    "label": 0
                },
                {
                    "sent": "Here's a character opening her mouth in horror or surprise, and we incorrectly say that she's speaking.",
                    "label": 0
                },
                {
                    "sent": "So it works.",
                    "label": 0
                },
                {
                    "sent": "We choose an operating point.",
                    "label": 0
                },
                {
                    "sent": "You'll see later where we can.",
                    "label": 0
                },
                {
                    "sent": "But 99% of the tracks that we say are speaking are correct.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does that do?",
                    "label": 0
                },
                {
                    "sent": "The effect of that is where this is correct.",
                    "label": 0
                },
                {
                    "sent": "So here we had two characters.",
                    "label": 0
                },
                {
                    "sent": "We find that this one is speaking, so we now have a label for this character, and we know this one doesn't have a label here.",
                    "label": 0
                },
                {
                    "sent": "We had the character Buffy was labeled, she wasn't detected.",
                    "label": 0
                },
                {
                    "sent": "This character is not detected speaking, so correctly, we don't have any information here, but it's not incorrect information here.",
                    "label": 0
                },
                {
                    "sent": "We had this reaction shot.",
                    "label": 0
                },
                {
                    "sent": "And we find that neither the speaking so we don't incorrectly label them with this label Tara.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what this is done is really improved.",
                    "label": 0
                },
                {
                    "sent": "The strength of our supervision.",
                    "label": 0
                },
                {
                    "sent": "It's not complete, but it's now much stronger.",
                    "label": 0
                },
                {
                    "sent": "Going from here.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now moving on to the third part of the talk, which is the learning part.",
                    "label": 0
                },
                {
                    "sent": "OK, switch on the vision.",
                    "label": 0
                },
                {
                    "sent": "We've got our text annotation for improved our text annotation.",
                    "label": 0
                },
                {
                    "sent": "Now how do we combine all these things together?",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we've got these tracks and essentially what we want to do is to label all these tracked faces so our granularity is a tracker faces a set of faces, maybe 100 faces linked together, and each one of them will be hundreds of these through a video.",
                    "label": 0
                },
                {
                    "sent": "We want to label them with the character.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right now some of the tracks are labeled, so for the ones where they've been given a single label by the speaker detection, we're going to treat those as being correct and being exemplars.",
                    "label": 0
                },
                {
                    "sent": "So we're going to the moment in true.",
                    "label": 0
                },
                {
                    "sent": "Assume that that there are positive set we've labeled them.",
                    "label": 0
                },
                {
                    "sent": "And then the other tracks which don't have a label, and so this now becomes like a semi supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "You have some data which is labeled and some which is not.",
                    "label": 0
                },
                {
                    "sent": "And as you'll see as well, it's noisy.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's hard and that and so we take a simple approach, which is to use the nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "So here's one track of this character, Buffy, which has been labeled.",
                    "label": 0
                },
                {
                    "sent": "Here's another track which is being labeled, and these are some of the other characters which have been labeled.",
                    "label": 0
                },
                {
                    "sent": "We have a new track which is unlabeled and what we do is associated with the nearest track.",
                    "label": 0
                },
                {
                    "sent": "Using this min distance I showed before and assign it that label.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a simple classifier, but we want to capture the fact that some of the classifications are better than others.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we had.",
                    "label": 0
                },
                {
                    "sent": "I'm a track here.",
                    "label": 0
                },
                {
                    "sent": "We could be very certain that it was this character Buffy as if it was here where it's close to two different characters were not sure which one it is so we can use nearest neighbor classifier, but we're going to associate a confidence with that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do that by making like a mock posterior.",
                    "label": 0
                },
                {
                    "sent": "Where we normalize the minimum distance by the summer distances to all the other characters OK, and what happens then is if we're very sure that it's.",
                    "label": 0
                },
                {
                    "sent": "One character will have a high confidence, probably near one, and if it's confused it will have a low confidence and we can use that for ranking.",
                    "label": 0
                },
                {
                    "sent": "And we can also refuse to predict, so that if something has a low confidence, we can say, well, we're not going to predict what that track is.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am.",
                    "label": 0
                },
                {
                    "sent": "So we've tested this on two episodes of Vampire Slayer.",
                    "label": 1
                },
                {
                    "sent": "The statistics are here.",
                    "label": 0
                },
                {
                    "sent": "We've got 130,000 frames, 50,000 faces detected 1000 tracks, so we want to label 1000 tracks on the right you see a Gallery of the people we want to label.",
                    "label": 1
                },
                {
                    "sent": "These are the main characters from those episodes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His what we get out of it.",
                    "label": 0
                },
                {
                    "sent": "So this is like plug and play.",
                    "label": 0
                },
                {
                    "sent": "We Simply put in the DVD, supply the script and this is what comes out.",
                    "label": 0
                },
                {
                    "sent": "We can see we label many of the characters, even though their faces might be small and the pose might be unusual and.",
                    "label": 0
                },
                {
                    "sent": "Check.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here you can see the lighting.",
                    "label": 0
                },
                {
                    "sent": "The expressions we can do as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what can be done automatically as an overview.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some statistics were pressing position recall here, so I think most of you know president recall is but essentially a perfect graph would be a straight line up in the right hand corner here.",
                    "label": 0
                },
                {
                    "sent": "Anne, and if you just concentrate on this graph is one episode.",
                    "label": 0
                },
                {
                    "sent": "This is the other episode you can see that we do very well and then die off slowly ordering these in this confidence level.",
                    "label": 0
                },
                {
                    "sent": "So remember for each of the tracks we had a probability associated with it and that gives us our ranking here.",
                    "label": 0
                },
                {
                    "sent": "And so statistic would be.",
                    "label": 0
                },
                {
                    "sent": "At around up to about 80% recall, we get 80% precision, which is obviously very good being done automatically.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to show you typical results, so I'm going to run a sequence and what you'll see is many shots.",
                    "label": 0
                },
                {
                    "sent": "The labels of the characters all generated automatically, and there are two errors which will be in red.",
                    "label": 0
                },
                {
                    "sent": "Get the sound on this as well.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what we produced.",
                    "label": 0
                },
                {
                    "sent": "Skip this to 6 bit for you.",
                    "label": 0
                },
                {
                    "sent": "The important thing here on this slide is that if we just use the subtitles and didn't do the speaker detection, we only get 45% correct and by using factory by using the subtitles with the speaker detection we double the precision.",
                    "label": 0
                },
                {
                    "sent": "I guess what the visual speaker detection gives us.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One point map for learning.",
                    "label": 0
                },
                {
                    "sent": "I guess you saw.",
                    "label": 0
                },
                {
                    "sent": "We used a simple nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "But some of these labels are wrong.",
                    "label": 0
                },
                {
                    "sent": "As I've said, the speaker detection isn't perfect, so we've really gotta noisy label problem, summer incorrect and support vector machines are designed to deal with a certain amount of noise in the training data.",
                    "label": 0
                },
                {
                    "sent": "So we can take the nearest neighbor classifier, put it on one side, replace it by an SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it's still.",
                    "label": 0
                },
                {
                    "sent": "Our units are tracks with training data.",
                    "label": 0
                },
                {
                    "sent": "Each training data is a track and we have positive training data for the characters, which is labeled ones.",
                    "label": 0
                },
                {
                    "sent": "We do a one versus all.",
                    "label": 0
                },
                {
                    "sent": "SVM for each character in turn and look at this graph here that the dashed line here is the nearest neighbor classifier shows you before the blue is after running through an SVM.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the SVM does overcome some of the noise in that rate in the training data.",
                    "label": 0
                },
                {
                    "sent": "And the rate is, if we had, if the if we had perfect classification from speaker detection, OK, that's the best we could do so using something which is tolerant to noise in the training data is better.",
                    "label": 0
                },
                {
                    "sent": "Which back in SVM is better than the nearest neighbor classifier?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm so.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you one extension.",
                    "label": 0
                },
                {
                    "sent": "I've got some just that's the main part of the problem.",
                    "label": 0
                },
                {
                    "sent": "We can label DVD's, we can label television material with subtitles.",
                    "label": 0
                },
                {
                    "sent": "Where would this go next?",
                    "label": 0
                },
                {
                    "sent": "One thing.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do is deal with profiles OK so we can detect profiles.",
                    "label": 0
                },
                {
                    "sent": "We can run exactly the same ideas.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "We can do facial feature detection profile.",
                    "label": 0
                },
                {
                    "sent": "We can do speaker detection in profile.",
                    "label": 1
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "Track between profiles and frontal.",
                    "label": 0
                },
                {
                    "sent": "So here you see a sequence where we're switching between profile and frontal trackers using this detection independently each frame and then linking by KLT tracks as before.",
                    "label": 0
                },
                {
                    "sent": "So now we can find profile matches another.",
                    "label": 0
                },
                {
                    "sent": "Secret shop Switcher have track Switcher profiles, and they're labeled.",
                    "label": 0
                },
                {
                    "sent": "We can pull them into shots which have frontal and profile.",
                    "label": 0
                },
                {
                    "sent": "We can switch between the two.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can extend this in terms of coverage.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Miss this come to the end.",
                    "label": 0
                },
                {
                    "sent": "So to conclude, then.",
                    "label": 0
                },
                {
                    "sent": "We've done what we set out to do this is we walk through this problem where we would go next in terms of improvements.",
                    "label": 0
                },
                {
                    "sent": "Is building up a larger data set of videos so we can go between episodes go between movies once we've learned.",
                    "label": 0
                },
                {
                    "sent": "Face models and classifiers for different places.",
                    "label": 0
                },
                {
                    "sent": "Extend the visual description.",
                    "label": 0
                },
                {
                    "sent": "Extend the classification so we're going to see this often.",
                    "label": 0
                },
                {
                    "sent": "As I said, some classification of sets of vectors and also moved from just recognizing faces to recognizing other things, recognizing emotions, recognizing actions, and we're going to talk by Ivan leftover this afternoon on recognizing actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this idea can just run and run.",
                    "label": 0
                },
                {
                    "sent": "This is a sensible, sensible question.",
                    "label": 0
                },
                {
                    "sent": "We didn't need to in terms of the video using the vision was sufficient for our needs, but yes we could.",
                    "label": 0
                },
                {
                    "sent": "It could help us.",
                    "label": 0
                },
                {
                    "sent": "It could help us determine the gender of the people.",
                    "label": 0
                },
                {
                    "sent": "It could help us, but there's only so much you can do in one at one go, but it's a good thing to integrate as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we've tried several features, So what we were using originally is sift descriptors around each of the facial features.",
                    "label": 0
                },
                {
                    "sent": "So the vector would just be concatenated.",
                    "label": 0
                },
                {
                    "sent": "Sift descriptors and there we were using usually shift as a four by four grid.",
                    "label": 0
                },
                {
                    "sent": "We were using a three by three grid.",
                    "label": 0
                },
                {
                    "sent": "We found that we looked at performance over different grid resolutions.",
                    "label": 0
                },
                {
                    "sent": "We found a three by three grid with shift work better.",
                    "label": 0
                },
                {
                    "sent": "We've also used raw intensities, and we've used gradients.",
                    "label": 0
                },
                {
                    "sent": "We've compared all of these are current.",
                    "label": 0
                },
                {
                    "sent": "Work finds out the safe descriptors are still best.",
                    "label": 0
                },
                {
                    "sent": "Testing in.",
                    "label": 0
                },
                {
                    "sent": "So the results I showed it is within an episode we we've got.",
                    "label": 0
                },
                {
                    "sent": "The text we have.",
                    "label": 0
                },
                {
                    "sent": "Say 500 tracks.",
                    "label": 0
                },
                {
                    "sent": "Yes, why do you actually have to have results for that?",
                    "label": 0
                },
                {
                    "sent": "So here I didn't show them, but this is going from training on one episode and testing on the other episode.",
                    "label": 0
                },
                {
                    "sent": "And this is a sort of results we get, so it's obviously much more demanding task as we're just using visual models.",
                    "label": 0
                },
                {
                    "sent": "Here is comparing three different types of.",
                    "label": 0
                },
                {
                    "sent": "What is several things changing here that the blue one, which is our best one?",
                    "label": 0
                },
                {
                    "sent": "That moment is using some extra facial feature normalization for intensities due to build trigs?",
                    "label": 0
                },
                {
                    "sent": "And then a random firm classifier, but that's maybe not so important.",
                    "label": 0
                },
                {
                    "sent": "So this is our results going from one episode to the other.",
                    "label": 0
                }
            ]
        }
    }
}