{
    "id": "u3jaswdq6tht5g75j5jvygxudncslgsn",
    "title": "Monotone multi-armed bandit allocations",
    "info": {
        "author": [
            "Aleksandrs Slivkins, Microsoft Research"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_slivkins_monotone/",
    "segmentation": [
        [
            "OK, so.",
            "I'm going to talk about a new twist on multi arm bandit algorithms, which is motivated by.",
            "Paper click advertisement on the web so."
        ],
        [
            "I guess.",
            "Many or all of you know very well what Bennett algorithms are, but I will need kind of slightly nonstandard setup.",
            "So OK, in each round we select a monkey alternatives called Arms.",
            "We assume rewards are fixed in advance, but not revealed to the algorithm.",
            "The goal is to maximize the total reward overtime.",
            "Now when I say that rewards are fixed, I'm going to specifically mean that.",
            "The environment has in mind a table and you know the IT center of this table is the reward of.",
            "You know that arm I would get in around see if it is chosen around C and will use the definition later on.",
            "We'll call it a realization meaning like realization of their words.",
            "Now let us Oh yeah, and we assume that realization is generated by a random process and we know the algorithm knows that this random process belongs to some family of.",
            "Allowed random process and this way we can model, you know, adversarial rewards and stochastic rewards and whatnot.",
            "So it's kind of a general way to encompass many building settings that are out there."
        ],
        [
            "Now let us kind of in reach of the model A little bit to something that we call a bandit allocation rules, which is essentially a bandit algorithm.",
            "That additional inputs a vector of bids, like you know for each other, and we input a bid.",
            "Then we run a bandit algorithm, get some rewards which we call Rd awards, and then we scale each row rewarded by the corresponding build.",
            "So why does this make sense well?",
            "It's motivated by the.",
            "Paper click advertisement where you know we can.",
            "Where arms correspond to adds an.",
            "You know an idealized setting to think about is when each each agent or each advertiser shows up with one ad in each round one it is shown to a user an whenever.",
            "Oh yeah, and the important thing why it's called.",
            "A paper click is that values generator don't like a common assumption or common model.",
            "Is that values generated only when given AD is clicked.",
            "So each time Addi is clicked, the correspondent agent receives the.",
            "You know some amount of value, namely by.",
            "So then we can think of rewards as clicks, and then you know click probabilities are not entirely known needs to be estimated overtime, so it's like a bandit problem and the total value created in this ad allocation problem is exactly the total reward of the bandit allocation role.",
            "So I mean, so far I'm just saying that OK, this model of.",
            "Bandit allocation rules is is motivated by ads.",
            "Now you know so far, I didn't really say anything.",
            "You know much you because.",
            "If we just care about optimizing the, you know if we just care about optimizing the total reward or or optimizing regret, then it is just exactly like, well, almost exactly like a bandit problem.",
            "Like you know, a standard bandit problem with one difference that these numbers because the the bids are submitted by people and people can lie.",
            "And that creates a host of new Prob."
        ],
        [
            "So to be.",
            "A little bit more specific.",
            "So let me set up an.",
            "Sort of idealized model for what happens in like auctions in, which adds lots are sold there called AD auctions and it's so this idealized model of like adoptions with learning new click with learning the click probability that is something called.",
            "Bandit auctions it's really simple.",
            "1st Every agent submits a bill, then the bandit Allocation rule is run, and then the payments are assigned.",
            "Now, like I said, the issues that people can lie, namely the common assumption, is that each agents value per click is a private information.",
            "That is, it is known to this agent, but it's not known to anybody else, including the algorithm.",
            "So when the agents can lie about this value, if it benefits them.",
            "So it's nice to incentivize them to do, you know, to do what is desirable?",
            "In particular it's, it would be nice if we could incentivize them to tell the truth.",
            "So a standard notion in auction design is that auction is truthful.",
            "If for each agent the truth telling is no worse than lying no matter what other agents do.",
            "Now, OK. A standard question to ask about truthful auctions is the following."
        ],
        [
            "So when so take a given allocation rule.",
            "I mean, you know given algorithm that allocates ads and the question is so when a bandit allocation rule can be extended to truthful auction by finding a suitable payment role.",
            "So and there is a clean answer to that, namely it happens if and only if the bandit allocation rule has this property called monotonicity.",
            "Namely, if we increase anyone build while keeping.",
            "All other bids fixed then the this can only increase the total reward of the corresponding arm.",
            "So, uh.",
            "It's like, you know, if I build more, I can only get more.",
            "I cannot get less.",
            "So now that we define monotonicity, is the property of a bandit allocation rule.",
            "We can kind of forget about all the rest that I told you about bandit auctions and just focus on this property of monotonicity bandit allocation rules, which is the GNU twist that I was talking about that that I promised.",
            "So now that defines a problem.",
            "So for any given by the setting that you like, I design A monotone Bennett allocation rule and you know.",
            "As usual, we want to optimize the total reward or optimize regret.",
            "So and one can think about any like really anybody in setting you like.",
            "And there are two versions of this question which are interested in.",
            "One is sort of like a strong version.",
            "We want monotonicity and the corresponding truthfulness property hold for every realization.",
            "And kind of a Model X property is in expectation authorizations I in expectation over clicks.",
            "So."
        ],
        [
            "The status of the the current status of this is that for stochastic rewards, and that is when in every round the reward of each arm is assembled.",
            "An independent sample from some distributions specific to the term.",
            "For that case, the problem is essentially solved.",
            "It turns out that the standard optimal algorithm called UCB one.",
            "Well, a slight variant of it is.",
            "Monotone and expectation and then, but it's not monotone for each realization, but one can design A kind of immersive, much more sophisticated sophisticated algorithm, which is model which is monotone for each realization, and it also has an optimal regret.",
            "So essentially for that case the problem is solved.",
            "For anything else, the problem is wide open and nice target seems to be adverse aerial rewards, so there is a monotone bandit allocation rule with regret.",
            "Enter the 2/3.",
            "We know that just for bandit algorithm, the optimal optimal regulators rooty so it's not clear if you can get it.",
            "And also I mean one can ask this question for any like anybody inciting.",
            "That's it, thanks.",
            "Where is Elvis area mean in the context of the context of truthfulness?",
            "Or other settle means adversarial rewards, I mean same.",
            "I mean.",
            "Exactly the same thing as it means for you know any bandit problem like there is an oblivious set.",
            "Well in this case I mean oblivious adversary.",
            "That fixes all the clicks in advance in a specific sense."
        ],
        [
            "Of that table.",
            "Like it fixes in advance what would happen in.",
            "So the clicks if I'm the better, I don't know the clicks.",
            "Yes, exactly, very odd.",
            "Oh it seems like an odd notion of truthfulness.",
            "If you're fixing the click sequence and truthful with respect to, I mean the clicks are not known in advance to anybody, so it would be reassuring for the builder.",
            "It's, you know that no matter what happens in the future, the right thing for him to do used to be the true value."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about a new twist on multi arm bandit algorithms, which is motivated by.",
                    "label": 0
                },
                {
                    "sent": "Paper click advertisement on the web so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Many or all of you know very well what Bennett algorithms are, but I will need kind of slightly nonstandard setup.",
                    "label": 0
                },
                {
                    "sent": "So OK, in each round we select a monkey alternatives called Arms.",
                    "label": 0
                },
                {
                    "sent": "We assume rewards are fixed in advance, but not revealed to the algorithm.",
                    "label": 1
                },
                {
                    "sent": "The goal is to maximize the total reward overtime.",
                    "label": 0
                },
                {
                    "sent": "Now when I say that rewards are fixed, I'm going to specifically mean that.",
                    "label": 1
                },
                {
                    "sent": "The environment has in mind a table and you know the IT center of this table is the reward of.",
                    "label": 0
                },
                {
                    "sent": "You know that arm I would get in around see if it is chosen around C and will use the definition later on.",
                    "label": 0
                },
                {
                    "sent": "We'll call it a realization meaning like realization of their words.",
                    "label": 1
                },
                {
                    "sent": "Now let us Oh yeah, and we assume that realization is generated by a random process and we know the algorithm knows that this random process belongs to some family of.",
                    "label": 0
                },
                {
                    "sent": "Allowed random process and this way we can model, you know, adversarial rewards and stochastic rewards and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a general way to encompass many building settings that are out there.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let us kind of in reach of the model A little bit to something that we call a bandit allocation rules, which is essentially a bandit algorithm.",
                    "label": 0
                },
                {
                    "sent": "That additional inputs a vector of bids, like you know for each other, and we input a bid.",
                    "label": 1
                },
                {
                    "sent": "Then we run a bandit algorithm, get some rewards which we call Rd awards, and then we scale each row rewarded by the corresponding build.",
                    "label": 0
                },
                {
                    "sent": "So why does this make sense well?",
                    "label": 0
                },
                {
                    "sent": "It's motivated by the.",
                    "label": 0
                },
                {
                    "sent": "Paper click advertisement where you know we can.",
                    "label": 0
                },
                {
                    "sent": "Where arms correspond to adds an.",
                    "label": 0
                },
                {
                    "sent": "You know an idealized setting to think about is when each each agent or each advertiser shows up with one ad in each round one it is shown to a user an whenever.",
                    "label": 1
                },
                {
                    "sent": "Oh yeah, and the important thing why it's called.",
                    "label": 1
                },
                {
                    "sent": "A paper click is that values generator don't like a common assumption or common model.",
                    "label": 0
                },
                {
                    "sent": "Is that values generated only when given AD is clicked.",
                    "label": 0
                },
                {
                    "sent": "So each time Addi is clicked, the correspondent agent receives the.",
                    "label": 1
                },
                {
                    "sent": "You know some amount of value, namely by.",
                    "label": 0
                },
                {
                    "sent": "So then we can think of rewards as clicks, and then you know click probabilities are not entirely known needs to be estimated overtime, so it's like a bandit problem and the total value created in this ad allocation problem is exactly the total reward of the bandit allocation role.",
                    "label": 0
                },
                {
                    "sent": "So I mean, so far I'm just saying that OK, this model of.",
                    "label": 0
                },
                {
                    "sent": "Bandit allocation rules is is motivated by ads.",
                    "label": 0
                },
                {
                    "sent": "Now you know so far, I didn't really say anything.",
                    "label": 0
                },
                {
                    "sent": "You know much you because.",
                    "label": 0
                },
                {
                    "sent": "If we just care about optimizing the, you know if we just care about optimizing the total reward or or optimizing regret, then it is just exactly like, well, almost exactly like a bandit problem.",
                    "label": 0
                },
                {
                    "sent": "Like you know, a standard bandit problem with one difference that these numbers because the the bids are submitted by people and people can lie.",
                    "label": 0
                },
                {
                    "sent": "And that creates a host of new Prob.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to be.",
                    "label": 0
                },
                {
                    "sent": "A little bit more specific.",
                    "label": 0
                },
                {
                    "sent": "So let me set up an.",
                    "label": 0
                },
                {
                    "sent": "Sort of idealized model for what happens in like auctions in, which adds lots are sold there called AD auctions and it's so this idealized model of like adoptions with learning new click with learning the click probability that is something called.",
                    "label": 0
                },
                {
                    "sent": "Bandit auctions it's really simple.",
                    "label": 0
                },
                {
                    "sent": "1st Every agent submits a bill, then the bandit Allocation rule is run, and then the payments are assigned.",
                    "label": 1
                },
                {
                    "sent": "Now, like I said, the issues that people can lie, namely the common assumption, is that each agents value per click is a private information.",
                    "label": 0
                },
                {
                    "sent": "That is, it is known to this agent, but it's not known to anybody else, including the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So when the agents can lie about this value, if it benefits them.",
                    "label": 1
                },
                {
                    "sent": "So it's nice to incentivize them to do, you know, to do what is desirable?",
                    "label": 0
                },
                {
                    "sent": "In particular it's, it would be nice if we could incentivize them to tell the truth.",
                    "label": 0
                },
                {
                    "sent": "So a standard notion in auction design is that auction is truthful.",
                    "label": 0
                },
                {
                    "sent": "If for each agent the truth telling is no worse than lying no matter what other agents do.",
                    "label": 1
                },
                {
                    "sent": "Now, OK. A standard question to ask about truthful auctions is the following.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when so take a given allocation rule.",
                    "label": 1
                },
                {
                    "sent": "I mean, you know given algorithm that allocates ads and the question is so when a bandit allocation rule can be extended to truthful auction by finding a suitable payment role.",
                    "label": 1
                },
                {
                    "sent": "So and there is a clean answer to that, namely it happens if and only if the bandit allocation rule has this property called monotonicity.",
                    "label": 0
                },
                {
                    "sent": "Namely, if we increase anyone build while keeping.",
                    "label": 1
                },
                {
                    "sent": "All other bids fixed then the this can only increase the total reward of the corresponding arm.",
                    "label": 0
                },
                {
                    "sent": "So, uh.",
                    "label": 0
                },
                {
                    "sent": "It's like, you know, if I build more, I can only get more.",
                    "label": 0
                },
                {
                    "sent": "I cannot get less.",
                    "label": 0
                },
                {
                    "sent": "So now that we define monotonicity, is the property of a bandit allocation rule.",
                    "label": 0
                },
                {
                    "sent": "We can kind of forget about all the rest that I told you about bandit auctions and just focus on this property of monotonicity bandit allocation rules, which is the GNU twist that I was talking about that that I promised.",
                    "label": 0
                },
                {
                    "sent": "So now that defines a problem.",
                    "label": 0
                },
                {
                    "sent": "So for any given by the setting that you like, I design A monotone Bennett allocation rule and you know.",
                    "label": 0
                },
                {
                    "sent": "As usual, we want to optimize the total reward or optimize regret.",
                    "label": 0
                },
                {
                    "sent": "So and one can think about any like really anybody in setting you like.",
                    "label": 0
                },
                {
                    "sent": "And there are two versions of this question which are interested in.",
                    "label": 0
                },
                {
                    "sent": "One is sort of like a strong version.",
                    "label": 1
                },
                {
                    "sent": "We want monotonicity and the corresponding truthfulness property hold for every realization.",
                    "label": 0
                },
                {
                    "sent": "And kind of a Model X property is in expectation authorizations I in expectation over clicks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The status of the the current status of this is that for stochastic rewards, and that is when in every round the reward of each arm is assembled.",
                    "label": 0
                },
                {
                    "sent": "An independent sample from some distributions specific to the term.",
                    "label": 0
                },
                {
                    "sent": "For that case, the problem is essentially solved.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the standard optimal algorithm called UCB one.",
                    "label": 0
                },
                {
                    "sent": "Well, a slight variant of it is.",
                    "label": 0
                },
                {
                    "sent": "Monotone and expectation and then, but it's not monotone for each realization, but one can design A kind of immersive, much more sophisticated sophisticated algorithm, which is model which is monotone for each realization, and it also has an optimal regret.",
                    "label": 1
                },
                {
                    "sent": "So essentially for that case the problem is solved.",
                    "label": 1
                },
                {
                    "sent": "For anything else, the problem is wide open and nice target seems to be adverse aerial rewards, so there is a monotone bandit allocation rule with regret.",
                    "label": 0
                },
                {
                    "sent": "Enter the 2/3.",
                    "label": 1
                },
                {
                    "sent": "We know that just for bandit algorithm, the optimal optimal regulators rooty so it's not clear if you can get it.",
                    "label": 0
                },
                {
                    "sent": "And also I mean one can ask this question for any like anybody inciting.",
                    "label": 0
                },
                {
                    "sent": "That's it, thanks.",
                    "label": 0
                },
                {
                    "sent": "Where is Elvis area mean in the context of the context of truthfulness?",
                    "label": 0
                },
                {
                    "sent": "Or other settle means adversarial rewards, I mean same.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same thing as it means for you know any bandit problem like there is an oblivious set.",
                    "label": 0
                },
                {
                    "sent": "Well in this case I mean oblivious adversary.",
                    "label": 0
                },
                {
                    "sent": "That fixes all the clicks in advance in a specific sense.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that table.",
                    "label": 0
                },
                {
                    "sent": "Like it fixes in advance what would happen in.",
                    "label": 0
                },
                {
                    "sent": "So the clicks if I'm the better, I don't know the clicks.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly, very odd.",
                    "label": 0
                },
                {
                    "sent": "Oh it seems like an odd notion of truthfulness.",
                    "label": 0
                },
                {
                    "sent": "If you're fixing the click sequence and truthful with respect to, I mean the clicks are not known in advance to anybody, so it would be reassuring for the builder.",
                    "label": 0
                },
                {
                    "sent": "It's, you know that no matter what happens in the future, the right thing for him to do used to be the true value.",
                    "label": 0
                }
            ]
        }
    }
}