{
    "id": "5ajw4rne6n6hz2he6aqbu7and6wgsdjr",
    "title": "A Spectral Learning Algorithm for Finite State Transducers",
    "info": {
        "author": [
            "Borja Balle, Departament de Llenguatges i Sistemes Inform\u00e0tics, Technical University of Catalonia"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_balle_spectral/",
    "segmentation": [
        [
            "Joint work with Ariana Cottone and Chevy Carreras.",
            "You're sitting in there.",
            "And we will present the spectral learning algorithm for finding state transducers, and I'll start by describing this."
        ],
        [
            "Object will talk about.",
            "Which in this case are probabilistic transducers which we use them to model input output relations between sequences over some finite alphabets and we.",
            "We assume that this input output relations are modeled using hidden states.",
            "And so the model would defines its conditional distribution over an output sequence.",
            "I given an input sequence X.",
            "And so the model has needed some independent assumptions that you can see in this diagram.",
            "So you're given an input sequence, an output sequence, and.",
            "Us as time grows, the what you observe at time T or for example time three depends only on the hidden state where the machine is at time 3.",
            "On the next step, at the next hidden state, the state four depends on the current state and the symbol you're observing at this time.",
            "And these machines that define these conditional probabilities over sequences can be used in many applications like NLP or biology, and in general anywhere where you have data which appears in pairs of sequences.",
            "And the problem we want to solve here is learning this conditional probabilities and in general this is hard cause of this hidden state layer.",
            "And what many people do in practices use EM algorithm.",
            "Another iterative methods that usually have local minima and conversions problems.",
            "So that's why the problem is interesting."
        ],
        [
            "And what we do here is represent the spectral algorithm for doing that, which is fast.",
            "An scales very good with the size of the training set for which we have pack style theorem with gives you convergence guarantees.",
            "And the idea of the algorithm is that it is based on a representation of these transducers that's called observe operator models.",
            "And this means that we represent the parameters of the transducer in the form of some matrices that I'll explain in a minute.",
            "On using these matrices, we can use a spectral methods, in particular.",
            "Single value decomposition's to learn these objects and there's been some recent literature in the spectral methods for running other objects for, for example, at gyms.",
            "So the difference here would be that in the transductive setting you have an input that's conditioning your output, which you don't have in the HMM setting.",
            "And then, in addition to the theoretical guarantees, we show that in practice we outperforming them in time and."
        ],
        [
            "Predictive performance.",
            "So in the rest of the talk, just introduced these observable operators.",
            "Then I'll explain how do you learn these operators and I'll present some of our expand."
        ],
        [
            "OK, so for the rest of the talk I will assume that the sequences X&Y are of the same length over some finite alphabets X&Y.",
            "An would our model does is compute this conditional probability.",
            "And the nice thing is that you can express this probability as a product of matrices.",
            "An initial vector Alpha.",
            "Ann, well to derive this expression you rely on the independence assumptions I showed in the initial diagram.",
            "Ann, it's mainly the derivation is mainly using the usual forward Barwa backward algorithm.",
            "So you can think that this is another way of stating the forward backward algorithm, where Alpha is initial state distribution an each of these matrices, which is which are operators on this operator operator model.",
            "Define the so given actualization of the.",
            "Of the state distribution.",
            "For example, at time TI have a state distribution of a T and I multiply by one of these operators, and I obtained the state distribution in a time T + 1.",
            "And what makes these models so these models are called observable operator models?",
            "Becausw the operator you choose at any point in time depends only on what you're observing, which is the input and the output.",
            "And now that you know what the operators are, I'll tell you."
        ],
        [
            "Inside these matrices an.",
            "It turns out that, well, we can fix some notation.",
            "We let the input symbols we buy.",
            "A is.",
            "The output symbols are beasts and the hidden states are sees their K input symbols, L output symbols and M states.",
            "And in these operators these matrices are defined in terms of the the usual and normal parameters you have in Pristiq transducers, which are the probability of next state given a current observation and current state which we collect in the in matrices and probability of summer vacation given the current state.",
            "And by the factorization, so the dependence assumptions we can compute this operator as a product of two matrices, one depending on the input symbol and another one depending on the output symbol.",
            "So we end up with.",
            "With K of this M * M matrices, the TAS, then this, there's another.",
            "There's L N * M matrices, but this ones are diagonal.",
            "By this chroniker Delta.",
            "So we like to collect these these the handle on this.",
            "This on Rosen metrics we call.",
            "Oh and then there's the initial state distribution.",
            "But now that we have these operators, we see that in terms of learning, this is not really helping us, because the probabilities in this operator depend on the on a conditioned on hidden states which we cannot observe in the sample because we are only observing the input and output sequences.",
            "So we sell the operation.",
            "The parameters depend on hidden states.",
            "And we would like to have some other parameters which give us the same model, but where the numbers in the new set of operators where the numbers in these operators.",
            "Can be learned from some."
        ],
        [
            "And to do that, we rely on the very simple observation.",
            "That is that if we have a change of change of basis Q that is an invertible matrix and we apply these transformations.",
            "So we.",
            "We apply this change of basis to any of the operators.",
            "Then we are still computing the same conditional distribution.",
            "So now we're left with the with the task of finding some queue where the new operators can be learned from the sample and the idea to do that comes from literature in the 80s on subspace identification methods which deal with linear systems on continuous state.",
            "Ann were supported to other other fields closer to these FT transition learning like in the literature of multiplicity, automata and spectral."
        ],
        [
            "Ning.",
            "So the idea is that to try to find a queue that depends on.",
            "On unigram bigram, antigen probabilities which you can easily observe in the sample.",
            "So for example we define a vector or which has which collects the marginal probability of the first symbol being each one of the input on the output alphabet, sorry.",
            "On the bigram probabilities and then triggering probabilities when here we are conditioning on the second input symbol.",
            "Ann, it turns out that this set of parameters of parameters, unigram, bigram and trigram probabilities.",
            "Are are enough to define to find the new operators, so we have the following theorem which says that if we take Q equal to U transpose oh where U collects the singular vectors of this bigram matrix P. Then we can.",
            "We can apply this change of basis Q to our operators.",
            "And express the new operators in terms of observable quantities.",
            "So that would be European PIV, which you can estimate from a sample.",
            "So once the one has this theorem that so maybe an institution would say that ropy and PBS efficient statistics of the model that is, given these parameters, we can compute anything that the model can compute any any conditional probability we can easily obtain."
        ],
        [
            "A learning algorithm that does the following.",
            "You give it an input and output alphabet and you supply a number of hidden states an.",
            "And given an input training sample of purse, an purse is why?",
            "Why, why why I, sorry?",
            "We can approximate these matrices, the unigram, the bigram, and the tree gram matrices with relative frequencies in the sample.",
            "And afterwards we can perform these singular vector decomposition on this approximation of P and have an approximate U.",
            "And finally, we can use the previous expressions, so we use these three expressions on these.",
            "All these approximations to compute the new operators an these new operators can be used to compute conditional probabilities.",
            "On a very nice thing about this algorithm is that it scales very good.",
            "In fact, it scales linearly with the size of the training sample.",
            "Becausw to compute all these probabilities, it only takes over and time, where N is the size of the sample, and then the hard part of the computation.",
            "The SVD depends only on the size of the output alphabet."
        ],
        [
            "And another nice thing about this algorithm is that one can prove a pack style result fully for it.",
            "And so to prove the result, you have to assume some things.",
            "So for example, we have an input distribution on the.",
            "On X star which has a length expected length, Lambda and where we have this parameter mu which is the smallest probability of the symbol appearing in the second place in the input sequences.",
            "Then we assume that we have an FST that is a conditional distribution with M states which has to satisfy some wrong assumptions that I won't discuss here.",
            "You can see more details in the paper and wrong assumptions.",
            "I mean assumptions on the rank of the OH and the matrix.",
            "And then we assume that we are given IID samples from the joint distribution on this input and conditional distribution, and then we can prove the following, which is a pack style result that says that if the if the algorithm receives enough sample where the sample bound is this one.",
            "Then, with probability at least one minus Delta is with high probability and the algorithm will provide you with the model that has a small error less than epsilon where this error term is in fact the L1 distance between the joint distribution.",
            "With the original FST and the joint distribution with the model output by the algorithm.",
            "And if, if you're familiar with previous work done and similar things for Hmm's, the difference here in this theorem is that you have to take into account the role of the input distribution, where for Hmm's you didn't have to do that because it's only your learning distribution, not a conditional distribution."
        ],
        [
            "And we perform some experiments with this algorithm and the first set of experiments is synthetic experiment where we would like to compare against other spectral baselines in a setting where the hypothesis holds.",
            "So that's why the spammers synthetic, because they are generated with tests that we that we know, and in fact we generally run, we randomly generate some FCS with this alphabet and number of hidden states.",
            "And we run our algorithm and which is the FST and with the red line.",
            "And we run two baselines.",
            "And here you can see the performance in number of training samples and thousands against the L1 distance to the target with respect the output model.",
            "And these baselines, the first one, the one in the green line, is, we call it hmm.",
            "And what we do there is try to model the the input output joint distribution with a single HMM.",
            "On the HMM would we do is try to learn a different hmm for each of the input symbols because we know that.",
            "That if we have operators that depend on input and output symbols, we can multiply them in place.",
            "So we learn one hmm for each of the input symbols.",
            "And what we observe is first that with large training samples, the our algorithm is better.",
            "And we also observed that with the small training samples, the HMM approach the joint approaches is better is some better.",
            "And it turns out that this there's a similar fact in classification, where it is known that sometimes with a small sample sizes, generative models perform better than discriminative models.",
            "So we believe this is another.",
            "Realization of this kind of phenomenon."
        ],
        [
            "And then we perform the experiment in a real task, which is a transliteration task.",
            "And if you've walked around nothing, you have seen a lot of examples of these tasks where you have the all the names of the streets are first in the in.",
            "Hellenic alphabet and underneath the arinda in Latin alphabet.",
            "So here the task is similar, but we used an English to Russian transliteration task so.",
            "We want to be given a name written in English.",
            "Producer name written in Cyrillic which challenge more more or less equivalently and this is necessary when you have named entities for which there is no translation.",
            "So names you don't translate then you just find something that sounds the same.",
            "And the goal of this experiment was to compare our algorithm with them in a real task, where the modeling assumptions fail.",
            "So our we know that our algorithm can learn FS because we have the theorem.",
            "But if you are given real data, maybe it doesn't work, but it turns out that it works and it performs similar to EM when.",
            "When you are given enough training samples and it performs better when the training set is smaller.",
            "And we run that with two and three hidden states.",
            "Here M was used to train and the same model.",
            "The same kind of model that we are learning with the FST.",
            "So we dealt them to run the parameters of an FST in observable operator models.",
            "An apart from the performance, which is normalized edit distance, we also see the that our algorithm is with is much faster than the M. So for example, in this point in the plot the spectrum to 26 seconds to train while the M 2000 seconds because we have to do many iterations in the M and one iteration.",
            "Yeah right, 37 seconds.",
            "On a detail here, in this payment is that as I told you, our spectral algorithm only learns conditional probabilities between aligned sequences.",
            "So here sometimes the sequences were not aligned like in this example, where these two hours go to a single Y.",
            "So we represent the data and produce some alignments, but that was outside the method because our method does not deal with alignment."
        ],
        [
            "Right now.",
            "And so to summarize our contributions, I'd say that we provide the fast and scalable spectral method for learning input output operator models.",
            "For instance, users an we can give a strong theoretical guarantees for it, and with very few assumptions on the input distribution, which is also interesting, and we have seen that we outperformed the previous spectral algorithms on the FFT faster and better than them in some real tasks."
        ],
        [
            "And that job.",
            "OK, thank you for the talk.",
            "I didn't understand clearly if in your's input of your algorithm you have pairs of sequences of pairs of symbols.",
            "Sorry, it's pairs of sequences.",
            "OK."
        ],
        [
            "So X sub OK so I used Subs for indices inside sequences, an uppers, upper indices to you know different examples.",
            "So these are these are sequence pairs of sequences OK and in your bound I mean in the back result.",
            "Yeah I'm surprised that you don't have the size of the input sequence or the output sequence in particular.",
            "It seems not abuse to be able to some about all the possible outputs with wise style I mean.",
            "Well, yes, you have the size of the inputs is measured by this Lambda, which is the expected length which appears here.",
            "So you can also drive around if you have a distribution that generates only sequences of certain length.",
            "But we we we prefer to state this in a more general setting where your training examples.",
            "They all have the same length, so you have 3 examples of different lengths.",
            "But in your case the number of outputs are finite or infinite.",
            "What do you mean?",
            "The number of?",
            "Uh?",
            "Because given an X can you have an infinite output sequences?",
            "Why or is this just know they have the same length?",
            "Oh OK. OK, and I guess you have probably negative probabilities sometimes before conventions of the argument.",
            "Sorry you may have some negative probabilities before the convergence of the algorithm.",
            "In practice, I mean in your experiments.",
            "Yeah, well, the algorithm is in it's one step.",
            "Yes, but I mean if you work with HMM, generally you measure the maximum likelihood of the sample.",
            "So I guess in your case sometimes you have negative probabilities.",
            "No, when you compute the probability of the sequence.",
            "Well, I think that this unit it can happen, but not if you have enough samples.",
            "So if the if the B if these matrices are well enough estimated you don't see a negative probabilities.",
            "OK, I have two questions.",
            "The first one is you start by saying that EM get stuck in local Optima and I was wondering then for your algorithm what happens with local optimize that issue resolved or not?",
            "And my second question is very often when we try to learn models where there are some hidden variables is going to be identifiability issues.",
            "So does your algorithm deal with these things or not?",
            "And how is that reflected in the edit distance measure?",
            "OK, so for the first for the first question we don't have a local Optima, so that's what the theorem is saying.",
            "If the sample is large enough, you'll find a set of operators that that has a small distance.",
            "Show us, for example, if you give epsilon to smaller and smaller, you're half as more and more error, but you require more and more samples.",
            "Ann for the second question.",
            "So we are not recovering probabilities on.",
            "So we're not recovering these operators in this form.",
            "That would give us probabilities on.",
            "So probably this condition on hidden States and where we could do this kind of inference, I think that you're suggesting, but we recover another set of operators where the operating in a different basis, so we're not working directly with the original hidden state, so we have a kind of transformation of the hidden state space.",
            "So this answers your question your question.",
            "I have a question.",
            "How does this scale with this output number of output states?",
            "So you have this factor there and that the practice of the practical number of outputs you can work with you have.",
            "This is Corinne Lick alphabet.",
            "Can you also have thousands of outputs or?",
            "Well, that's a good question, yeah, so we haven't tried already with very large output of what you see here that ask US 34 symbols.",
            "And probably this cubic dependence would get very bad with large output alphabets.",
            "But we are aware that there are various other variations on the composition algorithm for matrices that work much faster and give approximations to SVD.",
            "So we would like to try that and see if.",
            "If we can, if we recover similar things in practice and if we can prove some theorems for that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint work with Ariana Cottone and Chevy Carreras.",
                    "label": 0
                },
                {
                    "sent": "You're sitting in there.",
                    "label": 0
                },
                {
                    "sent": "And we will present the spectral learning algorithm for finding state transducers, and I'll start by describing this.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Object will talk about.",
                    "label": 0
                },
                {
                    "sent": "Which in this case are probabilistic transducers which we use them to model input output relations between sequences over some finite alphabets and we.",
                    "label": 0
                },
                {
                    "sent": "We assume that this input output relations are modeled using hidden states.",
                    "label": 1
                },
                {
                    "sent": "And so the model would defines its conditional distribution over an output sequence.",
                    "label": 0
                },
                {
                    "sent": "I given an input sequence X.",
                    "label": 0
                },
                {
                    "sent": "And so the model has needed some independent assumptions that you can see in this diagram.",
                    "label": 0
                },
                {
                    "sent": "So you're given an input sequence, an output sequence, and.",
                    "label": 0
                },
                {
                    "sent": "Us as time grows, the what you observe at time T or for example time three depends only on the hidden state where the machine is at time 3.",
                    "label": 0
                },
                {
                    "sent": "On the next step, at the next hidden state, the state four depends on the current state and the symbol you're observing at this time.",
                    "label": 0
                },
                {
                    "sent": "And these machines that define these conditional probabilities over sequences can be used in many applications like NLP or biology, and in general anywhere where you have data which appears in pairs of sequences.",
                    "label": 1
                },
                {
                    "sent": "And the problem we want to solve here is learning this conditional probabilities and in general this is hard cause of this hidden state layer.",
                    "label": 0
                },
                {
                    "sent": "And what many people do in practices use EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "Another iterative methods that usually have local minima and conversions problems.",
                    "label": 0
                },
                {
                    "sent": "So that's why the problem is interesting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we do here is represent the spectral algorithm for doing that, which is fast.",
                    "label": 1
                },
                {
                    "sent": "An scales very good with the size of the training set for which we have pack style theorem with gives you convergence guarantees.",
                    "label": 1
                },
                {
                    "sent": "And the idea of the algorithm is that it is based on a representation of these transducers that's called observe operator models.",
                    "label": 0
                },
                {
                    "sent": "And this means that we represent the parameters of the transducer in the form of some matrices that I'll explain in a minute.",
                    "label": 0
                },
                {
                    "sent": "On using these matrices, we can use a spectral methods, in particular.",
                    "label": 1
                },
                {
                    "sent": "Single value decomposition's to learn these objects and there's been some recent literature in the spectral methods for running other objects for, for example, at gyms.",
                    "label": 1
                },
                {
                    "sent": "So the difference here would be that in the transductive setting you have an input that's conditioning your output, which you don't have in the HMM setting.",
                    "label": 0
                },
                {
                    "sent": "And then, in addition to the theoretical guarantees, we show that in practice we outperforming them in time and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Predictive performance.",
                    "label": 0
                },
                {
                    "sent": "So in the rest of the talk, just introduced these observable operators.",
                    "label": 1
                },
                {
                    "sent": "Then I'll explain how do you learn these operators and I'll present some of our expand.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for the rest of the talk I will assume that the sequences X&Y are of the same length over some finite alphabets X&Y.",
                    "label": 0
                },
                {
                    "sent": "An would our model does is compute this conditional probability.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing is that you can express this probability as a product of matrices.",
                    "label": 0
                },
                {
                    "sent": "An initial vector Alpha.",
                    "label": 1
                },
                {
                    "sent": "Ann, well to derive this expression you rely on the independence assumptions I showed in the initial diagram.",
                    "label": 0
                },
                {
                    "sent": "Ann, it's mainly the derivation is mainly using the usual forward Barwa backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you can think that this is another way of stating the forward backward algorithm, where Alpha is initial state distribution an each of these matrices, which is which are operators on this operator operator model.",
                    "label": 0
                },
                {
                    "sent": "Define the so given actualization of the.",
                    "label": 0
                },
                {
                    "sent": "Of the state distribution.",
                    "label": 0
                },
                {
                    "sent": "For example, at time TI have a state distribution of a T and I multiply by one of these operators, and I obtained the state distribution in a time T + 1.",
                    "label": 0
                },
                {
                    "sent": "And what makes these models so these models are called observable operator models?",
                    "label": 1
                },
                {
                    "sent": "Becausw the operator you choose at any point in time depends only on what you're observing, which is the input and the output.",
                    "label": 0
                },
                {
                    "sent": "And now that you know what the operators are, I'll tell you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inside these matrices an.",
                    "label": 0
                },
                {
                    "sent": "It turns out that, well, we can fix some notation.",
                    "label": 0
                },
                {
                    "sent": "We let the input symbols we buy.",
                    "label": 0
                },
                {
                    "sent": "A is.",
                    "label": 0
                },
                {
                    "sent": "The output symbols are beasts and the hidden states are sees their K input symbols, L output symbols and M states.",
                    "label": 0
                },
                {
                    "sent": "And in these operators these matrices are defined in terms of the the usual and normal parameters you have in Pristiq transducers, which are the probability of next state given a current observation and current state which we collect in the in matrices and probability of summer vacation given the current state.",
                    "label": 0
                },
                {
                    "sent": "And by the factorization, so the dependence assumptions we can compute this operator as a product of two matrices, one depending on the input symbol and another one depending on the output symbol.",
                    "label": 0
                },
                {
                    "sent": "So we end up with.",
                    "label": 0
                },
                {
                    "sent": "With K of this M * M matrices, the TAS, then this, there's another.",
                    "label": 0
                },
                {
                    "sent": "There's L N * M matrices, but this ones are diagonal.",
                    "label": 0
                },
                {
                    "sent": "By this chroniker Delta.",
                    "label": 0
                },
                {
                    "sent": "So we like to collect these these the handle on this.",
                    "label": 0
                },
                {
                    "sent": "This on Rosen metrics we call.",
                    "label": 0
                },
                {
                    "sent": "Oh and then there's the initial state distribution.",
                    "label": 0
                },
                {
                    "sent": "But now that we have these operators, we see that in terms of learning, this is not really helping us, because the probabilities in this operator depend on the on a conditioned on hidden states which we cannot observe in the sample because we are only observing the input and output sequences.",
                    "label": 0
                },
                {
                    "sent": "So we sell the operation.",
                    "label": 0
                },
                {
                    "sent": "The parameters depend on hidden states.",
                    "label": 0
                },
                {
                    "sent": "And we would like to have some other parameters which give us the same model, but where the numbers in the new set of operators where the numbers in these operators.",
                    "label": 0
                },
                {
                    "sent": "Can be learned from some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to do that, we rely on the very simple observation.",
                    "label": 0
                },
                {
                    "sent": "That is that if we have a change of change of basis Q that is an invertible matrix and we apply these transformations.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We apply this change of basis to any of the operators.",
                    "label": 0
                },
                {
                    "sent": "Then we are still computing the same conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So now we're left with the with the task of finding some queue where the new operators can be learned from the sample and the idea to do that comes from literature in the 80s on subspace identification methods which deal with linear systems on continuous state.",
                    "label": 1
                },
                {
                    "sent": "Ann were supported to other other fields closer to these FT transition learning like in the literature of multiplicity, automata and spectral.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ning.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that to try to find a queue that depends on.",
                    "label": 0
                },
                {
                    "sent": "On unigram bigram, antigen probabilities which you can easily observe in the sample.",
                    "label": 0
                },
                {
                    "sent": "So for example we define a vector or which has which collects the marginal probability of the first symbol being each one of the input on the output alphabet, sorry.",
                    "label": 0
                },
                {
                    "sent": "On the bigram probabilities and then triggering probabilities when here we are conditioning on the second input symbol.",
                    "label": 0
                },
                {
                    "sent": "Ann, it turns out that this set of parameters of parameters, unigram, bigram and trigram probabilities.",
                    "label": 1
                },
                {
                    "sent": "Are are enough to define to find the new operators, so we have the following theorem which says that if we take Q equal to U transpose oh where U collects the singular vectors of this bigram matrix P. Then we can.",
                    "label": 1
                },
                {
                    "sent": "We can apply this change of basis Q to our operators.",
                    "label": 1
                },
                {
                    "sent": "And express the new operators in terms of observable quantities.",
                    "label": 0
                },
                {
                    "sent": "So that would be European PIV, which you can estimate from a sample.",
                    "label": 0
                },
                {
                    "sent": "So once the one has this theorem that so maybe an institution would say that ropy and PBS efficient statistics of the model that is, given these parameters, we can compute anything that the model can compute any any conditional probability we can easily obtain.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A learning algorithm that does the following.",
                    "label": 1
                },
                {
                    "sent": "You give it an input and output alphabet and you supply a number of hidden states an.",
                    "label": 1
                },
                {
                    "sent": "And given an input training sample of purse, an purse is why?",
                    "label": 0
                },
                {
                    "sent": "Why, why why I, sorry?",
                    "label": 1
                },
                {
                    "sent": "We can approximate these matrices, the unigram, the bigram, and the tree gram matrices with relative frequencies in the sample.",
                    "label": 0
                },
                {
                    "sent": "And afterwards we can perform these singular vector decomposition on this approximation of P and have an approximate U.",
                    "label": 0
                },
                {
                    "sent": "And finally, we can use the previous expressions, so we use these three expressions on these.",
                    "label": 1
                },
                {
                    "sent": "All these approximations to compute the new operators an these new operators can be used to compute conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "On a very nice thing about this algorithm is that it scales very good.",
                    "label": 0
                },
                {
                    "sent": "In fact, it scales linearly with the size of the training sample.",
                    "label": 0
                },
                {
                    "sent": "Becausw to compute all these probabilities, it only takes over and time, where N is the size of the sample, and then the hard part of the computation.",
                    "label": 0
                },
                {
                    "sent": "The SVD depends only on the size of the output alphabet.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And another nice thing about this algorithm is that one can prove a pack style result fully for it.",
                    "label": 0
                },
                {
                    "sent": "And so to prove the result, you have to assume some things.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have an input distribution on the.",
                    "label": 0
                },
                {
                    "sent": "On X star which has a length expected length, Lambda and where we have this parameter mu which is the smallest probability of the symbol appearing in the second place in the input sequences.",
                    "label": 0
                },
                {
                    "sent": "Then we assume that we have an FST that is a conditional distribution with M states which has to satisfy some wrong assumptions that I won't discuss here.",
                    "label": 1
                },
                {
                    "sent": "You can see more details in the paper and wrong assumptions.",
                    "label": 0
                },
                {
                    "sent": "I mean assumptions on the rank of the OH and the matrix.",
                    "label": 0
                },
                {
                    "sent": "And then we assume that we are given IID samples from the joint distribution on this input and conditional distribution, and then we can prove the following, which is a pack style result that says that if the if the algorithm receives enough sample where the sample bound is this one.",
                    "label": 0
                },
                {
                    "sent": "Then, with probability at least one minus Delta is with high probability and the algorithm will provide you with the model that has a small error less than epsilon where this error term is in fact the L1 distance between the joint distribution.",
                    "label": 1
                },
                {
                    "sent": "With the original FST and the joint distribution with the model output by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And if, if you're familiar with previous work done and similar things for Hmm's, the difference here in this theorem is that you have to take into account the role of the input distribution, where for Hmm's you didn't have to do that because it's only your learning distribution, not a conditional distribution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we perform some experiments with this algorithm and the first set of experiments is synthetic experiment where we would like to compare against other spectral baselines in a setting where the hypothesis holds.",
                    "label": 0
                },
                {
                    "sent": "So that's why the spammers synthetic, because they are generated with tests that we that we know, and in fact we generally run, we randomly generate some FCS with this alphabet and number of hidden states.",
                    "label": 0
                },
                {
                    "sent": "And we run our algorithm and which is the FST and with the red line.",
                    "label": 0
                },
                {
                    "sent": "And we run two baselines.",
                    "label": 0
                },
                {
                    "sent": "And here you can see the performance in number of training samples and thousands against the L1 distance to the target with respect the output model.",
                    "label": 0
                },
                {
                    "sent": "And these baselines, the first one, the one in the green line, is, we call it hmm.",
                    "label": 0
                },
                {
                    "sent": "And what we do there is try to model the the input output joint distribution with a single HMM.",
                    "label": 0
                },
                {
                    "sent": "On the HMM would we do is try to learn a different hmm for each of the input symbols because we know that.",
                    "label": 0
                },
                {
                    "sent": "That if we have operators that depend on input and output symbols, we can multiply them in place.",
                    "label": 0
                },
                {
                    "sent": "So we learn one hmm for each of the input symbols.",
                    "label": 0
                },
                {
                    "sent": "And what we observe is first that with large training samples, the our algorithm is better.",
                    "label": 0
                },
                {
                    "sent": "And we also observed that with the small training samples, the HMM approach the joint approaches is better is some better.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this there's a similar fact in classification, where it is known that sometimes with a small sample sizes, generative models perform better than discriminative models.",
                    "label": 0
                },
                {
                    "sent": "So we believe this is another.",
                    "label": 0
                },
                {
                    "sent": "Realization of this kind of phenomenon.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we perform the experiment in a real task, which is a transliteration task.",
                    "label": 0
                },
                {
                    "sent": "And if you've walked around nothing, you have seen a lot of examples of these tasks where you have the all the names of the streets are first in the in.",
                    "label": 0
                },
                {
                    "sent": "Hellenic alphabet and underneath the arinda in Latin alphabet.",
                    "label": 0
                },
                {
                    "sent": "So here the task is similar, but we used an English to Russian transliteration task so.",
                    "label": 1
                },
                {
                    "sent": "We want to be given a name written in English.",
                    "label": 0
                },
                {
                    "sent": "Producer name written in Cyrillic which challenge more more or less equivalently and this is necessary when you have named entities for which there is no translation.",
                    "label": 0
                },
                {
                    "sent": "So names you don't translate then you just find something that sounds the same.",
                    "label": 0
                },
                {
                    "sent": "And the goal of this experiment was to compare our algorithm with them in a real task, where the modeling assumptions fail.",
                    "label": 1
                },
                {
                    "sent": "So our we know that our algorithm can learn FS because we have the theorem.",
                    "label": 0
                },
                {
                    "sent": "But if you are given real data, maybe it doesn't work, but it turns out that it works and it performs similar to EM when.",
                    "label": 0
                },
                {
                    "sent": "When you are given enough training samples and it performs better when the training set is smaller.",
                    "label": 0
                },
                {
                    "sent": "And we run that with two and three hidden states.",
                    "label": 0
                },
                {
                    "sent": "Here M was used to train and the same model.",
                    "label": 0
                },
                {
                    "sent": "The same kind of model that we are learning with the FST.",
                    "label": 0
                },
                {
                    "sent": "So we dealt them to run the parameters of an FST in observable operator models.",
                    "label": 0
                },
                {
                    "sent": "An apart from the performance, which is normalized edit distance, we also see the that our algorithm is with is much faster than the M. So for example, in this point in the plot the spectrum to 26 seconds to train while the M 2000 seconds because we have to do many iterations in the M and one iteration.",
                    "label": 0
                },
                {
                    "sent": "Yeah right, 37 seconds.",
                    "label": 0
                },
                {
                    "sent": "On a detail here, in this payment is that as I told you, our spectral algorithm only learns conditional probabilities between aligned sequences.",
                    "label": 0
                },
                {
                    "sent": "So here sometimes the sequences were not aligned like in this example, where these two hours go to a single Y.",
                    "label": 0
                },
                {
                    "sent": "So we represent the data and produce some alignments, but that was outside the method because our method does not deal with alignment.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "And so to summarize our contributions, I'd say that we provide the fast and scalable spectral method for learning input output operator models.",
                    "label": 0
                },
                {
                    "sent": "For instance, users an we can give a strong theoretical guarantees for it, and with very few assumptions on the input distribution, which is also interesting, and we have seen that we outperformed the previous spectral algorithms on the FFT faster and better than them in some real tasks.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that job.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for the talk.",
                    "label": 0
                },
                {
                    "sent": "I didn't understand clearly if in your's input of your algorithm you have pairs of sequences of pairs of symbols.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it's pairs of sequences.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So X sub OK so I used Subs for indices inside sequences, an uppers, upper indices to you know different examples.",
                    "label": 0
                },
                {
                    "sent": "So these are these are sequence pairs of sequences OK and in your bound I mean in the back result.",
                    "label": 0
                },
                {
                    "sent": "Yeah I'm surprised that you don't have the size of the input sequence or the output sequence in particular.",
                    "label": 0
                },
                {
                    "sent": "It seems not abuse to be able to some about all the possible outputs with wise style I mean.",
                    "label": 0
                },
                {
                    "sent": "Well, yes, you have the size of the inputs is measured by this Lambda, which is the expected length which appears here.",
                    "label": 0
                },
                {
                    "sent": "So you can also drive around if you have a distribution that generates only sequences of certain length.",
                    "label": 0
                },
                {
                    "sent": "But we we we prefer to state this in a more general setting where your training examples.",
                    "label": 0
                },
                {
                    "sent": "They all have the same length, so you have 3 examples of different lengths.",
                    "label": 0
                },
                {
                    "sent": "But in your case the number of outputs are finite or infinite.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "The number of?",
                    "label": 0
                },
                {
                    "sent": "Uh?",
                    "label": 0
                },
                {
                    "sent": "Because given an X can you have an infinite output sequences?",
                    "label": 0
                },
                {
                    "sent": "Why or is this just know they have the same length?",
                    "label": 0
                },
                {
                    "sent": "Oh OK. OK, and I guess you have probably negative probabilities sometimes before conventions of the argument.",
                    "label": 0
                },
                {
                    "sent": "Sorry you may have some negative probabilities before the convergence of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "In practice, I mean in your experiments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, the algorithm is in it's one step.",
                    "label": 0
                },
                {
                    "sent": "Yes, but I mean if you work with HMM, generally you measure the maximum likelihood of the sample.",
                    "label": 0
                },
                {
                    "sent": "So I guess in your case sometimes you have negative probabilities.",
                    "label": 0
                },
                {
                    "sent": "No, when you compute the probability of the sequence.",
                    "label": 0
                },
                {
                    "sent": "Well, I think that this unit it can happen, but not if you have enough samples.",
                    "label": 0
                },
                {
                    "sent": "So if the if the B if these matrices are well enough estimated you don't see a negative probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, I have two questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is you start by saying that EM get stuck in local Optima and I was wondering then for your algorithm what happens with local optimize that issue resolved or not?",
                    "label": 0
                },
                {
                    "sent": "And my second question is very often when we try to learn models where there are some hidden variables is going to be identifiability issues.",
                    "label": 0
                },
                {
                    "sent": "So does your algorithm deal with these things or not?",
                    "label": 0
                },
                {
                    "sent": "And how is that reflected in the edit distance measure?",
                    "label": 0
                },
                {
                    "sent": "OK, so for the first for the first question we don't have a local Optima, so that's what the theorem is saying.",
                    "label": 0
                },
                {
                    "sent": "If the sample is large enough, you'll find a set of operators that that has a small distance.",
                    "label": 0
                },
                {
                    "sent": "Show us, for example, if you give epsilon to smaller and smaller, you're half as more and more error, but you require more and more samples.",
                    "label": 0
                },
                {
                    "sent": "Ann for the second question.",
                    "label": 0
                },
                {
                    "sent": "So we are not recovering probabilities on.",
                    "label": 0
                },
                {
                    "sent": "So we're not recovering these operators in this form.",
                    "label": 0
                },
                {
                    "sent": "That would give us probabilities on.",
                    "label": 0
                },
                {
                    "sent": "So probably this condition on hidden States and where we could do this kind of inference, I think that you're suggesting, but we recover another set of operators where the operating in a different basis, so we're not working directly with the original hidden state, so we have a kind of transformation of the hidden state space.",
                    "label": 0
                },
                {
                    "sent": "So this answers your question your question.",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "How does this scale with this output number of output states?",
                    "label": 0
                },
                {
                    "sent": "So you have this factor there and that the practice of the practical number of outputs you can work with you have.",
                    "label": 0
                },
                {
                    "sent": "This is Corinne Lick alphabet.",
                    "label": 0
                },
                {
                    "sent": "Can you also have thousands of outputs or?",
                    "label": 0
                },
                {
                    "sent": "Well, that's a good question, yeah, so we haven't tried already with very large output of what you see here that ask US 34 symbols.",
                    "label": 0
                },
                {
                    "sent": "And probably this cubic dependence would get very bad with large output alphabets.",
                    "label": 0
                },
                {
                    "sent": "But we are aware that there are various other variations on the composition algorithm for matrices that work much faster and give approximations to SVD.",
                    "label": 0
                },
                {
                    "sent": "So we would like to try that and see if.",
                    "label": 0
                },
                {
                    "sent": "If we can, if we recover similar things in practice and if we can prove some theorems for that.",
                    "label": 0
                }
            ]
        }
    }
}