{
    "id": "ochjzt7ikoajyubyocjd2nteaznfwgl3",
    "title": "Unsupervised Transfer Classification: Application to Text Categorization",
    "info": {
        "author": [
            "Tianbao Yang, Computer Science Department, University of Iowa"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd2010_yang_utlatc/",
    "segmentation": [
        [
            "So OK classification problem has been started for decades, not only in machine learning but also in data mining.",
            "In the beginning, classification is started early in supervised learning fashion, where we have a bunch of examples on.",
            "Each example has a class label.",
            "Later on people realize for some datasets, the label could be very expensive to get, so they started to consider semi supervised learning.",
            "Here we only have small amount of labeled examples, while most examples unlabeled.",
            "So right now the question is what if no label information is available at all, but we still want to build a classification model.",
            "So this task is impossible but not with some additional information.",
            "So in this."
        ],
        [
            "Paper we consider a new scenario which is we called unsupervised transfer classification.",
            "The setup is we have a collection of training examples and their assignments to a set of classes which we called auxiliary classes, but the goal is trying to build a classification model for the target class.",
            "There is no labeling labeled training examples for the target class, but we do assume some additional information such as the prior probability for the target class, the conditional probabilities for the talk about given the auxiliary classes.",
            "OK."
        ],
        [
            "Give you some motivation examples we can consider image annotation or social tagging.",
            "So the problem is how to predict annotation tag or social tag that does not appear in the training data.",
            "So here this tag notation tag or social tags that appear in the training data can be consider us the target class and the text that appeared in the training data can be considered as auxiliary classes."
        ],
        [
            "OK, so in order to better position our work, we leased several topics skills that is related to our work.",
            "The first one is transfer learning.",
            "The transfer learning is to try try to transfer knowledge from a source domain to attacker domain and we also try to transfer the label information for the auxiliary classes to the target class differences that we do not assume any label information for the target class and also some religional includes multi label learning maximum entropy model.",
            "Details in the paper."
        ],
        [
            "OK, so let's jump through this unsupervised transfer classification problem, so we'll see we have ourselves training examples XI and their assignments to a set of classes which we call our theory classes C1 to CK.",
            "But the goal is to try to build a classification model on a target task.",
            "City that does not belong to this auxiliary classes.",
            "OK, but we do assume some additional information such as the prior probability for the class, the conditional probability for the packet cuts given auxiliary classes.",
            "And which we call this information as to customization and the goal is to use this data and this information to build the classification model for the target class."
        ],
        [
            "OK, so in this paper we consider to use maximum entropy model to build the classification model.",
            "The reason is that we know that the maximum entropy model essentially use the sufficient statistics to compete to build the classification model.",
            "So if we have a way to estimate the sufficient statistics.",
            "Even without knowing the exact label for each individual example, we can still build the classification model.",
            "So taking a look at this maximum entropy model, the objective is.",
            "Interview for the conditional classification model on the training examples by maximizing this interview, we tend to favor uniform distribution.",
            "We do have some constraints on the conditional classification model, so the constraints is that the feature statistics computed from the conditional model is equal to the feature statistics computed from the training data.",
            "So here the FJ is the JS feature function defined on the training data.",
            "And this Telefunken is a Delta Connect function, which is 1 if Y is quite wide and 0 otherwise.",
            "OK."
        ],
        [
            "So.",
            "In the first step, to reach our goal, we first generalize the traditional maximum entropy model.",
            "So take a look at this equality constraint again.",
            "So it is intuitively correct that we hope these two features listings are equal to each other.",
            "But the question is, is there any just theoretical justification for this equality constraints?",
            "So by concentration inequality we can show that the difference between these two features statistics is actually bounded and in order of one of square root.",
            "So if the number of examples goes to Infinity, there's two features.",
            "Trees are exactly equal to each other.",
            "But with finite number of examples and also always show later this one this feature spiritistic cannot be computed directly from the data.",
            "So we need a way to estimate these features therapeutics.",
            "So there is no reason for us to continue to use this equally constraints, so we generally first generalize this equality constraint into inequality constraints.",
            "So here are this epsilon variables captures the difference between these two features statistics."
        ],
        [
            "And using this inequality constraints we have this generalized maximum entropy model.",
            "So in the objective we as a regularizer in turn on the issue variables in order to decide these variables automatically.",
            "OK, so we in this paper will consider to use this generalized maximum entropy model for the unsupervised transfer classification problem.",
            "But the question right now is we cannot compute."
        ],
        [
            "This feature statistics directly from the data because we do not know the label information for the type of class on the training examples.",
            "So the main contribution of the base of this paper is to present a robust approach to estimate these features listex.",
            "OK, I'm the way.",
            "Also verified the proposed approach by no bells in theoretically and empirically."
        ],
        [
            "So the question is how to estimate this feature statistics.",
            "First, we know that this quantity can be considered as an empirical average of this joint expectation.",
            "Consider the XYT is random variables.",
            "OK, and this joint expectation can be further written into the prior probability prior probability for the target class times this conditional probability.",
            "So if we have a way to estimate this conditional probability and we can use this estimation as a approximation for this condition.",
            "OK."
        ],
        [
            "So next, how to estimate this?",
            "The conditional expectation.",
            "So the strategy is to try to build up the relationship between the conditional expectation for the target class and the conditional expectation for the auxiliary classes.",
            "So in order to do that, we need to make some assumption.",
            "So assumption that we made is actually this independence assumption.",
            "So we just give us the conditional probability X.",
            "Given the one auxiliary class can be written as X, given the type class paragraphs given the auxiliary classes for each value of the target class.",
            "OK, from this independence of something we can derive that.",
            "So that the conditional expect the relationship between the conditional expectation for the auxiliary class and for the target class game.",
            "OK, and we have these equations for all auxiliary classes.",
            "And for features so we put on there for this conditional expectation, we can.",
            "We can get our estimation from the data because we know the label information.",
            "So put that is estimation into a matrix in head matrix and this conditional probability is that the class information we assumed to be new.",
            "So we put in the matrix and this quantity is.",
            "Is that what we want to estimate?",
            "So we put this into your mic."
        ],
        [
            "So we have this close relationship between a hat and W, and to get a solution for W we solve this least square problem over you.",
            "OK?"
        ],
        [
            "OK so I'm not getting this estimation wake applied into the generalized maximum maximum entropy model and solving it still problem over the deal variables, Lambda one, Lambda zero.",
            "We can get a solution for the condition classification model on the target class.",
            "And here the Africa is some function of EU.",
            "In previous slides you can find division definition in the paper."
        ],
        [
            "OK, so you're some consistent analysis results.",
            "So are so kill the Landstar is optimal dual solution.",
            "Using the label information for the Tiger class on training examples.",
            "So we assume the labeling information is known.",
            "This is optimal solution by assuming that.",
            "This one Lambda is a deal solution obtained by the proposed approach.",
            "So we show that with a large probability, the difference between these two solutions is is bounded in one over gamma times some constant Times Square root of the over.",
            "So we can see that when the number of examples is very large, the difference between these two solutions should be small.",
            "OK."
        ],
        [
            "So next I will show you some experimental results.",
            "So we did the experiment on the task of text categorization using some multi label devices.",
            "Including team say 2007 in Rome datasets and Bibtex and deliciousness.",
            "So in order to evaluate the proposed approach so this data contains multiple labels, so we leave one label also the target class and the remaining labels of auxiliary classes, and we compute AUC area and the RC curve for each target files and report the average one."
        ],
        [
            "So, for the baselines we choose three approach.",
            "For the first one weight rowing, a classifier for each auxiliary class, and we linearly combine these class files together.",
            "The classifier for the target class and refer it as to see model.",
            "In the second approach, we first predict the assignments of the assignment of the target class for the training examples by linearly combining labels for the auxiliary classes and then training a classifier using the predicted labels.",
            "Political class.",
            "And the River is us to say label in the third approach we we we compute the feature statistics.",
            "But for the target class by linearly combining those for the auxiliary classes.",
            "And use in general is maximum entropy model.",
            "So the proposed approach is referred to GMM regression."
        ],
        [
            "So this slide shows a comparison between our approach with the baseline and we can see that our approach is significantly better than the baselines.",
            "Even some baseline is doing OK on some data set, but not as bad as I approach."
        ],
        [
            "And these two figures shows the comparison between the unsupervised transfer classification with supervised classification.",
            "So in supervised classification, the blue curves we incrementally increasing increase the number of labeled examples for the target task we will try to look at how many labeled examples are needed to get the same performance as unsupervised transfer classification, and we observe that this number is actually significant.",
            "1500 for BX 2005, 1500 four divisions and you can find some for other two data sets in the paper."
        ],
        [
            "OK, the problems with previous experiment is that the class information is computed from the data using the label information for the auxiliary classes and also for the target class.",
            "But we cannot do that in the real application because we do not know the label information for the target glass in real application.",
            "So in the following experiments we try to use the class information estimated from external sources.",
            "So we choose PBX and initial data set.",
            "For big banks we used salami and SMD stove laborat to estimate the cost information.",
            "For delicious, we use delicious websites, so estimate that.",
            "And here are the results.",
            "This column shows the result using the class information estimates from the data and these two cousins shows the result using the class information is estimated from external sources and we can see that even using the class information is estimated from the external sources.",
            "We can still get compatible results or that using the class information estimated from the data.",
            "In order to be sent to me is better than SMD celebrity cause the baby text data set is actually called from before me."
        ],
        [
            "OK.",
            "So this slide show the comparison between the unsupervised transfer classification with supervised classification by increasing the number of labeled examples.",
            "And again we see that the number, although not large previous but is also significant."
        ],
        [
            "So conclusions we consider new problem which is unsupervised transfer classification.",
            "We present our strategic framework for unsupervised transfer classification based on generalized maximum entropy model and we have a robust estimate for feature services for the target class and we have some consistent analysis.",
            "And for future work we consider to relax the independence assumption and also we consider to get a better estimation for the future statistics for the target class."
        ],
        [
            "Thanks.",
            "So this is not so much a question as an observation or another.",
            "Term for this problem is deep learning.",
            "Just point you to some people, primarily in the neural network community on Mccune for example, and others as well as some people in computer vision.",
            "We're trying to look at unsupervised extraction of sufficient statistics and alternate representation of the data and then to motivate them.",
            "Use those as input to a classification or learning algorithm, and there's some actual some some examples in computer vision, community robotics, community doing.",
            "Doing that for for object avoidance, for example.",
            "So it's not a question, it's just this is a very interesting area of research."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK classification problem has been started for decades, not only in machine learning but also in data mining.",
                    "label": 0
                },
                {
                    "sent": "In the beginning, classification is started early in supervised learning fashion, where we have a bunch of examples on.",
                    "label": 0
                },
                {
                    "sent": "Each example has a class label.",
                    "label": 0
                },
                {
                    "sent": "Later on people realize for some datasets, the label could be very expensive to get, so they started to consider semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Here we only have small amount of labeled examples, while most examples unlabeled.",
                    "label": 0
                },
                {
                    "sent": "So right now the question is what if no label information is available at all, but we still want to build a classification model.",
                    "label": 1
                },
                {
                    "sent": "So this task is impossible but not with some additional information.",
                    "label": 1
                },
                {
                    "sent": "So in this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper we consider a new scenario which is we called unsupervised transfer classification.",
                    "label": 0
                },
                {
                    "sent": "The setup is we have a collection of training examples and their assignments to a set of classes which we called auxiliary classes, but the goal is trying to build a classification model for the target class.",
                    "label": 1
                },
                {
                    "sent": "There is no labeling labeled training examples for the target class, but we do assume some additional information such as the prior probability for the target class, the conditional probabilities for the talk about given the auxiliary classes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give you some motivation examples we can consider image annotation or social tagging.",
                    "label": 1
                },
                {
                    "sent": "So the problem is how to predict annotation tag or social tag that does not appear in the training data.",
                    "label": 1
                },
                {
                    "sent": "So here this tag notation tag or social tags that appear in the training data can be consider us the target class and the text that appeared in the training data can be considered as auxiliary classes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in order to better position our work, we leased several topics skills that is related to our work.",
                    "label": 0
                },
                {
                    "sent": "The first one is transfer learning.",
                    "label": 0
                },
                {
                    "sent": "The transfer learning is to try try to transfer knowledge from a source domain to attacker domain and we also try to transfer the label information for the auxiliary classes to the target class differences that we do not assume any label information for the target class and also some religional includes multi label learning maximum entropy model.",
                    "label": 1
                },
                {
                    "sent": "Details in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's jump through this unsupervised transfer classification problem, so we'll see we have ourselves training examples XI and their assignments to a set of classes which we call our theory classes C1 to CK.",
                    "label": 1
                },
                {
                    "sent": "But the goal is to try to build a classification model on a target task.",
                    "label": 1
                },
                {
                    "sent": "City that does not belong to this auxiliary classes.",
                    "label": 1
                },
                {
                    "sent": "OK, but we do assume some additional information such as the prior probability for the class, the conditional probability for the packet cuts given auxiliary classes.",
                    "label": 0
                },
                {
                    "sent": "And which we call this information as to customization and the goal is to use this data and this information to build the classification model for the target class.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in this paper we consider to use maximum entropy model to build the classification model.",
                    "label": 0
                },
                {
                    "sent": "The reason is that we know that the maximum entropy model essentially use the sufficient statistics to compete to build the classification model.",
                    "label": 0
                },
                {
                    "sent": "So if we have a way to estimate the sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "Even without knowing the exact label for each individual example, we can still build the classification model.",
                    "label": 0
                },
                {
                    "sent": "So taking a look at this maximum entropy model, the objective is.",
                    "label": 1
                },
                {
                    "sent": "Interview for the conditional classification model on the training examples by maximizing this interview, we tend to favor uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "We do have some constraints on the conditional classification model, so the constraints is that the feature statistics computed from the conditional model is equal to the feature statistics computed from the training data.",
                    "label": 1
                },
                {
                    "sent": "So here the FJ is the JS feature function defined on the training data.",
                    "label": 0
                },
                {
                    "sent": "And this Telefunken is a Delta Connect function, which is 1 if Y is quite wide and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the first step, to reach our goal, we first generalize the traditional maximum entropy model.",
                    "label": 0
                },
                {
                    "sent": "So take a look at this equality constraint again.",
                    "label": 0
                },
                {
                    "sent": "So it is intuitively correct that we hope these two features listings are equal to each other.",
                    "label": 0
                },
                {
                    "sent": "But the question is, is there any just theoretical justification for this equality constraints?",
                    "label": 1
                },
                {
                    "sent": "So by concentration inequality we can show that the difference between these two features statistics is actually bounded and in order of one of square root.",
                    "label": 0
                },
                {
                    "sent": "So if the number of examples goes to Infinity, there's two features.",
                    "label": 0
                },
                {
                    "sent": "Trees are exactly equal to each other.",
                    "label": 0
                },
                {
                    "sent": "But with finite number of examples and also always show later this one this feature spiritistic cannot be computed directly from the data.",
                    "label": 0
                },
                {
                    "sent": "So we need a way to estimate these features therapeutics.",
                    "label": 0
                },
                {
                    "sent": "So there is no reason for us to continue to use this equally constraints, so we generally first generalize this equality constraint into inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "So here are this epsilon variables captures the difference between these two features statistics.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And using this inequality constraints we have this generalized maximum entropy model.",
                    "label": 0
                },
                {
                    "sent": "So in the objective we as a regularizer in turn on the issue variables in order to decide these variables automatically.",
                    "label": 0
                },
                {
                    "sent": "OK, so we in this paper will consider to use this generalized maximum entropy model for the unsupervised transfer classification problem.",
                    "label": 0
                },
                {
                    "sent": "But the question right now is we cannot compute.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This feature statistics directly from the data because we do not know the label information for the type of class on the training examples.",
                    "label": 0
                },
                {
                    "sent": "So the main contribution of the base of this paper is to present a robust approach to estimate these features listex.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm the way.",
                    "label": 0
                },
                {
                    "sent": "Also verified the proposed approach by no bells in theoretically and empirically.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is how to estimate this feature statistics.",
                    "label": 1
                },
                {
                    "sent": "First, we know that this quantity can be considered as an empirical average of this joint expectation.",
                    "label": 0
                },
                {
                    "sent": "Consider the XYT is random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, and this joint expectation can be further written into the prior probability prior probability for the target class times this conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So if we have a way to estimate this conditional probability and we can use this estimation as a approximation for this condition.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next, how to estimate this?",
                    "label": 0
                },
                {
                    "sent": "The conditional expectation.",
                    "label": 0
                },
                {
                    "sent": "So the strategy is to try to build up the relationship between the conditional expectation for the target class and the conditional expectation for the auxiliary classes.",
                    "label": 1
                },
                {
                    "sent": "So in order to do that, we need to make some assumption.",
                    "label": 1
                },
                {
                    "sent": "So assumption that we made is actually this independence assumption.",
                    "label": 0
                },
                {
                    "sent": "So we just give us the conditional probability X.",
                    "label": 0
                },
                {
                    "sent": "Given the one auxiliary class can be written as X, given the type class paragraphs given the auxiliary classes for each value of the target class.",
                    "label": 0
                },
                {
                    "sent": "OK, from this independence of something we can derive that.",
                    "label": 0
                },
                {
                    "sent": "So that the conditional expect the relationship between the conditional expectation for the auxiliary class and for the target class game.",
                    "label": 0
                },
                {
                    "sent": "OK, and we have these equations for all auxiliary classes.",
                    "label": 0
                },
                {
                    "sent": "And for features so we put on there for this conditional expectation, we can.",
                    "label": 0
                },
                {
                    "sent": "We can get our estimation from the data because we know the label information.",
                    "label": 0
                },
                {
                    "sent": "So put that is estimation into a matrix in head matrix and this conditional probability is that the class information we assumed to be new.",
                    "label": 0
                },
                {
                    "sent": "So we put in the matrix and this quantity is.",
                    "label": 0
                },
                {
                    "sent": "Is that what we want to estimate?",
                    "label": 0
                },
                {
                    "sent": "So we put this into your mic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have this close relationship between a hat and W, and to get a solution for W we solve this least square problem over you.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I'm not getting this estimation wake applied into the generalized maximum maximum entropy model and solving it still problem over the deal variables, Lambda one, Lambda zero.",
                    "label": 0
                },
                {
                    "sent": "We can get a solution for the condition classification model on the target class.",
                    "label": 0
                },
                {
                    "sent": "And here the Africa is some function of EU.",
                    "label": 1
                },
                {
                    "sent": "In previous slides you can find division definition in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you're some consistent analysis results.",
                    "label": 0
                },
                {
                    "sent": "So are so kill the Landstar is optimal dual solution.",
                    "label": 1
                },
                {
                    "sent": "Using the label information for the Tiger class on training examples.",
                    "label": 1
                },
                {
                    "sent": "So we assume the labeling information is known.",
                    "label": 0
                },
                {
                    "sent": "This is optimal solution by assuming that.",
                    "label": 1
                },
                {
                    "sent": "This one Lambda is a deal solution obtained by the proposed approach.",
                    "label": 0
                },
                {
                    "sent": "So we show that with a large probability, the difference between these two solutions is is bounded in one over gamma times some constant Times Square root of the over.",
                    "label": 0
                },
                {
                    "sent": "So we can see that when the number of examples is very large, the difference between these two solutions should be small.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next I will show you some experimental results.",
                    "label": 0
                },
                {
                    "sent": "So we did the experiment on the task of text categorization using some multi label devices.",
                    "label": 0
                },
                {
                    "sent": "Including team say 2007 in Rome datasets and Bibtex and deliciousness.",
                    "label": 0
                },
                {
                    "sent": "So in order to evaluate the proposed approach so this data contains multiple labels, so we leave one label also the target class and the remaining labels of auxiliary classes, and we compute AUC area and the RC curve for each target files and report the average one.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, for the baselines we choose three approach.",
                    "label": 0
                },
                {
                    "sent": "For the first one weight rowing, a classifier for each auxiliary class, and we linearly combine these class files together.",
                    "label": 1
                },
                {
                    "sent": "The classifier for the target class and refer it as to see model.",
                    "label": 0
                },
                {
                    "sent": "In the second approach, we first predict the assignments of the assignment of the target class for the training examples by linearly combining labels for the auxiliary classes and then training a classifier using the predicted labels.",
                    "label": 1
                },
                {
                    "sent": "Political class.",
                    "label": 0
                },
                {
                    "sent": "And the River is us to say label in the third approach we we we compute the feature statistics.",
                    "label": 1
                },
                {
                    "sent": "But for the target class by linearly combining those for the auxiliary classes.",
                    "label": 0
                },
                {
                    "sent": "And use in general is maximum entropy model.",
                    "label": 0
                },
                {
                    "sent": "So the proposed approach is referred to GMM regression.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this slide shows a comparison between our approach with the baseline and we can see that our approach is significantly better than the baselines.",
                    "label": 0
                },
                {
                    "sent": "Even some baseline is doing OK on some data set, but not as bad as I approach.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these two figures shows the comparison between the unsupervised transfer classification with supervised classification.",
                    "label": 0
                },
                {
                    "sent": "So in supervised classification, the blue curves we incrementally increasing increase the number of labeled examples for the target task we will try to look at how many labeled examples are needed to get the same performance as unsupervised transfer classification, and we observe that this number is actually significant.",
                    "label": 0
                },
                {
                    "sent": "1500 for BX 2005, 1500 four divisions and you can find some for other two data sets in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the problems with previous experiment is that the class information is computed from the data using the label information for the auxiliary classes and also for the target class.",
                    "label": 0
                },
                {
                    "sent": "But we cannot do that in the real application because we do not know the label information for the target glass in real application.",
                    "label": 0
                },
                {
                    "sent": "So in the following experiments we try to use the class information estimated from external sources.",
                    "label": 1
                },
                {
                    "sent": "So we choose PBX and initial data set.",
                    "label": 0
                },
                {
                    "sent": "For big banks we used salami and SMD stove laborat to estimate the cost information.",
                    "label": 0
                },
                {
                    "sent": "For delicious, we use delicious websites, so estimate that.",
                    "label": 0
                },
                {
                    "sent": "And here are the results.",
                    "label": 0
                },
                {
                    "sent": "This column shows the result using the class information estimates from the data and these two cousins shows the result using the class information is estimated from external sources and we can see that even using the class information is estimated from the external sources.",
                    "label": 0
                },
                {
                    "sent": "We can still get compatible results or that using the class information estimated from the data.",
                    "label": 0
                },
                {
                    "sent": "In order to be sent to me is better than SMD celebrity cause the baby text data set is actually called from before me.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this slide show the comparison between the unsupervised transfer classification with supervised classification by increasing the number of labeled examples.",
                    "label": 1
                },
                {
                    "sent": "And again we see that the number, although not large previous but is also significant.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusions we consider new problem which is unsupervised transfer classification.",
                    "label": 1
                },
                {
                    "sent": "We present our strategic framework for unsupervised transfer classification based on generalized maximum entropy model and we have a robust estimate for feature services for the target class and we have some consistent analysis.",
                    "label": 1
                },
                {
                    "sent": "And for future work we consider to relax the independence assumption and also we consider to get a better estimation for the future statistics for the target class.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So this is not so much a question as an observation or another.",
                    "label": 0
                },
                {
                    "sent": "Term for this problem is deep learning.",
                    "label": 0
                },
                {
                    "sent": "Just point you to some people, primarily in the neural network community on Mccune for example, and others as well as some people in computer vision.",
                    "label": 0
                },
                {
                    "sent": "We're trying to look at unsupervised extraction of sufficient statistics and alternate representation of the data and then to motivate them.",
                    "label": 0
                },
                {
                    "sent": "Use those as input to a classification or learning algorithm, and there's some actual some some examples in computer vision, community robotics, community doing.",
                    "label": 0
                },
                {
                    "sent": "Doing that for for object avoidance, for example.",
                    "label": 0
                },
                {
                    "sent": "So it's not a question, it's just this is a very interesting area of research.",
                    "label": 0
                }
            ]
        }
    }
}