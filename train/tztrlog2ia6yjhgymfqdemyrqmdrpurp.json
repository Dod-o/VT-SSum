{
    "id": "tztrlog2ia6yjhgymfqdemyrqmdrpurp",
    "title": "Learning Incoherent Sparse and Low-Rank Patterns from Multiple Tasks",
    "info": {
        "author": [
            "Jianhui Chen, Department of Computer Science and Engineering, Arizona State University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_chen_lislrpmt/",
    "segmentation": [
        [
            "OK, good morning everyone.",
            "I'm John Coltrane from Arizona State.",
            "My talk is about learning incoherent, sparse, and low rank patterns from multiple tasks.",
            "This is joint work with video and GPS."
        ],
        [
            "In real world applications we often need to learn multiple tasks.",
            "Assume we are given multiple tasks.",
            "So for each task we need to learn a functional mapping as defined on the data point X and the corresponding label one.",
            "So.",
            "In tradition, we can apply the single task learning scheme to learn the multiple tasks.",
            "In this scheme we we do not utilize the task relationship, we just learn each task independently so we can just start from task one and then go to task two and the last one.",
            "Recently people are more interested in the multi task learning.",
            "So in the second one we assume the tasks are correlated.",
            "So we try to utilize the task relationship and put them together and learn them simultaneously.",
            "Add the major difference between the two.",
            "3rd is whether we utilized the task relationship in our work.",
            "We focus on the second one."
        ],
        [
            "So we start from each."
        ],
        [
            "Action a natural question is what's the goal of multi task learning?",
            "The most natural approach for task learning is to apply the single task learning.",
            "So through the multi task learning scheme we hope we can improve the overall generalization performance, but how the performance could be improved?",
            "In single task learning we just learn the task in dependently, but in reality the task could be related so so we try to find the relationship of the tasks and.",
            "It means the learning result of the first task could be useful for the learning of the rest task.",
            "So if we learn the task simultaneously and utilize the relationship, we could improve the performance.",
            "And when do we need the multitask learning in supervised learning?",
            "In general, labeling the training data could be expensive, so if we are giving a large number of training data, large number of tasks, but for each task the training sample size is very limited.",
            "I will could apply the multitask learning."
        ],
        [
            "The key issue in multitask learning is to model the task relationship.",
            "So many approaches has been proposed for this purpose, such as share hidden units in the neural network and model the common prior invention network.",
            "And so on.",
            "Our work is related to the two work.",
            "Learn a shared low rank structure from multiple tasks and apply that wisdom regularization for multi task."
        ],
        [
            "Learning.",
            "Multitask learning has been applied in many areas such as Bell Informatics, medical image analysis, web search ranking, and computer vision."
        ],
        [
            "In our work, we propose a novel multitask learning formulation."
        ],
        [
            "Assume we are giving them tasks.",
            "For each task, we are giving a set of training data X&Y.",
            "Denote the training data point pair.",
            "And if we learned them, predict EM functions.",
            "We could use the function for future prediction.",
            "In our work we focus on the linear classifiers.",
            "For the M tasks we are given, so now we have a linear prediction function.",
            "For each function we have a transformation vector.",
            "So each task could be represented using the transformation vector.",
            "In the multi task learning we assume all the tasks are correlated under while some underlying relationship."
        ],
        [
            "So.",
            "The low ranks our structure has been applied for capture the task relationship, so for them tasks we have transformation vectors and they can form a predictive structure.",
            "We assume such a structure like in a low dimensional subspace and the end task could be related using just a small set of basis vectors.",
            "But the limitation is the low rank structure could be too restrictive for the tasks, although the tasks are correlated, they are still different in some sense.",
            "So we consider to learn an independent component for each task.",
            "But in high dimensional data analysis, the effective feature could be sparse.",
            "So we think the independent component could be sparse and we propose to model the task relationship using the low rank structure and sparse structure as well."
        ],
        [
            "So we denote the concatenation of the transformation vector using the capital Z the matrix and they could be decomposed as two component P&Q.",
            "Here are pure AP includes the independent sparse.",
            "Transformation vector.",
            "And I killed Dinos.",
            "The low rank structures.",
            "The end task could be correlated using just a small set of the transformation vector.",
            "So now we're giving a."
        ],
        [
            "Give an example for the incoherent sparse and low rank structure for the face recognition problem.",
            "Assume we need to recognize 5 persons, so each recognition of each person corresponding to 1 task.",
            "So through the multitask learning, we hope we can obtain the sparse structure on the left and the low rank structure on the right.",
            "We can resolve our.",
            "The sparse structure can capture the detailed futuremark information of the human face and the low rank structure could capture the rough shape of the human face.",
            "So in our experiments we demonstrate.",
            "The sparse and low rank structure can improve the performance.",
            "Are in the in Mannix."
        ],
        [
            "So based on the same machine we propose the multi last multi task learning formulation.",
            "So we have the incoherent structure.",
            "There is equal to the sum of P&Q and the sparse structure is induced while the L0 norm and the low rank structure is induced while the rank rank constraint on the on the matrix Q.",
            "This formulation is interpretable, but it is still some limitation.",
            "It's it's nonconvex, so it's not easy for us to find the globally optimal solution, and it's NP hard, so so far we do not know any efficient solution for this one.",
            "We consider to find a convex counterpart for this formulation.",
            "The direct approach is to substitute the term of the zero norm and the rank function, so this substitution is."
        ],
        [
            "Based on the concept of common, low convex slope.",
            "In the literature it is known.",
            "The convex over our love of L0 norm is L1 norm and similarly chase them is the convex envelope of the rank function.",
            "So convex envelope is defined as the tightest convex function which approximates the function from below.",
            "So on the left side we have the non convex function on the right side we have the convex envelope and the right side could approximately the left side reasonably so based on."
        ],
        [
            "This substitution we obtain a convex relaxation.",
            "In this formulation, we still have the incoherent structure.",
            "They equal to sum of P&Q, but now the.",
            "Support structure is induced while the L1 norm and L random approximate L zero norm and the low rank structure is induced while the trace norm and trace them can approximate the rank function.",
            "This function looks better, is convex in the objective function we have a non smooth term and the domain set is convex and is bounded.",
            "So we can.",
            "This formulation can be first.",
            "Can be further reformulated as a semidefinite program and many existing servers, such as the domain, can be applied to solve this problem, but SDP is not scalable for large scale data set in experiments it can only handle like several hundreds of optimization variables."
        ],
        [
            "So we consider to apply.",
            "The projected gradient scheme to solve this problem?"
        ],
        [
            "So recall we need to solve this convex formulation.",
            "It can be rewritten into this compact form, so we have smooth term now smooth term and the convex domain set.",
            "The project is a greater scheme, finds the optimal solution by constructing a solution sequence so we can start from arbitrary starting point T1 and based on some updating scheme we get T2T3.",
            "Finally, this sequence can be proved to converge to the optimum solution T star, and in this scheme we try to avoid the computation of the heist matrix, so that's why it's more efficient than the 2nd order method."
        ],
        [
            "So for our particular multitask formulation, we need to solve this optimization.",
            "Problem so here.",
            "TTI plus one denotes the solution point, and as I noted the searching point solution part is always visible for our problem searching point is not necessary and the construction of searching point depends on the detailed algorithms and it can lead to different conversion rate in the algorithm.",
            "1 / L I specified the step size on on the gradient step on SI.",
            "So in the most simple case, if GI is equal to 0 and M correspond to the whole real space, than the optimization problem can be reduced as the most simple one.",
            "So in that case the problem is reduced to the smooth objective function and unconstrained problem.",
            "So I skipped the detailed description of the scheme here so more details can be found from."
        ],
        [
            "Our paper.",
            "So.",
            "For our for our multi task than any formulation the optimization problem could be represented in this form.",
            "So in this one we have two variables TP and TQ.",
            "We can observe TP and TQ actually are decompiled, so this problem can be optimized through the two formulation for the left one we optimize the TP, TP and the left one has closed form.",
            "For the right one is a matrix projection problem and we show the optimal solution can be obtained by.",
            "So an SVD and projection prob."
        ],
        [
            "Now we presented the detailed."
        ],
        [
            "Algorithms?",
            "So for this type of algorithm we need to we have two component.",
            "We need to find the value of Li Wireline search and specify the corresponding step size and we solve the second optimization problem.",
            "So as we just discussed the construction as I depends on the detailed algorithm.",
            "So we present the two algorithms, the left one projected gradient algorithm.",
            "So for this one we just set as I equal to TI and this can lead to the convergence convergence rate of 1 / K for the right one.",
            "The accelerator version is based on the scheme proposed by next and we set as I as the linear combination of the TI and the previous previous solution point TMS one and we can obtain the convergence rate of one over.",
            "Case Square and then numerically the second one is much much more efficient than the."
        ],
        [
            "Just one.",
            "We also conduct experiments on."
        ],
        [
            "A collection of data set and we do comparison on the on the proposed multi task learning formulation and other methods we do.",
            "We measure we.",
            "We use a UCS one as the performance measures from the experiments.",
            "We can observe the proposed incoherent, sparse and low rank structure can improve the performance and we also observe the Sparks pass structure is particularly efficient for the multimedia data.",
            "And the low rank structure is more important on the image data and the biologic data.",
            "We are."
        ],
        [
            "Also do comparison on the efficiency, so we compare the PG method and the accelerated version.",
            "We compare the convergence curve and the number of iterations and the computation time so the result shows AG is much more efficient than the first one."
        ],
        [
            "So in conclusion we propose multi task learning formulation and its efficient algorithm.",
            "We also conduct experiments for demonstration in the future.",
            "We plan to conduct theoretical analysis on the proposed formulation and we plan to apply the formulation on more real world applications.",
            "So that's the protection, thank you."
        ],
        [
            "Kill trap.",
            "Actually, yeah, I I think.",
            "Oh OK, so in our formulation.",
            "The transformation could be decomposed as P&Q, so P is in the objective function as sterilization and kill is used as a constraint.",
            "IS uses in the constraint?",
            "Yeah, because actually.",
            "If I put the theoretically, I think that equipment if we move from constraint to objective function.",
            "But in the experiment if we put in the objective function I cannot achieve the exact low rank.",
            "Yeah, actually when I did the experiment there are some.",
            "I mean just the money.",
            "The testimony is the singular value is very small, like that empowered by minors for something.",
            "But it is not exactly 0, but if we put on on the computer I can get exactly 0 if I do a projection.",
            "So I just want to ask you what can last function use?",
            "I don't see that in our experiments we use the least square loss, but actually any smooth function can be applied for this formulation."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "I'm John Coltrane from Arizona State.",
                    "label": 0
                },
                {
                    "sent": "My talk is about learning incoherent, sparse, and low rank patterns from multiple tasks.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with video and GPS.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In real world applications we often need to learn multiple tasks.",
                    "label": 0
                },
                {
                    "sent": "Assume we are given multiple tasks.",
                    "label": 0
                },
                {
                    "sent": "So for each task we need to learn a functional mapping as defined on the data point X and the corresponding label one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In tradition, we can apply the single task learning scheme to learn the multiple tasks.",
                    "label": 1
                },
                {
                    "sent": "In this scheme we we do not utilize the task relationship, we just learn each task independently so we can just start from task one and then go to task two and the last one.",
                    "label": 0
                },
                {
                    "sent": "Recently people are more interested in the multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So in the second one we assume the tasks are correlated.",
                    "label": 0
                },
                {
                    "sent": "So we try to utilize the task relationship and put them together and learn them simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Add the major difference between the two.",
                    "label": 0
                },
                {
                    "sent": "3rd is whether we utilized the task relationship in our work.",
                    "label": 0
                },
                {
                    "sent": "We focus on the second one.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we start from each.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action a natural question is what's the goal of multi task learning?",
                    "label": 0
                },
                {
                    "sent": "The most natural approach for task learning is to apply the single task learning.",
                    "label": 0
                },
                {
                    "sent": "So through the multi task learning scheme we hope we can improve the overall generalization performance, but how the performance could be improved?",
                    "label": 1
                },
                {
                    "sent": "In single task learning we just learn the task in dependently, but in reality the task could be related so so we try to find the relationship of the tasks and.",
                    "label": 0
                },
                {
                    "sent": "It means the learning result of the first task could be useful for the learning of the rest task.",
                    "label": 0
                },
                {
                    "sent": "So if we learn the task simultaneously and utilize the relationship, we could improve the performance.",
                    "label": 1
                },
                {
                    "sent": "And when do we need the multitask learning in supervised learning?",
                    "label": 0
                },
                {
                    "sent": "In general, labeling the training data could be expensive, so if we are giving a large number of training data, large number of tasks, but for each task the training sample size is very limited.",
                    "label": 0
                },
                {
                    "sent": "I will could apply the multitask learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The key issue in multitask learning is to model the task relationship.",
                    "label": 0
                },
                {
                    "sent": "So many approaches has been proposed for this purpose, such as share hidden units in the neural network and model the common prior invention network.",
                    "label": 1
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Our work is related to the two work.",
                    "label": 0
                },
                {
                    "sent": "Learn a shared low rank structure from multiple tasks and apply that wisdom regularization for multi task.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Multitask learning has been applied in many areas such as Bell Informatics, medical image analysis, web search ranking, and computer vision.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our work, we propose a novel multitask learning formulation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assume we are giving them tasks.",
                    "label": 0
                },
                {
                    "sent": "For each task, we are giving a set of training data X&Y.",
                    "label": 0
                },
                {
                    "sent": "Denote the training data point pair.",
                    "label": 0
                },
                {
                    "sent": "And if we learned them, predict EM functions.",
                    "label": 0
                },
                {
                    "sent": "We could use the function for future prediction.",
                    "label": 0
                },
                {
                    "sent": "In our work we focus on the linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "For the M tasks we are given, so now we have a linear prediction function.",
                    "label": 0
                },
                {
                    "sent": "For each function we have a transformation vector.",
                    "label": 0
                },
                {
                    "sent": "So each task could be represented using the transformation vector.",
                    "label": 0
                },
                {
                    "sent": "In the multi task learning we assume all the tasks are correlated under while some underlying relationship.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The low ranks our structure has been applied for capture the task relationship, so for them tasks we have transformation vectors and they can form a predictive structure.",
                    "label": 0
                },
                {
                    "sent": "We assume such a structure like in a low dimensional subspace and the end task could be related using just a small set of basis vectors.",
                    "label": 0
                },
                {
                    "sent": "But the limitation is the low rank structure could be too restrictive for the tasks, although the tasks are correlated, they are still different in some sense.",
                    "label": 0
                },
                {
                    "sent": "So we consider to learn an independent component for each task.",
                    "label": 0
                },
                {
                    "sent": "But in high dimensional data analysis, the effective feature could be sparse.",
                    "label": 0
                },
                {
                    "sent": "So we think the independent component could be sparse and we propose to model the task relationship using the low rank structure and sparse structure as well.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we denote the concatenation of the transformation vector using the capital Z the matrix and they could be decomposed as two component P&Q.",
                    "label": 0
                },
                {
                    "sent": "Here are pure AP includes the independent sparse.",
                    "label": 0
                },
                {
                    "sent": "Transformation vector.",
                    "label": 0
                },
                {
                    "sent": "And I killed Dinos.",
                    "label": 0
                },
                {
                    "sent": "The low rank structures.",
                    "label": 0
                },
                {
                    "sent": "The end task could be correlated using just a small set of the transformation vector.",
                    "label": 0
                },
                {
                    "sent": "So now we're giving a.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give an example for the incoherent sparse and low rank structure for the face recognition problem.",
                    "label": 0
                },
                {
                    "sent": "Assume we need to recognize 5 persons, so each recognition of each person corresponding to 1 task.",
                    "label": 0
                },
                {
                    "sent": "So through the multitask learning, we hope we can obtain the sparse structure on the left and the low rank structure on the right.",
                    "label": 0
                },
                {
                    "sent": "We can resolve our.",
                    "label": 0
                },
                {
                    "sent": "The sparse structure can capture the detailed futuremark information of the human face and the low rank structure could capture the rough shape of the human face.",
                    "label": 1
                },
                {
                    "sent": "So in our experiments we demonstrate.",
                    "label": 1
                },
                {
                    "sent": "The sparse and low rank structure can improve the performance.",
                    "label": 0
                },
                {
                    "sent": "Are in the in Mannix.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based on the same machine we propose the multi last multi task learning formulation.",
                    "label": 1
                },
                {
                    "sent": "So we have the incoherent structure.",
                    "label": 1
                },
                {
                    "sent": "There is equal to the sum of P&Q and the sparse structure is induced while the L0 norm and the low rank structure is induced while the rank rank constraint on the on the matrix Q.",
                    "label": 1
                },
                {
                    "sent": "This formulation is interpretable, but it is still some limitation.",
                    "label": 0
                },
                {
                    "sent": "It's it's nonconvex, so it's not easy for us to find the globally optimal solution, and it's NP hard, so so far we do not know any efficient solution for this one.",
                    "label": 0
                },
                {
                    "sent": "We consider to find a convex counterpart for this formulation.",
                    "label": 0
                },
                {
                    "sent": "The direct approach is to substitute the term of the zero norm and the rank function, so this substitution is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based on the concept of common, low convex slope.",
                    "label": 0
                },
                {
                    "sent": "In the literature it is known.",
                    "label": 0
                },
                {
                    "sent": "The convex over our love of L0 norm is L1 norm and similarly chase them is the convex envelope of the rank function.",
                    "label": 0
                },
                {
                    "sent": "So convex envelope is defined as the tightest convex function which approximates the function from below.",
                    "label": 1
                },
                {
                    "sent": "So on the left side we have the non convex function on the right side we have the convex envelope and the right side could approximately the left side reasonably so based on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This substitution we obtain a convex relaxation.",
                    "label": 1
                },
                {
                    "sent": "In this formulation, we still have the incoherent structure.",
                    "label": 0
                },
                {
                    "sent": "They equal to sum of P&Q, but now the.",
                    "label": 0
                },
                {
                    "sent": "Support structure is induced while the L1 norm and L random approximate L zero norm and the low rank structure is induced while the trace norm and trace them can approximate the rank function.",
                    "label": 0
                },
                {
                    "sent": "This function looks better, is convex in the objective function we have a non smooth term and the domain set is convex and is bounded.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "This formulation can be first.",
                    "label": 0
                },
                {
                    "sent": "Can be further reformulated as a semidefinite program and many existing servers, such as the domain, can be applied to solve this problem, but SDP is not scalable for large scale data set in experiments it can only handle like several hundreds of optimization variables.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we consider to apply.",
                    "label": 0
                },
                {
                    "sent": "The projected gradient scheme to solve this problem?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So recall we need to solve this convex formulation.",
                    "label": 0
                },
                {
                    "sent": "It can be rewritten into this compact form, so we have smooth term now smooth term and the convex domain set.",
                    "label": 1
                },
                {
                    "sent": "The project is a greater scheme, finds the optimal solution by constructing a solution sequence so we can start from arbitrary starting point T1 and based on some updating scheme we get T2T3.",
                    "label": 0
                },
                {
                    "sent": "Finally, this sequence can be proved to converge to the optimum solution T star, and in this scheme we try to avoid the computation of the heist matrix, so that's why it's more efficient than the 2nd order method.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for our particular multitask formulation, we need to solve this optimization.",
                    "label": 0
                },
                {
                    "sent": "Problem so here.",
                    "label": 0
                },
                {
                    "sent": "TTI plus one denotes the solution point, and as I noted the searching point solution part is always visible for our problem searching point is not necessary and the construction of searching point depends on the detailed algorithms and it can lead to different conversion rate in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "1 / L I specified the step size on on the gradient step on SI.",
                    "label": 0
                },
                {
                    "sent": "So in the most simple case, if GI is equal to 0 and M correspond to the whole real space, than the optimization problem can be reduced as the most simple one.",
                    "label": 1
                },
                {
                    "sent": "So in that case the problem is reduced to the smooth objective function and unconstrained problem.",
                    "label": 0
                },
                {
                    "sent": "So I skipped the detailed description of the scheme here so more details can be found from.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our paper.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For our for our multi task than any formulation the optimization problem could be represented in this form.",
                    "label": 0
                },
                {
                    "sent": "So in this one we have two variables TP and TQ.",
                    "label": 0
                },
                {
                    "sent": "We can observe TP and TQ actually are decompiled, so this problem can be optimized through the two formulation for the left one we optimize the TP, TP and the left one has closed form.",
                    "label": 1
                },
                {
                    "sent": "For the right one is a matrix projection problem and we show the optimal solution can be obtained by.",
                    "label": 1
                },
                {
                    "sent": "So an SVD and projection prob.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we presented the detailed.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "So for this type of algorithm we need to we have two component.",
                    "label": 0
                },
                {
                    "sent": "We need to find the value of Li Wireline search and specify the corresponding step size and we solve the second optimization problem.",
                    "label": 1
                },
                {
                    "sent": "So as we just discussed the construction as I depends on the detailed algorithm.",
                    "label": 1
                },
                {
                    "sent": "So we present the two algorithms, the left one projected gradient algorithm.",
                    "label": 1
                },
                {
                    "sent": "So for this one we just set as I equal to TI and this can lead to the convergence convergence rate of 1 / K for the right one.",
                    "label": 0
                },
                {
                    "sent": "The accelerator version is based on the scheme proposed by next and we set as I as the linear combination of the TI and the previous previous solution point TMS one and we can obtain the convergence rate of one over.",
                    "label": 0
                },
                {
                    "sent": "Case Square and then numerically the second one is much much more efficient than the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just one.",
                    "label": 0
                },
                {
                    "sent": "We also conduct experiments on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A collection of data set and we do comparison on the on the proposed multi task learning formulation and other methods we do.",
                    "label": 0
                },
                {
                    "sent": "We measure we.",
                    "label": 0
                },
                {
                    "sent": "We use a UCS one as the performance measures from the experiments.",
                    "label": 0
                },
                {
                    "sent": "We can observe the proposed incoherent, sparse and low rank structure can improve the performance and we also observe the Sparks pass structure is particularly efficient for the multimedia data.",
                    "label": 1
                },
                {
                    "sent": "And the low rank structure is more important on the image data and the biologic data.",
                    "label": 1
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also do comparison on the efficiency, so we compare the PG method and the accelerated version.",
                    "label": 0
                },
                {
                    "sent": "We compare the convergence curve and the number of iterations and the computation time so the result shows AG is much more efficient than the first one.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion we propose multi task learning formulation and its efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "We also conduct experiments for demonstration in the future.",
                    "label": 1
                },
                {
                    "sent": "We plan to conduct theoretical analysis on the proposed formulation and we plan to apply the formulation on more real world applications.",
                    "label": 1
                },
                {
                    "sent": "So that's the protection, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kill trap.",
                    "label": 0
                },
                {
                    "sent": "Actually, yeah, I I think.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so in our formulation.",
                    "label": 0
                },
                {
                    "sent": "The transformation could be decomposed as P&Q, so P is in the objective function as sterilization and kill is used as a constraint.",
                    "label": 0
                },
                {
                    "sent": "IS uses in the constraint?",
                    "label": 0
                },
                {
                    "sent": "Yeah, because actually.",
                    "label": 0
                },
                {
                    "sent": "If I put the theoretically, I think that equipment if we move from constraint to objective function.",
                    "label": 0
                },
                {
                    "sent": "But in the experiment if we put in the objective function I cannot achieve the exact low rank.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually when I did the experiment there are some.",
                    "label": 0
                },
                {
                    "sent": "I mean just the money.",
                    "label": 0
                },
                {
                    "sent": "The testimony is the singular value is very small, like that empowered by minors for something.",
                    "label": 0
                },
                {
                    "sent": "But it is not exactly 0, but if we put on on the computer I can get exactly 0 if I do a projection.",
                    "label": 0
                },
                {
                    "sent": "So I just want to ask you what can last function use?",
                    "label": 0
                },
                {
                    "sent": "I don't see that in our experiments we use the least square loss, but actually any smooth function can be applied for this formulation.",
                    "label": 0
                }
            ]
        }
    }
}