{
    "id": "jv7yhuk5qq7l27da6oftcwdfjtjeg674",
    "title": "Deep Learning on Graphs",
    "info": {
        "author": [
            "Jure Leskovec, Computer Science Department, Stanford University"
        ],
        "published": "Jan. 17, 2019",
        "recorded": "November 2018",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning"
        ]
    },
    "url": "http://videolectures.net/solomon_leskovec_deep_learning/",
    "segmentation": [
        [
            "Thank you for having me.",
            "It's exciting to be home and to tell you what I've been up to in the last year.",
            "So this is work about taking kind of the deep learning technology and see how it applies to graphs and applications of it.",
            "Will will come from what I'm doing at Pinterest.",
            "Applications will come through the collaborations I have, the chance.",
            "Acrobatic bio hub so kind of biologists and so on.",
            "So this won't be about social networks, right?",
            "My research is about networks.",
            "Why I like networks is because you can take.",
            "Many different types of data, many different types of complex systems, and you can."
        ],
        [
            "And take them, create this diagrams of interactions and you can represent them with the same underlying mathematical representation with a graph with a set of nodes and a set of relationships between those nodes.",
            "An if we think about, you know if we want to compute or do any kind of machine learning on these types of data, then the quest."
        ],
        [
            "Use what kind of tasks can we do on top of networks, right?",
            "And here are some examples, right?",
            "I can do what is called node classification, which is I want to be able to predict a type or a color of a given node.",
            "I can do link prediction which is about predicting whether a pair of nodes is linked or not, and then I can also analyze networks at the level of groups of nodes, and I can think about detecting clusters or communities of nodes, and I can also think about working with entire graphs.",
            "Thinking about how similar are different networks and how do I learn similarities between them and kind of the technology will be talking about today, will allow us to solve any any of these kind of abstract problems and I'll give you examples later, right?"
        ],
        [
            "So just to be kind of more concrete, you could.",
            "You could imagine you are given a graph and you know some nodes have colors and the Gray nodes have no colors and the question is how would you infer the color of the Gray node, right?",
            "Is it is it orange or is it Violet?",
            "And kind of the one of the one of the stumbling blocks in applying machine learning to these types of data is that you have to somehow feature eyes.",
            "The graph come up with some kind of features, attributes of the nodes in the network, so then you can build some predictors over that right, and you know, here are some ideas how you may want to go and create those features, but it's very time consuming and it's very unattractive and boring thing to do.",
            "So the question is, can we go around that?"
        ],
        [
            "So in some sense, if you think about machine learning lifecycle, let's say applied to graphs but applied to any other kind of data as well is that you start with the raw data, their own network you want to now create some kind of feature representation or feature you want to do kind of feature engineering to come up with the features so that then you can apply your favorite machine learning algorithm to be able to produce the model and make some predictions right?",
            "And usually the most consuming time is happens here where you do the feature engineering where you want to say oh how do I describe?",
            "The position and the network structure of the network around the node.",
            "What kind of intuitions do I want to build so that I create the feature vector so that I can go?",
            "And apply my method right and we do this every single time for every single application.",
            "So the question then becomes, could I somehow sidestep this feature learning part and just automatically learn over networks and forget about feature engineering, right?",
            "So the idea of what?"
        ],
        [
            "We'll explore today is how can you take a network and build this mapping that takes a node in the network and Maps it to some vector of real numbers.",
            "We will call this vector on embedding feature representation of that particular node, and our hope will be that this particular vector in D dimensions will somehow capture the network structure around this particular node.",
            "And just to give you an example what I'm talking."
        ],
        [
            "About"
        ],
        [
            "You can take the most famous network of them all.",
            "This is the Zachary Karate club network.",
            "These are people who went to the.",
            "To train karate and then there was a.",
            "There was a quarrel and this network split into two groups.",
            "But basically what I'm trying to say here is these are the nodes and here is for example some kind of embedding of these nodes in 222 dimensions and you can see how knows that have similar positions in the network are kind of embedded into similar part of the space an if I'm able to do these types of things but not only in two dimensions but in more dimensions.",
            "And if I can learn this embeddings in some kind of supervised way, then it allows me to reasonover graphs in a very interesting way, right?"
        ],
        [
            "So in some sense, the goal is to map every node to some low dimensional space and then basically similarity between nodes in this space will somehow encode, you know link strength, strength, network similarity between the nodes and the idea is that this will both kind of incorporate the network information as well as the node information in creating this low dimensional embedding, and after that I can use that for.",
            "Anomaly detection, you know, node classification, clustering link prediction, things like that.",
            "OK, so this is kind of abstractly what we want.",
            "Another way you."
        ],
        [
            "And think about this is that I want to learn functions that you know will take in as an entire network, or will take in a node of the network and will produce some kind of embedding of those nodes, right?",
            "So I want to build figure out how to take nodes and map them to some small dimensional space so that nodes that are similar in the network are embedded close together.",
            "In practice this dimensionality D is, let's say around 1000.",
            "OK, So what I would like to do 'cause you know?",
            "We like."
        ],
        [
            "Neural networks because they're kind of very powerful computation type graphs is I want to take my neural network and oh sorry, my real graph, and I'd like to pass it through the neural network so so that at the end I can make certain predictions about the nodes of the graph or about the entire graph and the question really is, how could I design A neural network architecture that on one end will take the graph and you know on the other end will compute something useful for me.",
            "So let me now tell you why is doing that hard."
        ],
        [
            "The first thing why this is hard is that the modern deep learning toolbox is designed for simple data types, right?",
            "So if you think about images, images are just simple grid graphs of fixed size, right?",
            "So when you when you have convolutional neural networks and you operate and you apply your convolutional operator, you're basically sliding over this fixed size grid graph, right?",
            "If you think about language speech, it's even simpler.",
            "It's a sequence, right?",
            "So it's like a.",
            "Chain graph, right?",
            "If you think about the networks we want to work with, they have."
        ],
        [
            "Much more complex structures, right?",
            "So they have arbitrary size.",
            "They have kind of complex topological properties, no spatial locality.",
            "Like grids and chains.",
            "Even more complicated is that the nodes of the network have an arbitrary ordering, so it's basically the same thing as saying you can flip the order of rows and columns in the image, and it should still be the same image.",
            "Many times.",
            "These graphs are dynamic, have multi model features.",
            "And so on.",
            "So in some sense, the question then becomes how do I take, you know, the success of convolutional neural networks when applied to images and sequences text and generalize these beyond simple lattices.",
            "OK, so that's kind of the topic.",
            "The topic for this talk.",
            "And as we will go and unfold this basically my talk will have four parts.",
            "I will first tell you how to do this, and then I'll give you 3 applications of this idea to three different domains.",
            "One will be large scale.",
            "Commander Systems, another one will be about drug side effect prediction and one will be about reasoning over knowledge graphs.",
            "All these three things will be able to do with this approach."
        ],
        [
            "OK, so we'll talk about this as the method we developed in our group called Graph Sage about a year ago, and it's about kind of supervised feature learning networks and at least what we do."
        ],
        [
            "To do is kind of.",
            "We'd like to generalize convolutional operators over images to general graphs.",
            "Now, how do?",
            "How do convolutions work?",
            "The idea is that you have that you have a an image, and then you have this convolutional operator that basically slides over the over the image, and then you know you have multiple multiple layers.",
            "As you take these operators, you create new feature Maps, do subsampling more convolutions, and kind of spread this over multiple layers.",
            "To compute some output of based on that input image.",
            "Now what does?"
        ],
        [
            "Does a single layer convolutional operator look like?",
            "It basically takes a little Patch of the image and computes some function of it.",
            "So another way how you could think of it from the graph point of view is that basically we have this subset of three by three pixels and somehow you are aggregating their their information, their values into the center node.",
            "Somehow you weigh them and aggregate them together, right?",
            "And you do this for every kind of possible window as you are sliding it.",
            "Over your image."
        ],
        [
            "The question becomes, what do you do when your graphs are not grids and don't have this nice spatial locality where you kind of where the notion of a sliding window kind of doesn't exist, right?",
            "As in social networks, information, networks and so on.",
            "So I'll give you now a few ideas how people try to do this, but I would say was not to."
        ],
        [
            "Successful so one way you could do this is to say I will take my graph.",
            "I will create the adjacency matrix.",
            "Plus I have some features, some properties of the nodes I will create this big matrix and I will take every row of this matrix and pass it through a big neural network.",
            "So that here at the end I can predict something about each individual node and this might sound like a good idea, but it's kind of not right.",
            "The reason it's not is 'cause the number of parameters you have in this network.",
            "Is is kind of order of the number of North of the number of nodes in your network.",
            "So basically the number of parameters scales linearly with the with the size of your data.",
            "Not sure how could you apply this with the graphs of different sizes and then this is not invariant to node reordering right?",
            "If I renumber my node in a different order then the rules of this matrix will shift so that so the inputs to this network will get confused and I don't know how to generalize across networks, so this was kind of 1 approach.",
            "Another approach called graph combo."
        ],
        [
            "Social networks try to come up with this idea of a sliding window where basically the idea is that we'll pick a node will somehow try to create a sub graph of it, and then for every node will have kind of a different sub graph.",
            "And now we have to do some kind of graph normalization so that we can take this graph, spread it into a fixed size vector which we can then use as an input into our neural network, right?",
            "And of course what is the problem?",
            "The problem is this this little subgraphs.",
            "Around different nodes have different structures and the question is how do you come up with some kind of Canonical node ordering that is consistent between different graphs?",
            "So these were all kind of previous approaches."
        ],
        [
            "We are interested in basically exploiting this idea that the nodes neighborhood in the in the social network defines the computation graph, right?",
            "So the idea is that basically if we want to compute maybe the label of my node I, I will look at the network neighborhood of that node and that will define the structure of the computation graph.",
            "So it will define the structure of the social network and then what I will learn is how to take information at the nodes around my target node.",
            "I and I will learn how to kind of propagate and transform.",
            "Information from the neighbors to compute something about node I that's kind of the high level idea, right?",
            "I want to learn how to propagate information over the graph in order to compute something about the node.",
            "So this is now the high level idea."
        ],
        [
            "Here is a concrete example.",
            "If you think here of my little graph on the left as the input graph.",
            "Here I give you the neural network architecture to compute something about the no day.",
            "Why is this network the way it is is because node A has three neighbors, his neighbors BC and D. So here they are.",
            "Neighbors B is here C&E right?",
            "And then for example, node D here has one neighbor.",
            "It has no day.",
            "So here is no day right?",
            "So now this is my neural network architecture where I on the left I put in.",
            "The properties, the features, the attributes of the nodes, and then I propagate this information through this neural network structure.",
            "So it means that each node defies defines its own computation graph, right?",
            "Basically, and then each edge of this graph has has two operations.",
            "Basically, one is the transformation operations which are labeled as Q, and then I have this aggregation operation that takes this transforming information from the neighbors.",
            "Combined aggregates it combines it with node zone information to create the next level representation of the node.",
            "So to be a bit more precise."
        ],
        [
            "Case here is how this recursive operation looks like.",
            "Basically what I'm saying is to say to compute the representation of node A at level K + 1.",
            "What do I do?",
            "I go over the neighbors of the node A. I take the representation of the neighbors from the previous level.",
            "I transform it with some matrix, send it through a non linearity.",
            "Think of this as a sigmoid function.",
            "I have some aggregator function like a summation or an average or something like that.",
            "And then I concatenate that with the transformed information about node A from the previous level.",
            "Again, pass this through the non linearity to get the.",
            "New representation for node A at the next level, right?",
            "So this was at level K. This is now K + 1, right?",
            "So I start with level zero.",
            "I transformed once to get level one.",
            "I transformed again to get level two.",
            "OK, so that's basically the the the entire idea.",
            "This is any kind of.",
            "Order invariant aggregation function, like a summation or Max pooling.",
            "And what is how do I initialize these representations of nodes at the 1st at the zero iteration?",
            "So basically here at the beginning is simply to use the feature vector information about each individual node, so this would be if this would be a social network.",
            "This could be like persons age and location and gender and education and things like that OK.",
            "So that's the idea.",
            "Any questions?",
            "All good.",
            "All good.",
            "OK, so if this is the idea then right every node will have a different computation graph because every node has a different structure of the network around them, right?",
            "So this is the structure of the network for node A.",
            "For example, here is the structure of the neural network for node D for the blue node, right?",
            "'cause this has only one neighbor A and a then has three neighbors.",
            "I think this is DC&B and so on and so forth, right?",
            "So every node will have a different.",
            "Computation, computation graph.",
            "Different neural network now.",
            "Let me give you kind of an overview how we are going to think about this class of models, and then I'll tell you what we can do with it, right?",
            "So the idea is that for a given input graph, for every node I will go and extract the network neighborhood that defines the computation graph.",
            "I will define away how do I, what kind of operations I do in these boxes, right?",
            "So basically, how do I transform and how do I aggregate information?",
            "These boxes will have a certain.",
            "Set of parameters that I will try to learn or train.",
            "I will have some task that based on this final transformed representation of the node I will.",
            "I will measure some success on that task, so I will define some kind of a loss function so that then I'll be able to do back propagation between the.",
            "You know what my prediction is and what the truth is to update the parameters of these boxes."
        ],
        [
            "And then write what I will do is.",
            "I will now train.",
            "I will try to say how do I set the operations in these boxes so that my loss my error is as small as possible right?",
            "And I will do this across these different neural networks.",
            "Of course, the way I define this right now it's impossible to train the entire network on just one data point on one label becausw.",
            "Each node has one label.",
            "So what do we do is we?"
        ],
        [
            "Assume that we assume what is called parameter sharing, right?",
            "I will assume that the structure parameters of these boxes are shared across all the nodes, right?",
            "So that even though the structure of each individual network is different, the the operations in here, the parameters of those operations are shared across all the computation graphs, right?",
            "So in our case we would have two parameters, two types of parameters AW and.",
            "I think this should be a cue from my previous discussion.",
            "Where they are indexed by level, so I will have a shared WNQ here and then another shared W&Q for the for the top level.",
            "OK, and why is this cool?",
            "This is cool because it allows me to generalize across nodes and allows me to generalize across graphs, right?",
            "So it gives."
        ],
        [
            "This kind of inductive capability where I can train, train an extract my computation graphs from 1 version of the network, figure out the parameters of this transformation boxes, and then at some point in time later and you know derives in order to compute something for that node, all I have to do is I have to create a computation graph for itself and then propagate the information to be able to predict about the nodes so I don't have to do any kind of retraining or anything else, right 'cause?",
            "Once, once I have the parameters of this particular operations fixed, I can just easily compute.",
            "This is one way how I can do this?",
            "So basically I can I can apply my Model 2 nodes that I have never trained on or, but I have never seen in the past because of this parameter sharing, even though these nodes might have network structures and neighborhoods I have never seen before and not only that, you can do this inside the same graph."
        ],
        [
            "You can also transfer across the graphs, right?",
            "You could train on one graph and then apply the same neural networks on some brand new graphs that you have never never seen before, right?",
            "So this allows you basically to have this kind of inductive capability where you could, you know, train on one interaction graph of let's say proteins and then apply this to a newly collected data of protein interactions in in a new network in a new Organism.",
            "In a new network.",
            "So this is how we can do."
        ],
        [
            "Thanks and then what is there to say?",
            "How we train this?",
            "The way we train this is kind of in this neural Network World.",
            "It's just like any other neural network, right?",
            "I have my neural network.",
            "I have this operations parameterized here with my WNQ, all old at every level.",
            "I only have two different parameters that are shared across all the nodes of that level and also across all the different neural networks, all the different computation graphs.",
            "Here are right, this W Sync user my weights my parameters.",
            "At the end I have some loss function on the final node, so for example for classification or regression I could simply have a loss function like this where this is, you know, something.",
            "I'm predicting about node A and this is the truth about no day and I want a prediction to be as close to the truth as possible if I'm if I'm having some kind of pairwise comparison, I could I could use a hinge loss.",
            "But basically the idea is that if I can measure this loss, then I can back propagate the gradient of the loss with respect to my parameters to update the parameters.",
            "Basically using stochastic gradient descent and this way learn the parameters of my neural network model.",
            "This is this kind of this approach is very or this general idea is very flexible 'cause it allows me to put it, gives me a lot of flexibility.",
            "How do I use this and how do I define?"
        ],
        [
            "Things right, So what?",
            "It allows me to do is to figure out what kind of aggregation function do I want to use?",
            "What kind of transformation function on the individual features do I want to do?",
            "I want to apply and I can apply this to in some sense.",
            "Any kind of task in a sense that I can use some kind of unsupervised maybe edge prediction loss.",
            "I can do some kind of neighborhood sampling when I define the computation graph, I don't have to take all the nodes, but I can take a subset of nodes.",
            "This becomes very important on big graphs where some node may have a million neighbors and I don't want to now aggregate over a million nodes, so there's a lot that's interesting here.",
            "It's also very interesting how do you scale this computation, 'cause basically every computation graph is different, so it's not that you are pushing a lot of information through one network, but you push one piece information separately through each individual network, and then you can also think about how do you generalize this notion of.",
            "Aggregating the neighborhood, which happens through this neural network."
        ],
        [
            "Write an as I said about aggregating the neighborhood.",
            "You can also pick different aggregation functions.",
            "Meaning when you basically take the information from your neighbors, you could simply say I'll take the mean of the information from my neighbors.",
            "I can do some kind of Max pooling where for every coordinate I'll take the maximum of the values from my neighbors.",
            "You could even use an LTM.",
            "This is basically like a sequence based model where if you do this over random ordering of the.",
            "Of nodes or a couple of different orders of the nodes, you can basically learn a permutation invariant LCM.",
            "As I said, you can use different loss functions.",
            "What is nice is that the model has a constant number of parameters that is independent of the graph size.",
            "Allows for fast and scalable inference, and these things can be kind of applied to any node in the network, even if those nodes did not exist in the network at the time of training, right?",
            "So this is kind of what I wanted to say at the technical level about this.",
            "Good yes great.",
            "I imagine in practice this vectors H are initialized randomly and then you optimize them until they converge.",
            "Not really.",
            "Not really.",
            "So the the age 0, right?",
            "The thing that goes in here.",
            "That's the feature vector of the node.",
            "This is something that you don't know from the biggest no.",
            "Let's say sorry.",
            "I'm using feature vector twice, so here I'm using like input information.",
            "Like you know if this is a node in the social network, it's like age, gender, education, location, things like that, right?",
            "And then what comes out here is some abstract feature vector.",
            "OK, but what I put in at the beginning is some information about the node that I have an if I have nothing I can put in a random vectors in here, but generally I would use.",
            "You know in my next example.",
            "Which will be about Pinterest.",
            "Each node has an image associated with it, so the input on the right hand side is are the images that then get transformed through this graph.",
            "OK, my quest.",
            "If.",
            "Nuno you already have all those parameters.",
            "Yeah, how do you infer its representation?",
            "You know what is the intuition?",
            "The way if node A is new, I generate a computation graph for it.",
            "I see whatever nodes are touched here on the right I use their input features as the input and then propagate it through this network to get the output for no day.",
            "Where, where the where the matrices are piece exactly where those Q matrices and W matrices were learned before.",
            "Hi great, thank you yes.",
            "Number of computation graphs you prepare for large networks.",
            "Great, so how can I answer your question when I show you my first application?"
        ],
        [
            "'cause we're really able to do this really large scale.",
            "So basically everything I talk to you today about is all done this year.",
            "So the previous paper was this year.",
            "This is this year.",
            "KDD and this is running in production at Pinterest and you know we just replaced graph with pill because it's kind of similar but a bit different for the use case of Pinterest right?",
            "So what do we want to do is we want to use this to make recommendations and when we do recommendations we have kind of usually two types of data.",
            "We have a lot of good content based features.",
            "We know a lot about the user.",
            "We know we know a lot about.",
            "Let's say the item, the product.",
            "We know, the categorization, we have, some text, we have the images, but we also have the network structure right?",
            "We know how these pieces, how different users interact with different pieces of content or with different items.",
            "And the question is.",
            "How can we learn about each individual item beyond just the features we know about that item, but also exploited the graph?",
            "OK, so into."
        ],
        [
            "Pinterest Our graph is a bipartite network.",
            "Where are the top?",
            "I have pins.",
            "Pins are just little bookmarks where I where every bookmarks comes with an image and a bit of text, and then what humans do.",
            "They take these bookmarks and organize them into boards.",
            "Board is just a collection of pins and this graph at Pinterest has about four billion of images, about 3 billion of boards and about 200 billion.",
            "150 billion connections here, right?",
            "And whenever you save something, you know if you like this chair, you save it into your mid century modern chairs collection and somebody else can take that same chair and you know, save it into chairs, board or something, right?",
            "So this is how Pinterest operates and one of the fundamental tasks is to learn what objects are."
        ],
        [
            "Dated right, so the idea is if I show you this pin and then you know a successful recommendation would be a similar pin.",
            "Kind of a similar, similar similar image and a bad recommendation would be a sweater.",
            "And the way you can do this is basically the task is learning the embedding the coordinates of every node I such that you know the distance between cake one and K2 is smaller than the distance between the cake one and the sweater.",
            "OK, so that's what we want to do.",
            "That's our loss function.",
            "What is interesting is that in this graph we will have around 3 four billion nodes, and after we do some cleaning, we have around 20 billion collection connections edges.",
            "And these data scatter genius because we want to use both kind of the rich text and an image information.",
            "And we also want to exploit the graph information.",
            "OK, now what is the hard part here?",
            "The hard part here is that we'd like to do recommendations, right?",
            "Given the source, I want to say what is related and when I say what is related, it's."
        ],
        [
            "Really the question is, you know what is related among the three billion other things you could choose, right?",
            "So in some sense you want to be able to learn this coordinates these distances to a superfine level of granularity, right?",
            "'cause if that right answer is not among the top 100 closest neighbors, it doesn't exist for you, so it means that this.",
            "This means that out of the three billion items you have to recognize the correct one you know at the level of 100.",
            "So we need to be able.",
            "So some sense to learn with the resolution of one versus I know 30,000,000 or whatever the math works out right.",
            "And the way you can learn with such level of granularity is to come up with this idea that is called curriculum training, where the idea is that you know.",
            "If I say this is the, this is the source that is, you know example before I had this cake.",
            "So now this is my source pin and I say this is a positive pin.",
            "You know it's a it's a card that you know.",
            "This one says 1000.",
            "Thank you.",
            "This one says you are.",
            "Very special to me.",
            "Thank you so much, right?",
            "So this is a positive example related pin.",
            "An easy negative example is a random some other random pin like a cottage so but then right if you will try to say these two have to be closer in the space than those two you need kind of an infinite number of training examples because these differences are so obvious.",
            "So in order to force the model to learn really well, we come up with this hard negative examples, right?",
            "This is visually similar, it's a card, but it's a.",
            "Happy birthday card, right?",
            "And the way we train the model is that first we train it to distinguish between positive and easy negatives.",
            "But as the time goes on.",
            "We start injecting to.",
            "The model is hard examples and kind of force it to learn these distances to a very fine granularity.",
            "The reason why we why we want to do this is that the way we use this is is that when the source pin comes, you are basically asking who are the nearest neighbors of the source pin in the embedding space, and you want this guy to be among top 100 closest neighbors because that's the only way you will be able to extract it, OK?"
        ],
        [
            "So how do we train this?",
            "We train this on a part of the graph of around 300 million nodes, 1 billion edges and over 1.2 billion training pairs, and the way the way we do this is that we generate these networks.",
            "But basically this is what material was asking.",
            "But when you do this, basically we kind of fix the template of the network.",
            "We fixed the maximum degree in this network so that all neural networks have the same memory footprint, so that when you push them through the GPU.",
            "This really flies right?",
            "So the way we do this is that we have this producer consumer type of workload where the producer is generating this neural networks very fast, and then the consumer the GPU is taking them and passing information through them and you want all these networks to have kind of the same memory layout so you can really push them out very very quickly and generally we would would take about 2000 networks.",
            "And compute the gradients over the 2000 networks per mini batch.",
            "For people who know what I talk about, OK, great.",
            "So this is how you do it now that you have figured out right those parameters here, the question is how do we now infer the coordinates of all the all the three billion nodes?"
        ],
        [
            "And what we do to do this?",
            "We use MapReduce, right?",
            "MapReduce is cool because it's really about propagating information one step in the graph, right?",
            "So basically in two iterations of MapReduce, we are able to compute the embeddings of all the nodes, right?",
            "And MapReduce is cool, 'cause it avoids you or prevent you from doing repeated computation, right?",
            "If you need to compute the level one embedding for this particular node, because this this.",
            "Particular node will appear in many many different networks, becauses in many many different computation graphs MapReduce will make this computation only once and then share the results with whoever else needs to consume that information, right?",
            "So again, MapReduce works really well for this and allows you to scale to these huge networks."
        ],
        [
            "To show you how well this works, the task is what we call related PIN recommendation where given the query pink you I want to say what is the pin X that the person is going to click next and this is in some sense a nearest neighbor task where I say how?",
            "What is the rank of the of the true related pin X based on the distance from the query Q an Now what are the things we will compare against?",
            "We'll compare against something that does only visual, so says I have this image.",
            "I have the other image.",
            "How similar are the images?",
            "Then we'll do something that is text based based on word two VEC.",
            "I'll give you an example of.",
            "Compare against a purely graph based random walk based method that is right now heavily used that Pinterest I developed this thing as well and then we'll compare about against our PIN Sage experimental setup is that we will take these 3 billion pins and bad each pin in 1000 dimensional space and then we'll do nearest neighbor generator computation in across the space to generate recommendations right so?",
            "We also have a super fast high dimensional nearest neighbor engine that allows us to do this.",
            "How well this works."
        ],
        [
            "Here here I'm showing you performance in terms of mean reciprocal rank.",
            "Higher values are better.",
            "I'm comparing against the the textual matching visual matching and then the the basically the graph based algorithm that essentially learns how to take takes as basic features, visual and annotation and then learns how to aggregate that information from across the graph to compute something about the node itself.",
            "Um?",
            "Sorry, random walk approach which you were presenting last year.",
            "Random Walk is somewhere here.",
            "I'll give you the next example, which I think will."
        ],
        [
            "So how why things work and why things fail right for this particular query image?",
            "Here are three different approaches.",
            "How do you find similar pins?",
            "If you do it based on visual similarity, then you kind of get the visual part correct, but you kind of miss miss the topic right.",
            "These are some loggers cutting down the tree and you know that's more about farmers and soldiers and so on, right?",
            "If you do it based on text, you basically lack resolution, right?",
            "This is about trees and that's about trees and logging as well.",
            "Right, similar happens when you only use the graph structure, but basically when you use the graph structure plus the older image information and pass that through the through the graph Sage architecture, you get really good recommendations, right?",
            "It's all about kind of historic.",
            "Maybe this or not, but all his kind of historic pictures of people cutting down trees.",
            "OK, so this is 1 example."
        ],
        [
            "Here's another example.",
            "This is about planting.",
            "When you do, visual basically confuses soil with ground meat, so we get kind of Korean food and stuff and then right text.",
            "Again, you don't have enough resolution.",
            "Same.",
            "Same with graph, but if you combine combine the two, you basically get something about gardening, little plants and so on, right so?",
            "This is this is how well this works and right now this particular system is running in production at Pinterest and it's making product recommendations by both on the visual information as well as the graph structure right?",
            "So the way you can really think of this is to say if I want to learn something about one object, it's not enough, only that the object nodes about itself, but it's good that it knows also who are its neighbors in the graph.",
            "And then we learn how to aggregate the information about across the graph to.",
            "Kind of energy representation of the object itself.",
            "OK, so that's number one."
        ],
        [
            "Similarity between between nodes in the graph.",
            "The second thing I want to show is about predicting drug side effects, so this will be this is joint work with marine cogic Nick.",
            "And was published that Bioinformatics Journal this year, so let me tell you again.",
            "Kind of the same technology will be applied but with a bit different twist so."
        ],
        [
            "What is the idea the idea is to be able to do to predict what is called polypharmacy side effects.",
            "If you look in US, you know half of the people between 70 and 79 take more than five drugs, and many patients take 20 drugs to treat whatever diseases they have.",
            "And the problem is right when you do drug development.",
            "When you do drug trials.",
            "You cannot now combinatorially evaluate every drug with every possible other drug up to I know size 5 or 10 to see that there are no unwanted side effects, right?",
            "So the problem becomes that even if one drug is safe and the other drug is safe in isolation, when you take both at the same time, there are, there might be some side effects that happen because of the interaction between the two graphs between the two drugs.",
            "So what we want to do is we want to be able to predict these types of things from.",
            "Paralyze drug interactions.",
            "So how are we going to do this?",
            "We're going."
        ],
        [
            "To create this type of multi Model 2 layer network that has two types of nodes, the circles are the proteins and the edges between the circles are physical protein interactions and we have around 20,000 proteins and about 700,000 edges.",
            "Then every triangle is a different drug and drug.",
            "Drugs target proteins.",
            "So we have the drug target relationships between about.",
            "19,000 edges and there is about 5000 drugs and then right?",
            "So this means how basically each drug, each molecule targets certain proteins and changes their function.",
            "So we create those edges and then the way we model side effects is remodeled side effects between a pair of drug with the relationship.",
            "A typed relationship between a pair of drug where this would say that drugs C&D recalls a side effect R2.",
            "OK, and that there are some FDA national adverse Event Reporting System and we have about 4.6 million of these adverse side effects and final graph right?",
            "We have about 1000 different side effects so we have about 1000 types of different edges.",
            "OK, So what will be my task?",
            "My task will be that given a pair of drugs predict what type of edge is there between them?",
            "Where a type of an edge is one of the 1000 different types, which is the type of the side effect that might occur.",
            "OK, so that's what we want to do."
        ],
        [
            "So how are we going to do this?",
            "We'll do this in two steps.",
            "We will first build an encoder that will take a node in the network and create an embedding, and then we'll have a decoder that will take the embedding embeddings of two nodes of two drug nodes and it will predict whether there is there is a relationship and what type of relationships is between that pair of drugs, which basically means what kind of side effect is caused.",
            "OK, so that's the that's the idea.",
            "So how will this?"
        ],
        [
            "Look like so the idea is that I want to compute the embedding for the drug, see here in the center.",
            "So my neural network now will incorporate information from many different levels, right?",
            "It will incorporate the information based on the side effects of drug C to the other drugs in the graph.",
            "So this is trying to be indicated here and we will have a separate kind of neural network for every different side effect type, right?",
            "So here is the.",
            "Structure of the neural network for side effect type R1, which is, you know, some gastrointestinal bleeding, right?",
            "And then you know there will be another different type of a neural network for their side effects are two becausw drugs.",
            "She has side effects are two with D&S right?",
            "So here's that same kind of neural network as we talked before and then the last part of the neural network will come from the protein interactions, right?",
            "So here is the structure of the neural network.",
            "From the protein interactions and now we will learn how to fuse all this information to come up with an embedding for node C in our network.",
            "OK, and now here is.",
            "Here is my embedding that we will compute and now that we have this we need to.",
            "Now we have an encoder and of course we will have a different structure of the neural network."
        ],
        [
            "For every different node in this graph, and again what we have to learn is we will assume parameter sharing and we need to learn the parameters of these different boxes, right?",
            "And these parameters will be unique for each side effect, app type or relationship type.",
            "So these are now the encoders that basically take the information from the drugs neighborhood aggregated to produce an embedding for that given drug.",
            "Now the next step is we need to build a decoder."
        ],
        [
            "A decoder takes 2 drugs, their embeddings and needs to predict the type of relationship they have, right?",
            "So what we're doing is we have 1000 different link prediction tasks right?",
            "One link prediction task for every separate side effect right?",
            "And the way the way we do this is that."
        ],
        [
            "We take these embeddings.",
            "And then basically we use a different a different set of parameters, a different kind of logistic regression model that predicts the probability for each different side effect type.",
            "OK, and now we can basically again put all this together where we can train this in a supervised way over the known side effect types.",
            "We can back propagate the gradients and figure out all the parameters of this model so that we can make correct predictions.",
            "And you can say, how well does this work?"
        ],
        [
            "We are able to beat state of the art for around 36 to 40% over what people are using before and maybe what is even more interesting is that we."
        ],
        [
            "Lost our model and said just go and make the predictions that you are most confident about.",
            "So here are ten of our most confident predictions where we say that drugs CND will interact with these given side effect and these are predictions that are not in our data set.",
            "This is not kind of hold out.",
            "These are edges that don't exist in our data set, so these are true predictions.",
            "We have no clue about them.",
            "So what we did then is to ask our doctors at the medical school to go over the latest research and see.",
            "If there is any evidence in the literature that some of these might be true."
        ],
        [
            "And we were able to find evidence for five out of those 10.",
            "And you can see that these are all kind of papers from the last one 2 two years, where these types of interactions were discovered and they are not part of our data set.",
            "So this is really kind of prediction.",
            "This is true prediction.",
            "This is not cross validation, it's really about something that doesn't didn't exist.",
            "And of course, why is this interesting?",
            "Is cause this.",
            "This examples that haven't yet been discovered.",
            "These are not necessarily mistakes, right?",
            "We can think of this as hypothesis about what else, what other types of side effects we may expect for this particular sets of drugs.",
            "Good, so this was my."
        ],
        [
            "Example number 2.",
            "And the last thing I would like to say is give you my example #3 which will be about how could you use these two reason in knowledge graphs.",
            "OK and this is again a paper that will be presented in a few weeks at NIPS in Montreal."
        ],
        [
            "So what do we mean?",
            "So what we mean is that I will.",
            "I want to go and create heterogeneous knowledge graphs, right?",
            "I can take all of my knowledge of biology and create this type of heterogeneous network where I have proteins that interact with each other using let's say, different types of interactions.",
            "I can have diseases, diseases.",
            "Things are associated with each other.",
            "I have drugs, drugs can treat each other.",
            "Drugs target different proteins.",
            "Drugs can cause side effects.",
            "Proteins are part of different biological processes.",
            "Biological processes are part of a hierarchy, right?",
            "So this is kind of a schema for my knowledge graph about how biology works and these numbers here tell me how many nodes of a different type do I have, right?",
            "So there's 17,000 proteins, 14,000 diseases, in this case, 11,000 drugs and around 10,000 side effects, and I can also take, let's say, a social community.",
            "This is Reddit and I can create a knowledge graph about that where I say.",
            "I have users users can upload, create, download or comment on a post.",
            "Posts contain words, pose belong to communities, users subscribe to communities right?",
            "And I can create a network with these different relationship types.",
            "Between half a million users.",
            "150,000 posts 250,000 words and 105 different communities.",
            "And then the question is, what if I want to start asking interesting?"
        ],
        [
            "Kind of complex queries over this over this over this graph, right?",
            "So I can say predict me all communities C in which user U is likely to upload the post and the way you can think of these types of questions you can think of them as in this case as of as path queries, right?",
            "I'm saying I want a user, you that that is the author deposed that belongs to Community C right?",
            "And the way you would do this traditionally is that you would have your graph.",
            "You would have that query template.",
            "And then you would try to match that query template to your graph and wherever it matches you would say, oh, I found the answer.",
            "Of course, the problem is that these graphs might be might be noisy.",
            "They might have missing relationship."
        ],
        [
            "So it may not be possible for you to even like do this hard subgraph matching becausw.",
            "The edges might be missing or noisy, right?",
            "So the question is, could you use any of these embedding technology to do this kind of graph?",
            "Graph subgraph matching without doing the subgraph matching?",
            "So what we tried and works quite Interestingly is the following."
        ],
        [
            "The idea is that I will take my input graph and I will learn coordinates of the nodes and then the logical operations.",
            "I will learn them a spatial operations in this space, right?",
            "So if I want to be able to answer that particular query up there with the two anchor nodes D1 and D2, then kind of get together to a P and then to see this query would be basically predict me.",
            "Drugs.",
            "That are associated with a given protein that is associated with diseases D1 and D2, and the way I can think of this is that I will start with diseases D1 and D2.",
            "I will apply a given projection operation, which is basically just multiply with the matrix in this embedding space.",
            "Then I'll apply the set intersection operation that will take this to individual points.",
            "Create one point of it, and then I'll do the last step from P2C by applying a different projection operation.",
            "Wherever I end up, I'll do the nearest neighbour.",
            "Query here and whatever nodes are in this area.",
            "Those are answers to my query.",
            "OK, so that's essentially the idea, so let me just walk you through."
        ],
        [
            "How we think of this right?",
            "So imagine I want to say, you know, predict me what users are likely to read about events shared by media outlets A&B, where A&B are two concrete media outlets, right?",
            "So I say, nodes A&B have a particular position in the graph, so now I will apply the.",
            "The projection operator for the read relation, right?",
            "Because I want to say users users is reading about this particular media types, then I will take the.",
            "The as I set projection according to the reading operation.",
            "Then I'll take the intersection operation between the two media types to get to 1 node with kind of 1 location and then in the in the.",
            "These are basically now."
        ],
        [
            "Events that are likely to be shared with both."
        ],
        [
            "Outlets A&B and then I will do the last step, which will be the user is likely to retweet events shared by these two particular.",
            "Outlets so now basically, whatever, whatever, whatever are the nearest neighbors of of that particular node in my upper space, those will be the answers to my query.",
            "OK, that's that's kind of the idea.",
            "So how do we do this?",
            "How do I do this?",
            "There are two things I want to learn.",
            "I want to learn node embeddings, coordinates of the nodes.",
            "I want to learn this projection, an intersection operators that have.",
            "Different types of parameters projection operation is just basically a matrix R intersection operator has is more.",
            "It's kind of a little neural network plus A plus a matrix.",
            "And then the way I will train this is again basically using some kind of standard negative examples where I'll do where I will say for this query this is the correct answer and this is an incorrect answer.",
            "And then I also come up with a bit more clever scheme how to generate.",
            "Negative examples so that I can learn those node embeddings well as well as this spatial operations over the query so."
        ],
        [
            "And as I said before, given an arbitrary query, basically the way we execute this is by applying correct set of spatial operations wherever wherever is the final point.",
            "Wherever is the final point.",
            "Wherever we end up, we just do the nearest neighbor and whatever is close to this node.",
            "This will be the answer to our query and I can really show that this works."
        ],
        [
            "Prizing Lee well, so I had two networks before I had the bio network that I was talking about and I had the Reddit network.",
            "Here is the performance in terms of giving the correct answer to the query for queries with different different logical structures, right?",
            "So these are just one step.",
            "These are kind of a two step type queries.",
            "You know multiple starting points like a really like.",
            "Long chain of in directions and so on.",
            "And we can do this with AUC.",
            "You know, for the biological network above, .9 and four edit between, let's say .8 and 9 right?",
            "And this is quite interesting 'cause it really basically sidesteps this graph.",
            "Matching in this combinatorial explosion and really allows me to do these things.",
            "Basically to translate these problems into doing proper spatial operations.",
            "In the embedding space plus nearest neighbor and what we're trying to do now is understand.",
            "Why does this work and how do we need to train it and so on.",
            "But I think it's a very kind of interesting and exciting direction.",
            "So."
        ],
        [
            "So I will quickly conclude.",
            "So what I was talking about is this idea.",
            "How do we learn embeddings features of nodes of nodes in the network?",
            "An hour, 3 talking about?"
        ],
        [
            "Graph convolutional neural networks that basically take take this idea of convolutions from the computer vision literature, but generalize them to arbitrary graphs, and basically they allow us to fuse note feature information in the graph information to basically obtain state of the art accuracy on any kind of node classification and link prediction.",
            "What is nice is that the model size the parameters are independent of the data size, so these things can be really scaled.",
            "Two large graphs.",
            "It's running in production at Pinterest.",
            "And leads to significant performance gains and what is nice is that you can use this for tasks so different as making pin recommendations, predicting drug side effects, or answering complex queries over graphs, right so?"
        ],
        [
            "You know what have you learned in the last 1 two years?",
            "Is that this kind of representation learning paradigm, specially in a supervised way can actually be extended to graphs that kind of know row structural feature engineering is needed and that it allows us to effectively combine attributes data with the network information in this way and reach what we know about each object by also fusing in the information from the network structure.",
            "And it's also interesting, right?",
            "That basically we can use this to do this type of end to end training instead of this separate cut kind of multi stage type training especially.",
            "I think when I showed you the drug side effect prediction right, that was quite complex.",
            "You had like at the end you were making thousand predictions.",
            "You had two networks, one for every node and these things can all nicely be done and really trained in an end to end way.",
            "Water."
        ],
        [
            "Some next steps.",
            "Next steps are thinking about how does this generalize to dynamic evolving graphs?",
            "How can we build kind of domain specific versions of this architecture for various kinds of applications?",
            "I gave you an example of recommender systems, but there is much more that can be done.",
            "This type of things can be used for graph generation as well, so one of the research projects we have is to generate graphs of molecules where basically the idea is that you say.",
            "I want the molecule with these types of properties and we'd be able to generate your formula, generate your graph of that molecule, and then right as I showed your example, it seems that this idea of going beyond just predicting single edges, but doing this kind of multi hop prediction or answering this logical queries is a very interesting research direction, and the last the last thing is like any kind of understanding on the representational capacity of these models, why they work when they work, and how.",
            "Is also very interesting, so this is all I wanted to say.",
            "And I'd be very happy to take questions."
        ],
        [
            "I say thank you to the basically PhD students and postdocs who did all the work and funding members and collaborators.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you for having me.",
                    "label": 0
                },
                {
                    "sent": "It's exciting to be home and to tell you what I've been up to in the last year.",
                    "label": 0
                },
                {
                    "sent": "So this is work about taking kind of the deep learning technology and see how it applies to graphs and applications of it.",
                    "label": 1
                },
                {
                    "sent": "Will will come from what I'm doing at Pinterest.",
                    "label": 0
                },
                {
                    "sent": "Applications will come through the collaborations I have, the chance.",
                    "label": 0
                },
                {
                    "sent": "Acrobatic bio hub so kind of biologists and so on.",
                    "label": 0
                },
                {
                    "sent": "So this won't be about social networks, right?",
                    "label": 0
                },
                {
                    "sent": "My research is about networks.",
                    "label": 0
                },
                {
                    "sent": "Why I like networks is because you can take.",
                    "label": 0
                },
                {
                    "sent": "Many different types of data, many different types of complex systems, and you can.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And take them, create this diagrams of interactions and you can represent them with the same underlying mathematical representation with a graph with a set of nodes and a set of relationships between those nodes.",
                    "label": 0
                },
                {
                    "sent": "An if we think about, you know if we want to compute or do any kind of machine learning on these types of data, then the quest.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use what kind of tasks can we do on top of networks, right?",
                    "label": 0
                },
                {
                    "sent": "And here are some examples, right?",
                    "label": 0
                },
                {
                    "sent": "I can do what is called node classification, which is I want to be able to predict a type or a color of a given node.",
                    "label": 1
                },
                {
                    "sent": "I can do link prediction which is about predicting whether a pair of nodes is linked or not, and then I can also analyze networks at the level of groups of nodes, and I can think about detecting clusters or communities of nodes, and I can also think about working with entire graphs.",
                    "label": 0
                },
                {
                    "sent": "Thinking about how similar are different networks and how do I learn similarities between them and kind of the technology will be talking about today, will allow us to solve any any of these kind of abstract problems and I'll give you examples later, right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to be kind of more concrete, you could.",
                    "label": 0
                },
                {
                    "sent": "You could imagine you are given a graph and you know some nodes have colors and the Gray nodes have no colors and the question is how would you infer the color of the Gray node, right?",
                    "label": 0
                },
                {
                    "sent": "Is it is it orange or is it Violet?",
                    "label": 0
                },
                {
                    "sent": "And kind of the one of the one of the stumbling blocks in applying machine learning to these types of data is that you have to somehow feature eyes.",
                    "label": 0
                },
                {
                    "sent": "The graph come up with some kind of features, attributes of the nodes in the network, so then you can build some predictors over that right, and you know, here are some ideas how you may want to go and create those features, but it's very time consuming and it's very unattractive and boring thing to do.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we go around that?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in some sense, if you think about machine learning lifecycle, let's say applied to graphs but applied to any other kind of data as well is that you start with the raw data, their own network you want to now create some kind of feature representation or feature you want to do kind of feature engineering to come up with the features so that then you can apply your favorite machine learning algorithm to be able to produce the model and make some predictions right?",
                    "label": 1
                },
                {
                    "sent": "And usually the most consuming time is happens here where you do the feature engineering where you want to say oh how do I describe?",
                    "label": 0
                },
                {
                    "sent": "The position and the network structure of the network around the node.",
                    "label": 0
                },
                {
                    "sent": "What kind of intuitions do I want to build so that I create the feature vector so that I can go?",
                    "label": 1
                },
                {
                    "sent": "And apply my method right and we do this every single time for every single application.",
                    "label": 0
                },
                {
                    "sent": "So the question then becomes, could I somehow sidestep this feature learning part and just automatically learn over networks and forget about feature engineering, right?",
                    "label": 0
                },
                {
                    "sent": "So the idea of what?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll explore today is how can you take a network and build this mapping that takes a node in the network and Maps it to some vector of real numbers.",
                    "label": 0
                },
                {
                    "sent": "We will call this vector on embedding feature representation of that particular node, and our hope will be that this particular vector in D dimensions will somehow capture the network structure around this particular node.",
                    "label": 0
                },
                {
                    "sent": "And just to give you an example what I'm talking.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can take the most famous network of them all.",
                    "label": 0
                },
                {
                    "sent": "This is the Zachary Karate club network.",
                    "label": 1
                },
                {
                    "sent": "These are people who went to the.",
                    "label": 0
                },
                {
                    "sent": "To train karate and then there was a.",
                    "label": 0
                },
                {
                    "sent": "There was a quarrel and this network split into two groups.",
                    "label": 0
                },
                {
                    "sent": "But basically what I'm trying to say here is these are the nodes and here is for example some kind of embedding of these nodes in 222 dimensions and you can see how knows that have similar positions in the network are kind of embedded into similar part of the space an if I'm able to do these types of things but not only in two dimensions but in more dimensions.",
                    "label": 1
                },
                {
                    "sent": "And if I can learn this embeddings in some kind of supervised way, then it allows me to reasonover graphs in a very interesting way, right?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in some sense, the goal is to map every node to some low dimensional space and then basically similarity between nodes in this space will somehow encode, you know link strength, strength, network similarity between the nodes and the idea is that this will both kind of incorporate the network information as well as the node information in creating this low dimensional embedding, and after that I can use that for.",
                    "label": 1
                },
                {
                    "sent": "Anomaly detection, you know, node classification, clustering link prediction, things like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of abstractly what we want.",
                    "label": 0
                },
                {
                    "sent": "Another way you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And think about this is that I want to learn functions that you know will take in as an entire network, or will take in a node of the network and will produce some kind of embedding of those nodes, right?",
                    "label": 0
                },
                {
                    "sent": "So I want to build figure out how to take nodes and map them to some small dimensional space so that nodes that are similar in the network are embedded close together.",
                    "label": 1
                },
                {
                    "sent": "In practice this dimensionality D is, let's say around 1000.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I would like to do 'cause you know?",
                    "label": 0
                },
                {
                    "sent": "We like.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neural networks because they're kind of very powerful computation type graphs is I want to take my neural network and oh sorry, my real graph, and I'd like to pass it through the neural network so so that at the end I can make certain predictions about the nodes of the graph or about the entire graph and the question really is, how could I design A neural network architecture that on one end will take the graph and you know on the other end will compute something useful for me.",
                    "label": 0
                },
                {
                    "sent": "So let me now tell you why is doing that hard.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first thing why this is hard is that the modern deep learning toolbox is designed for simple data types, right?",
                    "label": 1
                },
                {
                    "sent": "So if you think about images, images are just simple grid graphs of fixed size, right?",
                    "label": 0
                },
                {
                    "sent": "So when you when you have convolutional neural networks and you operate and you apply your convolutional operator, you're basically sliding over this fixed size grid graph, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about language speech, it's even simpler.",
                    "label": 0
                },
                {
                    "sent": "It's a sequence, right?",
                    "label": 0
                },
                {
                    "sent": "So it's like a.",
                    "label": 0
                },
                {
                    "sent": "Chain graph, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about the networks we want to work with, they have.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much more complex structures, right?",
                    "label": 1
                },
                {
                    "sent": "So they have arbitrary size.",
                    "label": 1
                },
                {
                    "sent": "They have kind of complex topological properties, no spatial locality.",
                    "label": 1
                },
                {
                    "sent": "Like grids and chains.",
                    "label": 1
                },
                {
                    "sent": "Even more complicated is that the nodes of the network have an arbitrary ordering, so it's basically the same thing as saying you can flip the order of rows and columns in the image, and it should still be the same image.",
                    "label": 0
                },
                {
                    "sent": "Many times.",
                    "label": 0
                },
                {
                    "sent": "These graphs are dynamic, have multi model features.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, the question then becomes how do I take, you know, the success of convolutional neural networks when applied to images and sequences text and generalize these beyond simple lattices.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of the topic.",
                    "label": 0
                },
                {
                    "sent": "The topic for this talk.",
                    "label": 0
                },
                {
                    "sent": "And as we will go and unfold this basically my talk will have four parts.",
                    "label": 0
                },
                {
                    "sent": "I will first tell you how to do this, and then I'll give you 3 applications of this idea to three different domains.",
                    "label": 0
                },
                {
                    "sent": "One will be large scale.",
                    "label": 0
                },
                {
                    "sent": "Commander Systems, another one will be about drug side effect prediction and one will be about reasoning over knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "All these three things will be able to do with this approach.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we'll talk about this as the method we developed in our group called Graph Sage about a year ago, and it's about kind of supervised feature learning networks and at least what we do.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do is kind of.",
                    "label": 0
                },
                {
                    "sent": "We'd like to generalize convolutional operators over images to general graphs.",
                    "label": 0
                },
                {
                    "sent": "Now, how do?",
                    "label": 0
                },
                {
                    "sent": "How do convolutions work?",
                    "label": 0
                },
                {
                    "sent": "The idea is that you have that you have a an image, and then you have this convolutional operator that basically slides over the over the image, and then you know you have multiple multiple layers.",
                    "label": 0
                },
                {
                    "sent": "As you take these operators, you create new feature Maps, do subsampling more convolutions, and kind of spread this over multiple layers.",
                    "label": 0
                },
                {
                    "sent": "To compute some output of based on that input image.",
                    "label": 0
                },
                {
                    "sent": "Now what does?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does a single layer convolutional operator look like?",
                    "label": 1
                },
                {
                    "sent": "It basically takes a little Patch of the image and computes some function of it.",
                    "label": 0
                },
                {
                    "sent": "So another way how you could think of it from the graph point of view is that basically we have this subset of three by three pixels and somehow you are aggregating their their information, their values into the center node.",
                    "label": 0
                },
                {
                    "sent": "Somehow you weigh them and aggregate them together, right?",
                    "label": 0
                },
                {
                    "sent": "And you do this for every kind of possible window as you are sliding it.",
                    "label": 0
                },
                {
                    "sent": "Over your image.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question becomes, what do you do when your graphs are not grids and don't have this nice spatial locality where you kind of where the notion of a sliding window kind of doesn't exist, right?",
                    "label": 0
                },
                {
                    "sent": "As in social networks, information, networks and so on.",
                    "label": 1
                },
                {
                    "sent": "So I'll give you now a few ideas how people try to do this, but I would say was not to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Successful so one way you could do this is to say I will take my graph.",
                    "label": 0
                },
                {
                    "sent": "I will create the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "Plus I have some features, some properties of the nodes I will create this big matrix and I will take every row of this matrix and pass it through a big neural network.",
                    "label": 0
                },
                {
                    "sent": "So that here at the end I can predict something about each individual node and this might sound like a good idea, but it's kind of not right.",
                    "label": 0
                },
                {
                    "sent": "The reason it's not is 'cause the number of parameters you have in this network.",
                    "label": 0
                },
                {
                    "sent": "Is is kind of order of the number of North of the number of nodes in your network.",
                    "label": 0
                },
                {
                    "sent": "So basically the number of parameters scales linearly with the with the size of your data.",
                    "label": 0
                },
                {
                    "sent": "Not sure how could you apply this with the graphs of different sizes and then this is not invariant to node reordering right?",
                    "label": 0
                },
                {
                    "sent": "If I renumber my node in a different order then the rules of this matrix will shift so that so the inputs to this network will get confused and I don't know how to generalize across networks, so this was kind of 1 approach.",
                    "label": 0
                },
                {
                    "sent": "Another approach called graph combo.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Social networks try to come up with this idea of a sliding window where basically the idea is that we'll pick a node will somehow try to create a sub graph of it, and then for every node will have kind of a different sub graph.",
                    "label": 0
                },
                {
                    "sent": "And now we have to do some kind of graph normalization so that we can take this graph, spread it into a fixed size vector which we can then use as an input into our neural network, right?",
                    "label": 0
                },
                {
                    "sent": "And of course what is the problem?",
                    "label": 0
                },
                {
                    "sent": "The problem is this this little subgraphs.",
                    "label": 0
                },
                {
                    "sent": "Around different nodes have different structures and the question is how do you come up with some kind of Canonical node ordering that is consistent between different graphs?",
                    "label": 0
                },
                {
                    "sent": "So these were all kind of previous approaches.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are interested in basically exploiting this idea that the nodes neighborhood in the in the social network defines the computation graph, right?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that basically if we want to compute maybe the label of my node I, I will look at the network neighborhood of that node and that will define the structure of the computation graph.",
                    "label": 0
                },
                {
                    "sent": "So it will define the structure of the social network and then what I will learn is how to take information at the nodes around my target node.",
                    "label": 0
                },
                {
                    "sent": "I and I will learn how to kind of propagate and transform.",
                    "label": 1
                },
                {
                    "sent": "Information from the neighbors to compute something about node I that's kind of the high level idea, right?",
                    "label": 0
                },
                {
                    "sent": "I want to learn how to propagate information over the graph in order to compute something about the node.",
                    "label": 1
                },
                {
                    "sent": "So this is now the high level idea.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a concrete example.",
                    "label": 1
                },
                {
                    "sent": "If you think here of my little graph on the left as the input graph.",
                    "label": 0
                },
                {
                    "sent": "Here I give you the neural network architecture to compute something about the no day.",
                    "label": 0
                },
                {
                    "sent": "Why is this network the way it is is because node A has three neighbors, his neighbors BC and D. So here they are.",
                    "label": 0
                },
                {
                    "sent": "Neighbors B is here C&E right?",
                    "label": 0
                },
                {
                    "sent": "And then for example, node D here has one neighbor.",
                    "label": 0
                },
                {
                    "sent": "It has no day.",
                    "label": 0
                },
                {
                    "sent": "So here is no day right?",
                    "label": 0
                },
                {
                    "sent": "So now this is my neural network architecture where I on the left I put in.",
                    "label": 0
                },
                {
                    "sent": "The properties, the features, the attributes of the nodes, and then I propagate this information through this neural network structure.",
                    "label": 0
                },
                {
                    "sent": "So it means that each node defies defines its own computation graph, right?",
                    "label": 1
                },
                {
                    "sent": "Basically, and then each edge of this graph has has two operations.",
                    "label": 1
                },
                {
                    "sent": "Basically, one is the transformation operations which are labeled as Q, and then I have this aggregation operation that takes this transforming information from the neighbors.",
                    "label": 0
                },
                {
                    "sent": "Combined aggregates it combines it with node zone information to create the next level representation of the node.",
                    "label": 0
                },
                {
                    "sent": "So to be a bit more precise.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case here is how this recursive operation looks like.",
                    "label": 0
                },
                {
                    "sent": "Basically what I'm saying is to say to compute the representation of node A at level K + 1.",
                    "label": 1
                },
                {
                    "sent": "What do I do?",
                    "label": 0
                },
                {
                    "sent": "I go over the neighbors of the node A. I take the representation of the neighbors from the previous level.",
                    "label": 0
                },
                {
                    "sent": "I transform it with some matrix, send it through a non linearity.",
                    "label": 0
                },
                {
                    "sent": "Think of this as a sigmoid function.",
                    "label": 1
                },
                {
                    "sent": "I have some aggregator function like a summation or an average or something like that.",
                    "label": 0
                },
                {
                    "sent": "And then I concatenate that with the transformed information about node A from the previous level.",
                    "label": 0
                },
                {
                    "sent": "Again, pass this through the non linearity to get the.",
                    "label": 0
                },
                {
                    "sent": "New representation for node A at the next level, right?",
                    "label": 0
                },
                {
                    "sent": "So this was at level K. This is now K + 1, right?",
                    "label": 0
                },
                {
                    "sent": "So I start with level zero.",
                    "label": 0
                },
                {
                    "sent": "I transformed once to get level one.",
                    "label": 0
                },
                {
                    "sent": "I transformed again to get level two.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's basically the the the entire idea.",
                    "label": 0
                },
                {
                    "sent": "This is any kind of.",
                    "label": 1
                },
                {
                    "sent": "Order invariant aggregation function, like a summation or Max pooling.",
                    "label": 0
                },
                {
                    "sent": "And what is how do I initialize these representations of nodes at the 1st at the zero iteration?",
                    "label": 0
                },
                {
                    "sent": "So basically here at the beginning is simply to use the feature vector information about each individual node, so this would be if this would be a social network.",
                    "label": 0
                },
                {
                    "sent": "This could be like persons age and location and gender and education and things like that OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "All good.",
                    "label": 0
                },
                {
                    "sent": "All good.",
                    "label": 0
                },
                {
                    "sent": "OK, so if this is the idea then right every node will have a different computation graph because every node has a different structure of the network around them, right?",
                    "label": 1
                },
                {
                    "sent": "So this is the structure of the network for node A.",
                    "label": 0
                },
                {
                    "sent": "For example, here is the structure of the neural network for node D for the blue node, right?",
                    "label": 0
                },
                {
                    "sent": "'cause this has only one neighbor A and a then has three neighbors.",
                    "label": 0
                },
                {
                    "sent": "I think this is DC&B and so on and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So every node will have a different.",
                    "label": 0
                },
                {
                    "sent": "Computation, computation graph.",
                    "label": 0
                },
                {
                    "sent": "Different neural network now.",
                    "label": 0
                },
                {
                    "sent": "Let me give you kind of an overview how we are going to think about this class of models, and then I'll tell you what we can do with it, right?",
                    "label": 1
                },
                {
                    "sent": "So the idea is that for a given input graph, for every node I will go and extract the network neighborhood that defines the computation graph.",
                    "label": 0
                },
                {
                    "sent": "I will define away how do I, what kind of operations I do in these boxes, right?",
                    "label": 0
                },
                {
                    "sent": "So basically, how do I transform and how do I aggregate information?",
                    "label": 0
                },
                {
                    "sent": "These boxes will have a certain.",
                    "label": 0
                },
                {
                    "sent": "Set of parameters that I will try to learn or train.",
                    "label": 0
                },
                {
                    "sent": "I will have some task that based on this final transformed representation of the node I will.",
                    "label": 0
                },
                {
                    "sent": "I will measure some success on that task, so I will define some kind of a loss function so that then I'll be able to do back propagation between the.",
                    "label": 0
                },
                {
                    "sent": "You know what my prediction is and what the truth is to update the parameters of these boxes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then write what I will do is.",
                    "label": 0
                },
                {
                    "sent": "I will now train.",
                    "label": 0
                },
                {
                    "sent": "I will try to say how do I set the operations in these boxes so that my loss my error is as small as possible right?",
                    "label": 0
                },
                {
                    "sent": "And I will do this across these different neural networks.",
                    "label": 0
                },
                {
                    "sent": "Of course, the way I define this right now it's impossible to train the entire network on just one data point on one label becausw.",
                    "label": 0
                },
                {
                    "sent": "Each node has one label.",
                    "label": 0
                },
                {
                    "sent": "So what do we do is we?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assume that we assume what is called parameter sharing, right?",
                    "label": 0
                },
                {
                    "sent": "I will assume that the structure parameters of these boxes are shared across all the nodes, right?",
                    "label": 0
                },
                {
                    "sent": "So that even though the structure of each individual network is different, the the operations in here, the parameters of those operations are shared across all the computation graphs, right?",
                    "label": 0
                },
                {
                    "sent": "So in our case we would have two parameters, two types of parameters AW and.",
                    "label": 0
                },
                {
                    "sent": "I think this should be a cue from my previous discussion.",
                    "label": 0
                },
                {
                    "sent": "Where they are indexed by level, so I will have a shared WNQ here and then another shared W&Q for the for the top level.",
                    "label": 0
                },
                {
                    "sent": "OK, and why is this cool?",
                    "label": 0
                },
                {
                    "sent": "This is cool because it allows me to generalize across nodes and allows me to generalize across graphs, right?",
                    "label": 0
                },
                {
                    "sent": "So it gives.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of inductive capability where I can train, train an extract my computation graphs from 1 version of the network, figure out the parameters of this transformation boxes, and then at some point in time later and you know derives in order to compute something for that node, all I have to do is I have to create a computation graph for itself and then propagate the information to be able to predict about the nodes so I don't have to do any kind of retraining or anything else, right 'cause?",
                    "label": 0
                },
                {
                    "sent": "Once, once I have the parameters of this particular operations fixed, I can just easily compute.",
                    "label": 0
                },
                {
                    "sent": "This is one way how I can do this?",
                    "label": 0
                },
                {
                    "sent": "So basically I can I can apply my Model 2 nodes that I have never trained on or, but I have never seen in the past because of this parameter sharing, even though these nodes might have network structures and neighborhoods I have never seen before and not only that, you can do this inside the same graph.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also transfer across the graphs, right?",
                    "label": 0
                },
                {
                    "sent": "You could train on one graph and then apply the same neural networks on some brand new graphs that you have never never seen before, right?",
                    "label": 0
                },
                {
                    "sent": "So this allows you basically to have this kind of inductive capability where you could, you know, train on one interaction graph of let's say proteins and then apply this to a newly collected data of protein interactions in in a new network in a new Organism.",
                    "label": 1
                },
                {
                    "sent": "In a new network.",
                    "label": 0
                },
                {
                    "sent": "So this is how we can do.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks and then what is there to say?",
                    "label": 0
                },
                {
                    "sent": "How we train this?",
                    "label": 0
                },
                {
                    "sent": "The way we train this is kind of in this neural Network World.",
                    "label": 0
                },
                {
                    "sent": "It's just like any other neural network, right?",
                    "label": 0
                },
                {
                    "sent": "I have my neural network.",
                    "label": 0
                },
                {
                    "sent": "I have this operations parameterized here with my WNQ, all old at every level.",
                    "label": 0
                },
                {
                    "sent": "I only have two different parameters that are shared across all the nodes of that level and also across all the different neural networks, all the different computation graphs.",
                    "label": 0
                },
                {
                    "sent": "Here are right, this W Sync user my weights my parameters.",
                    "label": 0
                },
                {
                    "sent": "At the end I have some loss function on the final node, so for example for classification or regression I could simply have a loss function like this where this is, you know, something.",
                    "label": 0
                },
                {
                    "sent": "I'm predicting about node A and this is the truth about no day and I want a prediction to be as close to the truth as possible if I'm if I'm having some kind of pairwise comparison, I could I could use a hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But basically the idea is that if I can measure this loss, then I can back propagate the gradient of the loss with respect to my parameters to update the parameters.",
                    "label": 1
                },
                {
                    "sent": "Basically using stochastic gradient descent and this way learn the parameters of my neural network model.",
                    "label": 0
                },
                {
                    "sent": "This is this kind of this approach is very or this general idea is very flexible 'cause it allows me to put it, gives me a lot of flexibility.",
                    "label": 0
                },
                {
                    "sent": "How do I use this and how do I define?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things right, So what?",
                    "label": 0
                },
                {
                    "sent": "It allows me to do is to figure out what kind of aggregation function do I want to use?",
                    "label": 0
                },
                {
                    "sent": "What kind of transformation function on the individual features do I want to do?",
                    "label": 0
                },
                {
                    "sent": "I want to apply and I can apply this to in some sense.",
                    "label": 0
                },
                {
                    "sent": "Any kind of task in a sense that I can use some kind of unsupervised maybe edge prediction loss.",
                    "label": 1
                },
                {
                    "sent": "I can do some kind of neighborhood sampling when I define the computation graph, I don't have to take all the nodes, but I can take a subset of nodes.",
                    "label": 0
                },
                {
                    "sent": "This becomes very important on big graphs where some node may have a million neighbors and I don't want to now aggregate over a million nodes, so there's a lot that's interesting here.",
                    "label": 0
                },
                {
                    "sent": "It's also very interesting how do you scale this computation, 'cause basically every computation graph is different, so it's not that you are pushing a lot of information through one network, but you push one piece information separately through each individual network, and then you can also think about how do you generalize this notion of.",
                    "label": 1
                },
                {
                    "sent": "Aggregating the neighborhood, which happens through this neural network.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Write an as I said about aggregating the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "You can also pick different aggregation functions.",
                    "label": 0
                },
                {
                    "sent": "Meaning when you basically take the information from your neighbors, you could simply say I'll take the mean of the information from my neighbors.",
                    "label": 0
                },
                {
                    "sent": "I can do some kind of Max pooling where for every coordinate I'll take the maximum of the values from my neighbors.",
                    "label": 0
                },
                {
                    "sent": "You could even use an LTM.",
                    "label": 0
                },
                {
                    "sent": "This is basically like a sequence based model where if you do this over random ordering of the.",
                    "label": 0
                },
                {
                    "sent": "Of nodes or a couple of different orders of the nodes, you can basically learn a permutation invariant LCM.",
                    "label": 0
                },
                {
                    "sent": "As I said, you can use different loss functions.",
                    "label": 1
                },
                {
                    "sent": "What is nice is that the model has a constant number of parameters that is independent of the graph size.",
                    "label": 1
                },
                {
                    "sent": "Allows for fast and scalable inference, and these things can be kind of applied to any node in the network, even if those nodes did not exist in the network at the time of training, right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of what I wanted to say at the technical level about this.",
                    "label": 0
                },
                {
                    "sent": "Good yes great.",
                    "label": 0
                },
                {
                    "sent": "I imagine in practice this vectors H are initialized randomly and then you optimize them until they converge.",
                    "label": 0
                },
                {
                    "sent": "Not really.",
                    "label": 0
                },
                {
                    "sent": "Not really.",
                    "label": 0
                },
                {
                    "sent": "So the the age 0, right?",
                    "label": 0
                },
                {
                    "sent": "The thing that goes in here.",
                    "label": 0
                },
                {
                    "sent": "That's the feature vector of the node.",
                    "label": 0
                },
                {
                    "sent": "This is something that you don't know from the biggest no.",
                    "label": 0
                },
                {
                    "sent": "Let's say sorry.",
                    "label": 0
                },
                {
                    "sent": "I'm using feature vector twice, so here I'm using like input information.",
                    "label": 0
                },
                {
                    "sent": "Like you know if this is a node in the social network, it's like age, gender, education, location, things like that, right?",
                    "label": 0
                },
                {
                    "sent": "And then what comes out here is some abstract feature vector.",
                    "label": 0
                },
                {
                    "sent": "OK, but what I put in at the beginning is some information about the node that I have an if I have nothing I can put in a random vectors in here, but generally I would use.",
                    "label": 0
                },
                {
                    "sent": "You know in my next example.",
                    "label": 0
                },
                {
                    "sent": "Which will be about Pinterest.",
                    "label": 0
                },
                {
                    "sent": "Each node has an image associated with it, so the input on the right hand side is are the images that then get transformed through this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, my quest.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Nuno you already have all those parameters.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how do you infer its representation?",
                    "label": 0
                },
                {
                    "sent": "You know what is the intuition?",
                    "label": 0
                },
                {
                    "sent": "The way if node A is new, I generate a computation graph for it.",
                    "label": 0
                },
                {
                    "sent": "I see whatever nodes are touched here on the right I use their input features as the input and then propagate it through this network to get the output for no day.",
                    "label": 0
                },
                {
                    "sent": "Where, where the where the matrices are piece exactly where those Q matrices and W matrices were learned before.",
                    "label": 0
                },
                {
                    "sent": "Hi great, thank you yes.",
                    "label": 0
                },
                {
                    "sent": "Number of computation graphs you prepare for large networks.",
                    "label": 0
                },
                {
                    "sent": "Great, so how can I answer your question when I show you my first application?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause we're really able to do this really large scale.",
                    "label": 0
                },
                {
                    "sent": "So basically everything I talk to you today about is all done this year.",
                    "label": 0
                },
                {
                    "sent": "So the previous paper was this year.",
                    "label": 0
                },
                {
                    "sent": "This is this year.",
                    "label": 0
                },
                {
                    "sent": "KDD and this is running in production at Pinterest and you know we just replaced graph with pill because it's kind of similar but a bit different for the use case of Pinterest right?",
                    "label": 0
                },
                {
                    "sent": "So what do we want to do is we want to use this to make recommendations and when we do recommendations we have kind of usually two types of data.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of good content based features.",
                    "label": 0
                },
                {
                    "sent": "We know a lot about the user.",
                    "label": 0
                },
                {
                    "sent": "We know we know a lot about.",
                    "label": 0
                },
                {
                    "sent": "Let's say the item, the product.",
                    "label": 0
                },
                {
                    "sent": "We know, the categorization, we have, some text, we have the images, but we also have the network structure right?",
                    "label": 0
                },
                {
                    "sent": "We know how these pieces, how different users interact with different pieces of content or with different items.",
                    "label": 0
                },
                {
                    "sent": "And the question is.",
                    "label": 0
                },
                {
                    "sent": "How can we learn about each individual item beyond just the features we know about that item, but also exploited the graph?",
                    "label": 0
                },
                {
                    "sent": "OK, so into.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pinterest Our graph is a bipartite network.",
                    "label": 0
                },
                {
                    "sent": "Where are the top?",
                    "label": 0
                },
                {
                    "sent": "I have pins.",
                    "label": 0
                },
                {
                    "sent": "Pins are just little bookmarks where I where every bookmarks comes with an image and a bit of text, and then what humans do.",
                    "label": 1
                },
                {
                    "sent": "They take these bookmarks and organize them into boards.",
                    "label": 0
                },
                {
                    "sent": "Board is just a collection of pins and this graph at Pinterest has about four billion of images, about 3 billion of boards and about 200 billion.",
                    "label": 1
                },
                {
                    "sent": "150 billion connections here, right?",
                    "label": 0
                },
                {
                    "sent": "And whenever you save something, you know if you like this chair, you save it into your mid century modern chairs collection and somebody else can take that same chair and you know, save it into chairs, board or something, right?",
                    "label": 0
                },
                {
                    "sent": "So this is how Pinterest operates and one of the fundamental tasks is to learn what objects are.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dated right, so the idea is if I show you this pin and then you know a successful recommendation would be a similar pin.",
                    "label": 0
                },
                {
                    "sent": "Kind of a similar, similar similar image and a bad recommendation would be a sweater.",
                    "label": 1
                },
                {
                    "sent": "And the way you can do this is basically the task is learning the embedding the coordinates of every node I such that you know the distance between cake one and K2 is smaller than the distance between the cake one and the sweater.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's what we want to do.",
                    "label": 0
                },
                {
                    "sent": "That's our loss function.",
                    "label": 0
                },
                {
                    "sent": "What is interesting is that in this graph we will have around 3 four billion nodes, and after we do some cleaning, we have around 20 billion collection connections edges.",
                    "label": 1
                },
                {
                    "sent": "And these data scatter genius because we want to use both kind of the rich text and an image information.",
                    "label": 0
                },
                {
                    "sent": "And we also want to exploit the graph information.",
                    "label": 0
                },
                {
                    "sent": "OK, now what is the hard part here?",
                    "label": 0
                },
                {
                    "sent": "The hard part here is that we'd like to do recommendations, right?",
                    "label": 0
                },
                {
                    "sent": "Given the source, I want to say what is related and when I say what is related, it's.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really the question is, you know what is related among the three billion other things you could choose, right?",
                    "label": 0
                },
                {
                    "sent": "So in some sense you want to be able to learn this coordinates these distances to a superfine level of granularity, right?",
                    "label": 0
                },
                {
                    "sent": "'cause if that right answer is not among the top 100 closest neighbors, it doesn't exist for you, so it means that this.",
                    "label": 0
                },
                {
                    "sent": "This means that out of the three billion items you have to recognize the correct one you know at the level of 100.",
                    "label": 0
                },
                {
                    "sent": "So we need to be able.",
                    "label": 1
                },
                {
                    "sent": "So some sense to learn with the resolution of one versus I know 30,000,000 or whatever the math works out right.",
                    "label": 1
                },
                {
                    "sent": "And the way you can learn with such level of granularity is to come up with this idea that is called curriculum training, where the idea is that you know.",
                    "label": 1
                },
                {
                    "sent": "If I say this is the, this is the source that is, you know example before I had this cake.",
                    "label": 0
                },
                {
                    "sent": "So now this is my source pin and I say this is a positive pin.",
                    "label": 0
                },
                {
                    "sent": "You know it's a it's a card that you know.",
                    "label": 0
                },
                {
                    "sent": "This one says 1000.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "This one says you are.",
                    "label": 0
                },
                {
                    "sent": "Very special to me.",
                    "label": 0
                },
                {
                    "sent": "Thank you so much, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a positive example related pin.",
                    "label": 0
                },
                {
                    "sent": "An easy negative example is a random some other random pin like a cottage so but then right if you will try to say these two have to be closer in the space than those two you need kind of an infinite number of training examples because these differences are so obvious.",
                    "label": 0
                },
                {
                    "sent": "So in order to force the model to learn really well, we come up with this hard negative examples, right?",
                    "label": 0
                },
                {
                    "sent": "This is visually similar, it's a card, but it's a.",
                    "label": 0
                },
                {
                    "sent": "Happy birthday card, right?",
                    "label": 0
                },
                {
                    "sent": "And the way we train the model is that first we train it to distinguish between positive and easy negatives.",
                    "label": 0
                },
                {
                    "sent": "But as the time goes on.",
                    "label": 0
                },
                {
                    "sent": "We start injecting to.",
                    "label": 0
                },
                {
                    "sent": "The model is hard examples and kind of force it to learn these distances to a very fine granularity.",
                    "label": 0
                },
                {
                    "sent": "The reason why we why we want to do this is that the way we use this is is that when the source pin comes, you are basically asking who are the nearest neighbors of the source pin in the embedding space, and you want this guy to be among top 100 closest neighbors because that's the only way you will be able to extract it, OK?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we train this?",
                    "label": 0
                },
                {
                    "sent": "We train this on a part of the graph of around 300 million nodes, 1 billion edges and over 1.2 billion training pairs, and the way the way we do this is that we generate these networks.",
                    "label": 1
                },
                {
                    "sent": "But basically this is what material was asking.",
                    "label": 0
                },
                {
                    "sent": "But when you do this, basically we kind of fix the template of the network.",
                    "label": 0
                },
                {
                    "sent": "We fixed the maximum degree in this network so that all neural networks have the same memory footprint, so that when you push them through the GPU.",
                    "label": 0
                },
                {
                    "sent": "This really flies right?",
                    "label": 0
                },
                {
                    "sent": "So the way we do this is that we have this producer consumer type of workload where the producer is generating this neural networks very fast, and then the consumer the GPU is taking them and passing information through them and you want all these networks to have kind of the same memory layout so you can really push them out very very quickly and generally we would would take about 2000 networks.",
                    "label": 0
                },
                {
                    "sent": "And compute the gradients over the 2000 networks per mini batch.",
                    "label": 0
                },
                {
                    "sent": "For people who know what I talk about, OK, great.",
                    "label": 0
                },
                {
                    "sent": "So this is how you do it now that you have figured out right those parameters here, the question is how do we now infer the coordinates of all the all the three billion nodes?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we do to do this?",
                    "label": 0
                },
                {
                    "sent": "We use MapReduce, right?",
                    "label": 0
                },
                {
                    "sent": "MapReduce is cool because it's really about propagating information one step in the graph, right?",
                    "label": 0
                },
                {
                    "sent": "So basically in two iterations of MapReduce, we are able to compute the embeddings of all the nodes, right?",
                    "label": 0
                },
                {
                    "sent": "And MapReduce is cool, 'cause it avoids you or prevent you from doing repeated computation, right?",
                    "label": 1
                },
                {
                    "sent": "If you need to compute the level one embedding for this particular node, because this this.",
                    "label": 0
                },
                {
                    "sent": "Particular node will appear in many many different networks, becauses in many many different computation graphs MapReduce will make this computation only once and then share the results with whoever else needs to consume that information, right?",
                    "label": 0
                },
                {
                    "sent": "So again, MapReduce works really well for this and allows you to scale to these huge networks.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To show you how well this works, the task is what we call related PIN recommendation where given the query pink you I want to say what is the pin X that the person is going to click next and this is in some sense a nearest neighbor task where I say how?",
                    "label": 1
                },
                {
                    "sent": "What is the rank of the of the true related pin X based on the distance from the query Q an Now what are the things we will compare against?",
                    "label": 0
                },
                {
                    "sent": "We'll compare against something that does only visual, so says I have this image.",
                    "label": 0
                },
                {
                    "sent": "I have the other image.",
                    "label": 0
                },
                {
                    "sent": "How similar are the images?",
                    "label": 0
                },
                {
                    "sent": "Then we'll do something that is text based based on word two VEC.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example of.",
                    "label": 1
                },
                {
                    "sent": "Compare against a purely graph based random walk based method that is right now heavily used that Pinterest I developed this thing as well and then we'll compare about against our PIN Sage experimental setup is that we will take these 3 billion pins and bad each pin in 1000 dimensional space and then we'll do nearest neighbor generator computation in across the space to generate recommendations right so?",
                    "label": 0
                },
                {
                    "sent": "We also have a super fast high dimensional nearest neighbor engine that allows us to do this.",
                    "label": 0
                },
                {
                    "sent": "How well this works.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here here I'm showing you performance in terms of mean reciprocal rank.",
                    "label": 1
                },
                {
                    "sent": "Higher values are better.",
                    "label": 1
                },
                {
                    "sent": "I'm comparing against the the textual matching visual matching and then the the basically the graph based algorithm that essentially learns how to take takes as basic features, visual and annotation and then learns how to aggregate that information from across the graph to compute something about the node itself.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Sorry, random walk approach which you were presenting last year.",
                    "label": 0
                },
                {
                    "sent": "Random Walk is somewhere here.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the next example, which I think will.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how why things work and why things fail right for this particular query image?",
                    "label": 0
                },
                {
                    "sent": "Here are three different approaches.",
                    "label": 0
                },
                {
                    "sent": "How do you find similar pins?",
                    "label": 0
                },
                {
                    "sent": "If you do it based on visual similarity, then you kind of get the visual part correct, but you kind of miss miss the topic right.",
                    "label": 0
                },
                {
                    "sent": "These are some loggers cutting down the tree and you know that's more about farmers and soldiers and so on, right?",
                    "label": 0
                },
                {
                    "sent": "If you do it based on text, you basically lack resolution, right?",
                    "label": 0
                },
                {
                    "sent": "This is about trees and that's about trees and logging as well.",
                    "label": 0
                },
                {
                    "sent": "Right, similar happens when you only use the graph structure, but basically when you use the graph structure plus the older image information and pass that through the through the graph Sage architecture, you get really good recommendations, right?",
                    "label": 0
                },
                {
                    "sent": "It's all about kind of historic.",
                    "label": 0
                },
                {
                    "sent": "Maybe this or not, but all his kind of historic pictures of people cutting down trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 1 example.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example.",
                    "label": 0
                },
                {
                    "sent": "This is about planting.",
                    "label": 0
                },
                {
                    "sent": "When you do, visual basically confuses soil with ground meat, so we get kind of Korean food and stuff and then right text.",
                    "label": 0
                },
                {
                    "sent": "Again, you don't have enough resolution.",
                    "label": 0
                },
                {
                    "sent": "Same.",
                    "label": 0
                },
                {
                    "sent": "Same with graph, but if you combine combine the two, you basically get something about gardening, little plants and so on, right so?",
                    "label": 0
                },
                {
                    "sent": "This is this is how well this works and right now this particular system is running in production at Pinterest and it's making product recommendations by both on the visual information as well as the graph structure right?",
                    "label": 0
                },
                {
                    "sent": "So the way you can really think of this is to say if I want to learn something about one object, it's not enough, only that the object nodes about itself, but it's good that it knows also who are its neighbors in the graph.",
                    "label": 0
                },
                {
                    "sent": "And then we learn how to aggregate the information about across the graph to.",
                    "label": 0
                },
                {
                    "sent": "Kind of energy representation of the object itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's number one.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similarity between between nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "The second thing I want to show is about predicting drug side effects, so this will be this is joint work with marine cogic Nick.",
                    "label": 1
                },
                {
                    "sent": "And was published that Bioinformatics Journal this year, so let me tell you again.",
                    "label": 1
                },
                {
                    "sent": "Kind of the same technology will be applied but with a bit different twist so.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the idea the idea is to be able to do to predict what is called polypharmacy side effects.",
                    "label": 1
                },
                {
                    "sent": "If you look in US, you know half of the people between 70 and 79 take more than five drugs, and many patients take 20 drugs to treat whatever diseases they have.",
                    "label": 1
                },
                {
                    "sent": "And the problem is right when you do drug development.",
                    "label": 0
                },
                {
                    "sent": "When you do drug trials.",
                    "label": 0
                },
                {
                    "sent": "You cannot now combinatorially evaluate every drug with every possible other drug up to I know size 5 or 10 to see that there are no unwanted side effects, right?",
                    "label": 0
                },
                {
                    "sent": "So the problem becomes that even if one drug is safe and the other drug is safe in isolation, when you take both at the same time, there are, there might be some side effects that happen because of the interaction between the two graphs between the two drugs.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to be able to predict these types of things from.",
                    "label": 0
                },
                {
                    "sent": "Paralyze drug interactions.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to do this?",
                    "label": 0
                },
                {
                    "sent": "We're going.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To create this type of multi Model 2 layer network that has two types of nodes, the circles are the proteins and the edges between the circles are physical protein interactions and we have around 20,000 proteins and about 700,000 edges.",
                    "label": 0
                },
                {
                    "sent": "Then every triangle is a different drug and drug.",
                    "label": 0
                },
                {
                    "sent": "Drugs target proteins.",
                    "label": 0
                },
                {
                    "sent": "So we have the drug target relationships between about.",
                    "label": 1
                },
                {
                    "sent": "19,000 edges and there is about 5000 drugs and then right?",
                    "label": 0
                },
                {
                    "sent": "So this means how basically each drug, each molecule targets certain proteins and changes their function.",
                    "label": 0
                },
                {
                    "sent": "So we create those edges and then the way we model side effects is remodeled side effects between a pair of drug with the relationship.",
                    "label": 0
                },
                {
                    "sent": "A typed relationship between a pair of drug where this would say that drugs C&D recalls a side effect R2.",
                    "label": 1
                },
                {
                    "sent": "OK, and that there are some FDA national adverse Event Reporting System and we have about 4.6 million of these adverse side effects and final graph right?",
                    "label": 1
                },
                {
                    "sent": "We have about 1000 different side effects so we have about 1000 types of different edges.",
                    "label": 0
                },
                {
                    "sent": "OK, So what will be my task?",
                    "label": 0
                },
                {
                    "sent": "My task will be that given a pair of drugs predict what type of edge is there between them?",
                    "label": 0
                },
                {
                    "sent": "Where a type of an edge is one of the 1000 different types, which is the type of the side effect that might occur.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what we want to do.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how are we going to do this?",
                    "label": 0
                },
                {
                    "sent": "We'll do this in two steps.",
                    "label": 0
                },
                {
                    "sent": "We will first build an encoder that will take a node in the network and create an embedding, and then we'll have a decoder that will take the embedding embeddings of two nodes of two drug nodes and it will predict whether there is there is a relationship and what type of relationships is between that pair of drugs, which basically means what kind of side effect is caused.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So how will this?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look like so the idea is that I want to compute the embedding for the drug, see here in the center.",
                    "label": 0
                },
                {
                    "sent": "So my neural network now will incorporate information from many different levels, right?",
                    "label": 0
                },
                {
                    "sent": "It will incorporate the information based on the side effects of drug C to the other drugs in the graph.",
                    "label": 0
                },
                {
                    "sent": "So this is trying to be indicated here and we will have a separate kind of neural network for every different side effect type, right?",
                    "label": 0
                },
                {
                    "sent": "So here is the.",
                    "label": 0
                },
                {
                    "sent": "Structure of the neural network for side effect type R1, which is, you know, some gastrointestinal bleeding, right?",
                    "label": 0
                },
                {
                    "sent": "And then you know there will be another different type of a neural network for their side effects are two becausw drugs.",
                    "label": 0
                },
                {
                    "sent": "She has side effects are two with D&S right?",
                    "label": 0
                },
                {
                    "sent": "So here's that same kind of neural network as we talked before and then the last part of the neural network will come from the protein interactions, right?",
                    "label": 0
                },
                {
                    "sent": "So here is the structure of the neural network.",
                    "label": 0
                },
                {
                    "sent": "From the protein interactions and now we will learn how to fuse all this information to come up with an embedding for node C in our network.",
                    "label": 0
                },
                {
                    "sent": "OK, and now here is.",
                    "label": 0
                },
                {
                    "sent": "Here is my embedding that we will compute and now that we have this we need to.",
                    "label": 1
                },
                {
                    "sent": "Now we have an encoder and of course we will have a different structure of the neural network.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For every different node in this graph, and again what we have to learn is we will assume parameter sharing and we need to learn the parameters of these different boxes, right?",
                    "label": 0
                },
                {
                    "sent": "And these parameters will be unique for each side effect, app type or relationship type.",
                    "label": 0
                },
                {
                    "sent": "So these are now the encoders that basically take the information from the drugs neighborhood aggregated to produce an embedding for that given drug.",
                    "label": 1
                },
                {
                    "sent": "Now the next step is we need to build a decoder.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A decoder takes 2 drugs, their embeddings and needs to predict the type of relationship they have, right?",
                    "label": 1
                },
                {
                    "sent": "So what we're doing is we have 1000 different link prediction tasks right?",
                    "label": 0
                },
                {
                    "sent": "One link prediction task for every separate side effect right?",
                    "label": 1
                },
                {
                    "sent": "And the way the way we do this is that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take these embeddings.",
                    "label": 0
                },
                {
                    "sent": "And then basically we use a different a different set of parameters, a different kind of logistic regression model that predicts the probability for each different side effect type.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we can basically again put all this together where we can train this in a supervised way over the known side effect types.",
                    "label": 0
                },
                {
                    "sent": "We can back propagate the gradients and figure out all the parameters of this model so that we can make correct predictions.",
                    "label": 0
                },
                {
                    "sent": "And you can say, how well does this work?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are able to beat state of the art for around 36 to 40% over what people are using before and maybe what is even more interesting is that we.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lost our model and said just go and make the predictions that you are most confident about.",
                    "label": 0
                },
                {
                    "sent": "So here are ten of our most confident predictions where we say that drugs CND will interact with these given side effect and these are predictions that are not in our data set.",
                    "label": 0
                },
                {
                    "sent": "This is not kind of hold out.",
                    "label": 0
                },
                {
                    "sent": "These are edges that don't exist in our data set, so these are true predictions.",
                    "label": 0
                },
                {
                    "sent": "We have no clue about them.",
                    "label": 0
                },
                {
                    "sent": "So what we did then is to ask our doctors at the medical school to go over the latest research and see.",
                    "label": 0
                },
                {
                    "sent": "If there is any evidence in the literature that some of these might be true.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we were able to find evidence for five out of those 10.",
                    "label": 0
                },
                {
                    "sent": "And you can see that these are all kind of papers from the last one 2 two years, where these types of interactions were discovered and they are not part of our data set.",
                    "label": 0
                },
                {
                    "sent": "So this is really kind of prediction.",
                    "label": 0
                },
                {
                    "sent": "This is true prediction.",
                    "label": 0
                },
                {
                    "sent": "This is not cross validation, it's really about something that doesn't didn't exist.",
                    "label": 0
                },
                {
                    "sent": "And of course, why is this interesting?",
                    "label": 0
                },
                {
                    "sent": "Is cause this.",
                    "label": 0
                },
                {
                    "sent": "This examples that haven't yet been discovered.",
                    "label": 0
                },
                {
                    "sent": "These are not necessarily mistakes, right?",
                    "label": 0
                },
                {
                    "sent": "We can think of this as hypothesis about what else, what other types of side effects we may expect for this particular sets of drugs.",
                    "label": 0
                },
                {
                    "sent": "Good, so this was my.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example number 2.",
                    "label": 0
                },
                {
                    "sent": "And the last thing I would like to say is give you my example #3 which will be about how could you use these two reason in knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "OK and this is again a paper that will be presented in a few weeks at NIPS in Montreal.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we mean?",
                    "label": 0
                },
                {
                    "sent": "So what we mean is that I will.",
                    "label": 0
                },
                {
                    "sent": "I want to go and create heterogeneous knowledge graphs, right?",
                    "label": 1
                },
                {
                    "sent": "I can take all of my knowledge of biology and create this type of heterogeneous network where I have proteins that interact with each other using let's say, different types of interactions.",
                    "label": 0
                },
                {
                    "sent": "I can have diseases, diseases.",
                    "label": 0
                },
                {
                    "sent": "Things are associated with each other.",
                    "label": 0
                },
                {
                    "sent": "I have drugs, drugs can treat each other.",
                    "label": 0
                },
                {
                    "sent": "Drugs target different proteins.",
                    "label": 0
                },
                {
                    "sent": "Drugs can cause side effects.",
                    "label": 0
                },
                {
                    "sent": "Proteins are part of different biological processes.",
                    "label": 0
                },
                {
                    "sent": "Biological processes are part of a hierarchy, right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a schema for my knowledge graph about how biology works and these numbers here tell me how many nodes of a different type do I have, right?",
                    "label": 0
                },
                {
                    "sent": "So there's 17,000 proteins, 14,000 diseases, in this case, 11,000 drugs and around 10,000 side effects, and I can also take, let's say, a social community.",
                    "label": 0
                },
                {
                    "sent": "This is Reddit and I can create a knowledge graph about that where I say.",
                    "label": 0
                },
                {
                    "sent": "I have users users can upload, create, download or comment on a post.",
                    "label": 0
                },
                {
                    "sent": "Posts contain words, pose belong to communities, users subscribe to communities right?",
                    "label": 0
                },
                {
                    "sent": "And I can create a network with these different relationship types.",
                    "label": 0
                },
                {
                    "sent": "Between half a million users.",
                    "label": 0
                },
                {
                    "sent": "150,000 posts 250,000 words and 105 different communities.",
                    "label": 0
                },
                {
                    "sent": "And then the question is, what if I want to start asking interesting?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of complex queries over this over this over this graph, right?",
                    "label": 0
                },
                {
                    "sent": "So I can say predict me all communities C in which user U is likely to upload the post and the way you can think of these types of questions you can think of them as in this case as of as path queries, right?",
                    "label": 0
                },
                {
                    "sent": "I'm saying I want a user, you that that is the author deposed that belongs to Community C right?",
                    "label": 0
                },
                {
                    "sent": "And the way you would do this traditionally is that you would have your graph.",
                    "label": 0
                },
                {
                    "sent": "You would have that query template.",
                    "label": 0
                },
                {
                    "sent": "And then you would try to match that query template to your graph and wherever it matches you would say, oh, I found the answer.",
                    "label": 0
                },
                {
                    "sent": "Of course, the problem is that these graphs might be might be noisy.",
                    "label": 0
                },
                {
                    "sent": "They might have missing relationship.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it may not be possible for you to even like do this hard subgraph matching becausw.",
                    "label": 0
                },
                {
                    "sent": "The edges might be missing or noisy, right?",
                    "label": 0
                },
                {
                    "sent": "So the question is, could you use any of these embedding technology to do this kind of graph?",
                    "label": 0
                },
                {
                    "sent": "Graph subgraph matching without doing the subgraph matching?",
                    "label": 0
                },
                {
                    "sent": "So what we tried and works quite Interestingly is the following.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is that I will take my input graph and I will learn coordinates of the nodes and then the logical operations.",
                    "label": 1
                },
                {
                    "sent": "I will learn them a spatial operations in this space, right?",
                    "label": 0
                },
                {
                    "sent": "So if I want to be able to answer that particular query up there with the two anchor nodes D1 and D2, then kind of get together to a P and then to see this query would be basically predict me.",
                    "label": 0
                },
                {
                    "sent": "Drugs.",
                    "label": 0
                },
                {
                    "sent": "That are associated with a given protein that is associated with diseases D1 and D2, and the way I can think of this is that I will start with diseases D1 and D2.",
                    "label": 0
                },
                {
                    "sent": "I will apply a given projection operation, which is basically just multiply with the matrix in this embedding space.",
                    "label": 1
                },
                {
                    "sent": "Then I'll apply the set intersection operation that will take this to individual points.",
                    "label": 0
                },
                {
                    "sent": "Create one point of it, and then I'll do the last step from P2C by applying a different projection operation.",
                    "label": 0
                },
                {
                    "sent": "Wherever I end up, I'll do the nearest neighbour.",
                    "label": 0
                },
                {
                    "sent": "Query here and whatever nodes are in this area.",
                    "label": 0
                },
                {
                    "sent": "Those are answers to my query.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's essentially the idea, so let me just walk you through.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How we think of this right?",
                    "label": 0
                },
                {
                    "sent": "So imagine I want to say, you know, predict me what users are likely to read about events shared by media outlets A&B, where A&B are two concrete media outlets, right?",
                    "label": 1
                },
                {
                    "sent": "So I say, nodes A&B have a particular position in the graph, so now I will apply the.",
                    "label": 0
                },
                {
                    "sent": "The projection operator for the read relation, right?",
                    "label": 0
                },
                {
                    "sent": "Because I want to say users users is reading about this particular media types, then I will take the.",
                    "label": 0
                },
                {
                    "sent": "The as I set projection according to the reading operation.",
                    "label": 0
                },
                {
                    "sent": "Then I'll take the intersection operation between the two media types to get to 1 node with kind of 1 location and then in the in the.",
                    "label": 0
                },
                {
                    "sent": "These are basically now.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Events that are likely to be shared with both.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Outlets A&B and then I will do the last step, which will be the user is likely to retweet events shared by these two particular.",
                    "label": 1
                },
                {
                    "sent": "Outlets so now basically, whatever, whatever, whatever are the nearest neighbors of of that particular node in my upper space, those will be the answers to my query.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's kind of the idea.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this?",
                    "label": 0
                },
                {
                    "sent": "How do I do this?",
                    "label": 0
                },
                {
                    "sent": "There are two things I want to learn.",
                    "label": 0
                },
                {
                    "sent": "I want to learn node embeddings, coordinates of the nodes.",
                    "label": 0
                },
                {
                    "sent": "I want to learn this projection, an intersection operators that have.",
                    "label": 0
                },
                {
                    "sent": "Different types of parameters projection operation is just basically a matrix R intersection operator has is more.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a little neural network plus A plus a matrix.",
                    "label": 0
                },
                {
                    "sent": "And then the way I will train this is again basically using some kind of standard negative examples where I'll do where I will say for this query this is the correct answer and this is an incorrect answer.",
                    "label": 0
                },
                {
                    "sent": "And then I also come up with a bit more clever scheme how to generate.",
                    "label": 0
                },
                {
                    "sent": "Negative examples so that I can learn those node embeddings well as well as this spatial operations over the query so.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I said before, given an arbitrary query, basically the way we execute this is by applying correct set of spatial operations wherever wherever is the final point.",
                    "label": 0
                },
                {
                    "sent": "Wherever is the final point.",
                    "label": 0
                },
                {
                    "sent": "Wherever we end up, we just do the nearest neighbor and whatever is close to this node.",
                    "label": 0
                },
                {
                    "sent": "This will be the answer to our query and I can really show that this works.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prizing Lee well, so I had two networks before I had the bio network that I was talking about and I had the Reddit network.",
                    "label": 0
                },
                {
                    "sent": "Here is the performance in terms of giving the correct answer to the query for queries with different different logical structures, right?",
                    "label": 0
                },
                {
                    "sent": "So these are just one step.",
                    "label": 0
                },
                {
                    "sent": "These are kind of a two step type queries.",
                    "label": 0
                },
                {
                    "sent": "You know multiple starting points like a really like.",
                    "label": 0
                },
                {
                    "sent": "Long chain of in directions and so on.",
                    "label": 0
                },
                {
                    "sent": "And we can do this with AUC.",
                    "label": 0
                },
                {
                    "sent": "You know, for the biological network above, .9 and four edit between, let's say .8 and 9 right?",
                    "label": 0
                },
                {
                    "sent": "And this is quite interesting 'cause it really basically sidesteps this graph.",
                    "label": 0
                },
                {
                    "sent": "Matching in this combinatorial explosion and really allows me to do these things.",
                    "label": 0
                },
                {
                    "sent": "Basically to translate these problems into doing proper spatial operations.",
                    "label": 0
                },
                {
                    "sent": "In the embedding space plus nearest neighbor and what we're trying to do now is understand.",
                    "label": 0
                },
                {
                    "sent": "Why does this work and how do we need to train it and so on.",
                    "label": 0
                },
                {
                    "sent": "But I think it's a very kind of interesting and exciting direction.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will quickly conclude.",
                    "label": 0
                },
                {
                    "sent": "So what I was talking about is this idea.",
                    "label": 0
                },
                {
                    "sent": "How do we learn embeddings features of nodes of nodes in the network?",
                    "label": 1
                },
                {
                    "sent": "An hour, 3 talking about?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graph convolutional neural networks that basically take take this idea of convolutions from the computer vision literature, but generalize them to arbitrary graphs, and basically they allow us to fuse note feature information in the graph information to basically obtain state of the art accuracy on any kind of node classification and link prediction.",
                    "label": 1
                },
                {
                    "sent": "What is nice is that the model size the parameters are independent of the data size, so these things can be really scaled.",
                    "label": 0
                },
                {
                    "sent": "Two large graphs.",
                    "label": 0
                },
                {
                    "sent": "It's running in production at Pinterest.",
                    "label": 1
                },
                {
                    "sent": "And leads to significant performance gains and what is nice is that you can use this for tasks so different as making pin recommendations, predicting drug side effects, or answering complex queries over graphs, right so?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know what have you learned in the last 1 two years?",
                    "label": 0
                },
                {
                    "sent": "Is that this kind of representation learning paradigm, specially in a supervised way can actually be extended to graphs that kind of know row structural feature engineering is needed and that it allows us to effectively combine attributes data with the network information in this way and reach what we know about each object by also fusing in the information from the network structure.",
                    "label": 1
                },
                {
                    "sent": "And it's also interesting, right?",
                    "label": 1
                },
                {
                    "sent": "That basically we can use this to do this type of end to end training instead of this separate cut kind of multi stage type training especially.",
                    "label": 0
                },
                {
                    "sent": "I think when I showed you the drug side effect prediction right, that was quite complex.",
                    "label": 0
                },
                {
                    "sent": "You had like at the end you were making thousand predictions.",
                    "label": 0
                },
                {
                    "sent": "You had two networks, one for every node and these things can all nicely be done and really trained in an end to end way.",
                    "label": 0
                },
                {
                    "sent": "Water.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some next steps.",
                    "label": 0
                },
                {
                    "sent": "Next steps are thinking about how does this generalize to dynamic evolving graphs?",
                    "label": 1
                },
                {
                    "sent": "How can we build kind of domain specific versions of this architecture for various kinds of applications?",
                    "label": 0
                },
                {
                    "sent": "I gave you an example of recommender systems, but there is much more that can be done.",
                    "label": 1
                },
                {
                    "sent": "This type of things can be used for graph generation as well, so one of the research projects we have is to generate graphs of molecules where basically the idea is that you say.",
                    "label": 0
                },
                {
                    "sent": "I want the molecule with these types of properties and we'd be able to generate your formula, generate your graph of that molecule, and then right as I showed your example, it seems that this idea of going beyond just predicting single edges, but doing this kind of multi hop prediction or answering this logical queries is a very interesting research direction, and the last the last thing is like any kind of understanding on the representational capacity of these models, why they work when they work, and how.",
                    "label": 0
                },
                {
                    "sent": "Is also very interesting, so this is all I wanted to say.",
                    "label": 0
                },
                {
                    "sent": "And I'd be very happy to take questions.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I say thank you to the basically PhD students and postdocs who did all the work and funding members and collaborators.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}