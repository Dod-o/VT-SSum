{
    "id": "imu56e4jntmje25c3p4cz2i4bt5sgc3e",
    "title": "A Comparison of Data Structures to Manage URIs on the Web of Data",
    "info": {
        "author": [
            "Ruslan Mavlyutov, eXascale Infolab, University of Fribourg"
        ],
        "published": "July 15, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2015_mavlyutov_web_data/",
    "segmentation": [
        [
            "Welcome everyone.",
            "First of all, thank you for coming even though it is the last session.",
            "My Steven said my talk will be very technical.",
            "I will try not to be very boring."
        ],
        [
            "My name is Ruslan.",
            "I'm from exascale in follow up that resides in University of Fribourg, Switzerland.",
            "This is a photo of our group at the scale we are doing research in three major topics.",
            "Those are semantic, web, distributed databases and information retrieval.",
            "When it comes to semantic web, we have our own super fast and efficient triple storage called Diplo Dokus and it's over marching.",
            "By the way, sitting here watching show itself.",
            "So we were benchmarking into load in search of potential bottlenecks in performance and we found that adding the certain circumstances dictionary operations might take a bit of time."
        ],
        [
            "Just to get into a context, what kind of dictionary operations?",
            "So basically, like almost any other triple storage, we're not working with triples as strings.",
            "We upload that we convert everything into integer or your eyes into integer IDs, and for example when we return query result, we do a backward conversion while the backward conversion from integer to string is quite straightforward, assuming that you keep your eyes is an array and the internal ID is just an index in that array.",
            "Everyone, everything is simple like constants, time and almost no memory overhead.",
            "Well this is very simple, but the initial mapping from string to ideas requires dedicated data structure and might take from 10 to 60% of operational time, especially during the upload phase or when you process RDF streams.",
            "To tackle this problem we were trying different data structures and data sets and there is also our experiments came up with the paper that I'm going to present you now."
        ],
        [
            "So in the paper, we've empirically compared different dictionary paradigms and implementations.",
            "We analyzed tradeoffs.",
            "In the case of semantic Web, you arise and we came up with solutions for different use cases of working with your eyes."
        ],
        [
            "A little bit about experimental setup.",
            "What we're actually doing, so we take data set and they take a certain implementation of data structure, let's say C plus implementation of binary search tree.",
            "Then we start with an empty dictionary.",
            "We insert their 100,000 you rise from the data set.",
            "We measure how much time did it take an what is the memory consumption of a dictionary after that insert?",
            "And when we do 100,000 random queries from those who writes that already in the dictionary, and we again look how much time did it take, then we will repeat this circle again and again until we put all your eyes from the data set in the dictionary and in the end we have these profiles of insert query time and memory consumption.",
            "This profiles we can analyze and compare with results for other data structures, for example or other data sets we repeat.",
            "Each experiment at least 10 times an for each point on the graph, we report the average of value in the interquartile range.",
            "Then we separate the different cases.",
            "One is the data set is sorted and another then the keys are shuffled because that makes a serious effect on the performance of certain data structures.",
            "And finally we did all experiments on the machine with 64 gigabytes of RAM, which allowed us to work with data sets of up to 1100.",
            "Millions of your eyes."
        ],
        [
            "Now we can.",
            "Let's see what kind of data sets we had.",
            "We have.",
            "We created six data sets, three of them of different sizes from Lehigh University benchmark.",
            "This benchmark is bout universities and all kinds of entities around them.",
            "Then we have a data set from Berlin benchmark.",
            "It is about electronic Commerce and also data set from our own benchmark called Bologna Bench, which is surprisingly about Bologna process.",
            "And finally we have also data set from DB.",
            "Media and this does stand sort of apart from the others because it has completely different UI structure while the previous data set have very structured and regular your eyes.",
            "For example, let's take Lehigh University data set, so there are like objects of around 50 classes and object name is just a concatenation of its class, an index, and to get full URL you just concatenate names of parents of all names of all parents in the universe hierarchy.",
            "So it yields very structured your eyes.",
            "Indeed, it's always like the same prefix, which is actually a host name and then name of real objects.",
            "And as you can understand, those names can be maybe very diverse and this makes a.",
            "Huge affect.",
            "Certain data structures.",
            "And the performance in certain data structures."
        ],
        [
            "Let's take a look at the macro parameters so we can see that the data set sizes will rise from twenty millions to 100 millions, your eyes.",
            "The average you rewrite length arise from 50 to 100 symbols and we also tried to estimate the informational entropy.",
            "We did it we through the compression rate which we were able to achieve with zip compressor and indeed you can see that the period data set is much harder to compress only 10 times, then for example.",
            "For example, Berlin benchmark, which you can press up to 70 times."
        ],
        [
            "Now that kind of data structures we were using.",
            "First, I want to say that we were doing everything in C and we came up with nine other structures.",
            "Most of when all of them are well known and frequently used for purpose and but we also wanted to include something you which appeared recently.",
            "So for example, for search trees we have.",
            "STL map and Estates implementation of B + 3.",
            "We have four implementations of hash tables to them are from STL and boost and two from Google.",
            "And finally we have free implementations of tries.",
            "Actually these are fridge different generations of tries.",
            "I would say first one is lexicographic try which stands for classic try where you have one symbol in a note.",
            "Then we have.",
            "R3, which is actually a new version of RDX.",
            "Try in red extra you merge with children on all nodes that have only one child.",
            "And finally we have an H drive which stands for out say updated version of the birth dry in the burst dry.",
            "You may have several keys in leaf nodes and they use a dedicated structure for leaves.",
            "In this case, the original idea of using Linkedlist were replaced with hash tables.",
            "Not simple hash tables, but cache conscious hash tables."
        ],
        [
            "Yeah, now we can proceed to results.",
            "I just want to say that we got around 200 graphs.",
            "Of course I'm not going to show you all of them, I will just scheme for the main insides.",
            "But if you're curious, please don't hesitate to Wales visit our website.",
            "There you can find the code data sets, result logs and also graphs.",
            "And."
        ],
        [
            "Now we can proceed the results.",
            "I will start with hash tables.",
            "Here we can see.",
            "In certain query profiles for stale, unordered map and the data set is the biggest one we have the keys in this case are sorted.",
            "You can immediately support.",
            "I hope I'm pointing right.",
            "You can immediately spot those peaks.",
            "Those pics are exactly actually timeouts.",
            "Then the hash table is filled to a certain amount which is defined by the value of fill factor.",
            "At this point it has to create another table which is twice as bigger and move.",
            "Records to this new table, but as we know from the books, it's absolutely safe because the size of those peaks is.",
            "Is proportional to the amount of cheap inserts that will be for it.",
            "So in the end we have something what is called amortized constant time.",
            "I hope you see I put as a red dashed line line, the average in their time, and taking into account that Y axis here is logarithmic, we have 10% difference from the bottom blue line, which is absolutely fine if we take a look at the query trend will see that it's slightly gross, but.",
            "The increasing query time is mostly due to the fact that the data structure is getting bigger, so the amount of cache misses increases.",
            "That's why even though this trend should be constant, we have.",
            "Slow growing of time.",
            "So as I said, this data set was sorted and let's put on this graph the results for shuffle data set."
        ],
        [
            "I hope you see those these trends perfectly coincide.",
            "This shows the perfect amazing feature of hash tables.",
            "They don't really care if your data set is sorted or not.",
            "This is very important feature if you're processing big data sets which you cannot sort, or you processing RDF streams.",
            "If I put here the result."
        ],
        [
            "For our other hash tables and we will see that well in general their coincide, but there is sort of outlier.",
            "Google sparse hash map.",
            "It's in their time is bigger, but it's mostly due to the fact that a few factories hire it allows to save some memory, but increase the number of collisions.",
            "And in general like in their time getting bigger."
        ],
        [
            "Let's see how hash tables behave on different data sets.",
            "These are results for Google Dense hash map.",
            "The fastest we have and I put here transfer all six data sets.",
            "We have.",
            "The keys are sorted.",
            "We again see that trends.",
            "Coincide kind of coincide, and this is another good feature that hash tables don't really depends on.",
            "The average year I length and type of the data set, that type of your eyes.",
            "So too."
        ],
        [
            "Summarize a little bit about hash tables, then generally quite fast, they have constant insert and query time.",
            "They perform.",
            "This doesn't depend on keys order and data set type and another very important features that they're quite adjustable.",
            "You can play with Phil Factor.",
            "You can create your own hash function.",
            "For example if you like processing your eyes, you can in the hash function you can hash on the last part of the string and drop the host name.",
            "That will make the process even faster.",
            "And hopefully I don't know I didn't test it by the way, and.",
            "Yeah also you can pre allocate for example the size of the table.",
            "If you know the size of the data set you will have so that would.",
            "So at this point we can actually summarize and say, well, hash tables are perfect for working with strings and if you have that issue you should take hash tables.",
            "But they have one important drawback.",
            "They consume a lot of memory and I will show it later.",
            "Now let's.",
            "Proceed to the search trees."
        ],
        [
            "Again, you can see that in the article and query profile for the biggest data set we have source, the keys are sorted.",
            "I put as a red dash, light.",
            "The result line is the results for Google Dance Cache map.",
            "And you can immediately see two things.",
            "First of all, the trends are logarithmic.",
            "This is what we know from books, and the 2nd is that their performance is.",
            "Quite lower comparing to.",
            "Comparing to hash table.",
            "But that is not the main issue actually.",
            "So what will happen if the if the keys are not sorted?"
        ],
        [
            "This shuffle data set sorted other set shuffle sorted so we see that the performance decreases dramatically.",
            "And what is especially bad?",
            "It's not about performance of insert time, it's also performance of query time.",
            "That means that if you created your dictionary in a wrong way.",
            "You will get an inferior very bad performance during query time during queries.",
            "And yeah, that's very bad, obviously.",
            "Let's see the performance on different data sets."
        ],
        [
            "So these are results for STL map all data sets and they're sorted.",
            "This trends perfectly reflect the order."
        ],
        [
            "Of the data sets, if we sort them by every two right length.",
            "Basically, it's not surprising just because, well, if you insert the key into the.",
            "Search tree or your query key.",
            "You have to do this logarithmic amount of comparisons and when you compare two strings it's all about their length.",
            "So in common, the longer you rise you have, the more time it will take for you to work with search trees.",
            "Summer."
        ],
        [
            "It's a little bit, so search trees in general have logarithmic complexity.",
            "There, slower, slower than hash tables, their performance decrease dramatically if the keys are shuffled and their performance also depends on your eye length, so that makes the performance completely inferior comparing to hash tables."
        ],
        [
            "There is absolutely no reason to use search trees.",
            "Well, except for the case, if you're talking bout B + 3 an on disk operations, but we're not talking about this case right now, OK?",
            "Now let's proceed to tries."
        ],
        [
            "And again, you can see the profile, insert, profile and query profile and red dotted line is a result of Google dense hash map.",
            "The fastest so far and we can see that at least for new data structures, the performance is quite good.",
            "And in terms of insert time, it's even faster.",
            "But what will happen if the data set is shuffled?"
        ],
        [
            "Again, catastrophe and.",
            "But at least this time in this list, this time we can say that it is most at least if you're talking about new data structures, we can say that it's mostly about insert time.",
            "So query time doesn't degrade that much.",
            "So which is already."
        ],
        [
            "Something if we let it take a look on the performance on different data sets I showed here average query and insert time is a function of every to right size.",
            "And we see almost perfect lines, but except for the one case, it's DB pedia data set with lexicographic tree.",
            "And we can guess why it happens.",
            "As I said, the pager data set is quite complex and for that reason try stars branching very early.",
            "Those not notes that are closer to the route.",
            "They have a lot of children but then you have long tails without any branching and since lexicographic trees as I said it's a classical try.",
            "It has to create a separate node.",
            "Or for each letter?",
            "Almost for each key which it insert, it has to create enormous amount of nodes.",
            "Something similar happens when you do query, but in this case we just have to jump through the memory until we reach the leaf.",
            "You have to do a lot of jumps.",
            "Basically that makes performance or flexographic tree on this data set in bad comparing to other data sets and other data structures."
        ],
        [
            "To summarize about tries so they in general quite fast if not talking bout primordial classic, try but the insert time degrades when the data set is shuffled and their performance slightly depends on the URL length, so that would make these tries poor candidate comparing 2 hash tables, but they have very important adventure advantage.",
            "They consume much less memory."
        ],
        [
            "Finally, I can show you total memory consumption.",
            "This is total memory consumption for all data sets and all data structures.",
            "And here you can see that hash tables and search trees 'cause you consumes up to three times more memory than the original data set, while four tries those numbers, numbers are from zero point 5 to one.",
            "1.5 and two.",
            "So in case of Hat H Drive, we actually have data set compression which is super cool if you have a lot of data sets.",
            "If you have a lot of dictionaries and you have limited amount of memory now."
        ],
        [
            "And conclude my talk.",
            "So first of all, for in memory operations there is absolutely no need to use search trees.",
            "In case if you're processing data sets which you cannot sort or your assistant processing streams is Vera TOS hash tables.",
            "If you have certain amount of static dictionaries which you can keep sort it, or you upload them before the system starts working, you can use tries so that you save some memory.",
            "Then it's good to embrace new data structures like H dry or our tree.",
            "And finally, I want to say that there is still room for research.",
            "Because as we have seen, none of the dot structures were actually able to leverage the highly regular structure of your eyes.",
            "And of course you have.",
            "If you know your data set before head, you can do this optimizations by yourself.",
            "For example, if you're working with that, Oblu can account again.",
            "Cut the hostnames and keep just dressed in the dictionary.",
            "But if you don't know your data set, if your data set is skater genius, you probably would like to have something more universal adaptive, and I would even say intelligent.",
            "That's it, thank."
        ],
        [
            "Question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Welcome everyone.",
                    "label": 0
                },
                {
                    "sent": "First of all, thank you for coming even though it is the last session.",
                    "label": 0
                },
                {
                    "sent": "My Steven said my talk will be very technical.",
                    "label": 0
                },
                {
                    "sent": "I will try not to be very boring.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Ruslan.",
                    "label": 0
                },
                {
                    "sent": "I'm from exascale in follow up that resides in University of Fribourg, Switzerland.",
                    "label": 0
                },
                {
                    "sent": "This is a photo of our group at the scale we are doing research in three major topics.",
                    "label": 0
                },
                {
                    "sent": "Those are semantic, web, distributed databases and information retrieval.",
                    "label": 0
                },
                {
                    "sent": "When it comes to semantic web, we have our own super fast and efficient triple storage called Diplo Dokus and it's over marching.",
                    "label": 0
                },
                {
                    "sent": "By the way, sitting here watching show itself.",
                    "label": 0
                },
                {
                    "sent": "So we were benchmarking into load in search of potential bottlenecks in performance and we found that adding the certain circumstances dictionary operations might take a bit of time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to get into a context, what kind of dictionary operations?",
                    "label": 0
                },
                {
                    "sent": "So basically, like almost any other triple storage, we're not working with triples as strings.",
                    "label": 0
                },
                {
                    "sent": "We upload that we convert everything into integer or your eyes into integer IDs, and for example when we return query result, we do a backward conversion while the backward conversion from integer to string is quite straightforward, assuming that you keep your eyes is an array and the internal ID is just an index in that array.",
                    "label": 0
                },
                {
                    "sent": "Everyone, everything is simple like constants, time and almost no memory overhead.",
                    "label": 0
                },
                {
                    "sent": "Well this is very simple, but the initial mapping from string to ideas requires dedicated data structure and might take from 10 to 60% of operational time, especially during the upload phase or when you process RDF streams.",
                    "label": 0
                },
                {
                    "sent": "To tackle this problem we were trying different data structures and data sets and there is also our experiments came up with the paper that I'm going to present you now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the paper, we've empirically compared different dictionary paradigms and implementations.",
                    "label": 1
                },
                {
                    "sent": "We analyzed tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "In the case of semantic Web, you arise and we came up with solutions for different use cases of working with your eyes.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit about experimental setup.",
                    "label": 0
                },
                {
                    "sent": "What we're actually doing, so we take data set and they take a certain implementation of data structure, let's say C plus implementation of binary search tree.",
                    "label": 0
                },
                {
                    "sent": "Then we start with an empty dictionary.",
                    "label": 0
                },
                {
                    "sent": "We insert their 100,000 you rise from the data set.",
                    "label": 0
                },
                {
                    "sent": "We measure how much time did it take an what is the memory consumption of a dictionary after that insert?",
                    "label": 1
                },
                {
                    "sent": "And when we do 100,000 random queries from those who writes that already in the dictionary, and we again look how much time did it take, then we will repeat this circle again and again until we put all your eyes from the data set in the dictionary and in the end we have these profiles of insert query time and memory consumption.",
                    "label": 0
                },
                {
                    "sent": "This profiles we can analyze and compare with results for other data structures, for example or other data sets we repeat.",
                    "label": 0
                },
                {
                    "sent": "Each experiment at least 10 times an for each point on the graph, we report the average of value in the interquartile range.",
                    "label": 0
                },
                {
                    "sent": "Then we separate the different cases.",
                    "label": 0
                },
                {
                    "sent": "One is the data set is sorted and another then the keys are shuffled because that makes a serious effect on the performance of certain data structures.",
                    "label": 0
                },
                {
                    "sent": "And finally we did all experiments on the machine with 64 gigabytes of RAM, which allowed us to work with data sets of up to 1100.",
                    "label": 0
                },
                {
                    "sent": "Millions of your eyes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "Let's see what kind of data sets we had.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "We created six data sets, three of them of different sizes from Lehigh University benchmark.",
                    "label": 0
                },
                {
                    "sent": "This benchmark is bout universities and all kinds of entities around them.",
                    "label": 0
                },
                {
                    "sent": "Then we have a data set from Berlin benchmark.",
                    "label": 0
                },
                {
                    "sent": "It is about electronic Commerce and also data set from our own benchmark called Bologna Bench, which is surprisingly about Bologna process.",
                    "label": 0
                },
                {
                    "sent": "And finally we have also data set from DB.",
                    "label": 0
                },
                {
                    "sent": "Media and this does stand sort of apart from the others because it has completely different UI structure while the previous data set have very structured and regular your eyes.",
                    "label": 0
                },
                {
                    "sent": "For example, let's take Lehigh University data set, so there are like objects of around 50 classes and object name is just a concatenation of its class, an index, and to get full URL you just concatenate names of parents of all names of all parents in the universe hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So it yields very structured your eyes.",
                    "label": 0
                },
                {
                    "sent": "Indeed, it's always like the same prefix, which is actually a host name and then name of real objects.",
                    "label": 0
                },
                {
                    "sent": "And as you can understand, those names can be maybe very diverse and this makes a.",
                    "label": 0
                },
                {
                    "sent": "Huge affect.",
                    "label": 0
                },
                {
                    "sent": "Certain data structures.",
                    "label": 0
                },
                {
                    "sent": "And the performance in certain data structures.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's take a look at the macro parameters so we can see that the data set sizes will rise from twenty millions to 100 millions, your eyes.",
                    "label": 0
                },
                {
                    "sent": "The average you rewrite length arise from 50 to 100 symbols and we also tried to estimate the informational entropy.",
                    "label": 0
                },
                {
                    "sent": "We did it we through the compression rate which we were able to achieve with zip compressor and indeed you can see that the period data set is much harder to compress only 10 times, then for example.",
                    "label": 0
                },
                {
                    "sent": "For example, Berlin benchmark, which you can press up to 70 times.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that kind of data structures we were using.",
                    "label": 1
                },
                {
                    "sent": "First, I want to say that we were doing everything in C and we came up with nine other structures.",
                    "label": 0
                },
                {
                    "sent": "Most of when all of them are well known and frequently used for purpose and but we also wanted to include something you which appeared recently.",
                    "label": 0
                },
                {
                    "sent": "So for example, for search trees we have.",
                    "label": 1
                },
                {
                    "sent": "STL map and Estates implementation of B + 3.",
                    "label": 0
                },
                {
                    "sent": "We have four implementations of hash tables to them are from STL and boost and two from Google.",
                    "label": 0
                },
                {
                    "sent": "And finally we have free implementations of tries.",
                    "label": 0
                },
                {
                    "sent": "Actually these are fridge different generations of tries.",
                    "label": 0
                },
                {
                    "sent": "I would say first one is lexicographic try which stands for classic try where you have one symbol in a note.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "R3, which is actually a new version of RDX.",
                    "label": 0
                },
                {
                    "sent": "Try in red extra you merge with children on all nodes that have only one child.",
                    "label": 0
                },
                {
                    "sent": "And finally we have an H drive which stands for out say updated version of the birth dry in the burst dry.",
                    "label": 0
                },
                {
                    "sent": "You may have several keys in leaf nodes and they use a dedicated structure for leaves.",
                    "label": 1
                },
                {
                    "sent": "In this case, the original idea of using Linkedlist were replaced with hash tables.",
                    "label": 0
                },
                {
                    "sent": "Not simple hash tables, but cache conscious hash tables.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, now we can proceed to results.",
                    "label": 0
                },
                {
                    "sent": "I just want to say that we got around 200 graphs.",
                    "label": 0
                },
                {
                    "sent": "Of course I'm not going to show you all of them, I will just scheme for the main insides.",
                    "label": 0
                },
                {
                    "sent": "But if you're curious, please don't hesitate to Wales visit our website.",
                    "label": 1
                },
                {
                    "sent": "There you can find the code data sets, result logs and also graphs.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can proceed the results.",
                    "label": 0
                },
                {
                    "sent": "I will start with hash tables.",
                    "label": 1
                },
                {
                    "sent": "Here we can see.",
                    "label": 0
                },
                {
                    "sent": "In certain query profiles for stale, unordered map and the data set is the biggest one we have the keys in this case are sorted.",
                    "label": 0
                },
                {
                    "sent": "You can immediately support.",
                    "label": 0
                },
                {
                    "sent": "I hope I'm pointing right.",
                    "label": 0
                },
                {
                    "sent": "You can immediately spot those peaks.",
                    "label": 0
                },
                {
                    "sent": "Those pics are exactly actually timeouts.",
                    "label": 0
                },
                {
                    "sent": "Then the hash table is filled to a certain amount which is defined by the value of fill factor.",
                    "label": 0
                },
                {
                    "sent": "At this point it has to create another table which is twice as bigger and move.",
                    "label": 0
                },
                {
                    "sent": "Records to this new table, but as we know from the books, it's absolutely safe because the size of those peaks is.",
                    "label": 0
                },
                {
                    "sent": "Is proportional to the amount of cheap inserts that will be for it.",
                    "label": 0
                },
                {
                    "sent": "So in the end we have something what is called amortized constant time.",
                    "label": 0
                },
                {
                    "sent": "I hope you see I put as a red dashed line line, the average in their time, and taking into account that Y axis here is logarithmic, we have 10% difference from the bottom blue line, which is absolutely fine if we take a look at the query trend will see that it's slightly gross, but.",
                    "label": 0
                },
                {
                    "sent": "The increasing query time is mostly due to the fact that the data structure is getting bigger, so the amount of cache misses increases.",
                    "label": 0
                },
                {
                    "sent": "That's why even though this trend should be constant, we have.",
                    "label": 0
                },
                {
                    "sent": "Slow growing of time.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this data set was sorted and let's put on this graph the results for shuffle data set.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I hope you see those these trends perfectly coincide.",
                    "label": 0
                },
                {
                    "sent": "This shows the perfect amazing feature of hash tables.",
                    "label": 1
                },
                {
                    "sent": "They don't really care if your data set is sorted or not.",
                    "label": 0
                },
                {
                    "sent": "This is very important feature if you're processing big data sets which you cannot sort, or you processing RDF streams.",
                    "label": 0
                },
                {
                    "sent": "If I put here the result.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For our other hash tables and we will see that well in general their coincide, but there is sort of outlier.",
                    "label": 1
                },
                {
                    "sent": "Google sparse hash map.",
                    "label": 0
                },
                {
                    "sent": "It's in their time is bigger, but it's mostly due to the fact that a few factories hire it allows to save some memory, but increase the number of collisions.",
                    "label": 0
                },
                {
                    "sent": "And in general like in their time getting bigger.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see how hash tables behave on different data sets.",
                    "label": 0
                },
                {
                    "sent": "These are results for Google Dense hash map.",
                    "label": 1
                },
                {
                    "sent": "The fastest we have and I put here transfer all six data sets.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "The keys are sorted.",
                    "label": 0
                },
                {
                    "sent": "We again see that trends.",
                    "label": 0
                },
                {
                    "sent": "Coincide kind of coincide, and this is another good feature that hash tables don't really depends on.",
                    "label": 0
                },
                {
                    "sent": "The average year I length and type of the data set, that type of your eyes.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summarize a little bit about hash tables, then generally quite fast, they have constant insert and query time.",
                    "label": 1
                },
                {
                    "sent": "They perform.",
                    "label": 0
                },
                {
                    "sent": "This doesn't depend on keys order and data set type and another very important features that they're quite adjustable.",
                    "label": 1
                },
                {
                    "sent": "You can play with Phil Factor.",
                    "label": 0
                },
                {
                    "sent": "You can create your own hash function.",
                    "label": 0
                },
                {
                    "sent": "For example if you like processing your eyes, you can in the hash function you can hash on the last part of the string and drop the host name.",
                    "label": 0
                },
                {
                    "sent": "That will make the process even faster.",
                    "label": 0
                },
                {
                    "sent": "And hopefully I don't know I didn't test it by the way, and.",
                    "label": 0
                },
                {
                    "sent": "Yeah also you can pre allocate for example the size of the table.",
                    "label": 0
                },
                {
                    "sent": "If you know the size of the data set you will have so that would.",
                    "label": 0
                },
                {
                    "sent": "So at this point we can actually summarize and say, well, hash tables are perfect for working with strings and if you have that issue you should take hash tables.",
                    "label": 0
                },
                {
                    "sent": "But they have one important drawback.",
                    "label": 0
                },
                {
                    "sent": "They consume a lot of memory and I will show it later.",
                    "label": 0
                },
                {
                    "sent": "Now let's.",
                    "label": 0
                },
                {
                    "sent": "Proceed to the search trees.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, you can see that in the article and query profile for the biggest data set we have source, the keys are sorted.",
                    "label": 0
                },
                {
                    "sent": "I put as a red dash, light.",
                    "label": 0
                },
                {
                    "sent": "The result line is the results for Google Dance Cache map.",
                    "label": 0
                },
                {
                    "sent": "And you can immediately see two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, the trends are logarithmic.",
                    "label": 0
                },
                {
                    "sent": "This is what we know from books, and the 2nd is that their performance is.",
                    "label": 0
                },
                {
                    "sent": "Quite lower comparing to.",
                    "label": 0
                },
                {
                    "sent": "Comparing to hash table.",
                    "label": 0
                },
                {
                    "sent": "But that is not the main issue actually.",
                    "label": 0
                },
                {
                    "sent": "So what will happen if the if the keys are not sorted?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This shuffle data set sorted other set shuffle sorted so we see that the performance decreases dramatically.",
                    "label": 0
                },
                {
                    "sent": "And what is especially bad?",
                    "label": 0
                },
                {
                    "sent": "It's not about performance of insert time, it's also performance of query time.",
                    "label": 0
                },
                {
                    "sent": "That means that if you created your dictionary in a wrong way.",
                    "label": 0
                },
                {
                    "sent": "You will get an inferior very bad performance during query time during queries.",
                    "label": 0
                },
                {
                    "sent": "And yeah, that's very bad, obviously.",
                    "label": 0
                },
                {
                    "sent": "Let's see the performance on different data sets.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are results for STL map all data sets and they're sorted.",
                    "label": 0
                },
                {
                    "sent": "This trends perfectly reflect the order.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the data sets, if we sort them by every two right length.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's not surprising just because, well, if you insert the key into the.",
                    "label": 0
                },
                {
                    "sent": "Search tree or your query key.",
                    "label": 0
                },
                {
                    "sent": "You have to do this logarithmic amount of comparisons and when you compare two strings it's all about their length.",
                    "label": 0
                },
                {
                    "sent": "So in common, the longer you rise you have, the more time it will take for you to work with search trees.",
                    "label": 0
                },
                {
                    "sent": "Summer.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a little bit, so search trees in general have logarithmic complexity.",
                    "label": 0
                },
                {
                    "sent": "There, slower, slower than hash tables, their performance decrease dramatically if the keys are shuffled and their performance also depends on your eye length, so that makes the performance completely inferior comparing to hash tables.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is absolutely no reason to use search trees.",
                    "label": 0
                },
                {
                    "sent": "Well, except for the case, if you're talking bout B + 3 an on disk operations, but we're not talking about this case right now, OK?",
                    "label": 0
                },
                {
                    "sent": "Now let's proceed to tries.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, you can see the profile, insert, profile and query profile and red dotted line is a result of Google dense hash map.",
                    "label": 0
                },
                {
                    "sent": "The fastest so far and we can see that at least for new data structures, the performance is quite good.",
                    "label": 0
                },
                {
                    "sent": "And in terms of insert time, it's even faster.",
                    "label": 0
                },
                {
                    "sent": "But what will happen if the data set is shuffled?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, catastrophe and.",
                    "label": 0
                },
                {
                    "sent": "But at least this time in this list, this time we can say that it is most at least if you're talking about new data structures, we can say that it's mostly about insert time.",
                    "label": 0
                },
                {
                    "sent": "So query time doesn't degrade that much.",
                    "label": 0
                },
                {
                    "sent": "So which is already.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something if we let it take a look on the performance on different data sets I showed here average query and insert time is a function of every to right size.",
                    "label": 0
                },
                {
                    "sent": "And we see almost perfect lines, but except for the one case, it's DB pedia data set with lexicographic tree.",
                    "label": 1
                },
                {
                    "sent": "And we can guess why it happens.",
                    "label": 0
                },
                {
                    "sent": "As I said, the pager data set is quite complex and for that reason try stars branching very early.",
                    "label": 0
                },
                {
                    "sent": "Those not notes that are closer to the route.",
                    "label": 0
                },
                {
                    "sent": "They have a lot of children but then you have long tails without any branching and since lexicographic trees as I said it's a classical try.",
                    "label": 0
                },
                {
                    "sent": "It has to create a separate node.",
                    "label": 0
                },
                {
                    "sent": "Or for each letter?",
                    "label": 0
                },
                {
                    "sent": "Almost for each key which it insert, it has to create enormous amount of nodes.",
                    "label": 0
                },
                {
                    "sent": "Something similar happens when you do query, but in this case we just have to jump through the memory until we reach the leaf.",
                    "label": 0
                },
                {
                    "sent": "You have to do a lot of jumps.",
                    "label": 0
                },
                {
                    "sent": "Basically that makes performance or flexographic tree on this data set in bad comparing to other data sets and other data structures.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To summarize about tries so they in general quite fast if not talking bout primordial classic, try but the insert time degrades when the data set is shuffled and their performance slightly depends on the URL length, so that would make these tries poor candidate comparing 2 hash tables, but they have very important adventure advantage.",
                    "label": 0
                },
                {
                    "sent": "They consume much less memory.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, I can show you total memory consumption.",
                    "label": 1
                },
                {
                    "sent": "This is total memory consumption for all data sets and all data structures.",
                    "label": 0
                },
                {
                    "sent": "And here you can see that hash tables and search trees 'cause you consumes up to three times more memory than the original data set, while four tries those numbers, numbers are from zero point 5 to one.",
                    "label": 0
                },
                {
                    "sent": "1.5 and two.",
                    "label": 0
                },
                {
                    "sent": "So in case of Hat H Drive, we actually have data set compression which is super cool if you have a lot of data sets.",
                    "label": 0
                },
                {
                    "sent": "If you have a lot of dictionaries and you have limited amount of memory now.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "So first of all, for in memory operations there is absolutely no need to use search trees.",
                    "label": 0
                },
                {
                    "sent": "In case if you're processing data sets which you cannot sort or your assistant processing streams is Vera TOS hash tables.",
                    "label": 0
                },
                {
                    "sent": "If you have certain amount of static dictionaries which you can keep sort it, or you upload them before the system starts working, you can use tries so that you save some memory.",
                    "label": 0
                },
                {
                    "sent": "Then it's good to embrace new data structures like H dry or our tree.",
                    "label": 0
                },
                {
                    "sent": "And finally, I want to say that there is still room for research.",
                    "label": 1
                },
                {
                    "sent": "Because as we have seen, none of the dot structures were actually able to leverage the highly regular structure of your eyes.",
                    "label": 0
                },
                {
                    "sent": "And of course you have.",
                    "label": 0
                },
                {
                    "sent": "If you know your data set before head, you can do this optimizations by yourself.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're working with that, Oblu can account again.",
                    "label": 0
                },
                {
                    "sent": "Cut the hostnames and keep just dressed in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "But if you don't know your data set, if your data set is skater genius, you probably would like to have something more universal adaptive, and I would even say intelligent.",
                    "label": 0
                },
                {
                    "sent": "That's it, thank.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        }
    }
}