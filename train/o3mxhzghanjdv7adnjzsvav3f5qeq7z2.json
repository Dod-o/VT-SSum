{
    "id": "o3mxhzghanjdv7adnjzsvav3f5qeq7z2",
    "title": "Amilcare",
    "info": {
        "author": [
            "Fabio Ciravegna, Department of Computer Science, University of Sheffield"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2005",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/pcw05_ciravegna_a/",
    "segmentation": [
        [
            "Facing.",
            "The other thing is that if you think that the organizers of a challenge have an advantage in running their systems is wrong.",
            "We actually were so busy busy presenting the in preparing everything that we actually while the other people worked on their system for two months.",
            "We actually worked on the system for one week.",
            "In the end when we were there, the whole task was ready.",
            "So to some extent this is just to say that, well, yeah, it's embarrassing when you're ready, organizer and you get it."
        ],
        [
            "Yes, results, but this is really what what happened.",
            "So how many Coryza distributed 3 for research?",
            "If you want to download it from the web, you sent me an email, I'll send you the path.",
            "It is mainly thought as a support to annotation.",
            "This is very important because.",
            "The idea is that when you annotate a set of documents, So what happens is that if you really annotate manually all the documents, what happens is that you will annotate many, many cases that are already seen.",
            "So the idea is that I mean there is a system that works in the background of an annotation system and which suggests every time when it has learned a bit every time."
        ],
        [
            "The document that is presented to the user is pre annotated by the annotation system, so most of the annotations are already there and when the system the user corrects the system retrain so it works in the background.",
            "This has some implications in the kind of system it is.",
            "The kind of approach it uses and we will see it actually this year."
        ],
        [
            "Moment as support to annotation in a number of 3rd party system or our systems.",
            "So it is it is fairly used at the moment.",
            "It's not a commercial system, but the requirements are that it must have high precision because in this setting you don't want to annoy the user with the wrong annotations here.",
            "This is really something that annoys the user if the system starts suggesting something is wrong, so it's high procedure.",
            "It tends to be high precision, lower recall, high precision.",
            "And also for using of for reason of application again assistant that learns in the background is assistant tests."
        ],
        [
            "Q So it's intrusive and if it you don't provide a steep learning curve or something like that in terms of user is again in using the system, no one is going to use it.",
            "So it's really something that we have focused a lot in in the development.",
            "In trying to have as soon as possible high precision, but also a lot of focus on high gain from the user point of view.",
            "And it is also used as support to unsupervised large scale annotation for the semantic web."
        ],
        [
            "In another system.",
            "So this is."
        ],
        [
            "Why I said well, I mean could it was a system that we had off the shelf.",
            "It is based on rule induction and it is a covering algorithm and the idea is that covering algorithm expect you to know that you start from from an instance.",
            "You have a pool of instances, you start from one instance.",
            "You induce your rules and then you remove from the pool all the instances that are covered by the current rules that you have.",
            "You know the AP squared algorithm.",
            "It is actually a slight variation.",
            "This is a new version, the standard LP squared algorithm was based on generalization, so we started with a very complex rule and then started removing conditions.",
            "This is the other way around.",
            "So this is the."
        ],
        [
            "But this is how we represent an initial instance.",
            "These are the features that we consider.",
            "In general, this is again, the examples are on the similar announcement.",
            "So we use a number of features that are not.",
            "In this case the ones that we use for the challenge, and there is a.",
            "The place where you put this tag there.",
            "So what do you want?",
            "This is the initial instance.",
            "How do we represent the text?",
            "And this is the kind of rule that we want to do something for example.",
            "So for example, in this case there is at followed by a digit and then a time ID.",
            "That is something like ampm and this is where you put the start tag.",
            "This is similar to what Aidan said we represent for when you have a piece of information, we represent the two tags, the limiting the information independently.",
            "So This is why the start tag.",
            "The time will start, for example.",
            "And."
        ],
        [
            "The way in which we develop the rules is that we start from an empty room that is just the tagging.",
            "This is of course high Rico.",
            "Very, very low precision actually doesn't apply.",
            "We progressively generate all the possible combinations.",
            "Unconditional features in an efficient ways.",
            "We will see for the window within the window of W words the typical is 5 case.",
            "We test the rules on the training corpus using the score that I will mention now and we select the best K rules among the developed one.",
            "A note here K is actually unbounded in every experiment.",
            "If we don't put a boundary to K, the system performs very well and we will see that it generates loss and loss of rules.",
            "The accepted rules cover the training site, but again we will see how they are covered.",
            "First of all, this score."
        ],
        [
            "When the roll is is induced, we apply to the training corpus.",
            "We have the number of correct matches plus an epsilon not to over zero and divided by the number of matches.",
            "So how good is the accuracy of the precision of the role of like?",
            "But we also associate the number of matches and represents somehow the recall.",
            "We have two thresholds for selecting the rules.",
            "One is the minimum acceptable accuracy.",
            "That means that whatever is above of that is accepted is retained by the system.",
            "But they are not necessarily used for covering.",
            "There is another threshold above that says whatever is above will be actually used for covering, so for removing instances from the from the training corpus.",
            "This is because we have notice that typically some rules are very good at extracting information, but you can generate better rules if you consider more instances, so you reduce the amount of coverage that you have.",
            "This is the way in which we generate."
        ],
        [
            "The rules this is the search space, so you start with the empty roll with no condition that matches all the corpus.",
            "Again, we use both as positive examples.",
            "The tags is as negative example, everything else.",
            "And we start generating rules with one condition distributed over the whole window.",
            "In this case it's 2 words to the left and to the right.",
            "We generally use five and five.",
            "At this point, we start generating the combination by simply taking the intersection of the matches, so it becomes very quickly because even if potentially they so space complexity is very large, what actually happens is that you compute this in a very efficient way by just using intersection of vectors of integers, and this is a continuation until."
        ],
        [
            "Get the full rule.",
            "This is just for in this case for words, you have to think that there are all the features, of course.",
            "But of course this is not enough, so there is a strong pruning that is good."
        ],
        [
            "On in the in this induction space for efficiency reasons, but also to reduce overfitting because it's very easy to overfit the training corpus when you have time words or window.",
            "Brilliant Answer, Remove rules that have features that we don't like during the Diva Lady induction.",
            "So it means that we stop some of the paths in the search space.",
            "And also we remove rules that have identical coverage if two combination of features produce exactly the same coverage, we remove one of the two and we select which one is best for us.",
            "They must be identical, not subsumed, because we have again, we have notice this very mentally, that this is the best way of doing it.",
            "It also proves that tend to have very limited number of matches are not specialized further of course, so this.",
            "In the end, we generate for every instance loss in lots of rules actually, and this is an important feature as well.",
            "We will see the relation between redundancy and system performance is now the.",
            "Again, in principle, the complexity is very large.",
            "In practice, the system is very efficient.",
            "Especially because consider that it is working in the background for annotation.",
            "You can't wait for 20 minutes for receiving suggestions.",
            "You really have suggestions at runtime."
        ],
        [
            "But the real interesting part of America is not the part that I've mentioned, but the way in which the rules are actually the type of rules that are developed.",
            "What I mentioned so far is the way in which we.",
            "Developed the first level of rules.",
            "Again, we have something very similar to what Aidan said.",
            "We have an initial set of rules that will put some tags and then we have another set of rules that we use those tags to add other missing tags.",
            "And then there are some correction rules in the Pascal challenge.",
            "They don't count for anything, so we don't mention that in other other tasks they are actually relevant.",
            "So let."
        ],
        [
            "About the complex rules, the problem is that the best rules.",
            "So the first type of rules, well, we tend to apply very strict conditions and they have high precision but very low recall.",
            "If you apply just the the best rules on the on the Pascal challenge, the F measure is 20 is the worst system in the hole in the whole challenge is really the whole works well.",
            "So what we need to do is to order a code.",
            "And the solution is to take some of the low precision high recall rules that we have discarded as a.",
            "Vessels, and we use the surrounding tag to constrain the rule application, so again, it's very similar to Aidan.",
            "What Aiden did does, but we do it in a different way.",
            "First of all, we don't use just the start to understand the end, but also the following tag to the previous tag.",
            "So in this way we represent the fact that there can be a dependency sometimes between timing speaker.",
            "In this case this is for sure very relevant in the in the Pascal challenge when you have the typical structure is date of submission date of four answer to the to the to the orders and date of camera ready copy.",
            "This is the way we should model this guy.",
            "This is just.",
            "Impasse.",
            "This is an example of a."
        ],
        [
            "Very bad rule in general.",
            "Whatever every time you have a digit followed by PM.",
            "This is the end of an S time.",
            "This is low precision because it applies also for anytime, for example, for every time expression actually.",
            "But if you use it in the context of an S time, this is very good.",
            "So, but what happens is what actually chose.",
            "Ask to trade, and that in a window on the right hand side there are two potential places where it applies.",
            "So if you can see that both again this rule has 50% accuracy and you don't select it.",
            "And we we we look at the maximum distance between two tags that we have seen on the on the test on the training corpus.",
            "So if the slot in this case is as a maximum length of four, we just consider 4 for tag for words.",
            "But actually what we do we try to."
        ],
        [
            "Learning time, we select the rules so we select this.",
            "Do we consider this to buy my trying to minimize the number of matches then sorry the length of the information.",
            "So what we say is, it's true there are two, but in computing the accuracy of the rule, we just consider this one because it is the closest one and we see how the rule performs.",
            "And the same applies in this case when you have the location."
        ],
        [
            "With respect to S time, you take the fathers one, so again you try to minimize this length.",
            "But you do that at selection time.",
            "You don't do it well.",
            "You do also test time is very important because you tune the rules to to try to minimize this thing.",
            "And this is the good thing and a bad thing in the approach as we will see.",
            "And an Apple."
        ],
        [
            "Nation time you first we first of all apply the best rules, so we had some tags and then we start a loop.",
            "In which context rules will add some tags and the same tax we used what addnew tax?",
            "These are the results compare."
        ],
        [
            "With whatever other system performance better you can see, the green ones are where am incorrect, performs very well and the yellow one is where it performs very badly.",
            "So look at that for example, this this one.",
            "I mean, I guess this is the acronym for conference gets 91% while the best other system performs at 45.",
            "This is definitely very good.",
            "But this one is the opposite, where the best system goes to 066.",
            "I think it's the last system.",
            "I mean, I guess half of it.",
            "So there are mixed results.",
            "Overall this is it is better than the other system, but there are some worry places and look.",
            "If you look at the."
        ],
        [
            "Love announcements that Aiden presented.",
            "These are new results with respect to the ones that he mentioned.",
            "He performs fairly generally better, but there are 1, two and three slots and we should perform specially.",
            "Now, on average I don't know.",
            "I haven't seen the global feed F major, but it tends to be more or less the same butter.",
            "On some stuff that goes down and the same applies to the CMU seminar now."
        ],
        [
            "Well, for location we have as an 8% less.",
            "While in the other it is better, so this is a characteristic of the system actually.",
            "This is the effect of."
        ],
        [
            "Actual rules you see is quite dramatic.",
            "This is without contextual rules.",
            "This is with context rules, so it's really important not just in this.",
            "In this task, on every task, representing both recovering recall and actually using the dependency among tags.",
            "The interesting thing is that the first part of the algorithm is very bad, so one question is what happens if you use one of the other algorithms that will perform better.",
            "At this point, this is the dependency between."
        ],
        [
            "Seat number of rules generated and accuracy, so this is the F measure for every slot and these are the number of rules, so you will see that when you have a limited number of slots and a limited number unlimited F measure.",
            "Actually it's because America is not able to develop rules, and in particular it is not able to develop complex models."
        ],
        [
            "Learning curve as CMS was designed as companion to annotation interfaces high precision.",
            "This is the precision you see that from the."
        ],
        [
            "The first block of 40 tax immediately goes over 80% average, but if you look at recall, of course it goes."
        ],
        [
            "Slowly and again.",
            "This is how it is meant to perform, and this is how it performs on every task that we have seen."
        ],
        [
            "I'm.",
            "Concerning active learning, again, the approach that we used was very naive and it was meant to be.",
            "It is again due to the application.",
            "Or in the in terms of annotation, what we want to do.",
            "We want to avoid when people are not a documents that they imitate documents that are very similar one to each other because it is not important for the learner to have two identical tags.",
            "So what we use is a simple measure we take.",
            "We take the rules that we have developed.",
            "We applied to the UN annotated corpus.",
            "We look at the difference between the expected number of tags, expected types of tags in the in the training corpus, and the one that we are able to extract.",
            "We select the documents in which we don't get good results.",
            "We skip the ones where we don't get anything because they could be relevant and we select them.",
            "So it's it's very it's very naive in this sense and it works.",
            "More or less."
        ],
        [
            "Well, you have games here.",
            "This is the.",
            "One the train, the effect when you use the random corpus.",
            "This is when you use active learning.",
            "There are two cases in which it is reduced.",
            "Now there is something to say here.",
            "I don't consider this a good result, but of course you don't know how.",
            "The random corpus is, so it is possible that even the random corpus is a good one, or it is a bad one, but I don't think this is a good result in general.",
            "What we have is that we tend to have better performance on other tasks on this we didn't.",
            "At."
        ],
        [
            "So the conclusion, I think that the the only thing I can say that remain current resting is for the contextual rules.",
            "Because they really represent two things they recovery, called in representing the dependency among the tags that are the two things that I didn't mention as important.",
            "Actually, the initial tag is very, very bad, so it's 20% is not compatible to any other system.",
            "But of course it's.",
            "It's also because we want to have high precision, because if you use contextual rules on imprecise tax, it will simply diverge.",
            "So wanted to some extent.",
            "In many experiments, we can show that if we renew all the tie, all the rules for one tag and we just use contextual rules, you can reach up to 90% precision.",
            "Really sorry, recall in task where you generally receive 90%, so it means that you recover whatever you have lost with the best at best dagger annotations.",
            "So I think again this is this is good result.",
            "An interesting feature of the system.",
            "I said this represents dependency between tag.",
            "For example in conference naming conference acronym, the concatenation is extremely important.",
            "The acronym per say.",
            "Is just an acronym, a date per say, is just a date.",
            "In we think that in the implicit relation, of course they are implicit.",
            "You don't model explicitly the way in which they are dependent, but if you don't represent the fact that somehow they are concatenated.",
            "It is you lose something, you lose a piece of information you are not able to represent it.",
            "In Wisc, Steven Sutherland try to do that by actually model modeling the path, the complete path, but that was that was something that we had in mind when when we designed this, But the problem is that when you do, when you define very long path, of course you have data sparsity because you will.",
            "You will learn different rules every time for the same rule for the same pattern.",
            "So what we do, we use pairs of tags and one is the start tag, the other one is the end tag.",
            "One is the end tag.",
            "And the start of the following time.",
            "So we represent in a very implicit way, and it is possible to show that you're you need less examples to do that.",
            "And but the conventional rules are a solution that not always work, so there I think I'm very worried by the fact that on some specific slots we tend to perform much less than the other systems.",
            "In general this is on every time on every task.",
            "So what we have to do as a future work is to try to understand why.",
            "We dropped so dramatically in some cases and not in others.",
            "So what is that doesn't work?",
            "It seems like something doesn't start like an engine apart of the engine that doesn't work, so I think this is our our future work and the other future work is to take the contextual rules and combine them using another system that the level of the initial target rules performance much better to see.",
            "Maybe we don't get any gain, maybe we diverge or whatever, but this is again.",
            "Something to test?",
            "OK, I think this is my.",
            "Right, you use your professional rules not only on the orphans.",
            "You're more than doubles, so if you're using that overall now, just unpack the car inspected to be near so you know from the training corpus that show the staff will perceive the end time, so you want the office in this case, but also the fact that from the training hope you have served, for example, the start time is followed by a name time, they're choose different, so use this information here and use the window.",
            "You know that the maximum distance that you've seen when they are adjacent is that OK?",
            "But you don't just use them to fix the orphans, no, no.",
            "This is why we can represent the concatenations and the second question was so you have this kind of shortest match bias.",
            "No.",
            "Did you observe any correlation?",
            "Weird between these sort of bias, either for shortest match and the longest match, and the performance on the difference wells.",
            "So is it the case that I need to reduce does worse where wherever, whenever you might prefer a longest match biased, and I think this is?",
            "I haven't checked, but this is my explanation for rotation that tends to be very long and very regular and.",
            "Wyndham"
        ],
        [
            "For me it again to be very long and very regular.",
            "I think this is I mentioned some pointed.",
            "I think this is a good thing and a bad thing at the same time we probably what we should do is to develop two types of contextual rules.",
            "One that tries to maximize and the other one that tries to minimize.",
            "But we haven't done.",
            "Select do you think that the from what do we see that that's in some cases good performance was being really good and but there should.",
            "Also you were much lower and was wondering whether.",
            "I remember your equal is paid is independent on the contextual rules and I was wondering if this could be attributed to.",
            "But some slots have more redundancy in their language, so that you can extract contextual rules more efficient like they are presented in more than once, maybe in calls for papers, or they have like lesser variety of of of context of the appearing, so they actually system to extract the contextual rules better than other without appearing more this irregular.",
            "Is it the context in the sense of linguistic complex or the variety of the linguistic context or the variety of slots?",
            "Combination of slots?",
            "Yeah, I mean I was thinking of linguistics.",
            "Well, we haven't checked the best possible before.",
            "Sure, there is one thing that is very difficult when user fix it window that is 2 typically recognize the end time because the difference between a start tag and end time or the camera ready copy data and the submission date is given by the left context.",
            "And if it is a bit long in your window you don't see the context from their items.",
            "So this is where you tend to use the.",
            "Philosophical, semantic point.",
            "You can describe any computation by rules.",
            "So if you take the output from any machine.",
            "Describing his rules, so I guess Wonder Woman duction is the right kind of terminology as well method, because most naturally think of rules.",
            "Something that you could easily rank in if then form was smaller.",
            "Looks more like.",
            "Although you can describe anything, yes, yeah, I think this is a good point.",
            "There's another language person, probably I've used the wrong terminology, yeah.",
            "Think about.",
            "As additional features.",
            "And this means that we can go over the.",
            "Fix windows because you could have.",
            "Save it.",
            "As a feature.",
            "The longest.",
            "Teacher saying before I had this time.",
            "Was three 510 words for.",
            "Yeah.",
            "OK, yeah, that's a good way.",
            "Yes, I think."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Facing.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that if you think that the organizers of a challenge have an advantage in running their systems is wrong.",
                    "label": 0
                },
                {
                    "sent": "We actually were so busy busy presenting the in preparing everything that we actually while the other people worked on their system for two months.",
                    "label": 0
                },
                {
                    "sent": "We actually worked on the system for one week.",
                    "label": 0
                },
                {
                    "sent": "In the end when we were there, the whole task was ready.",
                    "label": 0
                },
                {
                    "sent": "So to some extent this is just to say that, well, yeah, it's embarrassing when you're ready, organizer and you get it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, results, but this is really what what happened.",
                    "label": 0
                },
                {
                    "sent": "So how many Coryza distributed 3 for research?",
                    "label": 1
                },
                {
                    "sent": "If you want to download it from the web, you sent me an email, I'll send you the path.",
                    "label": 0
                },
                {
                    "sent": "It is mainly thought as a support to annotation.",
                    "label": 1
                },
                {
                    "sent": "This is very important because.",
                    "label": 0
                },
                {
                    "sent": "The idea is that when you annotate a set of documents, So what happens is that if you really annotate manually all the documents, what happens is that you will annotate many, many cases that are already seen.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that I mean there is a system that works in the background of an annotation system and which suggests every time when it has learned a bit every time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The document that is presented to the user is pre annotated by the annotation system, so most of the annotations are already there and when the system the user corrects the system retrain so it works in the background.",
                    "label": 0
                },
                {
                    "sent": "This has some implications in the kind of system it is.",
                    "label": 0
                },
                {
                    "sent": "The kind of approach it uses and we will see it actually this year.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moment as support to annotation in a number of 3rd party system or our systems.",
                    "label": 1
                },
                {
                    "sent": "So it is it is fairly used at the moment.",
                    "label": 0
                },
                {
                    "sent": "It's not a commercial system, but the requirements are that it must have high precision because in this setting you don't want to annoy the user with the wrong annotations here.",
                    "label": 0
                },
                {
                    "sent": "This is really something that annoys the user if the system starts suggesting something is wrong, so it's high procedure.",
                    "label": 1
                },
                {
                    "sent": "It tends to be high precision, lower recall, high precision.",
                    "label": 0
                },
                {
                    "sent": "And also for using of for reason of application again assistant that learns in the background is assistant tests.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Q So it's intrusive and if it you don't provide a steep learning curve or something like that in terms of user is again in using the system, no one is going to use it.",
                    "label": 0
                },
                {
                    "sent": "So it's really something that we have focused a lot in in the development.",
                    "label": 0
                },
                {
                    "sent": "In trying to have as soon as possible high precision, but also a lot of focus on high gain from the user point of view.",
                    "label": 0
                },
                {
                    "sent": "And it is also used as support to unsupervised large scale annotation for the semantic web.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In another system.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why I said well, I mean could it was a system that we had off the shelf.",
                    "label": 0
                },
                {
                    "sent": "It is based on rule induction and it is a covering algorithm and the idea is that covering algorithm expect you to know that you start from from an instance.",
                    "label": 0
                },
                {
                    "sent": "You have a pool of instances, you start from one instance.",
                    "label": 0
                },
                {
                    "sent": "You induce your rules and then you remove from the pool all the instances that are covered by the current rules that you have.",
                    "label": 0
                },
                {
                    "sent": "You know the AP squared algorithm.",
                    "label": 0
                },
                {
                    "sent": "It is actually a slight variation.",
                    "label": 0
                },
                {
                    "sent": "This is a new version, the standard LP squared algorithm was based on generalization, so we started with a very complex rule and then started removing conditions.",
                    "label": 0
                },
                {
                    "sent": "This is the other way around.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is how we represent an initial instance.",
                    "label": 0
                },
                {
                    "sent": "These are the features that we consider.",
                    "label": 0
                },
                {
                    "sent": "In general, this is again, the examples are on the similar announcement.",
                    "label": 0
                },
                {
                    "sent": "So we use a number of features that are not.",
                    "label": 0
                },
                {
                    "sent": "In this case the ones that we use for the challenge, and there is a.",
                    "label": 0
                },
                {
                    "sent": "The place where you put this tag there.",
                    "label": 0
                },
                {
                    "sent": "So what do you want?",
                    "label": 0
                },
                {
                    "sent": "This is the initial instance.",
                    "label": 0
                },
                {
                    "sent": "How do we represent the text?",
                    "label": 0
                },
                {
                    "sent": "And this is the kind of rule that we want to do something for example.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this case there is at followed by a digit and then a time ID.",
                    "label": 0
                },
                {
                    "sent": "That is something like ampm and this is where you put the start tag.",
                    "label": 0
                },
                {
                    "sent": "This is similar to what Aidan said we represent for when you have a piece of information, we represent the two tags, the limiting the information independently.",
                    "label": 0
                },
                {
                    "sent": "So This is why the start tag.",
                    "label": 0
                },
                {
                    "sent": "The time will start, for example.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way in which we develop the rules is that we start from an empty room that is just the tagging.",
                    "label": 0
                },
                {
                    "sent": "This is of course high Rico.",
                    "label": 0
                },
                {
                    "sent": "Very, very low precision actually doesn't apply.",
                    "label": 0
                },
                {
                    "sent": "We progressively generate all the possible combinations.",
                    "label": 0
                },
                {
                    "sent": "Unconditional features in an efficient ways.",
                    "label": 0
                },
                {
                    "sent": "We will see for the window within the window of W words the typical is 5 case.",
                    "label": 0
                },
                {
                    "sent": "We test the rules on the training corpus using the score that I will mention now and we select the best K rules among the developed one.",
                    "label": 0
                },
                {
                    "sent": "A note here K is actually unbounded in every experiment.",
                    "label": 0
                },
                {
                    "sent": "If we don't put a boundary to K, the system performs very well and we will see that it generates loss and loss of rules.",
                    "label": 0
                },
                {
                    "sent": "The accepted rules cover the training site, but again we will see how they are covered.",
                    "label": 0
                },
                {
                    "sent": "First of all, this score.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When the roll is is induced, we apply to the training corpus.",
                    "label": 0
                },
                {
                    "sent": "We have the number of correct matches plus an epsilon not to over zero and divided by the number of matches.",
                    "label": 0
                },
                {
                    "sent": "So how good is the accuracy of the precision of the role of like?",
                    "label": 0
                },
                {
                    "sent": "But we also associate the number of matches and represents somehow the recall.",
                    "label": 0
                },
                {
                    "sent": "We have two thresholds for selecting the rules.",
                    "label": 0
                },
                {
                    "sent": "One is the minimum acceptable accuracy.",
                    "label": 0
                },
                {
                    "sent": "That means that whatever is above of that is accepted is retained by the system.",
                    "label": 0
                },
                {
                    "sent": "But they are not necessarily used for covering.",
                    "label": 0
                },
                {
                    "sent": "There is another threshold above that says whatever is above will be actually used for covering, so for removing instances from the from the training corpus.",
                    "label": 0
                },
                {
                    "sent": "This is because we have notice that typically some rules are very good at extracting information, but you can generate better rules if you consider more instances, so you reduce the amount of coverage that you have.",
                    "label": 0
                },
                {
                    "sent": "This is the way in which we generate.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The rules this is the search space, so you start with the empty roll with no condition that matches all the corpus.",
                    "label": 0
                },
                {
                    "sent": "Again, we use both as positive examples.",
                    "label": 0
                },
                {
                    "sent": "The tags is as negative example, everything else.",
                    "label": 0
                },
                {
                    "sent": "And we start generating rules with one condition distributed over the whole window.",
                    "label": 0
                },
                {
                    "sent": "In this case it's 2 words to the left and to the right.",
                    "label": 0
                },
                {
                    "sent": "We generally use five and five.",
                    "label": 0
                },
                {
                    "sent": "At this point, we start generating the combination by simply taking the intersection of the matches, so it becomes very quickly because even if potentially they so space complexity is very large, what actually happens is that you compute this in a very efficient way by just using intersection of vectors of integers, and this is a continuation until.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get the full rule.",
                    "label": 0
                },
                {
                    "sent": "This is just for in this case for words, you have to think that there are all the features, of course.",
                    "label": 0
                },
                {
                    "sent": "But of course this is not enough, so there is a strong pruning that is good.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On in the in this induction space for efficiency reasons, but also to reduce overfitting because it's very easy to overfit the training corpus when you have time words or window.",
                    "label": 0
                },
                {
                    "sent": "Brilliant Answer, Remove rules that have features that we don't like during the Diva Lady induction.",
                    "label": 0
                },
                {
                    "sent": "So it means that we stop some of the paths in the search space.",
                    "label": 0
                },
                {
                    "sent": "And also we remove rules that have identical coverage if two combination of features produce exactly the same coverage, we remove one of the two and we select which one is best for us.",
                    "label": 0
                },
                {
                    "sent": "They must be identical, not subsumed, because we have again, we have notice this very mentally, that this is the best way of doing it.",
                    "label": 0
                },
                {
                    "sent": "It also proves that tend to have very limited number of matches are not specialized further of course, so this.",
                    "label": 0
                },
                {
                    "sent": "In the end, we generate for every instance loss in lots of rules actually, and this is an important feature as well.",
                    "label": 1
                },
                {
                    "sent": "We will see the relation between redundancy and system performance is now the.",
                    "label": 0
                },
                {
                    "sent": "Again, in principle, the complexity is very large.",
                    "label": 0
                },
                {
                    "sent": "In practice, the system is very efficient.",
                    "label": 0
                },
                {
                    "sent": "Especially because consider that it is working in the background for annotation.",
                    "label": 0
                },
                {
                    "sent": "You can't wait for 20 minutes for receiving suggestions.",
                    "label": 0
                },
                {
                    "sent": "You really have suggestions at runtime.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the real interesting part of America is not the part that I've mentioned, but the way in which the rules are actually the type of rules that are developed.",
                    "label": 0
                },
                {
                    "sent": "What I mentioned so far is the way in which we.",
                    "label": 0
                },
                {
                    "sent": "Developed the first level of rules.",
                    "label": 0
                },
                {
                    "sent": "Again, we have something very similar to what Aidan said.",
                    "label": 0
                },
                {
                    "sent": "We have an initial set of rules that will put some tags and then we have another set of rules that we use those tags to add other missing tags.",
                    "label": 0
                },
                {
                    "sent": "And then there are some correction rules in the Pascal challenge.",
                    "label": 0
                },
                {
                    "sent": "They don't count for anything, so we don't mention that in other other tasks they are actually relevant.",
                    "label": 0
                },
                {
                    "sent": "So let.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the complex rules, the problem is that the best rules.",
                    "label": 0
                },
                {
                    "sent": "So the first type of rules, well, we tend to apply very strict conditions and they have high precision but very low recall.",
                    "label": 0
                },
                {
                    "sent": "If you apply just the the best rules on the on the Pascal challenge, the F measure is 20 is the worst system in the hole in the whole challenge is really the whole works well.",
                    "label": 0
                },
                {
                    "sent": "So what we need to do is to order a code.",
                    "label": 0
                },
                {
                    "sent": "And the solution is to take some of the low precision high recall rules that we have discarded as a.",
                    "label": 0
                },
                {
                    "sent": "Vessels, and we use the surrounding tag to constrain the rule application, so again, it's very similar to Aidan.",
                    "label": 0
                },
                {
                    "sent": "What Aiden did does, but we do it in a different way.",
                    "label": 0
                },
                {
                    "sent": "First of all, we don't use just the start to understand the end, but also the following tag to the previous tag.",
                    "label": 0
                },
                {
                    "sent": "So in this way we represent the fact that there can be a dependency sometimes between timing speaker.",
                    "label": 0
                },
                {
                    "sent": "In this case this is for sure very relevant in the in the Pascal challenge when you have the typical structure is date of submission date of four answer to the to the to the orders and date of camera ready copy.",
                    "label": 0
                },
                {
                    "sent": "This is the way we should model this guy.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "Impasse.",
                    "label": 0
                },
                {
                    "sent": "This is an example of a.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very bad rule in general.",
                    "label": 0
                },
                {
                    "sent": "Whatever every time you have a digit followed by PM.",
                    "label": 0
                },
                {
                    "sent": "This is the end of an S time.",
                    "label": 0
                },
                {
                    "sent": "This is low precision because it applies also for anytime, for example, for every time expression actually.",
                    "label": 0
                },
                {
                    "sent": "But if you use it in the context of an S time, this is very good.",
                    "label": 0
                },
                {
                    "sent": "So, but what happens is what actually chose.",
                    "label": 0
                },
                {
                    "sent": "Ask to trade, and that in a window on the right hand side there are two potential places where it applies.",
                    "label": 0
                },
                {
                    "sent": "So if you can see that both again this rule has 50% accuracy and you don't select it.",
                    "label": 0
                },
                {
                    "sent": "And we we we look at the maximum distance between two tags that we have seen on the on the test on the training corpus.",
                    "label": 0
                },
                {
                    "sent": "So if the slot in this case is as a maximum length of four, we just consider 4 for tag for words.",
                    "label": 0
                },
                {
                    "sent": "But actually what we do we try to.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning time, we select the rules so we select this.",
                    "label": 0
                },
                {
                    "sent": "Do we consider this to buy my trying to minimize the number of matches then sorry the length of the information.",
                    "label": 0
                },
                {
                    "sent": "So what we say is, it's true there are two, but in computing the accuracy of the rule, we just consider this one because it is the closest one and we see how the rule performs.",
                    "label": 0
                },
                {
                    "sent": "And the same applies in this case when you have the location.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With respect to S time, you take the fathers one, so again you try to minimize this length.",
                    "label": 0
                },
                {
                    "sent": "But you do that at selection time.",
                    "label": 0
                },
                {
                    "sent": "You don't do it well.",
                    "label": 0
                },
                {
                    "sent": "You do also test time is very important because you tune the rules to to try to minimize this thing.",
                    "label": 0
                },
                {
                    "sent": "And this is the good thing and a bad thing in the approach as we will see.",
                    "label": 0
                },
                {
                    "sent": "And an Apple.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation time you first we first of all apply the best rules, so we had some tags and then we start a loop.",
                    "label": 0
                },
                {
                    "sent": "In which context rules will add some tags and the same tax we used what addnew tax?",
                    "label": 0
                },
                {
                    "sent": "These are the results compare.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With whatever other system performance better you can see, the green ones are where am incorrect, performs very well and the yellow one is where it performs very badly.",
                    "label": 0
                },
                {
                    "sent": "So look at that for example, this this one.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess this is the acronym for conference gets 91% while the best other system performs at 45.",
                    "label": 0
                },
                {
                    "sent": "This is definitely very good.",
                    "label": 0
                },
                {
                    "sent": "But this one is the opposite, where the best system goes to 066.",
                    "label": 0
                },
                {
                    "sent": "I think it's the last system.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess half of it.",
                    "label": 0
                },
                {
                    "sent": "So there are mixed results.",
                    "label": 0
                },
                {
                    "sent": "Overall this is it is better than the other system, but there are some worry places and look.",
                    "label": 0
                },
                {
                    "sent": "If you look at the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love announcements that Aiden presented.",
                    "label": 0
                },
                {
                    "sent": "These are new results with respect to the ones that he mentioned.",
                    "label": 0
                },
                {
                    "sent": "He performs fairly generally better, but there are 1, two and three slots and we should perform specially.",
                    "label": 0
                },
                {
                    "sent": "Now, on average I don't know.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen the global feed F major, but it tends to be more or less the same butter.",
                    "label": 0
                },
                {
                    "sent": "On some stuff that goes down and the same applies to the CMU seminar now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, for location we have as an 8% less.",
                    "label": 0
                },
                {
                    "sent": "While in the other it is better, so this is a characteristic of the system actually.",
                    "label": 0
                },
                {
                    "sent": "This is the effect of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actual rules you see is quite dramatic.",
                    "label": 0
                },
                {
                    "sent": "This is without contextual rules.",
                    "label": 0
                },
                {
                    "sent": "This is with context rules, so it's really important not just in this.",
                    "label": 0
                },
                {
                    "sent": "In this task, on every task, representing both recovering recall and actually using the dependency among tags.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that the first part of the algorithm is very bad, so one question is what happens if you use one of the other algorithms that will perform better.",
                    "label": 0
                },
                {
                    "sent": "At this point, this is the dependency between.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seat number of rules generated and accuracy, so this is the F measure for every slot and these are the number of rules, so you will see that when you have a limited number of slots and a limited number unlimited F measure.",
                    "label": 0
                },
                {
                    "sent": "Actually it's because America is not able to develop rules, and in particular it is not able to develop complex models.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning curve as CMS was designed as companion to annotation interfaces high precision.",
                    "label": 0
                },
                {
                    "sent": "This is the precision you see that from the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first block of 40 tax immediately goes over 80% average, but if you look at recall, of course it goes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slowly and again.",
                    "label": 0
                },
                {
                    "sent": "This is how it is meant to perform, and this is how it performs on every task that we have seen.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Concerning active learning, again, the approach that we used was very naive and it was meant to be.",
                    "label": 1
                },
                {
                    "sent": "It is again due to the application.",
                    "label": 1
                },
                {
                    "sent": "Or in the in terms of annotation, what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to avoid when people are not a documents that they imitate documents that are very similar one to each other because it is not important for the learner to have two identical tags.",
                    "label": 0
                },
                {
                    "sent": "So what we use is a simple measure we take.",
                    "label": 0
                },
                {
                    "sent": "We take the rules that we have developed.",
                    "label": 0
                },
                {
                    "sent": "We applied to the UN annotated corpus.",
                    "label": 0
                },
                {
                    "sent": "We look at the difference between the expected number of tags, expected types of tags in the in the training corpus, and the one that we are able to extract.",
                    "label": 1
                },
                {
                    "sent": "We select the documents in which we don't get good results.",
                    "label": 0
                },
                {
                    "sent": "We skip the ones where we don't get anything because they could be relevant and we select them.",
                    "label": 0
                },
                {
                    "sent": "So it's it's very it's very naive in this sense and it works.",
                    "label": 0
                },
                {
                    "sent": "More or less.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you have games here.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "One the train, the effect when you use the random corpus.",
                    "label": 0
                },
                {
                    "sent": "This is when you use active learning.",
                    "label": 0
                },
                {
                    "sent": "There are two cases in which it is reduced.",
                    "label": 0
                },
                {
                    "sent": "Now there is something to say here.",
                    "label": 0
                },
                {
                    "sent": "I don't consider this a good result, but of course you don't know how.",
                    "label": 0
                },
                {
                    "sent": "The random corpus is, so it is possible that even the random corpus is a good one, or it is a bad one, but I don't think this is a good result in general.",
                    "label": 0
                },
                {
                    "sent": "What we have is that we tend to have better performance on other tasks on this we didn't.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conclusion, I think that the the only thing I can say that remain current resting is for the contextual rules.",
                    "label": 0
                },
                {
                    "sent": "Because they really represent two things they recovery, called in representing the dependency among the tags that are the two things that I didn't mention as important.",
                    "label": 0
                },
                {
                    "sent": "Actually, the initial tag is very, very bad, so it's 20% is not compatible to any other system.",
                    "label": 1
                },
                {
                    "sent": "But of course it's.",
                    "label": 1
                },
                {
                    "sent": "It's also because we want to have high precision, because if you use contextual rules on imprecise tax, it will simply diverge.",
                    "label": 0
                },
                {
                    "sent": "So wanted to some extent.",
                    "label": 1
                },
                {
                    "sent": "In many experiments, we can show that if we renew all the tie, all the rules for one tag and we just use contextual rules, you can reach up to 90% precision.",
                    "label": 0
                },
                {
                    "sent": "Really sorry, recall in task where you generally receive 90%, so it means that you recover whatever you have lost with the best at best dagger annotations.",
                    "label": 1
                },
                {
                    "sent": "So I think again this is this is good result.",
                    "label": 0
                },
                {
                    "sent": "An interesting feature of the system.",
                    "label": 1
                },
                {
                    "sent": "I said this represents dependency between tag.",
                    "label": 0
                },
                {
                    "sent": "For example in conference naming conference acronym, the concatenation is extremely important.",
                    "label": 0
                },
                {
                    "sent": "The acronym per say.",
                    "label": 0
                },
                {
                    "sent": "Is just an acronym, a date per say, is just a date.",
                    "label": 0
                },
                {
                    "sent": "In we think that in the implicit relation, of course they are implicit.",
                    "label": 0
                },
                {
                    "sent": "You don't model explicitly the way in which they are dependent, but if you don't represent the fact that somehow they are concatenated.",
                    "label": 0
                },
                {
                    "sent": "It is you lose something, you lose a piece of information you are not able to represent it.",
                    "label": 0
                },
                {
                    "sent": "In Wisc, Steven Sutherland try to do that by actually model modeling the path, the complete path, but that was that was something that we had in mind when when we designed this, But the problem is that when you do, when you define very long path, of course you have data sparsity because you will.",
                    "label": 0
                },
                {
                    "sent": "You will learn different rules every time for the same rule for the same pattern.",
                    "label": 0
                },
                {
                    "sent": "So what we do, we use pairs of tags and one is the start tag, the other one is the end tag.",
                    "label": 0
                },
                {
                    "sent": "One is the end tag.",
                    "label": 0
                },
                {
                    "sent": "And the start of the following time.",
                    "label": 0
                },
                {
                    "sent": "So we represent in a very implicit way, and it is possible to show that you're you need less examples to do that.",
                    "label": 0
                },
                {
                    "sent": "And but the conventional rules are a solution that not always work, so there I think I'm very worried by the fact that on some specific slots we tend to perform much less than the other systems.",
                    "label": 0
                },
                {
                    "sent": "In general this is on every time on every task.",
                    "label": 0
                },
                {
                    "sent": "So what we have to do as a future work is to try to understand why.",
                    "label": 0
                },
                {
                    "sent": "We dropped so dramatically in some cases and not in others.",
                    "label": 0
                },
                {
                    "sent": "So what is that doesn't work?",
                    "label": 0
                },
                {
                    "sent": "It seems like something doesn't start like an engine apart of the engine that doesn't work, so I think this is our our future work and the other future work is to take the contextual rules and combine them using another system that the level of the initial target rules performance much better to see.",
                    "label": 0
                },
                {
                    "sent": "Maybe we don't get any gain, maybe we diverge or whatever, but this is again.",
                    "label": 0
                },
                {
                    "sent": "Something to test?",
                    "label": 0
                },
                {
                    "sent": "OK, I think this is my.",
                    "label": 0
                },
                {
                    "sent": "Right, you use your professional rules not only on the orphans.",
                    "label": 0
                },
                {
                    "sent": "You're more than doubles, so if you're using that overall now, just unpack the car inspected to be near so you know from the training corpus that show the staff will perceive the end time, so you want the office in this case, but also the fact that from the training hope you have served, for example, the start time is followed by a name time, they're choose different, so use this information here and use the window.",
                    "label": 0
                },
                {
                    "sent": "You know that the maximum distance that you've seen when they are adjacent is that OK?",
                    "label": 0
                },
                {
                    "sent": "But you don't just use them to fix the orphans, no, no.",
                    "label": 0
                },
                {
                    "sent": "This is why we can represent the concatenations and the second question was so you have this kind of shortest match bias.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Did you observe any correlation?",
                    "label": 0
                },
                {
                    "sent": "Weird between these sort of bias, either for shortest match and the longest match, and the performance on the difference wells.",
                    "label": 0
                },
                {
                    "sent": "So is it the case that I need to reduce does worse where wherever, whenever you might prefer a longest match biased, and I think this is?",
                    "label": 0
                },
                {
                    "sent": "I haven't checked, but this is my explanation for rotation that tends to be very long and very regular and.",
                    "label": 0
                },
                {
                    "sent": "Wyndham",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For me it again to be very long and very regular.",
                    "label": 0
                },
                {
                    "sent": "I think this is I mentioned some pointed.",
                    "label": 0
                },
                {
                    "sent": "I think this is a good thing and a bad thing at the same time we probably what we should do is to develop two types of contextual rules.",
                    "label": 1
                },
                {
                    "sent": "One that tries to maximize and the other one that tries to minimize.",
                    "label": 0
                },
                {
                    "sent": "But we haven't done.",
                    "label": 0
                },
                {
                    "sent": "Select do you think that the from what do we see that that's in some cases good performance was being really good and but there should.",
                    "label": 0
                },
                {
                    "sent": "Also you were much lower and was wondering whether.",
                    "label": 0
                },
                {
                    "sent": "I remember your equal is paid is independent on the contextual rules and I was wondering if this could be attributed to.",
                    "label": 0
                },
                {
                    "sent": "But some slots have more redundancy in their language, so that you can extract contextual rules more efficient like they are presented in more than once, maybe in calls for papers, or they have like lesser variety of of of context of the appearing, so they actually system to extract the contextual rules better than other without appearing more this irregular.",
                    "label": 0
                },
                {
                    "sent": "Is it the context in the sense of linguistic complex or the variety of the linguistic context or the variety of slots?",
                    "label": 0
                },
                {
                    "sent": "Combination of slots?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean I was thinking of linguistics.",
                    "label": 0
                },
                {
                    "sent": "Well, we haven't checked the best possible before.",
                    "label": 0
                },
                {
                    "sent": "Sure, there is one thing that is very difficult when user fix it window that is 2 typically recognize the end time because the difference between a start tag and end time or the camera ready copy data and the submission date is given by the left context.",
                    "label": 0
                },
                {
                    "sent": "And if it is a bit long in your window you don't see the context from their items.",
                    "label": 0
                },
                {
                    "sent": "So this is where you tend to use the.",
                    "label": 0
                },
                {
                    "sent": "Philosophical, semantic point.",
                    "label": 0
                },
                {
                    "sent": "You can describe any computation by rules.",
                    "label": 0
                },
                {
                    "sent": "So if you take the output from any machine.",
                    "label": 0
                },
                {
                    "sent": "Describing his rules, so I guess Wonder Woman duction is the right kind of terminology as well method, because most naturally think of rules.",
                    "label": 0
                },
                {
                    "sent": "Something that you could easily rank in if then form was smaller.",
                    "label": 0
                },
                {
                    "sent": "Looks more like.",
                    "label": 0
                },
                {
                    "sent": "Although you can describe anything, yes, yeah, I think this is a good point.",
                    "label": 0
                },
                {
                    "sent": "There's another language person, probably I've used the wrong terminology, yeah.",
                    "label": 0
                },
                {
                    "sent": "Think about.",
                    "label": 0
                },
                {
                    "sent": "As additional features.",
                    "label": 0
                },
                {
                    "sent": "And this means that we can go over the.",
                    "label": 0
                },
                {
                    "sent": "Fix windows because you could have.",
                    "label": 0
                },
                {
                    "sent": "Save it.",
                    "label": 0
                },
                {
                    "sent": "As a feature.",
                    "label": 0
                },
                {
                    "sent": "The longest.",
                    "label": 0
                },
                {
                    "sent": "Teacher saying before I had this time.",
                    "label": 0
                },
                {
                    "sent": "Was three 510 words for.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, that's a good way.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think.",
                    "label": 0
                }
            ]
        }
    }
}