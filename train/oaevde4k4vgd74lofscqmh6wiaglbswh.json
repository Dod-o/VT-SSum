{
    "id": "oaevde4k4vgd74lofscqmh6wiaglbswh",
    "title": "Learning with Multiple Similarity Functions",
    "info": {
        "author": [
            "Avrim Blum, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Multiple Kernel Learning"
        ]
    },
    "url": "http://videolectures.net/lkasok08_blum_lwmsf/",
    "segmentation": [
        [
            "Yes, this is joint work with Nina Balcan at Microsoft, soon to be Georgia Tech and not these three pro at PTI."
        ],
        [
            "So, so the theme of this talk is going to talk about.",
            "A theory of some natural sufficient conditions for similarity functions to be useful for classification tasks that doesn't require the similarity functions to be legal kernels or refer to implicit spaces, but includes the notion of large margin kernels.",
            "And then I'll talk about given this, some extensions to learning with multiple similarity functions or learning when we have a class of some wearing functions, and we're hoping that some similarity function in that class is good.",
            "So just like.",
            "We're talking about with learning a kernel, but for more general similarity functions that aren't legal kernels.",
            "And this will turn out to have some interesting properties.",
            "And also I think some really interesting open questions.",
            "OK, so kernels."
        ],
        [
            "So the way I want to think about kernels, just the legal definition of dot product.",
            "So it's just a pairwise function, takes into objects, outputs a number such that there is some implicit mapping Phi, so that K of XY is 5 X .5 Y.",
            "Typically, this implicit mapping is taking it to much higher dimensional space.",
            "Then so much higher dimensional in your input space and what's great about kernels is that you know many algorithms when they interact with their data.",
            "They only interact by taking dot product, so if every time I took a dot product you replaced it with invocation of your kernel function, then they're acting like the data is in this higher dimensional space.",
            "And Moreover, even though this is a potential high dimensional space, if data separable by a large margin."
        ],
        [
            "In that space we get good generalization abilities, standard theory and kernels are useful for many, many different kinds of data.",
            "Very, very nice.",
            "So one of the limitations of the current theory is."
        ],
        [
            "Is that you in practice when you construct a kernel for some problem like sequences or.",
            "Hello I like to like say, protein sequences.",
            "You're you're thinking of them as measure of similarity.",
            "You thinking, Gee, what would be a good measure of similarity between these objects?",
            "But our theory is in terms of margins and implicit space, and you're not really thinking about G. Which of these is more likely to have a large margin?",
            "Some implicit space?",
            "You're really thinking you know what makes sense as a measure of similarity.",
            "So the theory is maybe not the best for intuition in terms of helping you give guidance to what you're looking for when you're constructing some similarity function.",
            "Also, the kernel requirement might rule out.",
            "The most natural similarity function for your domain.",
            "So like the Smith Waterman score that we saw this morning, it's a dynamic programming algorithm.",
            "It's not dot product.",
            "You can, you can do things to it, but but it would be nice to just talk about directly.",
            "OK, so so the question.",
            "Well, look at it, you know.",
            "Can we give an alternative?",
            "Thing for this box here maybe a more general theoretical explanation, OK?",
            "So."
        ],
        [
            "So in recent work we've been looking at notions of good similarity functions, have a number of appealing properties, and in particular I'll be talking about the notion of what we're looking for in a similarity function that's in terms of natural direct quantities, so there's no implicit space, no requirement that your similarity function be illegal.",
            "Dot product is just a measure of similarity between objects.",
            "And if you satisfy this property, then then you can prove that you can learn well without too much data.",
            "The usual thing you'd like to be able to prove.",
            "And then we'll see.",
            "That's also broad enough to include the usual notion of a good kernel, so that if you were illegal kernel with a large margin, you would also have this property here.",
            "OK, and then in a in a pack sense, in a distribution specific pack sense, you can actually formally talk about using this to learn classes of functions that actually don't have good kernels.",
            "And then we can extend this to learning with multiple similarity functions so.",
            "So what if you have some space of similarity functions and your hope?",
            "Is it some similar function in this space is good, but you don't know which one in advance, and so you want to learn the similarity function while you're learning the.",
            "Solving your classification problem.",
            "OK, So what I want to do is start with.",
            "A preliminary notion of what we're looking for that will satisfy this condition of being in terms of natural direct quantities, but not be broad enough to include all good kernels.",
            "So this first attempt so like this will include things that aren't legal kernels, but not include all the large margin kernels, and then will generalize this to it to our main notion.",
            "Then we'll look at this issue of learning from a class of functions.",
            "OK, so let me start with the first one.",
            "So, so the property give you a property which is intuitive and sufficient for learning.",
            "For learning I won't prove to you, it's intuitive.",
            "But I'll have sufficient.",
            "OK, but I can't, but you'll see."
        ],
        [
            "Very intuitive, so OK, so if we buy a learning problem, so I mean we're trying to learn the data is coming from some distribution over labeled examples and just notation wise I'm going to X&Y for examples and L for labels.",
            "OK, so why is not a label here?",
            "Why is an example?",
            "Our goal is out with a good classification rule for this learning problem.",
            "OK, so here's here's the condition we want.",
            "Let's say a similarity function is good for our learning problem.",
            "If most examples are on average more similar to other examples of their own label than two examples of the opposite label.",
            "And not just a tiny bit more similar, but by some gap gamma.",
            "OK, so here is the so we can say the similarity function under this definition is epsilon gamma.",
            "Good for this learning problem.",
            "If most of the examples are on average gamma more similar to points of their own labels that this quantity here.",
            "How similar are you on average 2 examples why of your label?",
            "So by some gap gamma more than your average similarity, the points of the other label, right?",
            "So we want we have images, we want the images of men to be on average more similar images of men, images, women, an image of women on average more similar images of women.",
            "And this isn't so that's that's the kind of thing.",
            "So I claim it's a natural condition that you might want.",
            "There's no notion of an implicit space here, and if you satisfy this condition, life is good.",
            "OK, learning is very easy.",
            "How do you learn given this condition so?",
            "All you have to do is do a kind of average nearest neighbor algorithm."
        ],
        [
            "So if I give you a similarity function satisfying this property, very simple algorithm for learning.",
            "Just draw a bunch of positive examples, draw a bunch of negative examples given a new test point, just classify it based on.",
            "Are you on average more similar to the positive examples in your training set?",
            "Or an average more similar to the negative examples your training set?",
            "And the point is, as long as your training set is big enough, roughly, you need one over gamma."
        ],
        [
            "Square to your.",
            "Just estimating these expectations, you know that these two expectations.",
            "These are bounded qualities between plus 1 -- 1.",
            "There's a gap gamma, so you need basically one over gamma squared samples to estimate that some extra log factors enough so that with high probability almost all the examples satisfying this condition empirically will have this quality bigger than that quality.",
            "Great, so we just take a bunch of data, classify based on average nearest neighbor.",
            "OK, so intuitive definition simple algorithm.",
            "Right, unfortunately.",
            "This is not broad."
        ],
        [
            "Enough to capture the notion of good kernels.",
            "In fact, you wouldn't expect it to, because if it was then you wouldn't need SVM, right?",
            "'cause if it was, you would say oh anywhere instead of SVM, I could just do average nearest neighbor, alright, but OK, this is not broad enough to capture here, just a nice simple example.",
            "Imagine that our similarity functions just dot product, so our implicit space is just.",
            "The plane here and imagine our positive examples are split evenly here and here are negatives are down here and this is a 30 degree angle like this.",
            "So this is a large.",
            "This is a.",
            "A good kernel is a large margin separator.",
            "Here's margin is 1/2.",
            "But it doesn't satisfy this condition in particular.",
            "These examples by DOT product are actually more similar to negatives than they are the positives on average.",
            "Why is that?",
            "Well?",
            "The DOT product in this vector in this vector is 1/2.",
            "But so they have these guys have average similarity 1/2 to the negatives, but to the positives.",
            "Well, half deposit, they have similarity 1, two and half.",
            "They have similarity negative 1/2 two and the average of one negative half is only 1/4.",
            "So.",
            "So these are actually on average more similar to the other labeled into their own, so it's OK.",
            "It wasn't proper.",
            "So we didn't replace SVM with average nearest neighbor.",
            "So on the other hand, if you notice that as long as we didn't pick these.",
            "Points why from over there if we only picked the points, why?",
            "From over here things would be fine, so that suggests a way of broadening this.",
            "Let's say the similarity."
        ],
        [
            "Action is good if there exists.",
            "A non negligible region R of representative points such that most examples are on average more similar to the representative points of their label than to the representative point to the other lady.",
            "OK. That's what we want, so let's say what we want is that there should exist somewhere nonnegligible, Resighted representative points so that most examples are on average more similar to the representative points on their label and the representative points the other label.",
            "So we're broadening.",
            "It was even if we didn't satisfy the previous condition, you know, as long as, by throwing some points out and not including them in the wise, then we would satisfy that condition.",
            "OK, so there's so this is.",
            "This is what we want.",
            "Still pretty intuitive and.",
            "So here so here."
        ],
        [
            "Definition will say this similarity function with extra parameter epsilon gamma Tau good.",
            "So is epsilon gamma Tau good if there exists somewhere a set of reasonable representative points Y so that most examples are on average more similar to the reasonable points?",
            "Why of their label then to the reasonable points?",
            "Why of the other label by some gap gamma?",
            "And the Tao is coming in 'cause we want these reasonable points not to have like actually exist.",
            "Not not, be like exponentially hard to find.",
            "You know, there's some some how probability mass of reasonable points.",
            "We can hope to actually see them when we draw data.",
            "So.",
            "Good, so that'll that'll be our notion.",
            "And we want most examples X to satisfy this condition.",
            "And technically the ones that don't satisfy this condition well, this epsilon is really a hinge loss, so we want most examples satisfy this.",
            "And the ones that don't shouldn't totally dissatisfied.",
            "So the.",
            "But the average amount by which examples don't satisfy this should be at most epsilon.",
            "So one thing it's nice, so I claim that given this condition, so this is now a broader definition is not so obvious.",
            "How you can learn.",
            "Given this condition, we can't just do the average nearest neighbor anymore, because 'cause we are care about these these reasonable points.",
            "But here is a very natural algorithm.",
            "What we'll do is draw a bunch of landmark points, random points that can be unlabeled if we have unlabeled data.",
            "We just take a bunch of random unlabeled points from our distribution while I went through ID will call them landmarks and then we'll just re represent our data.",
            "So this is an empirical similarity map.",
            "Take each example X and just write it as explicitly.",
            "How similar are you to each of these landmarks?",
            "So just kind of measuring this by how similar am I to each of these landmarks?",
            "Well, this is no longer necessarily.",
            "That product is just a measure of similarity, so we really represent the data like that.",
            "OK, so we have data in our space?",
            "We?",
            "Re represented by just how similar MIT each of these landmarks, and I claim that that actually the world is nice in this space.",
            "I claim that as long as we have enough landmarks.",
            "Then with high probability there is a linear separator in this space that has good L1 margin.",
            "So how many landmarks do we need?",
            "So the number is roughly one over gamma squared, one over gamma squared times Tau, so that's that's enough so that.",
            "So tell us what fraction of data are reasonable, so this is enough so that somewhere in this set we don't know where.",
            "But somewhere in here are the kind of one over gamma squared reasonable landmarks.",
            "The rest might be totally crazy, but somewhere in our set of landmarks somewhere there are some good ones.",
            "And then I claim that in this space there will be a linear separator of Goodell, one margin, and the reason is so you can't see down here most of you.",
            "But the reason is think about the following weight vector.",
            "So I want this.",
            "I want to weight vector here.",
            "Think of a weight vector that puts Weight 0 on the points.",
            "Why that are not in the set R?",
            "And wait.",
            "If there are N plus reason reasonable.",
            "Positive landmarks in here.",
            "They all get weight 1 / N plus and if there are N minus reasonable negative landmarks in here, they all get weight minus 1 / N minus.",
            "OK, so the way through the zero 1 / N plus.",
            "Minus 1 minus this is a separator which is doing what it's exactly computing www.fof X is exactly computing so R, so I'm just hoping this NR exists.",
            "I don't really know what it looks like.",
            "But you know, somehow it's I think of it as the data minus stuff.",
            "That's that's kind of a little bit bizarre.",
            "So throwing those out, look at the ones leftover.",
            "And really what this is.",
            "With this, the claim is in this space there is a weight vector that is doing average nearest neighbor over the reasonable points.",
            "Weight 0.",
            "On the ones that are not inside are so they go away and it's averaging.",
            "The reasonable positives and then minus the average reasonable negatives to give you this this gap.",
            "So in the nice thing is this weight vector has aloe L1 length because it's the L1 length is 2, you have 1 / N plus on all the N plus reasonable pauses.",
            "That adds up to one and negative 1 / N minus all the N minus reasonable negatives up to minus one before L1 is it.",
            "Absolute value says another one, so length to margin, roughly gamma, maybe gamma over 2, 'cause we're estimating these two.",
            "So we have a nice.",
            "Separator in L1, so that's great, so so the the algorithm suggested by this definition is do this empirical similarity map."
        ],
        [
            "And then in this space, use an algorithm for learning when there's a good L1 margin when your separator.",
            "So maybe the window algorithm, or just direct L1 optimization.",
            "OK, so so so this condition.",
            "Then we can.",
            "We can learn and the sample size you need.",
            "You need enough unlabeled points, enough landmarks to be able to see enough reasonable points.",
            "A number of labeled points you need is the usual one over gamma squared kind of term, and then log of the.",
            "Dimension of this space because we're doing L1, which gets you that log?",
            "So OK, so the number of labeled examples is roughly one over the margin squared, one over the epsilon.",
            "The accuracy looking for.",
            "OK, so this definition is sufficient to learn well using this kind of empirical similarity map followed by an L1 optimization.",
            "OK, so so now."
        ],
        [
            "So.",
            "And I claim that some similar functions are good.",
            "Kernel is also satisfies this definition, although the gammas get squared, so there's a little bit of loss when you translate over from a large margin kernel to a good similarity function.",
            "Um?"
        ],
        [
            "But the other hand you can also show a separation.",
            "You can show there exists classes that have similarity functions satisfying this definition, but don't have large margin kernels that don't have, so you can have a class of functions where there's no kernel that's simultaneously large margin for all functions in the class.",
            "But you can have a similarity function that's simultaneously a good similarity function.",
            "Satisfying this definition for all functions in the class you get kind of nice separation.",
            "OK."
        ],
        [
            "So.",
            "So in some sense we can think of this as it gives one way of justifying a very natural algorithmic rule, which is do this kind of pickle similarity map optimizer.",
            "Good OK."
        ],
        [
            "Now we get to multiple similarity functions, so one other thing that's nice is now the following problem becomes very very easy.",
            "So imagine the following setting.",
            "Imagine we have a bunch of similarity functions.",
            "They are of them, and we're hoping that some unknown convex combination of them is good.",
            "Right, that's just like kind of thing with Chrome, so with some.",
            "Collection summary functions we're hoping at some convex combination is good, so how can we do learning?",
            "So here's a really, really easy way to do learning.",
            "Take your landmarks as before, and then just concatenate the feature empirical similarity map using similarity function one.",
            "The empirical oops.",
            "I interleaved it this way.",
            "OK, this is landmark.",
            "One empirical similarity by similarity function.",
            "One similarity function to up to similarity function R Alright Landmark 2 the same thing.",
            "Landmark is just concatenating.",
            "The map you would get using similar function one the map would get using similar function to adopt A function R. So it's just.",
            "We just made the space instead of D length.",
            "It's D times are.",
            "What's really nice is because.",
            "And then just run the same L1 optimization algorithm we had before.",
            "What's nice is that because we're talking L1 margins, the margin has not gotten worse."
        ],
        [
            "At all, so the claim is in this space with high probability, there is a linear separator with the same L1 margin we have with just one similarity function.",
            "So the sample complexity only goes up by the dimension of the space which the log of the demand.",
            "It's only going up in this log term, so we only have a log rhythmic penalty in how many similarity functions we're using.",
            "OK, so that's only a log rhythmic penalty as we add more and more similarity functions.",
            "OK, so why is it that the margin didn't change?",
            "So here's the reason.",
            "So I think this is kind of."
        ],
        [
            "You hear so.",
            "Imagine the mapping.",
            "Imagine that we knew what that convex combination of similar functions was.",
            "Let me call that K. 0 So KO as the magic convex combination we're shooting for and imagine we did the mapping using that one.",
            "Well, we know there is some goods.",
            "Good vector of Goodell.",
            "One margin in that space.",
            "Now we're not doing that.",
            "That mapping 'cause we don't know what that similarity function is.",
            "But just consider that weight vector.",
            "The one that's good in that space and just expand it out.",
            "Expand out so.",
            "So take W 1.",
            "The first coordinate there and just replace it with with A1, W 1, A2, W 1, DA R. So just take that each component and just expand it out this way.",
            "So we just stretched it out, keeping the same L1 value.",
            "We just stick W one, just we split it into little pieces and we take the second quarter W2 and split into little pieces.",
            "Split by these alphas and the claim is that this new expand vector W died with what we're doing.",
            "Here is the same as.",
            "This W 0 down with the this mapping F0 so we just spread it out.",
            "We didn't change the L1 margin at all so we really don't have that much penalty for having more similarity functions so it's kind of interesting.",
            "This is a nice thing without one margin.",
            "Alright."
        ],
        [
            "So OK, so so.",
            "Saying there's interesting fact here because our property was defined in terms of L1.",
            "There's no change in margin, not too much penalty for concatenating our feature spaces.",
            "And then if we had an L2 margin, things got bigger, the margin would drop by a factor square root of R and we would get this kind of a much bigger penalty.",
            "So that's what that's nice with L1.",
            "The album is very simple, we just concatenate.",
            "Now alternative algorithms.",
            "You can do a joint optimization, you could just try to directly solve for this convex combination.",
            "This this case you are looking for and the vector W combined.",
            "So there's a good L1 margin and actually the bound we just gave implies that this algorithm would also have good sample complexity as well.",
            "Unfortunately, we don't know how to do this efficiently, unlike with kernels.",
            "OK, so that's one of the open questions.",
            "I don't know how to do that efficiently for general similarity functions."
        ],
        [
            "Also sending large margin kernel also satisfies this condition.",
            "It means that you can you can only get just a log rhythmic penalty for kernels if you just just do this, although are in the worst case, we do have potentially a square in the gamma parameter.",
            "So so doing this potentially there's a margin gets worse but you don't have this penalty for how many similarity functions you're complaining."
        ],
        [
            "So open questions.",
            "So one thing, one nice open questions.",
            "We don't know how to efficiently deal with.",
            "General convex classes of similarity function, so this is particular to a convex combination of some number, but we don't know how to deal with a more general convex class.",
            "It's not clear how to do this concatenation then if I just give you a general convex class.",
            "Does not just something of this form.",
            "We don't know how to efficiently implement a direct joint optimization for a convex combination of similarity functions.",
            "Alternately, perhaps you can use the concatenation algorithm to extract a good convex combination.",
            "Not sure.",
            "Also, these are two quite different styles of algorithm where you just concatenate when you try to directly optimize.",
            "So interesting to think what might be in between there.",
            "And finally it seems there's potential for using this for transfer learning, where you use the weights for one problem for some other problem."
        ],
        [
            "Enjoy your party.",
            "So.",
            "Do you have any thoughts on how to use your setting?",
            "Because you wouldn't know Gamma rry to preauricular.",
            "Yeah yeah.",
            "So what do you do if you don't know like say the margins appear?",
            "So the.",
            "The nice thing with the L1 margins, you don't have a huge penalty for adding extra features, so the question.",
            "So the gamma comes into how many landmarks do you need?",
            "Me or the size of our so the other part is just an L1 optimization.",
            "You can do that so so the one part where knowing something comes in is how many landmarks do you need.",
            "Luckily there's only a log rhythmic penalty number of landmarks so you could.",
            "You know, hope to just make some guess.",
            "I mean one thing I don't, so the theory just says.",
            "Pick them at random.",
            "Presumably there are better things to do than pick them at random, but it seems very hard theoretically to say anything about that.",
            "You can have an unpolished.",
            "Would be 1 alternative would be to use.",
            "Implicit feature map.",
            "And then some sample to the subset of points.",
            "If you have too many training and Apple could do what he did so so the.",
            "What you do differently than the L1 margin on top of that settlement at the end, right?",
            "Yes, right, so where do you see the advantage of doing that?",
            "Learning a linear combination instead of multiple learning about this so?",
            "I. OK, so you mean?",
            "OK, maybe I'm not sure, so the advantage of using L1 versus L2 or the advantage of having this kind of notion of goodness of similarity function.",
            "The same definition.",
            "Function gives you kind of kernel which is then followed by construction and then you do the L1 instead of the Mail.",
            "Where do you see that manager one?",
            "I think they all one procedure is nice because.",
            "Then you're a lot less sensitive to how many?",
            "How many features you have?",
            "Yeah once one.",
            "Several.",
            "Yeah.",
            "You just map on your points to the values of the code and this gives you exactly the same as the indent marks.",
            "You get econometrics, but one resident.",
            "And why do L1 afterwards instead of just in case because you have to do that, you have to do the L1 rather than two, and those get these grantees.",
            "Real function is good is supporting these definitions for every function.",
            "Then you'll have to do the L1 in order to get the learning guarantees.",
            "Constructed this."
        ],
        [
            "And then you did Elton Netspace.",
            "There's no one.",
            "Then you'd get significantly worse guarantees based on the assumptions and that.",
            "Yes, uh, so you get the L2 margin gets worse, but the L1 margin doesn't.",
            "So that maybe we can talk about if I Mail it.",
            "So I went to Princeton.",
            "We were able to minimize the weights that we get, the better performance or better mountains and I just got an 80.",
            "So in your case.",
            "Type me if if we could efficiently in this case.",
            "If we could do."
        ],
        [
            "Actually learn the weight.",
            "The capacity would be smaller, so the bounds mean you ought to have better generalization, but we just.",
            "You know in here we have a very simple, efficient algorithm with the first case, and we're not sure how to do this efficiently in this case, so.",
            "Yeah.",
            "So you definitely gets better results, but.",
            "The bitter, bitter bills that are in Mexico Minister we definitely better.",
            "But it will be only very slightly better, because so that's a point distinction, because here in in the L1 case, so or in the similarity case, if you use that information then you didn't get a worse than next week, but actually it's not done much worse because you only get to the.",
            "What you say is only a little bit effective lovedean.",
            "Otherwise you could say that after maybe that'd be, but in the in the words in the criminal case you'd pay much bigger price in the ministry.",
            "So so you can know them in next week would be here worse.",
            "You're not losing quite as much as you're losing, so this procedure for the 90 cases much more sensible than the pigmentation.",
            "Oh forgive.",
            "Super.",
            "You can choose.",
            "Discuss.",
            "Yeah, good question then.",
            "Yeah.",
            "Log R or.",
            "You are giving bounds that show that you can have instead imperative.",
            "So when you ask naughty this afternoon.",
            "Black"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, this is joint work with Nina Balcan at Microsoft, soon to be Georgia Tech and not these three pro at PTI.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so the theme of this talk is going to talk about.",
                    "label": 1
                },
                {
                    "sent": "A theory of some natural sufficient conditions for similarity functions to be useful for classification tasks that doesn't require the similarity functions to be legal kernels or refer to implicit spaces, but includes the notion of large margin kernels.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk about given this, some extensions to learning with multiple similarity functions or learning when we have a class of some wearing functions, and we're hoping that some similarity function in that class is good.",
                    "label": 0
                },
                {
                    "sent": "So just like.",
                    "label": 0
                },
                {
                    "sent": "We're talking about with learning a kernel, but for more general similarity functions that aren't legal kernels.",
                    "label": 0
                },
                {
                    "sent": "And this will turn out to have some interesting properties.",
                    "label": 0
                },
                {
                    "sent": "And also I think some really interesting open questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so kernels.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way I want to think about kernels, just the legal definition of dot product.",
                    "label": 0
                },
                {
                    "sent": "So it's just a pairwise function, takes into objects, outputs a number such that there is some implicit mapping Phi, so that K of XY is 5 X .5 Y.",
                    "label": 1
                },
                {
                    "sent": "Typically, this implicit mapping is taking it to much higher dimensional space.",
                    "label": 1
                },
                {
                    "sent": "Then so much higher dimensional in your input space and what's great about kernels is that you know many algorithms when they interact with their data.",
                    "label": 0
                },
                {
                    "sent": "They only interact by taking dot product, so if every time I took a dot product you replaced it with invocation of your kernel function, then they're acting like the data is in this higher dimensional space.",
                    "label": 1
                },
                {
                    "sent": "And Moreover, even though this is a potential high dimensional space, if data separable by a large margin.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that space we get good generalization abilities, standard theory and kernels are useful for many, many different kinds of data.",
                    "label": 1
                },
                {
                    "sent": "Very, very nice.",
                    "label": 0
                },
                {
                    "sent": "So one of the limitations of the current theory is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that you in practice when you construct a kernel for some problem like sequences or.",
                    "label": 0
                },
                {
                    "sent": "Hello I like to like say, protein sequences.",
                    "label": 0
                },
                {
                    "sent": "You're you're thinking of them as measure of similarity.",
                    "label": 1
                },
                {
                    "sent": "You thinking, Gee, what would be a good measure of similarity between these objects?",
                    "label": 0
                },
                {
                    "sent": "But our theory is in terms of margins and implicit space, and you're not really thinking about G. Which of these is more likely to have a large margin?",
                    "label": 0
                },
                {
                    "sent": "Some implicit space?",
                    "label": 0
                },
                {
                    "sent": "You're really thinking you know what makes sense as a measure of similarity.",
                    "label": 1
                },
                {
                    "sent": "So the theory is maybe not the best for intuition in terms of helping you give guidance to what you're looking for when you're constructing some similarity function.",
                    "label": 1
                },
                {
                    "sent": "Also, the kernel requirement might rule out.",
                    "label": 0
                },
                {
                    "sent": "The most natural similarity function for your domain.",
                    "label": 0
                },
                {
                    "sent": "So like the Smith Waterman score that we saw this morning, it's a dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not dot product.",
                    "label": 0
                },
                {
                    "sent": "You can, you can do things to it, but but it would be nice to just talk about directly.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the question.",
                    "label": 0
                },
                {
                    "sent": "Well, look at it, you know.",
                    "label": 0
                },
                {
                    "sent": "Can we give an alternative?",
                    "label": 1
                },
                {
                    "sent": "Thing for this box here maybe a more general theoretical explanation, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in recent work we've been looking at notions of good similarity functions, have a number of appealing properties, and in particular I'll be talking about the notion of what we're looking for in a similarity function that's in terms of natural direct quantities, so there's no implicit space, no requirement that your similarity function be illegal.",
                    "label": 1
                },
                {
                    "sent": "Dot product is just a measure of similarity between objects.",
                    "label": 0
                },
                {
                    "sent": "And if you satisfy this property, then then you can prove that you can learn well without too much data.",
                    "label": 0
                },
                {
                    "sent": "The usual thing you'd like to be able to prove.",
                    "label": 0
                },
                {
                    "sent": "And then we'll see.",
                    "label": 1
                },
                {
                    "sent": "That's also broad enough to include the usual notion of a good kernel, so that if you were illegal kernel with a large margin, you would also have this property here.",
                    "label": 1
                },
                {
                    "sent": "OK, and then in a in a pack sense, in a distribution specific pack sense, you can actually formally talk about using this to learn classes of functions that actually don't have good kernels.",
                    "label": 1
                },
                {
                    "sent": "And then we can extend this to learning with multiple similarity functions so.",
                    "label": 0
                },
                {
                    "sent": "So what if you have some space of similarity functions and your hope?",
                    "label": 0
                },
                {
                    "sent": "Is it some similar function in this space is good, but you don't know which one in advance, and so you want to learn the similarity function while you're learning the.",
                    "label": 0
                },
                {
                    "sent": "Solving your classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I want to do is start with.",
                    "label": 0
                },
                {
                    "sent": "A preliminary notion of what we're looking for that will satisfy this condition of being in terms of natural direct quantities, but not be broad enough to include all good kernels.",
                    "label": 0
                },
                {
                    "sent": "So this first attempt so like this will include things that aren't legal kernels, but not include all the large margin kernels, and then will generalize this to it to our main notion.",
                    "label": 0
                },
                {
                    "sent": "Then we'll look at this issue of learning from a class of functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start with the first one.",
                    "label": 0
                },
                {
                    "sent": "So, so the property give you a property which is intuitive and sufficient for learning.",
                    "label": 0
                },
                {
                    "sent": "For learning I won't prove to you, it's intuitive.",
                    "label": 0
                },
                {
                    "sent": "But I'll have sufficient.",
                    "label": 0
                },
                {
                    "sent": "OK, but I can't, but you'll see.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very intuitive, so OK, so if we buy a learning problem, so I mean we're trying to learn the data is coming from some distribution over labeled examples and just notation wise I'm going to X&Y for examples and L for labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is not a label here?",
                    "label": 0
                },
                {
                    "sent": "Why is an example?",
                    "label": 0
                },
                {
                    "sent": "Our goal is out with a good classification rule for this learning problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's here's the condition we want.",
                    "label": 0
                },
                {
                    "sent": "Let's say a similarity function is good for our learning problem.",
                    "label": 1
                },
                {
                    "sent": "If most examples are on average more similar to other examples of their own label than two examples of the opposite label.",
                    "label": 0
                },
                {
                    "sent": "And not just a tiny bit more similar, but by some gap gamma.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the so we can say the similarity function under this definition is epsilon gamma.",
                    "label": 1
                },
                {
                    "sent": "Good for this learning problem.",
                    "label": 0
                },
                {
                    "sent": "If most of the examples are on average gamma more similar to points of their own labels that this quantity here.",
                    "label": 1
                },
                {
                    "sent": "How similar are you on average 2 examples why of your label?",
                    "label": 1
                },
                {
                    "sent": "So by some gap gamma more than your average similarity, the points of the other label, right?",
                    "label": 0
                },
                {
                    "sent": "So we want we have images, we want the images of men to be on average more similar images of men, images, women, an image of women on average more similar images of women.",
                    "label": 0
                },
                {
                    "sent": "And this isn't so that's that's the kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So I claim it's a natural condition that you might want.",
                    "label": 0
                },
                {
                    "sent": "There's no notion of an implicit space here, and if you satisfy this condition, life is good.",
                    "label": 0
                },
                {
                    "sent": "OK, learning is very easy.",
                    "label": 0
                },
                {
                    "sent": "How do you learn given this condition so?",
                    "label": 0
                },
                {
                    "sent": "All you have to do is do a kind of average nearest neighbor algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if I give you a similarity function satisfying this property, very simple algorithm for learning.",
                    "label": 0
                },
                {
                    "sent": "Just draw a bunch of positive examples, draw a bunch of negative examples given a new test point, just classify it based on.",
                    "label": 1
                },
                {
                    "sent": "Are you on average more similar to the positive examples in your training set?",
                    "label": 0
                },
                {
                    "sent": "Or an average more similar to the negative examples your training set?",
                    "label": 0
                },
                {
                    "sent": "And the point is, as long as your training set is big enough, roughly, you need one over gamma.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Square to your.",
                    "label": 0
                },
                {
                    "sent": "Just estimating these expectations, you know that these two expectations.",
                    "label": 0
                },
                {
                    "sent": "These are bounded qualities between plus 1 -- 1.",
                    "label": 1
                },
                {
                    "sent": "There's a gap gamma, so you need basically one over gamma squared samples to estimate that some extra log factors enough so that with high probability almost all the examples satisfying this condition empirically will have this quality bigger than that quality.",
                    "label": 0
                },
                {
                    "sent": "Great, so we just take a bunch of data, classify based on average nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "OK, so intuitive definition simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "This is not broad.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enough to capture the notion of good kernels.",
                    "label": 0
                },
                {
                    "sent": "In fact, you wouldn't expect it to, because if it was then you wouldn't need SVM, right?",
                    "label": 0
                },
                {
                    "sent": "'cause if it was, you would say oh anywhere instead of SVM, I could just do average nearest neighbor, alright, but OK, this is not broad enough to capture here, just a nice simple example.",
                    "label": 0
                },
                {
                    "sent": "Imagine that our similarity functions just dot product, so our implicit space is just.",
                    "label": 0
                },
                {
                    "sent": "The plane here and imagine our positive examples are split evenly here and here are negatives are down here and this is a 30 degree angle like this.",
                    "label": 0
                },
                {
                    "sent": "So this is a large.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "A good kernel is a large margin separator.",
                    "label": 0
                },
                {
                    "sent": "Here's margin is 1/2.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't satisfy this condition in particular.",
                    "label": 0
                },
                {
                    "sent": "These examples by DOT product are actually more similar to negatives than they are the positives on average.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "The DOT product in this vector in this vector is 1/2.",
                    "label": 0
                },
                {
                    "sent": "But so they have these guys have average similarity 1/2 to the negatives, but to the positives.",
                    "label": 0
                },
                {
                    "sent": "Well, half deposit, they have similarity 1, two and half.",
                    "label": 0
                },
                {
                    "sent": "They have similarity negative 1/2 two and the average of one negative half is only 1/4.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So these are actually on average more similar to the other labeled into their own, so it's OK.",
                    "label": 0
                },
                {
                    "sent": "It wasn't proper.",
                    "label": 0
                },
                {
                    "sent": "So we didn't replace SVM with average nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, if you notice that as long as we didn't pick these.",
                    "label": 0
                },
                {
                    "sent": "Points why from over there if we only picked the points, why?",
                    "label": 0
                },
                {
                    "sent": "From over here things would be fine, so that suggests a way of broadening this.",
                    "label": 0
                },
                {
                    "sent": "Let's say the similarity.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action is good if there exists.",
                    "label": 0
                },
                {
                    "sent": "A non negligible region R of representative points such that most examples are on average more similar to the representative points of their label than to the representative point to the other lady.",
                    "label": 1
                },
                {
                    "sent": "OK. That's what we want, so let's say what we want is that there should exist somewhere nonnegligible, Resighted representative points so that most examples are on average more similar to the representative points on their label and the representative points the other label.",
                    "label": 0
                },
                {
                    "sent": "So we're broadening.",
                    "label": 0
                },
                {
                    "sent": "It was even if we didn't satisfy the previous condition, you know, as long as, by throwing some points out and not including them in the wise, then we would satisfy that condition.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's so this is.",
                    "label": 0
                },
                {
                    "sent": "This is what we want.",
                    "label": 0
                },
                {
                    "sent": "Still pretty intuitive and.",
                    "label": 0
                },
                {
                    "sent": "So here so here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Definition will say this similarity function with extra parameter epsilon gamma Tau good.",
                    "label": 0
                },
                {
                    "sent": "So is epsilon gamma Tau good if there exists somewhere a set of reasonable representative points Y so that most examples are on average more similar to the reasonable points?",
                    "label": 1
                },
                {
                    "sent": "Why of their label then to the reasonable points?",
                    "label": 1
                },
                {
                    "sent": "Why of the other label by some gap gamma?",
                    "label": 0
                },
                {
                    "sent": "And the Tao is coming in 'cause we want these reasonable points not to have like actually exist.",
                    "label": 0
                },
                {
                    "sent": "Not not, be like exponentially hard to find.",
                    "label": 0
                },
                {
                    "sent": "You know, there's some some how probability mass of reasonable points.",
                    "label": 0
                },
                {
                    "sent": "We can hope to actually see them when we draw data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Good, so that'll that'll be our notion.",
                    "label": 0
                },
                {
                    "sent": "And we want most examples X to satisfy this condition.",
                    "label": 0
                },
                {
                    "sent": "And technically the ones that don't satisfy this condition well, this epsilon is really a hinge loss, so we want most examples satisfy this.",
                    "label": 0
                },
                {
                    "sent": "And the ones that don't shouldn't totally dissatisfied.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "But the average amount by which examples don't satisfy this should be at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "So one thing it's nice, so I claim that given this condition, so this is now a broader definition is not so obvious.",
                    "label": 0
                },
                {
                    "sent": "How you can learn.",
                    "label": 0
                },
                {
                    "sent": "Given this condition, we can't just do the average nearest neighbor anymore, because 'cause we are care about these these reasonable points.",
                    "label": 0
                },
                {
                    "sent": "But here is a very natural algorithm.",
                    "label": 0
                },
                {
                    "sent": "What we'll do is draw a bunch of landmark points, random points that can be unlabeled if we have unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "We just take a bunch of random unlabeled points from our distribution while I went through ID will call them landmarks and then we'll just re represent our data.",
                    "label": 0
                },
                {
                    "sent": "So this is an empirical similarity map.",
                    "label": 0
                },
                {
                    "sent": "Take each example X and just write it as explicitly.",
                    "label": 0
                },
                {
                    "sent": "How similar are you to each of these landmarks?",
                    "label": 0
                },
                {
                    "sent": "So just kind of measuring this by how similar am I to each of these landmarks?",
                    "label": 0
                },
                {
                    "sent": "Well, this is no longer necessarily.",
                    "label": 0
                },
                {
                    "sent": "That product is just a measure of similarity, so we really represent the data like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have data in our space?",
                    "label": 0
                },
                {
                    "sent": "We?",
                    "label": 0
                },
                {
                    "sent": "Re represented by just how similar MIT each of these landmarks, and I claim that that actually the world is nice in this space.",
                    "label": 0
                },
                {
                    "sent": "I claim that as long as we have enough landmarks.",
                    "label": 0
                },
                {
                    "sent": "Then with high probability there is a linear separator in this space that has good L1 margin.",
                    "label": 0
                },
                {
                    "sent": "So how many landmarks do we need?",
                    "label": 0
                },
                {
                    "sent": "So the number is roughly one over gamma squared, one over gamma squared times Tau, so that's that's enough so that.",
                    "label": 0
                },
                {
                    "sent": "So tell us what fraction of data are reasonable, so this is enough so that somewhere in this set we don't know where.",
                    "label": 0
                },
                {
                    "sent": "But somewhere in here are the kind of one over gamma squared reasonable landmarks.",
                    "label": 0
                },
                {
                    "sent": "The rest might be totally crazy, but somewhere in our set of landmarks somewhere there are some good ones.",
                    "label": 0
                },
                {
                    "sent": "And then I claim that in this space there will be a linear separator of Goodell, one margin, and the reason is so you can't see down here most of you.",
                    "label": 0
                },
                {
                    "sent": "But the reason is think about the following weight vector.",
                    "label": 0
                },
                {
                    "sent": "So I want this.",
                    "label": 0
                },
                {
                    "sent": "I want to weight vector here.",
                    "label": 0
                },
                {
                    "sent": "Think of a weight vector that puts Weight 0 on the points.",
                    "label": 0
                },
                {
                    "sent": "Why that are not in the set R?",
                    "label": 0
                },
                {
                    "sent": "And wait.",
                    "label": 0
                },
                {
                    "sent": "If there are N plus reason reasonable.",
                    "label": 0
                },
                {
                    "sent": "Positive landmarks in here.",
                    "label": 0
                },
                {
                    "sent": "They all get weight 1 / N plus and if there are N minus reasonable negative landmarks in here, they all get weight minus 1 / N minus.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way through the zero 1 / N plus.",
                    "label": 0
                },
                {
                    "sent": "Minus 1 minus this is a separator which is doing what it's exactly computing www.fof X is exactly computing so R, so I'm just hoping this NR exists.",
                    "label": 0
                },
                {
                    "sent": "I don't really know what it looks like.",
                    "label": 0
                },
                {
                    "sent": "But you know, somehow it's I think of it as the data minus stuff.",
                    "label": 0
                },
                {
                    "sent": "That's that's kind of a little bit bizarre.",
                    "label": 0
                },
                {
                    "sent": "So throwing those out, look at the ones leftover.",
                    "label": 0
                },
                {
                    "sent": "And really what this is.",
                    "label": 0
                },
                {
                    "sent": "With this, the claim is in this space there is a weight vector that is doing average nearest neighbor over the reasonable points.",
                    "label": 0
                },
                {
                    "sent": "Weight 0.",
                    "label": 0
                },
                {
                    "sent": "On the ones that are not inside are so they go away and it's averaging.",
                    "label": 0
                },
                {
                    "sent": "The reasonable positives and then minus the average reasonable negatives to give you this this gap.",
                    "label": 0
                },
                {
                    "sent": "So in the nice thing is this weight vector has aloe L1 length because it's the L1 length is 2, you have 1 / N plus on all the N plus reasonable pauses.",
                    "label": 0
                },
                {
                    "sent": "That adds up to one and negative 1 / N minus all the N minus reasonable negatives up to minus one before L1 is it.",
                    "label": 0
                },
                {
                    "sent": "Absolute value says another one, so length to margin, roughly gamma, maybe gamma over 2, 'cause we're estimating these two.",
                    "label": 0
                },
                {
                    "sent": "So we have a nice.",
                    "label": 0
                },
                {
                    "sent": "Separator in L1, so that's great, so so the the algorithm suggested by this definition is do this empirical similarity map.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in this space, use an algorithm for learning when there's a good L1 margin when your separator.",
                    "label": 0
                },
                {
                    "sent": "So maybe the window algorithm, or just direct L1 optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so this condition.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "We can learn and the sample size you need.",
                    "label": 0
                },
                {
                    "sent": "You need enough unlabeled points, enough landmarks to be able to see enough reasonable points.",
                    "label": 0
                },
                {
                    "sent": "A number of labeled points you need is the usual one over gamma squared kind of term, and then log of the.",
                    "label": 0
                },
                {
                    "sent": "Dimension of this space because we're doing L1, which gets you that log?",
                    "label": 0
                },
                {
                    "sent": "So OK, so the number of labeled examples is roughly one over the margin squared, one over the epsilon.",
                    "label": 0
                },
                {
                    "sent": "The accuracy looking for.",
                    "label": 0
                },
                {
                    "sent": "OK, so this definition is sufficient to learn well using this kind of empirical similarity map followed by an L1 optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, so so now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And I claim that some similar functions are good.",
                    "label": 0
                },
                {
                    "sent": "Kernel is also satisfies this definition, although the gammas get squared, so there's a little bit of loss when you translate over from a large margin kernel to a good similarity function.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the other hand you can also show a separation.",
                    "label": 1
                },
                {
                    "sent": "You can show there exists classes that have similarity functions satisfying this definition, but don't have large margin kernels that don't have, so you can have a class of functions where there's no kernel that's simultaneously large margin for all functions in the class.",
                    "label": 0
                },
                {
                    "sent": "But you can have a similarity function that's simultaneously a good similarity function.",
                    "label": 1
                },
                {
                    "sent": "Satisfying this definition for all functions in the class you get kind of nice separation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in some sense we can think of this as it gives one way of justifying a very natural algorithmic rule, which is do this kind of pickle similarity map optimizer.",
                    "label": 0
                },
                {
                    "sent": "Good OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we get to multiple similarity functions, so one other thing that's nice is now the following problem becomes very very easy.",
                    "label": 1
                },
                {
                    "sent": "So imagine the following setting.",
                    "label": 0
                },
                {
                    "sent": "Imagine we have a bunch of similarity functions.",
                    "label": 0
                },
                {
                    "sent": "They are of them, and we're hoping that some unknown convex combination of them is good.",
                    "label": 0
                },
                {
                    "sent": "Right, that's just like kind of thing with Chrome, so with some.",
                    "label": 0
                },
                {
                    "sent": "Collection summary functions we're hoping at some convex combination is good, so how can we do learning?",
                    "label": 0
                },
                {
                    "sent": "So here's a really, really easy way to do learning.",
                    "label": 0
                },
                {
                    "sent": "Take your landmarks as before, and then just concatenate the feature empirical similarity map using similarity function one.",
                    "label": 1
                },
                {
                    "sent": "The empirical oops.",
                    "label": 0
                },
                {
                    "sent": "I interleaved it this way.",
                    "label": 0
                },
                {
                    "sent": "OK, this is landmark.",
                    "label": 0
                },
                {
                    "sent": "One empirical similarity by similarity function.",
                    "label": 0
                },
                {
                    "sent": "One similarity function to up to similarity function R Alright Landmark 2 the same thing.",
                    "label": 0
                },
                {
                    "sent": "Landmark is just concatenating.",
                    "label": 0
                },
                {
                    "sent": "The map you would get using similar function one the map would get using similar function to adopt A function R. So it's just.",
                    "label": 0
                },
                {
                    "sent": "We just made the space instead of D length.",
                    "label": 0
                },
                {
                    "sent": "It's D times are.",
                    "label": 0
                },
                {
                    "sent": "What's really nice is because.",
                    "label": 0
                },
                {
                    "sent": "And then just run the same L1 optimization algorithm we had before.",
                    "label": 0
                },
                {
                    "sent": "What's nice is that because we're talking L1 margins, the margin has not gotten worse.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At all, so the claim is in this space with high probability, there is a linear separator with the same L1 margin we have with just one similarity function.",
                    "label": 0
                },
                {
                    "sent": "So the sample complexity only goes up by the dimension of the space which the log of the demand.",
                    "label": 0
                },
                {
                    "sent": "It's only going up in this log term, so we only have a log rhythmic penalty in how many similarity functions we're using.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's only a log rhythmic penalty as we add more and more similarity functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is it that the margin didn't change?",
                    "label": 0
                },
                {
                    "sent": "So here's the reason.",
                    "label": 0
                },
                {
                    "sent": "So I think this is kind of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You hear so.",
                    "label": 0
                },
                {
                    "sent": "Imagine the mapping.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we knew what that convex combination of similar functions was.",
                    "label": 1
                },
                {
                    "sent": "Let me call that K. 0 So KO as the magic convex combination we're shooting for and imagine we did the mapping using that one.",
                    "label": 0
                },
                {
                    "sent": "Well, we know there is some goods.",
                    "label": 0
                },
                {
                    "sent": "Good vector of Goodell.",
                    "label": 0
                },
                {
                    "sent": "One margin in that space.",
                    "label": 0
                },
                {
                    "sent": "Now we're not doing that.",
                    "label": 0
                },
                {
                    "sent": "That mapping 'cause we don't know what that similarity function is.",
                    "label": 0
                },
                {
                    "sent": "But just consider that weight vector.",
                    "label": 0
                },
                {
                    "sent": "The one that's good in that space and just expand it out.",
                    "label": 0
                },
                {
                    "sent": "Expand out so.",
                    "label": 0
                },
                {
                    "sent": "So take W 1.",
                    "label": 0
                },
                {
                    "sent": "The first coordinate there and just replace it with with A1, W 1, A2, W 1, DA R. So just take that each component and just expand it out this way.",
                    "label": 0
                },
                {
                    "sent": "So we just stretched it out, keeping the same L1 value.",
                    "label": 0
                },
                {
                    "sent": "We just stick W one, just we split it into little pieces and we take the second quarter W2 and split into little pieces.",
                    "label": 0
                },
                {
                    "sent": "Split by these alphas and the claim is that this new expand vector W died with what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Here is the same as.",
                    "label": 0
                },
                {
                    "sent": "This W 0 down with the this mapping F0 so we just spread it out.",
                    "label": 0
                },
                {
                    "sent": "We didn't change the L1 margin at all so we really don't have that much penalty for having more similarity functions so it's kind of interesting.",
                    "label": 1
                },
                {
                    "sent": "This is a nice thing without one margin.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so so.",
                    "label": 0
                },
                {
                    "sent": "Saying there's interesting fact here because our property was defined in terms of L1.",
                    "label": 0
                },
                {
                    "sent": "There's no change in margin, not too much penalty for concatenating our feature spaces.",
                    "label": 0
                },
                {
                    "sent": "And then if we had an L2 margin, things got bigger, the margin would drop by a factor square root of R and we would get this kind of a much bigger penalty.",
                    "label": 0
                },
                {
                    "sent": "So that's what that's nice with L1.",
                    "label": 0
                },
                {
                    "sent": "The album is very simple, we just concatenate.",
                    "label": 0
                },
                {
                    "sent": "Now alternative algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can do a joint optimization, you could just try to directly solve for this convex combination.",
                    "label": 0
                },
                {
                    "sent": "This this case you are looking for and the vector W combined.",
                    "label": 0
                },
                {
                    "sent": "So there's a good L1 margin and actually the bound we just gave implies that this algorithm would also have good sample complexity as well.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we don't know how to do this efficiently, unlike with kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one of the open questions.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do that efficiently for general similarity functions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also sending large margin kernel also satisfies this condition.",
                    "label": 0
                },
                {
                    "sent": "It means that you can you can only get just a log rhythmic penalty for kernels if you just just do this, although are in the worst case, we do have potentially a square in the gamma parameter.",
                    "label": 0
                },
                {
                    "sent": "So so doing this potentially there's a margin gets worse but you don't have this penalty for how many similarity functions you're complaining.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So open questions.",
                    "label": 0
                },
                {
                    "sent": "So one thing, one nice open questions.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to efficiently deal with.",
                    "label": 0
                },
                {
                    "sent": "General convex classes of similarity function, so this is particular to a convex combination of some number, but we don't know how to deal with a more general convex class.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how to do this concatenation then if I just give you a general convex class.",
                    "label": 0
                },
                {
                    "sent": "Does not just something of this form.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to efficiently implement a direct joint optimization for a convex combination of similarity functions.",
                    "label": 0
                },
                {
                    "sent": "Alternately, perhaps you can use the concatenation algorithm to extract a good convex combination.",
                    "label": 0
                },
                {
                    "sent": "Not sure.",
                    "label": 0
                },
                {
                    "sent": "Also, these are two quite different styles of algorithm where you just concatenate when you try to directly optimize.",
                    "label": 0
                },
                {
                    "sent": "So interesting to think what might be in between there.",
                    "label": 0
                },
                {
                    "sent": "And finally it seems there's potential for using this for transfer learning, where you use the weights for one problem for some other problem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enjoy your party.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Do you have any thoughts on how to use your setting?",
                    "label": 0
                },
                {
                    "sent": "Because you wouldn't know Gamma rry to preauricular.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So what do you do if you don't know like say the margins appear?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The nice thing with the L1 margins, you don't have a huge penalty for adding extra features, so the question.",
                    "label": 0
                },
                {
                    "sent": "So the gamma comes into how many landmarks do you need?",
                    "label": 0
                },
                {
                    "sent": "Me or the size of our so the other part is just an L1 optimization.",
                    "label": 0
                },
                {
                    "sent": "You can do that so so the one part where knowing something comes in is how many landmarks do you need.",
                    "label": 0
                },
                {
                    "sent": "Luckily there's only a log rhythmic penalty number of landmarks so you could.",
                    "label": 0
                },
                {
                    "sent": "You know, hope to just make some guess.",
                    "label": 0
                },
                {
                    "sent": "I mean one thing I don't, so the theory just says.",
                    "label": 0
                },
                {
                    "sent": "Pick them at random.",
                    "label": 0
                },
                {
                    "sent": "Presumably there are better things to do than pick them at random, but it seems very hard theoretically to say anything about that.",
                    "label": 0
                },
                {
                    "sent": "You can have an unpolished.",
                    "label": 0
                },
                {
                    "sent": "Would be 1 alternative would be to use.",
                    "label": 0
                },
                {
                    "sent": "Implicit feature map.",
                    "label": 0
                },
                {
                    "sent": "And then some sample to the subset of points.",
                    "label": 0
                },
                {
                    "sent": "If you have too many training and Apple could do what he did so so the.",
                    "label": 0
                },
                {
                    "sent": "What you do differently than the L1 margin on top of that settlement at the end, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, right, so where do you see the advantage of doing that?",
                    "label": 0
                },
                {
                    "sent": "Learning a linear combination instead of multiple learning about this so?",
                    "label": 0
                },
                {
                    "sent": "I. OK, so you mean?",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I'm not sure, so the advantage of using L1 versus L2 or the advantage of having this kind of notion of goodness of similarity function.",
                    "label": 0
                },
                {
                    "sent": "The same definition.",
                    "label": 0
                },
                {
                    "sent": "Function gives you kind of kernel which is then followed by construction and then you do the L1 instead of the Mail.",
                    "label": 0
                },
                {
                    "sent": "Where do you see that manager one?",
                    "label": 0
                },
                {
                    "sent": "I think they all one procedure is nice because.",
                    "label": 0
                },
                {
                    "sent": "Then you're a lot less sensitive to how many?",
                    "label": 0
                },
                {
                    "sent": "How many features you have?",
                    "label": 0
                },
                {
                    "sent": "Yeah once one.",
                    "label": 0
                },
                {
                    "sent": "Several.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You just map on your points to the values of the code and this gives you exactly the same as the indent marks.",
                    "label": 0
                },
                {
                    "sent": "You get econometrics, but one resident.",
                    "label": 0
                },
                {
                    "sent": "And why do L1 afterwards instead of just in case because you have to do that, you have to do the L1 rather than two, and those get these grantees.",
                    "label": 0
                },
                {
                    "sent": "Real function is good is supporting these definitions for every function.",
                    "label": 0
                },
                {
                    "sent": "Then you'll have to do the L1 in order to get the learning guarantees.",
                    "label": 0
                },
                {
                    "sent": "Constructed this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you did Elton Netspace.",
                    "label": 0
                },
                {
                    "sent": "There's no one.",
                    "label": 0
                },
                {
                    "sent": "Then you'd get significantly worse guarantees based on the assumptions and that.",
                    "label": 0
                },
                {
                    "sent": "Yes, uh, so you get the L2 margin gets worse, but the L1 margin doesn't.",
                    "label": 0
                },
                {
                    "sent": "So that maybe we can talk about if I Mail it.",
                    "label": 0
                },
                {
                    "sent": "So I went to Princeton.",
                    "label": 0
                },
                {
                    "sent": "We were able to minimize the weights that we get, the better performance or better mountains and I just got an 80.",
                    "label": 0
                },
                {
                    "sent": "So in your case.",
                    "label": 0
                },
                {
                    "sent": "Type me if if we could efficiently in this case.",
                    "label": 0
                },
                {
                    "sent": "If we could do.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually learn the weight.",
                    "label": 0
                },
                {
                    "sent": "The capacity would be smaller, so the bounds mean you ought to have better generalization, but we just.",
                    "label": 0
                },
                {
                    "sent": "You know in here we have a very simple, efficient algorithm with the first case, and we're not sure how to do this efficiently in this case, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you definitely gets better results, but.",
                    "label": 0
                },
                {
                    "sent": "The bitter, bitter bills that are in Mexico Minister we definitely better.",
                    "label": 0
                },
                {
                    "sent": "But it will be only very slightly better, because so that's a point distinction, because here in in the L1 case, so or in the similarity case, if you use that information then you didn't get a worse than next week, but actually it's not done much worse because you only get to the.",
                    "label": 0
                },
                {
                    "sent": "What you say is only a little bit effective lovedean.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you could say that after maybe that'd be, but in the in the words in the criminal case you'd pay much bigger price in the ministry.",
                    "label": 0
                },
                {
                    "sent": "So so you can know them in next week would be here worse.",
                    "label": 0
                },
                {
                    "sent": "You're not losing quite as much as you're losing, so this procedure for the 90 cases much more sensible than the pigmentation.",
                    "label": 0
                },
                {
                    "sent": "Oh forgive.",
                    "label": 0
                },
                {
                    "sent": "Super.",
                    "label": 0
                },
                {
                    "sent": "You can choose.",
                    "label": 0
                },
                {
                    "sent": "Discuss.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good question then.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Log R or.",
                    "label": 0
                },
                {
                    "sent": "You are giving bounds that show that you can have instead imperative.",
                    "label": 0
                },
                {
                    "sent": "So when you ask naughty this afternoon.",
                    "label": 0
                },
                {
                    "sent": "Black",
                    "label": 0
                }
            ]
        }
    }
}