{
    "id": "hn2lwkffwsiwbw6qmjypipto6kaouiym",
    "title": "FastXML: A Fast, Accurate and Stable Tree-classifier for eXtreme Multi-label Learning",
    "info": {
        "author": [
            "Yashoteja Prabhu, Indian Institute of Technology Delhi"
        ],
        "published": "Oct. 8, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_prabhu_fast_xml/",
    "segmentation": [
        [
            "My name is Sasha, I'ma pages to render tight Italy.",
            "Our paper is about fast XML, a new algorithm for the fast and scalable.",
            "Oh, is it not audible?",
            "OK.",
            "So our paper is about fast XML, a new algorithm for the fast and scalable learning of extreme multi label models."
        ],
        [
            "Multi Label learning refers to the task of assigning a set of relevant or categories or labels for a given instance.",
            "Extreme multi Label Learning is a kind of multi label learning problem where the number of labels can be potentially in millions.",
            "Able learning is a new paradigm which came into focus almost last year and it allows the reformulation of code, machine learning tasks like ranking and recommendation."
        ],
        [
            "Our our approach to ranking and recommendation task involves learning a function F that Maps an instance X to a subset of relevant items from a large item set, why?",
            "And then given a new user will use the learn function to recommend a set of items relevant to him or her."
        ],
        [
            "This approach is different from many traditional approaches to ranking and recommendation, which learn a joint function over both the user and an item indicating how relevant the item is to the user.",
            "Are making recommendation would then involve evaluating the joint function on the user paid once with each of the million items.",
            "This makes the recommendation task very slow, unnecessary.",
            "Its use of some clever heuristics to downsample the item set."
        ],
        [
            "Our approach is also different from other traditional class of techniques for recommendation tasks which resort to factorization of a partially filled user item relevance matrix.",
            "The missing entries would then be recovered simply by multiplying the factors corresponding to users and items.",
            "Scaling the factorization to millions of users and items would need expensive, expensive computational resources.",
            "Also, the users with no previous ratings need to be handled separately."
        ],
        [
            "Recently some novel or extreme multi label algorithms have been proposed and successfully applied to ranking and recommendation tasks in our our www 2013 paper.",
            "We proposed an algorithm called Multi Label Random Forest operated as a Miller if.",
            "For extreme by table learning, we applied the algorithm to task of recommending queries for a given web page and demonstrated that it obtains higher pressure and recall compared to traditional methods.",
            "And makes reasonable recommendations even for web pages containing very little text."
        ],
        [
            "In their eyes, May 2013 paper Western western Macarian.",
            "He proposed a new algorithm for extreme multi label learning called label Partitioning for Sub Linear Ranking."
        ],
        [
            "Abbreviated as LPSR.",
            "They applied a PSR to a task of YouTube video recommendation and showed that it drastically cuts down the prediction time compared to traditional anchors while also improving prediction accuracy.",
            "Armillary fondell PSR have many advantages.",
            "They make predictions quickly within milliseconds.",
            "They have been shown to scale to a large number of labels.",
            "Miller F. 210 million and PSR to 1 million.",
            "They award the extremely costly manually manual annotation by generating training data automatically.",
            "A Miller, if also proposes a few techniques to handle issues of missing and noisy labels.",
            "Further, both methods outperform traditional.",
            "Methods in terms of prediction speed and accuracy."
        ],
        [
            "Emory Ferndale PSR also have some serious limitations.",
            "They're slow during training.",
            "I need expensive distributed computing resources to scale to large datasets.",
            "Both Miller and LP S are techniques are tree based and they tend to grow imbalanced trees.",
            "That's hurting prediction performance.",
            "Also, there is a scope for improvement in prediction accuracy as we demonstrate through our experiments."
        ],
        [
            "In our fast XML algorithm we address the limitations of embarrassment PSR while retaining their advantages or main novelty is a new node partitioning objective which is ranked sensitive and directly optimizes the indices ranking metric.",
            "We also propose a fast alternating minimization algorithm for efficiently optimizing the objective.",
            "Fast XML learns balanced trees and improves prediction accuracy over Emily refundable PSR.",
            "In our experiments, we demonstrate that fast XML can train on a million labeled data set in under 9 hours on a single box, and obtains an accuracy gain of vital 20% over baselines."
        ],
        [
            "Tree based methods are very well suited for extreme multi label learning.",
            "In fact Miller SLPSR and fast XML all three are tree base.",
            "The main idea in a tree based method is to learn a hierarchy or the labels while ensuring that approximately half of the labels in our node go to each node.",
            "It's allowed this kind of balanced partitioning and shows that the tree depth, and consequently prediction time is logarithmic in the number of labels."
        ],
        [
            "In fast XML we learn an ensemble of such trees, each tree learning a different hierarchy compared to others.",
            "When a novel test instance comes in, we drop it down.",
            "Each of these trees down to leaf nodes.",
            "The leaf nodes contain a label, probability distribution or small set of labels active in them.",
            "To make final predictions, we aggregate the probability distributions over all the different trees and ranking rank the items based on the final probability scores.",
            "The main difference between the three extreme multi label methods is in the criterias.",
            "Was splitting a node Miller?",
            "If optimizes our multi label variant of Gini index while PSR uses simple K means clustering over the feature space, neither of them directly optimize the ranking loss which is of interest to us in ranking task.",
            "In fast XML our objective directly minimizes and this is the loss.",
            "Thus ensuring a good ranking."
        ],
        [
            "Now I'll illustrate the node positioning mechanism of Aztec symbol with a simple example or consider a node containing 10 different users.",
            "The users line in a 2 dimensional feature space as shown."
        ],
        [
            "Each user is associated with a set of roots.",
            "They like the Union of all these sets, comprise active fruit site in the node."
        ],
        [
            "The node partitioning algorithm begins with a random allotment of users to left or the right child with equal probability.",
            "Here the left child is a color coded blue and the right child is color coded pink.",
            "In the equation, Delta eyes are binary variables taking values plus one or minus one indicating which child the instance belongs to."
        ],
        [
            "In the next step."
        ],
        [
            "We rank the labels in the Left child according to their frequency of occurrence among the instances of the Left child.",
            "Similarly, we rank the labels in the right child.",
            "In practice, instances associated with a large number of labels tend to have larger influence than once with fewer labels.",
            "Hence, we normalize the contribution of different instances using weights NY as shown in the equation."
        ],
        [
            "In the next step we re re allot each instance to the node whose ranking gives the instance of higher indices gain.",
            "In this example oranges and pomegranates are ranked higher in the left child, so this instance should be moved to the left child."
        ],
        [
            "Similarly, all the remaining instances are re allotted to obtain this file final configuration."
        ],
        [
            "After this we do at another iteration of label ranking."
        ],
        [
            "An instance assignment.",
            "Now the instances have reached the stable state.",
            "Further iterations will not change the configuration."
        ],
        [
            "Within user instance allotments are binary labels and learn a linear separator in the feature space by solving the L1 regularize logistic regression as shown in the equation.",
            "The L1 regularizer specifies the feature quotients, this avoiding overfitting and also creating more compact models."
        ],
        [
            "The overall algorithm can be shown to minimize the unified objective shown at the top here."
        ],
        [
            "The node partitioning procedure is then applied recursively to the left child."
        ],
        [
            "And then to the right child node.",
            "And so."
        ],
        [
            "On the tree is terminated when each leaf node contains."
        ],
        [
            "A few instances."
        ],
        [
            "We conducted our experiments on a three large.",
            "They are scale datasets with up to a million labels.",
            "Vicki L Assistance is a cleaned up version of Wikipedia obtained from the Assist Easy Challenge website and it contains around 300,000 labels.",
            "Add 4:30 K and adds 1 million I proprietary datasets.",
            "And contain around 400,001 million labels respectively.",
            "In order to benchmark the performances of different methods in various label regimes.",
            "We also report results on four smaller datasets.",
            "Delicious Medium will RC 1X and Webtech when label sets ranging from hundreds to thousands of labels."
        ],
        [
            "Our baselines eidemiller FL PSR and once all assume all this methods were implemented in C and executed on the 3.3 GHz single core desktop.",
            "Well, the parameters for baselines were tuned rigorously using a validation set.",
            "Fast XML was executed using fixed parameter setting.",
            "On the four small size datasets for similar was found to be consistently more accurate compared to the three baselines.",
            "Fastic similar achieved a gain of up to 8% and 3% in terms of pressure at 5 compared to LP, Sr and Miller respectively.",
            "On the smallest data set big tech, the gains obtained by Untuned Fast XML were minimal, but could go up to 1% on fine tuning the fast XML para meters."
        ],
        [
            "On the large datasets, MLR FPS and one versus all baselines were too slow to be trained on it to stop.",
            "Hence we use a simple variant of LPS are called LP S Our neighbors as our baseline on large datasets on VLCC Fast XML took around 9 hours while being 22% more accurate than LP S our neighbors.",
            "In terms of position at one."
        ],
        [
            "On at 4:30 K training time for four 6 meters within two hours, while again impression at one was around 8%."
        ],
        [
            "On ads 1 million training time for Fast XML was around 8 hours, while the guy impression at one was around 6%."
        ],
        [
            "Fast XML training can be trivially paralyzed by growing each tree in dependently on a different core.",
            "We trained fast XML trees or up to 16 cores in parallel and obtain the speed of up to 13 times while using 16 to 16 cores fast.",
            "XML took only about 30 minutes to try, known as 1,000,000."
        ],
        [
            "We studied the effect of multiple updates to W on ads for 30K data set and found that more than one iteration gives minimal gains in positions while considerably increasing the training time.",
            "So it is safe to terminate node optimization after one update to W."
        ],
        [
            "The trees grown by fast XML were found to be very well balanced while the trees grown by Miller and help server imbalanced, the imbalance in LPSR trees were found to increase with larger datasets and sparser features."
        ],
        [
            "In order to understand the importance of NDC term in the objective.",
            "We also tried out different variants of fast XML by replacing indices by indecision at 5 and precision at 5.",
            "And this is yet, and this is optimization consistently outperforms these variants on both small."
        ],
        [
            "As well as large datasets."
        ],
        [
            "With more and more trees, for example, tends to give lesser and lesser gains in positions, we found that on most datasets 30 trees are enough to time close to optimum precisions."
        ],
        [
            "To conclude, fast XML improves position by around 20% or the state of the art and can be trained efficiently on up to a million labeled data set on a single code desktop within a few hours.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Sasha, I'ma pages to render tight Italy.",
                    "label": 0
                },
                {
                    "sent": "Our paper is about fast XML, a new algorithm for the fast and scalable.",
                    "label": 1
                },
                {
                    "sent": "Oh, is it not audible?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So our paper is about fast XML, a new algorithm for the fast and scalable learning of extreme multi label models.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multi Label learning refers to the task of assigning a set of relevant or categories or labels for a given instance.",
                    "label": 0
                },
                {
                    "sent": "Extreme multi Label Learning is a kind of multi label learning problem where the number of labels can be potentially in millions.",
                    "label": 0
                },
                {
                    "sent": "Able learning is a new paradigm which came into focus almost last year and it allows the reformulation of code, machine learning tasks like ranking and recommendation.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our our approach to ranking and recommendation task involves learning a function F that Maps an instance X to a subset of relevant items from a large item set, why?",
                    "label": 0
                },
                {
                    "sent": "And then given a new user will use the learn function to recommend a set of items relevant to him or her.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This approach is different from many traditional approaches to ranking and recommendation, which learn a joint function over both the user and an item indicating how relevant the item is to the user.",
                    "label": 1
                },
                {
                    "sent": "Are making recommendation would then involve evaluating the joint function on the user paid once with each of the million items.",
                    "label": 0
                },
                {
                    "sent": "This makes the recommendation task very slow, unnecessary.",
                    "label": 0
                },
                {
                    "sent": "Its use of some clever heuristics to downsample the item set.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our approach is also different from other traditional class of techniques for recommendation tasks which resort to factorization of a partially filled user item relevance matrix.",
                    "label": 0
                },
                {
                    "sent": "The missing entries would then be recovered simply by multiplying the factors corresponding to users and items.",
                    "label": 0
                },
                {
                    "sent": "Scaling the factorization to millions of users and items would need expensive, expensive computational resources.",
                    "label": 0
                },
                {
                    "sent": "Also, the users with no previous ratings need to be handled separately.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recently some novel or extreme multi label algorithms have been proposed and successfully applied to ranking and recommendation tasks in our our www 2013 paper.",
                    "label": 0
                },
                {
                    "sent": "We proposed an algorithm called Multi Label Random Forest operated as a Miller if.",
                    "label": 1
                },
                {
                    "sent": "For extreme by table learning, we applied the algorithm to task of recommending queries for a given web page and demonstrated that it obtains higher pressure and recall compared to traditional methods.",
                    "label": 0
                },
                {
                    "sent": "And makes reasonable recommendations even for web pages containing very little text.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In their eyes, May 2013 paper Western western Macarian.",
                    "label": 0
                },
                {
                    "sent": "He proposed a new algorithm for extreme multi label learning called label Partitioning for Sub Linear Ranking.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Abbreviated as LPSR.",
                    "label": 0
                },
                {
                    "sent": "They applied a PSR to a task of YouTube video recommendation and showed that it drastically cuts down the prediction time compared to traditional anchors while also improving prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "Armillary fondell PSR have many advantages.",
                    "label": 0
                },
                {
                    "sent": "They make predictions quickly within milliseconds.",
                    "label": 0
                },
                {
                    "sent": "They have been shown to scale to a large number of labels.",
                    "label": 1
                },
                {
                    "sent": "Miller F. 210 million and PSR to 1 million.",
                    "label": 0
                },
                {
                    "sent": "They award the extremely costly manually manual annotation by generating training data automatically.",
                    "label": 0
                },
                {
                    "sent": "A Miller, if also proposes a few techniques to handle issues of missing and noisy labels.",
                    "label": 1
                },
                {
                    "sent": "Further, both methods outperform traditional.",
                    "label": 0
                },
                {
                    "sent": "Methods in terms of prediction speed and accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Emory Ferndale PSR also have some serious limitations.",
                    "label": 0
                },
                {
                    "sent": "They're slow during training.",
                    "label": 0
                },
                {
                    "sent": "I need expensive distributed computing resources to scale to large datasets.",
                    "label": 0
                },
                {
                    "sent": "Both Miller and LP S are techniques are tree based and they tend to grow imbalanced trees.",
                    "label": 0
                },
                {
                    "sent": "That's hurting prediction performance.",
                    "label": 0
                },
                {
                    "sent": "Also, there is a scope for improvement in prediction accuracy as we demonstrate through our experiments.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our fast XML algorithm we address the limitations of embarrassment PSR while retaining their advantages or main novelty is a new node partitioning objective which is ranked sensitive and directly optimizes the indices ranking metric.",
                    "label": 1
                },
                {
                    "sent": "We also propose a fast alternating minimization algorithm for efficiently optimizing the objective.",
                    "label": 1
                },
                {
                    "sent": "Fast XML learns balanced trees and improves prediction accuracy over Emily refundable PSR.",
                    "label": 1
                },
                {
                    "sent": "In our experiments, we demonstrate that fast XML can train on a million labeled data set in under 9 hours on a single box, and obtains an accuracy gain of vital 20% over baselines.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tree based methods are very well suited for extreme multi label learning.",
                    "label": 1
                },
                {
                    "sent": "In fact Miller SLPSR and fast XML all three are tree base.",
                    "label": 0
                },
                {
                    "sent": "The main idea in a tree based method is to learn a hierarchy or the labels while ensuring that approximately half of the labels in our node go to each node.",
                    "label": 0
                },
                {
                    "sent": "It's allowed this kind of balanced partitioning and shows that the tree depth, and consequently prediction time is logarithmic in the number of labels.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fast XML we learn an ensemble of such trees, each tree learning a different hierarchy compared to others.",
                    "label": 0
                },
                {
                    "sent": "When a novel test instance comes in, we drop it down.",
                    "label": 0
                },
                {
                    "sent": "Each of these trees down to leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "The leaf nodes contain a label, probability distribution or small set of labels active in them.",
                    "label": 0
                },
                {
                    "sent": "To make final predictions, we aggregate the probability distributions over all the different trees and ranking rank the items based on the final probability scores.",
                    "label": 0
                },
                {
                    "sent": "The main difference between the three extreme multi label methods is in the criterias.",
                    "label": 0
                },
                {
                    "sent": "Was splitting a node Miller?",
                    "label": 0
                },
                {
                    "sent": "If optimizes our multi label variant of Gini index while PSR uses simple K means clustering over the feature space, neither of them directly optimize the ranking loss which is of interest to us in ranking task.",
                    "label": 0
                },
                {
                    "sent": "In fast XML our objective directly minimizes and this is the loss.",
                    "label": 0
                },
                {
                    "sent": "Thus ensuring a good ranking.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'll illustrate the node positioning mechanism of Aztec symbol with a simple example or consider a node containing 10 different users.",
                    "label": 0
                },
                {
                    "sent": "The users line in a 2 dimensional feature space as shown.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each user is associated with a set of roots.",
                    "label": 0
                },
                {
                    "sent": "They like the Union of all these sets, comprise active fruit site in the node.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The node partitioning algorithm begins with a random allotment of users to left or the right child with equal probability.",
                    "label": 0
                },
                {
                    "sent": "Here the left child is a color coded blue and the right child is color coded pink.",
                    "label": 0
                },
                {
                    "sent": "In the equation, Delta eyes are binary variables taking values plus one or minus one indicating which child the instance belongs to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next step.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We rank the labels in the Left child according to their frequency of occurrence among the instances of the Left child.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we rank the labels in the right child.",
                    "label": 0
                },
                {
                    "sent": "In practice, instances associated with a large number of labels tend to have larger influence than once with fewer labels.",
                    "label": 0
                },
                {
                    "sent": "Hence, we normalize the contribution of different instances using weights NY as shown in the equation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next step we re re allot each instance to the node whose ranking gives the instance of higher indices gain.",
                    "label": 0
                },
                {
                    "sent": "In this example oranges and pomegranates are ranked higher in the left child, so this instance should be moved to the left child.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, all the remaining instances are re allotted to obtain this file final configuration.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After this we do at another iteration of label ranking.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An instance assignment.",
                    "label": 0
                },
                {
                    "sent": "Now the instances have reached the stable state.",
                    "label": 0
                },
                {
                    "sent": "Further iterations will not change the configuration.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Within user instance allotments are binary labels and learn a linear separator in the feature space by solving the L1 regularize logistic regression as shown in the equation.",
                    "label": 0
                },
                {
                    "sent": "The L1 regularizer specifies the feature quotients, this avoiding overfitting and also creating more compact models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The overall algorithm can be shown to minimize the unified objective shown at the top here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The node partitioning procedure is then applied recursively to the left child.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then to the right child node.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the tree is terminated when each leaf node contains.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A few instances.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We conducted our experiments on a three large.",
                    "label": 0
                },
                {
                    "sent": "They are scale datasets with up to a million labels.",
                    "label": 0
                },
                {
                    "sent": "Vicki L Assistance is a cleaned up version of Wikipedia obtained from the Assist Easy Challenge website and it contains around 300,000 labels.",
                    "label": 0
                },
                {
                    "sent": "Add 4:30 K and adds 1 million I proprietary datasets.",
                    "label": 0
                },
                {
                    "sent": "And contain around 400,001 million labels respectively.",
                    "label": 0
                },
                {
                    "sent": "In order to benchmark the performances of different methods in various label regimes.",
                    "label": 0
                },
                {
                    "sent": "We also report results on four smaller datasets.",
                    "label": 0
                },
                {
                    "sent": "Delicious Medium will RC 1X and Webtech when label sets ranging from hundreds to thousands of labels.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our baselines eidemiller FL PSR and once all assume all this methods were implemented in C and executed on the 3.3 GHz single core desktop.",
                    "label": 0
                },
                {
                    "sent": "Well, the parameters for baselines were tuned rigorously using a validation set.",
                    "label": 0
                },
                {
                    "sent": "Fast XML was executed using fixed parameter setting.",
                    "label": 0
                },
                {
                    "sent": "On the four small size datasets for similar was found to be consistently more accurate compared to the three baselines.",
                    "label": 0
                },
                {
                    "sent": "Fastic similar achieved a gain of up to 8% and 3% in terms of pressure at 5 compared to LP, Sr and Miller respectively.",
                    "label": 0
                },
                {
                    "sent": "On the smallest data set big tech, the gains obtained by Untuned Fast XML were minimal, but could go up to 1% on fine tuning the fast XML para meters.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the large datasets, MLR FPS and one versus all baselines were too slow to be trained on it to stop.",
                    "label": 0
                },
                {
                    "sent": "Hence we use a simple variant of LPS are called LP S Our neighbors as our baseline on large datasets on VLCC Fast XML took around 9 hours while being 22% more accurate than LP S our neighbors.",
                    "label": 0
                },
                {
                    "sent": "In terms of position at one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On at 4:30 K training time for four 6 meters within two hours, while again impression at one was around 8%.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On ads 1 million training time for Fast XML was around 8 hours, while the guy impression at one was around 6%.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fast XML training can be trivially paralyzed by growing each tree in dependently on a different core.",
                    "label": 0
                },
                {
                    "sent": "We trained fast XML trees or up to 16 cores in parallel and obtain the speed of up to 13 times while using 16 to 16 cores fast.",
                    "label": 0
                },
                {
                    "sent": "XML took only about 30 minutes to try, known as 1,000,000.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We studied the effect of multiple updates to W on ads for 30K data set and found that more than one iteration gives minimal gains in positions while considerably increasing the training time.",
                    "label": 0
                },
                {
                    "sent": "So it is safe to terminate node optimization after one update to W.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The trees grown by fast XML were found to be very well balanced while the trees grown by Miller and help server imbalanced, the imbalance in LPSR trees were found to increase with larger datasets and sparser features.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to understand the importance of NDC term in the objective.",
                    "label": 0
                },
                {
                    "sent": "We also tried out different variants of fast XML by replacing indices by indecision at 5 and precision at 5.",
                    "label": 0
                },
                {
                    "sent": "And this is yet, and this is optimization consistently outperforms these variants on both small.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well as large datasets.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With more and more trees, for example, tends to give lesser and lesser gains in positions, we found that on most datasets 30 trees are enough to time close to optimum precisions.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude, fast XML improves position by around 20% or the state of the art and can be trained efficiently on up to a million labeled data set on a single code desktop within a few hours.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}