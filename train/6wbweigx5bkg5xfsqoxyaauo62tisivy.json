{
    "id": "6wbweigx5bkg5xfsqoxyaauo62tisivy",
    "title": "Comparison of distances for multi-label classification with PCTs",
    "info": {
        "author": [
            "Valentin Gjorgjioski, Department of Knowledge Technologies, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 4, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Multi-Task Learning"
        ]
    },
    "url": "http://videolectures.net/sikdd2011_gjorgjioski_classification/",
    "segmentation": [
        [
            "Good afternoon, my name is Valentina Jessica and I'm going to present today the.",
            "Topic comparison of distances for multi label classification with the cities.",
            "This is recent work with my colleagues, Dragon Quarter and."
        ],
        [
            "At the beginning I will give a short outline how this talk will.",
            "Go on so the first stage I will give a short introduction about the predictive clustering trees about the distances for structured data.",
            "Enter the whole idea of this work.",
            "Next I will talk talk a bit more about predictive clustering trees.",
            "The concept and how they work.",
            "Further on I will.",
            "I will introduce the distances that we have interested in this work and used to used in this work to compare.",
            "And towards the end I will continue with the experimental design and the results that we've obtained from the work and at the end, of course, the conclusions from this work."
        ],
        [
            "So to start with the introduction, the first question is what we want to do in this work and answer this question is we want to do multi label classification.",
            "Or more specifically we want to test some distances and to compare these distances on how they perform when used to when we do multi label classification, how we're going to do this so?",
            "In the literature, we know two approaches, how we how we do multi label classification.",
            "The first approaches problem transformation and this means that the problem itself from multi label classification problem is transformed to a problem of regular classification and then either multiple classifiers are build it or some other approaches taken to solve the multi label classification problem the next.",
            "So the next approach which is found so far is algorithm adaptation, which means that algorithms are changed and adapted to handle this kind of problems.",
            "In this work we will focus on the 2nd on the 2nd.",
            "Approach and we will show how we do an adaptation of predictive clustering trees to handle multi label classification using distances and then of course we show comparison of these distances.",
            "So basically this so far is.",
            "Is done for many different data types and this can be, let's say multiple targets or time series for his.",
            "And now our aim in this work was to to extend the P cities to to work on the sets.",
            "For this, for this purpose we are using several distances that are defined on the sets or that are specifically designed for handling multi label data, and these distances are named here.",
            "Hamming distance, Jaccard distance and matching distance."
        ],
        [
            "So, predictive clustering trees concept is implemented in the class framework and it is.",
            "It is based on the well known concept of top down induction decision trees.",
            "The idea here is that the dog rhythm is pretty similar as the top down reduction of decision trees with the change of that calculation of the heuristic.",
            "When we when we are doing the split in the top down in the regular top down induction decision trees, we are doing the split using information gain and here we are doing the split using various reduction that is big cause the predictive clustering trees are seeing the whole tree is.",
            "You have clusters groups.",
            "This we don't want to do.",
            "No, I know.",
            "OK.",
            "So, uh.",
            "Yes, I.",
            "So the predictive clustering trees are seeing the whole 3 as a hierarchy of a clusters, which means all the data is put in the in the top node on the 1st cluster, and then the splits are done down.",
            "And on the way for in each step to do a split we calculate this variance reduction, which is which is the sum of all pairwise distances between the between the elements in the cluster and then the split that best that optimizes the best or reduces the best variance.",
            "We use that split and we continue on.",
            "The this process stops when when we when we do statistical tests and we show that such St cannot further improve the variance and then we create a leaf.",
            "Once we have created the tree, we need to find a prototype for each leave because we want to use these three for prediction and prototype usually is defined as an average of all elements in the in the leaf.",
            "But here when we have complex data with its.",
            "It's difficult, it's almost impossible to define average, let's say average between sets is not defined, and that's why we are using here Metroid and for prototype we're taking the the the element of the cluster that minimizes this sum that minimizes.",
            "Average because you haven't defined arithmetical operations on those sets, you can't some upsets and then divide by something.",
            "So it's the actual reason is because you are using discrete values and couple of.",
            "So what kind of vector so well, yes, actually these sets here in this talk actually vectors of nominal values, that's true.",
            "But in general case you can have sets of anything, so it's it's almost impossible to define.",
            "Average, so we're defining this as method, and that's the element that minimizes this soon so."
        ],
        [
            "OK, and for multi label classification.",
            "The the one adaptation which which is needed to be done of predictive clustering increases to to.",
            "To involve distances on set cause multi label classification we see the multi label classification problem on the target set we have multiple labels and this is actually set of labels.",
            "So if we introduce the introduce distances on sets then we can do predictive clustering trees for multi label learning.",
            "So we we have implemented several distances.",
            "The first distances Euclidean distance and now here is 1 interesting question, how Euclidean distance is defined on sets?",
            "Because we know that the Euclidean distance is actually defined on vectors.",
            "Both are here in Multi label classification and multi label learning we have for each example we have labels and we have.",
            "Set of this label and we can represent the each set as a tuple of 01 values.",
            "So when we represent each set as a vector, then we can use Euclidean distance on the vector representation of these sets and we can say that this is 1 defined distance on this set.",
            "Way across similarity, it's yes.",
            "OK. Also, coming distance is defined as the difference between the cardinality of the Union and cardinality of the intersection of two sets.",
            "If given two sets."
        ],
        [
            "Then the car distance.",
            "Oh it is a.",
            "The symmetrical differences.",
            "So it is the ratio between the symmetrical difference and the cardinality of the Union.",
            "And matching distance this distance is well known in the literature and used for sets.",
            "Here we have adopted this distance specifically for for this type of data, which are sets of nominals.",
            "And when we transform this matching distance, when we simplify this formula, we will come to this, so it's a difference between the maximum maximum cardinality.",
            "So the larger cardinality of the larger set minus the intersection of the cardinality of the intersection of two sets."
        ],
        [
            "So in this light I will give example of this distance.",
            "How are they calculated and what they mean?",
            "So if we have two sets, let's say set South of ABC and safety of AMD, then Euclidean distance of these two sets.",
            "If we have this vector representations given here, then the ability on distance is sqrt 3.",
            "Are humming distances 3 because we have the the symmetrical differences BC and D123?",
            "In Jakarta is 3 / 4, so the symmetrical difference over the union of these sets matching distance is shown in this example.",
            "So what is the matching distance in?",
            "In in fact it tries to match the equal element, or in general case the most poses the most closest element and then it calculates the distance between the elements that have no Metro have match, that is, that is not equal.",
            "And then we have some penalty for the elements that are not matched with any element.",
            "So if we if we.",
            "Calculate the metric distance.",
            "Here the metric distances 1 two this red lines calculated in the measure distance and when the element is same, then we're not calculating anything there.",
            "So we we are showing here that this distance is how they are calculated and we're showing how they how they are different from each other.",
            "It's it's interesting here to say that the Hamming distance, by definition is always the Euclidean squared or vice versa.",
            "Euclidean is square root of the Hamming distance."
        ],
        [
            "So there are some calculated.",
            "Once we have defined these distances, we have taken 6 datasets to test these distances and to test the approach of predictive clustering trees.",
            "We can say that this these datasets.",
            "They are somehow ordered in terms of their complexity and these distances are from different domains.",
            "Domains are ecology, music, text and so on.",
            "Although datasets are split to training and testing data set, we have various number of examples, various number of labels.",
            "And overall so this label cardinality various label cardinality actually is the number that somehow tells us how many labels for each example we have in in average and on the data set.",
            "So we have from from labels starting from 1.2, which means people up to.",
            "I don't know here.",
            "6.34 so.",
            "Examples in Enron case.",
            "Oh well, talk to me.",
            "You know females.",
            "The number of examples are whatever.",
            "I mean.",
            "What examples in this case and put the labels?",
            "I'm not sure about this.",
            "Mityana also.",
            "I don't know what are the levels in this.",
            "Maybe Sasha?",
            "Newspapers.",
            "Yeah, but.",
            "But in the case of Enron, what I mean you have.",
            "2000 users suffer millions of emails.",
            "Central, I don't know.",
            "This is something which is, so most of these so and Ronan Medical these datasets, which now quite extensively used to evaluate my PlayStation averages.",
            "So this is like getting the status of UCI where you take these and you test on those datasets.",
            "Hello.",
            "I think often.",
            "But this is, I mean everything.",
            "What I know about these datasets and nothing fits here.",
            "Yeah, it's probably a sample.",
            "Some small sample.",
            "Yes, I think that Enron is actually sample of this data set.",
            "It's not the whole data set I've written.",
            "The description of the data set that it's not the whole data set, but it's a sample taken for some reason and some prefiltering was done to this data set."
        ],
        [
            "So further on we have defined different evaluation measures that are commonly used in multi label classification.",
            "The the first one is given their accuracy, which is somehow an average of operations between the the intersection and union of the predicted and the real set.",
            "Or yes, the predicted in the real set of labels, so H. Office, I mean the predicted and why I means the real set and that's across older older evolution measures.",
            "Precision is defined as average of ratios of intersection and personality of the real set of labels, and so on.",
            "Here is interesting to .222 of this evaluation measure.",
            "The one is.",
            "4 score and the other one is subset accuracy.",
            "Therefore score is calculated as a.",
            "So the F1 score is calculated as a mean of precision and recall, and the subset accuracy which is.",
            "Somehow define touristic 'cause it takes in account only when the when the sets the predicted set and the.",
            "Predicted certain the real set are equal so it doesn't take into account any intersection or something, but or symmetrical difference, but it takes in account only when the window two sets is.",
            "Is equal."
        ],
        [
            "So we have set up this problem in that way that for each problem we have training and testing datasets.",
            "During the learning we have used pre pruning with F tests.",
            "We have selected the optimal significance level by three cross fold validation.",
            "Three fold cross validation from the from the following followers here.",
            "So from 0.01 to 0.05.",
            "And we compare the capacities with different measures used for learning.",
            "So we compared Euclidean, Hamming, Jakarta and matching distance.",
            "Enter this.",
            "This performance of all distances are somehow compared across the whole 6 evaluation measures which were previously diff."
        ],
        [
            "And.",
            "The result.",
            "So we have datasets in the roles and we have distances in the columns, so we have each one table for each evaluation measure.",
            "The first evaluation measure is coming close.",
            "Measure we can see here that humming performs best, which somehow is expected to cause it.",
            "Minimize.",
            "Minimize is exactly that.",
            "The coming close function.",
            "And also one interesting thing is to do mention here that Jakarta performs worse on on every data set in this in this case.",
            "The the next the next table is precision evolution measure.",
            "We can see here that there are some something very interesting cause the Jaccard measure performs best on three datasets and on the rest of the rest of the data set it performs worse, which is the first.",
            "The first is something that we can easily explain.",
            "Becausw Jakarta by definition is very color correlated.",
            "With the precision by definition, but but these results here somehow surprising and hard to hard to implement.",
            "Hard to explain.",
            "What is the statistical significance of what's the average variance here?",
            "So these numbers, I mean, they jump extremely a lot so.",
            "So it will be interesting to see what the actual well these numbers jump a lot, because here we show we are not showing relative measure but absolute one.",
            "So the precision is absolute measurement.",
            "These numbers cannot be compared across the data set but just just in the single line.",
            "Yeah OK, yeah."
        ],
        [
            "So the F1 score, which is somehow balance between the precision, is between the precision and recall.",
            "Also somehow that the Hamming is is the best.",
            "And, uh, then closely followed by matching distance and by Jakarta New Kleidion recall shows pretty the same as on the previous slide."
        ],
        [
            "So if you remember the coming close measure where we have coming was the best in Jakarta was the worse."
        ],
        [
            "Pretty same happens also to recall and here we can say that jakartan Euclidean somehow better than the rest and the cart is the worst distance measure in this case."
        ],
        [
            "But when we come to accuracy and to the subset accuracy, we were obtaining a bit different result and.",
            "Specifically in accuracy we can buy.",
            "We can say that the matching distance is the best distance if we do the ranks and then.",
            "If you do the ranks across the whole data set, then matching distance is the best when when compared by accuracy.",
            "And here it's interesting to point out that Euclidean distance is based on three datasets, but it is also worse in two datasets and this is we have checked this and this is due to Euclidean distance is working very good on this data set there where the number of labels is very very small or close to one.",
            "So if we go back to the.",
            "Flight with the datasets."
        ],
        [
            "We will see that Euclidean distance works best on.",
            "In measure of accuracy, performance as well based on emotions, Madonna and Medical where the where the label cardinalities very small or close to one.",
            "And when this label cardinality."
        ],
        [
            "Gross, then the Euclidean distance performs worst according to the accuracy and the last one is the subset accuracy.",
            "This, as I said, is very strict and it's easier to obtain even numbers like 0 in Jakarta, 'cause we only count the sets that are really matched, not the intersection of the sets or whatever else.",
            "So in this case, when we need really to to hit the real, the real set then then we show somehow that the matching distance is maybe the best.",
            "The best choice here."
        ],
        [
            "So.",
            "To conclude, overall best distance measure can be.",
            "Found, but best choice we should remember that the best choice is the one that optimizes selected evolution measure.",
            "So we should use the distance measure that.",
            "That we are really interested in optimizing it, but not something else we've seen from the result that coming is best for coming close optimization.",
            "Matching distance performs best for subset accuracy.",
            "Average strength overall datasets on all evolution measures are showing that, on average, Hamming distance is the best and it is closely followed by matching distance and Euclidean distance.",
            "So with this I will conclude my talk.",
            "So thank you for intention and if you have questions.",
            "OK. Any questions?",
            "Similarities.",
            "Why didn't you try cosine similarity?",
            "Because having the zeros and ones sounds more suitable for cosine similarity, Euclidean.",
            "Is there any reason that you didn't try it?",
            "No, we just we simply.",
            "I will simply just missed it.",
            "Yeah, so I would certainly improve your stuff on the relation site and understanding of datasets because these datasets have at least extremely different properties and you might.",
            "It's hard to understand the effects of all these measures.",
            "In the presence of this.",
            "Let's say in the presence of this extremely hitter genius data set.",
            "So this would be, let's say, one way of saying it.",
            "Also standing water.",
            "This texture texture datasets versus structured datasets of this.",
            "Also.",
            "OK thanks.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, my name is Valentina Jessica and I'm going to present today the.",
                    "label": 0
                },
                {
                    "sent": "Topic comparison of distances for multi label classification with the cities.",
                    "label": 1
                },
                {
                    "sent": "This is recent work with my colleagues, Dragon Quarter and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the beginning I will give a short outline how this talk will.",
                    "label": 0
                },
                {
                    "sent": "Go on so the first stage I will give a short introduction about the predictive clustering trees about the distances for structured data.",
                    "label": 1
                },
                {
                    "sent": "Enter the whole idea of this work.",
                    "label": 0
                },
                {
                    "sent": "Next I will talk talk a bit more about predictive clustering trees.",
                    "label": 0
                },
                {
                    "sent": "The concept and how they work.",
                    "label": 0
                },
                {
                    "sent": "Further on I will.",
                    "label": 0
                },
                {
                    "sent": "I will introduce the distances that we have interested in this work and used to used in this work to compare.",
                    "label": 0
                },
                {
                    "sent": "And towards the end I will continue with the experimental design and the results that we've obtained from the work and at the end, of course, the conclusions from this work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to start with the introduction, the first question is what we want to do in this work and answer this question is we want to do multi label classification.",
                    "label": 0
                },
                {
                    "sent": "Or more specifically we want to test some distances and to compare these distances on how they perform when used to when we do multi label classification, how we're going to do this so?",
                    "label": 0
                },
                {
                    "sent": "In the literature, we know two approaches, how we how we do multi label classification.",
                    "label": 0
                },
                {
                    "sent": "The first approaches problem transformation and this means that the problem itself from multi label classification problem is transformed to a problem of regular classification and then either multiple classifiers are build it or some other approaches taken to solve the multi label classification problem the next.",
                    "label": 0
                },
                {
                    "sent": "So the next approach which is found so far is algorithm adaptation, which means that algorithms are changed and adapted to handle this kind of problems.",
                    "label": 0
                },
                {
                    "sent": "In this work we will focus on the 2nd on the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Approach and we will show how we do an adaptation of predictive clustering trees to handle multi label classification using distances and then of course we show comparison of these distances.",
                    "label": 0
                },
                {
                    "sent": "So basically this so far is.",
                    "label": 0
                },
                {
                    "sent": "Is done for many different data types and this can be, let's say multiple targets or time series for his.",
                    "label": 0
                },
                {
                    "sent": "And now our aim in this work was to to extend the P cities to to work on the sets.",
                    "label": 0
                },
                {
                    "sent": "For this, for this purpose we are using several distances that are defined on the sets or that are specifically designed for handling multi label data, and these distances are named here.",
                    "label": 0
                },
                {
                    "sent": "Hamming distance, Jaccard distance and matching distance.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, predictive clustering trees concept is implemented in the class framework and it is.",
                    "label": 1
                },
                {
                    "sent": "It is based on the well known concept of top down induction decision trees.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that the dog rhythm is pretty similar as the top down reduction of decision trees with the change of that calculation of the heuristic.",
                    "label": 0
                },
                {
                    "sent": "When we when we are doing the split in the top down in the regular top down induction decision trees, we are doing the split using information gain and here we are doing the split using various reduction that is big cause the predictive clustering trees are seeing the whole tree is.",
                    "label": 0
                },
                {
                    "sent": "You have clusters groups.",
                    "label": 0
                },
                {
                    "sent": "This we don't want to do.",
                    "label": 0
                },
                {
                    "sent": "No, I know.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, uh.",
                    "label": 0
                },
                {
                    "sent": "Yes, I.",
                    "label": 0
                },
                {
                    "sent": "So the predictive clustering trees are seeing the whole 3 as a hierarchy of a clusters, which means all the data is put in the in the top node on the 1st cluster, and then the splits are done down.",
                    "label": 0
                },
                {
                    "sent": "And on the way for in each step to do a split we calculate this variance reduction, which is which is the sum of all pairwise distances between the between the elements in the cluster and then the split that best that optimizes the best or reduces the best variance.",
                    "label": 0
                },
                {
                    "sent": "We use that split and we continue on.",
                    "label": 0
                },
                {
                    "sent": "The this process stops when when we when we do statistical tests and we show that such St cannot further improve the variance and then we create a leaf.",
                    "label": 0
                },
                {
                    "sent": "Once we have created the tree, we need to find a prototype for each leave because we want to use these three for prediction and prototype usually is defined as an average of all elements in the in the leaf.",
                    "label": 0
                },
                {
                    "sent": "But here when we have complex data with its.",
                    "label": 0
                },
                {
                    "sent": "It's difficult, it's almost impossible to define average, let's say average between sets is not defined, and that's why we are using here Metroid and for prototype we're taking the the the element of the cluster that minimizes this sum that minimizes.",
                    "label": 0
                },
                {
                    "sent": "Average because you haven't defined arithmetical operations on those sets, you can't some upsets and then divide by something.",
                    "label": 0
                },
                {
                    "sent": "So it's the actual reason is because you are using discrete values and couple of.",
                    "label": 0
                },
                {
                    "sent": "So what kind of vector so well, yes, actually these sets here in this talk actually vectors of nominal values, that's true.",
                    "label": 0
                },
                {
                    "sent": "But in general case you can have sets of anything, so it's it's almost impossible to define.",
                    "label": 0
                },
                {
                    "sent": "Average, so we're defining this as method, and that's the element that minimizes this soon so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and for multi label classification.",
                    "label": 0
                },
                {
                    "sent": "The the one adaptation which which is needed to be done of predictive clustering increases to to.",
                    "label": 0
                },
                {
                    "sent": "To involve distances on set cause multi label classification we see the multi label classification problem on the target set we have multiple labels and this is actually set of labels.",
                    "label": 0
                },
                {
                    "sent": "So if we introduce the introduce distances on sets then we can do predictive clustering trees for multi label learning.",
                    "label": 0
                },
                {
                    "sent": "So we we have implemented several distances.",
                    "label": 0
                },
                {
                    "sent": "The first distances Euclidean distance and now here is 1 interesting question, how Euclidean distance is defined on sets?",
                    "label": 1
                },
                {
                    "sent": "Because we know that the Euclidean distance is actually defined on vectors.",
                    "label": 0
                },
                {
                    "sent": "Both are here in Multi label classification and multi label learning we have for each example we have labels and we have.",
                    "label": 0
                },
                {
                    "sent": "Set of this label and we can represent the each set as a tuple of 01 values.",
                    "label": 1
                },
                {
                    "sent": "So when we represent each set as a vector, then we can use Euclidean distance on the vector representation of these sets and we can say that this is 1 defined distance on this set.",
                    "label": 0
                },
                {
                    "sent": "Way across similarity, it's yes.",
                    "label": 0
                },
                {
                    "sent": "OK. Also, coming distance is defined as the difference between the cardinality of the Union and cardinality of the intersection of two sets.",
                    "label": 0
                },
                {
                    "sent": "If given two sets.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the car distance.",
                    "label": 0
                },
                {
                    "sent": "Oh it is a.",
                    "label": 0
                },
                {
                    "sent": "The symmetrical differences.",
                    "label": 0
                },
                {
                    "sent": "So it is the ratio between the symmetrical difference and the cardinality of the Union.",
                    "label": 0
                },
                {
                    "sent": "And matching distance this distance is well known in the literature and used for sets.",
                    "label": 0
                },
                {
                    "sent": "Here we have adopted this distance specifically for for this type of data, which are sets of nominals.",
                    "label": 0
                },
                {
                    "sent": "And when we transform this matching distance, when we simplify this formula, we will come to this, so it's a difference between the maximum maximum cardinality.",
                    "label": 0
                },
                {
                    "sent": "So the larger cardinality of the larger set minus the intersection of the cardinality of the intersection of two sets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this light I will give example of this distance.",
                    "label": 1
                },
                {
                    "sent": "How are they calculated and what they mean?",
                    "label": 0
                },
                {
                    "sent": "So if we have two sets, let's say set South of ABC and safety of AMD, then Euclidean distance of these two sets.",
                    "label": 0
                },
                {
                    "sent": "If we have this vector representations given here, then the ability on distance is sqrt 3.",
                    "label": 0
                },
                {
                    "sent": "Are humming distances 3 because we have the the symmetrical differences BC and D123?",
                    "label": 0
                },
                {
                    "sent": "In Jakarta is 3 / 4, so the symmetrical difference over the union of these sets matching distance is shown in this example.",
                    "label": 0
                },
                {
                    "sent": "So what is the matching distance in?",
                    "label": 0
                },
                {
                    "sent": "In in fact it tries to match the equal element, or in general case the most poses the most closest element and then it calculates the distance between the elements that have no Metro have match, that is, that is not equal.",
                    "label": 0
                },
                {
                    "sent": "And then we have some penalty for the elements that are not matched with any element.",
                    "label": 0
                },
                {
                    "sent": "So if we if we.",
                    "label": 0
                },
                {
                    "sent": "Calculate the metric distance.",
                    "label": 0
                },
                {
                    "sent": "Here the metric distances 1 two this red lines calculated in the measure distance and when the element is same, then we're not calculating anything there.",
                    "label": 0
                },
                {
                    "sent": "So we we are showing here that this distance is how they are calculated and we're showing how they how they are different from each other.",
                    "label": 0
                },
                {
                    "sent": "It's it's interesting here to say that the Hamming distance, by definition is always the Euclidean squared or vice versa.",
                    "label": 0
                },
                {
                    "sent": "Euclidean is square root of the Hamming distance.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some calculated.",
                    "label": 0
                },
                {
                    "sent": "Once we have defined these distances, we have taken 6 datasets to test these distances and to test the approach of predictive clustering trees.",
                    "label": 0
                },
                {
                    "sent": "We can say that this these datasets.",
                    "label": 0
                },
                {
                    "sent": "They are somehow ordered in terms of their complexity and these distances are from different domains.",
                    "label": 0
                },
                {
                    "sent": "Domains are ecology, music, text and so on.",
                    "label": 0
                },
                {
                    "sent": "Although datasets are split to training and testing data set, we have various number of examples, various number of labels.",
                    "label": 1
                },
                {
                    "sent": "And overall so this label cardinality various label cardinality actually is the number that somehow tells us how many labels for each example we have in in average and on the data set.",
                    "label": 0
                },
                {
                    "sent": "So we have from from labels starting from 1.2, which means people up to.",
                    "label": 0
                },
                {
                    "sent": "I don't know here.",
                    "label": 0
                },
                {
                    "sent": "6.34 so.",
                    "label": 0
                },
                {
                    "sent": "Examples in Enron case.",
                    "label": 0
                },
                {
                    "sent": "Oh well, talk to me.",
                    "label": 0
                },
                {
                    "sent": "You know females.",
                    "label": 1
                },
                {
                    "sent": "The number of examples are whatever.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "What examples in this case and put the labels?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure about this.",
                    "label": 0
                },
                {
                    "sent": "Mityana also.",
                    "label": 0
                },
                {
                    "sent": "I don't know what are the levels in this.",
                    "label": 0
                },
                {
                    "sent": "Maybe Sasha?",
                    "label": 0
                },
                {
                    "sent": "Newspapers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but.",
                    "label": 0
                },
                {
                    "sent": "But in the case of Enron, what I mean you have.",
                    "label": 0
                },
                {
                    "sent": "2000 users suffer millions of emails.",
                    "label": 0
                },
                {
                    "sent": "Central, I don't know.",
                    "label": 0
                },
                {
                    "sent": "This is something which is, so most of these so and Ronan Medical these datasets, which now quite extensively used to evaluate my PlayStation averages.",
                    "label": 0
                },
                {
                    "sent": "So this is like getting the status of UCI where you take these and you test on those datasets.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "I think often.",
                    "label": 0
                },
                {
                    "sent": "But this is, I mean everything.",
                    "label": 0
                },
                {
                    "sent": "What I know about these datasets and nothing fits here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's probably a sample.",
                    "label": 0
                },
                {
                    "sent": "Some small sample.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think that Enron is actually sample of this data set.",
                    "label": 0
                },
                {
                    "sent": "It's not the whole data set I've written.",
                    "label": 0
                },
                {
                    "sent": "The description of the data set that it's not the whole data set, but it's a sample taken for some reason and some prefiltering was done to this data set.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So further on we have defined different evaluation measures that are commonly used in multi label classification.",
                    "label": 0
                },
                {
                    "sent": "The the first one is given their accuracy, which is somehow an average of operations between the the intersection and union of the predicted and the real set.",
                    "label": 0
                },
                {
                    "sent": "Or yes, the predicted in the real set of labels, so H. Office, I mean the predicted and why I means the real set and that's across older older evolution measures.",
                    "label": 0
                },
                {
                    "sent": "Precision is defined as average of ratios of intersection and personality of the real set of labels, and so on.",
                    "label": 0
                },
                {
                    "sent": "Here is interesting to .222 of this evaluation measure.",
                    "label": 0
                },
                {
                    "sent": "The one is.",
                    "label": 0
                },
                {
                    "sent": "4 score and the other one is subset accuracy.",
                    "label": 0
                },
                {
                    "sent": "Therefore score is calculated as a.",
                    "label": 0
                },
                {
                    "sent": "So the F1 score is calculated as a mean of precision and recall, and the subset accuracy which is.",
                    "label": 0
                },
                {
                    "sent": "Somehow define touristic 'cause it takes in account only when the when the sets the predicted set and the.",
                    "label": 0
                },
                {
                    "sent": "Predicted certain the real set are equal so it doesn't take into account any intersection or something, but or symmetrical difference, but it takes in account only when the window two sets is.",
                    "label": 0
                },
                {
                    "sent": "Is equal.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have set up this problem in that way that for each problem we have training and testing datasets.",
                    "label": 1
                },
                {
                    "sent": "During the learning we have used pre pruning with F tests.",
                    "label": 1
                },
                {
                    "sent": "We have selected the optimal significance level by three cross fold validation.",
                    "label": 1
                },
                {
                    "sent": "Three fold cross validation from the from the following followers here.",
                    "label": 1
                },
                {
                    "sent": "So from 0.01 to 0.05.",
                    "label": 0
                },
                {
                    "sent": "And we compare the capacities with different measures used for learning.",
                    "label": 0
                },
                {
                    "sent": "So we compared Euclidean, Hamming, Jakarta and matching distance.",
                    "label": 0
                },
                {
                    "sent": "Enter this.",
                    "label": 0
                },
                {
                    "sent": "This performance of all distances are somehow compared across the whole 6 evaluation measures which were previously diff.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The result.",
                    "label": 0
                },
                {
                    "sent": "So we have datasets in the roles and we have distances in the columns, so we have each one table for each evaluation measure.",
                    "label": 0
                },
                {
                    "sent": "The first evaluation measure is coming close.",
                    "label": 0
                },
                {
                    "sent": "Measure we can see here that humming performs best, which somehow is expected to cause it.",
                    "label": 0
                },
                {
                    "sent": "Minimize.",
                    "label": 0
                },
                {
                    "sent": "Minimize is exactly that.",
                    "label": 0
                },
                {
                    "sent": "The coming close function.",
                    "label": 0
                },
                {
                    "sent": "And also one interesting thing is to do mention here that Jakarta performs worse on on every data set in this in this case.",
                    "label": 0
                },
                {
                    "sent": "The the next the next table is precision evolution measure.",
                    "label": 0
                },
                {
                    "sent": "We can see here that there are some something very interesting cause the Jaccard measure performs best on three datasets and on the rest of the rest of the data set it performs worse, which is the first.",
                    "label": 0
                },
                {
                    "sent": "The first is something that we can easily explain.",
                    "label": 0
                },
                {
                    "sent": "Becausw Jakarta by definition is very color correlated.",
                    "label": 0
                },
                {
                    "sent": "With the precision by definition, but but these results here somehow surprising and hard to hard to implement.",
                    "label": 0
                },
                {
                    "sent": "Hard to explain.",
                    "label": 0
                },
                {
                    "sent": "What is the statistical significance of what's the average variance here?",
                    "label": 0
                },
                {
                    "sent": "So these numbers, I mean, they jump extremely a lot so.",
                    "label": 0
                },
                {
                    "sent": "So it will be interesting to see what the actual well these numbers jump a lot, because here we show we are not showing relative measure but absolute one.",
                    "label": 0
                },
                {
                    "sent": "So the precision is absolute measurement.",
                    "label": 0
                },
                {
                    "sent": "These numbers cannot be compared across the data set but just just in the single line.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the F1 score, which is somehow balance between the precision, is between the precision and recall.",
                    "label": 0
                },
                {
                    "sent": "Also somehow that the Hamming is is the best.",
                    "label": 0
                },
                {
                    "sent": "And, uh, then closely followed by matching distance and by Jakarta New Kleidion recall shows pretty the same as on the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you remember the coming close measure where we have coming was the best in Jakarta was the worse.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty same happens also to recall and here we can say that jakartan Euclidean somehow better than the rest and the cart is the worst distance measure in this case.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when we come to accuracy and to the subset accuracy, we were obtaining a bit different result and.",
                    "label": 0
                },
                {
                    "sent": "Specifically in accuracy we can buy.",
                    "label": 0
                },
                {
                    "sent": "We can say that the matching distance is the best distance if we do the ranks and then.",
                    "label": 0
                },
                {
                    "sent": "If you do the ranks across the whole data set, then matching distance is the best when when compared by accuracy.",
                    "label": 0
                },
                {
                    "sent": "And here it's interesting to point out that Euclidean distance is based on three datasets, but it is also worse in two datasets and this is we have checked this and this is due to Euclidean distance is working very good on this data set there where the number of labels is very very small or close to one.",
                    "label": 0
                },
                {
                    "sent": "So if we go back to the.",
                    "label": 0
                },
                {
                    "sent": "Flight with the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will see that Euclidean distance works best on.",
                    "label": 0
                },
                {
                    "sent": "In measure of accuracy, performance as well based on emotions, Madonna and Medical where the where the label cardinalities very small or close to one.",
                    "label": 0
                },
                {
                    "sent": "And when this label cardinality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gross, then the Euclidean distance performs worst according to the accuracy and the last one is the subset accuracy.",
                    "label": 0
                },
                {
                    "sent": "This, as I said, is very strict and it's easier to obtain even numbers like 0 in Jakarta, 'cause we only count the sets that are really matched, not the intersection of the sets or whatever else.",
                    "label": 0
                },
                {
                    "sent": "So in this case, when we need really to to hit the real, the real set then then we show somehow that the matching distance is maybe the best.",
                    "label": 0
                },
                {
                    "sent": "The best choice here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To conclude, overall best distance measure can be.",
                    "label": 1
                },
                {
                    "sent": "Found, but best choice we should remember that the best choice is the one that optimizes selected evolution measure.",
                    "label": 1
                },
                {
                    "sent": "So we should use the distance measure that.",
                    "label": 0
                },
                {
                    "sent": "That we are really interested in optimizing it, but not something else we've seen from the result that coming is best for coming close optimization.",
                    "label": 0
                },
                {
                    "sent": "Matching distance performs best for subset accuracy.",
                    "label": 1
                },
                {
                    "sent": "Average strength overall datasets on all evolution measures are showing that, on average, Hamming distance is the best and it is closely followed by matching distance and Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "So with this I will conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "So thank you for intention and if you have questions.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions?",
                    "label": 0
                },
                {
                    "sent": "Similarities.",
                    "label": 0
                },
                {
                    "sent": "Why didn't you try cosine similarity?",
                    "label": 0
                },
                {
                    "sent": "Because having the zeros and ones sounds more suitable for cosine similarity, Euclidean.",
                    "label": 0
                },
                {
                    "sent": "Is there any reason that you didn't try it?",
                    "label": 0
                },
                {
                    "sent": "No, we just we simply.",
                    "label": 0
                },
                {
                    "sent": "I will simply just missed it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I would certainly improve your stuff on the relation site and understanding of datasets because these datasets have at least extremely different properties and you might.",
                    "label": 0
                },
                {
                    "sent": "It's hard to understand the effects of all these measures.",
                    "label": 0
                },
                {
                    "sent": "In the presence of this.",
                    "label": 0
                },
                {
                    "sent": "Let's say in the presence of this extremely hitter genius data set.",
                    "label": 0
                },
                {
                    "sent": "So this would be, let's say, one way of saying it.",
                    "label": 0
                },
                {
                    "sent": "Also standing water.",
                    "label": 0
                },
                {
                    "sent": "This texture texture datasets versus structured datasets of this.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}