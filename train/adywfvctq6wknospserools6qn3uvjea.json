{
    "id": "adywfvctq6wknospserools6qn3uvjea",
    "title": "DeepWalk: Online Learning of Social Representations",
    "info": {
        "author": [
            "Bryan Perozzi, Computer Science Department, Stony Brook University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_perozzi_deep_walk/",
    "segmentation": [
        [
            "I am Brian Rosie and I'm here to talk to you today about our paper dpac online learning of social representations and this is joint work with my collaborators Avrami and Steve from Stony Brook University."
        ],
        [
            "And So what I want to talk about today really, is how do we use graphs as features and."
        ],
        [
            "So you know the first step in any machine learning process on graphs is to somehow extract features from it now, so we're trying to take this like kind of node link diagram thing or this adjacency matrix and convert this to something that we can actually feed into our classifiers.",
            "Now you could use the rows of the adjacency matrix by itself, but nobody does that so normally what you end up doing is sort of like extracting features like what's the degrees of the nodes, or maybe pairwise features like how many common neighbors do they share for link prediction?",
            "Or the Democrat are metric or something.",
            "And so you know, you also can get like group features.",
            "So like how are these guys and sort of the global structure of the graph where they kind of close so they kind of far are they in sort of something we can break off from each other?",
            "And these are these features are fed into every graph test you can imagine.",
            "So anomaly detection, attribute prediction, clustering, link prediction, you name it."
        ],
        [
            "So you know another way of creating features is by learning or by making latent dimension, sort of like a skinnier version of the adjacency matrix that captures the same sort of information.",
            "But it kind of in a continuous space.",
            "So we're like approximating it and you know, and there's sort of maybe we're completing it.",
            "There's a lot of ways that you can view this sort of problem of taking one matrix and making a skinny one out of it.",
            "But what I want to talk about today."
        ],
        [
            "Is really learning latent representations and so a deep local learns the latent representation of adjacency matrices using deep learning techniques developed for language modeling.",
            "And so you know, with these latent dimensions, you can feed them into whatever task that you have in mind."
        ],
        [
            "And so, like a visual example of the input here, sort of like the karate graph, which many of you may be familiar with and sort of.",
            "You know that's the input and then the output of this is sort of this representation in our two, so we chose a 2 dimensional embedding 'cause it was easy to visualize and we colored the communities here.",
            "So with a maximum modularity clustering on the discrete graph, and so you can see that sort of the Community structure is preserved and in the output, and that's kind of interesting.",
            "So this was like an encouraging 1st result."
        ],
        [
            "So the advantages of the Walker that it's online, so we don't actually have to look at the entire graph at once.",
            "Got this really great metaphor that people seem to love, so I guess that's an advantage and the pips pretty performant, so we compare against a different matrix decomposition techniques and we are, you know, I would say competitive if not superior and our results we show you are all superior, which is good and then we have an implementation available so you can grab this thing and you can, you know, run it under graph tonight."
        ],
        [
            "OK, so So what is language modeling and why do we care about?"
        ],
        [
            "So language modeling seeks to learn a representation that Maps a discrete word into a continuous space, and normally they do this with like a window around the word.",
            "So if you care about like the word moon and this sort of like example, you know you have like a Co occurrence like of words to the left and words to the right, and you are counting all these up inside a matrix and trying to keep track of like the correlations there, and the hope is that this low dimensional representation, you capture low dimensional representation, you learn, captures this structure.",
            "In the word space.",
            "So you want words that are close to each other in the way they use them to be close to each other in the embedding space.",
            "And this."
        ],
        [
            "Is not something that like I just made up.",
            "This is actually a very active research topic in NLP that's really kind of exploded along with the whole.",
            "You know deep learning phenomenon, so a lot of vision is the most popular.",
            "I would say use of it, but in NLP at ACL this year there were a ton of papers using distributed word representations along the same lines as this."
        ],
        [
            "So the word frequency and natural language follows a power law, and you know.",
            "So here's 10,000 articles in Wikipedia and the frequency of the word occurrences, which is good, and you know.",
            "So you see log log plot, straight line pretty good."
        ],
        [
            "And so it turns out that you know we take a take a scale free graph and we do a bunch of short random walks.",
            "We also get a power loss so."
        ],
        [
            "Maybe not that surprising, but still sort of like these things are the same in some loose sense.",
            "We have the same sorts of distribution with maybe different exponents that we're trying to model, but.",
            "So what we did here is like we show the YouTube social graph and like so we did short random walks and now we see we also get a power law a little, you know, not perfect on the end, but that's it.",
            "So random walk distance is also like it's a good feature, right?",
            "You've all heard of random walk distance.",
            "If you done machine learning on graphs because it captures nearness really well and there's this really pleasing connection to sort of spectral properties of the graph, including finding good cuts."
        ],
        [
            "So the cool idea here is that short random walks or sentence is and so that's like the take away too.",
            "So if you remember that you guys are going to be fine."
        ],
        [
            "So how does the clockwork you give us a graph?",
            "We take random.",
            "We take started.",
            "Each node.",
            "We take a random walk.",
            "We do a representation mapping and then we have to learn this representation and then at the end we have points in continuous space."
        ],
        [
            "So diving."
        ],
        [
            "Do it, we generate random walks for each vertex in the graph, and each random walk has a particular length and we're doing uniform random walks here.",
            "So we just take this step next step uniformly from the vertex neighbors."
        ],
        [
            "So so repres."
        ],
        [
            "Station mapping so OK.",
            "So as we have this we have this sequence of vertices that we visited in this random order and when we slide a window across it and the window focuses on the node in the center.",
            "So we look to the left of something and we looked at the right of the something and we're actually trying to do here is find a representation that maximizes the probability of returning the node to the left and the node to the right.",
            "So we're requiring the thing in the center predict the things around it, and these are hyperparameters, so.",
            "Window sizes are frequently not one.",
            "In fact, one is kind of an interesting, but the idea here is that we're using this representation, so we take the vertex we map it through this mapping function Phi, and we're using that representation to the actually do."
        ],
        [
            "Prediction.",
            "And so it turns out."
        ],
        [
            "That is actually pretty expensive, so you can't do that directly because we would have to enumerate every time we want to model.",
            "We might have to enumerate all the vertices, so that's not going to scale right?",
            "So this is really nice relaxation from michaelov that's used in Skip gram.",
            "If you guys are familiar, and so you say, OK, what we're going to do is we're going to relax and say we're going to learn to relax and say that there is a tree that we can learn parameters for that will allow us fast access to the individual vertex representations.",
            "And So what we actually do is when we look to the left and to the right of the vertex that, like of each representations, we slide this window along, we maximize the path through the tree to each of the leaf nodes.",
            "So we're saying that the probability is.",
            "So this is a you know, obviously an assumption, but we're saying that we can.",
            "We can learn something, something that allows us that this path through the tree is going to approximate what the real probability is, and so we have these.",
            "We learn the parameters for these logistic binary classifiers.",
            "We stick the representation at the top of the tree and we maximize the path to the things that are on the left hand side on the right hand side."
        ],
        [
            "And so to learn the parameters I mean we have to learn the vertex representations and the binary classifier classification weights.",
            "And so we randomly initialized the representations an for each of the nodes along the path from the root until the leaf we have to calculate the loss function and we are jointly updating both the classifier weights and the vertex representation simultaneously.",
            "And this is exactly skip gram so."
        ],
        [
            "Love."
        ],
        [
            "OK, so how does this actually work right?"
        ],
        [
            "So how we evaluate this in our paper is we try doing semi supervised network classification.",
            "So we take we take a partially labeled graph with node attribute swear.",
            "So the attributes here are usually like groups or community membership in a social network and we try to try to predict the label for things that don't have 'em.",
            "So this is very applicable problem.",
            "So you know in the little example right here you have like a group of Googlers, Anna Group of Stony Brook students and one passkey.",
            "Stony Brook student who just doesn't fill out his profile.",
            "And we want to predict what that profile entry is.",
            "And if you ever use Facebook and you get these nagging messages, they're trying to make the task easier for them by getting more labeled data."
        ],
        [
            "So the baselines we compare against here are the way to vote relational neighbor, which is an approximate inference technique like network classification and latent dimensions for spectral methods.",
            "So we do eigen decompositions on the Laplacian and the modularity matrix, which are two different ways of encoding kind of cut information.",
            "We also do.",
            "This is another technique that does that.",
            "K means on the adjacency matrix, and so that event it's not very performant, but it has the advantage of being sparse, so it's more scalable."
        ],
        [
            "And OK, so we have these tasks that we're comparing against, so here's one graph blog catalog.",
            "It's a small graph, but what we see here is that deep lock, and so it's also multi label classification.",
            "So we give you micro F1 macro F1.",
            "But what we see here is that the Dpac outperforms all the baselines, which is actually really interesting.",
            "And especially with a little sparse, so the task exactly as we label 10% of the nodes and we try to predict 90% of them, and then we label 20% of the nodes and try to predict 80% of them and so on and so forth.",
            "So it gets easier as we label more and more nodes 'cause there's less things to predict and we have more training data.",
            "But so there's that."
        ],
        [
            "Also we tried this on other graphs, flicker and we sort of were labeling less nodes Now, so we're kind of concentrating on the part where we're doing better on this graph.",
            "Spectral clustering was more more competitive than on the previous one, and like macro F1, but default kind of gets better as we go along.",
            "And this graph is still kind of small in the number of vertices, but it has a lot of edges."
        ],
        [
            "And then so we try to graph with more nodes and so here I couldn't do the eigendecomposition.",
            "So those two techniques are out and so now we have to compare against the weight of what relational neighbor and K means on the adjacency matrix and bubbles of those techniques.",
            "They don't do so well in sparsity in the presence of like data sparsity.",
            "So that's."
        ],
        [
            "Kind of a default in a nutshell.",
            "We also looked at parallelization, so you can because these these random walks can be generated in parallel and we can sample different parts of the graph that allows us to.",
            "You know, have different threads doing this at the same time, and one question is, are these gradient updates that were learning the model stomping on top of each other at the parallelization doesn't really affect representation quality, so it's a sparse of the graph.",
            "You know, the easier it is to achieve linear scalability.",
            "This is kind of like a unknown result and you can see this happening here where the models trained with more workers don't have a big difference in performance."
        ],
        [
            "So in conclusion."
        ],
        [
            "I guess the future work that you can do with this.",
            "You know you can work entirely with a compressed representation.",
            "I think this is really cool.",
            "You know you can build an updated representation as new data comes in without ever actually making the graph.",
            "And there's some challenges with the learning rate because you don't want to forget old information too quickly.",
            "So this is.",
            "That's why it's future work.",
            "'cause I think that's actually really the hard part of the problem.",
            "And then there's also this issue with like non random walk so frequently with web traffic or other sort of graphs that are a byproduct of user interaction.",
            "These outside processes are creating the graph and you know you can.",
            "They're not going to occur uniformly at random, but these can get extra information from that, and this is kind of a language modeling is doing, so you might be able to sort of create this compressed representation instead of ever having a graph representation like if that's if it's not what you want."
        ],
        [
            "The takeaway is that there's a lot of cool language modeling techniques that are being developed for deep learning right now that can be used for online learning of your network representations, and I think there's a lot to explore here."
        ],
        [
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am Brian Rosie and I'm here to talk to you today about our paper dpac online learning of social representations and this is joint work with my collaborators Avrami and Steve from Stony Brook University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what I want to talk about today really, is how do we use graphs as features and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know the first step in any machine learning process on graphs is to somehow extract features from it now, so we're trying to take this like kind of node link diagram thing or this adjacency matrix and convert this to something that we can actually feed into our classifiers.",
                    "label": 1
                },
                {
                    "sent": "Now you could use the rows of the adjacency matrix by itself, but nobody does that so normally what you end up doing is sort of like extracting features like what's the degrees of the nodes, or maybe pairwise features like how many common neighbors do they share for link prediction?",
                    "label": 0
                },
                {
                    "sent": "Or the Democrat are metric or something.",
                    "label": 0
                },
                {
                    "sent": "And so you know, you also can get like group features.",
                    "label": 0
                },
                {
                    "sent": "So like how are these guys and sort of the global structure of the graph where they kind of close so they kind of far are they in sort of something we can break off from each other?",
                    "label": 0
                },
                {
                    "sent": "And these are these features are fed into every graph test you can imagine.",
                    "label": 0
                },
                {
                    "sent": "So anomaly detection, attribute prediction, clustering, link prediction, you name it.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know another way of creating features is by learning or by making latent dimension, sort of like a skinnier version of the adjacency matrix that captures the same sort of information.",
                    "label": 0
                },
                {
                    "sent": "But it kind of in a continuous space.",
                    "label": 0
                },
                {
                    "sent": "So we're like approximating it and you know, and there's sort of maybe we're completing it.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of ways that you can view this sort of problem of taking one matrix and making a skinny one out of it.",
                    "label": 0
                },
                {
                    "sent": "But what I want to talk about today.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is really learning latent representations and so a deep local learns the latent representation of adjacency matrices using deep learning techniques developed for language modeling.",
                    "label": 0
                },
                {
                    "sent": "And so you know, with these latent dimensions, you can feed them into whatever task that you have in mind.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, like a visual example of the input here, sort of like the karate graph, which many of you may be familiar with and sort of.",
                    "label": 1
                },
                {
                    "sent": "You know that's the input and then the output of this is sort of this representation in our two, so we chose a 2 dimensional embedding 'cause it was easy to visualize and we colored the communities here.",
                    "label": 0
                },
                {
                    "sent": "So with a maximum modularity clustering on the discrete graph, and so you can see that sort of the Community structure is preserved and in the output, and that's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "So this was like an encouraging 1st result.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the advantages of the Walker that it's online, so we don't actually have to look at the entire graph at once.",
                    "label": 0
                },
                {
                    "sent": "Got this really great metaphor that people seem to love, so I guess that's an advantage and the pips pretty performant, so we compare against a different matrix decomposition techniques and we are, you know, I would say competitive if not superior and our results we show you are all superior, which is good and then we have an implementation available so you can grab this thing and you can, you know, run it under graph tonight.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so So what is language modeling and why do we care about?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So language modeling seeks to learn a representation that Maps a discrete word into a continuous space, and normally they do this with like a window around the word.",
                    "label": 1
                },
                {
                    "sent": "So if you care about like the word moon and this sort of like example, you know you have like a Co occurrence like of words to the left and words to the right, and you are counting all these up inside a matrix and trying to keep track of like the correlations there, and the hope is that this low dimensional representation, you capture low dimensional representation, you learn, captures this structure.",
                    "label": 0
                },
                {
                    "sent": "In the word space.",
                    "label": 0
                },
                {
                    "sent": "So you want words that are close to each other in the way they use them to be close to each other in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is not something that like I just made up.",
                    "label": 0
                },
                {
                    "sent": "This is actually a very active research topic in NLP that's really kind of exploded along with the whole.",
                    "label": 1
                },
                {
                    "sent": "You know deep learning phenomenon, so a lot of vision is the most popular.",
                    "label": 0
                },
                {
                    "sent": "I would say use of it, but in NLP at ACL this year there were a ton of papers using distributed word representations along the same lines as this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the word frequency and natural language follows a power law, and you know.",
                    "label": 1
                },
                {
                    "sent": "So here's 10,000 articles in Wikipedia and the frequency of the word occurrences, which is good, and you know.",
                    "label": 0
                },
                {
                    "sent": "So you see log log plot, straight line pretty good.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so it turns out that you know we take a take a scale free graph and we do a bunch of short random walks.",
                    "label": 0
                },
                {
                    "sent": "We also get a power loss so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe not that surprising, but still sort of like these things are the same in some loose sense.",
                    "label": 0
                },
                {
                    "sent": "We have the same sorts of distribution with maybe different exponents that we're trying to model, but.",
                    "label": 0
                },
                {
                    "sent": "So what we did here is like we show the YouTube social graph and like so we did short random walks and now we see we also get a power law a little, you know, not perfect on the end, but that's it.",
                    "label": 0
                },
                {
                    "sent": "So random walk distance is also like it's a good feature, right?",
                    "label": 1
                },
                {
                    "sent": "You've all heard of random walk distance.",
                    "label": 0
                },
                {
                    "sent": "If you done machine learning on graphs because it captures nearness really well and there's this really pleasing connection to sort of spectral properties of the graph, including finding good cuts.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the cool idea here is that short random walks or sentence is and so that's like the take away too.",
                    "label": 0
                },
                {
                    "sent": "So if you remember that you guys are going to be fine.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does the clockwork you give us a graph?",
                    "label": 0
                },
                {
                    "sent": "We take random.",
                    "label": 0
                },
                {
                    "sent": "We take started.",
                    "label": 0
                },
                {
                    "sent": "Each node.",
                    "label": 0
                },
                {
                    "sent": "We take a random walk.",
                    "label": 0
                },
                {
                    "sent": "We do a representation mapping and then we have to learn this representation and then at the end we have points in continuous space.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So diving.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it, we generate random walks for each vertex in the graph, and each random walk has a particular length and we're doing uniform random walks here.",
                    "label": 0
                },
                {
                    "sent": "So we just take this step next step uniformly from the vertex neighbors.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so repres.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station mapping so OK.",
                    "label": 0
                },
                {
                    "sent": "So as we have this we have this sequence of vertices that we visited in this random order and when we slide a window across it and the window focuses on the node in the center.",
                    "label": 0
                },
                {
                    "sent": "So we look to the left of something and we looked at the right of the something and we're actually trying to do here is find a representation that maximizes the probability of returning the node to the left and the node to the right.",
                    "label": 0
                },
                {
                    "sent": "So we're requiring the thing in the center predict the things around it, and these are hyperparameters, so.",
                    "label": 0
                },
                {
                    "sent": "Window sizes are frequently not one.",
                    "label": 0
                },
                {
                    "sent": "In fact, one is kind of an interesting, but the idea here is that we're using this representation, so we take the vertex we map it through this mapping function Phi, and we're using that representation to the actually do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction.",
                    "label": 0
                },
                {
                    "sent": "And so it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is actually pretty expensive, so you can't do that directly because we would have to enumerate every time we want to model.",
                    "label": 0
                },
                {
                    "sent": "We might have to enumerate all the vertices, so that's not going to scale right?",
                    "label": 0
                },
                {
                    "sent": "So this is really nice relaxation from michaelov that's used in Skip gram.",
                    "label": 0
                },
                {
                    "sent": "If you guys are familiar, and so you say, OK, what we're going to do is we're going to relax and say we're going to learn to relax and say that there is a tree that we can learn parameters for that will allow us fast access to the individual vertex representations.",
                    "label": 0
                },
                {
                    "sent": "And So what we actually do is when we look to the left and to the right of the vertex that, like of each representations, we slide this window along, we maximize the path through the tree to each of the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "So we're saying that the probability is.",
                    "label": 0
                },
                {
                    "sent": "So this is a you know, obviously an assumption, but we're saying that we can.",
                    "label": 0
                },
                {
                    "sent": "We can learn something, something that allows us that this path through the tree is going to approximate what the real probability is, and so we have these.",
                    "label": 0
                },
                {
                    "sent": "We learn the parameters for these logistic binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "We stick the representation at the top of the tree and we maximize the path to the things that are on the left hand side on the right hand side.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so to learn the parameters I mean we have to learn the vertex representations and the binary classifier classification weights.",
                    "label": 0
                },
                {
                    "sent": "And so we randomly initialized the representations an for each of the nodes along the path from the root until the leaf we have to calculate the loss function and we are jointly updating both the classifier weights and the vertex representation simultaneously.",
                    "label": 1
                },
                {
                    "sent": "And this is exactly skip gram so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how does this actually work right?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how we evaluate this in our paper is we try doing semi supervised network classification.",
                    "label": 1
                },
                {
                    "sent": "So we take we take a partially labeled graph with node attribute swear.",
                    "label": 0
                },
                {
                    "sent": "So the attributes here are usually like groups or community membership in a social network and we try to try to predict the label for things that don't have 'em.",
                    "label": 0
                },
                {
                    "sent": "So this is very applicable problem.",
                    "label": 0
                },
                {
                    "sent": "So you know in the little example right here you have like a group of Googlers, Anna Group of Stony Brook students and one passkey.",
                    "label": 0
                },
                {
                    "sent": "Stony Brook student who just doesn't fill out his profile.",
                    "label": 0
                },
                {
                    "sent": "And we want to predict what that profile entry is.",
                    "label": 0
                },
                {
                    "sent": "And if you ever use Facebook and you get these nagging messages, they're trying to make the task easier for them by getting more labeled data.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the baselines we compare against here are the way to vote relational neighbor, which is an approximate inference technique like network classification and latent dimensions for spectral methods.",
                    "label": 0
                },
                {
                    "sent": "So we do eigen decompositions on the Laplacian and the modularity matrix, which are two different ways of encoding kind of cut information.",
                    "label": 0
                },
                {
                    "sent": "We also do.",
                    "label": 0
                },
                {
                    "sent": "This is another technique that does that.",
                    "label": 0
                },
                {
                    "sent": "K means on the adjacency matrix, and so that event it's not very performant, but it has the advantage of being sparse, so it's more scalable.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And OK, so we have these tasks that we're comparing against, so here's one graph blog catalog.",
                    "label": 0
                },
                {
                    "sent": "It's a small graph, but what we see here is that deep lock, and so it's also multi label classification.",
                    "label": 0
                },
                {
                    "sent": "So we give you micro F1 macro F1.",
                    "label": 0
                },
                {
                    "sent": "But what we see here is that the Dpac outperforms all the baselines, which is actually really interesting.",
                    "label": 0
                },
                {
                    "sent": "And especially with a little sparse, so the task exactly as we label 10% of the nodes and we try to predict 90% of them, and then we label 20% of the nodes and try to predict 80% of them and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So it gets easier as we label more and more nodes 'cause there's less things to predict and we have more training data.",
                    "label": 0
                },
                {
                    "sent": "But so there's that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also we tried this on other graphs, flicker and we sort of were labeling less nodes Now, so we're kind of concentrating on the part where we're doing better on this graph.",
                    "label": 0
                },
                {
                    "sent": "Spectral clustering was more more competitive than on the previous one, and like macro F1, but default kind of gets better as we go along.",
                    "label": 0
                },
                {
                    "sent": "And this graph is still kind of small in the number of vertices, but it has a lot of edges.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then so we try to graph with more nodes and so here I couldn't do the eigendecomposition.",
                    "label": 0
                },
                {
                    "sent": "So those two techniques are out and so now we have to compare against the weight of what relational neighbor and K means on the adjacency matrix and bubbles of those techniques.",
                    "label": 0
                },
                {
                    "sent": "They don't do so well in sparsity in the presence of like data sparsity.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of a default in a nutshell.",
                    "label": 0
                },
                {
                    "sent": "We also looked at parallelization, so you can because these these random walks can be generated in parallel and we can sample different parts of the graph that allows us to.",
                    "label": 0
                },
                {
                    "sent": "You know, have different threads doing this at the same time, and one question is, are these gradient updates that were learning the model stomping on top of each other at the parallelization doesn't really affect representation quality, so it's a sparse of the graph.",
                    "label": 0
                },
                {
                    "sent": "You know, the easier it is to achieve linear scalability.",
                    "label": 0
                },
                {
                    "sent": "This is kind of like a unknown result and you can see this happening here where the models trained with more workers don't have a big difference in performance.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess the future work that you can do with this.",
                    "label": 0
                },
                {
                    "sent": "You know you can work entirely with a compressed representation.",
                    "label": 0
                },
                {
                    "sent": "I think this is really cool.",
                    "label": 0
                },
                {
                    "sent": "You know you can build an updated representation as new data comes in without ever actually making the graph.",
                    "label": 0
                },
                {
                    "sent": "And there's some challenges with the learning rate because you don't want to forget old information too quickly.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "That's why it's future work.",
                    "label": 1
                },
                {
                    "sent": "'cause I think that's actually really the hard part of the problem.",
                    "label": 0
                },
                {
                    "sent": "And then there's also this issue with like non random walk so frequently with web traffic or other sort of graphs that are a byproduct of user interaction.",
                    "label": 0
                },
                {
                    "sent": "These outside processes are creating the graph and you know you can.",
                    "label": 0
                },
                {
                    "sent": "They're not going to occur uniformly at random, but these can get extra information from that, and this is kind of a language modeling is doing, so you might be able to sort of create this compressed representation instead of ever having a graph representation like if that's if it's not what you want.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The takeaway is that there's a lot of cool language modeling techniques that are being developed for deep learning right now that can be used for online learning of your network representations, and I think there's a lot to explore here.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}