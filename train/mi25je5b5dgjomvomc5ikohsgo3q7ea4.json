{
    "id": "mi25je5b5dgjomvomc5ikohsgo3q7ea4",
    "title": "Learn to Weight Term in Information Retrieval Using Category Information",
    "info": {
        "author": [
            "Rong Jin, Department of Computer Science, KU Leuven"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/icml05_jin_lwtir/",
    "segmentation": [
        [
            "If you're in trouble using category information.",
            "Alright, so first outline my talk.",
            "I will first provide."
        ],
        [
            "An overview of two weighting method information retrieval and then I will discuss the framework of running term words."
        ],
        [
            "Using the category information.",
            "And then finally our discuss our empirical study results."
        ],
        [
            "OK.",
            "So in generally of information retrieve."
        ],
        [
            "So there are two different families of term weighting method.",
            "The first one is based on TF IDF weights, and this provides the most popular."
        ],
        [
            "Answer To use the information retrieval.",
            "So it consists of the three factors one is."
        ],
        [
            "The TF factors which describe how frequently award appear in the document, and then the second factor is, is so called."
        ],
        [
            "The inverse document frequency factors which describe how rare the word appear in the entire collection and the third one is the normalization factor for document lens which is trying to correct the bias of long long document."
        ],
        [
            "OK, so while probably most well known on TF IDF method is called Okapi.",
            "Then the second category of."
        ],
        [
            "Often with the method is called is based on the length."
        ],
        [
            "Motox so the idea is you assume that documents generated by."
        ],
        [
            "A static language model and then you can apply your favorite estimate."
        ],
        [
            "Either like maximum likelihood this measure to estimate the most like most property language model for each document."
        ],
        [
            "And typically speaking to avoid sparse data problem, we are incorporating smoothing technicals like generally closest smoothie or derechas music.",
            "Right so."
        ],
        [
            "Here is the simplest unigram language model using the GM smoothing.",
            "OK."
        ],
        [
            "Right, so let me briefly discuss the problem with existing term weighting methods in information retrieval."
        ],
        [
            "The essential difficulty with all."
        ],
        [
            "The term weighting, measured in information retrieval is lack of supervision.",
            "So, for example, if you looking."
        ],
        [
            "To the TF IDF weighting method is based on the intuition that if the world appeared really close entire collection then this word is very likely to informative to the content of document how they were these?"
        ],
        [
            "Kind of intuition may not necessary to be true for many cases.",
            "For example, you may run into a typo, which apparently is a real word, but it really does not tell you anything about the document.",
            "Right?"
        ],
        [
            "And then the second second, if you look at length mode approach, which basically is a generated approach for describing the appearance of the words in the document as the results turns out, most probability mass is concentrated on common words, not the words that are informed.",
            "If the content of document.",
            "So again, this is not the appropriate approach to determine the term waits for the document words."
        ],
        [
            "So what I want to describe in this talk is how can we learn the term weights based on the data using inside information.",
            "In particular here I will describe the framework using the category information as the basis to learn the term weights."
        ],
        [
            "So let's first look at the set up the problems.",
            "OK, so here we assume that for every document it had been assigned to a set of the categories I'll go is try to identify appropriate ways based on this additional information, which is a category information with documents, right?"
        ],
        [
            "So the main idea of this paper is very simple.",
            "That is, we know that for each document we actually have two different representations.",
            "Once we can represent the document as a set of word and we also can represent each document as a set of categories.",
            "So if."
        ],
        [
            "The two representations are reasonable comprehensive in terms of representing the accountant of documents.",
            "Then we actually can compute the similarity of two documents based on these two different representation.",
            "It's called SW&SC, right?",
            "So if you can."
        ],
        [
            "If I if a term weight is appropriate, then we would expect these two different similarity management will be consistent more or less to each other across the entire collection and that is Member Division of the entire work.",
            "So let's."
        ],
        [
            "Describe this whole idea formally.",
            "That is, for each document DI you have two representation, the backward backward representation, which is W vector WI and the set the adventure of the categories CI here.",
            "See each element in CI is a binary.",
            "Variables indicate if the document belongs to a certain category."
        ],
        [
            "OK, so now this pick up the most distinguished.",
            "The similarity measurement.",
            "Let's assume that the similarity measurement altered document is simply the weighted dot product between two vectors, right?",
            "Things?",
            "We have two different representation for each document.",
            "Then we have two different weighted dot products.",
            "So here I introduce mu to represent term ways and also I introduce it to represent the category which.",
            "So now with this."
        ],
        [
            "Rotation our problem is.",
            "That I would like to find out the category which eat enter whitmill such that they are consistent across the entire categories.",
            "So here we can introduce a loss of function error to compare how different the two similarity management is and we would like to find them UND to such that the last function is minimized across entire category entire collection."
        ],
        [
            "Alright, so well, the first approach is the I called regression approach.",
            "In this case, I assume that this law is the function is simply square of difference.",
            "Between these two."
        ],
        [
            "As in measurements right?",
            "And no surprise that you can easily write this.",
            "Cogentin function."
        ],
        [
            "So our goal is try to find them you and either such that these object function F regular regression is minimized while apparently you see that actually the minimizer here is simply just said the mutiny there to be 0, because that's apparently the minimum value you can reach right so?"
        ],
        [
            "So to avoid such a trivial solutions, we can introduce some kind of control."
        ],
        [
            "Early on.",
            "Other ways, for example, you can choose L2 constraints and turns out this is a simple eigenvalue problem eigenvector problem.",
            "But"
        ],
        [
            "The one problem with L2 regularizer active constraint is we may be able to get negative term ways, which is really contradict our intuition because when you get a negative term ways would make sense if you have two documents and even two document actually shared a common word on the words which has negative weights, which means whenever you see the common words for the two documents that actually these two document becomes less similar.",
            "And this definitely is not consistent with our intuition, so we need to sort of."
        ],
        [
            "By the negative ways.",
            "By introducing further nonnegative constraints as results, you can."
        ],
        [
            "I have this optimization function which projective object function and with nonnegative constraints plus everyone constraint, error, unknown constraint, and apparently this can be solved very efficiently using the quadratic programming technicals.",
            "So our."
        ],
        [
            "Yeah, the other approach that we take, I call the probabilistic approach.",
            "In this case, we do not compare."
        ],
        [
            "The similarity directly.",
            "Instead, we first convert similarity into probability using logit function and then we compare these two on probability function."
        ],
        [
            "Using simple cross entropy.",
            "OK, so in this case our object function Now becomes this negative cross engine."
        ],
        [
            "By comparing two probability function based on the two different measurements and again you can easily write this as an optimization problem.",
            "Again you see I introduce a regularizer Alpha W and FC just to prevent waste going to it from Infinity."
        ],
        [
            "So to solving the optimization problem is a little bit harder, but because we have these two set of sort of separate kind of independent weights, the weights for category and waits for turns.",
            "So we can employ alternating optimization.",
            "This is nature strategy, right?",
            "So first you can fix the category weights and learn the proper term weights, then match with the category similarity and the second part is you can.",
            "Learned."
        ],
        [
            "Turn weights, I learned the categories by fixing the term ways and so on and so forth, and again you can playing a sort of optimization trick like the bounding algorithm just to make the multivariate optimization become the.",
            "A single varage optimization problem.",
            "The details are all in the paper.",
            "I'm not going to the details."
        ],
        [
            "OK, so let me show you a little bit experiments.",
            "Empirical study that we have done."
        ],
        [
            "We use this collection coming from imageclass.",
            "It consists of."
        ],
        [
            "Around 30,000 different documents and all the documents being signed to 933."
        ],
        [
            "Even the categories average speak each document being signed to about five different categories.",
            "OK."
        ],
        [
            "I'm sorry.",
            "They they have some structure which I didn't look into the details an actually we don't have many queries totally.",
            "We have 30 queries available provided by the image craft.",
            "We use the file queries as the.",
            "As the training and the."
        ],
        [
            "The rest, as the evaluation tab.",
            "With very standard and we use the very very standard version metric.",
            "You know, average precision and precision for top retrieve documents.",
            "Also, precision recall curves.",
            "So now the baseline that we compare in our."
        ],
        [
            "Empirical study as three different baseline at actually four different baseline.",
            "The first issue that."
        ],
        [
            "Best approach to capital approach in the term TF IDF family.",
            "Also, this unigram language model approach using the generic mercy smoothing.",
            "And in addition to this, we are providing two different baselines that are utilized.",
            "The category information first one is very naive.",
            "We simply replaced inverse document frequency with.",
            "I called inverse the category frequency.",
            "And sex."
        ],
        [
            "One we actually modified core expansion little bit, so each time instead of introducing the common word appearing in the top, retrieve the document.",
            "We actually introduced common categories that appear in the top.",
            "Retrieve the documents again.",
            "You can easily using the language model approach to incorporating the expanded query.",
            "The category information here."
        ],
        [
            "K. So let's look into results.",
            "Little bits.",
            "OK, so here are the precision recall curve horizontally.",
            "We have a different precision recall point and vertically we have a precision and point, so you expect the best performing well should always on the right up corner."
        ],
        [
            "Right, and so the top two curve, the red curve and the blue curve and corresponding to the approach that we proposed in the paper.",
            "As you can see 40 record point, it is well above the rest of approaches.",
            "OK, we."
        ],
        [
            "Also examine the precision for the top.",
            "Retrieve the documents and again you observe that on the proposed approach for server Cerro on top Retrieve document is achieved status even better results compared to the rest of.",
            "Method and the other thing you notice here is the two knife implementation of exploring the category information.",
            "I called ICF inverse category frequency and the CQE the the query category query expansion.",
            "They actually not necessary and improve the performance compared to the simple bass line that does not exploit the category information.",
            "So again this indicate even though the site information.",
            "Category is useful.",
            "We have to explore this information wisely instead of just do it in a simple way."
        ],
        [
            "So finally I actually present here the precision across different query IDs, so horizontal is a different query ID and vertical is a different.",
            "Is the precision for different methods.",
            "And so the red bark responded to the public approach for the framework of exploring the category information.",
            "And so if you examine the entire graph, you'll see 16 out of 25 case, we're able to ask if we improve the accuracy performs.",
            "So as the sun."
        ],
        [
            "Three of my talk.",
            "In this talk, we proposed a framework of two exploring the category information and to determine the proper ways for terms.",
            "If we should retrieval, and we are proposing two different realization, one based on regression.",
            "They had one based on the probabilistic approach, and we did the empirical study using the image, collect collection and search the promising results.",
            "And as that as the future work are, we plan to first of all actually improve.",
            "Really, efficiency, because the entire optimization scheme is rather messy right now, and so to to handle large data.",
            "Large data set also actually the other fairly interesting direction we plan to explore is to apply this idea for the content based image retrieval.",
            "However, we assume that image has a certain annotated annotations, so in this case you have image collection, but each image in addition to its visual information.",
            "You also get a certain.",
            "Annotation words coming from a human being.",
            "Now if you want to search the image for the database that similar to your query image, you can sort of employ the same kind of ideas, because right now you can treat the.",
            "The annotation as kind of category information and visual information is like kind of words representation document, so I thought that's interesting.",
            "Things that to explore for this framework.",
            "Again, thank you for staying for the last talk and not leaving because it's close to break.",
            "OK, thanks.",
            "Oh, I should make it open question yes question.",
            "Home.",
            "How would you stop?",
            "I don't remember talking about the complexity.",
            "So the house is full scale for your text.",
            "You mean complexity of optimization?",
            "There's a say so.",
            "So here I have presented sort of two different approaches wise regression.",
            "The other one is based on.",
            "Based on the the logit function, converting the similarity into probability actually are the regression approach.",
            "You can do very efficiently because this existing software for quadratic programming that is very well known.",
            "You can do really efficient second part actually is quite massive, because we, even though we we get the reasonable, not complicated object function.",
            "But unfortunately you can show this is non convex and that actually make everything really messy.",
            "So I have to meet the second one is.",
            "Is fairly complicated.",
            "I didn't care.",
            "Creative, they compress the bonds, but it's really complicated.",
            "Tech stuff.",
            "I mean, I'm I'm really intrigued by this so I can imagine a lot of people would like to explore this so so we did the experiment with this rather tiny small collection, because this in this collection is only like 30,000 documents and doing so this document is rather short, less than 100 words.",
            "So we actually explore the advantage of this collection.",
            "But yes, it will be really will be a big issue.",
            "How to make it.",
            "Efficient, yes.",
            "Thanks, thank you.",
            "Excellent presentation."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you're in trouble using category information.",
                    "label": 1
                },
                {
                    "sent": "Alright, so first outline my talk.",
                    "label": 0
                },
                {
                    "sent": "I will first provide.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An overview of two weighting method information retrieval and then I will discuss the framework of running term words.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the category information.",
                    "label": 0
                },
                {
                    "sent": "And then finally our discuss our empirical study results.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in generally of information retrieve.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are two different families of term weighting method.",
                    "label": 0
                },
                {
                    "sent": "The first one is based on TF IDF weights, and this provides the most popular.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer To use the information retrieval.",
                    "label": 0
                },
                {
                    "sent": "So it consists of the three factors one is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The TF factors which describe how frequently award appear in the document, and then the second factor is, is so called.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The inverse document frequency factors which describe how rare the word appear in the entire collection and the third one is the normalization factor for document lens which is trying to correct the bias of long long document.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so while probably most well known on TF IDF method is called Okapi.",
                    "label": 0
                },
                {
                    "sent": "Then the second category of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Often with the method is called is based on the length.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motox so the idea is you assume that documents generated by.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A static language model and then you can apply your favorite estimate.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Either like maximum likelihood this measure to estimate the most like most property language model for each document.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And typically speaking to avoid sparse data problem, we are incorporating smoothing technicals like generally closest smoothie or derechas music.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the simplest unigram language model using the GM smoothing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let me briefly discuss the problem with existing term weighting methods in information retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The essential difficulty with all.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The term weighting, measured in information retrieval is lack of supervision.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you looking.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the TF IDF weighting method is based on the intuition that if the world appeared really close entire collection then this word is very likely to informative to the content of document how they were these?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of intuition may not necessary to be true for many cases.",
                    "label": 0
                },
                {
                    "sent": "For example, you may run into a typo, which apparently is a real word, but it really does not tell you anything about the document.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the second second, if you look at length mode approach, which basically is a generated approach for describing the appearance of the words in the document as the results turns out, most probability mass is concentrated on common words, not the words that are informed.",
                    "label": 0
                },
                {
                    "sent": "If the content of document.",
                    "label": 0
                },
                {
                    "sent": "So again, this is not the appropriate approach to determine the term waits for the document words.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I want to describe in this talk is how can we learn the term weights based on the data using inside information.",
                    "label": 0
                },
                {
                    "sent": "In particular here I will describe the framework using the category information as the basis to learn the term weights.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's first look at the set up the problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we assume that for every document it had been assigned to a set of the categories I'll go is try to identify appropriate ways based on this additional information, which is a category information with documents, right?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main idea of this paper is very simple.",
                    "label": 1
                },
                {
                    "sent": "That is, we know that for each document we actually have two different representations.",
                    "label": 0
                },
                {
                    "sent": "Once we can represent the document as a set of word and we also can represent each document as a set of categories.",
                    "label": 1
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The two representations are reasonable comprehensive in terms of representing the accountant of documents.",
                    "label": 0
                },
                {
                    "sent": "Then we actually can compute the similarity of two documents based on these two different representation.",
                    "label": 0
                },
                {
                    "sent": "It's called SW&SC, right?",
                    "label": 0
                },
                {
                    "sent": "So if you can.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I if a term weight is appropriate, then we would expect these two different similarity management will be consistent more or less to each other across the entire collection and that is Member Division of the entire work.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe this whole idea formally.",
                    "label": 0
                },
                {
                    "sent": "That is, for each document DI you have two representation, the backward backward representation, which is W vector WI and the set the adventure of the categories CI here.",
                    "label": 0
                },
                {
                    "sent": "See each element in CI is a binary.",
                    "label": 0
                },
                {
                    "sent": "Variables indicate if the document belongs to a certain category.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now this pick up the most distinguished.",
                    "label": 0
                },
                {
                    "sent": "The similarity measurement.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that the similarity measurement altered document is simply the weighted dot product between two vectors, right?",
                    "label": 0
                },
                {
                    "sent": "Things?",
                    "label": 0
                },
                {
                    "sent": "We have two different representation for each document.",
                    "label": 0
                },
                {
                    "sent": "Then we have two different weighted dot products.",
                    "label": 0
                },
                {
                    "sent": "So here I introduce mu to represent term ways and also I introduce it to represent the category which.",
                    "label": 0
                },
                {
                    "sent": "So now with this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rotation our problem is.",
                    "label": 0
                },
                {
                    "sent": "That I would like to find out the category which eat enter whitmill such that they are consistent across the entire categories.",
                    "label": 0
                },
                {
                    "sent": "So here we can introduce a loss of function error to compare how different the two similarity management is and we would like to find them UND to such that the last function is minimized across entire category entire collection.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so well, the first approach is the I called regression approach.",
                    "label": 0
                },
                {
                    "sent": "In this case, I assume that this law is the function is simply square of difference.",
                    "label": 0
                },
                {
                    "sent": "Between these two.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As in measurements right?",
                    "label": 0
                },
                {
                    "sent": "And no surprise that you can easily write this.",
                    "label": 0
                },
                {
                    "sent": "Cogentin function.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal is try to find them you and either such that these object function F regular regression is minimized while apparently you see that actually the minimizer here is simply just said the mutiny there to be 0, because that's apparently the minimum value you can reach right so?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to avoid such a trivial solutions, we can introduce some kind of control.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Early on.",
                    "label": 0
                },
                {
                    "sent": "Other ways, for example, you can choose L2 constraints and turns out this is a simple eigenvalue problem eigenvector problem.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The one problem with L2 regularizer active constraint is we may be able to get negative term ways, which is really contradict our intuition because when you get a negative term ways would make sense if you have two documents and even two document actually shared a common word on the words which has negative weights, which means whenever you see the common words for the two documents that actually these two document becomes less similar.",
                    "label": 0
                },
                {
                    "sent": "And this definitely is not consistent with our intuition, so we need to sort of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the negative ways.",
                    "label": 0
                },
                {
                    "sent": "By introducing further nonnegative constraints as results, you can.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have this optimization function which projective object function and with nonnegative constraints plus everyone constraint, error, unknown constraint, and apparently this can be solved very efficiently using the quadratic programming technicals.",
                    "label": 0
                },
                {
                    "sent": "So our.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, the other approach that we take, I call the probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "In this case, we do not compare.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The similarity directly.",
                    "label": 0
                },
                {
                    "sent": "Instead, we first convert similarity into probability using logit function and then we compare these two on probability function.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using simple cross entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case our object function Now becomes this negative cross engine.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By comparing two probability function based on the two different measurements and again you can easily write this as an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Again you see I introduce a regularizer Alpha W and FC just to prevent waste going to it from Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to solving the optimization problem is a little bit harder, but because we have these two set of sort of separate kind of independent weights, the weights for category and waits for turns.",
                    "label": 0
                },
                {
                    "sent": "So we can employ alternating optimization.",
                    "label": 1
                },
                {
                    "sent": "This is nature strategy, right?",
                    "label": 0
                },
                {
                    "sent": "So first you can fix the category weights and learn the proper term weights, then match with the category similarity and the second part is you can.",
                    "label": 1
                },
                {
                    "sent": "Learned.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn weights, I learned the categories by fixing the term ways and so on and so forth, and again you can playing a sort of optimization trick like the bounding algorithm just to make the multivariate optimization become the.",
                    "label": 0
                },
                {
                    "sent": "A single varage optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The details are all in the paper.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to the details.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me show you a little bit experiments.",
                    "label": 0
                },
                {
                    "sent": "Empirical study that we have done.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use this collection coming from imageclass.",
                    "label": 0
                },
                {
                    "sent": "It consists of.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around 30,000 different documents and all the documents being signed to 933.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even the categories average speak each document being signed to about five different categories.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "They they have some structure which I didn't look into the details an actually we don't have many queries totally.",
                    "label": 0
                },
                {
                    "sent": "We have 30 queries available provided by the image craft.",
                    "label": 0
                },
                {
                    "sent": "We use the file queries as the.",
                    "label": 0
                },
                {
                    "sent": "As the training and the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rest, as the evaluation tab.",
                    "label": 0
                },
                {
                    "sent": "With very standard and we use the very very standard version metric.",
                    "label": 0
                },
                {
                    "sent": "You know, average precision and precision for top retrieve documents.",
                    "label": 1
                },
                {
                    "sent": "Also, precision recall curves.",
                    "label": 0
                },
                {
                    "sent": "So now the baseline that we compare in our.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Empirical study as three different baseline at actually four different baseline.",
                    "label": 0
                },
                {
                    "sent": "The first issue that.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best approach to capital approach in the term TF IDF family.",
                    "label": 1
                },
                {
                    "sent": "Also, this unigram language model approach using the generic mercy smoothing.",
                    "label": 0
                },
                {
                    "sent": "And in addition to this, we are providing two different baselines that are utilized.",
                    "label": 0
                },
                {
                    "sent": "The category information first one is very naive.",
                    "label": 0
                },
                {
                    "sent": "We simply replaced inverse document frequency with.",
                    "label": 0
                },
                {
                    "sent": "I called inverse the category frequency.",
                    "label": 1
                },
                {
                    "sent": "And sex.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One we actually modified core expansion little bit, so each time instead of introducing the common word appearing in the top, retrieve the document.",
                    "label": 0
                },
                {
                    "sent": "We actually introduced common categories that appear in the top.",
                    "label": 0
                },
                {
                    "sent": "Retrieve the documents again.",
                    "label": 0
                },
                {
                    "sent": "You can easily using the language model approach to incorporating the expanded query.",
                    "label": 1
                },
                {
                    "sent": "The category information here.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. So let's look into results.",
                    "label": 0
                },
                {
                    "sent": "Little bits.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are the precision recall curve horizontally.",
                    "label": 0
                },
                {
                    "sent": "We have a different precision recall point and vertically we have a precision and point, so you expect the best performing well should always on the right up corner.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and so the top two curve, the red curve and the blue curve and corresponding to the approach that we proposed in the paper.",
                    "label": 0
                },
                {
                    "sent": "As you can see 40 record point, it is well above the rest of approaches.",
                    "label": 0
                },
                {
                    "sent": "OK, we.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also examine the precision for the top.",
                    "label": 0
                },
                {
                    "sent": "Retrieve the documents and again you observe that on the proposed approach for server Cerro on top Retrieve document is achieved status even better results compared to the rest of.",
                    "label": 0
                },
                {
                    "sent": "Method and the other thing you notice here is the two knife implementation of exploring the category information.",
                    "label": 0
                },
                {
                    "sent": "I called ICF inverse category frequency and the CQE the the query category query expansion.",
                    "label": 0
                },
                {
                    "sent": "They actually not necessary and improve the performance compared to the simple bass line that does not exploit the category information.",
                    "label": 0
                },
                {
                    "sent": "So again this indicate even though the site information.",
                    "label": 0
                },
                {
                    "sent": "Category is useful.",
                    "label": 0
                },
                {
                    "sent": "We have to explore this information wisely instead of just do it in a simple way.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally I actually present here the precision across different query IDs, so horizontal is a different query ID and vertical is a different.",
                    "label": 0
                },
                {
                    "sent": "Is the precision for different methods.",
                    "label": 0
                },
                {
                    "sent": "And so the red bark responded to the public approach for the framework of exploring the category information.",
                    "label": 0
                },
                {
                    "sent": "And so if you examine the entire graph, you'll see 16 out of 25 case, we're able to ask if we improve the accuracy performs.",
                    "label": 0
                },
                {
                    "sent": "So as the sun.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three of my talk.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we proposed a framework of two exploring the category information and to determine the proper ways for terms.",
                    "label": 1
                },
                {
                    "sent": "If we should retrieval, and we are proposing two different realization, one based on regression.",
                    "label": 1
                },
                {
                    "sent": "They had one based on the probabilistic approach, and we did the empirical study using the image, collect collection and search the promising results.",
                    "label": 1
                },
                {
                    "sent": "And as that as the future work are, we plan to first of all actually improve.",
                    "label": 1
                },
                {
                    "sent": "Really, efficiency, because the entire optimization scheme is rather messy right now, and so to to handle large data.",
                    "label": 0
                },
                {
                    "sent": "Large data set also actually the other fairly interesting direction we plan to explore is to apply this idea for the content based image retrieval.",
                    "label": 0
                },
                {
                    "sent": "However, we assume that image has a certain annotated annotations, so in this case you have image collection, but each image in addition to its visual information.",
                    "label": 0
                },
                {
                    "sent": "You also get a certain.",
                    "label": 0
                },
                {
                    "sent": "Annotation words coming from a human being.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to search the image for the database that similar to your query image, you can sort of employ the same kind of ideas, because right now you can treat the.",
                    "label": 0
                },
                {
                    "sent": "The annotation as kind of category information and visual information is like kind of words representation document, so I thought that's interesting.",
                    "label": 0
                },
                {
                    "sent": "Things that to explore for this framework.",
                    "label": 0
                },
                {
                    "sent": "Again, thank you for staying for the last talk and not leaving because it's close to break.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "Oh, I should make it open question yes question.",
                    "label": 0
                },
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "How would you stop?",
                    "label": 0
                },
                {
                    "sent": "I don't remember talking about the complexity.",
                    "label": 0
                },
                {
                    "sent": "So the house is full scale for your text.",
                    "label": 0
                },
                {
                    "sent": "You mean complexity of optimization?",
                    "label": 0
                },
                {
                    "sent": "There's a say so.",
                    "label": 0
                },
                {
                    "sent": "So here I have presented sort of two different approaches wise regression.",
                    "label": 0
                },
                {
                    "sent": "The other one is based on.",
                    "label": 1
                },
                {
                    "sent": "Based on the the logit function, converting the similarity into probability actually are the regression approach.",
                    "label": 0
                },
                {
                    "sent": "You can do very efficiently because this existing software for quadratic programming that is very well known.",
                    "label": 0
                },
                {
                    "sent": "You can do really efficient second part actually is quite massive, because we, even though we we get the reasonable, not complicated object function.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately you can show this is non convex and that actually make everything really messy.",
                    "label": 0
                },
                {
                    "sent": "So I have to meet the second one is.",
                    "label": 0
                },
                {
                    "sent": "Is fairly complicated.",
                    "label": 0
                },
                {
                    "sent": "I didn't care.",
                    "label": 0
                },
                {
                    "sent": "Creative, they compress the bonds, but it's really complicated.",
                    "label": 0
                },
                {
                    "sent": "Tech stuff.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm I'm really intrigued by this so I can imagine a lot of people would like to explore this so so we did the experiment with this rather tiny small collection, because this in this collection is only like 30,000 documents and doing so this document is rather short, less than 100 words.",
                    "label": 0
                },
                {
                    "sent": "So we actually explore the advantage of this collection.",
                    "label": 0
                },
                {
                    "sent": "But yes, it will be really will be a big issue.",
                    "label": 0
                },
                {
                    "sent": "How to make it.",
                    "label": 0
                },
                {
                    "sent": "Efficient, yes.",
                    "label": 0
                },
                {
                    "sent": "Thanks, thank you.",
                    "label": 0
                },
                {
                    "sent": "Excellent presentation.",
                    "label": 0
                }
            ]
        }
    }
}