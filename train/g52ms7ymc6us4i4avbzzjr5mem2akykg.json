{
    "id": "g52ms7ymc6us4i4avbzzjr5mem2akykg",
    "title": "Tutorial on Machine Learning Reductions",
    "info": {
        "author": [
            "John Langford, Toyota Technological Institute at Chicago"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2005",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss05us_langford_mlr/",
    "segmentation": [
        [
            "Then we should begin.",
            "Can everyone here alright?",
            "OK, so notice that food.",
            "Write down a bit for lunch.",
            "We're compensating that for that in two ways.",
            "One way is ordering more food.",
            "In other ways, I want to mention that there are four kinds of people in the world.",
            "Right, OK, so there's four people.",
            "There's those who paid for learning summer school and you know, they're paying for their food.",
            "This goes those who gave financial aid to and we paid for their food.",
            "So what this lecture is about?",
            "Is how to avoid inventing a new algorithm.",
            "New learning algorithm for every learning problem.",
            "Which is it's a very common kind of thing people you know, like oh, I have a new learning problem.",
            "Let's invent a new learning algorithm.",
            "But maybe we can avoid that, and maybe that it's beneficial in a lot of ways.",
            "If you have questions during this talk, you should feel free to ask them as they come up.",
            "I kind of like things interactive.",
            "Also, I have this feeling that I lose everyone."
        ],
        [
            "OK, so learning problems.",
            "There's a lot of different kinds of learning problems.",
            "There's sort of binary ones.",
            "Is it raining outside or not?",
            "There are custom Steve ones like which commute is faster and other people are actually commuting.",
            "Some people claim their commuting 2 hours per day.",
            "To see this talk.",
            "Wow.",
            "I couldn't do that.",
            "But if you're commuting, you have a choice of different paths you can take, right?",
            "There's different routes available and you want to choose the path, which kind of minimizes your time.",
            "It's it's a cost of classification problem.",
            "And then there's issues like which stock will go up.",
            "It's it's a favorite.",
            "And can I make the light before it turns red?",
            "Right so.",
            "Stopping early is OK because the penalty is not that bad.",
            "You lose a minute or two, but if you go too far, well that's very bad.",
            "'cause you get rammed.",
            "So it's an important weighted classification problem.",
            "OK.",
            "Different kinds of learning problems.",
            "And then reductions about how we can solve all of these with the same learning algorithm."
        ],
        [
            "So you can ask questions so a reduction takes one type of learning algorithm into different type of learning algorithm.",
            "So it solves a different type of learning problem.",
            "You can ask questions like how do I solve importance weighted binary classification of classifier?",
            "How do I sell custom classification or reinforcement learning or regression as many different kinds of learning problems?",
            "And one thing I should mention is.",
            "Although I'm going to be talking about reducing things to classification, it's easy to imagine that using another primitive, like say, regression.",
            "OK, so before I start into the grinding method."
        ],
        [
            "Maybe we should feel a little bit motivated and these are the basic for region 4 reasons why.",
            "You want to play with reductions in learning.",
            "So I'll go through each of these individually."
        ],
        [
            "OK, so.",
            "There's these binary classifiers which just decide between two possibilities.",
            "Then there are the multiclass classification problems.",
            "So one of the approaches that people do to solving multiclass classification is one against all, so they create a bunch of binary classifiers, one for each class.",
            "For class I, the binary classifier is it class I or not?",
            "Is a class J or not?",
            "Is the class well one there not too or not whatever?",
            "And then at prediction time what happens is you evaluate if you have K classes, then evaluate all K classifiers.",
            "And if only one raises his hand, then you say, yeah, it must be this guy.",
            "Ann, if two of them raise their hands, then you say home.",
            "I'll just randomize between the two.",
            "And if none of them raise their hands, then.",
            "They just kinda makes them all don't have any information.",
            "OK, so that's one against all.",
            "And then.",
            "If you go and you analyze the mathematics of this.",
            "What you discover is that this failure mode.",
            "Is significantly less severe than this failure mode, 'cause you still haven't even chance of picking the right guy in this kind of failure mode.",
            "But Dad's picking the right guy when this happens there.",
            "Much worse, like 1 / K. So you can take advantage of that and you can create instead of reducing to binary classification.",
            "You can reduce to.",
            "Importance weighted classification.",
            "So the importance of raising your hand when you're supposed to is larger than otherwise.",
            "Then you can reduce that to binary classification.",
            "You can compare these two methods.",
            "It turns out that this weighted one against all.",
            "He's just always better.",
            "This is the accuracy, so things above the line are wins for weighted one against all.",
            "This is a bunch of different.",
            "Learning algorithms and a bunch of different datasets, and I guess one time we did slightly worse, but every other time.",
            "We did better, sometimes significantly better.",
            "So there's some motivation in terms of working.",
            "And then."
        ],
        [
            "Another motivation is sort of codewise.",
            "Reductions are sort of inherently modular.",
            "The we do not appear inside of the workings of a learning algorithm, so we do not need to, yes.",
            "It's not reducing from waited one is is that something reduction?",
            "The reduction is from multiclass classification to binary classification.",
            "So that makes sense.",
            "So we're reducing from multiclass to binary, and you can do it sort of directly with this one against all.",
            "Or you can go from multiclass too.",
            "Weighted importance weighted binary and then to binary.",
            "So I was comparing.",
            "This.",
            "2.",
            "This.",
            "Reduction.",
            "Parameters of the given algorithm.",
            "No importance weighted binary.",
            "There's no tweaking parameters of algorithms, so all the reductions that I'm talking about are black box reductions.",
            "You can apply them to any classification algorithm, and you can apply them.",
            "Just given some standard interface to all classification algorithms, there are no special parameters or anything like that.",
            "So I was comparing this direct approach.",
            "For doing one versus all.",
            "2.",
            "Waited one versus all.",
            "Composed with something I want to talk about later, which is this.",
            "Costing production.",
            "Yeah.",
            "We don't work.",
            "Reductions to each other would make sense to compare to multiplication.",
            "Yeah, I agree.",
            "One thing which is important is that.",
            "Reductions are not competing with themselves, right?",
            "I mean, these are two different reductions.",
            "The one versus all in the way to win versus all composed with disgusting.",
            "But what you really want to compare with is.",
            "Otherwise you're doing multiclass classification, right?",
            "'cause you want to say this is the best way.",
            "That comparison is not done right here.",
            "It is done for some of the other things that are present later.",
            "OK, so modularity.",
            "Right no parameter tweaking.",
            "Just thinking about the classification algorithm with particular type.",
            "Or just using it as you would use things with that type.",
            "We're not actually fiddling with any parameters in the classification algorithm.",
            "You can reuse your old learning algorithms via these reductions, right so?",
            "Thanks from not having to think so much viewpoint in particular.",
            "Think there are maybe.",
            "Maybe there's 50 classification algorithms that have been invented.",
            "Which is pretty impressive because.",
            "Mr 50 public datasets.",
            "But if you can form a reduction to classification.",
            "Then you have 50 algorithms to apply to whatever learning problem you have.",
            "It takes awhile for 50 algorithms.",
            "And then.",
            "Yeah, you can.",
            "Actually, in addition to using the learning algorithms because it's entirely black box, you can use rolled code.",
            "Just a matter of matching up the interfaces, right?",
            "OK, so.",
            "Right, this is sort of a research motivation.",
            "Uh.",
            "What is reductionism?",
            "Reductionism is figuring out how to cut your."
        ],
        [
            "Homes in the small problems and then solve the small problems in the compose them to solve a big problem.",
            "And one of the reasons was kind of an interesting direction to look research wise, because this approach has worked extremely well at certain times in the past.",
            "So in particular.",
            "Computations are kind of reduced to a transistor.",
            "If you think about it.",
            "Just do one very simple computation and you know compose the right way and look it's an Opteron.",
            "And.",
            "People are getting very good at rendering scenes.",
            "The basic element of rendering is rendering a triangle, or maybe a very simple Polygon.",
            "And then actually much of science kind of follows this reductionist approach.",
            "So you take some.",
            "You try to figure how to predict the most elementary things, and then you sort of compose these predictions to figure out where the spaceship goes and so forth.",
            "OK, so.",
            "Last thing is that it's actually pretty easy to use.",
            "So figure out what the type of your learning problem is.",
            "This is something that you always need to do.",
            "And then.",
            "Maybe you can find some premade reduction.",
            "From your type of learning problem to whatever learning items you want to apply.",
            "And then you just.",
            "Compose these things, throwing your data and you get a predictor for your learning problem.",
            "Maybe this is a bit easier than other approaches which So what are two other approaches?",
            "Another approach is to just invent a learning algorithm to solve your B type problem."
        ],
        [
            "Lee we have Zubin Ghahramani who is a very good Beijing coming on Friday I think.",
            "And he will tell you that the right thing to do is to.",
            "Figure out a prior over the different data generating distributions for B and then take the prior update with Bayes law to get the posterior and so forth, right?",
            "But maybe specifying the prior is a little bit difficult.",
            "It can be a little bit tricky at times.",
            "Maybe this is an approach which lets you just try something quickly and then if it works great, and if it doesn't, then maybe just sit down and think a lot more.",
            "So what I'm trying to say is that this is sort of a reasonable first pass approach on any particular problem.",
            "OK.",
            "So.",
            "We're going to be reducing to classification.",
            "And maybe it's worthwhile to specify exactly what I mean by classification.",
            "So these are pretty standard definitions.",
            "You have the classic."
        ],
        [
            "Asian problem is defined by some measure.",
            "An extra 01.",
            "So think of the user probability distribution.",
            "You can draw the example of an X&Y pair.",
            "The why is just going to be 1 bit.",
            "And the classifier something which just Maps from this feature space to that one bit.",
            "And then classifier learning algorithm.",
            "Is going to be given some examples.",
            "And it's going to find a classifier.",
            "Which small error rate so the error rate?",
            "Is the probability that under a random draw from D the classifier is wrong?",
            "You want to minimize the error rate.",
            "So one thing which is very different about this analysis from some of the other things you see.",
            "Is that we are not assuming.",
            "With the data.",
            "Is drawn independently and identically.",
            "So maybe this is something which seems kind of.",
            "Esoteric, but it's this is actually a pretty important thing.",
            "The reason why it's important is.",
            "All the analysis that we're going to tell you today.",
            "Applies to every learning problem of some particular type.",
            "Because you can always say there is some measure producing some test example.",
            "Right?",
            "And since we're not actually going to assume that.",
            "The training set.",
            "Is drawn from the same distribution.",
            "This will always apply.",
            "Yes.",
            "No.",
            "There, so I guess.",
            "We can always say that for any particular test example, there is some distribution that you draw that from.",
            "This distribution may be entirely dependent.",
            "It may be actually deterministic.",
            "Just draw all the weight is on one particular example.",
            "But there's always some process.",
            "Producing the test example.",
            "And that is the process that we're actually going to be analyzing.",
            "It's kind of.",
            "But it's interesting that you can.",
            "Analyze this at all.",
            "You'll see why it's possible later.",
            "Are there any questions about the basic setup?",
            "The link is tricky.",
            "We're not assuming IID data.",
            "OK.",
            "So this is the outline."
        ],
        [
            "I'm going to go through four reductions, one of whom is from importance weighted classification classification.",
            "So that's this one over here.",
            "Another one is from.",
            "Bring the class probability to predicting the classifier just predicting classification.",
            "Another one is for multiclass classification classification.",
            "Which is this?",
            "Another one is from customs to classification classification.",
            "So it gives you some range of tools and should hopefully give you some idea of how this all works."
        ],
        [
            "OK.",
            "So importance weighted classification is very much like classification.",
            "Except.",
            "You have importances.",
            "That your distribution is over, right?",
            "So in addition to drawing a feature and a binary prediction.",
            "You draw some importance.",
            "And then you still are trying to find a classifier, which is the binary predictor.",
            "You are given an importance weighted data set.",
            "And your goal?",
            "Is to minimize your importance weight loss so.",
            "If this I was gone and it would just be a probability distribution, it would be the same as previously.",
            "So if I is always one, we're just minimizing error rate.",
            "But if I is varying, then we need to write out the probabilities in expectation.",
            "We're looking at the expected importance weighted.",
            "Errors.",
            "So this is this is the stoplight example.",
            "The error rate or the importance of stopping too early.",
            "Well doesn't matter that much right?",
            "But the importance of stopping too late that could matter quite a bit.",
            "I should mention one thing.",
            "Which is that I just put the slides all online.",
            "Which means the.",
            "Everybody who really likes taking notes you can continue, but.",
            "You know they're online."
        ],
        [
            "OK, so this is a basic course theorem which is sort of folklore from decision theory.",
            "And this is that you can turn this importance weighted problem into just an error rate minimization problem, and it has to do with sort of renaming the probability distribution.",
            "So I guess the claim is that for every classifier.",
            "For every.",
            "Importance weighted distribution.",
            "If you form this other distribution which is just just kind of relates to examples by the importance.",
            "Then the importance weighted loss equals.",
            "The error rate.",
            "Of this new distribution.",
            "Times this normalization term.",
            "And it's a very simple.",
            "Example of reduction.",
            "Because it says that.",
            "If you minimize your error, expect to D. Then it's equivalent to minimizing.",
            "The importance weighted loss Spectra DBO.",
            "The proof is it's just very simple.",
            "We're kind of unwinding the definitions.",
            "So in particular, we have our importance weighted loss.",
            "And then I can write out the expectation as a sum.",
            "So we have the eye here.",
            "And then.",
            "This D is items DBO so we just get the D and we have the normalization term which comes out.",
            "And this is just an expectation of an indicator function, which is a probability.",
            "Times this normalization term and that probability is just the error rate in respect to D. Right?",
            "So this is the simplest version of reduction, so we should make sure everyone gets it.",
            "So this theorem says that if we can optimize our binary error rate with respect to D, that's entirely equivalent up to some normalization term.",
            "To optimizing our importance weighted loss.",
            "The normalization turned by the way, has to be there, so if you took all of your importances and you doubled them.",
            "You would expect your important weight loss to double.",
            "Yeah.",
            "I. OK, alright so maybe.",
            "Thank you.",
            "So this notation says that.",
            "Expectation respect to XY and I drawn from disobey.",
            "Right, so this is the expected importance.",
            "This OK for probability.",
            "This is X&Y, drawn from D. I'm just ignoring the I. Yeah.",
            "Series just one distribution disobey.",
            "So.",
            "I guess the claim is that having one distribution DBO is sufficient.",
            "For any problem.",
            "Where you care about this kind of thing.",
            "And the reason why it's sufficient is because we're not actually relying upon this IID stuff.",
            "So there's only one example we need to care about the next test example, because there's only one example, there's only one distribution.",
            "That example is drawn from.",
            "OK.",
            "So that's a basic theorem.",
            "There's a question left right.",
            "The question is, how do we use this basic theorem for our learning algorithms?",
            "So.",
            "The way that you can shift from one distribution to another using rejection sampling.",
            "So the way that rejection sampling works is.",
            "You pick some car."
        ],
        [
            "Instant, which is larger than all the importances.",
            "It was pretty impressive.",
            "You have such power there.",
            "And then for each sample.",
            "You're going to flip a coin with the bias of I oversee, right?",
            "So this is less than one.",
            "And if result is.",
            "You keep it.",
            "Otherwise, you throw it away.",
            "OK, so this is some.",
            "Distribution turn up and you can prove that sample drawn from.",
            "Disobey and then.",
            "Using this process are actually drawn from D. So there's actually a lot of other methods which come to mind when people want to 1st tell people this.",
            "For example, they talk about duplicating examples and so forth.",
            "According to the IMPORTANCES and.",
            "Although that that core theorem did not rely on independence.",
            "It is certainly the case that many of our learning algorithms rely on independence in the data set.",
            "So you will be.",
            "Unhappy.",
            "If you do tricks with duplicating examples or even sampling with replacement.",
            "I don't want to go into what that is exactly, but.",
            "You'll be unhappy, and the reason why you'll be happy.",
            "The reason why I would be happy with this and unhappy with these other approaches is that rejection sampling actually preserves independence in the data.",
            "So this is something which the theorem does not tell you to do, but because they're learning algorithms have this weakness that they really rely upon IID samples.",
            "It's a good idea to try to preserve independence.",
            "In your reduction.",
            "But there's a drawback here.",
            "You have a question.",
            "Right exactly the drawback drawback here is that you end up throwing away a lot of your samples, especially when you're importances vary a lot.",
            "So.",
            "This.",
            "Doing this once is what the theorem motivates, but maybe what you want to do in practice is sort of do it multiple times.",
            "So we can rejection sample multiple times from this esobi.",
            "And then you can learn some classifier.",
            "And then just use a majority vote over these classifiers make a prediction.",
            "Is it the same as what?",
            "Yeah so.",
            "Ada Boost also does a majority vote.",
            "A lot of learning algorithms, a lot of major learning algorithms in majority votes.",
            "And you can think of.",
            "A lot of learning algorithms just themselves is doing majority votes.",
            "So.",
            "Predict according to the majority vote is a very common mechanism in learning, say again.",
            "Ah.",
            "Right, so in Ada boost, this is going to be talked about in much more detail when rupture.",
            "Visits, but Ada boost.",
            "The way it works is.",
            "It's also sort of a reduction.",
            "It's using an Oracle learning algorithm.",
            "And what happens is you.",
            "But you feed your Oracle learning algorithm importance weighted data.",
            "And there's a problem of fitting that too.",
            "To classification, in one way to do that fit is actually to use rejection sampling.",
            "That's right, and I understand that actually works pretty well amongst the different possibilities.",
            "Another approach people often uses, they turn to tweak the learning algorithm directly."
        ],
        [
            "Take the importances into account, and.",
            "My question is, that's no better in practice.",
            "Yeah.",
            "Have one.",
            "Yeah, that would be dangerous.",
            "The reason why that would be dangerous is because you lose this independence that you're trying to preserve.",
            "So if you could take each of these individual pieces.",
            "In union them together you concatenate them together.",
            "Then it's possible to end up with the same example twice.",
            "And the process of drawing decent from DBO and then rejection sampling could be of the sort where there's just no probability of having the same sample twice.",
            "So.",
            "You have to kind of keep straight with the theory motivates.",
            "Maybe I'm not keeping it straight.",
            "The theory says you can do any method of transforming distributions you want, as long as you transform to the right distribution, you're Golden.",
            "But in practice, what happens is.",
            "Your data really likes your learning algorithm really likes independent data.",
            "And because you're learning algorithm related data, you want to try to preserve the independence.",
            "If it's there.",
            "OK. Alright.",
            "Basic album very simple.",
            "Fact dead simple.",
            "You know there's an interesting thing about this algorithm.",
            "It actually works pretty well.",
            "So.",
            "There was some contest.",
            "So."
        ],
        [
            "KDD.",
            "Knowledge decision in data mining conference has various prediction challenges.",
            "In 1998 they had some prediction challenge, which was an important weighted prediction problem.",
            "I think it was a spam optimization problem, so you're working for a charity.",
            "You want to send out Flyers and give us money, but you don't want to send out too many Flyers 'cause their Flowers are expensive.",
            "So you need to somehow predict.",
            "Who's going to give you money if you send them a flyer and who will not?",
            "And then?",
            "In the ideal case, you can just send Flowers only to people who will give you money.",
            "So the standard way of trying to measure this is in terms of profit.",
            "This is where the winner was.",
            "And if you use that very simple piece of code.",
            "With naive Bayes you get here with boosted naive Bayes you do as well as the winner was a decision tree.",
            "You actually do good bit better.",
            "And with support vector machine, you're still a little bit below the winner, yeah?",
            "Projection sample so much that you essentially made the problem say NP complete or something.",
            "No why?",
            "Do you ever run into problems where rejection samples so much?",
            "The problem would be complete?",
            "Huawei do I not?",
            "Sample exponential number of times.",
            "I think it's worse than that.",
            "You could get really unlucky and you could reject some sample until your data set is so small you just can't solve the problem.",
            "So your area will always be .5 no matter what.",
            "Anything which throws out data could lead to that problem.",
            "So.",
            "Basically the issue is if you rejection sampling you get a smaller data set.",
            "Is that mess you up?",
            "Sometimes the answer is certainly yes.",
            "So my favorite example of how this can mess you up is if you're trying to learn.",
            "So the function that you're trying to learn is a parity of a subset of your bits, so you have a bunch of bits for your feature space, and you want to learn a parody of the subset.",
            "Then the number of samples you need.",
            "Information theoretically is equivalent to the number of features.",
            "It's the same as a number of features because the way you solve this problem is you just do Gaussian elimination.",
            "And then you can figure out what the bits are.",
            "Sir.",
            "If you lose 1 sample there, you're hosed.",
            "But it seems that in practice that's not the way learning problems really are.",
            "So in practice this seems to work pretty well.",
            "OK, so.",
            "Some regional performance.",
            "I think we can assume that basically every method was tried on this thing, which is not about this is that we didn't do it in 1998, right?",
            "We invented this later, but it still hopefully a little bit compelling because it's a very simple algorithm.",
            "Which uses sort of a tried and true technique and does reasonably well.",
            "OK."
        ],
        [
            "So that's how you go from importance weight classification classification.",
            "Any questions about that?",
            "OK, so the next one is a bit more.",
            "Interesting.",
            "You want to go from.",
            "Class probability."
        ],
        [
            "Addiction to classification.",
            "So.",
            "Trying to predict so, here's the definition.",
            "It's the same the definition of the problem is the same As for.",
            "Classification.",
            "But now we're trying to make a probabilistic classifier which predicts some value between zero and one.",
            "And trying to minimize our error rate or to minimize or squared error.",
            "Right, so if the actual probability of.",
            "0.5 given some particular X.",
            "We would want this value to be.",
            ".5 because that would minimize expected value here.",
            "If you think about this right, you realize this is also regression.",
            "At least it's a special kind of regression kind of regression where.",
            "You always observe zeros or ones, and you want to try to predict the probability of zero or one.",
            "Yeah.",
            "Hello.",
            "Yeah.",
            "Yeah, so this is like regression.",
            "But it is fairly common to use the log loss, so the way log loss is defined is.",
            "Maybe?",
            "Log of one over.",
            "The probability of the class that actually you.",
            "Probably that you predicted the class that you observed.",
            "Just a minute.",
            "So, so long this is pretty interesting and the reason why it's particularly interesting is because it's unbounded.",
            "And because it's unbounded, it turns out.",
            "You can't do it in this framework.",
            "This is a fundamental difficulty in the final difficulties that unboundedness exactly.",
            "So if you have a classifier which exercises the full range of values, there's no way to actually formal reduction in the framework that I'll tell you, you can easily avoid that.",
            "You can have your classifier clip torda link, predict things between oh point 1.9 and then everything is bounded, and then as possible.",
            "But the story is not so nice when you want to log probability.",
            "If arbitrarily close to 0 is still pretty bad, you can still be pretty large.",
            "Alright, yeah question.",
            "Posterior probabilities.",
            "Yeah.",
            "You can think about it that way.",
            "One way to think about this is here's a technique for taking any classifier learning algorithm and extracting a classifier probability or posterior probability.",
            "Which is maybe not so obvious that you can do when just predicting either zero or one that something in the interval.",
            "Yeah.",
            "Yeah, connection.",
            "People are getting it right.",
            "CPO bank alright good.",
            "So the reason why my loss is this?",
            "Is because I can observe these an I can compute this?",
            "But what you really want is to say that this minus the actual probability is small, right?",
            "And there's a kind of a cool, which is that.",
            "Or equivalent of to some additive constant.",
            "Right then that added additive constant is there for every classifier the same constant.",
            "So, roughly speaking, isn't it is if you plug in the probability of.",
            "Minus the actual observed Y squared tick spectation.",
            "The constants independent of C. And that's the only difference between these two things.",
            "OK, so that's the problem.",
            "And then maybe we want to think a little bit about why we want to do probability sometimes."
        ],
        [
            "'cause you know there's a lot of situations where you don't want to do probabilities.",
            "You want to make some prediction and you want to either.",
            "That prediction tells you what you do need to do it, or you don't do it.",
            "But there are times when probabilities are kind of required.",
            "One of 'em is when you're talking to a doctor.",
            "Maybe the doctor doesn't want to be unemployed, so he would prefer that you actually tell him probability.",
            "He makes judgments about what to do.",
            "Rather than having classifier just tell a doctor what to do.",
            "And then another situation which comes up is sometimes you have a lot of.",
            "Data sources which are spread out all over the place.",
            "And maybe it's expensive to communicate between these different places, so you want to summarize sort of probability of Y given X from each of the individual sensors, and then somewhere later you want to use them.",
            "So instead of instead of transmitting all the information from every location to 1 location, you can just transmit maybe the probability.",
            "From one location to the central location.",
            "They can be arbitrarily cheaper.",
            "Another reason why this comes up is because you sort of want compatibility.",
            "You want you want to think about.",
            "You know, here's one component of the system.",
            "It's using some sort of Asian technique, and here's some other component systems using some sort of classifier technique.",
            "And now maybe you want to extract some sort of probabilistic predictions so that you can kind of few things in the right way.",
            "OK, so there's some reasons.",
            "And then."
        ],
        [
            "Then how does this technique work?",
            "So I'm going to call this technique probing.",
            "So this is basic observation that starts things off, which is that if you have a perfect classifier and a perfect classifier says one, that's saying the probability of y = 1 given X is greater than .5.",
            "You go, that's interesting.",
            "What if?",
            "I picked some.",
            "Random uniform random variable between zero and one.",
            "And make this importance weighted transformation.",
            "Right, so I can create an importance weighted data set.",
            "And the importance will be this Y -- P. So if Y is 0 then it's P and if Y is one is 1 -- P. And then you can.",
            "Think about that a bit and you realize that.",
            "If the classifier for this importance, weighted transformation is perfect.",
            "That implies that when classifier says one.",
            "The probability of y = 1 given X is greater than P. So here's a particular.",
            "This is a proof.",
            "In one table.",
            "So if the prediction is.",
            "0.",
            "Then the expected to the importance is going to be P. And the probability of 0 is minus y = 1 given X.",
            "And the prediction is 1.",
            "The importance is 1 -- P and probably why probably y = 1 given X.",
            "And these two are equal.",
            "When?",
            "P equals.",
            "This probability y = 1 given X.",
            "It was just a matter of taking the sign which works out.",
            "It has to workout 'cause we thought about that case first.",
            "Yeah.",
            "Yeah, just well.",
            "Isn't for this analysis, it doesn't even matter.",
            "You can pick it, just pick it however you want.",
            "Point 3.2 whatever.",
            "And this analysis holds.",
            "The statement always holds.",
            "And the question about how we actually do it and there will be picking it randomly.",
            "Or maybe will be picking it from a grid, but.",
            "Sort of randomly, yes.",
            "She's perfect.",
            "Say again hope to see is perfect other classifiers properties it like that.",
            "So the question is, how do you know that the classifier is perfect?",
            "The answer is you don't, but there's some tricks which will.",
            "Help."
        ],
        [
            "Alright, so.",
            "Here's the probing algorithm.",
            "This is sort of a dataflow graph.",
            "So you start with your binary data set at the top.",
            "And then you choose some peas however you want.",
            "And then you create some importance weighted datasets.",
            "And then you apply your importance weighted learning algorithm and you get your importance weighted classifiers.",
            "And then when you want to make a prediction with some particular feature, you get your importance weighted predictions.",
            "And then.",
            "What we're going to do is we're going to look at the transition point.",
            "And say, oh, we must have passed the threshold here.",
            "So I'll predict probability, which is somewhere in between.",
            ".1 and .5.",
            "Right, so maybe .3.",
            "Yeah.",
            "No you don't.",
            "Yeah, yeah, yeah yeah, I'll get to that in a moment.",
            "You have a question.",
            "Yeah, so these numbers here discretized the line between zero and one.",
            "There's a question about how you want to choose your resolution and so forth.",
            "It varies a bit, yeah?",
            "Yeah, it's exactly the right question to ask.",
            "The same question is here.",
            "OK, so there's a few problems with this technique."
        ],
        [
            "Right, so so maybe we actually want to reduce all the way to binary classification, but I just told you how to do that right?",
            "So we can take importance weights and we can turn that into binary classification using rejection sampling.",
            "And maybe OK.",
            "So this is kind of a interesting point.",
            "So for the costing reduction, I said look, you really want to.",
            "Projection sample multiple times.",
            "But for the probing reduction.",
            "You don't, it's preferable.",
            "To choose more peas.",
            "Over rejection sampling multiple times, right?",
            "And so this general principle.",
            "If you can figure out how to do it right.",
            "Getting a little bit information from a lot of places is better than getting a lot of information from a few places.",
            "Right, so how do you deal with?",
            "Is it true that the classifiers are not very good?",
            "So the way you deal with it is you sort the predictions until you actually get?",
            "Something like this?",
            "So if you get 101, you sort and you get this.",
            "Then you just pretend that this is what your classifier is predicted.",
            "There's a question about why that's right, but turns out this exactly the right thing to do.",
            "And then there's this issue of how you discretize on P. You can do it with the uniform grid, but if you're in the transductive setting where you have a bunch of unlabeled data that you want to predict the probability of.",
            "Probably label love.",
            "Then you can actually do it on demand so you can.",
            "You start out making just issues, P = .5 and you see how much of your data ended up on this side versus that side.",
            "Then you can discretize to optimize your squared error loss.",
            "OK, so.",
            "This looks easy, but all the magic is happening here.",
            "'cause I'm telling you precisely how to cope with errors.",
            "Right?",
            "So how well does this work?"
        ],
        [
            "So here we actually compare with a lot of different techniques for making probabilistic predictions.",
            "Rather than just comparing with other reductions.",
            "Alright, so we have a bunch of datasets here.",
            "He had drawn from many different locations.",
            "Some of them are UCI, some of them are these key today championships.",
            "Yeah, I think it's a combination of those two sources.",
            "And then you can look at the squared error.",
            "So smaller is better.",
            "And you can compare several different approaches.",
            "So one approach people use is they want to support vector machine in the data set.",
            "Then they fit some sort of sigmoid to the margin.",
            "So the margin itself gives a pretty bad probability prediction.",
            "And there's no reason why should give a good probability prediction because.",
            "Well, it's not supposed to be a good probability prediction.",
            "They don't optimize for that, but if you fit a sigmoid to the margin actually comes out reasonably well.",
            "So that's what this.",
            "SVM plus SIG is, and that's the first blue line.",
            "And then you can run.",
            "This probing technique on top of just the support vector machine.",
            "So it's with the second blue line is.",
            "And then for the red lines you can run naive Bayes, which just makes a.",
            "The class probability prediction.",
            "Uh.",
            "Or you can run naive vision, then fit the sigmoid to the margin.",
            "That naive Bayes tells you.",
            "So this one.",
            "And then.",
            "You can do now you can do probing on top of naive Bayes.",
            "OK, so it's nice and then you can have a decision tree.",
            "And you can just tell a decision tree.",
            "Tell me a probability right?",
            "And it will tell you.",
            "Vote at the leaves or maybe some sort of.",
            "Modified voted believes.",
            "Or you can tell you this tree.",
            "You can run this bagging technique on top of a decision tree.",
            "So how does bagging work the way the bagging works is.",
            "Particular data.",
            "Can you sample with replacement from it?",
            "Same number of times you have data, so you did the same size.",
            "But I only have about 2/3 of the data and there will be some duplicates.",
            "Then you can run your decision tree on this.",
            "And you can repeat this process many different times.",
            "And then you can look at the vote across the different classifiers that you learn.",
            "To predict some sort of probability.",
            "Now, in general, this isn't.",
            "This is like the margin is not that this vote doesn't tell you the probability.",
            "Oddly enough, it works pretty well in practice.",
            "And then.",
            "You can run this probing on top of C 4.5.",
            "We've also done experiments with logistic regression, which is turned out to be much like support vector machines.",
            "So if you look at what the smallest thing is here.",
            "It's often.",
            "Probing in particular probing on top of.",
            "C 4.5.",
            "So it's a bit hard to see everything because far away and the differences are too large.",
            "But it turns out that it works a little bit better than than everything else.",
            "On this test we did closest competitor is actually see 4.5 with bagging.",
            "So C 4.5 with probing was the best and then C 4.5 with bagging and then.",
            "A few others, one occasionally.",
            "There was a question.",
            "Those differences are significant, yeah?",
            "With this graph should tell you that the approach is sane.",
            "Right?",
            "I will tell you personally that there was no throwing away data sets for the student work, right?",
            "So we just kind of we picked them.",
            "We ran the data, we ran it and there was.",
            "This is what we got.",
            "So the approach is saying it works.",
            "Maybe it has a slight edge, maybe not.",
            "It's not really significant.",
            "OK, so.",
            "The approach works."
        ],
        [
            "Right, so is this trick we're going to use?",
            "Throughout the rest of this lecture, which is that for the analysis, it's extremely convenient to analyze only one classifier, or to analyze the case we reduced to only one classifier.",
            "And it turns out, anytime you have a bunch of parallel calls, different classifiers.",
            "You can think about them as one classifier.",
            "So the way that you do this is you just think about taking all of your parallel calls and Union and unioning them together or concatenating them together.",
            "So getting all the datasets together.",
            "And then the major.",
            "That you'll be learning respect to is sort of the union of the concatenation of the measures for the mixture of the measures.",
            "And then you can think about running.",
            "Your learning algorithm.",
            "On this concatenated data set.",
            "To get one classifier.",
            "So the little trick you have to do is you put the name of the classifier inside of the feature space.",
            "It's not clear if you want to do in practice.",
            "We actually have not experimented with it very much.",
            "Typically we just run one different classifier for each choice of P, right.",
            "One different learning algorithm, but theoretically at least you could just run your learning algorithm once.",
            "With the name of the classifier embedded in the feature space.",
            "And then.",
            "Once you have this one classifier, you can get all your old classifiers, but just you know, putting in the name.",
            "This is a trick to take many different parallel calls and turn it into one call.",
            "This trick is theoretical at the moment there have been no experiments one way or another about how well this works in practice.",
            "So with this trick means is that you can think about drawing.",
            "From the distribution that C is going to be evaluated on, so you can you see multiple times, and there's some particular measure over how you use and the way that you draw from that measure is you draw from your original distribution.",
            "Then you draw maybe uniform randomly from P. An you evaluate right so it gives you some distribution over X cross P. For extra speed cross why which is hidden when you evaluate.",
            "So this is a basic trick that we're going to use over and over again.",
            "Is it clear?",
            "Yeah.",
            "Do you train again?",
            "Once you have the classifier, see again, we are going to train the classifier again.",
            "No, there's no this.",
            "All the training happens here.",
            "And in particular, the real training happens inside of.",
            "A right?",
            "So we're going to be reducing to importance weighted classification and importance weighted classification just binary classification.",
            "Level of binary classification would just run a.",
            "And everything happens, all the training happens in this step.",
            "OK so this is trick.",
            "Stick is mathematically convenient.",
            "It's motivations empirically, are unclear at this point.",
            "Maybe we want to maybe not let me tell you.",
            "Let me tell you one reason why we might want to know.",
            "One reason why we why we might not.",
            "One reason why we might want to.",
            "Is that?",
            "This algorithm is computationally intensive, right?",
            "You're running learning algorithm multiple different times, and that takes a bit of work.",
            "Yeah, so if you just wanna learn is a way to run the learning algorithm once then maybe you can get the same result but.",
            "Perhaps much cheaper computationally.",
            "Anna reason not.",
            "So there is this issue of preserving independence, but it's actually probably possible to cope with that instead of taking the actual union.",
            "You can just subsample from the Union such a ways to preserve the independence.",
            "More worrisome is maybe by augmenting the feature space you made the problem harder.",
            "Possibly so you need to think about that a little bit.",
            "If you decide to actually do exactly this.",
            "OK."
        ],
        [
            "So there's a theorem.",
            "The theorem is actually kind of nice.",
            "It says for every classifier.",
            "So the classifier is taking this feature space, cross the name.",
            "And predicting 1 bit.",
            "For every distribution on extra 01.",
            "This is what you wanted, right?",
            "This is the probability of y = 1 given X minus.",
            "Is bounded by something and that something is OK, so we have some some distribution.",
            "And we have this process of picking AP.",
            "And then evaluating AC, which gives the new distribution over binary problems.",
            "So this is the name of the new distribution.",
            "OK, so.",
            "This is a distribution.",
            "So we have the error rate is fixed distribution.",
            "Minus the minimum error rate.",
            "It's possible.",
            "So if I just talked about error rate.",
            "This would not be a convincing theorem.",
            "The reason why I would not be convincing is because the problems that I've created are inherently noisy.",
            "We have to subtract off the error rate due to the noise.",
            "And then then the theorem tells us something interesting.",
            "So it says that this quantity here is a regret, right?",
            "It's how would we do this?",
            "How could we could do?",
            "Which is a regret.",
            "Says that if we have a small regret.",
            "Respect to for this C. Then we have good probability estimates.",
            "So this is kind of cool because we can't actually observe this.",
            "We can compute this and we can do this.",
            "We don't know this, we know that, but nevertheless, minimizing this is equivalent to optimizing that.",
            "Yeah.",
            "Are you generating the data suggest?",
            "Right?",
            "So all that I said was.",
            "We have all these recipes with the importances.",
            "Previous to P we add the name into the feature space.",
            "Then we concatenate.",
            "Yeah, doesn't matter for the theorem.",
            "Yeah, that's how we might want to do in practice, but they're equivalent as far as the distribution that's induced.",
            "So this process and that process of their induce the same distribution over samples that you evaluate your classifier on.",
            "Only question is whether or not the training set.",
            "He's actually independent.",
            "So preserving independence if you're actually going to do this in practice, you probably want to do something like this.",
            "Appointment are there any other questions?",
            "OK, so.",
            "This theorem is a little bit.",
            "Less trivial to prove."
        ],
        [
            "Alright, so.",
            "Here's a pretty picture.",
            "And what does it tell us?",
            "So the importance weighted loss.",
            "Of predicting insulated loss.",
            "So when the actual answer is 0 importance, weight loss predicting wrong is like this red line.",
            "And when the answer is 1, it's like this blue line.",
            "And it will turn out that we have a convex combination of zero and one when the probability is actually something between zero and one.",
            "The loss is also linear respect to whatever the actual probability is.",
            "So this is actually the way the proof works, just in a picture, so it will turn out that if the actual properties .5, then the importance weight loss you suffer.",
            "But predicting wrong here much.",
            "And then, because we had a sorting step.",
            "So the way we prove this, just no idea something like that, but you prove this.",
            "You say look.",
            "Let's suppose we have an adversary.",
            "The adversary controls the classifiers.",
            "The adversary has some budget of importance weighted errors he can make.",
            "And then the question is, how far can you shift us off from the actual probability?",
            "2.",
            "If his budget is equal to the area of his Gray region.",
            "This is exactly the the worst case error mode.",
            "So he wouldn't air over here.",
            "Because since we sort.",
            "It's just going to cancel with an error over here, right?",
            "So that would accomplish nothing that would get to make negative progress for the adversary.",
            "And he wouldn't air over here.",
            "Because he pays a large importance to have the same effect as an error here.",
            "So the way the adversary the adversarial classifiers are going to.",
            "Air to induce the most.",
            "Shift away from the actual probability.",
            "Is by starting with airs for the.",
            "The peas very near the real probability and kind of filling in everything.",
            "So that's the most efficient error mechanism for an adversary.",
            "Because it's the most efficient error mechanism.",
            "What are you going to get the theorem?",
            "Because this is clearly which is rising linearly and this is so this is the.",
            "Integral of X.",
            "Which is going to give us an X squared, which is exactly the squared error that you observed in the theorem.",
            "Right, I'm not certain this is clear yet.",
            "Welcome."
        ],
        [
            "Activity.",
            "Let's try to do the math.",
            "OK, so.",
            "First claim is that the expected importance is 1/2.",
            "The reason why this is important is 1/2.",
            "Is because you're integrating.",
            "This triangle here.",
            "Right, and so the integral of X or the integral of 1 -- X from zero to one is 1/2.",
            "So respect the distribution which draws from D and then draws from P uniformly.",
            "Just have that half.",
            "OK, so because we know the expected since that means that we can sort of analyze just the importance weighted problem and then you know, multiplied by the appropriate constant.",
            "Using that costing theorem.",
            "Right, OK, so we have our regret here.",
            "And this regret this expanding the definition.",
            "Is.",
            "So that factor of two is just one over the importance.",
            "Don't worry bout that bookkeeping.",
            "So if the expected.",
            "Importance weight loss minus the minimum of these two values.",
            "The the best classifier predicts according to the minimum these two values.",
            "OK, so there's a little trick here.",
            "We do is.",
            "We say.",
            "We just change our expectation, right?",
            "So we think about drawing from X&P and then drawing from why given D of XY drawing, why from the given X?",
            "OK. And we can think about sort of fixing X&P.",
            "Then we can do the analysis, fix that fix NP, and then at the end it's analysis holds.",
            "We can just take the expectation over X&P.",
            "So if it holds for all choices of X&P and holds for an expectation over the choices.",
            "Right so at that point we can just kind of ignore this quantity.",
            "And this is question about.",
            "What?",
            "What is this inner thing?",
            "So this is 2 possibilities.",
            "Either C is going to predict according to the minimum of these two or speaking to predicting the maximum of these two right?",
            "So either suffer the minimum of these two.",
            "Losses and importance weighted losses.",
            "Or it will suffer the maximum of two.",
            "So it is the minimum, the minimum minimum.",
            "This is 0.",
            "If it's the maximum.",
            "It is minus the minimum.",
            "And that's this.",
            "So it's going to be.",
            "That too.",
            "Times.",
            "First term, minus the second term magnitude.",
            "And then it turns out that's two times.",
            "Make it a P minus.",
            "Actually, probability of 1 given X.",
            "So let's see if we can see that.",
            "Right, so I guess you have a -- P * D even minus 3 * -- T, which cancels, which means you left with a -- P and AD and I guess it just changed the order over there.",
            "OK, so.",
            "Very simple and.",
            "And then you can see the customs classification.",
            "Is this two times as P?",
            "Minus actual probability would do next.",
            "And that.",
            "That implies this is this line here is correct.",
            "The the cost of misclassifying when P is here is that much or maybe twice that much.",
            "This is a straight line.",
            "OK, so this is the end of the proof."
        ],
        [
            "Do you have a budget of some number of errors?",
            "And then.",
            "Question is, how do you use your budget?",
            "Is an adversary.",
            "So you want to try to induce the maximum you want to make this probing process.",
            "Make the biggest mistake you can.",
            "So the first observation is that all classifications are going to be on one side of the of 1 given X, one side of the probability of 1 given X.",
            "So it's only going to be piece that are above or piece that are below, not both.",
            "The reason why won't be both is because when you sort, you just cancel out.",
            "And then the other thing to observe is that the adversary wants to make his errors for the the closer piece.",
            "The reason why it's so is because.",
            "The amount the adversary page to make a mistake here is that much of the amount that he makes page to make a mistake here is that much so he always wants to air is close to.",
            "The actual.",
            "Do you have one given X is possible?",
            "So that means that this is sort of the characterization.",
            "Of how the adversary will air in the worst case, right?",
            "So he has some finite budget.",
            "If his budget is this area, this is how he will air.",
            "Or maybe he will actually have a triangle over here.",
            "Or maybe maybe he'll choose to air on some different problem and have a triangle over here.",
            "But he has some fixed budget.",
            "That's all there is.",
            "Right, so the adversary is going to induce some deviation Delta, so that's going to be the difference between.",
            "T of 1 given X&P.",
            "And.",
            "The budget required to do that.",
            "Is the.",
            "This is real.",
            "So this this is just computing the area of the triangle.",
            "And that's going to be Z ^2 / 2 * 2.",
            "Which means just well where.",
            "So that's that's the proof.",
            "Right, this is it has some fixed budget of importance with errors.",
            "In the.",
            "But that fixed budget only goes so far, it only goes.",
            "This bar.",
            "The question about this.",
            "This is sort of the first nontrivial proof.",
            "It's not too bad.",
            "'cause I mean, if you if you just.",
            "Think about it a bit.",
            "You realize this is the most efficient way for any adversary and.",
            "No implies the proof in places here.",
            "OK, so.",
            "Just caveats.",
            "One of them is log loss.",
            "Can't do log lost.",
            "Without making some sort of extra assumption or constraint, if you bounded things to her probability always being .1 or or .9, that would be OK. And if you.",
            "If you changed, we didn't analyze log loss, but sort of clicked log loss that would be equivalent.",
            "But there are issues with that, because if you analyze clipped log loss, then the best prediction possible.",
            "Actually becomes not the probability, but something else.",
            "This implies nothing about ranking in general, so one way that people often use."
        ],
        [
            "Abilities is it?",
            "Probability 1 subobjects, and then they sort them according to those probabilistic predictions.",
            "And then they start running down through them and checking to see.",
            "It is if you go through that sort.",
            "And there's no guarantees for that.",
            "Now.",
            "We actually did, of course.",
            "Try and see how well this works.",
            "For these things, and oddly enough, it actually works better than.",
            "The experiment results are even more impressive in some sense.",
            "When you use probing for these things.",
            "But there's no theory.",
            "Or at least there's no theoretical guarantees.",
            "OK, so I think it's time to take a break.",
            "See.",
            "Just take a 15 minute break.",
            "Seems good."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we should begin.",
                    "label": 0
                },
                {
                    "sent": "Can everyone here alright?",
                    "label": 0
                },
                {
                    "sent": "OK, so notice that food.",
                    "label": 0
                },
                {
                    "sent": "Write down a bit for lunch.",
                    "label": 0
                },
                {
                    "sent": "We're compensating that for that in two ways.",
                    "label": 0
                },
                {
                    "sent": "One way is ordering more food.",
                    "label": 0
                },
                {
                    "sent": "In other ways, I want to mention that there are four kinds of people in the world.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so there's four people.",
                    "label": 0
                },
                {
                    "sent": "There's those who paid for learning summer school and you know, they're paying for their food.",
                    "label": 0
                },
                {
                    "sent": "This goes those who gave financial aid to and we paid for their food.",
                    "label": 0
                },
                {
                    "sent": "So what this lecture is about?",
                    "label": 0
                },
                {
                    "sent": "Is how to avoid inventing a new algorithm.",
                    "label": 0
                },
                {
                    "sent": "New learning algorithm for every learning problem.",
                    "label": 0
                },
                {
                    "sent": "Which is it's a very common kind of thing people you know, like oh, I have a new learning problem.",
                    "label": 0
                },
                {
                    "sent": "Let's invent a new learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "But maybe we can avoid that, and maybe that it's beneficial in a lot of ways.",
                    "label": 0
                },
                {
                    "sent": "If you have questions during this talk, you should feel free to ask them as they come up.",
                    "label": 0
                },
                {
                    "sent": "I kind of like things interactive.",
                    "label": 0
                },
                {
                    "sent": "Also, I have this feeling that I lose everyone.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so learning problems.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of different kinds of learning problems.",
                    "label": 0
                },
                {
                    "sent": "There's sort of binary ones.",
                    "label": 0
                },
                {
                    "sent": "Is it raining outside or not?",
                    "label": 0
                },
                {
                    "sent": "There are custom Steve ones like which commute is faster and other people are actually commuting.",
                    "label": 0
                },
                {
                    "sent": "Some people claim their commuting 2 hours per day.",
                    "label": 0
                },
                {
                    "sent": "To see this talk.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "I couldn't do that.",
                    "label": 0
                },
                {
                    "sent": "But if you're commuting, you have a choice of different paths you can take, right?",
                    "label": 0
                },
                {
                    "sent": "There's different routes available and you want to choose the path, which kind of minimizes your time.",
                    "label": 0
                },
                {
                    "sent": "It's it's a cost of classification problem.",
                    "label": 0
                },
                {
                    "sent": "And then there's issues like which stock will go up.",
                    "label": 0
                },
                {
                    "sent": "It's it's a favorite.",
                    "label": 0
                },
                {
                    "sent": "And can I make the light before it turns red?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Stopping early is OK because the penalty is not that bad.",
                    "label": 0
                },
                {
                    "sent": "You lose a minute or two, but if you go too far, well that's very bad.",
                    "label": 0
                },
                {
                    "sent": "'cause you get rammed.",
                    "label": 0
                },
                {
                    "sent": "So it's an important weighted classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Different kinds of learning problems.",
                    "label": 0
                },
                {
                    "sent": "And then reductions about how we can solve all of these with the same learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can ask questions so a reduction takes one type of learning algorithm into different type of learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it solves a different type of learning problem.",
                    "label": 0
                },
                {
                    "sent": "You can ask questions like how do I solve importance weighted binary classification of classifier?",
                    "label": 0
                },
                {
                    "sent": "How do I sell custom classification or reinforcement learning or regression as many different kinds of learning problems?",
                    "label": 0
                },
                {
                    "sent": "And one thing I should mention is.",
                    "label": 0
                },
                {
                    "sent": "Although I'm going to be talking about reducing things to classification, it's easy to imagine that using another primitive, like say, regression.",
                    "label": 0
                },
                {
                    "sent": "OK, so before I start into the grinding method.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we should feel a little bit motivated and these are the basic for region 4 reasons why.",
                    "label": 0
                },
                {
                    "sent": "You want to play with reductions in learning.",
                    "label": 0
                },
                {
                    "sent": "So I'll go through each of these individually.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's these binary classifiers which just decide between two possibilities.",
                    "label": 0
                },
                {
                    "sent": "Then there are the multiclass classification problems.",
                    "label": 0
                },
                {
                    "sent": "So one of the approaches that people do to solving multiclass classification is one against all, so they create a bunch of binary classifiers, one for each class.",
                    "label": 0
                },
                {
                    "sent": "For class I, the binary classifier is it class I or not?",
                    "label": 0
                },
                {
                    "sent": "Is a class J or not?",
                    "label": 0
                },
                {
                    "sent": "Is the class well one there not too or not whatever?",
                    "label": 0
                },
                {
                    "sent": "And then at prediction time what happens is you evaluate if you have K classes, then evaluate all K classifiers.",
                    "label": 0
                },
                {
                    "sent": "And if only one raises his hand, then you say, yeah, it must be this guy.",
                    "label": 0
                },
                {
                    "sent": "Ann, if two of them raise their hands, then you say home.",
                    "label": 0
                },
                {
                    "sent": "I'll just randomize between the two.",
                    "label": 0
                },
                {
                    "sent": "And if none of them raise their hands, then.",
                    "label": 0
                },
                {
                    "sent": "They just kinda makes them all don't have any information.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one against all.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "If you go and you analyze the mathematics of this.",
                    "label": 0
                },
                {
                    "sent": "What you discover is that this failure mode.",
                    "label": 0
                },
                {
                    "sent": "Is significantly less severe than this failure mode, 'cause you still haven't even chance of picking the right guy in this kind of failure mode.",
                    "label": 0
                },
                {
                    "sent": "But Dad's picking the right guy when this happens there.",
                    "label": 0
                },
                {
                    "sent": "Much worse, like 1 / K. So you can take advantage of that and you can create instead of reducing to binary classification.",
                    "label": 0
                },
                {
                    "sent": "You can reduce to.",
                    "label": 0
                },
                {
                    "sent": "Importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "So the importance of raising your hand when you're supposed to is larger than otherwise.",
                    "label": 0
                },
                {
                    "sent": "Then you can reduce that to binary classification.",
                    "label": 0
                },
                {
                    "sent": "You can compare these two methods.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this weighted one against all.",
                    "label": 0
                },
                {
                    "sent": "He's just always better.",
                    "label": 0
                },
                {
                    "sent": "This is the accuracy, so things above the line are wins for weighted one against all.",
                    "label": 0
                },
                {
                    "sent": "This is a bunch of different.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithms and a bunch of different datasets, and I guess one time we did slightly worse, but every other time.",
                    "label": 0
                },
                {
                    "sent": "We did better, sometimes significantly better.",
                    "label": 0
                },
                {
                    "sent": "So there's some motivation in terms of working.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another motivation is sort of codewise.",
                    "label": 0
                },
                {
                    "sent": "Reductions are sort of inherently modular.",
                    "label": 0
                },
                {
                    "sent": "The we do not appear inside of the workings of a learning algorithm, so we do not need to, yes.",
                    "label": 0
                },
                {
                    "sent": "It's not reducing from waited one is is that something reduction?",
                    "label": 0
                },
                {
                    "sent": "The reduction is from multiclass classification to binary classification.",
                    "label": 0
                },
                {
                    "sent": "So that makes sense.",
                    "label": 0
                },
                {
                    "sent": "So we're reducing from multiclass to binary, and you can do it sort of directly with this one against all.",
                    "label": 0
                },
                {
                    "sent": "Or you can go from multiclass too.",
                    "label": 0
                },
                {
                    "sent": "Weighted importance weighted binary and then to binary.",
                    "label": 0
                },
                {
                    "sent": "So I was comparing.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Reduction.",
                    "label": 0
                },
                {
                    "sent": "Parameters of the given algorithm.",
                    "label": 0
                },
                {
                    "sent": "No importance weighted binary.",
                    "label": 0
                },
                {
                    "sent": "There's no tweaking parameters of algorithms, so all the reductions that I'm talking about are black box reductions.",
                    "label": 0
                },
                {
                    "sent": "You can apply them to any classification algorithm, and you can apply them.",
                    "label": 0
                },
                {
                    "sent": "Just given some standard interface to all classification algorithms, there are no special parameters or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So I was comparing this direct approach.",
                    "label": 0
                },
                {
                    "sent": "For doing one versus all.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Waited one versus all.",
                    "label": 0
                },
                {
                    "sent": "Composed with something I want to talk about later, which is this.",
                    "label": 0
                },
                {
                    "sent": "Costing production.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We don't work.",
                    "label": 0
                },
                {
                    "sent": "Reductions to each other would make sense to compare to multiplication.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "One thing which is important is that.",
                    "label": 0
                },
                {
                    "sent": "Reductions are not competing with themselves, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, these are two different reductions.",
                    "label": 0
                },
                {
                    "sent": "The one versus all in the way to win versus all composed with disgusting.",
                    "label": 0
                },
                {
                    "sent": "But what you really want to compare with is.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you're doing multiclass classification, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you want to say this is the best way.",
                    "label": 0
                },
                {
                    "sent": "That comparison is not done right here.",
                    "label": 0
                },
                {
                    "sent": "It is done for some of the other things that are present later.",
                    "label": 0
                },
                {
                    "sent": "OK, so modularity.",
                    "label": 0
                },
                {
                    "sent": "Right no parameter tweaking.",
                    "label": 0
                },
                {
                    "sent": "Just thinking about the classification algorithm with particular type.",
                    "label": 0
                },
                {
                    "sent": "Or just using it as you would use things with that type.",
                    "label": 0
                },
                {
                    "sent": "We're not actually fiddling with any parameters in the classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can reuse your old learning algorithms via these reductions, right so?",
                    "label": 0
                },
                {
                    "sent": "Thanks from not having to think so much viewpoint in particular.",
                    "label": 0
                },
                {
                    "sent": "Think there are maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's 50 classification algorithms that have been invented.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty impressive because.",
                    "label": 0
                },
                {
                    "sent": "Mr 50 public datasets.",
                    "label": 0
                },
                {
                    "sent": "But if you can form a reduction to classification.",
                    "label": 0
                },
                {
                    "sent": "Then you have 50 algorithms to apply to whatever learning problem you have.",
                    "label": 0
                },
                {
                    "sent": "It takes awhile for 50 algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can.",
                    "label": 0
                },
                {
                    "sent": "Actually, in addition to using the learning algorithms because it's entirely black box, you can use rolled code.",
                    "label": 0
                },
                {
                    "sent": "Just a matter of matching up the interfaces, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Right, this is sort of a research motivation.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "What is reductionism?",
                    "label": 0
                },
                {
                    "sent": "Reductionism is figuring out how to cut your.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Homes in the small problems and then solve the small problems in the compose them to solve a big problem.",
                    "label": 0
                },
                {
                    "sent": "And one of the reasons was kind of an interesting direction to look research wise, because this approach has worked extremely well at certain times in the past.",
                    "label": 0
                },
                {
                    "sent": "So in particular.",
                    "label": 0
                },
                {
                    "sent": "Computations are kind of reduced to a transistor.",
                    "label": 0
                },
                {
                    "sent": "If you think about it.",
                    "label": 0
                },
                {
                    "sent": "Just do one very simple computation and you know compose the right way and look it's an Opteron.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "People are getting very good at rendering scenes.",
                    "label": 0
                },
                {
                    "sent": "The basic element of rendering is rendering a triangle, or maybe a very simple Polygon.",
                    "label": 0
                },
                {
                    "sent": "And then actually much of science kind of follows this reductionist approach.",
                    "label": 0
                },
                {
                    "sent": "So you take some.",
                    "label": 0
                },
                {
                    "sent": "You try to figure how to predict the most elementary things, and then you sort of compose these predictions to figure out where the spaceship goes and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Last thing is that it's actually pretty easy to use.",
                    "label": 0
                },
                {
                    "sent": "So figure out what the type of your learning problem is.",
                    "label": 0
                },
                {
                    "sent": "This is something that you always need to do.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can find some premade reduction.",
                    "label": 0
                },
                {
                    "sent": "From your type of learning problem to whatever learning items you want to apply.",
                    "label": 0
                },
                {
                    "sent": "And then you just.",
                    "label": 0
                },
                {
                    "sent": "Compose these things, throwing your data and you get a predictor for your learning problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is a bit easier than other approaches which So what are two other approaches?",
                    "label": 0
                },
                {
                    "sent": "Another approach is to just invent a learning algorithm to solve your B type problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee we have Zubin Ghahramani who is a very good Beijing coming on Friday I think.",
                    "label": 0
                },
                {
                    "sent": "And he will tell you that the right thing to do is to.",
                    "label": 0
                },
                {
                    "sent": "Figure out a prior over the different data generating distributions for B and then take the prior update with Bayes law to get the posterior and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "But maybe specifying the prior is a little bit difficult.",
                    "label": 0
                },
                {
                    "sent": "It can be a little bit tricky at times.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is an approach which lets you just try something quickly and then if it works great, and if it doesn't, then maybe just sit down and think a lot more.",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to say is that this is sort of a reasonable first pass approach on any particular problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're going to be reducing to classification.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's worthwhile to specify exactly what I mean by classification.",
                    "label": 0
                },
                {
                    "sent": "So these are pretty standard definitions.",
                    "label": 0
                },
                {
                    "sent": "You have the classic.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian problem is defined by some measure.",
                    "label": 0
                },
                {
                    "sent": "An extra 01.",
                    "label": 0
                },
                {
                    "sent": "So think of the user probability distribution.",
                    "label": 0
                },
                {
                    "sent": "You can draw the example of an X&Y pair.",
                    "label": 0
                },
                {
                    "sent": "The why is just going to be 1 bit.",
                    "label": 0
                },
                {
                    "sent": "And the classifier something which just Maps from this feature space to that one bit.",
                    "label": 0
                },
                {
                    "sent": "And then classifier learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is going to be given some examples.",
                    "label": 0
                },
                {
                    "sent": "And it's going to find a classifier.",
                    "label": 0
                },
                {
                    "sent": "Which small error rate so the error rate?",
                    "label": 0
                },
                {
                    "sent": "Is the probability that under a random draw from D the classifier is wrong?",
                    "label": 0
                },
                {
                    "sent": "You want to minimize the error rate.",
                    "label": 0
                },
                {
                    "sent": "So one thing which is very different about this analysis from some of the other things you see.",
                    "label": 0
                },
                {
                    "sent": "Is that we are not assuming.",
                    "label": 0
                },
                {
                    "sent": "With the data.",
                    "label": 0
                },
                {
                    "sent": "Is drawn independently and identically.",
                    "label": 0
                },
                {
                    "sent": "So maybe this is something which seems kind of.",
                    "label": 0
                },
                {
                    "sent": "Esoteric, but it's this is actually a pretty important thing.",
                    "label": 0
                },
                {
                    "sent": "The reason why it's important is.",
                    "label": 0
                },
                {
                    "sent": "All the analysis that we're going to tell you today.",
                    "label": 0
                },
                {
                    "sent": "Applies to every learning problem of some particular type.",
                    "label": 0
                },
                {
                    "sent": "Because you can always say there is some measure producing some test example.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And since we're not actually going to assume that.",
                    "label": 0
                },
                {
                    "sent": "The training set.",
                    "label": 0
                },
                {
                    "sent": "Is drawn from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "This will always apply.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "There, so I guess.",
                    "label": 0
                },
                {
                    "sent": "We can always say that for any particular test example, there is some distribution that you draw that from.",
                    "label": 0
                },
                {
                    "sent": "This distribution may be entirely dependent.",
                    "label": 0
                },
                {
                    "sent": "It may be actually deterministic.",
                    "label": 0
                },
                {
                    "sent": "Just draw all the weight is on one particular example.",
                    "label": 0
                },
                {
                    "sent": "But there's always some process.",
                    "label": 0
                },
                {
                    "sent": "Producing the test example.",
                    "label": 0
                },
                {
                    "sent": "And that is the process that we're actually going to be analyzing.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                },
                {
                    "sent": "But it's interesting that you can.",
                    "label": 0
                },
                {
                    "sent": "Analyze this at all.",
                    "label": 0
                },
                {
                    "sent": "You'll see why it's possible later.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about the basic setup?",
                    "label": 0
                },
                {
                    "sent": "The link is tricky.",
                    "label": 0
                },
                {
                    "sent": "We're not assuming IID data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the outline.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to go through four reductions, one of whom is from importance weighted classification classification.",
                    "label": 0
                },
                {
                    "sent": "So that's this one over here.",
                    "label": 0
                },
                {
                    "sent": "Another one is from.",
                    "label": 0
                },
                {
                    "sent": "Bring the class probability to predicting the classifier just predicting classification.",
                    "label": 0
                },
                {
                    "sent": "Another one is for multiclass classification classification.",
                    "label": 0
                },
                {
                    "sent": "Which is this?",
                    "label": 0
                },
                {
                    "sent": "Another one is from customs to classification classification.",
                    "label": 0
                },
                {
                    "sent": "So it gives you some range of tools and should hopefully give you some idea of how this all works.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So importance weighted classification is very much like classification.",
                    "label": 0
                },
                {
                    "sent": "Except.",
                    "label": 0
                },
                {
                    "sent": "You have importances.",
                    "label": 0
                },
                {
                    "sent": "That your distribution is over, right?",
                    "label": 0
                },
                {
                    "sent": "So in addition to drawing a feature and a binary prediction.",
                    "label": 0
                },
                {
                    "sent": "You draw some importance.",
                    "label": 0
                },
                {
                    "sent": "And then you still are trying to find a classifier, which is the binary predictor.",
                    "label": 0
                },
                {
                    "sent": "You are given an importance weighted data set.",
                    "label": 0
                },
                {
                    "sent": "And your goal?",
                    "label": 0
                },
                {
                    "sent": "Is to minimize your importance weight loss so.",
                    "label": 0
                },
                {
                    "sent": "If this I was gone and it would just be a probability distribution, it would be the same as previously.",
                    "label": 0
                },
                {
                    "sent": "So if I is always one, we're just minimizing error rate.",
                    "label": 0
                },
                {
                    "sent": "But if I is varying, then we need to write out the probabilities in expectation.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the expected importance weighted.",
                    "label": 0
                },
                {
                    "sent": "Errors.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the stoplight example.",
                    "label": 0
                },
                {
                    "sent": "The error rate or the importance of stopping too early.",
                    "label": 0
                },
                {
                    "sent": "Well doesn't matter that much right?",
                    "label": 0
                },
                {
                    "sent": "But the importance of stopping too late that could matter quite a bit.",
                    "label": 0
                },
                {
                    "sent": "I should mention one thing.",
                    "label": 0
                },
                {
                    "sent": "Which is that I just put the slides all online.",
                    "label": 0
                },
                {
                    "sent": "Which means the.",
                    "label": 0
                },
                {
                    "sent": "Everybody who really likes taking notes you can continue, but.",
                    "label": 0
                },
                {
                    "sent": "You know they're online.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a basic course theorem which is sort of folklore from decision theory.",
                    "label": 0
                },
                {
                    "sent": "And this is that you can turn this importance weighted problem into just an error rate minimization problem, and it has to do with sort of renaming the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So I guess the claim is that for every classifier.",
                    "label": 0
                },
                {
                    "sent": "For every.",
                    "label": 0
                },
                {
                    "sent": "Importance weighted distribution.",
                    "label": 0
                },
                {
                    "sent": "If you form this other distribution which is just just kind of relates to examples by the importance.",
                    "label": 0
                },
                {
                    "sent": "Then the importance weighted loss equals.",
                    "label": 0
                },
                {
                    "sent": "The error rate.",
                    "label": 0
                },
                {
                    "sent": "Of this new distribution.",
                    "label": 0
                },
                {
                    "sent": "Times this normalization term.",
                    "label": 0
                },
                {
                    "sent": "And it's a very simple.",
                    "label": 0
                },
                {
                    "sent": "Example of reduction.",
                    "label": 0
                },
                {
                    "sent": "Because it says that.",
                    "label": 0
                },
                {
                    "sent": "If you minimize your error, expect to D. Then it's equivalent to minimizing.",
                    "label": 0
                },
                {
                    "sent": "The importance weighted loss Spectra DBO.",
                    "label": 0
                },
                {
                    "sent": "The proof is it's just very simple.",
                    "label": 0
                },
                {
                    "sent": "We're kind of unwinding the definitions.",
                    "label": 0
                },
                {
                    "sent": "So in particular, we have our importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "And then I can write out the expectation as a sum.",
                    "label": 0
                },
                {
                    "sent": "So we have the eye here.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This D is items DBO so we just get the D and we have the normalization term which comes out.",
                    "label": 0
                },
                {
                    "sent": "And this is just an expectation of an indicator function, which is a probability.",
                    "label": 0
                },
                {
                    "sent": "Times this normalization term and that probability is just the error rate in respect to D. Right?",
                    "label": 0
                },
                {
                    "sent": "So this is the simplest version of reduction, so we should make sure everyone gets it.",
                    "label": 0
                },
                {
                    "sent": "So this theorem says that if we can optimize our binary error rate with respect to D, that's entirely equivalent up to some normalization term.",
                    "label": 0
                },
                {
                    "sent": "To optimizing our importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "The normalization turned by the way, has to be there, so if you took all of your importances and you doubled them.",
                    "label": 0
                },
                {
                    "sent": "You would expect your important weight loss to double.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I. OK, alright so maybe.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So this notation says that.",
                    "label": 0
                },
                {
                    "sent": "Expectation respect to XY and I drawn from disobey.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is the expected importance.",
                    "label": 0
                },
                {
                    "sent": "This OK for probability.",
                    "label": 0
                },
                {
                    "sent": "This is X&Y, drawn from D. I'm just ignoring the I. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Series just one distribution disobey.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I guess the claim is that having one distribution DBO is sufficient.",
                    "label": 0
                },
                {
                    "sent": "For any problem.",
                    "label": 0
                },
                {
                    "sent": "Where you care about this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "And the reason why it's sufficient is because we're not actually relying upon this IID stuff.",
                    "label": 0
                },
                {
                    "sent": "So there's only one example we need to care about the next test example, because there's only one example, there's only one distribution.",
                    "label": 0
                },
                {
                    "sent": "That example is drawn from.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's a basic theorem.",
                    "label": 0
                },
                {
                    "sent": "There's a question left right.",
                    "label": 0
                },
                {
                    "sent": "The question is, how do we use this basic theorem for our learning algorithms?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The way that you can shift from one distribution to another using rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "So the way that rejection sampling works is.",
                    "label": 0
                },
                {
                    "sent": "You pick some car.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instant, which is larger than all the importances.",
                    "label": 0
                },
                {
                    "sent": "It was pretty impressive.",
                    "label": 0
                },
                {
                    "sent": "You have such power there.",
                    "label": 0
                },
                {
                    "sent": "And then for each sample.",
                    "label": 0
                },
                {
                    "sent": "You're going to flip a coin with the bias of I oversee, right?",
                    "label": 0
                },
                {
                    "sent": "So this is less than one.",
                    "label": 0
                },
                {
                    "sent": "And if result is.",
                    "label": 0
                },
                {
                    "sent": "You keep it.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, you throw it away.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is some.",
                    "label": 0
                },
                {
                    "sent": "Distribution turn up and you can prove that sample drawn from.",
                    "label": 0
                },
                {
                    "sent": "Disobey and then.",
                    "label": 0
                },
                {
                    "sent": "Using this process are actually drawn from D. So there's actually a lot of other methods which come to mind when people want to 1st tell people this.",
                    "label": 0
                },
                {
                    "sent": "For example, they talk about duplicating examples and so forth.",
                    "label": 0
                },
                {
                    "sent": "According to the IMPORTANCES and.",
                    "label": 0
                },
                {
                    "sent": "Although that that core theorem did not rely on independence.",
                    "label": 0
                },
                {
                    "sent": "It is certainly the case that many of our learning algorithms rely on independence in the data set.",
                    "label": 0
                },
                {
                    "sent": "So you will be.",
                    "label": 0
                },
                {
                    "sent": "Unhappy.",
                    "label": 0
                },
                {
                    "sent": "If you do tricks with duplicating examples or even sampling with replacement.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go into what that is exactly, but.",
                    "label": 0
                },
                {
                    "sent": "You'll be unhappy, and the reason why you'll be happy.",
                    "label": 0
                },
                {
                    "sent": "The reason why I would be happy with this and unhappy with these other approaches is that rejection sampling actually preserves independence in the data.",
                    "label": 0
                },
                {
                    "sent": "So this is something which the theorem does not tell you to do, but because they're learning algorithms have this weakness that they really rely upon IID samples.",
                    "label": 0
                },
                {
                    "sent": "It's a good idea to try to preserve independence.",
                    "label": 0
                },
                {
                    "sent": "In your reduction.",
                    "label": 0
                },
                {
                    "sent": "But there's a drawback here.",
                    "label": 0
                },
                {
                    "sent": "You have a question.",
                    "label": 0
                },
                {
                    "sent": "Right exactly the drawback drawback here is that you end up throwing away a lot of your samples, especially when you're importances vary a lot.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Doing this once is what the theorem motivates, but maybe what you want to do in practice is sort of do it multiple times.",
                    "label": 0
                },
                {
                    "sent": "So we can rejection sample multiple times from this esobi.",
                    "label": 0
                },
                {
                    "sent": "And then you can learn some classifier.",
                    "label": 0
                },
                {
                    "sent": "And then just use a majority vote over these classifiers make a prediction.",
                    "label": 0
                },
                {
                    "sent": "Is it the same as what?",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Ada Boost also does a majority vote.",
                    "label": 0
                },
                {
                    "sent": "A lot of learning algorithms, a lot of major learning algorithms in majority votes.",
                    "label": 0
                },
                {
                    "sent": "And you can think of.",
                    "label": 0
                },
                {
                    "sent": "A lot of learning algorithms just themselves is doing majority votes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Predict according to the majority vote is a very common mechanism in learning, say again.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Right, so in Ada boost, this is going to be talked about in much more detail when rupture.",
                    "label": 0
                },
                {
                    "sent": "Visits, but Ada boost.",
                    "label": 0
                },
                {
                    "sent": "The way it works is.",
                    "label": 0
                },
                {
                    "sent": "It's also sort of a reduction.",
                    "label": 0
                },
                {
                    "sent": "It's using an Oracle learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And what happens is you.",
                    "label": 0
                },
                {
                    "sent": "But you feed your Oracle learning algorithm importance weighted data.",
                    "label": 0
                },
                {
                    "sent": "And there's a problem of fitting that too.",
                    "label": 0
                },
                {
                    "sent": "To classification, in one way to do that fit is actually to use rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "That's right, and I understand that actually works pretty well amongst the different possibilities.",
                    "label": 0
                },
                {
                    "sent": "Another approach people often uses, they turn to tweak the learning algorithm directly.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the importances into account, and.",
                    "label": 0
                },
                {
                    "sent": "My question is, that's no better in practice.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Have one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be dangerous.",
                    "label": 0
                },
                {
                    "sent": "The reason why that would be dangerous is because you lose this independence that you're trying to preserve.",
                    "label": 0
                },
                {
                    "sent": "So if you could take each of these individual pieces.",
                    "label": 0
                },
                {
                    "sent": "In union them together you concatenate them together.",
                    "label": 0
                },
                {
                    "sent": "Then it's possible to end up with the same example twice.",
                    "label": 0
                },
                {
                    "sent": "And the process of drawing decent from DBO and then rejection sampling could be of the sort where there's just no probability of having the same sample twice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You have to kind of keep straight with the theory motivates.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm not keeping it straight.",
                    "label": 0
                },
                {
                    "sent": "The theory says you can do any method of transforming distributions you want, as long as you transform to the right distribution, you're Golden.",
                    "label": 0
                },
                {
                    "sent": "But in practice, what happens is.",
                    "label": 0
                },
                {
                    "sent": "Your data really likes your learning algorithm really likes independent data.",
                    "label": 0
                },
                {
                    "sent": "And because you're learning algorithm related data, you want to try to preserve the independence.",
                    "label": 0
                },
                {
                    "sent": "If it's there.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright.",
                    "label": 0
                },
                {
                    "sent": "Basic album very simple.",
                    "label": 0
                },
                {
                    "sent": "Fact dead simple.",
                    "label": 0
                },
                {
                    "sent": "You know there's an interesting thing about this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There was some contest.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "KDD.",
                    "label": 0
                },
                {
                    "sent": "Knowledge decision in data mining conference has various prediction challenges.",
                    "label": 0
                },
                {
                    "sent": "In 1998 they had some prediction challenge, which was an important weighted prediction problem.",
                    "label": 0
                },
                {
                    "sent": "I think it was a spam optimization problem, so you're working for a charity.",
                    "label": 0
                },
                {
                    "sent": "You want to send out Flyers and give us money, but you don't want to send out too many Flyers 'cause their Flowers are expensive.",
                    "label": 0
                },
                {
                    "sent": "So you need to somehow predict.",
                    "label": 0
                },
                {
                    "sent": "Who's going to give you money if you send them a flyer and who will not?",
                    "label": 0
                },
                {
                    "sent": "And then?",
                    "label": 0
                },
                {
                    "sent": "In the ideal case, you can just send Flowers only to people who will give you money.",
                    "label": 0
                },
                {
                    "sent": "So the standard way of trying to measure this is in terms of profit.",
                    "label": 0
                },
                {
                    "sent": "This is where the winner was.",
                    "label": 0
                },
                {
                    "sent": "And if you use that very simple piece of code.",
                    "label": 0
                },
                {
                    "sent": "With naive Bayes you get here with boosted naive Bayes you do as well as the winner was a decision tree.",
                    "label": 0
                },
                {
                    "sent": "You actually do good bit better.",
                    "label": 0
                },
                {
                    "sent": "And with support vector machine, you're still a little bit below the winner, yeah?",
                    "label": 0
                },
                {
                    "sent": "Projection sample so much that you essentially made the problem say NP complete or something.",
                    "label": 0
                },
                {
                    "sent": "No why?",
                    "label": 0
                },
                {
                    "sent": "Do you ever run into problems where rejection samples so much?",
                    "label": 0
                },
                {
                    "sent": "The problem would be complete?",
                    "label": 0
                },
                {
                    "sent": "Huawei do I not?",
                    "label": 0
                },
                {
                    "sent": "Sample exponential number of times.",
                    "label": 0
                },
                {
                    "sent": "I think it's worse than that.",
                    "label": 0
                },
                {
                    "sent": "You could get really unlucky and you could reject some sample until your data set is so small you just can't solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So your area will always be .5 no matter what.",
                    "label": 0
                },
                {
                    "sent": "Anything which throws out data could lead to that problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically the issue is if you rejection sampling you get a smaller data set.",
                    "label": 0
                },
                {
                    "sent": "Is that mess you up?",
                    "label": 0
                },
                {
                    "sent": "Sometimes the answer is certainly yes.",
                    "label": 0
                },
                {
                    "sent": "So my favorite example of how this can mess you up is if you're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "So the function that you're trying to learn is a parity of a subset of your bits, so you have a bunch of bits for your feature space, and you want to learn a parody of the subset.",
                    "label": 0
                },
                {
                    "sent": "Then the number of samples you need.",
                    "label": 0
                },
                {
                    "sent": "Information theoretically is equivalent to the number of features.",
                    "label": 1
                },
                {
                    "sent": "It's the same as a number of features because the way you solve this problem is you just do Gaussian elimination.",
                    "label": 0
                },
                {
                    "sent": "And then you can figure out what the bits are.",
                    "label": 0
                },
                {
                    "sent": "Sir.",
                    "label": 0
                },
                {
                    "sent": "If you lose 1 sample there, you're hosed.",
                    "label": 0
                },
                {
                    "sent": "But it seems that in practice that's not the way learning problems really are.",
                    "label": 0
                },
                {
                    "sent": "So in practice this seems to work pretty well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Some regional performance.",
                    "label": 0
                },
                {
                    "sent": "I think we can assume that basically every method was tried on this thing, which is not about this is that we didn't do it in 1998, right?",
                    "label": 0
                },
                {
                    "sent": "We invented this later, but it still hopefully a little bit compelling because it's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which uses sort of a tried and true technique and does reasonably well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's how you go from importance weight classification classification.",
                    "label": 0
                },
                {
                    "sent": "Any questions about that?",
                    "label": 0
                },
                {
                    "sent": "OK, so the next one is a bit more.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "You want to go from.",
                    "label": 0
                },
                {
                    "sent": "Class probability.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Addiction to classification.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Trying to predict so, here's the definition.",
                    "label": 0
                },
                {
                    "sent": "It's the same the definition of the problem is the same As for.",
                    "label": 0
                },
                {
                    "sent": "Classification.",
                    "label": 0
                },
                {
                    "sent": "But now we're trying to make a probabilistic classifier which predicts some value between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And trying to minimize our error rate or to minimize or squared error.",
                    "label": 0
                },
                {
                    "sent": "Right, so if the actual probability of.",
                    "label": 0
                },
                {
                    "sent": "0.5 given some particular X.",
                    "label": 0
                },
                {
                    "sent": "We would want this value to be.",
                    "label": 0
                },
                {
                    "sent": ".5 because that would minimize expected value here.",
                    "label": 0
                },
                {
                    "sent": "If you think about this right, you realize this is also regression.",
                    "label": 0
                },
                {
                    "sent": "At least it's a special kind of regression kind of regression where.",
                    "label": 0
                },
                {
                    "sent": "You always observe zeros or ones, and you want to try to predict the probability of zero or one.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is like regression.",
                    "label": 0
                },
                {
                    "sent": "But it is fairly common to use the log loss, so the way log loss is defined is.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Log of one over.",
                    "label": 0
                },
                {
                    "sent": "The probability of the class that actually you.",
                    "label": 0
                },
                {
                    "sent": "Probably that you predicted the class that you observed.",
                    "label": 0
                },
                {
                    "sent": "Just a minute.",
                    "label": 0
                },
                {
                    "sent": "So, so long this is pretty interesting and the reason why it's particularly interesting is because it's unbounded.",
                    "label": 0
                },
                {
                    "sent": "And because it's unbounded, it turns out.",
                    "label": 0
                },
                {
                    "sent": "You can't do it in this framework.",
                    "label": 0
                },
                {
                    "sent": "This is a fundamental difficulty in the final difficulties that unboundedness exactly.",
                    "label": 0
                },
                {
                    "sent": "So if you have a classifier which exercises the full range of values, there's no way to actually formal reduction in the framework that I'll tell you, you can easily avoid that.",
                    "label": 0
                },
                {
                    "sent": "You can have your classifier clip torda link, predict things between oh point 1.9 and then everything is bounded, and then as possible.",
                    "label": 0
                },
                {
                    "sent": "But the story is not so nice when you want to log probability.",
                    "label": 0
                },
                {
                    "sent": "If arbitrarily close to 0 is still pretty bad, you can still be pretty large.",
                    "label": 0
                },
                {
                    "sent": "Alright, yeah question.",
                    "label": 0
                },
                {
                    "sent": "Posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You can think about it that way.",
                    "label": 0
                },
                {
                    "sent": "One way to think about this is here's a technique for taking any classifier learning algorithm and extracting a classifier probability or posterior probability.",
                    "label": 0
                },
                {
                    "sent": "Which is maybe not so obvious that you can do when just predicting either zero or one that something in the interval.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, connection.",
                    "label": 0
                },
                {
                    "sent": "People are getting it right.",
                    "label": 0
                },
                {
                    "sent": "CPO bank alright good.",
                    "label": 0
                },
                {
                    "sent": "So the reason why my loss is this?",
                    "label": 0
                },
                {
                    "sent": "Is because I can observe these an I can compute this?",
                    "label": 0
                },
                {
                    "sent": "But what you really want is to say that this minus the actual probability is small, right?",
                    "label": 0
                },
                {
                    "sent": "And there's a kind of a cool, which is that.",
                    "label": 0
                },
                {
                    "sent": "Or equivalent of to some additive constant.",
                    "label": 0
                },
                {
                    "sent": "Right then that added additive constant is there for every classifier the same constant.",
                    "label": 0
                },
                {
                    "sent": "So, roughly speaking, isn't it is if you plug in the probability of.",
                    "label": 0
                },
                {
                    "sent": "Minus the actual observed Y squared tick spectation.",
                    "label": 0
                },
                {
                    "sent": "The constants independent of C. And that's the only difference between these two things.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the problem.",
                    "label": 0
                },
                {
                    "sent": "And then maybe we want to think a little bit about why we want to do probability sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause you know there's a lot of situations where you don't want to do probabilities.",
                    "label": 0
                },
                {
                    "sent": "You want to make some prediction and you want to either.",
                    "label": 0
                },
                {
                    "sent": "That prediction tells you what you do need to do it, or you don't do it.",
                    "label": 0
                },
                {
                    "sent": "But there are times when probabilities are kind of required.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is when you're talking to a doctor.",
                    "label": 0
                },
                {
                    "sent": "Maybe the doctor doesn't want to be unemployed, so he would prefer that you actually tell him probability.",
                    "label": 0
                },
                {
                    "sent": "He makes judgments about what to do.",
                    "label": 0
                },
                {
                    "sent": "Rather than having classifier just tell a doctor what to do.",
                    "label": 0
                },
                {
                    "sent": "And then another situation which comes up is sometimes you have a lot of.",
                    "label": 0
                },
                {
                    "sent": "Data sources which are spread out all over the place.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's expensive to communicate between these different places, so you want to summarize sort of probability of Y given X from each of the individual sensors, and then somewhere later you want to use them.",
                    "label": 0
                },
                {
                    "sent": "So instead of instead of transmitting all the information from every location to 1 location, you can just transmit maybe the probability.",
                    "label": 0
                },
                {
                    "sent": "From one location to the central location.",
                    "label": 0
                },
                {
                    "sent": "They can be arbitrarily cheaper.",
                    "label": 0
                },
                {
                    "sent": "Another reason why this comes up is because you sort of want compatibility.",
                    "label": 0
                },
                {
                    "sent": "You want you want to think about.",
                    "label": 0
                },
                {
                    "sent": "You know, here's one component of the system.",
                    "label": 0
                },
                {
                    "sent": "It's using some sort of Asian technique, and here's some other component systems using some sort of classifier technique.",
                    "label": 0
                },
                {
                    "sent": "And now maybe you want to extract some sort of probabilistic predictions so that you can kind of few things in the right way.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's some reasons.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then how does this technique work?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to call this technique probing.",
                    "label": 0
                },
                {
                    "sent": "So this is basic observation that starts things off, which is that if you have a perfect classifier and a perfect classifier says one, that's saying the probability of y = 1 given X is greater than .5.",
                    "label": 0
                },
                {
                    "sent": "You go, that's interesting.",
                    "label": 0
                },
                {
                    "sent": "What if?",
                    "label": 0
                },
                {
                    "sent": "I picked some.",
                    "label": 0
                },
                {
                    "sent": "Random uniform random variable between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And make this importance weighted transformation.",
                    "label": 0
                },
                {
                    "sent": "Right, so I can create an importance weighted data set.",
                    "label": 0
                },
                {
                    "sent": "And the importance will be this Y -- P. So if Y is 0 then it's P and if Y is one is 1 -- P. And then you can.",
                    "label": 0
                },
                {
                    "sent": "Think about that a bit and you realize that.",
                    "label": 0
                },
                {
                    "sent": "If the classifier for this importance, weighted transformation is perfect.",
                    "label": 0
                },
                {
                    "sent": "That implies that when classifier says one.",
                    "label": 0
                },
                {
                    "sent": "The probability of y = 1 given X is greater than P. So here's a particular.",
                    "label": 0
                },
                {
                    "sent": "This is a proof.",
                    "label": 0
                },
                {
                    "sent": "In one table.",
                    "label": 0
                },
                {
                    "sent": "So if the prediction is.",
                    "label": 0
                },
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "Then the expected to the importance is going to be P. And the probability of 0 is minus y = 1 given X.",
                    "label": 0
                },
                {
                    "sent": "And the prediction is 1.",
                    "label": 0
                },
                {
                    "sent": "The importance is 1 -- P and probably why probably y = 1 given X.",
                    "label": 0
                },
                {
                    "sent": "And these two are equal.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "P equals.",
                    "label": 0
                },
                {
                    "sent": "This probability y = 1 given X.",
                    "label": 0
                },
                {
                    "sent": "It was just a matter of taking the sign which works out.",
                    "label": 0
                },
                {
                    "sent": "It has to workout 'cause we thought about that case first.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just well.",
                    "label": 0
                },
                {
                    "sent": "Isn't for this analysis, it doesn't even matter.",
                    "label": 0
                },
                {
                    "sent": "You can pick it, just pick it however you want.",
                    "label": 0
                },
                {
                    "sent": "Point 3.2 whatever.",
                    "label": 0
                },
                {
                    "sent": "And this analysis holds.",
                    "label": 0
                },
                {
                    "sent": "The statement always holds.",
                    "label": 0
                },
                {
                    "sent": "And the question about how we actually do it and there will be picking it randomly.",
                    "label": 0
                },
                {
                    "sent": "Or maybe will be picking it from a grid, but.",
                    "label": 0
                },
                {
                    "sent": "Sort of randomly, yes.",
                    "label": 0
                },
                {
                    "sent": "She's perfect.",
                    "label": 0
                },
                {
                    "sent": "Say again hope to see is perfect other classifiers properties it like that.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do you know that the classifier is perfect?",
                    "label": 0
                },
                {
                    "sent": "The answer is you don't, but there's some tricks which will.",
                    "label": 0
                },
                {
                    "sent": "Help.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Here's the probing algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a dataflow graph.",
                    "label": 0
                },
                {
                    "sent": "So you start with your binary data set at the top.",
                    "label": 0
                },
                {
                    "sent": "And then you choose some peas however you want.",
                    "label": 0
                },
                {
                    "sent": "And then you create some importance weighted datasets.",
                    "label": 1
                },
                {
                    "sent": "And then you apply your importance weighted learning algorithm and you get your importance weighted classifiers.",
                    "label": 1
                },
                {
                    "sent": "And then when you want to make a prediction with some particular feature, you get your importance weighted predictions.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to look at the transition point.",
                    "label": 0
                },
                {
                    "sent": "And say, oh, we must have passed the threshold here.",
                    "label": 0
                },
                {
                    "sent": "So I'll predict probability, which is somewhere in between.",
                    "label": 0
                },
                {
                    "sent": ".1 and .5.",
                    "label": 0
                },
                {
                    "sent": "Right, so maybe .3.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No you don't.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah yeah, I'll get to that in a moment.",
                    "label": 0
                },
                {
                    "sent": "You have a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these numbers here discretized the line between zero and one.",
                    "label": 0
                },
                {
                    "sent": "There's a question about how you want to choose your resolution and so forth.",
                    "label": 0
                },
                {
                    "sent": "It varies a bit, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's exactly the right question to ask.",
                    "label": 0
                },
                {
                    "sent": "The same question is here.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a few problems with this technique.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so so maybe we actually want to reduce all the way to binary classification, but I just told you how to do that right?",
                    "label": 0
                },
                {
                    "sent": "So we can take importance weights and we can turn that into binary classification using rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "And maybe OK.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a interesting point.",
                    "label": 0
                },
                {
                    "sent": "So for the costing reduction, I said look, you really want to.",
                    "label": 0
                },
                {
                    "sent": "Projection sample multiple times.",
                    "label": 0
                },
                {
                    "sent": "But for the probing reduction.",
                    "label": 0
                },
                {
                    "sent": "You don't, it's preferable.",
                    "label": 0
                },
                {
                    "sent": "To choose more peas.",
                    "label": 0
                },
                {
                    "sent": "Over rejection sampling multiple times, right?",
                    "label": 0
                },
                {
                    "sent": "And so this general principle.",
                    "label": 0
                },
                {
                    "sent": "If you can figure out how to do it right.",
                    "label": 0
                },
                {
                    "sent": "Getting a little bit information from a lot of places is better than getting a lot of information from a few places.",
                    "label": 0
                },
                {
                    "sent": "Right, so how do you deal with?",
                    "label": 0
                },
                {
                    "sent": "Is it true that the classifiers are not very good?",
                    "label": 0
                },
                {
                    "sent": "So the way you deal with it is you sort the predictions until you actually get?",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "So if you get 101, you sort and you get this.",
                    "label": 0
                },
                {
                    "sent": "Then you just pretend that this is what your classifier is predicted.",
                    "label": 0
                },
                {
                    "sent": "There's a question about why that's right, but turns out this exactly the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "And then there's this issue of how you discretize on P. You can do it with the uniform grid, but if you're in the transductive setting where you have a bunch of unlabeled data that you want to predict the probability of.",
                    "label": 0
                },
                {
                    "sent": "Probably label love.",
                    "label": 0
                },
                {
                    "sent": "Then you can actually do it on demand so you can.",
                    "label": 0
                },
                {
                    "sent": "You start out making just issues, P = .5 and you see how much of your data ended up on this side versus that side.",
                    "label": 0
                },
                {
                    "sent": "Then you can discretize to optimize your squared error loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This looks easy, but all the magic is happening here.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm telling you precisely how to cope with errors.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So how well does this work?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we actually compare with a lot of different techniques for making probabilistic predictions.",
                    "label": 0
                },
                {
                    "sent": "Rather than just comparing with other reductions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we have a bunch of datasets here.",
                    "label": 0
                },
                {
                    "sent": "He had drawn from many different locations.",
                    "label": 0
                },
                {
                    "sent": "Some of them are UCI, some of them are these key today championships.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it's a combination of those two sources.",
                    "label": 0
                },
                {
                    "sent": "And then you can look at the squared error.",
                    "label": 1
                },
                {
                    "sent": "So smaller is better.",
                    "label": 0
                },
                {
                    "sent": "And you can compare several different approaches.",
                    "label": 0
                },
                {
                    "sent": "So one approach people use is they want to support vector machine in the data set.",
                    "label": 0
                },
                {
                    "sent": "Then they fit some sort of sigmoid to the margin.",
                    "label": 0
                },
                {
                    "sent": "So the margin itself gives a pretty bad probability prediction.",
                    "label": 0
                },
                {
                    "sent": "And there's no reason why should give a good probability prediction because.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not supposed to be a good probability prediction.",
                    "label": 0
                },
                {
                    "sent": "They don't optimize for that, but if you fit a sigmoid to the margin actually comes out reasonably well.",
                    "label": 0
                },
                {
                    "sent": "So that's what this.",
                    "label": 0
                },
                {
                    "sent": "SVM plus SIG is, and that's the first blue line.",
                    "label": 0
                },
                {
                    "sent": "And then you can run.",
                    "label": 0
                },
                {
                    "sent": "This probing technique on top of just the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So it's with the second blue line is.",
                    "label": 0
                },
                {
                    "sent": "And then for the red lines you can run naive Bayes, which just makes a.",
                    "label": 0
                },
                {
                    "sent": "The class probability prediction.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Or you can run naive vision, then fit the sigmoid to the margin.",
                    "label": 0
                },
                {
                    "sent": "That naive Bayes tells you.",
                    "label": 0
                },
                {
                    "sent": "So this one.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "You can do now you can do probing on top of naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's nice and then you can have a decision tree.",
                    "label": 0
                },
                {
                    "sent": "And you can just tell a decision tree.",
                    "label": 0
                },
                {
                    "sent": "Tell me a probability right?",
                    "label": 0
                },
                {
                    "sent": "And it will tell you.",
                    "label": 0
                },
                {
                    "sent": "Vote at the leaves or maybe some sort of.",
                    "label": 0
                },
                {
                    "sent": "Modified voted believes.",
                    "label": 0
                },
                {
                    "sent": "Or you can tell you this tree.",
                    "label": 0
                },
                {
                    "sent": "You can run this bagging technique on top of a decision tree.",
                    "label": 0
                },
                {
                    "sent": "So how does bagging work the way the bagging works is.",
                    "label": 0
                },
                {
                    "sent": "Particular data.",
                    "label": 0
                },
                {
                    "sent": "Can you sample with replacement from it?",
                    "label": 0
                },
                {
                    "sent": "Same number of times you have data, so you did the same size.",
                    "label": 0
                },
                {
                    "sent": "But I only have about 2/3 of the data and there will be some duplicates.",
                    "label": 0
                },
                {
                    "sent": "Then you can run your decision tree on this.",
                    "label": 0
                },
                {
                    "sent": "And you can repeat this process many different times.",
                    "label": 0
                },
                {
                    "sent": "And then you can look at the vote across the different classifiers that you learn.",
                    "label": 0
                },
                {
                    "sent": "To predict some sort of probability.",
                    "label": 0
                },
                {
                    "sent": "Now, in general, this isn't.",
                    "label": 0
                },
                {
                    "sent": "This is like the margin is not that this vote doesn't tell you the probability.",
                    "label": 0
                },
                {
                    "sent": "Oddly enough, it works pretty well in practice.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "You can run this probing on top of C 4.5.",
                    "label": 0
                },
                {
                    "sent": "We've also done experiments with logistic regression, which is turned out to be much like support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So if you look at what the smallest thing is here.",
                    "label": 0
                },
                {
                    "sent": "It's often.",
                    "label": 0
                },
                {
                    "sent": "Probing in particular probing on top of.",
                    "label": 0
                },
                {
                    "sent": "C 4.5.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit hard to see everything because far away and the differences are too large.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that it works a little bit better than than everything else.",
                    "label": 0
                },
                {
                    "sent": "On this test we did closest competitor is actually see 4.5 with bagging.",
                    "label": 1
                },
                {
                    "sent": "So C 4.5 with probing was the best and then C 4.5 with bagging and then.",
                    "label": 0
                },
                {
                    "sent": "A few others, one occasionally.",
                    "label": 0
                },
                {
                    "sent": "There was a question.",
                    "label": 0
                },
                {
                    "sent": "Those differences are significant, yeah?",
                    "label": 0
                },
                {
                    "sent": "With this graph should tell you that the approach is sane.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I will tell you personally that there was no throwing away data sets for the student work, right?",
                    "label": 0
                },
                {
                    "sent": "So we just kind of we picked them.",
                    "label": 0
                },
                {
                    "sent": "We ran the data, we ran it and there was.",
                    "label": 0
                },
                {
                    "sent": "This is what we got.",
                    "label": 0
                },
                {
                    "sent": "So the approach is saying it works.",
                    "label": 0
                },
                {
                    "sent": "Maybe it has a slight edge, maybe not.",
                    "label": 0
                },
                {
                    "sent": "It's not really significant.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The approach works.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so is this trick we're going to use?",
                    "label": 0
                },
                {
                    "sent": "Throughout the rest of this lecture, which is that for the analysis, it's extremely convenient to analyze only one classifier, or to analyze the case we reduced to only one classifier.",
                    "label": 0
                },
                {
                    "sent": "And it turns out, anytime you have a bunch of parallel calls, different classifiers.",
                    "label": 0
                },
                {
                    "sent": "You can think about them as one classifier.",
                    "label": 0
                },
                {
                    "sent": "So the way that you do this is you just think about taking all of your parallel calls and Union and unioning them together or concatenating them together.",
                    "label": 0
                },
                {
                    "sent": "So getting all the datasets together.",
                    "label": 0
                },
                {
                    "sent": "And then the major.",
                    "label": 0
                },
                {
                    "sent": "That you'll be learning respect to is sort of the union of the concatenation of the measures for the mixture of the measures.",
                    "label": 0
                },
                {
                    "sent": "And then you can think about running.",
                    "label": 0
                },
                {
                    "sent": "Your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "On this concatenated data set.",
                    "label": 0
                },
                {
                    "sent": "To get one classifier.",
                    "label": 0
                },
                {
                    "sent": "So the little trick you have to do is you put the name of the classifier inside of the feature space.",
                    "label": 0
                },
                {
                    "sent": "It's not clear if you want to do in practice.",
                    "label": 0
                },
                {
                    "sent": "We actually have not experimented with it very much.",
                    "label": 0
                },
                {
                    "sent": "Typically we just run one different classifier for each choice of P, right.",
                    "label": 0
                },
                {
                    "sent": "One different learning algorithm, but theoretically at least you could just run your learning algorithm once.",
                    "label": 0
                },
                {
                    "sent": "With the name of the classifier embedded in the feature space.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Once you have this one classifier, you can get all your old classifiers, but just you know, putting in the name.",
                    "label": 0
                },
                {
                    "sent": "This is a trick to take many different parallel calls and turn it into one call.",
                    "label": 0
                },
                {
                    "sent": "This trick is theoretical at the moment there have been no experiments one way or another about how well this works in practice.",
                    "label": 0
                },
                {
                    "sent": "So with this trick means is that you can think about drawing.",
                    "label": 0
                },
                {
                    "sent": "From the distribution that C is going to be evaluated on, so you can you see multiple times, and there's some particular measure over how you use and the way that you draw from that measure is you draw from your original distribution.",
                    "label": 0
                },
                {
                    "sent": "Then you draw maybe uniform randomly from P. An you evaluate right so it gives you some distribution over X cross P. For extra speed cross why which is hidden when you evaluate.",
                    "label": 0
                },
                {
                    "sent": "So this is a basic trick that we're going to use over and over again.",
                    "label": 0
                },
                {
                    "sent": "Is it clear?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Do you train again?",
                    "label": 0
                },
                {
                    "sent": "Once you have the classifier, see again, we are going to train the classifier again.",
                    "label": 0
                },
                {
                    "sent": "No, there's no this.",
                    "label": 0
                },
                {
                    "sent": "All the training happens here.",
                    "label": 0
                },
                {
                    "sent": "And in particular, the real training happens inside of.",
                    "label": 0
                },
                {
                    "sent": "A right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to be reducing to importance weighted classification and importance weighted classification just binary classification.",
                    "label": 0
                },
                {
                    "sent": "Level of binary classification would just run a.",
                    "label": 0
                },
                {
                    "sent": "And everything happens, all the training happens in this step.",
                    "label": 0
                },
                {
                    "sent": "OK so this is trick.",
                    "label": 0
                },
                {
                    "sent": "Stick is mathematically convenient.",
                    "label": 0
                },
                {
                    "sent": "It's motivations empirically, are unclear at this point.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to maybe not let me tell you.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you one reason why we might want to know.",
                    "label": 0
                },
                {
                    "sent": "One reason why we why we might not.",
                    "label": 0
                },
                {
                    "sent": "One reason why we might want to.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "This algorithm is computationally intensive, right?",
                    "label": 0
                },
                {
                    "sent": "You're running learning algorithm multiple different times, and that takes a bit of work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you just wanna learn is a way to run the learning algorithm once then maybe you can get the same result but.",
                    "label": 0
                },
                {
                    "sent": "Perhaps much cheaper computationally.",
                    "label": 0
                },
                {
                    "sent": "Anna reason not.",
                    "label": 0
                },
                {
                    "sent": "So there is this issue of preserving independence, but it's actually probably possible to cope with that instead of taking the actual union.",
                    "label": 0
                },
                {
                    "sent": "You can just subsample from the Union such a ways to preserve the independence.",
                    "label": 0
                },
                {
                    "sent": "More worrisome is maybe by augmenting the feature space you made the problem harder.",
                    "label": 0
                },
                {
                    "sent": "Possibly so you need to think about that a little bit.",
                    "label": 0
                },
                {
                    "sent": "If you decide to actually do exactly this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a theorem.",
                    "label": 0
                },
                {
                    "sent": "The theorem is actually kind of nice.",
                    "label": 0
                },
                {
                    "sent": "It says for every classifier.",
                    "label": 0
                },
                {
                    "sent": "So the classifier is taking this feature space, cross the name.",
                    "label": 0
                },
                {
                    "sent": "And predicting 1 bit.",
                    "label": 0
                },
                {
                    "sent": "For every distribution on extra 01.",
                    "label": 0
                },
                {
                    "sent": "This is what you wanted, right?",
                    "label": 0
                },
                {
                    "sent": "This is the probability of y = 1 given X minus.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by something and that something is OK, so we have some some distribution.",
                    "label": 0
                },
                {
                    "sent": "And we have this process of picking AP.",
                    "label": 0
                },
                {
                    "sent": "And then evaluating AC, which gives the new distribution over binary problems.",
                    "label": 0
                },
                {
                    "sent": "So this is the name of the new distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is a distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have the error rate is fixed distribution.",
                    "label": 0
                },
                {
                    "sent": "Minus the minimum error rate.",
                    "label": 0
                },
                {
                    "sent": "It's possible.",
                    "label": 0
                },
                {
                    "sent": "So if I just talked about error rate.",
                    "label": 0
                },
                {
                    "sent": "This would not be a convincing theorem.",
                    "label": 0
                },
                {
                    "sent": "The reason why I would not be convincing is because the problems that I've created are inherently noisy.",
                    "label": 0
                },
                {
                    "sent": "We have to subtract off the error rate due to the noise.",
                    "label": 0
                },
                {
                    "sent": "And then then the theorem tells us something interesting.",
                    "label": 0
                },
                {
                    "sent": "So it says that this quantity here is a regret, right?",
                    "label": 0
                },
                {
                    "sent": "It's how would we do this?",
                    "label": 0
                },
                {
                    "sent": "How could we could do?",
                    "label": 0
                },
                {
                    "sent": "Which is a regret.",
                    "label": 0
                },
                {
                    "sent": "Says that if we have a small regret.",
                    "label": 0
                },
                {
                    "sent": "Respect to for this C. Then we have good probability estimates.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of cool because we can't actually observe this.",
                    "label": 0
                },
                {
                    "sent": "We can compute this and we can do this.",
                    "label": 0
                },
                {
                    "sent": "We don't know this, we know that, but nevertheless, minimizing this is equivalent to optimizing that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Are you generating the data suggest?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So all that I said was.",
                    "label": 0
                },
                {
                    "sent": "We have all these recipes with the importances.",
                    "label": 0
                },
                {
                    "sent": "Previous to P we add the name into the feature space.",
                    "label": 0
                },
                {
                    "sent": "Then we concatenate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, doesn't matter for the theorem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's how we might want to do in practice, but they're equivalent as far as the distribution that's induced.",
                    "label": 0
                },
                {
                    "sent": "So this process and that process of their induce the same distribution over samples that you evaluate your classifier on.",
                    "label": 0
                },
                {
                    "sent": "Only question is whether or not the training set.",
                    "label": 0
                },
                {
                    "sent": "He's actually independent.",
                    "label": 0
                },
                {
                    "sent": "So preserving independence if you're actually going to do this in practice, you probably want to do something like this.",
                    "label": 0
                },
                {
                    "sent": "Appointment are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This theorem is a little bit.",
                    "label": 0
                },
                {
                    "sent": "Less trivial to prove.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Here's a pretty picture.",
                    "label": 0
                },
                {
                    "sent": "And what does it tell us?",
                    "label": 0
                },
                {
                    "sent": "So the importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "Of predicting insulated loss.",
                    "label": 0
                },
                {
                    "sent": "So when the actual answer is 0 importance, weight loss predicting wrong is like this red line.",
                    "label": 0
                },
                {
                    "sent": "And when the answer is 1, it's like this blue line.",
                    "label": 0
                },
                {
                    "sent": "And it will turn out that we have a convex combination of zero and one when the probability is actually something between zero and one.",
                    "label": 0
                },
                {
                    "sent": "The loss is also linear respect to whatever the actual probability is.",
                    "label": 0
                },
                {
                    "sent": "So this is actually the way the proof works, just in a picture, so it will turn out that if the actual properties .5, then the importance weight loss you suffer.",
                    "label": 0
                },
                {
                    "sent": "But predicting wrong here much.",
                    "label": 0
                },
                {
                    "sent": "And then, because we had a sorting step.",
                    "label": 0
                },
                {
                    "sent": "So the way we prove this, just no idea something like that, but you prove this.",
                    "label": 0
                },
                {
                    "sent": "You say look.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose we have an adversary.",
                    "label": 0
                },
                {
                    "sent": "The adversary controls the classifiers.",
                    "label": 0
                },
                {
                    "sent": "The adversary has some budget of importance weighted errors he can make.",
                    "label": 0
                },
                {
                    "sent": "And then the question is, how far can you shift us off from the actual probability?",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "If his budget is equal to the area of his Gray region.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the the worst case error mode.",
                    "label": 0
                },
                {
                    "sent": "So he wouldn't air over here.",
                    "label": 0
                },
                {
                    "sent": "Because since we sort.",
                    "label": 0
                },
                {
                    "sent": "It's just going to cancel with an error over here, right?",
                    "label": 0
                },
                {
                    "sent": "So that would accomplish nothing that would get to make negative progress for the adversary.",
                    "label": 0
                },
                {
                    "sent": "And he wouldn't air over here.",
                    "label": 0
                },
                {
                    "sent": "Because he pays a large importance to have the same effect as an error here.",
                    "label": 0
                },
                {
                    "sent": "So the way the adversary the adversarial classifiers are going to.",
                    "label": 0
                },
                {
                    "sent": "Air to induce the most.",
                    "label": 0
                },
                {
                    "sent": "Shift away from the actual probability.",
                    "label": 0
                },
                {
                    "sent": "Is by starting with airs for the.",
                    "label": 0
                },
                {
                    "sent": "The peas very near the real probability and kind of filling in everything.",
                    "label": 0
                },
                {
                    "sent": "So that's the most efficient error mechanism for an adversary.",
                    "label": 0
                },
                {
                    "sent": "Because it's the most efficient error mechanism.",
                    "label": 0
                },
                {
                    "sent": "What are you going to get the theorem?",
                    "label": 0
                },
                {
                    "sent": "Because this is clearly which is rising linearly and this is so this is the.",
                    "label": 0
                },
                {
                    "sent": "Integral of X.",
                    "label": 0
                },
                {
                    "sent": "Which is going to give us an X squared, which is exactly the squared error that you observed in the theorem.",
                    "label": 0
                },
                {
                    "sent": "Right, I'm not certain this is clear yet.",
                    "label": 0
                },
                {
                    "sent": "Welcome.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Activity.",
                    "label": 0
                },
                {
                    "sent": "Let's try to do the math.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "First claim is that the expected importance is 1/2.",
                    "label": 0
                },
                {
                    "sent": "The reason why this is important is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Is because you're integrating.",
                    "label": 0
                },
                {
                    "sent": "This triangle here.",
                    "label": 0
                },
                {
                    "sent": "Right, and so the integral of X or the integral of 1 -- X from zero to one is 1/2.",
                    "label": 0
                },
                {
                    "sent": "So respect the distribution which draws from D and then draws from P uniformly.",
                    "label": 0
                },
                {
                    "sent": "Just have that half.",
                    "label": 0
                },
                {
                    "sent": "OK, so because we know the expected since that means that we can sort of analyze just the importance weighted problem and then you know, multiplied by the appropriate constant.",
                    "label": 0
                },
                {
                    "sent": "Using that costing theorem.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so we have our regret here.",
                    "label": 0
                },
                {
                    "sent": "And this regret this expanding the definition.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "So that factor of two is just one over the importance.",
                    "label": 0
                },
                {
                    "sent": "Don't worry bout that bookkeeping.",
                    "label": 0
                },
                {
                    "sent": "So if the expected.",
                    "label": 0
                },
                {
                    "sent": "Importance weight loss minus the minimum of these two values.",
                    "label": 0
                },
                {
                    "sent": "The the best classifier predicts according to the minimum these two values.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a little trick here.",
                    "label": 0
                },
                {
                    "sent": "We do is.",
                    "label": 0
                },
                {
                    "sent": "We say.",
                    "label": 0
                },
                {
                    "sent": "We just change our expectation, right?",
                    "label": 0
                },
                {
                    "sent": "So we think about drawing from X&P and then drawing from why given D of XY drawing, why from the given X?",
                    "label": 0
                },
                {
                    "sent": "OK. And we can think about sort of fixing X&P.",
                    "label": 0
                },
                {
                    "sent": "Then we can do the analysis, fix that fix NP, and then at the end it's analysis holds.",
                    "label": 0
                },
                {
                    "sent": "We can just take the expectation over X&P.",
                    "label": 0
                },
                {
                    "sent": "So if it holds for all choices of X&P and holds for an expectation over the choices.",
                    "label": 0
                },
                {
                    "sent": "Right so at that point we can just kind of ignore this quantity.",
                    "label": 0
                },
                {
                    "sent": "And this is question about.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What is this inner thing?",
                    "label": 0
                },
                {
                    "sent": "So this is 2 possibilities.",
                    "label": 0
                },
                {
                    "sent": "Either C is going to predict according to the minimum of these two or speaking to predicting the maximum of these two right?",
                    "label": 0
                },
                {
                    "sent": "So either suffer the minimum of these two.",
                    "label": 0
                },
                {
                    "sent": "Losses and importance weighted losses.",
                    "label": 0
                },
                {
                    "sent": "Or it will suffer the maximum of two.",
                    "label": 0
                },
                {
                    "sent": "So it is the minimum, the minimum minimum.",
                    "label": 0
                },
                {
                    "sent": "This is 0.",
                    "label": 0
                },
                {
                    "sent": "If it's the maximum.",
                    "label": 0
                },
                {
                    "sent": "It is minus the minimum.",
                    "label": 0
                },
                {
                    "sent": "And that's this.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be.",
                    "label": 0
                },
                {
                    "sent": "That too.",
                    "label": 0
                },
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "First term, minus the second term magnitude.",
                    "label": 0
                },
                {
                    "sent": "And then it turns out that's two times.",
                    "label": 0
                },
                {
                    "sent": "Make it a P minus.",
                    "label": 0
                },
                {
                    "sent": "Actually, probability of 1 given X.",
                    "label": 0
                },
                {
                    "sent": "So let's see if we can see that.",
                    "label": 0
                },
                {
                    "sent": "Right, so I guess you have a -- P * D even minus 3 * -- T, which cancels, which means you left with a -- P and AD and I guess it just changed the order over there.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Very simple and.",
                    "label": 0
                },
                {
                    "sent": "And then you can see the customs classification.",
                    "label": 0
                },
                {
                    "sent": "Is this two times as P?",
                    "label": 0
                },
                {
                    "sent": "Minus actual probability would do next.",
                    "label": 0
                },
                {
                    "sent": "And that.",
                    "label": 0
                },
                {
                    "sent": "That implies this is this line here is correct.",
                    "label": 0
                },
                {
                    "sent": "The the cost of misclassifying when P is here is that much or maybe twice that much.",
                    "label": 0
                },
                {
                    "sent": "This is a straight line.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the end of the proof.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you have a budget of some number of errors?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Question is, how do you use your budget?",
                    "label": 0
                },
                {
                    "sent": "Is an adversary.",
                    "label": 0
                },
                {
                    "sent": "So you want to try to induce the maximum you want to make this probing process.",
                    "label": 0
                },
                {
                    "sent": "Make the biggest mistake you can.",
                    "label": 0
                },
                {
                    "sent": "So the first observation is that all classifications are going to be on one side of the of 1 given X, one side of the probability of 1 given X.",
                    "label": 0
                },
                {
                    "sent": "So it's only going to be piece that are above or piece that are below, not both.",
                    "label": 0
                },
                {
                    "sent": "The reason why won't be both is because when you sort, you just cancel out.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing to observe is that the adversary wants to make his errors for the the closer piece.",
                    "label": 0
                },
                {
                    "sent": "The reason why it's so is because.",
                    "label": 0
                },
                {
                    "sent": "The amount the adversary page to make a mistake here is that much of the amount that he makes page to make a mistake here is that much so he always wants to air is close to.",
                    "label": 0
                },
                {
                    "sent": "The actual.",
                    "label": 0
                },
                {
                    "sent": "Do you have one given X is possible?",
                    "label": 0
                },
                {
                    "sent": "So that means that this is sort of the characterization.",
                    "label": 0
                },
                {
                    "sent": "Of how the adversary will air in the worst case, right?",
                    "label": 0
                },
                {
                    "sent": "So he has some finite budget.",
                    "label": 0
                },
                {
                    "sent": "If his budget is this area, this is how he will air.",
                    "label": 0
                },
                {
                    "sent": "Or maybe he will actually have a triangle over here.",
                    "label": 0
                },
                {
                    "sent": "Or maybe maybe he'll choose to air on some different problem and have a triangle over here.",
                    "label": 0
                },
                {
                    "sent": "But he has some fixed budget.",
                    "label": 0
                },
                {
                    "sent": "That's all there is.",
                    "label": 0
                },
                {
                    "sent": "Right, so the adversary is going to induce some deviation Delta, so that's going to be the difference between.",
                    "label": 0
                },
                {
                    "sent": "T of 1 given X&P.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The budget required to do that.",
                    "label": 0
                },
                {
                    "sent": "Is the.",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                },
                {
                    "sent": "So this this is just computing the area of the triangle.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be Z ^2 / 2 * 2.",
                    "label": 0
                },
                {
                    "sent": "Which means just well where.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the proof.",
                    "label": 0
                },
                {
                    "sent": "Right, this is it has some fixed budget of importance with errors.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "But that fixed budget only goes so far, it only goes.",
                    "label": 0
                },
                {
                    "sent": "This bar.",
                    "label": 0
                },
                {
                    "sent": "The question about this.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the first nontrivial proof.",
                    "label": 0
                },
                {
                    "sent": "It's not too bad.",
                    "label": 0
                },
                {
                    "sent": "'cause I mean, if you if you just.",
                    "label": 0
                },
                {
                    "sent": "Think about it a bit.",
                    "label": 0
                },
                {
                    "sent": "You realize this is the most efficient way for any adversary and.",
                    "label": 0
                },
                {
                    "sent": "No implies the proof in places here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Just caveats.",
                    "label": 0
                },
                {
                    "sent": "One of them is log loss.",
                    "label": 0
                },
                {
                    "sent": "Can't do log lost.",
                    "label": 0
                },
                {
                    "sent": "Without making some sort of extra assumption or constraint, if you bounded things to her probability always being .1 or or .9, that would be OK. And if you.",
                    "label": 0
                },
                {
                    "sent": "If you changed, we didn't analyze log loss, but sort of clicked log loss that would be equivalent.",
                    "label": 0
                },
                {
                    "sent": "But there are issues with that, because if you analyze clipped log loss, then the best prediction possible.",
                    "label": 0
                },
                {
                    "sent": "Actually becomes not the probability, but something else.",
                    "label": 0
                },
                {
                    "sent": "This implies nothing about ranking in general, so one way that people often use.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abilities is it?",
                    "label": 0
                },
                {
                    "sent": "Probability 1 subobjects, and then they sort them according to those probabilistic predictions.",
                    "label": 0
                },
                {
                    "sent": "And then they start running down through them and checking to see.",
                    "label": 0
                },
                {
                    "sent": "It is if you go through that sort.",
                    "label": 0
                },
                {
                    "sent": "And there's no guarantees for that.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We actually did, of course.",
                    "label": 0
                },
                {
                    "sent": "Try and see how well this works.",
                    "label": 0
                },
                {
                    "sent": "For these things, and oddly enough, it actually works better than.",
                    "label": 0
                },
                {
                    "sent": "The experiment results are even more impressive in some sense.",
                    "label": 0
                },
                {
                    "sent": "When you use probing for these things.",
                    "label": 0
                },
                {
                    "sent": "But there's no theory.",
                    "label": 0
                },
                {
                    "sent": "Or at least there's no theoretical guarantees.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think it's time to take a break.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Just take a 15 minute break.",
                    "label": 0
                },
                {
                    "sent": "Seems good.",
                    "label": 0
                }
            ]
        }
    }
}