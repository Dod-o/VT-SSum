{
    "id": "rfrdmfx55onk2iniznlxkway7ll53n5r",
    "title": "Learning Sparsely Used Overcomplete Dictionaries",
    "info": {
        "author": [
            "Prateek Jain, Nuance Communications, Inc."
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_jain_learning/",
    "segmentation": [
        [
            "Good afternoon everyone, so this is joint work with Alec Anima, Praneeth and Russische."
        ],
        [
            "So in this work we consider the problem of dictionary learning, where the goal is to learn a dictionary so that each data point can be represented as a very sparse combination of columns of this dictionary.",
            "OK, so."
        ],
        [
            "Given a matrix Y which represents so each column of Y represents a data point, we want to learn this dictionary A as well as its coefficient matrix AX.",
            "So each column of X represents coefficients of each data point.",
            "In the case that we are interested in is the case of Overcomplete dictionaries, that is, a number of dictionary elements or number of columns.",
            "In this matrix A is much larger than the dimensionality of the data.",
            "OK, so for this."
        ],
        [
            "Look for this problem.",
            "There exists a long line of work, but the problem of Overcomplete dictionary from the perspective exact recovery was sort of open.",
            "Prior to this work and also concurrent work bias and you are a wrong gained Uncle Milton.",
            "And it was presented in the first session."
        ],
        [
            "So this problem in general is NP hard, and in fact note when identifiable.",
            "So we need to put certain assumptions on the problem to make it tractable.",
            "So the first assumption that we put is that the dictionary that the data points why are generated by multiplication of A and X?",
            "That is, there exists a dictionary, A and coefficient matrix X such that the data points are generated OK, and we assume that this dictionary is in current, that is, any two columns of the Dictionary of the dictionary.",
            "Are sort of orthogonal to each other.",
            "They have very small inner product.",
            "OK, and we assume that each coefficient vector is sampled IID and it is case sparse, and we assume that the nonzero coordinates of the coefficient vector are sampled uniformly at random.",
            "Thank you.",
            "So under this generative model we study this very popular empirical method called alternative."
        ],
        [
            "Animation, so the idea behind this method is very simple at that each step we fix our estimate of the dictionary A to be 80 and estimate X and this is like solving and different sparse recovery problems.",
            "So for that we use Lasso with some thresholding and then once we have the coefficient matrix then we learn matrix A using simple least squares.",
            "Now note that if the our initial estimate of dictionary is very poor, let's say it is all zeros, then we won't be able to move in anywhere.",
            "So we need to.",
            "Provide a good initialization method and for that we use the clustering process fitting method."
        ],
        [
            "And I'll briefly describe the method here.",
            "So the main idea behind the method is the following that if two data points, why nyj have large inner product between them?",
            "Then they should share at least one common element.",
            "That is, they should be at least one common dictionary meant that it is present in both Y&YZ and.",
            "The reason for that is that we assume A to be in current matrix, and Furthermore because its eyes are sampled uniformly at random.",
            "So two data points sharing more than one common element is very.",
            "Like the probability of that is very small, so using these two observations we basically form a correlation graph and we can perform clustering over this correlation graph very efficiently, and each cluster in this correlation graph denotes or represents one dictionary element.",
            "So we take all the data points in this one cluster, take their SVD and that will give us an estimate or initial estimate of the dictionary elements.",
            "Thank you."
        ],
        [
            "So assuming this initialization procedure and let's say login website and steps of the alternating animation, we can show that we will obtain for each dictionary element will obtain an A estimate which is at most epsilon away from the true dictionary element.",
            "So we are getting exact recovery up to arbitrary precision.",
            "And for this we need certain assumptions.",
            "So first assumption is that the dictionary is new in current and we need that each of the non zero coefficient is bounded between let's say 100.",
            "Although this assumption can be relaxed under it reasonably well and we assume that each of the coefficient vector is at most K sparse, where K is less than the 2006 where these the dimensionality of the detail.",
            "And we need the number of samples to grow with R-squared, where R is the number of elements in the dictionary, that is the number of columns in the matrix.",
            "So as I mentioned earlier that.",
            "Sorry in Aurora wrong and Moitra also obtained similar results.",
            "So let me just contrast the assumption so they can work with sparsity level, which is much higher that is approved, but the sample complexity of their method is slightly worse.",
            "They need a login or epsilon sitting in the sample complexity.",
            "So if epsilon is it's exponentially small in R, then our sample complexity slightly level."
        ],
        [
            "So in somebody I have we propose this novel initiation method for dictionary learning and show that after that an alternative minimizing procedure geometric converges to the global optimum.",
            "So future work we want to first of all like relax over sparsity assumption and also bring down the sample complexity of the procedure because in practice what we see is that just order R logger samples are enough for recovery.",
            "But currently over estimates are like.",
            "Oscar dollar you want to bring down that?"
        ],
        [
            "Sample complexity so please visit R poster for more details."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone, so this is joint work with Alec Anima, Praneeth and Russische.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work we consider the problem of dictionary learning, where the goal is to learn a dictionary so that each data point can be represented as a very sparse combination of columns of this dictionary.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given a matrix Y which represents so each column of Y represents a data point, we want to learn this dictionary A as well as its coefficient matrix AX.",
                    "label": 0
                },
                {
                    "sent": "So each column of X represents coefficients of each data point.",
                    "label": 0
                },
                {
                    "sent": "In the case that we are interested in is the case of Overcomplete dictionaries, that is, a number of dictionary elements or number of columns.",
                    "label": 1
                },
                {
                    "sent": "In this matrix A is much larger than the dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look for this problem.",
                    "label": 0
                },
                {
                    "sent": "There exists a long line of work, but the problem of Overcomplete dictionary from the perspective exact recovery was sort of open.",
                    "label": 1
                },
                {
                    "sent": "Prior to this work and also concurrent work bias and you are a wrong gained Uncle Milton.",
                    "label": 0
                },
                {
                    "sent": "And it was presented in the first session.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this problem in general is NP hard, and in fact note when identifiable.",
                    "label": 0
                },
                {
                    "sent": "So we need to put certain assumptions on the problem to make it tractable.",
                    "label": 0
                },
                {
                    "sent": "So the first assumption that we put is that the dictionary that the data points why are generated by multiplication of A and X?",
                    "label": 0
                },
                {
                    "sent": "That is, there exists a dictionary, A and coefficient matrix X such that the data points are generated OK, and we assume that this dictionary is in current, that is, any two columns of the Dictionary of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Are sort of orthogonal to each other.",
                    "label": 0
                },
                {
                    "sent": "They have very small inner product.",
                    "label": 0
                },
                {
                    "sent": "OK, and we assume that each coefficient vector is sampled IID and it is case sparse, and we assume that the nonzero coordinates of the coefficient vector are sampled uniformly at random.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So under this generative model we study this very popular empirical method called alternative.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Animation, so the idea behind this method is very simple at that each step we fix our estimate of the dictionary A to be 80 and estimate X and this is like solving and different sparse recovery problems.",
                    "label": 0
                },
                {
                    "sent": "So for that we use Lasso with some thresholding and then once we have the coefficient matrix then we learn matrix A using simple least squares.",
                    "label": 0
                },
                {
                    "sent": "Now note that if the our initial estimate of dictionary is very poor, let's say it is all zeros, then we won't be able to move in anywhere.",
                    "label": 0
                },
                {
                    "sent": "So we need to.",
                    "label": 0
                },
                {
                    "sent": "Provide a good initialization method and for that we use the clustering process fitting method.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll briefly describe the method here.",
                    "label": 0
                },
                {
                    "sent": "So the main idea behind the method is the following that if two data points, why nyj have large inner product between them?",
                    "label": 0
                },
                {
                    "sent": "Then they should share at least one common element.",
                    "label": 1
                },
                {
                    "sent": "That is, they should be at least one common dictionary meant that it is present in both Y&YZ and.",
                    "label": 0
                },
                {
                    "sent": "The reason for that is that we assume A to be in current matrix, and Furthermore because its eyes are sampled uniformly at random.",
                    "label": 0
                },
                {
                    "sent": "So two data points sharing more than one common element is very.",
                    "label": 1
                },
                {
                    "sent": "Like the probability of that is very small, so using these two observations we basically form a correlation graph and we can perform clustering over this correlation graph very efficiently, and each cluster in this correlation graph denotes or represents one dictionary element.",
                    "label": 0
                },
                {
                    "sent": "So we take all the data points in this one cluster, take their SVD and that will give us an estimate or initial estimate of the dictionary elements.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So assuming this initialization procedure and let's say login website and steps of the alternating animation, we can show that we will obtain for each dictionary element will obtain an A estimate which is at most epsilon away from the true dictionary element.",
                    "label": 0
                },
                {
                    "sent": "So we are getting exact recovery up to arbitrary precision.",
                    "label": 0
                },
                {
                    "sent": "And for this we need certain assumptions.",
                    "label": 0
                },
                {
                    "sent": "So first assumption is that the dictionary is new in current and we need that each of the non zero coefficient is bounded between let's say 100.",
                    "label": 0
                },
                {
                    "sent": "Although this assumption can be relaxed under it reasonably well and we assume that each of the coefficient vector is at most K sparse, where K is less than the 2006 where these the dimensionality of the detail.",
                    "label": 0
                },
                {
                    "sent": "And we need the number of samples to grow with R-squared, where R is the number of elements in the dictionary, that is the number of columns in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned earlier that.",
                    "label": 0
                },
                {
                    "sent": "Sorry in Aurora wrong and Moitra also obtained similar results.",
                    "label": 0
                },
                {
                    "sent": "So let me just contrast the assumption so they can work with sparsity level, which is much higher that is approved, but the sample complexity of their method is slightly worse.",
                    "label": 0
                },
                {
                    "sent": "They need a login or epsilon sitting in the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So if epsilon is it's exponentially small in R, then our sample complexity slightly level.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in somebody I have we propose this novel initiation method for dictionary learning and show that after that an alternative minimizing procedure geometric converges to the global optimum.",
                    "label": 1
                },
                {
                    "sent": "So future work we want to first of all like relax over sparsity assumption and also bring down the sample complexity of the procedure because in practice what we see is that just order R logger samples are enough for recovery.",
                    "label": 1
                },
                {
                    "sent": "But currently over estimates are like.",
                    "label": 0
                },
                {
                    "sent": "Oscar dollar you want to bring down that?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample complexity so please visit R poster for more details.",
                    "label": 0
                }
            ]
        }
    }
}