{
    "id": "udf7dwgvnpwnsmd5ucckkyr2qxszehku",
    "title": "Learning Non-Redundant Codebooks for Classifying Complex Objects",
    "info": {
        "author": [
            "Wei Zhang, HP Labs, Palo Alto, HP Labs"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Classification"
        ]
    },
    "url": "http://videolectures.net/icml09_zhang_lnrccco/",
    "segmentation": [
        [
            "OK, good afternoon.",
            "My name is Rachel and I'm presenting a joint work and Oregon's using our State University."
        ],
        [
            "So the title of the paper is learning non redundant code books for classifying complex objects.",
            "Here is the update of the this presentation and 1st I will introduce the general framework of using code books for object classification and then I will start to introduce our methods which is done in long redundant code books.",
            "First I will introduce the framework of our measure and then I will start to introduce two algorithms we developed for this general framework, and then I'll give you the experiments, results and finally the conclusions and future work."
        ],
        [
            "OK, first I will introduce the general framework of using codebooks for object classification."
        ],
        [
            "So we started on different application domains like for object recognition and for document classification.",
            "First, I will introduce our study on a biological object recognition problem which is so fly recognition and use this problem.",
            "We ought to recognize the.",
            "Object classes here, which are quite similar to each other, so each of the role here is representing some of the examples from the object classes.",
            "And there are significant variations within each of the object class in scale, rotation retransformation so, so it's a very challenging problem.",
            "And in order to handle this problem we use the bags of local features approach and result code booklet."
        ],
        [
            "Any for recognition.",
            "So for each of the training images, we will apply intriguing detector which detects some interest regions in the objects with some which has some properties and then each of the regions detected will be described by fixed length region descriptors which inscribes a texture information in that using that region so.",
            "By using each region, detectors and re descriptors, each of the training which will be represented by a bag of key points, and this is variable."
        ],
        [
            "So each bag can be the size of each bag can be different, and wait before you know the standard classifiers use a fixed lens feature vectors for classification.",
            "So in order to convert the bag of keypoints reputation of images, we learn code books without code book and with the cookbook is learned by clustering the local features extracted from the training images and so basically each of the code words in the code book.",
            "That is quite the distribution of the local features in the training images and the foreign you image the we apply the same intrusion detector and the region descriptors and then the local features extracted from that image is mapped to an image attribute vector which are fixed lens and each of the attributes here count number of occurrence of each of the code words in this visual codebook.",
            "So like the first.",
            "1st.",
            "Entry here that is counting the number of currents corresponding to code word one.",
            "So like in this image there 20 regions which are located to this code word and 61 has 17.",
            "So this is the most popular image attribute vector construction method which is called term frequency and so this method has been applied successfully applied for different object recognition problems.",
            "And similarly for document classification."
        ],
        [
            "We can extract the bag of words, reputation of the documents and but the problem here is the vocabulary size of the words is huge, so this why people so this each of the entry here is counting the number currents of the words in that document.",
            "But the problem here is vocabulary size is huge.",
            "So This is why people want cluster the words into form code code words and then based on this compact reputation of the distribution in the training documents.",
            "We can map the similarly Maps knew input document to a fixed lens attribute vector for classification."
        ],
        [
            "So here also is counting the number of occurrence of each of the code words is a document.",
            "So in all those cases, codebook learning has been achieved great success during the past several years."
        ],
        [
            "So here I will slap introduce our method.",
            "We want to propose Nonredundant Landing in the codebook learning approach.",
            "So this is our general framework instead.",
            "So during recent years codebook approaches have achieved great success, like from the beginning, key means to more complex Gaussian mixture modeling, information bottleneck and vocabulary trees and so on.",
            "But in most of these approaches, the only one single code book is constructed to represent the data, but for a lot of application domains, actually a single code book is not enough to capture all the information discriminatively information in that data set.",
            "So This is why we want to learn multiple codebooks and corresponding classifiers.",
            "So the motivation is want to improve the discriminative performance of any code book and classifier linear approach.",
            "By encouraging non redundancy in the learning process.",
            "So instead of going along this direction, we're kind of going in orthogonal direction, so the approach is vital in multiple codebooks and classifiers, and we want to wrap the codebook and classifier linear process inside the boosting procedure.",
            "So this slide through the general framework of our method."
        ],
        [
            "Basically, in this framework we want to learn multiple cookbooks to reprint the to capture different aspects of this discriminatory information is a training example and the first iteration we will use a uniform with all the training examples to build the first codebook and classifier and then based on the prediction of the first classifier we update the boosting ways in a standard way like a boost.",
            "And then we learn a second cookbook which will.",
            "Be more focused on the training examples with higher rates.",
            "Those are examples which are not well represented and classified.",
            "Improved iterations, and then we build a new code book and the new classifier and we do it interactively.",
            "So by doing that we are encourage ING the codebook construction process to be non redundant to each other."
        ],
        [
            "So based on this channel framework and different application domains, we propose two different algorithms.",
            "So first one is called boost relating this many for discrete feature space.",
            "Like for documents you know for documents we can build this joint distribution table, PT, XY and X is the feature points the words and Y represents the class labels and then this table is constructed at iteration based on the new boosting rates.",
            "So the.",
            "Training examples with higher ways will be contributed more in the this table and then we can apply any supervised and clustering algorithm to construct the code book.",
            "And since we are using the weights in the construction of this joint distribution table, more focus will be put on the on the training examples which are not represented previously.",
            "So, but for us some other application domains like for object recognition based on local features.",
            "Since you know like if you use CBD is 100, twenty 28 dimensional and the value of each dimension carriage from zero to 256.",
            "So it's almost like a continuous feature space.",
            "In this case it's very difficult to build this joint distribution table, but we can use the boost resampling algorithm to generate non redundant class reset by sampling the training examples account to the update.",
            "Boosting ways so training example with higher ways will be more likely to be sampled into this classroom set and then we can then Cook Book by the Standard Classic algorithm on this train, collecting state and introduce non redundant non redundancy uses learning process.",
            "So easy implementation expert experiments of this algorithms we use.",
            "We use some standard classroom algorithm codebook linear algorithms."
        ],
        [
            "Classification algorithms in order to prove that our method can is a general and effective framework.",
            "So for documents we use the information bottleneck, which is very successful in documents.",
            "To learn the codebook and we use the name base classifier for classification.",
            "By the way, we also use.",
            "We also tested on support vector Machine as a classification method.",
            "It turns out the results is very similar and for object recognition.",
            "Forced to fly recognition problem, we use a simple key means as a codebook learning approach and use bags in trees at classification problem algorithm."
        ],
        [
            "So yeah, you will see like experiments for both application domains."
        ],
        [
            "We will prove that actually out the by introducing redundancy in the codebook learning process, we can achieve significantly improved performance on different tests."
        ],
        [
            "Datasets.",
            "First I will show the results on stuff like recognition problem.",
            "So this data set has night classes and retest on some problems in this data set likes to fly two is testing to is trying to classify two classes in data set.",
            "Stuff like 4 is trying to recognize."
        ],
        [
            "For classes and stuff like Night Train to recognize all the night classes data set so it's the complexity is increasing.",
            "As you can see here we compare.",
            "Our method boosts boost.",
            "Is this supposed resampling algorithm with some state of art methods and you can see we build the size of each codebook is of 100 and we use 50 boosting iterations.",
            "And as you can see here, for all these two datasets, we achieved significantly improved performance compared with this base lies.",
            "And in order to further prove that the advantage of the Nonredundant cookbook linear approach, we also compared to some baseline algorithms.",
            "So for all these test test testing tasks we compare our boost resampling algorithm with single, which instead of building multiple codebooks, we are.",
            "We only build one of equal size K * T and then we learn classifier the same classifier from.",
            "From the single code book to see how it works and to compare this this algorithm.",
            "With this algorithm, we will show the advantage of learning multiple cookbooks and also we compare is random which is instead of using weighted sampling based on the boosting weights, we just use simply use use backing.",
            "So basically unified uniform random sampling which do not consider the boosting ways we compared to random in order to prove that.",
            "Non redundant Denny is helping to capture all the discriminative information from the data set, so from the results you can see compare with these baselines on all the training training, all the tested tasks, which is significantly improved performance.",
            "As you can see here, especially for the most challenging through finite problem boost resampled algorithm achieved 77 error reduction comparison.",
            "Single method."
        ],
        [
            "Here is a learning curve.",
            "Our learning curve of our method.",
            "As you can see here, with the increase of the boosting iterations, basically with the addition of more non redundant code, books are all the tested three tested tasks.",
            "Our method achieved very very smooth performance improvements with the addition of new non redundant code books and usually converts within like 30 iterations.",
            "And it's very robust to overfitting."
        ],
        [
            "And then I will introduce the results on document classification datasets.",
            "We tested on newsgroup datasets and enrolled data set, and so we take some subset of these datasets to just classify writing classes from those datasets, and similarly we compare with random which is trained by backing style and we compare two versions of single baseline method, one lens, the code book.",
            "Of equal size 1000, the other ones are the best basic code book of Science 100.",
            "So as you can see here, compare with these baselines.",
            "Similarly we can see here the random method outperforms these two method, so it proves that Lenny multiple codebooks is really really helping to capture all the discriminating information.",
            "But you will also notice here that by introducing redundancy using learning process we can even further improve the performance.",
            "Of the code books."
        ],
        [
            "And here is a learning curve of our method.",
            "So similarly, you can see for all the testing tasks the learning curve is pretty smooth and it's very robust to overfitting.",
            "So on this task the convergence of the method is faster than an object recognition due to the complexity of the task itself.",
            "OK, finally so."
        ],
        [
            "Conclusions and future work.",
            "So our conclusion is nonredundant learning is simple and general framework to effectively improve the performance of code books and it has been proved proved by our experiments results on different application domains.",
            "And of course there are."
        ],
        [
            "The future work.",
            "The first future work we would like to exploit to study the underlying reasons for the effectiveness of non redundant code books.",
            "So now from the experiments we just know it's working.",
            "It's working very well but we don't quite understand why.",
            "So we want to do some analyzes like the discriminative and analysis to analyze the discriminating power, the code words and we also want to analyze the non redundancy between different code books.",
            "So we want to get some Members.",
            "Some understanding of this principle of our method, and then we ought to.",
            "Developing new methods with optimized based on the analyzing results and of course we will also do more compressus compilation experiments on well established datasets.",
            "So finally this work."
        ],
        [
            "Is supported by Oregon State University in state ID project and supported by NSF funding.",
            "Thank you very much."
        ],
        [
            "This questions.",
            "Yeah.",
            "I was wondering if you go back to the routine slide in there, one other one driver starts in effect what you doing it between 50 iterations is you're learning code book about 50 times bigger.",
            "Did you ever try to discover the effect of having a repeat kind of redundant code with their from having all the other classifiers that were trained for science?",
            "You mean the size of the codebook itself?",
            "In a single classifier which use these much larger convertible, which was the concatenation of the codebook from all the tribulations of boosting, yes, very good point.",
            "Can you repeat the question right?",
            "So the question is, you know if you just concatenate all the code books we learn and apply the multiple classifier based on the single Huge Cookbook.",
            "Yes, it's a very good point.",
            "Actually is some experiments we're doing now.",
            "We want to test OK, if you want we can buy all the code, books and.",
            "Then multiple classifiers or we just, you know, build one code book which are smaller size and multiple classifiers.",
            "Yes, so we want to do more experiments just like I said in the future work, we want to explore why it's working.",
            "So it's a classifier learning all the code, book learning or something.",
            "You know it's kind of connected.",
            "So yeah, it's very good point, thank you.",
            "Other questions yes, yes.",
            "What are service integrity is doing and it's been doing for the last five years.",
            "His name is Steve Plus the store, but this is where it ends up being very similar to what we're doing and very it was very well and most vision tasks.",
            "Can you repeat the question?",
            "Yes, actually.",
            "So we test on this stupid data sets and we compare with some basic visual codebook algorithms.",
            "I think the most closely related wise vocabulary trees method here, because it also leads multiple codebooks.",
            "Yeah, so we would like to explore.",
            "We compare on more data sets with the recent algorithms and we want to start with a.",
            "Also the.",
            "So reason why you know this method is working so well.",
            "It does work so well as well for all the music community, so you have like tons of big girls in the CVS.",
            "1.",
            "Massive, thick, influenced by sample, and I think it should be added as a 11 column.",
            "In your operation was strongly suspect that you could get the same result as the left column.",
            "I'm not sure if this is correct.",
            "Yeah, but you see the different problems, so you might be available to use visual techniques as well.",
            "These are big.",
            "This is dad.",
            "Yes, so actually recently there are a lot of papers along this direction, so we would like to explore.",
            "So if we combine the nonredundant learning with the existing without cookbook method, whether we can prove the performance or it's just drop down.",
            "So it's very good point.",
            "So we want to study this advantage of this general framework.",
            "Any other questions?",
            "So let's thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good afternoon.",
                    "label": 0
                },
                {
                    "sent": "My name is Rachel and I'm presenting a joint work and Oregon's using our State University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the title of the paper is learning non redundant code books for classifying complex objects.",
                    "label": 0
                },
                {
                    "sent": "Here is the update of the this presentation and 1st I will introduce the general framework of using code books for object classification and then I will start to introduce our methods which is done in long redundant code books.",
                    "label": 0
                },
                {
                    "sent": "First I will introduce the framework of our measure and then I will start to introduce two algorithms we developed for this general framework, and then I'll give you the experiments, results and finally the conclusions and future work.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, first I will introduce the general framework of using codebooks for object classification.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we started on different application domains like for object recognition and for document classification.",
                    "label": 0
                },
                {
                    "sent": "First, I will introduce our study on a biological object recognition problem which is so fly recognition and use this problem.",
                    "label": 0
                },
                {
                    "sent": "We ought to recognize the.",
                    "label": 0
                },
                {
                    "sent": "Object classes here, which are quite similar to each other, so each of the role here is representing some of the examples from the object classes.",
                    "label": 0
                },
                {
                    "sent": "And there are significant variations within each of the object class in scale, rotation retransformation so, so it's a very challenging problem.",
                    "label": 0
                },
                {
                    "sent": "And in order to handle this problem we use the bags of local features approach and result code booklet.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any for recognition.",
                    "label": 0
                },
                {
                    "sent": "So for each of the training images, we will apply intriguing detector which detects some interest regions in the objects with some which has some properties and then each of the regions detected will be described by fixed length region descriptors which inscribes a texture information in that using that region so.",
                    "label": 0
                },
                {
                    "sent": "By using each region, detectors and re descriptors, each of the training which will be represented by a bag of key points, and this is variable.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So each bag can be the size of each bag can be different, and wait before you know the standard classifiers use a fixed lens feature vectors for classification.",
                    "label": 0
                },
                {
                    "sent": "So in order to convert the bag of keypoints reputation of images, we learn code books without code book and with the cookbook is learned by clustering the local features extracted from the training images and so basically each of the code words in the code book.",
                    "label": 0
                },
                {
                    "sent": "That is quite the distribution of the local features in the training images and the foreign you image the we apply the same intrusion detector and the region descriptors and then the local features extracted from that image is mapped to an image attribute vector which are fixed lens and each of the attributes here count number of occurrence of each of the code words in this visual codebook.",
                    "label": 1
                },
                {
                    "sent": "So like the first.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "Entry here that is counting the number of currents corresponding to code word one.",
                    "label": 0
                },
                {
                    "sent": "So like in this image there 20 regions which are located to this code word and 61 has 17.",
                    "label": 0
                },
                {
                    "sent": "So this is the most popular image attribute vector construction method which is called term frequency and so this method has been applied successfully applied for different object recognition problems.",
                    "label": 1
                },
                {
                    "sent": "And similarly for document classification.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can extract the bag of words, reputation of the documents and but the problem here is the vocabulary size of the words is huge, so this why people so this each of the entry here is counting the number currents of the words in that document.",
                    "label": 1
                },
                {
                    "sent": "But the problem here is vocabulary size is huge.",
                    "label": 1
                },
                {
                    "sent": "So This is why people want cluster the words into form code code words and then based on this compact reputation of the distribution in the training documents.",
                    "label": 0
                },
                {
                    "sent": "We can map the similarly Maps knew input document to a fixed lens attribute vector for classification.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here also is counting the number of occurrence of each of the code words is a document.",
                    "label": 0
                },
                {
                    "sent": "So in all those cases, codebook learning has been achieved great success during the past several years.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I will slap introduce our method.",
                    "label": 0
                },
                {
                    "sent": "We want to propose Nonredundant Landing in the codebook learning approach.",
                    "label": 0
                },
                {
                    "sent": "So this is our general framework instead.",
                    "label": 0
                },
                {
                    "sent": "So during recent years codebook approaches have achieved great success, like from the beginning, key means to more complex Gaussian mixture modeling, information bottleneck and vocabulary trees and so on.",
                    "label": 0
                },
                {
                    "sent": "But in most of these approaches, the only one single code book is constructed to represent the data, but for a lot of application domains, actually a single code book is not enough to capture all the information discriminatively information in that data set.",
                    "label": 0
                },
                {
                    "sent": "So This is why we want to learn multiple codebooks and corresponding classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the motivation is want to improve the discriminative performance of any code book and classifier linear approach.",
                    "label": 1
                },
                {
                    "sent": "By encouraging non redundancy in the learning process.",
                    "label": 0
                },
                {
                    "sent": "So instead of going along this direction, we're kind of going in orthogonal direction, so the approach is vital in multiple codebooks and classifiers, and we want to wrap the codebook and classifier linear process inside the boosting procedure.",
                    "label": 1
                },
                {
                    "sent": "So this slide through the general framework of our method.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, in this framework we want to learn multiple cookbooks to reprint the to capture different aspects of this discriminatory information is a training example and the first iteration we will use a uniform with all the training examples to build the first codebook and classifier and then based on the prediction of the first classifier we update the boosting ways in a standard way like a boost.",
                    "label": 0
                },
                {
                    "sent": "And then we learn a second cookbook which will.",
                    "label": 0
                },
                {
                    "sent": "Be more focused on the training examples with higher rates.",
                    "label": 0
                },
                {
                    "sent": "Those are examples which are not well represented and classified.",
                    "label": 0
                },
                {
                    "sent": "Improved iterations, and then we build a new code book and the new classifier and we do it interactively.",
                    "label": 0
                },
                {
                    "sent": "So by doing that we are encourage ING the codebook construction process to be non redundant to each other.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based on this channel framework and different application domains, we propose two different algorithms.",
                    "label": 0
                },
                {
                    "sent": "So first one is called boost relating this many for discrete feature space.",
                    "label": 0
                },
                {
                    "sent": "Like for documents you know for documents we can build this joint distribution table, PT, XY and X is the feature points the words and Y represents the class labels and then this table is constructed at iteration based on the new boosting rates.",
                    "label": 1
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Training examples with higher ways will be contributed more in the this table and then we can apply any supervised and clustering algorithm to construct the code book.",
                    "label": 0
                },
                {
                    "sent": "And since we are using the weights in the construction of this joint distribution table, more focus will be put on the on the training examples which are not represented previously.",
                    "label": 0
                },
                {
                    "sent": "So, but for us some other application domains like for object recognition based on local features.",
                    "label": 0
                },
                {
                    "sent": "Since you know like if you use CBD is 100, twenty 28 dimensional and the value of each dimension carriage from zero to 256.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like a continuous feature space.",
                    "label": 1
                },
                {
                    "sent": "In this case it's very difficult to build this joint distribution table, but we can use the boost resampling algorithm to generate non redundant class reset by sampling the training examples account to the update.",
                    "label": 0
                },
                {
                    "sent": "Boosting ways so training example with higher ways will be more likely to be sampled into this classroom set and then we can then Cook Book by the Standard Classic algorithm on this train, collecting state and introduce non redundant non redundancy uses learning process.",
                    "label": 0
                },
                {
                    "sent": "So easy implementation expert experiments of this algorithms we use.",
                    "label": 0
                },
                {
                    "sent": "We use some standard classroom algorithm codebook linear algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification algorithms in order to prove that our method can is a general and effective framework.",
                    "label": 0
                },
                {
                    "sent": "So for documents we use the information bottleneck, which is very successful in documents.",
                    "label": 0
                },
                {
                    "sent": "To learn the codebook and we use the name base classifier for classification.",
                    "label": 0
                },
                {
                    "sent": "By the way, we also use.",
                    "label": 0
                },
                {
                    "sent": "We also tested on support vector Machine as a classification method.",
                    "label": 0
                },
                {
                    "sent": "It turns out the results is very similar and for object recognition.",
                    "label": 0
                },
                {
                    "sent": "Forced to fly recognition problem, we use a simple key means as a codebook learning approach and use bags in trees at classification problem algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, you will see like experiments for both application domains.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will prove that actually out the by introducing redundancy in the codebook learning process, we can achieve significantly improved performance on different tests.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datasets.",
                    "label": 0
                },
                {
                    "sent": "First I will show the results on stuff like recognition problem.",
                    "label": 0
                },
                {
                    "sent": "So this data set has night classes and retest on some problems in this data set likes to fly two is testing to is trying to classify two classes in data set.",
                    "label": 0
                },
                {
                    "sent": "Stuff like 4 is trying to recognize.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For classes and stuff like Night Train to recognize all the night classes data set so it's the complexity is increasing.",
                    "label": 0
                },
                {
                    "sent": "As you can see here we compare.",
                    "label": 0
                },
                {
                    "sent": "Our method boosts boost.",
                    "label": 0
                },
                {
                    "sent": "Is this supposed resampling algorithm with some state of art methods and you can see we build the size of each codebook is of 100 and we use 50 boosting iterations.",
                    "label": 0
                },
                {
                    "sent": "And as you can see here, for all these two datasets, we achieved significantly improved performance compared with this base lies.",
                    "label": 0
                },
                {
                    "sent": "And in order to further prove that the advantage of the Nonredundant cookbook linear approach, we also compared to some baseline algorithms.",
                    "label": 0
                },
                {
                    "sent": "So for all these test test testing tasks we compare our boost resampling algorithm with single, which instead of building multiple codebooks, we are.",
                    "label": 0
                },
                {
                    "sent": "We only build one of equal size K * T and then we learn classifier the same classifier from.",
                    "label": 0
                },
                {
                    "sent": "From the single code book to see how it works and to compare this this algorithm.",
                    "label": 0
                },
                {
                    "sent": "With this algorithm, we will show the advantage of learning multiple cookbooks and also we compare is random which is instead of using weighted sampling based on the boosting weights, we just use simply use use backing.",
                    "label": 1
                },
                {
                    "sent": "So basically unified uniform random sampling which do not consider the boosting ways we compared to random in order to prove that.",
                    "label": 1
                },
                {
                    "sent": "Non redundant Denny is helping to capture all the discriminative information from the data set, so from the results you can see compare with these baselines on all the training training, all the tested tasks, which is significantly improved performance.",
                    "label": 1
                },
                {
                    "sent": "As you can see here, especially for the most challenging through finite problem boost resampled algorithm achieved 77 error reduction comparison.",
                    "label": 0
                },
                {
                    "sent": "Single method.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a learning curve.",
                    "label": 0
                },
                {
                    "sent": "Our learning curve of our method.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, with the increase of the boosting iterations, basically with the addition of more non redundant code, books are all the tested three tested tasks.",
                    "label": 0
                },
                {
                    "sent": "Our method achieved very very smooth performance improvements with the addition of new non redundant code books and usually converts within like 30 iterations.",
                    "label": 0
                },
                {
                    "sent": "And it's very robust to overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I will introduce the results on document classification datasets.",
                    "label": 0
                },
                {
                    "sent": "We tested on newsgroup datasets and enrolled data set, and so we take some subset of these datasets to just classify writing classes from those datasets, and similarly we compare with random which is trained by backing style and we compare two versions of single baseline method, one lens, the code book.",
                    "label": 0
                },
                {
                    "sent": "Of equal size 1000, the other ones are the best basic code book of Science 100.",
                    "label": 0
                },
                {
                    "sent": "So as you can see here, compare with these baselines.",
                    "label": 0
                },
                {
                    "sent": "Similarly we can see here the random method outperforms these two method, so it proves that Lenny multiple codebooks is really really helping to capture all the discriminating information.",
                    "label": 0
                },
                {
                    "sent": "But you will also notice here that by introducing redundancy using learning process we can even further improve the performance.",
                    "label": 0
                },
                {
                    "sent": "Of the code books.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is a learning curve of our method.",
                    "label": 0
                },
                {
                    "sent": "So similarly, you can see for all the testing tasks the learning curve is pretty smooth and it's very robust to overfitting.",
                    "label": 0
                },
                {
                    "sent": "So on this task the convergence of the method is faster than an object recognition due to the complexity of the task itself.",
                    "label": 0
                },
                {
                    "sent": "OK, finally so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusions and future work.",
                    "label": 0
                },
                {
                    "sent": "So our conclusion is nonredundant learning is simple and general framework to effectively improve the performance of code books and it has been proved proved by our experiments results on different application domains.",
                    "label": 0
                },
                {
                    "sent": "And of course there are.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The future work.",
                    "label": 0
                },
                {
                    "sent": "The first future work we would like to exploit to study the underlying reasons for the effectiveness of non redundant code books.",
                    "label": 1
                },
                {
                    "sent": "So now from the experiments we just know it's working.",
                    "label": 0
                },
                {
                    "sent": "It's working very well but we don't quite understand why.",
                    "label": 0
                },
                {
                    "sent": "So we want to do some analyzes like the discriminative and analysis to analyze the discriminating power, the code words and we also want to analyze the non redundancy between different code books.",
                    "label": 0
                },
                {
                    "sent": "So we want to get some Members.",
                    "label": 0
                },
                {
                    "sent": "Some understanding of this principle of our method, and then we ought to.",
                    "label": 0
                },
                {
                    "sent": "Developing new methods with optimized based on the analyzing results and of course we will also do more compressus compilation experiments on well established datasets.",
                    "label": 0
                },
                {
                    "sent": "So finally this work.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is supported by Oregon State University in state ID project and supported by NSF funding.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you go back to the routine slide in there, one other one driver starts in effect what you doing it between 50 iterations is you're learning code book about 50 times bigger.",
                    "label": 0
                },
                {
                    "sent": "Did you ever try to discover the effect of having a repeat kind of redundant code with their from having all the other classifiers that were trained for science?",
                    "label": 0
                },
                {
                    "sent": "You mean the size of the codebook itself?",
                    "label": 0
                },
                {
                    "sent": "In a single classifier which use these much larger convertible, which was the concatenation of the codebook from all the tribulations of boosting, yes, very good point.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat the question right?",
                    "label": 0
                },
                {
                    "sent": "So the question is, you know if you just concatenate all the code books we learn and apply the multiple classifier based on the single Huge Cookbook.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Actually is some experiments we're doing now.",
                    "label": 0
                },
                {
                    "sent": "We want to test OK, if you want we can buy all the code, books and.",
                    "label": 0
                },
                {
                    "sent": "Then multiple classifiers or we just, you know, build one code book which are smaller size and multiple classifiers.",
                    "label": 0
                },
                {
                    "sent": "Yes, so we want to do more experiments just like I said in the future work, we want to explore why it's working.",
                    "label": 0
                },
                {
                    "sent": "So it's a classifier learning all the code, book learning or something.",
                    "label": 0
                },
                {
                    "sent": "You know it's kind of connected.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's very good point, thank you.",
                    "label": 0
                },
                {
                    "sent": "Other questions yes, yes.",
                    "label": 0
                },
                {
                    "sent": "What are service integrity is doing and it's been doing for the last five years.",
                    "label": 0
                },
                {
                    "sent": "His name is Steve Plus the store, but this is where it ends up being very similar to what we're doing and very it was very well and most vision tasks.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat the question?",
                    "label": 0
                },
                {
                    "sent": "Yes, actually.",
                    "label": 0
                },
                {
                    "sent": "So we test on this stupid data sets and we compare with some basic visual codebook algorithms.",
                    "label": 0
                },
                {
                    "sent": "I think the most closely related wise vocabulary trees method here, because it also leads multiple codebooks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we would like to explore.",
                    "label": 0
                },
                {
                    "sent": "We compare on more data sets with the recent algorithms and we want to start with a.",
                    "label": 0
                },
                {
                    "sent": "Also the.",
                    "label": 0
                },
                {
                    "sent": "So reason why you know this method is working so well.",
                    "label": 0
                },
                {
                    "sent": "It does work so well as well for all the music community, so you have like tons of big girls in the CVS.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Massive, thick, influenced by sample, and I think it should be added as a 11 column.",
                    "label": 0
                },
                {
                    "sent": "In your operation was strongly suspect that you could get the same result as the left column.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if this is correct.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you see the different problems, so you might be available to use visual techniques as well.",
                    "label": 0
                },
                {
                    "sent": "These are big.",
                    "label": 0
                },
                {
                    "sent": "This is dad.",
                    "label": 0
                },
                {
                    "sent": "Yes, so actually recently there are a lot of papers along this direction, so we would like to explore.",
                    "label": 0
                },
                {
                    "sent": "So if we combine the nonredundant learning with the existing without cookbook method, whether we can prove the performance or it's just drop down.",
                    "label": 0
                },
                {
                    "sent": "So it's very good point.",
                    "label": 0
                },
                {
                    "sent": "So we want to study this advantage of this general framework.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So let's thank you again.",
                    "label": 0
                }
            ]
        }
    }
}