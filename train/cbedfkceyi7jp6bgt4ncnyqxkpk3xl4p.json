{
    "id": "cbedfkceyi7jp6bgt4ncnyqxkpk3xl4p",
    "title": "Semantic Topic Compass - Classification based on Unsupervised Feature Ambiguity Gradation",
    "info": {
        "author": [
            "Hassan Saif, Knowledge Media Institute (KMI), Open University (OU)"
        ],
        "published": "July 28, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_saif_topic_compass/",
    "segmentation": [
        [
            "Alright, so my name is Hassan Saif.",
            "I'm from the Open University and today we're going to talk about Semantic Topic Compass which is a framework for topic classification based on unsupervised feature ambiguity graduation."
        ],
        [
            "So I'm going to start the talk by background about topic classification on social media and then discuss the problem of ambiguity or ambiguous features for topic classification.",
            "After that represents the our framework for feature selection or ambiguous ambiguous feature selection basically, and then discuss the evaluation and the results."
        ],
        [
            "So topic classification on social media is basically on specifically on Twitter.",
            "Is the task of Ant ifying.",
            "Whether a tweet is related or unrelated to a given topic.",
            "It's."
        ],
        [
            "Relatively recent research work that's being done on Twitter with the civil works have been approached the problem mainly using machine learning approach where machine learning classifier is trained from features extracted from tweets labeled with their typical orientation.",
            "After that, the classifier is trained.",
            "Classifiers then apply to tweets in order to antifi their retina sore.",
            "The topic."
        ],
        [
            "Several features have been used to this end or to train the classifiers.",
            "They can be categorized as either lexical features or semantic features.",
            "The lexical features are the features that are extracted from the lexical representation of tweet messages, such as word, any grams or mainly unigrams.",
            "Other works basically used a combination of words.",
            "Sorry word unigrams and named entities in the tweets and others use Twitter features such as emoticons, operated words in the tweets and URLs, usernames, etc.",
            "The other type of features that have been used or the semantic features with are extracted from extending resources such as ontologies and semantic networks.",
            "For example, some works have extracted the entities in tweets and they map these entities to their semantic categories or concepts or.",
            "Types in DDL.",
            "Others use semantic resources from prophase ontology.",
            "Now the common limitation of using this kind of features is the high dimensionality of the feature space that the classifier is trained from, which in some cases reach to millions of features when we dealing with Twitter of large Twitter corpora corpora.",
            "Now in order."
        ],
        [
            "Reduce the dimensionality of the feature space.",
            "Several works have used feature selection, different feature selection methods.",
            "This feature, these can be also either based on time frequency such as term frequency, TF, IDF, Delta, IDF, etc.",
            "The other type of feature selection methods they rely on information gain criteria such as mutual information and pointwise mutual information.",
            "Now there are two limitations for using these methods for feature selection.",
            "The first one is they are not tolerant to imbalanced class distribution and the data set, meaning that they assign always high discrimination value 2 features that are.",
            "Or occur with the dominant class in the data.",
            "The other problem is they basically perform they have low performance.",
            "In the case of ambiguous features now."
        ],
        [
            "What are the biggest features or basically in general, what is the ambiguity for topic classification?",
            "So ambiguity in terms of classification occurs in two different scenarios.",
            "The first one is basically when the presence and absence of a feature in a given class is almost the same, right?",
            "So the classifier wouldn't be able to differentiate whether this feature is related or unrelated to a given topic.",
            "For example, if I have a topic like war conflict, I can define three sets or three types of features.",
            "The semantically related features such As for example Izers or Islamic State in Iraq and Syria.",
            "Hard revolution and also the other type is semantically unrelated features which are features that are related to the topics such as Mashable or Jay-Z or any other related features.",
            "And in between there are features that are kind of contribute equally to relative yonassan relatedness to the topic like Greece or Aljazeera and then basically the classifiers fail in using this kind of features.",
            "The other case where the ambiguously ambiguity occurs in for topic classification is basically when the semantic of features change by changing the event or the topic on Twitter, for example, the word but clone have been used, let's say or general use of particular was or a semantic particular basically refer to the theater in Paris, which is related to the topic entertainments.",
            "However, after the Paris attacks or the terrorist attacks in Paris in 2000."
        ],
        [
            "15 but has been used to refer to the tourist attack and which is related to the topic violence.",
            "Another example is ISIS.",
            "So eyes as before 2013 or before the emergence of the terrorist group let's lamic State of Iraq and Syria, ISIS was used to denote different actually concepts such as people's name or companies names, and even like eyes as we say is, is is basically the Egyptian goddess which is related to the Egyptian mythology as a topic.",
            "After 2013, Izers have been used or has been used to denote the jihadist militant group which is related to the topic terrorism.",
            "Now."
        ],
        [
            "In order to find these kind of ambiguous features, very few works actually have been proposed to deal with this problem on Twitter.",
            "One thing you needed work is the ambiguity measure feature selection methods.",
            "This method is unsupervised and it's basically assign high score to features that only occur with one class.",
            "& let's say load discrimination value to all other features and their works.",
            "The authors argue that their their method outperforms the ad ratio information gain anti squared methods.",
            "Ever there's always.",
            "However, there are two limitations for this method.",
            "First, it requires tweets labeled with their topical orientation because it's basically supervised and the other one it doesn't account.",
            "Let's say or doesn't consider the semantics of words when calculating or the semantic of features when calculating their ambiguity.",
            "So too."
        ],
        [
            "Address these limitations.",
            "We propose a framework called semantic topic compost, which is unsupervised approach for topic feature representation.",
            "It basically allows for amplifying or measuring the relevance or a relevance score of features for a given topic and also uses two types of semantics.",
            "Distributional semantics on conceptual semantics in order to unidentified those ambiguous features for a given topic.",
            "So the pipeline in our framework breaks down into three main steps or phases.",
            "In the first step.",
            "Basically we build a semantic representation of the feature space for a given topic.",
            "After that we use this representation to extract and wait the features based on their relative Ness Analytics to a given topic and the final step is to train the topic classifiers."
        ],
        [
            "So.",
            "In order to extract the semantics of features or words in our framework, we rely on the semantic distribution of semantic hypothesis.",
            "So which state?",
            "Basically that words that Co occur in similar contexts tend to have similar meaning.",
            "So for example, the word eyes is when it occurs with words like Assad, Middle East, behaved or kill, then it's semantics refers to the jihadist group which is related to the topic, violence or terrorism.",
            "On the other hand, where eyes or eases basically occurs with words like gods or ancient or Egyptians.",
            "There is more likely failing to the Egyptian goddess, which is related to the Greek.",
            "Sorry, the Egyptian mythology as."
        ],
        [
            "Topic.",
            "Now, in order to implement this hypothesis or use this hypothesis in our framework, we represent each term in our in the Dayton given data set and of a given topic.",
            "Basically like ISIS as a vector of its context, terms that occur with with the word with the term izers.",
            "For example, Bludan Syria, and then for each context, then we assign two values topic prior which denote the initial relativeness of awards to a given topic and the other value is the degree of correlation between the context term.",
            "And the mentor and its base is calculated based on the TF IDF measurement."
        ],
        [
            "After that we transform the vector representation or the context vector representation of the term izers.",
            "For example here into 2D circle representation, where the main center of the circle is the main term izers and each term each point in the circle denotes a context time that occur with ISIS in a given corpus and the position of each context term is defined basically jointly based based on the correlation between the two terms and also the prior topic.",
            "Topics prior."
        ],
        [
            "Now after we resent this, say the contextual or distribution semantics of each term by means of a 2D circle, we move to construct the topics feature space.",
            "Now we also had used the secular presentation where each point here denotes actually or refer to the circle that we already extracted for each term.",
            "And its position based on calculating the geometric median of the circle of the term right?",
            "So this geometric or less dramatic nature of the two D circle allows us to define four different quadrants, the other two quadrants are those or contain features that are related to the topic.",
            "Moreover, the upper left quadrant are contains features basically that are strongly related while the upper right quadrant contains features that are really unrelated.",
            "Also really related so.",
            "The same for the lower two quadrants.",
            "We have the weekly unrelated features on the right and then the strongly unrelated topic features to the given topic on the left.",
            "So as I mentioned, we."
        ],
        [
            "In order to position the terms inside the circle, we use the topics prior and in order to delete the prayers of the topics prior of a given word.",
            "We basically take like a topic for a given topic.",
            "Basically the topic and then we look into Wikipedia articles that talking about this topic and then we measure for each term in the article it's rated nishta the given topic based on mutual information criteria.",
            "Right so the."
        ],
        [
            "Step in the pipeline or the framework is basically to use the feature space or the semantic representation of the topic feature space in order to extract and weight the features based on the quadrants they belong to.",
            "So for example, we give highway to those strongly related features which which lie in the strongly related quadrant, and we give lower weights to those dots.",
            "For example, are strongly unrelated to the given topic.",
            "After that we use the features extracted from the feature presentation to train the topic classifier.",
            "We also tried to enrich the."
        ],
        [
            "A feature space, or the secular representation of the topic, feature space by the conceptual semantics of words.",
            "So, the conceptual semantics of words refer to the explicit semantics or semantic concepts that of a given word in the tweet which is extracted from ontologies like Pedia.",
            "So on the rationale behind using the conceptual semantics is that we sometimes lack context which prevent us from determining whether award belongs to the topic or doesn't.",
            "Right, so for example, if I have the tweet like ISIS continuous operating like a tumor.",
            "Or the context?",
            "He doesn't really allow to say whether eyes is related to topic tourism or not.",
            "However, by looking into its explicit semantic category or concept, indeed, yeah, we can see that it's related or it's mapped to jihadist militants, which somehow is related to the topic terrorism or topic violence.",
            "Now we enrich the feature space or the semantic feature space representation in our framework by basically extracting 1st and entities in the tweets.",
            "And then map these entities to three different type of concepts or semantic information.",
            "The first one is the concepts extracted by Alchemy API service, for example, as is organization.",
            "The other one is the media categories.",
            "Eyes is jihadist group and then DVD classes.",
            "So we we do the richemond by either adding the extracted concepts directly to the feature space, add additional features, or by replacing the entities with their associated semantic concepts."
        ],
        [
            "Right?"
        ],
        [
            "As for experiments, we basically experiment with our framework using three different Twitter datasets.",
            "That are from different or representing different three different topics.",
            "Disaster accident, low crime, and war conflict.",
            "So tweets in these datasets are manually labeled, tormented with their typical orientation.",
            "We also compare our order, compare the performance of the classifiers trained from our features against features extracted based on the lexical information of the trees or the semantic information.",
            "These features.",
            "For example, our TF IDF LDA or topics extracted by the LDA method.",
            "And the ambiguity measurement method.",
            "As for semantic features, we compare against DVD categories, DVD classes and Alchemy API."
        ],
        [
            "Concepts.",
            "So results cool."
        ],
        [
            "Now the first line of evaluation we did, or the first part of the evaluation we did, we try to investigate the impact of the feature waiting on the classification performance.",
            "As I said in our our representation, we try to categorize the features based on their illness, our non redness to the topic, right?",
            "So in order to do that we train the classifier from features in all the quadrants except one quadrant each time.",
            "So for example, we train the classifier from features in quadrant.",
            "123 but we leave out the features that are weakly or strongly unrelated which are in quadrant four, and we measure the performance.",
            "Our baseline for comparison.",
            "Here is the TF method or features extracted based on the time, frequency and what we can see.",
            "Basically that using all the features from the from the semantic representation of the feature space improve basically the precision by up to 7%.",
            "However, we produce lower recall.",
            "I'll explain later.",
            "So what we notice that removing those unrelated the weekly?",
            "Sorry there's strongly unrelated features and the weekly related features in produce.",
            "Basically the highest performance in F measure comparing to all other settings.",
            "Right so the."
        ],
        [
            "Second line of the evaluation we did.",
            "Basically we compare the performance of our features against the four type of lexical features and the four types of the semantic features.",
            "OK, I'm going to look at this table right."
        ],
        [
            "Here we measure the performance and precision recall and what we see that the best performing lexical features are the topics or the LDA topics with 85% in precision.",
            "The best performing semantic features are the DVD categories.",
            "However, we can see that the SDS which is the semantic topic Compass features outperform all other types of features by at least 7% in precision.",
            "As I explained earlier that using the features by our extracted from our framework reduced lower recall, right?",
            "So here we we measure also the recall of the classifiers trained from different type of features and what we see that basically in reaching the last three columns and reaching the framework with the conceptual semantics as explained earlier basically.",
            "Improve the recall of the framework, and even more actually recall off the for the classifiers comparing to all other features."
        ],
        [
            "OK, so conclusion we introduced Semantic Topic Compass framework which is unsupervised approach that allows to antifi the retina cordinate illness for features for topic classification.",
            "It uses both distribution and consumption semantics of words in order to a notify these ambiguous features and wait them for classifier training.",
            "Our results show that the features extracted by our framework outperform all lexical and semantic features in precision.",
            "By up to 7% and also we found that removing weekly related features.",
            "From our feature space also improves the performance in F measure.",
            "Finally, we notice that enriching the framework with conceptual semantics reduce the highest recall of the classifiers comparing to all other features."
        ],
        [
            "That's it, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so my name is Hassan Saif.",
                    "label": 0
                },
                {
                    "sent": "I'm from the Open University and today we're going to talk about Semantic Topic Compass which is a framework for topic classification based on unsupervised feature ambiguity graduation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to start the talk by background about topic classification on social media and then discuss the problem of ambiguity or ambiguous features for topic classification.",
                    "label": 0
                },
                {
                    "sent": "After that represents the our framework for feature selection or ambiguous ambiguous feature selection basically, and then discuss the evaluation and the results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So topic classification on social media is basically on specifically on Twitter.",
                    "label": 1
                },
                {
                    "sent": "Is the task of Ant ifying.",
                    "label": 0
                },
                {
                    "sent": "Whether a tweet is related or unrelated to a given topic.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relatively recent research work that's being done on Twitter with the civil works have been approached the problem mainly using machine learning approach where machine learning classifier is trained from features extracted from tweets labeled with their typical orientation.",
                    "label": 1
                },
                {
                    "sent": "After that, the classifier is trained.",
                    "label": 0
                },
                {
                    "sent": "Classifiers then apply to tweets in order to antifi their retina sore.",
                    "label": 0
                },
                {
                    "sent": "The topic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Several features have been used to this end or to train the classifiers.",
                    "label": 0
                },
                {
                    "sent": "They can be categorized as either lexical features or semantic features.",
                    "label": 0
                },
                {
                    "sent": "The lexical features are the features that are extracted from the lexical representation of tweet messages, such as word, any grams or mainly unigrams.",
                    "label": 1
                },
                {
                    "sent": "Other works basically used a combination of words.",
                    "label": 0
                },
                {
                    "sent": "Sorry word unigrams and named entities in the tweets and others use Twitter features such as emoticons, operated words in the tweets and URLs, usernames, etc.",
                    "label": 0
                },
                {
                    "sent": "The other type of features that have been used or the semantic features with are extracted from extending resources such as ontologies and semantic networks.",
                    "label": 0
                },
                {
                    "sent": "For example, some works have extracted the entities in tweets and they map these entities to their semantic categories or concepts or.",
                    "label": 0
                },
                {
                    "sent": "Types in DDL.",
                    "label": 0
                },
                {
                    "sent": "Others use semantic resources from prophase ontology.",
                    "label": 1
                },
                {
                    "sent": "Now the common limitation of using this kind of features is the high dimensionality of the feature space that the classifier is trained from, which in some cases reach to millions of features when we dealing with Twitter of large Twitter corpora corpora.",
                    "label": 0
                },
                {
                    "sent": "Now in order.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reduce the dimensionality of the feature space.",
                    "label": 0
                },
                {
                    "sent": "Several works have used feature selection, different feature selection methods.",
                    "label": 0
                },
                {
                    "sent": "This feature, these can be also either based on time frequency such as term frequency, TF, IDF, Delta, IDF, etc.",
                    "label": 0
                },
                {
                    "sent": "The other type of feature selection methods they rely on information gain criteria such as mutual information and pointwise mutual information.",
                    "label": 0
                },
                {
                    "sent": "Now there are two limitations for using these methods for feature selection.",
                    "label": 0
                },
                {
                    "sent": "The first one is they are not tolerant to imbalanced class distribution and the data set, meaning that they assign always high discrimination value 2 features that are.",
                    "label": 1
                },
                {
                    "sent": "Or occur with the dominant class in the data.",
                    "label": 0
                },
                {
                    "sent": "The other problem is they basically perform they have low performance.",
                    "label": 1
                },
                {
                    "sent": "In the case of ambiguous features now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the biggest features or basically in general, what is the ambiguity for topic classification?",
                    "label": 0
                },
                {
                    "sent": "So ambiguity in terms of classification occurs in two different scenarios.",
                    "label": 0
                },
                {
                    "sent": "The first one is basically when the presence and absence of a feature in a given class is almost the same, right?",
                    "label": 1
                },
                {
                    "sent": "So the classifier wouldn't be able to differentiate whether this feature is related or unrelated to a given topic.",
                    "label": 0
                },
                {
                    "sent": "For example, if I have a topic like war conflict, I can define three sets or three types of features.",
                    "label": 0
                },
                {
                    "sent": "The semantically related features such As for example Izers or Islamic State in Iraq and Syria.",
                    "label": 0
                },
                {
                    "sent": "Hard revolution and also the other type is semantically unrelated features which are features that are related to the topics such as Mashable or Jay-Z or any other related features.",
                    "label": 0
                },
                {
                    "sent": "And in between there are features that are kind of contribute equally to relative yonassan relatedness to the topic like Greece or Aljazeera and then basically the classifiers fail in using this kind of features.",
                    "label": 0
                },
                {
                    "sent": "The other case where the ambiguously ambiguity occurs in for topic classification is basically when the semantic of features change by changing the event or the topic on Twitter, for example, the word but clone have been used, let's say or general use of particular was or a semantic particular basically refer to the theater in Paris, which is related to the topic entertainments.",
                    "label": 0
                },
                {
                    "sent": "However, after the Paris attacks or the terrorist attacks in Paris in 2000.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "15 but has been used to refer to the tourist attack and which is related to the topic violence.",
                    "label": 0
                },
                {
                    "sent": "Another example is ISIS.",
                    "label": 0
                },
                {
                    "sent": "So eyes as before 2013 or before the emergence of the terrorist group let's lamic State of Iraq and Syria, ISIS was used to denote different actually concepts such as people's name or companies names, and even like eyes as we say is, is is basically the Egyptian goddess which is related to the Egyptian mythology as a topic.",
                    "label": 0
                },
                {
                    "sent": "After 2013, Izers have been used or has been used to denote the jihadist militant group which is related to the topic terrorism.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to find these kind of ambiguous features, very few works actually have been proposed to deal with this problem on Twitter.",
                    "label": 0
                },
                {
                    "sent": "One thing you needed work is the ambiguity measure feature selection methods.",
                    "label": 1
                },
                {
                    "sent": "This method is unsupervised and it's basically assign high score to features that only occur with one class.",
                    "label": 0
                },
                {
                    "sent": "& let's say load discrimination value to all other features and their works.",
                    "label": 1
                },
                {
                    "sent": "The authors argue that their their method outperforms the ad ratio information gain anti squared methods.",
                    "label": 0
                },
                {
                    "sent": "Ever there's always.",
                    "label": 0
                },
                {
                    "sent": "However, there are two limitations for this method.",
                    "label": 0
                },
                {
                    "sent": "First, it requires tweets labeled with their topical orientation because it's basically supervised and the other one it doesn't account.",
                    "label": 1
                },
                {
                    "sent": "Let's say or doesn't consider the semantics of words when calculating or the semantic of features when calculating their ambiguity.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Address these limitations.",
                    "label": 0
                },
                {
                    "sent": "We propose a framework called semantic topic compost, which is unsupervised approach for topic feature representation.",
                    "label": 1
                },
                {
                    "sent": "It basically allows for amplifying or measuring the relevance or a relevance score of features for a given topic and also uses two types of semantics.",
                    "label": 0
                },
                {
                    "sent": "Distributional semantics on conceptual semantics in order to unidentified those ambiguous features for a given topic.",
                    "label": 0
                },
                {
                    "sent": "So the pipeline in our framework breaks down into three main steps or phases.",
                    "label": 0
                },
                {
                    "sent": "In the first step.",
                    "label": 1
                },
                {
                    "sent": "Basically we build a semantic representation of the feature space for a given topic.",
                    "label": 0
                },
                {
                    "sent": "After that we use this representation to extract and wait the features based on their relative Ness Analytics to a given topic and the final step is to train the topic classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In order to extract the semantics of features or words in our framework, we rely on the semantic distribution of semantic hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So which state?",
                    "label": 0
                },
                {
                    "sent": "Basically that words that Co occur in similar contexts tend to have similar meaning.",
                    "label": 1
                },
                {
                    "sent": "So for example, the word eyes is when it occurs with words like Assad, Middle East, behaved or kill, then it's semantics refers to the jihadist group which is related to the topic, violence or terrorism.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, where eyes or eases basically occurs with words like gods or ancient or Egyptians.",
                    "label": 0
                },
                {
                    "sent": "There is more likely failing to the Egyptian goddess, which is related to the Greek.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the Egyptian mythology as.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic.",
                    "label": 0
                },
                {
                    "sent": "Now, in order to implement this hypothesis or use this hypothesis in our framework, we represent each term in our in the Dayton given data set and of a given topic.",
                    "label": 0
                },
                {
                    "sent": "Basically like ISIS as a vector of its context, terms that occur with with the word with the term izers.",
                    "label": 0
                },
                {
                    "sent": "For example, Bludan Syria, and then for each context, then we assign two values topic prior which denote the initial relativeness of awards to a given topic and the other value is the degree of correlation between the context term.",
                    "label": 1
                },
                {
                    "sent": "And the mentor and its base is calculated based on the TF IDF measurement.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After that we transform the vector representation or the context vector representation of the term izers.",
                    "label": 0
                },
                {
                    "sent": "For example here into 2D circle representation, where the main center of the circle is the main term izers and each term each point in the circle denotes a context time that occur with ISIS in a given corpus and the position of each context term is defined basically jointly based based on the correlation between the two terms and also the prior topic.",
                    "label": 0
                },
                {
                    "sent": "Topics prior.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now after we resent this, say the contextual or distribution semantics of each term by means of a 2D circle, we move to construct the topics feature space.",
                    "label": 1
                },
                {
                    "sent": "Now we also had used the secular presentation where each point here denotes actually or refer to the circle that we already extracted for each term.",
                    "label": 0
                },
                {
                    "sent": "And its position based on calculating the geometric median of the circle of the term right?",
                    "label": 0
                },
                {
                    "sent": "So this geometric or less dramatic nature of the two D circle allows us to define four different quadrants, the other two quadrants are those or contain features that are related to the topic.",
                    "label": 1
                },
                {
                    "sent": "Moreover, the upper left quadrant are contains features basically that are strongly related while the upper right quadrant contains features that are really unrelated.",
                    "label": 0
                },
                {
                    "sent": "Also really related so.",
                    "label": 0
                },
                {
                    "sent": "The same for the lower two quadrants.",
                    "label": 1
                },
                {
                    "sent": "We have the weekly unrelated features on the right and then the strongly unrelated topic features to the given topic on the left.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to position the terms inside the circle, we use the topics prior and in order to delete the prayers of the topics prior of a given word.",
                    "label": 0
                },
                {
                    "sent": "We basically take like a topic for a given topic.",
                    "label": 1
                },
                {
                    "sent": "Basically the topic and then we look into Wikipedia articles that talking about this topic and then we measure for each term in the article it's rated nishta the given topic based on mutual information criteria.",
                    "label": 0
                },
                {
                    "sent": "Right so the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step in the pipeline or the framework is basically to use the feature space or the semantic representation of the topic feature space in order to extract and weight the features based on the quadrants they belong to.",
                    "label": 0
                },
                {
                    "sent": "So for example, we give highway to those strongly related features which which lie in the strongly related quadrant, and we give lower weights to those dots.",
                    "label": 1
                },
                {
                    "sent": "For example, are strongly unrelated to the given topic.",
                    "label": 1
                },
                {
                    "sent": "After that we use the features extracted from the feature presentation to train the topic classifier.",
                    "label": 0
                },
                {
                    "sent": "We also tried to enrich the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A feature space, or the secular representation of the topic, feature space by the conceptual semantics of words.",
                    "label": 1
                },
                {
                    "sent": "So, the conceptual semantics of words refer to the explicit semantics or semantic concepts that of a given word in the tweet which is extracted from ontologies like Pedia.",
                    "label": 0
                },
                {
                    "sent": "So on the rationale behind using the conceptual semantics is that we sometimes lack context which prevent us from determining whether award belongs to the topic or doesn't.",
                    "label": 0
                },
                {
                    "sent": "Right, so for example, if I have the tweet like ISIS continuous operating like a tumor.",
                    "label": 0
                },
                {
                    "sent": "Or the context?",
                    "label": 0
                },
                {
                    "sent": "He doesn't really allow to say whether eyes is related to topic tourism or not.",
                    "label": 0
                },
                {
                    "sent": "However, by looking into its explicit semantic category or concept, indeed, yeah, we can see that it's related or it's mapped to jihadist militants, which somehow is related to the topic terrorism or topic violence.",
                    "label": 0
                },
                {
                    "sent": "Now we enrich the feature space or the semantic feature space representation in our framework by basically extracting 1st and entities in the tweets.",
                    "label": 0
                },
                {
                    "sent": "And then map these entities to three different type of concepts or semantic information.",
                    "label": 0
                },
                {
                    "sent": "The first one is the concepts extracted by Alchemy API service, for example, as is organization.",
                    "label": 0
                },
                {
                    "sent": "The other one is the media categories.",
                    "label": 0
                },
                {
                    "sent": "Eyes is jihadist group and then DVD classes.",
                    "label": 0
                },
                {
                    "sent": "So we we do the richemond by either adding the extracted concepts directly to the feature space, add additional features, or by replacing the entities with their associated semantic concepts.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As for experiments, we basically experiment with our framework using three different Twitter datasets.",
                    "label": 0
                },
                {
                    "sent": "That are from different or representing different three different topics.",
                    "label": 0
                },
                {
                    "sent": "Disaster accident, low crime, and war conflict.",
                    "label": 0
                },
                {
                    "sent": "So tweets in these datasets are manually labeled, tormented with their typical orientation.",
                    "label": 0
                },
                {
                    "sent": "We also compare our order, compare the performance of the classifiers trained from our features against features extracted based on the lexical information of the trees or the semantic information.",
                    "label": 0
                },
                {
                    "sent": "These features.",
                    "label": 0
                },
                {
                    "sent": "For example, our TF IDF LDA or topics extracted by the LDA method.",
                    "label": 0
                },
                {
                    "sent": "And the ambiguity measurement method.",
                    "label": 0
                },
                {
                    "sent": "As for semantic features, we compare against DVD categories, DVD classes and Alchemy API.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concepts.",
                    "label": 0
                },
                {
                    "sent": "So results cool.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the first line of evaluation we did, or the first part of the evaluation we did, we try to investigate the impact of the feature waiting on the classification performance.",
                    "label": 1
                },
                {
                    "sent": "As I said in our our representation, we try to categorize the features based on their illness, our non redness to the topic, right?",
                    "label": 0
                },
                {
                    "sent": "So in order to do that we train the classifier from features in all the quadrants except one quadrant each time.",
                    "label": 0
                },
                {
                    "sent": "So for example, we train the classifier from features in quadrant.",
                    "label": 0
                },
                {
                    "sent": "123 but we leave out the features that are weakly or strongly unrelated which are in quadrant four, and we measure the performance.",
                    "label": 0
                },
                {
                    "sent": "Our baseline for comparison.",
                    "label": 0
                },
                {
                    "sent": "Here is the TF method or features extracted based on the time, frequency and what we can see.",
                    "label": 0
                },
                {
                    "sent": "Basically that using all the features from the from the semantic representation of the feature space improve basically the precision by up to 7%.",
                    "label": 0
                },
                {
                    "sent": "However, we produce lower recall.",
                    "label": 0
                },
                {
                    "sent": "I'll explain later.",
                    "label": 0
                },
                {
                    "sent": "So what we notice that removing those unrelated the weekly?",
                    "label": 0
                },
                {
                    "sent": "Sorry there's strongly unrelated features and the weekly related features in produce.",
                    "label": 0
                },
                {
                    "sent": "Basically the highest performance in F measure comparing to all other settings.",
                    "label": 0
                },
                {
                    "sent": "Right so the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second line of the evaluation we did.",
                    "label": 0
                },
                {
                    "sent": "Basically we compare the performance of our features against the four type of lexical features and the four types of the semantic features.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to look at this table right.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we measure the performance and precision recall and what we see that the best performing lexical features are the topics or the LDA topics with 85% in precision.",
                    "label": 0
                },
                {
                    "sent": "The best performing semantic features are the DVD categories.",
                    "label": 0
                },
                {
                    "sent": "However, we can see that the SDS which is the semantic topic Compass features outperform all other types of features by at least 7% in precision.",
                    "label": 0
                },
                {
                    "sent": "As I explained earlier that using the features by our extracted from our framework reduced lower recall, right?",
                    "label": 0
                },
                {
                    "sent": "So here we we measure also the recall of the classifiers trained from different type of features and what we see that basically in reaching the last three columns and reaching the framework with the conceptual semantics as explained earlier basically.",
                    "label": 0
                },
                {
                    "sent": "Improve the recall of the framework, and even more actually recall off the for the classifiers comparing to all other features.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so conclusion we introduced Semantic Topic Compass framework which is unsupervised approach that allows to antifi the retina cordinate illness for features for topic classification.",
                    "label": 1
                },
                {
                    "sent": "It uses both distribution and consumption semantics of words in order to a notify these ambiguous features and wait them for classifier training.",
                    "label": 0
                },
                {
                    "sent": "Our results show that the features extracted by our framework outperform all lexical and semantic features in precision.",
                    "label": 1
                },
                {
                    "sent": "By up to 7% and also we found that removing weekly related features.",
                    "label": 1
                },
                {
                    "sent": "From our feature space also improves the performance in F measure.",
                    "label": 1
                },
                {
                    "sent": "Finally, we notice that enriching the framework with conceptual semantics reduce the highest recall of the classifiers comparing to all other features.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}