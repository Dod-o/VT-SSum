{
    "id": "ohuoolaqh6ajack27w4ltt34rprkhmh2",
    "title": "Generalization bounds",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "March 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/mlss05us_langford_gb/",
    "segmentation": [
        [
            "Morning.",
            "So.",
            "There's some caveats I should tell you for today's lecture.",
            "One caveat is that I was really not kidding in the introduction when I said that there are hundreds of papers on this subject.",
            "This is only a very small piece.",
            "It is the piece that I consider most basic and perhaps even most useful.",
            "But there's a great deal of other work which I'm not going to cover, and which could easily be useful for various tasks.",
            "So once again, if you have questions during the talk, feel free to to ask them then.",
            "Oh donaj, you made it?",
            "We were all kinds of organized for this lecture.",
            "Donaji, actually.",
            "Printed out and copied slides.",
            "So not only are they on the web, there also in your hand shortly.",
            "OK, so."
        ],
        [
            "What are generalization bounds?",
            "So first of all, we need to start out what learning is and a reasonable understanding about weren't what learning is.",
            "It's acquiring prediction ability.",
            "So the process of learning is the process of acquiring your ability to predict in the future.",
            "So.",
            "If you think about this a bit, you realize that you know in the worst case you can't do this.",
            "You could have an adversary that knows the process that you're using to learn and just chooses to screw you every time.",
            "But you know, maybe in the real world you don't always have an adversary.",
            "And then the question is.",
            "How can we even analyze this?",
            "And one very common way is you assume that you have independent examples that are each coming from the same distribution over and over again, right?",
            "And then.",
            "And then it becomes possible to actually predict because.",
            "Doing well in the past means that you have some some reasonable expectation of doing well in the future, so the mathematics I'm talking about today is how doing well in the past gives you some reasonable expectation about how you do in the future.",
            "So the thing to understand about this is that it is significantly different from my other lecture, and that we're assuming independent examples independent and identically distributed examples in fact.",
            "So.",
            "This is something that.",
            "I think is kind of critical to properly understanding the mathematics.",
            "You have to really watch the assumptions right.",
            "This is an assumption of independence and what that means.",
            "This applies when your examples are independent.",
            "In practice, often your cables will not be independent.",
            "Sometimes they will be, or at least close enough to where we can use the same mass.",
            "So this is something with a more limited scope and we talked about last time, but it's answering a question that cannot be answered without making some assumptions about how the past relates to the future.",
            "Independence is the standard way of doing that.",
            "Yeah.",
            "Yes, so the question is is first of all at the same as the no free lunch theorem?",
            "And the answer is yes.",
            "So no free lunch theorem is kind of this stated in the basean way.",
            "If you just think about it in an entirely adversarial way to trivial statement.",
            "OK, so."
        ],
        [
            "Why do we want to study this?",
            "So maybe if we study this then we get better methods for learning and actually for verifying our predictive ability, right so?",
            "Maybe if you're working on the space shuttle or something.",
            "Boss wants to know that that things you predict are actually going to work pretty well in the future.",
            "So maybe you want to actually have some technique for coming up to him saying here's a certificate which says I have successfully learned to predict when O rings will fail.",
            "It may be one that certificate to be, you know, pretty darn ironclad.",
            "So the things we're talking about today give you some mechanism for doing that.",
            "Right?"
        ],
        [
            "So.",
            "Better methods of learning.",
            "So when people are faced with this problem right now.",
            "Well, they typically do is they take if they have a bunch of examples in front of them.",
            "They take, you know, the first 2/3 or whatever, or the 1st, all but 100 and they train on that and the test on the last 1/3 or last 100 or something like that, right?",
            "So we divide things into a training set, test set which are in the training set testing test set.",
            "And then the hope is that maybe maybe we can actually do better.",
            "There are other techniques we'll be talking about today for giving you a proof of the same.",
            "Degree of validity.",
            "As I observed, a good test error rate.",
            "And actually, a better degree of validity in many ways.",
            "And then the other thing that you get when you look at this is you.",
            "You learn a little bit about how learning algorithms should be designed.",
            "So in particular we."
        ],
        [
            "Look at these bounds.",
            "We can go look there is overfitting.",
            "So Rob Novak actually showed some good examples of that yesterday.",
            "And then.",
            "When you know what overfitting is, maybe that can help you with learning algorithm design.",
            "Maybe you can get a better mechanism for pruning decision trees.",
            "Or maybe we can think about why exactly large margins are good.",
            "And.",
            "This is another example sparsity, right?",
            "So if your learning algorithm depends upon only a small subset of the training examples, it turns out that you can use data nice generalization bound.",
            "Alright, so these are the motivations.",
            "And then this is the outline I'm going to go through the basic model that will be."
        ],
        [
            "Sing with.",
            "And then exactly how you use a test set.",
            "With a bound, how do you get a bound on the deviations in a test set?",
            "So as I said, this is the most basic and you'll see the mathematics is very basic here.",
            "And I think this is the simplest.",
            "This is the simplest bound which exhibits what exactly overfitting is.",
            "This Oakland Raider bound.",
            "And then this is maybe a little bit more advances that the PAC Bayes bound, which is slight generalization on the local measure bound.",
            "Which has some nice applications.",
            "OK."
        ],
        [
            "So the model the model is much like you've seen.",
            "You know you have an input space.",
            "You have your binary output space.",
            "Give you classifier.",
            "And then we're going doing this.",
            "This is what's different from.",
            "My previous lecture were making an assumption this assumption.",
            "By the way, I should mention this.",
            "Independence is a real assumption in the sense that you cannot verify that your samples are coming independently.",
            "You can verify that they're not coming independently by saying look, it's 00000.",
            "That doesn't make any sense, it's gotta be.",
            "But you cannot verify independence in an efficient manner.",
            "Do you even think about it?",
            "Verifying independence?",
            "You would need to have a number of samples which is proportional to the size of X.",
            "And that is not going to happen for almost all applications of learning.",
            "So all of our samples for all of this talk come independently from the same distribution.",
            "And what exactly M is that varies from bound to bound, but everything is coming independently."
        ],
        [
            "OK, so.",
            "Since we're only doing classification, and since we're only going to be worrying about error rates.",
            "This is a more compact way of specifying the error rate, right?",
            "So this is the error rate of classifier C on the distribution D. Yeah, that's just familiar thing familiar with this probability of the classifiers wrong for random draws from D. And of course, we're not going to assume that we know D, because that would be assuming that we know the solution.",
            "Instead, we're just going to know some sort of approximation."
        ],
        [
            "In the form of independently drawn samples S. Indeed.",
            "Let me try this technology.",
            "Right so S is infinitely drawn from D. Him times.",
            "An when I say.",
            "Oh, I see there's a normalization issue here.",
            "So there should be an M here 'cause it's M times the probability equals the sum.",
            "Of the indicator functions.",
            "Right, so when I say X&Y drawn from S, I'm just saying uniform distribution over S. So this has a lot of different names depending on how exactly it's used.",
            "This is your proxy for the true error rate, and it's called the training error or the test error, the observed error and exactly what term you use depends upon the context.",
            "Right so.",
            "This is just a note about what that means.",
            "OK, so very basic setup."
        ],
        [
            "And now.",
            "We need to start asking yourself some questions.",
            "So.",
            "It turns out you can exactly specify the distribution of this observation.",
            "When you know the true error rate.",
            "It's a it's a binomial distribution.",
            "So maybe we can actually show you what that means.",
            "So the black there is what a binomial distribution looks like.",
            "And the mean of the binomial distribution.",
            "Is right."
        ],
        [
            "On the red line.",
            "OK. You can see that it kind of gets.",
            "More Gaussian like in the center and it gets more peaked and skewed towards the edges.",
            "Here you can see it better.",
            "So this is the final distribution we're working with.",
            "Is something which.",
            "Near 0 or near 1 becomes peaked.",
            "And which in the middle is approximable by Gaussian pretty well.",
            "OK, so by number distribution?",
            "And then how do we use this observation?",
            "You've already seen the video, so the picture doesn't matter so much."
        ],
        [
            "OK, so the way we use this is by thinking about the cumulative distribution of the binomial right?",
            "So this is the probability.",
            "That it should go away.",
            "The probability of the observation is less than K. Right, so the number of.",
            "The number of heads or the number of errors that you observe for your classifier is bounded by K. And this is going to be the sum of binomial coefficients, summer probability to individual event.",
            "So as far as the pretty picture goes.",
            "See what this means is just.",
            "We're going to be.",
            "Cutting things off and computing the area down here, right?"
        ],
        [
            "So the thing which is kind of rough about these binomials is that you don't actually know what the true error rate is, right since you don't know what the true error rate of your classifier is.",
            "We don't know C sub D. We have to somehow get rid of.",
            "The knowledge of that.",
            "And the way that you get rid of the knowledge of that is.",
            "The technical term is using the pivot of the cumulative.",
            "So in particular, you ask yourself, what is the largest true error rate such that.",
            "The probability of observing K or fewer aheads K or fewer errors is at least Delta.",
            "OK.",
            "So if we go back to.",
            "Here.",
            "If the green line is K. And we have some some very small Delta think it's Delta is 1 / 100 or something like that here.",
            "Then the red line.",
            "Is that maximum?",
            "Right?",
            "So that is the particular P which achieves this maximum.",
            "And we can compute.",
            "As you can see, this is easily computable.",
            "OK, so so that's the basic model, and if you observations about the basic model and now we want to go and we want to actually compute various bow."
        ],
        [
            "Because, you know, it's the business we're in.",
            "So we have our standard technique where we take our data and we cut into the training set the test set training set and testing the test set.",
            "And now."
        ],
        [
            "Maybe instead of reporting the test set error rate, we want to report some sort of confidence interval.",
            "We want to say well in the future we expect that you know our area is not going to be too large and we need to quantify exactly what we mean by too large.",
            "So there's a very simple bound which applies here."
        ],
        [
            "It says that for every classifier, see for every distribution D. For every choice of confidence.",
            "The probability over random draw from the samples or involve the samples you see.",
            "That this pivot of the binomial distribution is greater than the true error rate.",
            "Is greater than one is Delta.",
            "Right, so that that says that.",
            "The particular P. So what you're doing is you're looking for the worst case true error rates that you have a reasonable probability in observation, right?",
            "And that's exactly what the Max is doing, and this is the answer that you get.",
            "So the proof is very easy.",
            "You just say look.",
            "With probability of 1 minus Delta, our observation is not in our tail of size Delta.",
            "And so.",
            "By the definition of this pin bar.",
            "This quantity is greater than the CCD.",
            "OK, so let's go back here and look at this.",
            "So this is doing the computation right?",
            "So we get this observation.",
            "And then we compute our confidence interval in this with the red line is.",
            "And we can just do this easily.",
            "You know, no matter where we are.",
            "There's some very simple things here which I think we should talk about.",
            "Whenever.",
            "People think about bounds of this sort is that there's a very common failure mode.",
            "What they do is they say, well with probability 1 minus Delta and my error rate will be less than.",
            "Then this pin bar in the future.",
            "And that's not the right way to think about it, because it's not what it says.",
            "This says that.",
            "This isn't randomizing over the future, this is randomizing over over the past right?",
            "So in particular, if you have beige and inclinations you tend to think of this as a fixed variable, but this is a random variable for this analysis.",
            "And you have to keep that in mind when you're interpreting a statement.",
            "What this says is not.",
            "OK, what would be false is the probability.",
            "That I have an error rate greater than this is less than that.",
            "That would be wrong.",
            "What would be correct is to say that.",
            "If you use this test set bound.",
            "In the future, on independent samples from multiple times, and the proportion of the times the bound will be wrong is 1 minus Delta.",
            "So it sort of doesn't say something about your individual error rate.",
            "It's easy to actually violate this.",
            "All mentioned insect, but.",
            "With the settings that you have some mechanism which will not fail you very often.",
            "For evaluating bounds.",
            "So if you're the government in your valuating drug companies.",
            "This says that the drug companies cannot cheat too much, right?",
            "'cause you know that they do want to cheat.",
            "Actually, that hasn't cover techniques for cheating.",
            "Let me tell you how you can achieve a little bit right so.",
            "This is the secret dark star.",
            "This is the dark force, yes?",
            "So one way that you can cheat is you can draw twice, right?",
            "You draw 2S is and then you evaluate.",
            "You take the best one right?",
            "That's cheating.",
            "Soda companies do this right.",
            "They run two studies and then they report the results of which everyone turns out the best.",
            "And then they got discovered doing this, and then they got their hand slapped.",
            "So that's one way you can cheat and the bounds is not valid.",
            "If you do that kind of thing.",
            "OK, so.",
            "Hope we're clear on the meaning.",
            "If there any questions and feel free to ask."
        ],
        [
            "So this proof actually has a nice graphical way to think about it.",
            "What you do is, you say I don't know what my distribution is.",
            "It has some true error rate which you can't see, right?",
            "So there's some true error rate here which is inducing this binomial distribution.",
            "There's some tree right here.",
            "Which is inducing the purple binomial distribution is another one for the yellow.",
            "But whatever material rate is, I'm going to exclude the possibility that I'm in the tail, right?",
            "So this is the tail of size Delta.",
            "And then I'm going to get some particular observation.",
            "And then I'm going to."
        ],
        [
            "Throw away all the possibilities which are excluded by.",
            "This is in the tail of that blue binomial, so so just not not allowed, right?",
            "'cause with probability one myself.",
            "So we're just not there.",
            "And then you have a set of possibilities.",
            "There's the purple, and there's a yellow and there's everything in between and over here, and so forth.",
            "So you just go well.",
            "You know what's the?"
        ],
        [
            "One right?",
            "So this is the graphical form of computing that bound.",
            "OK."
        ],
        [
            "So there's some things to note about this.",
            "It's extremely satisfying in some ways.",
            "First of all, there is no chance you'll be able to improve on this.",
            "When you have this particular setting.",
            "You can, in addition to bounding.",
            "The.",
            "And listen to upper bounding deteriorate.",
            "You can also lower bound the error rate using the same technique.",
            "And the use here is sort of.",
            "Verification of learning right?",
            "So this is 1 technique you could go to your boss and say look though rings are not going to feel very often."
        ],
        [
            "Right so.",
            "There are some difficulties with this, which is that you know people don't naturally intuitively know it.",
            "Bin Bar means maybe it helps if you have little pretty pictures.",
            "Animated but.",
            "There are approximations to this pin bar which you will often see if you go and you look at the literature for bounds.",
            "And.",
            "Many of these approximations use a technique which turn off started this exponential moment method.",
            "In particular, he applied it to.",
            "Binomial variables directly.",
            "So.",
            "Right, what does this mean?",
            "So let's start here.",
            "For every classifier C and for every distribution D, you can say that the true error rate.",
            "Is bounded by.",
            "The average training error rate or average test error rate plus this square root log, one over Delta divided by two in.",
            "And that could make some sense to you, because this is basically saying we have.",
            "A Gaussian like quantity, right?",
            "If you go and you look at the pretty picture you see you, it's a Gaussian like quantity, specially in the middle.",
            "So when you're over near the edge, the bound becomes very loose 'cause you can see that the distance between the green and the red.",
            "Is smaller than it is here, right?",
            "But you know, you always lose a bit with approximations.",
            "And then up here is a tighter approximation.",
            "So.",
            "Show you which one that is.",
            "So the yellow.",
            "Which is right there?",
            "Right, so the blue is the square root approximation.",
            "You can see how it becomes very loose over here.",
            "And the yellow is the relative entropy approximation.",
            "So it behaves maybe a little bit better.",
            "It's always a bit.",
            "It's never worse than the square root approximation, but it's still not quite as tight as the actual exact computation.",
            "OK, so So what is this thing here?",
            "So KL divergences?",
            "Who knows?",
            "KL divergent here.",
            "Good good number.",
            "OK, so this is the the KL divergences between.",
            "A coin of that bison coin of that bias.",
            "And.",
            "Which I say.",
            "For those who don't know the KL divergent's.",
            "It has a interpretation in computer science.",
            "The kill diverges between Q&P.",
            "Is the number of extra bits that you need to encode the output of a source.",
            "Which is generating things according to Q but with a code optimized for P. So if you if you have a source and you can the minimum.",
            "Number of bits you can use to encode the source.",
            "It is just the entropy of the source.",
            "But if you have a coaches mismatched, this is the penalty due to the mismatch.",
            "Or Q is the reality and P is which you created the code for.",
            "It's interesting that I guess, that information theory comes up in learning theory."
        ],
        [
            "And you'll see some more of that later.",
            "OK, so here's an example.",
            "We can.",
            "Say suppose Delta is wow.",
            "Shouldn't be shooting myself in the eye.",
            "Suppose Delta is .1, so we want things to be correct with.",
            "You know, we want to bound to hold probably .9.",
            "Suppose we have 100 examples.",
            "And suppose we observe 2 error rates in our test set to two errors in our test set.",
            "Then the square root turn off bound gives us this interval.",
            "So I've actually taken the Delta and I have allocated.",
            ".05 to computing the upper bound in .05, computing the lower bound.",
            "And the exact calculation gives us this.",
            "So I have another little program.",
            "Which we should do right?",
            "So.",
            "Here's your opportunity to break my program.",
            "What is a good bound error rate?",
            "Should be more than 13.",
            "300 Yeah bound error rate of 30 would not be so.",
            "You want 30 examples?",
            "OK, that's too easy for me.",
            "But bound error rate.",
            "What's a good bound error rate?",
            ".2 OK. And now you had some statement about number of examples, but 30 is too easy, right?",
            "Oh no, you can actually get by with less.",
            "Let me plug it in.",
            "And then we have a number of errors, right?",
            "That's the other quantity.",
            "Hi.",
            "The boy we suck up possibility.",
            "How about one is is a compliment of nine right?",
            "So it's the same computation but sort of the other direction or something?",
            "Right so.",
            "There we go.",
            ".27 right so our bound on is .27.",
            "So anybody else have numbers?",
            "They want me to plug in?",
            "We can try the nine, but it won't be so satisfying.",
            "Alright, so.",
            "If there's, if you're computing the lower bound on.",
            "Zero Point 210 nine is equivalent to computing the upper bound on 0.210.",
            "One 'cause you can just one minus the bound.",
            "Right, so I feel like you know, maybe we should do something a bit more impressive.",
            "Or do you know it works?",
            "OK.",
            "So again.",
            "You're sick, mother.",
            "This is the calculation of Ben Bar.",
            "Beyond.",
            "So you would be.",
            "Videos.",
            "So the confidence is .2.",
            "Which means that with probability .8.",
            "The bound will hold.",
            "For future evaluations.",
            "And we're not doing.",
            "We're explicitly not approximating this as a Gaussian.",
            "Instead, we're actually doing the exact calculation for binomial distribution.",
            "So let me show you some about why you want to do that.",
            "No.",
            "Too much.",
            "OK, so let's compare.",
            "So this is a very common approach which I would like to.",
            "Dump cold water on.",
            "So the very common approach is you say, well, you know binomial is kind of like a Gaussian, so."
        ],
        [
            "So what I'm going to do is I'm going to approximate it by Gaussian the computer a confidence interval for a Gaussian, right?",
            "So his cage number of test errors and image image number of examples.",
            "This is your observed mean, and this is your observed variation.",
            "And then you can compute abound, which is maybe this K / N + 2 Sigma, which is gives you.",
            "About a 95% confidence interval for if if this was.",
            "Actually a gaussian.",
            "And the question is, how do these actually compare in practice?",
            "So right we did a few calculations.",
            "So."
        ],
        [
            "Take a bunch of datasets, cut them up in the training set and test set, and then on the test set, go and compute these various bounds.",
            "So let's see what how does this work, the.",
            "For each test set, there are two bars.",
            "The one.",
            "The one on the left.",
            "Is the test set bound so just described?",
            "And the one on the right.",
            "Is the Gaussian approximation, which is just just described.",
            "And.",
            "You can see a few things which are kind of odd.",
            "So over here.",
            "You see that the Gaussian approximation says that your error rate is less than 1.5.",
            "So this is not very satisfying, of course, because it's like saying.",
            "Suppose I flip the coin.",
            "I will get less than 1.5 hits.",
            "Right?",
            "Nothing much happens.",
            "So these datasets.",
            "To.",
            "For this data set, it was very, very tiny, which means that the size of the of the test set was like two or three, right?",
            "And you know, we observed zero errors, right?",
            "So?",
            "If you observe zero errors with this Gaussian approximation, something horrible happens, right?",
            "So if this is 0?",
            "Then this will be 0.",
            "And that means this will be 0.",
            "Right?",
            "So you start to fly the shuttle and it'll blow up because you're overconfident.",
            "But on the other hand.",
            "For the test set down calculation, it takes into account the number of examples in a very explicit way and so.",
            "You know it gives you something much more reasonable, since maybe you want to run a few more experiments.",
            "However.",
            "This data set, which turns out to be very easy.",
            "It's also large, so it's large enough to have a large test set.",
            "And that means that the both bounds the error observed here is 0 again and both bounds are actually very near 101, is very near to 0, which is appropriate because you have a lot of test examples and you did well on each of them.",
            "OK.",
            "So what's happening is?",
            "We're not getting this weirdness.",
            "Where are bounds above 1?",
            "And we're not getting overconfident, so this is maybe the most important part.",
            "So if you want to go and evaluate some sort of binomial confidence interval, I really think you should just use the binomial confidence interval."
        ],
        [
            "It's just saying what I said."
        ],
        [
            "Ah.",
            "So we can think about the process of evaluating these games.",
            "These bounds is a game.",
            "In step one.",
            "A learner chooses a classifier.",
            "An in Step 2, the verifier so that that's the drug company, right that use their drug and then.",
            "In Step 2, the verifier, which is maybe the government, goes and says, oh here's a study we're going to.",
            "We grab some people.",
            "We test the drug on them, and then you know, we see how well it works.",
            "So this is the valid order of operations for this particular bound.",
            "If you get these out of order, things fail.",
            "So in particular, if the drug company draws the examples and evaluates the bound, it does it multiple times before choosing which one to report, then then that's bad.",
            "Alright, so I call this an interactive proof of learning, but it's kind of interactive in the trivial sense.",
            "Don't think it's important here.",
            "Is that the choice of classifier comes before the examples are drawn.",
            "So there's a."
        ],
        [
            "Another slight variation on this, which is very commonly used in machine learning.",
            "It's K fold cross validation.",
            "So what happens in K fold cross validation is you have examples and instead of dividing into a training set and test set, you divide it into K equal sized sets.",
            "So.",
            "And then on each set.",
            "You train on everything but that set.",
            "Then you test on this hell that subset, right?",
            "So each individual run of this is like a train and a test on a set of size M / K. This is not well understood theoretically.",
            "The best result we have is that the confidence interval should be smaller than something.",
            "Then for test set bound of size M / K. In practice, this often works much better.",
            "But there are worst case choices of distribution and learning album such as this bound is actually realized.",
            "So.",
            "What this means for you in practice?",
            "Is that leave one out cross validation where K = M?",
            "You should not trust too much.",
            "It is possible to actually severely overfit easily with leave one out cross validation.",
            "There are some learning algorithms which are stable in some sense.",
            "Where this is proved to work, but for general learning algorithms it's not proved to work.",
            "Anyway, there's a big open problem here.",
            "And I thought a bit about trying to solve this right, so maybe it's possible to state much tighter bounds if you go when you look at.",
            "How much the different classifiers that you learn all the different K folds disagree with each other, right?",
            "So the bad examples are always the ones where they disagree with each other a lot.",
            "Maybe if you can, if you just look at them and you say look.",
            "They agree almost all the time.",
            "You can see the tighter bound.",
            "But it's a very difficult analysis.",
            "I would say don't try to do that unless you feel very overconfident.",
            "Alright, so are the questions about cross validation.",
            "Yeah.",
            "Call transgenes yeah, the the big problem is that things are not in the individual.",
            "Estimates are not independent, they're dependent in tracking those dependencies is very tricky.",
            "Maybe I should mention exactly one way to leave when I cross validation can fail horribly.",
            "So here's an example.",
            "Suppose I have a learning algorithm learning algorithm.",
            "Is the followings it takes a look at the examples and it says if there are an even number of ones I'll predict 0.",
            "And if there are an odd number of ones, I'll predict one all the time, right?",
            "So it's either 0 all the time or one all the time.",
            "Now suppose.",
            "Suppose we have.",
            "A distribution.",
            "With just, you know, pics label with probability .5.",
            "Enable zero probability .5.",
            "Right so then maybe we end up with.",
            "I don't know.",
            "46 heads, right?",
            "So if we hold out a tail.",
            "Window with 46 ones.",
            "So if we hold out a 0.",
            "Then we're going to predict correctly, because it will just the parity will be even.",
            "We hold out a one and will also predict correctly because now the pair of switches in those 45 heads 45 ones.",
            "So we're going to predict the guy that we held out perfectly.",
            "Right, So what that says is that.",
            "Although our error rate is .5.",
            "The observation that will get from leave one out cross validation is.",
            "No jurors.",
            "Yeah.",
            "Why not just use Bootstrap?",
            "Right?",
            "The state of analysis of Bootstrap is still not quite satisfying.",
            "It's not as good As for the test set bound.",
            "I will talk about that later.",
            "OK."
        ],
        [
            "So that's the test set bound.",
            "That's the basics, which I think everybody should know about these things.",
            "That is the practical part, which can be used now.",
            "These next things are hopefully give you some intuition about good learning algorithms and so forth, but they're not directly used that much yet.",
            "I guess Rob Novak.",
            "Yesterday was talking about a variant of the Oakland Raider Band, which is getting pretty near to use."
        ],
        [
            "OK, so the first thing is why?",
            "You will find as you go out in the world for the test head bound works great in practice.",
            "And the question is, why do we expend?",
            "100 papers on trying to do something other than tested found.",
            "It's a good question.",
            "And it actually has some pretty good answers.",
            "One of them is that there exists learning problems where the process of taking the training set, taking a big set coming into training set and test set is very, very bad.",
            "In particular.",
            "Taking out these few examples for the test set.",
            "Can destroy your ability to learn.",
            "And in fact, you can imagine learning problems where taking out just one example destroys your ability to learn so.",
            "So let me tell you the example, we're just taking out one is very bad.",
            "So the gamble is.",
            "Learning a parity of a subset of the bits right so the target the label is a parity of a subset of bits that you have a set of bits for your input, right?",
            "The way to learn this when you have no noise is to use Gaussian elimination.",
            "So you just think of your examples.",
            "Is forming a matrix and you use Gaussian elimination to figure out which bits are the critical bits.",
            "And if you have a number of examples which is less than the number of features you lose.",
            "Your error rate is .5.",
            "If you have any number of examples which is equal to the number of features in their in their independent.",
            "Which they are with probability .5 or so.",
            "Then you win.",
            "You get it perfectly.",
            "Now this is kind of an extreme example and you may not believe to have parity in the real world.",
            "But it is observed that there are phase transitions and how well you can learn is a function of the number of examples on a lot of natural learning problems.",
            "So this is a real point.",
            "The other reason why is because you want to use these bounds to really guide how you learn.",
            "What classifier do you choose?",
            "You can't do that very well with training with test set bounds because.",
            "The process of guiding learning implies evaluating a lot of different classifiers.",
            "And setting aside separate a separate pool of examples for each classifier you evaluate.",
            "Is the kind of thing which will result in very bad judgments, because you'll take all your information your fraction into very small sets and just not be able to predict very well in the end.",
            "So this is what motivates training set bounds.",
            "So training set bounds are bounds where.",
            "We just have one big training set.",
            "We learn the training set.",
            "And then we compute a bound.",
            "For what we learned.",
            "There's no cutting anymore."
        ],
        [
            "So this is the interactive proof of learning associated with the Oaklands Razor bound protocol.",
            "What happens is your learner.",
            "First uses some.",
            "Prior over classifiers.",
            "So this is a prior in quotes 'cause it's not a Bayesian priors, just some particular normalized measure over classifiers.",
            "So if you were beige and then you would be expecting distribution over distributions, is the distribution over classifiers?",
            "Right, so we commit to a distribution over classifiers.",
            "And then we draw training examples.",
            "The verifier draws them, gives them to the learner learner, chooses some classifier.",
            "And then the bound is evaluated.",
            "So the choice of classifier could be dependent upon on how the band will be evaluated.",
            "This is just a deterministic process at this point, right?",
            "So for the Oakland Raiders, OK, just sort of reuse the examples over and over again in your choice of classifier."
        ],
        [
            "OK, so the Oakland Razor bound says.",
            "For every choice of these priors over classifiers.",
            "For every.",
            "For every distribution.",
            "For every choice of confidence.",
            "The probability.",
            "But for every classifier.",
            "The true error rate is bounded by this quantity.",
            "Is high 1 minus Delta?",
            "So.",
            "Call Larry so.",
            "The only difference from the test set bound.",
            "Is it Delta is going to Delta times Pfc.",
            "That's it, that's the only difference mathematically.",
            "So I think yesterday Rob Novak was working with with this.",
            "Kind of result.",
            "Which is just taking this and using the turn off approximation.",
            "So he had a description links which you can think of a description links as.",
            "Is also a major over classifiers.",
            "Using this crafts inequality.",
            "So you can say the probability of a classifier according to the prior is 2 to the minus description links.",
            "And if you, as long as you have.",
            "A good way of describing things that's a valid choice."
        ],
        [
            "So this proof is slightly more complicated than for the test set bound, but it's only one page, it's very easy.",
            "First of all, you start with the test set bound.",
            "And.",
            "Instead of.",
            "Plugging in Delta, you plug in Delta times Pfc, 'cause this is valid valid for all for all Delta, right so?",
            "This is the Delta Times Pfc is the Delta in the test set bound, which means we have a Delta times Pfc there."
        ],
        [
            "Then we just negate this.",
            "So it's just a matter of.",
            "Working with what negation means.",
            "So this is something happens with high probability and now something happens with low probability with the opposite happens with low probability."
        ],
        [
            "Then we apply the Union bound.",
            "The union bound says that the probability of A or B.",
            "Is less than the probability plus probability of B. Whoa.",
            "Deboarding you to fix that?",
            "So.",
            "Little bit too energetic for my cybernetic enhancement.",
            "Say again.",
            "Yeah, it could be.",
            "I'm just trying to distract you.",
            "Yeah yeah, yeah.",
            "The mathematics strikes back.",
            "Right so.",
            "We use this union bound over and over again that gives us a sum.",
            "Of these Delta times PFC's.",
            "And Pfc, the sum of the pieces.",
            "One or or something less than one.",
            "Which case we can just.",
            "Put in Delta instead.",
            "And then of course you do."
        ],
        [
            "8 again.",
            "Right and then it gives you the.",
            "The countries are bound.",
            "OK, so.",
            "In pictures."
        ],
        [
            "What this looks like is you have a bunch of different classifiers.",
            "And they each have a different size cut in their tail, right?",
            "So?",
            "The size of the cut is Delta times Pfc.",
            "So it varies from classifier the classifier.",
            "So this classifier has a very small Pfc.",
            "In this classifier has larger Pfc, just the area equals Delta times Pfc.",
            "And once again, you don't know what the actual area classifiers are.",
            "So we're just going to go through the same analysis that we went through for the test set out.",
            "So."
        ],
        [
            "Our learner chooses some classifier.",
            "That classifier has a specified size for Delta times Pfc.",
            "That's what the red is.",
            "These areas are supposed to be the same.",
            "And then."
        ],
        [
            "You have, you don't know what the true error rate is, so you just.",
            "Choose the worst one right there worst one consistent with not being in the tail.",
            "OK, so let's go back a little bit.",
            "So the key difference aside from this here.",
            "Is we have for all C inside of the probability.",
            "That means.",
            "This holds for all.",
            "See, it holds for whichever.",
            "See you're learning and happens to choose.",
            "That means you can evaluate.",
            "You can use this bound over and over again in the process of learning itself.",
            "So I think Rob talked about overfitting yesterday, but the way that overfitting is expressed here is if you have a very complex classifier, this PC is going to get very small and that means you're bound is going to get very bad because those times Pfc is going to get very very small.",
            "So this gives you some way of trading off between the two possibilities.",
            "Right example.",
            "So everything is the same as before except now our Pfc is .1."
        ],
        [
            "And now we can go and we can compute the square root turn off approximation, which is this.",
            "Confidence interval.",
            "Just as an unsatisfying we will observe.",
            "No more than.",
            "How do you think about that?",
            "Well, anyway, you shouldn't have.",
            "A number of heads which is minus .143.",
            "And if you do the exact calculation, you just get something.",
            "A bit tighter, which is always within the interval from zero to 1.",
            "Alright, so.",
            "Once again, we can go and we can actually apply."
        ],
        [
            "As to various.",
            "Lord albums on various datasets and particularly algorithm will be some sort of decision tree.",
            "The probability of failure will be set to Delta.",
            "All the problems are sort of discrete problems from this UCI database and machine learning problems.",
            "And then there's two things are going to compare.",
            "One of them is the train and test approach.",
            "And the other one is the train and everything approach, right?",
            "So the training test approach is just going to look at the test set bound.",
            "And on the train, and everything approached, I'm just going to use this Oakland razor bound.",
            "One other caveat, which is very important.",
            "Another horrible, horrible way to cheat, which is very hard to catch.",
            "Is.",
            "In your choice of datasets, right?",
            "So I will tell you that I chose datasets before I evaluated everything and I'm reporting all all of the results.",
            "You should do that too."
        ],
        [
            "Alright, so.",
            "This is the results.",
            "So you have a test set bound which is the left here.",
            "And you have.",
            "A optimization bound which is the right.",
            "And the code which is being used here is something very like the first code which Rob talked about yesterday.",
            "So there was a question yesterday about how the constants work out right.",
            "And this direct comparison can maybe tell you something about how the constants actually workout.",
            "What you see is that the Oaklands razor bound is typically.",
            "A little bit looser, right?",
            "So forth.",
            "It's not a lot looser.",
            "It turns out that it's quite a bit looser here.",
            "This is a pretty large data set in terms of number of examples, so there's maybe a factor of 10 there.",
            "Nothing.",
            "Of course, keep in mind is that the training set.",
            "Is about five times larger than the test set here.",
            "Sometimes the Circumciser bound is actually winning.",
            "So over here it wins.",
            "These are not very impressive confidence intervals.",
            "What's happening is the number of examples in the test set is small.",
            "And then it becomes very important that we actually have.",
            "The five times more examples in the training set, even though.",
            "Multiplying by Pfc makes things a bit worse.",
            "Yeah.",
            "Choice measure over here.",
            "Passengers to choice of 1.",
            "You're actually doing model selection, right?",
            "Basically why you're putting on a prior probability of some sort of everything that could possibly be.",
            "I wanted to do that now with my just made up.",
            "How many possible things would be?",
            "So I'm just wondering how sensitive all of this.",
            "Right, so the question is, how sensitive is it to sort of your choice of learning algorithm?",
            "Right?",
            "Break.",
            "Possible.",
            "$1,000,000.",
            "The Kia they should.",
            "Right, so the question is, how do you define Pfc?",
            "And the answer so far is you do some soul searching.",
            "Right, how sensitive are the results to the actual choice of Pfc?",
            "And the answer is it can be quite sensitive at times.",
            "So in particular, if your choice of P of C was just a uniform distribution over all classifiers.",
            "That would be very bad.",
            "It's important that your choice of Pfc be sort of well aligned with your algorithm.",
            "If you want these bounds to be tight.",
            "So in particular, the pruning process here shows the classifier which minimized.",
            "The bound according to this particular choice of Pfc.",
            "R. So minimized amongst the prunings of the tree that was grown.",
            "I wish I had a good answer for you.",
            "Think hard is not a really great answer when you want something to actually workout.",
            "I think it's an honest evaluation of the state of things right now.",
            "I came here.",
            "Impossible.",
            "Right, right?",
            "So if you are doing some sort of continuous set of classifiers, then Pfc will be 0 and then of course the Oakland Raider Band is not going to tell you anything useful.",
            "So one thing to understand, these training set bounds is that these are bounds, they're not.",
            "Let me see this another way.",
            "You could take the same learning algorithm.",
            "You could plug in the Pfc which is uniform over all possible decision trees.",
            "The band would be horrible, but your performance would be exactly the same as here because it's the same learning algorithm, right?",
            "So just because the bound says you have a horrible classifier does not mean that you have a horrible classifier.",
            "And that's something to always understand about bounds.",
            "So this particular bound will not apply in a strong way when when your PC is over continuous set.",
            "Let me tell you two things.",
            "First of all, the PAC Bayes bound will apply there and second of all there's these VC bounds, so I will not be talking about that.",
            "Apply there to some extent.",
            "You start to get into issues of it's very difficult to avoid issues of how tight is the bound we start working with continuous sets of classifiers.",
            "Yeah.",
            "This phone 'cause we are working on the fire system later.",
            "We constrain these mountains working only with my sisters.",
            "It is almost like though.",
            "It's a date.",
            "Go to play.",
            "Just trying to.",
            "Yeah, every argument for a unbiased estimator of this particular boundary by system.",
            "Right, so in statistics there's this notion of an unbiased estimator, right?",
            "We do not advise that these are pessimistic, right?",
            "So these bounds are are supposed to always be a bit worse.",
            "This test set bound maybe you can prove that you can't do anything better.",
            "But for the training set bounds, you can often be the case that a different training set bound will actually do much better.",
            "Right, so this is this example of the uniform distribution.",
            "Overall, classifiers versus some reasonable description language.",
            "The distribution induced by some reasonable description language.",
            "OK.",
            "I think it's time for a break.",
            "Let's take a break for about 10 minutes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's some caveats I should tell you for today's lecture.",
                    "label": 0
                },
                {
                    "sent": "One caveat is that I was really not kidding in the introduction when I said that there are hundreds of papers on this subject.",
                    "label": 0
                },
                {
                    "sent": "This is only a very small piece.",
                    "label": 0
                },
                {
                    "sent": "It is the piece that I consider most basic and perhaps even most useful.",
                    "label": 0
                },
                {
                    "sent": "But there's a great deal of other work which I'm not going to cover, and which could easily be useful for various tasks.",
                    "label": 0
                },
                {
                    "sent": "So once again, if you have questions during the talk, feel free to to ask them then.",
                    "label": 0
                },
                {
                    "sent": "Oh donaj, you made it?",
                    "label": 0
                },
                {
                    "sent": "We were all kinds of organized for this lecture.",
                    "label": 0
                },
                {
                    "sent": "Donaji, actually.",
                    "label": 0
                },
                {
                    "sent": "Printed out and copied slides.",
                    "label": 0
                },
                {
                    "sent": "So not only are they on the web, there also in your hand shortly.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What are generalization bounds?",
                    "label": 0
                },
                {
                    "sent": "So first of all, we need to start out what learning is and a reasonable understanding about weren't what learning is.",
                    "label": 0
                },
                {
                    "sent": "It's acquiring prediction ability.",
                    "label": 0
                },
                {
                    "sent": "So the process of learning is the process of acquiring your ability to predict in the future.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you think about this a bit, you realize that you know in the worst case you can't do this.",
                    "label": 0
                },
                {
                    "sent": "You could have an adversary that knows the process that you're using to learn and just chooses to screw you every time.",
                    "label": 0
                },
                {
                    "sent": "But you know, maybe in the real world you don't always have an adversary.",
                    "label": 0
                },
                {
                    "sent": "And then the question is.",
                    "label": 0
                },
                {
                    "sent": "How can we even analyze this?",
                    "label": 0
                },
                {
                    "sent": "And one very common way is you assume that you have independent examples that are each coming from the same distribution over and over again, right?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And then it becomes possible to actually predict because.",
                    "label": 0
                },
                {
                    "sent": "Doing well in the past means that you have some some reasonable expectation of doing well in the future, so the mathematics I'm talking about today is how doing well in the past gives you some reasonable expectation about how you do in the future.",
                    "label": 0
                },
                {
                    "sent": "So the thing to understand about this is that it is significantly different from my other lecture, and that we're assuming independent examples independent and identically distributed examples in fact.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is something that.",
                    "label": 0
                },
                {
                    "sent": "I think is kind of critical to properly understanding the mathematics.",
                    "label": 0
                },
                {
                    "sent": "You have to really watch the assumptions right.",
                    "label": 0
                },
                {
                    "sent": "This is an assumption of independence and what that means.",
                    "label": 0
                },
                {
                    "sent": "This applies when your examples are independent.",
                    "label": 0
                },
                {
                    "sent": "In practice, often your cables will not be independent.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they will be, or at least close enough to where we can use the same mass.",
                    "label": 0
                },
                {
                    "sent": "So this is something with a more limited scope and we talked about last time, but it's answering a question that cannot be answered without making some assumptions about how the past relates to the future.",
                    "label": 0
                },
                {
                    "sent": "Independence is the standard way of doing that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, so the question is is first of all at the same as the no free lunch theorem?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So no free lunch theorem is kind of this stated in the basean way.",
                    "label": 0
                },
                {
                    "sent": "If you just think about it in an entirely adversarial way to trivial statement.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why do we want to study this?",
                    "label": 0
                },
                {
                    "sent": "So maybe if we study this then we get better methods for learning and actually for verifying our predictive ability, right so?",
                    "label": 0
                },
                {
                    "sent": "Maybe if you're working on the space shuttle or something.",
                    "label": 0
                },
                {
                    "sent": "Boss wants to know that that things you predict are actually going to work pretty well in the future.",
                    "label": 0
                },
                {
                    "sent": "So maybe you want to actually have some technique for coming up to him saying here's a certificate which says I have successfully learned to predict when O rings will fail.",
                    "label": 0
                },
                {
                    "sent": "It may be one that certificate to be, you know, pretty darn ironclad.",
                    "label": 0
                },
                {
                    "sent": "So the things we're talking about today give you some mechanism for doing that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Better methods of learning.",
                    "label": 0
                },
                {
                    "sent": "So when people are faced with this problem right now.",
                    "label": 0
                },
                {
                    "sent": "Well, they typically do is they take if they have a bunch of examples in front of them.",
                    "label": 0
                },
                {
                    "sent": "They take, you know, the first 2/3 or whatever, or the 1st, all but 100 and they train on that and the test on the last 1/3 or last 100 or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So we divide things into a training set, test set which are in the training set testing test set.",
                    "label": 0
                },
                {
                    "sent": "And then the hope is that maybe maybe we can actually do better.",
                    "label": 0
                },
                {
                    "sent": "There are other techniques we'll be talking about today for giving you a proof of the same.",
                    "label": 0
                },
                {
                    "sent": "Degree of validity.",
                    "label": 0
                },
                {
                    "sent": "As I observed, a good test error rate.",
                    "label": 1
                },
                {
                    "sent": "And actually, a better degree of validity in many ways.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing that you get when you look at this is you.",
                    "label": 0
                },
                {
                    "sent": "You learn a little bit about how learning algorithms should be designed.",
                    "label": 0
                },
                {
                    "sent": "So in particular we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at these bounds.",
                    "label": 0
                },
                {
                    "sent": "We can go look there is overfitting.",
                    "label": 0
                },
                {
                    "sent": "So Rob Novak actually showed some good examples of that yesterday.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "When you know what overfitting is, maybe that can help you with learning algorithm design.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can get a better mechanism for pruning decision trees.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we can think about why exactly large margins are good.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is another example sparsity, right?",
                    "label": 0
                },
                {
                    "sent": "So if your learning algorithm depends upon only a small subset of the training examples, it turns out that you can use data nice generalization bound.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these are the motivations.",
                    "label": 0
                },
                {
                    "sent": "And then this is the outline I'm going to go through the basic model that will be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing with.",
                    "label": 0
                },
                {
                    "sent": "And then exactly how you use a test set.",
                    "label": 0
                },
                {
                    "sent": "With a bound, how do you get a bound on the deviations in a test set?",
                    "label": 0
                },
                {
                    "sent": "So as I said, this is the most basic and you'll see the mathematics is very basic here.",
                    "label": 0
                },
                {
                    "sent": "And I think this is the simplest.",
                    "label": 0
                },
                {
                    "sent": "This is the simplest bound which exhibits what exactly overfitting is.",
                    "label": 0
                },
                {
                    "sent": "This Oakland Raider bound.",
                    "label": 0
                },
                {
                    "sent": "And then this is maybe a little bit more advances that the PAC Bayes bound, which is slight generalization on the local measure bound.",
                    "label": 0
                },
                {
                    "sent": "Which has some nice applications.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the model the model is much like you've seen.",
                    "label": 0
                },
                {
                    "sent": "You know you have an input space.",
                    "label": 0
                },
                {
                    "sent": "You have your binary output space.",
                    "label": 0
                },
                {
                    "sent": "Give you classifier.",
                    "label": 0
                },
                {
                    "sent": "And then we're going doing this.",
                    "label": 0
                },
                {
                    "sent": "This is what's different from.",
                    "label": 0
                },
                {
                    "sent": "My previous lecture were making an assumption this assumption.",
                    "label": 0
                },
                {
                    "sent": "By the way, I should mention this.",
                    "label": 0
                },
                {
                    "sent": "Independence is a real assumption in the sense that you cannot verify that your samples are coming independently.",
                    "label": 0
                },
                {
                    "sent": "You can verify that they're not coming independently by saying look, it's 00000.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make any sense, it's gotta be.",
                    "label": 0
                },
                {
                    "sent": "But you cannot verify independence in an efficient manner.",
                    "label": 0
                },
                {
                    "sent": "Do you even think about it?",
                    "label": 0
                },
                {
                    "sent": "Verifying independence?",
                    "label": 0
                },
                {
                    "sent": "You would need to have a number of samples which is proportional to the size of X.",
                    "label": 0
                },
                {
                    "sent": "And that is not going to happen for almost all applications of learning.",
                    "label": 0
                },
                {
                    "sent": "So all of our samples for all of this talk come independently from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "And what exactly M is that varies from bound to bound, but everything is coming independently.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Since we're only doing classification, and since we're only going to be worrying about error rates.",
                    "label": 0
                },
                {
                    "sent": "This is a more compact way of specifying the error rate, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the error rate of classifier C on the distribution D. Yeah, that's just familiar thing familiar with this probability of the classifiers wrong for random draws from D. And of course, we're not going to assume that we know D, because that would be assuming that we know the solution.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're just going to know some sort of approximation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the form of independently drawn samples S. Indeed.",
                    "label": 0
                },
                {
                    "sent": "Let me try this technology.",
                    "label": 0
                },
                {
                    "sent": "Right so S is infinitely drawn from D. Him times.",
                    "label": 0
                },
                {
                    "sent": "An when I say.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see there's a normalization issue here.",
                    "label": 0
                },
                {
                    "sent": "So there should be an M here 'cause it's M times the probability equals the sum.",
                    "label": 0
                },
                {
                    "sent": "Of the indicator functions.",
                    "label": 0
                },
                {
                    "sent": "Right, so when I say X&Y drawn from S, I'm just saying uniform distribution over S. So this has a lot of different names depending on how exactly it's used.",
                    "label": 0
                },
                {
                    "sent": "This is your proxy for the true error rate, and it's called the training error or the test error, the observed error and exactly what term you use depends upon the context.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "This is just a note about what that means.",
                    "label": 0
                },
                {
                    "sent": "OK, so very basic setup.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "We need to start asking yourself some questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It turns out you can exactly specify the distribution of this observation.",
                    "label": 0
                },
                {
                    "sent": "When you know the true error rate.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "So maybe we can actually show you what that means.",
                    "label": 0
                },
                {
                    "sent": "So the black there is what a binomial distribution looks like.",
                    "label": 0
                },
                {
                    "sent": "And the mean of the binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "Is right.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the red line.",
                    "label": 0
                },
                {
                    "sent": "OK. You can see that it kind of gets.",
                    "label": 0
                },
                {
                    "sent": "More Gaussian like in the center and it gets more peaked and skewed towards the edges.",
                    "label": 0
                },
                {
                    "sent": "Here you can see it better.",
                    "label": 0
                },
                {
                    "sent": "So this is the final distribution we're working with.",
                    "label": 0
                },
                {
                    "sent": "Is something which.",
                    "label": 0
                },
                {
                    "sent": "Near 0 or near 1 becomes peaked.",
                    "label": 0
                },
                {
                    "sent": "And which in the middle is approximable by Gaussian pretty well.",
                    "label": 0
                },
                {
                    "sent": "OK, so by number distribution?",
                    "label": 0
                },
                {
                    "sent": "And then how do we use this observation?",
                    "label": 0
                },
                {
                    "sent": "You've already seen the video, so the picture doesn't matter so much.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the way we use this is by thinking about the cumulative distribution of the binomial right?",
                    "label": 0
                },
                {
                    "sent": "So this is the probability.",
                    "label": 0
                },
                {
                    "sent": "That it should go away.",
                    "label": 0
                },
                {
                    "sent": "The probability of the observation is less than K. Right, so the number of.",
                    "label": 0
                },
                {
                    "sent": "The number of heads or the number of errors that you observe for your classifier is bounded by K. And this is going to be the sum of binomial coefficients, summer probability to individual event.",
                    "label": 0
                },
                {
                    "sent": "So as far as the pretty picture goes.",
                    "label": 0
                },
                {
                    "sent": "See what this means is just.",
                    "label": 0
                },
                {
                    "sent": "We're going to be.",
                    "label": 0
                },
                {
                    "sent": "Cutting things off and computing the area down here, right?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the thing which is kind of rough about these binomials is that you don't actually know what the true error rate is, right since you don't know what the true error rate of your classifier is.",
                    "label": 0
                },
                {
                    "sent": "We don't know C sub D. We have to somehow get rid of.",
                    "label": 0
                },
                {
                    "sent": "The knowledge of that.",
                    "label": 0
                },
                {
                    "sent": "And the way that you get rid of the knowledge of that is.",
                    "label": 0
                },
                {
                    "sent": "The technical term is using the pivot of the cumulative.",
                    "label": 0
                },
                {
                    "sent": "So in particular, you ask yourself, what is the largest true error rate such that.",
                    "label": 0
                },
                {
                    "sent": "The probability of observing K or fewer aheads K or fewer errors is at least Delta.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if we go back to.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "If the green line is K. And we have some some very small Delta think it's Delta is 1 / 100 or something like that here.",
                    "label": 0
                },
                {
                    "sent": "Then the red line.",
                    "label": 0
                },
                {
                    "sent": "Is that maximum?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that is the particular P which achieves this maximum.",
                    "label": 0
                },
                {
                    "sent": "And we can compute.",
                    "label": 0
                },
                {
                    "sent": "As you can see, this is easily computable.",
                    "label": 0
                },
                {
                    "sent": "OK, so so that's the basic model, and if you observations about the basic model and now we want to go and we want to actually compute various bow.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because, you know, it's the business we're in.",
                    "label": 0
                },
                {
                    "sent": "So we have our standard technique where we take our data and we cut into the training set the test set training set and testing the test set.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe instead of reporting the test set error rate, we want to report some sort of confidence interval.",
                    "label": 0
                },
                {
                    "sent": "We want to say well in the future we expect that you know our area is not going to be too large and we need to quantify exactly what we mean by too large.",
                    "label": 0
                },
                {
                    "sent": "So there's a very simple bound which applies here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It says that for every classifier, see for every distribution D. For every choice of confidence.",
                    "label": 0
                },
                {
                    "sent": "The probability over random draw from the samples or involve the samples you see.",
                    "label": 0
                },
                {
                    "sent": "That this pivot of the binomial distribution is greater than the true error rate.",
                    "label": 0
                },
                {
                    "sent": "Is greater than one is Delta.",
                    "label": 0
                },
                {
                    "sent": "Right, so that that says that.",
                    "label": 0
                },
                {
                    "sent": "The particular P. So what you're doing is you're looking for the worst case true error rates that you have a reasonable probability in observation, right?",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what the Max is doing, and this is the answer that you get.",
                    "label": 0
                },
                {
                    "sent": "So the proof is very easy.",
                    "label": 0
                },
                {
                    "sent": "You just say look.",
                    "label": 0
                },
                {
                    "sent": "With probability of 1 minus Delta, our observation is not in our tail of size Delta.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "By the definition of this pin bar.",
                    "label": 0
                },
                {
                    "sent": "This quantity is greater than the CCD.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go back here and look at this.",
                    "label": 0
                },
                {
                    "sent": "So this is doing the computation right?",
                    "label": 0
                },
                {
                    "sent": "So we get this observation.",
                    "label": 0
                },
                {
                    "sent": "And then we compute our confidence interval in this with the red line is.",
                    "label": 0
                },
                {
                    "sent": "And we can just do this easily.",
                    "label": 0
                },
                {
                    "sent": "You know, no matter where we are.",
                    "label": 0
                },
                {
                    "sent": "There's some very simple things here which I think we should talk about.",
                    "label": 0
                },
                {
                    "sent": "Whenever.",
                    "label": 0
                },
                {
                    "sent": "People think about bounds of this sort is that there's a very common failure mode.",
                    "label": 0
                },
                {
                    "sent": "What they do is they say, well with probability 1 minus Delta and my error rate will be less than.",
                    "label": 0
                },
                {
                    "sent": "Then this pin bar in the future.",
                    "label": 0
                },
                {
                    "sent": "And that's not the right way to think about it, because it's not what it says.",
                    "label": 0
                },
                {
                    "sent": "This says that.",
                    "label": 0
                },
                {
                    "sent": "This isn't randomizing over the future, this is randomizing over over the past right?",
                    "label": 0
                },
                {
                    "sent": "So in particular, if you have beige and inclinations you tend to think of this as a fixed variable, but this is a random variable for this analysis.",
                    "label": 0
                },
                {
                    "sent": "And you have to keep that in mind when you're interpreting a statement.",
                    "label": 0
                },
                {
                    "sent": "What this says is not.",
                    "label": 0
                },
                {
                    "sent": "OK, what would be false is the probability.",
                    "label": 0
                },
                {
                    "sent": "That I have an error rate greater than this is less than that.",
                    "label": 0
                },
                {
                    "sent": "That would be wrong.",
                    "label": 0
                },
                {
                    "sent": "What would be correct is to say that.",
                    "label": 0
                },
                {
                    "sent": "If you use this test set bound.",
                    "label": 0
                },
                {
                    "sent": "In the future, on independent samples from multiple times, and the proportion of the times the bound will be wrong is 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "So it sort of doesn't say something about your individual error rate.",
                    "label": 0
                },
                {
                    "sent": "It's easy to actually violate this.",
                    "label": 0
                },
                {
                    "sent": "All mentioned insect, but.",
                    "label": 0
                },
                {
                    "sent": "With the settings that you have some mechanism which will not fail you very often.",
                    "label": 0
                },
                {
                    "sent": "For evaluating bounds.",
                    "label": 0
                },
                {
                    "sent": "So if you're the government in your valuating drug companies.",
                    "label": 0
                },
                {
                    "sent": "This says that the drug companies cannot cheat too much, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you know that they do want to cheat.",
                    "label": 0
                },
                {
                    "sent": "Actually, that hasn't cover techniques for cheating.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you how you can achieve a little bit right so.",
                    "label": 0
                },
                {
                    "sent": "This is the secret dark star.",
                    "label": 0
                },
                {
                    "sent": "This is the dark force, yes?",
                    "label": 0
                },
                {
                    "sent": "So one way that you can cheat is you can draw twice, right?",
                    "label": 0
                },
                {
                    "sent": "You draw 2S is and then you evaluate.",
                    "label": 0
                },
                {
                    "sent": "You take the best one right?",
                    "label": 0
                },
                {
                    "sent": "That's cheating.",
                    "label": 0
                },
                {
                    "sent": "Soda companies do this right.",
                    "label": 0
                },
                {
                    "sent": "They run two studies and then they report the results of which everyone turns out the best.",
                    "label": 0
                },
                {
                    "sent": "And then they got discovered doing this, and then they got their hand slapped.",
                    "label": 0
                },
                {
                    "sent": "So that's one way you can cheat and the bounds is not valid.",
                    "label": 0
                },
                {
                    "sent": "If you do that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Hope we're clear on the meaning.",
                    "label": 0
                },
                {
                    "sent": "If there any questions and feel free to ask.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this proof actually has a nice graphical way to think about it.",
                    "label": 0
                },
                {
                    "sent": "What you do is, you say I don't know what my distribution is.",
                    "label": 0
                },
                {
                    "sent": "It has some true error rate which you can't see, right?",
                    "label": 1
                },
                {
                    "sent": "So there's some true error rate here which is inducing this binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "There's some tree right here.",
                    "label": 0
                },
                {
                    "sent": "Which is inducing the purple binomial distribution is another one for the yellow.",
                    "label": 0
                },
                {
                    "sent": "But whatever material rate is, I'm going to exclude the possibility that I'm in the tail, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the tail of size Delta.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to get some particular observation.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Throw away all the possibilities which are excluded by.",
                    "label": 0
                },
                {
                    "sent": "This is in the tail of that blue binomial, so so just not not allowed, right?",
                    "label": 0
                },
                {
                    "sent": "'cause with probability one myself.",
                    "label": 0
                },
                {
                    "sent": "So we're just not there.",
                    "label": 0
                },
                {
                    "sent": "And then you have a set of possibilities.",
                    "label": 0
                },
                {
                    "sent": "There's the purple, and there's a yellow and there's everything in between and over here, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you just go well.",
                    "label": 0
                },
                {
                    "sent": "You know what's the?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One right?",
                    "label": 0
                },
                {
                    "sent": "So this is the graphical form of computing that bound.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's some things to note about this.",
                    "label": 0
                },
                {
                    "sent": "It's extremely satisfying in some ways.",
                    "label": 0
                },
                {
                    "sent": "First of all, there is no chance you'll be able to improve on this.",
                    "label": 0
                },
                {
                    "sent": "When you have this particular setting.",
                    "label": 0
                },
                {
                    "sent": "You can, in addition to bounding.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "And listen to upper bounding deteriorate.",
                    "label": 0
                },
                {
                    "sent": "You can also lower bound the error rate using the same technique.",
                    "label": 0
                },
                {
                    "sent": "And the use here is sort of.",
                    "label": 0
                },
                {
                    "sent": "Verification of learning right?",
                    "label": 0
                },
                {
                    "sent": "So this is 1 technique you could go to your boss and say look though rings are not going to feel very often.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "There are some difficulties with this, which is that you know people don't naturally intuitively know it.",
                    "label": 0
                },
                {
                    "sent": "Bin Bar means maybe it helps if you have little pretty pictures.",
                    "label": 0
                },
                {
                    "sent": "Animated but.",
                    "label": 0
                },
                {
                    "sent": "There are approximations to this pin bar which you will often see if you go and you look at the literature for bounds.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Many of these approximations use a technique which turn off started this exponential moment method.",
                    "label": 0
                },
                {
                    "sent": "In particular, he applied it to.",
                    "label": 0
                },
                {
                    "sent": "Binomial variables directly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, what does this mean?",
                    "label": 0
                },
                {
                    "sent": "So let's start here.",
                    "label": 0
                },
                {
                    "sent": "For every classifier C and for every distribution D, you can say that the true error rate.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by.",
                    "label": 0
                },
                {
                    "sent": "The average training error rate or average test error rate plus this square root log, one over Delta divided by two in.",
                    "label": 0
                },
                {
                    "sent": "And that could make some sense to you, because this is basically saying we have.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian like quantity, right?",
                    "label": 0
                },
                {
                    "sent": "If you go and you look at the pretty picture you see you, it's a Gaussian like quantity, specially in the middle.",
                    "label": 0
                },
                {
                    "sent": "So when you're over near the edge, the bound becomes very loose 'cause you can see that the distance between the green and the red.",
                    "label": 0
                },
                {
                    "sent": "Is smaller than it is here, right?",
                    "label": 0
                },
                {
                    "sent": "But you know, you always lose a bit with approximations.",
                    "label": 0
                },
                {
                    "sent": "And then up here is a tighter approximation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Show you which one that is.",
                    "label": 0
                },
                {
                    "sent": "So the yellow.",
                    "label": 0
                },
                {
                    "sent": "Which is right there?",
                    "label": 0
                },
                {
                    "sent": "Right, so the blue is the square root approximation.",
                    "label": 0
                },
                {
                    "sent": "You can see how it becomes very loose over here.",
                    "label": 0
                },
                {
                    "sent": "And the yellow is the relative entropy approximation.",
                    "label": 0
                },
                {
                    "sent": "So it behaves maybe a little bit better.",
                    "label": 0
                },
                {
                    "sent": "It's always a bit.",
                    "label": 0
                },
                {
                    "sent": "It's never worse than the square root approximation, but it's still not quite as tight as the actual exact computation.",
                    "label": 0
                },
                {
                    "sent": "OK, so So what is this thing here?",
                    "label": 0
                },
                {
                    "sent": "So KL divergences?",
                    "label": 0
                },
                {
                    "sent": "Who knows?",
                    "label": 0
                },
                {
                    "sent": "KL divergent here.",
                    "label": 0
                },
                {
                    "sent": "Good good number.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the the KL divergences between.",
                    "label": 0
                },
                {
                    "sent": "A coin of that bison coin of that bias.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Which I say.",
                    "label": 0
                },
                {
                    "sent": "For those who don't know the KL divergent's.",
                    "label": 0
                },
                {
                    "sent": "It has a interpretation in computer science.",
                    "label": 0
                },
                {
                    "sent": "The kill diverges between Q&P.",
                    "label": 0
                },
                {
                    "sent": "Is the number of extra bits that you need to encode the output of a source.",
                    "label": 0
                },
                {
                    "sent": "Which is generating things according to Q but with a code optimized for P. So if you if you have a source and you can the minimum.",
                    "label": 0
                },
                {
                    "sent": "Number of bits you can use to encode the source.",
                    "label": 0
                },
                {
                    "sent": "It is just the entropy of the source.",
                    "label": 0
                },
                {
                    "sent": "But if you have a coaches mismatched, this is the penalty due to the mismatch.",
                    "label": 0
                },
                {
                    "sent": "Or Q is the reality and P is which you created the code for.",
                    "label": 0
                },
                {
                    "sent": "It's interesting that I guess, that information theory comes up in learning theory.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you'll see some more of that later.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an example.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Say suppose Delta is wow.",
                    "label": 0
                },
                {
                    "sent": "Shouldn't be shooting myself in the eye.",
                    "label": 0
                },
                {
                    "sent": "Suppose Delta is .1, so we want things to be correct with.",
                    "label": 0
                },
                {
                    "sent": "You know, we want to bound to hold probably .9.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have 100 examples.",
                    "label": 0
                },
                {
                    "sent": "And suppose we observe 2 error rates in our test set to two errors in our test set.",
                    "label": 0
                },
                {
                    "sent": "Then the square root turn off bound gives us this interval.",
                    "label": 0
                },
                {
                    "sent": "So I've actually taken the Delta and I have allocated.",
                    "label": 0
                },
                {
                    "sent": ".05 to computing the upper bound in .05, computing the lower bound.",
                    "label": 0
                },
                {
                    "sent": "And the exact calculation gives us this.",
                    "label": 0
                },
                {
                    "sent": "So I have another little program.",
                    "label": 0
                },
                {
                    "sent": "Which we should do right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's your opportunity to break my program.",
                    "label": 0
                },
                {
                    "sent": "What is a good bound error rate?",
                    "label": 0
                },
                {
                    "sent": "Should be more than 13.",
                    "label": 0
                },
                {
                    "sent": "300 Yeah bound error rate of 30 would not be so.",
                    "label": 0
                },
                {
                    "sent": "You want 30 examples?",
                    "label": 0
                },
                {
                    "sent": "OK, that's too easy for me.",
                    "label": 0
                },
                {
                    "sent": "But bound error rate.",
                    "label": 0
                },
                {
                    "sent": "What's a good bound error rate?",
                    "label": 0
                },
                {
                    "sent": ".2 OK. And now you had some statement about number of examples, but 30 is too easy, right?",
                    "label": 0
                },
                {
                    "sent": "Oh no, you can actually get by with less.",
                    "label": 0
                },
                {
                    "sent": "Let me plug it in.",
                    "label": 0
                },
                {
                    "sent": "And then we have a number of errors, right?",
                    "label": 0
                },
                {
                    "sent": "That's the other quantity.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "The boy we suck up possibility.",
                    "label": 0
                },
                {
                    "sent": "How about one is is a compliment of nine right?",
                    "label": 0
                },
                {
                    "sent": "So it's the same computation but sort of the other direction or something?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": ".27 right so our bound on is .27.",
                    "label": 0
                },
                {
                    "sent": "So anybody else have numbers?",
                    "label": 0
                },
                {
                    "sent": "They want me to plug in?",
                    "label": 0
                },
                {
                    "sent": "We can try the nine, but it won't be so satisfying.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "If there's, if you're computing the lower bound on.",
                    "label": 0
                },
                {
                    "sent": "Zero Point 210 nine is equivalent to computing the upper bound on 0.210.",
                    "label": 0
                },
                {
                    "sent": "One 'cause you can just one minus the bound.",
                    "label": 0
                },
                {
                    "sent": "Right, so I feel like you know, maybe we should do something a bit more impressive.",
                    "label": 0
                },
                {
                    "sent": "Or do you know it works?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again.",
                    "label": 0
                },
                {
                    "sent": "You're sick, mother.",
                    "label": 0
                },
                {
                    "sent": "This is the calculation of Ben Bar.",
                    "label": 0
                },
                {
                    "sent": "Beyond.",
                    "label": 0
                },
                {
                    "sent": "So you would be.",
                    "label": 0
                },
                {
                    "sent": "Videos.",
                    "label": 0
                },
                {
                    "sent": "So the confidence is .2.",
                    "label": 0
                },
                {
                    "sent": "Which means that with probability .8.",
                    "label": 0
                },
                {
                    "sent": "The bound will hold.",
                    "label": 0
                },
                {
                    "sent": "For future evaluations.",
                    "label": 0
                },
                {
                    "sent": "And we're not doing.",
                    "label": 0
                },
                {
                    "sent": "We're explicitly not approximating this as a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're actually doing the exact calculation for binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "So let me show you some about why you want to do that.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Too much.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's compare.",
                    "label": 0
                },
                {
                    "sent": "So this is a very common approach which I would like to.",
                    "label": 0
                },
                {
                    "sent": "Dump cold water on.",
                    "label": 0
                },
                {
                    "sent": "So the very common approach is you say, well, you know binomial is kind of like a Gaussian, so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to do is I'm going to approximate it by Gaussian the computer a confidence interval for a Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "So his cage number of test errors and image image number of examples.",
                    "label": 0
                },
                {
                    "sent": "This is your observed mean, and this is your observed variation.",
                    "label": 0
                },
                {
                    "sent": "And then you can compute abound, which is maybe this K / N + 2 Sigma, which is gives you.",
                    "label": 0
                },
                {
                    "sent": "About a 95% confidence interval for if if this was.",
                    "label": 0
                },
                {
                    "sent": "Actually a gaussian.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how do these actually compare in practice?",
                    "label": 0
                },
                {
                    "sent": "So right we did a few calculations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Take a bunch of datasets, cut them up in the training set and test set, and then on the test set, go and compute these various bounds.",
                    "label": 0
                },
                {
                    "sent": "So let's see what how does this work, the.",
                    "label": 0
                },
                {
                    "sent": "For each test set, there are two bars.",
                    "label": 0
                },
                {
                    "sent": "The one.",
                    "label": 0
                },
                {
                    "sent": "The one on the left.",
                    "label": 0
                },
                {
                    "sent": "Is the test set bound so just described?",
                    "label": 0
                },
                {
                    "sent": "And the one on the right.",
                    "label": 0
                },
                {
                    "sent": "Is the Gaussian approximation, which is just just described.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can see a few things which are kind of odd.",
                    "label": 0
                },
                {
                    "sent": "So over here.",
                    "label": 0
                },
                {
                    "sent": "You see that the Gaussian approximation says that your error rate is less than 1.5.",
                    "label": 1
                },
                {
                    "sent": "So this is not very satisfying, of course, because it's like saying.",
                    "label": 0
                },
                {
                    "sent": "Suppose I flip the coin.",
                    "label": 0
                },
                {
                    "sent": "I will get less than 1.5 hits.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Nothing much happens.",
                    "label": 0
                },
                {
                    "sent": "So these datasets.",
                    "label": 0
                },
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "For this data set, it was very, very tiny, which means that the size of the of the test set was like two or three, right?",
                    "label": 0
                },
                {
                    "sent": "And you know, we observed zero errors, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "If you observe zero errors with this Gaussian approximation, something horrible happens, right?",
                    "label": 0
                },
                {
                    "sent": "So if this is 0?",
                    "label": 0
                },
                {
                    "sent": "Then this will be 0.",
                    "label": 0
                },
                {
                    "sent": "And that means this will be 0.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So you start to fly the shuttle and it'll blow up because you're overconfident.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand.",
                    "label": 0
                },
                {
                    "sent": "For the test set down calculation, it takes into account the number of examples in a very explicit way and so.",
                    "label": 0
                },
                {
                    "sent": "You know it gives you something much more reasonable, since maybe you want to run a few more experiments.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "This data set, which turns out to be very easy.",
                    "label": 0
                },
                {
                    "sent": "It's also large, so it's large enough to have a large test set.",
                    "label": 0
                },
                {
                    "sent": "And that means that the both bounds the error observed here is 0 again and both bounds are actually very near 101, is very near to 0, which is appropriate because you have a lot of test examples and you did well on each of them.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is?",
                    "label": 0
                },
                {
                    "sent": "We're not getting this weirdness.",
                    "label": 0
                },
                {
                    "sent": "Where are bounds above 1?",
                    "label": 0
                },
                {
                    "sent": "And we're not getting overconfident, so this is maybe the most important part.",
                    "label": 0
                },
                {
                    "sent": "So if you want to go and evaluate some sort of binomial confidence interval, I really think you should just use the binomial confidence interval.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just saying what I said.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "So we can think about the process of evaluating these games.",
                    "label": 0
                },
                {
                    "sent": "These bounds is a game.",
                    "label": 0
                },
                {
                    "sent": "In step one.",
                    "label": 0
                },
                {
                    "sent": "A learner chooses a classifier.",
                    "label": 0
                },
                {
                    "sent": "An in Step 2, the verifier so that that's the drug company, right that use their drug and then.",
                    "label": 0
                },
                {
                    "sent": "In Step 2, the verifier, which is maybe the government, goes and says, oh here's a study we're going to.",
                    "label": 0
                },
                {
                    "sent": "We grab some people.",
                    "label": 0
                },
                {
                    "sent": "We test the drug on them, and then you know, we see how well it works.",
                    "label": 0
                },
                {
                    "sent": "So this is the valid order of operations for this particular bound.",
                    "label": 0
                },
                {
                    "sent": "If you get these out of order, things fail.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if the drug company draws the examples and evaluates the bound, it does it multiple times before choosing which one to report, then then that's bad.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I call this an interactive proof of learning, but it's kind of interactive in the trivial sense.",
                    "label": 0
                },
                {
                    "sent": "Don't think it's important here.",
                    "label": 0
                },
                {
                    "sent": "Is that the choice of classifier comes before the examples are drawn.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another slight variation on this, which is very commonly used in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It's K fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "So what happens in K fold cross validation is you have examples and instead of dividing into a training set and test set, you divide it into K equal sized sets.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And then on each set.",
                    "label": 0
                },
                {
                    "sent": "You train on everything but that set.",
                    "label": 0
                },
                {
                    "sent": "Then you test on this hell that subset, right?",
                    "label": 0
                },
                {
                    "sent": "So each individual run of this is like a train and a test on a set of size M / K. This is not well understood theoretically.",
                    "label": 0
                },
                {
                    "sent": "The best result we have is that the confidence interval should be smaller than something.",
                    "label": 0
                },
                {
                    "sent": "Then for test set bound of size M / K. In practice, this often works much better.",
                    "label": 0
                },
                {
                    "sent": "But there are worst case choices of distribution and learning album such as this bound is actually realized.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What this means for you in practice?",
                    "label": 0
                },
                {
                    "sent": "Is that leave one out cross validation where K = M?",
                    "label": 1
                },
                {
                    "sent": "You should not trust too much.",
                    "label": 0
                },
                {
                    "sent": "It is possible to actually severely overfit easily with leave one out cross validation.",
                    "label": 0
                },
                {
                    "sent": "There are some learning algorithms which are stable in some sense.",
                    "label": 0
                },
                {
                    "sent": "Where this is proved to work, but for general learning algorithms it's not proved to work.",
                    "label": 0
                },
                {
                    "sent": "Anyway, there's a big open problem here.",
                    "label": 0
                },
                {
                    "sent": "And I thought a bit about trying to solve this right, so maybe it's possible to state much tighter bounds if you go when you look at.",
                    "label": 0
                },
                {
                    "sent": "How much the different classifiers that you learn all the different K folds disagree with each other, right?",
                    "label": 0
                },
                {
                    "sent": "So the bad examples are always the ones where they disagree with each other a lot.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you can, if you just look at them and you say look.",
                    "label": 0
                },
                {
                    "sent": "They agree almost all the time.",
                    "label": 0
                },
                {
                    "sent": "You can see the tighter bound.",
                    "label": 0
                },
                {
                    "sent": "But it's a very difficult analysis.",
                    "label": 0
                },
                {
                    "sent": "I would say don't try to do that unless you feel very overconfident.",
                    "label": 0
                },
                {
                    "sent": "Alright, so are the questions about cross validation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Call transgenes yeah, the the big problem is that things are not in the individual.",
                    "label": 0
                },
                {
                    "sent": "Estimates are not independent, they're dependent in tracking those dependencies is very tricky.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should mention exactly one way to leave when I cross validation can fail horribly.",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have a learning algorithm learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is the followings it takes a look at the examples and it says if there are an even number of ones I'll predict 0.",
                    "label": 0
                },
                {
                    "sent": "And if there are an odd number of ones, I'll predict one all the time, right?",
                    "label": 0
                },
                {
                    "sent": "So it's either 0 all the time or one all the time.",
                    "label": 0
                },
                {
                    "sent": "Now suppose.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have.",
                    "label": 0
                },
                {
                    "sent": "A distribution.",
                    "label": 0
                },
                {
                    "sent": "With just, you know, pics label with probability .5.",
                    "label": 0
                },
                {
                    "sent": "Enable zero probability .5.",
                    "label": 0
                },
                {
                    "sent": "Right so then maybe we end up with.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "46 heads, right?",
                    "label": 0
                },
                {
                    "sent": "So if we hold out a tail.",
                    "label": 0
                },
                {
                    "sent": "Window with 46 ones.",
                    "label": 0
                },
                {
                    "sent": "So if we hold out a 0.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to predict correctly, because it will just the parity will be even.",
                    "label": 0
                },
                {
                    "sent": "We hold out a one and will also predict correctly because now the pair of switches in those 45 heads 45 ones.",
                    "label": 0
                },
                {
                    "sent": "So we're going to predict the guy that we held out perfectly.",
                    "label": 0
                },
                {
                    "sent": "Right, So what that says is that.",
                    "label": 0
                },
                {
                    "sent": "Although our error rate is .5.",
                    "label": 0
                },
                {
                    "sent": "The observation that will get from leave one out cross validation is.",
                    "label": 0
                },
                {
                    "sent": "No jurors.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why not just use Bootstrap?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The state of analysis of Bootstrap is still not quite satisfying.",
                    "label": 0
                },
                {
                    "sent": "It's not as good As for the test set bound.",
                    "label": 0
                },
                {
                    "sent": "I will talk about that later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the test set bound.",
                    "label": 0
                },
                {
                    "sent": "That's the basics, which I think everybody should know about these things.",
                    "label": 0
                },
                {
                    "sent": "That is the practical part, which can be used now.",
                    "label": 0
                },
                {
                    "sent": "These next things are hopefully give you some intuition about good learning algorithms and so forth, but they're not directly used that much yet.",
                    "label": 0
                },
                {
                    "sent": "I guess Rob Novak.",
                    "label": 0
                },
                {
                    "sent": "Yesterday was talking about a variant of the Oakland Raider Band, which is getting pretty near to use.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first thing is why?",
                    "label": 0
                },
                {
                    "sent": "You will find as you go out in the world for the test head bound works great in practice.",
                    "label": 0
                },
                {
                    "sent": "And the question is, why do we expend?",
                    "label": 0
                },
                {
                    "sent": "100 papers on trying to do something other than tested found.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "And it actually has some pretty good answers.",
                    "label": 0
                },
                {
                    "sent": "One of them is that there exists learning problems where the process of taking the training set, taking a big set coming into training set and test set is very, very bad.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                },
                {
                    "sent": "Taking out these few examples for the test set.",
                    "label": 1
                },
                {
                    "sent": "Can destroy your ability to learn.",
                    "label": 0
                },
                {
                    "sent": "And in fact, you can imagine learning problems where taking out just one example destroys your ability to learn so.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you the example, we're just taking out one is very bad.",
                    "label": 0
                },
                {
                    "sent": "So the gamble is.",
                    "label": 0
                },
                {
                    "sent": "Learning a parity of a subset of the bits right so the target the label is a parity of a subset of bits that you have a set of bits for your input, right?",
                    "label": 0
                },
                {
                    "sent": "The way to learn this when you have no noise is to use Gaussian elimination.",
                    "label": 0
                },
                {
                    "sent": "So you just think of your examples.",
                    "label": 0
                },
                {
                    "sent": "Is forming a matrix and you use Gaussian elimination to figure out which bits are the critical bits.",
                    "label": 0
                },
                {
                    "sent": "And if you have a number of examples which is less than the number of features you lose.",
                    "label": 0
                },
                {
                    "sent": "Your error rate is .5.",
                    "label": 0
                },
                {
                    "sent": "If you have any number of examples which is equal to the number of features in their in their independent.",
                    "label": 0
                },
                {
                    "sent": "Which they are with probability .5 or so.",
                    "label": 0
                },
                {
                    "sent": "Then you win.",
                    "label": 0
                },
                {
                    "sent": "You get it perfectly.",
                    "label": 0
                },
                {
                    "sent": "Now this is kind of an extreme example and you may not believe to have parity in the real world.",
                    "label": 0
                },
                {
                    "sent": "But it is observed that there are phase transitions and how well you can learn is a function of the number of examples on a lot of natural learning problems.",
                    "label": 0
                },
                {
                    "sent": "So this is a real point.",
                    "label": 0
                },
                {
                    "sent": "The other reason why is because you want to use these bounds to really guide how you learn.",
                    "label": 0
                },
                {
                    "sent": "What classifier do you choose?",
                    "label": 0
                },
                {
                    "sent": "You can't do that very well with training with test set bounds because.",
                    "label": 0
                },
                {
                    "sent": "The process of guiding learning implies evaluating a lot of different classifiers.",
                    "label": 0
                },
                {
                    "sent": "And setting aside separate a separate pool of examples for each classifier you evaluate.",
                    "label": 0
                },
                {
                    "sent": "Is the kind of thing which will result in very bad judgments, because you'll take all your information your fraction into very small sets and just not be able to predict very well in the end.",
                    "label": 0
                },
                {
                    "sent": "So this is what motivates training set bounds.",
                    "label": 0
                },
                {
                    "sent": "So training set bounds are bounds where.",
                    "label": 0
                },
                {
                    "sent": "We just have one big training set.",
                    "label": 0
                },
                {
                    "sent": "We learn the training set.",
                    "label": 0
                },
                {
                    "sent": "And then we compute a bound.",
                    "label": 0
                },
                {
                    "sent": "For what we learned.",
                    "label": 0
                },
                {
                    "sent": "There's no cutting anymore.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the interactive proof of learning associated with the Oaklands Razor bound protocol.",
                    "label": 0
                },
                {
                    "sent": "What happens is your learner.",
                    "label": 0
                },
                {
                    "sent": "First uses some.",
                    "label": 0
                },
                {
                    "sent": "Prior over classifiers.",
                    "label": 0
                },
                {
                    "sent": "So this is a prior in quotes 'cause it's not a Bayesian priors, just some particular normalized measure over classifiers.",
                    "label": 0
                },
                {
                    "sent": "So if you were beige and then you would be expecting distribution over distributions, is the distribution over classifiers?",
                    "label": 0
                },
                {
                    "sent": "Right, so we commit to a distribution over classifiers.",
                    "label": 0
                },
                {
                    "sent": "And then we draw training examples.",
                    "label": 0
                },
                {
                    "sent": "The verifier draws them, gives them to the learner learner, chooses some classifier.",
                    "label": 0
                },
                {
                    "sent": "And then the bound is evaluated.",
                    "label": 0
                },
                {
                    "sent": "So the choice of classifier could be dependent upon on how the band will be evaluated.",
                    "label": 0
                },
                {
                    "sent": "This is just a deterministic process at this point, right?",
                    "label": 0
                },
                {
                    "sent": "So for the Oakland Raiders, OK, just sort of reuse the examples over and over again in your choice of classifier.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the Oakland Razor bound says.",
                    "label": 1
                },
                {
                    "sent": "For every choice of these priors over classifiers.",
                    "label": 0
                },
                {
                    "sent": "For every.",
                    "label": 0
                },
                {
                    "sent": "For every distribution.",
                    "label": 0
                },
                {
                    "sent": "For every choice of confidence.",
                    "label": 0
                },
                {
                    "sent": "The probability.",
                    "label": 0
                },
                {
                    "sent": "But for every classifier.",
                    "label": 0
                },
                {
                    "sent": "The true error rate is bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "Is high 1 minus Delta?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Call Larry so.",
                    "label": 0
                },
                {
                    "sent": "The only difference from the test set bound.",
                    "label": 1
                },
                {
                    "sent": "Is it Delta is going to Delta times Pfc.",
                    "label": 0
                },
                {
                    "sent": "That's it, that's the only difference mathematically.",
                    "label": 0
                },
                {
                    "sent": "So I think yesterday Rob Novak was working with with this.",
                    "label": 0
                },
                {
                    "sent": "Kind of result.",
                    "label": 0
                },
                {
                    "sent": "Which is just taking this and using the turn off approximation.",
                    "label": 0
                },
                {
                    "sent": "So he had a description links which you can think of a description links as.",
                    "label": 0
                },
                {
                    "sent": "Is also a major over classifiers.",
                    "label": 0
                },
                {
                    "sent": "Using this crafts inequality.",
                    "label": 0
                },
                {
                    "sent": "So you can say the probability of a classifier according to the prior is 2 to the minus description links.",
                    "label": 0
                },
                {
                    "sent": "And if you, as long as you have.",
                    "label": 0
                },
                {
                    "sent": "A good way of describing things that's a valid choice.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this proof is slightly more complicated than for the test set bound, but it's only one page, it's very easy.",
                    "label": 0
                },
                {
                    "sent": "First of all, you start with the test set bound.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Instead of.",
                    "label": 0
                },
                {
                    "sent": "Plugging in Delta, you plug in Delta times Pfc, 'cause this is valid valid for all for all Delta, right so?",
                    "label": 0
                },
                {
                    "sent": "This is the Delta Times Pfc is the Delta in the test set bound, which means we have a Delta times Pfc there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we just negate this.",
                    "label": 0
                },
                {
                    "sent": "So it's just a matter of.",
                    "label": 0
                },
                {
                    "sent": "Working with what negation means.",
                    "label": 0
                },
                {
                    "sent": "So this is something happens with high probability and now something happens with low probability with the opposite happens with low probability.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we apply the Union bound.",
                    "label": 0
                },
                {
                    "sent": "The union bound says that the probability of A or B.",
                    "label": 0
                },
                {
                    "sent": "Is less than the probability plus probability of B. Whoa.",
                    "label": 0
                },
                {
                    "sent": "Deboarding you to fix that?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Little bit too energetic for my cybernetic enhancement.",
                    "label": 0
                },
                {
                    "sent": "Say again.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it could be.",
                    "label": 0
                },
                {
                    "sent": "I'm just trying to distract you.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "The mathematics strikes back.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "We use this union bound over and over again that gives us a sum.",
                    "label": 0
                },
                {
                    "sent": "Of these Delta times PFC's.",
                    "label": 0
                },
                {
                    "sent": "And Pfc, the sum of the pieces.",
                    "label": 0
                },
                {
                    "sent": "One or or something less than one.",
                    "label": 0
                },
                {
                    "sent": "Which case we can just.",
                    "label": 0
                },
                {
                    "sent": "Put in Delta instead.",
                    "label": 0
                },
                {
                    "sent": "And then of course you do.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "8 again.",
                    "label": 0
                },
                {
                    "sent": "Right and then it gives you the.",
                    "label": 0
                },
                {
                    "sent": "The countries are bound.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "In pictures.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What this looks like is you have a bunch of different classifiers.",
                    "label": 0
                },
                {
                    "sent": "And they each have a different size cut in their tail, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "The size of the cut is Delta times Pfc.",
                    "label": 0
                },
                {
                    "sent": "So it varies from classifier the classifier.",
                    "label": 0
                },
                {
                    "sent": "So this classifier has a very small Pfc.",
                    "label": 0
                },
                {
                    "sent": "In this classifier has larger Pfc, just the area equals Delta times Pfc.",
                    "label": 0
                },
                {
                    "sent": "And once again, you don't know what the actual area classifiers are.",
                    "label": 0
                },
                {
                    "sent": "So we're just going to go through the same analysis that we went through for the test set out.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our learner chooses some classifier.",
                    "label": 0
                },
                {
                    "sent": "That classifier has a specified size for Delta times Pfc.",
                    "label": 0
                },
                {
                    "sent": "That's what the red is.",
                    "label": 0
                },
                {
                    "sent": "These areas are supposed to be the same.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have, you don't know what the true error rate is, so you just.",
                    "label": 1
                },
                {
                    "sent": "Choose the worst one right there worst one consistent with not being in the tail.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go back a little bit.",
                    "label": 0
                },
                {
                    "sent": "So the key difference aside from this here.",
                    "label": 0
                },
                {
                    "sent": "Is we have for all C inside of the probability.",
                    "label": 0
                },
                {
                    "sent": "That means.",
                    "label": 0
                },
                {
                    "sent": "This holds for all.",
                    "label": 0
                },
                {
                    "sent": "See, it holds for whichever.",
                    "label": 0
                },
                {
                    "sent": "See you're learning and happens to choose.",
                    "label": 0
                },
                {
                    "sent": "That means you can evaluate.",
                    "label": 0
                },
                {
                    "sent": "You can use this bound over and over again in the process of learning itself.",
                    "label": 0
                },
                {
                    "sent": "So I think Rob talked about overfitting yesterday, but the way that overfitting is expressed here is if you have a very complex classifier, this PC is going to get very small and that means you're bound is going to get very bad because those times Pfc is going to get very very small.",
                    "label": 0
                },
                {
                    "sent": "So this gives you some way of trading off between the two possibilities.",
                    "label": 0
                },
                {
                    "sent": "Right example.",
                    "label": 0
                },
                {
                    "sent": "So everything is the same as before except now our Pfc is .1.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can go and we can compute the square root turn off approximation, which is this.",
                    "label": 0
                },
                {
                    "sent": "Confidence interval.",
                    "label": 0
                },
                {
                    "sent": "Just as an unsatisfying we will observe.",
                    "label": 0
                },
                {
                    "sent": "No more than.",
                    "label": 0
                },
                {
                    "sent": "How do you think about that?",
                    "label": 0
                },
                {
                    "sent": "Well, anyway, you shouldn't have.",
                    "label": 0
                },
                {
                    "sent": "A number of heads which is minus .143.",
                    "label": 0
                },
                {
                    "sent": "And if you do the exact calculation, you just get something.",
                    "label": 0
                },
                {
                    "sent": "A bit tighter, which is always within the interval from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Once again, we can go and we can actually apply.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As to various.",
                    "label": 0
                },
                {
                    "sent": "Lord albums on various datasets and particularly algorithm will be some sort of decision tree.",
                    "label": 0
                },
                {
                    "sent": "The probability of failure will be set to Delta.",
                    "label": 0
                },
                {
                    "sent": "All the problems are sort of discrete problems from this UCI database and machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "And then there's two things are going to compare.",
                    "label": 0
                },
                {
                    "sent": "One of them is the train and test approach.",
                    "label": 0
                },
                {
                    "sent": "And the other one is the train and everything approach, right?",
                    "label": 0
                },
                {
                    "sent": "So the training test approach is just going to look at the test set bound.",
                    "label": 1
                },
                {
                    "sent": "And on the train, and everything approached, I'm just going to use this Oakland razor bound.",
                    "label": 0
                },
                {
                    "sent": "One other caveat, which is very important.",
                    "label": 0
                },
                {
                    "sent": "Another horrible, horrible way to cheat, which is very hard to catch.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "In your choice of datasets, right?",
                    "label": 0
                },
                {
                    "sent": "So I will tell you that I chose datasets before I evaluated everything and I'm reporting all all of the results.",
                    "label": 0
                },
                {
                    "sent": "You should do that too.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "This is the results.",
                    "label": 0
                },
                {
                    "sent": "So you have a test set bound which is the left here.",
                    "label": 1
                },
                {
                    "sent": "And you have.",
                    "label": 0
                },
                {
                    "sent": "A optimization bound which is the right.",
                    "label": 0
                },
                {
                    "sent": "And the code which is being used here is something very like the first code which Rob talked about yesterday.",
                    "label": 0
                },
                {
                    "sent": "So there was a question yesterday about how the constants work out right.",
                    "label": 0
                },
                {
                    "sent": "And this direct comparison can maybe tell you something about how the constants actually workout.",
                    "label": 1
                },
                {
                    "sent": "What you see is that the Oaklands razor bound is typically.",
                    "label": 0
                },
                {
                    "sent": "A little bit looser, right?",
                    "label": 0
                },
                {
                    "sent": "So forth.",
                    "label": 0
                },
                {
                    "sent": "It's not a lot looser.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's quite a bit looser here.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty large data set in terms of number of examples, so there's maybe a factor of 10 there.",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "Of course, keep in mind is that the training set.",
                    "label": 0
                },
                {
                    "sent": "Is about five times larger than the test set here.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the Circumciser bound is actually winning.",
                    "label": 0
                },
                {
                    "sent": "So over here it wins.",
                    "label": 0
                },
                {
                    "sent": "These are not very impressive confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "What's happening is the number of examples in the test set is small.",
                    "label": 0
                },
                {
                    "sent": "And then it becomes very important that we actually have.",
                    "label": 0
                },
                {
                    "sent": "The five times more examples in the training set, even though.",
                    "label": 0
                },
                {
                    "sent": "Multiplying by Pfc makes things a bit worse.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Choice measure over here.",
                    "label": 0
                },
                {
                    "sent": "Passengers to choice of 1.",
                    "label": 0
                },
                {
                    "sent": "You're actually doing model selection, right?",
                    "label": 0
                },
                {
                    "sent": "Basically why you're putting on a prior probability of some sort of everything that could possibly be.",
                    "label": 0
                },
                {
                    "sent": "I wanted to do that now with my just made up.",
                    "label": 0
                },
                {
                    "sent": "How many possible things would be?",
                    "label": 0
                },
                {
                    "sent": "So I'm just wondering how sensitive all of this.",
                    "label": 0
                },
                {
                    "sent": "Right, so the question is, how sensitive is it to sort of your choice of learning algorithm?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Break.",
                    "label": 0
                },
                {
                    "sent": "Possible.",
                    "label": 0
                },
                {
                    "sent": "$1,000,000.",
                    "label": 0
                },
                {
                    "sent": "The Kia they should.",
                    "label": 0
                },
                {
                    "sent": "Right, so the question is, how do you define Pfc?",
                    "label": 0
                },
                {
                    "sent": "And the answer so far is you do some soul searching.",
                    "label": 0
                },
                {
                    "sent": "Right, how sensitive are the results to the actual choice of Pfc?",
                    "label": 0
                },
                {
                    "sent": "And the answer is it can be quite sensitive at times.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if your choice of P of C was just a uniform distribution over all classifiers.",
                    "label": 0
                },
                {
                    "sent": "That would be very bad.",
                    "label": 0
                },
                {
                    "sent": "It's important that your choice of Pfc be sort of well aligned with your algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you want these bounds to be tight.",
                    "label": 0
                },
                {
                    "sent": "So in particular, the pruning process here shows the classifier which minimized.",
                    "label": 0
                },
                {
                    "sent": "The bound according to this particular choice of Pfc.",
                    "label": 0
                },
                {
                    "sent": "R. So minimized amongst the prunings of the tree that was grown.",
                    "label": 0
                },
                {
                    "sent": "I wish I had a good answer for you.",
                    "label": 0
                },
                {
                    "sent": "Think hard is not a really great answer when you want something to actually workout.",
                    "label": 0
                },
                {
                    "sent": "I think it's an honest evaluation of the state of things right now.",
                    "label": 0
                },
                {
                    "sent": "I came here.",
                    "label": 0
                },
                {
                    "sent": "Impossible.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So if you are doing some sort of continuous set of classifiers, then Pfc will be 0 and then of course the Oakland Raider Band is not going to tell you anything useful.",
                    "label": 0
                },
                {
                    "sent": "So one thing to understand, these training set bounds is that these are bounds, they're not.",
                    "label": 0
                },
                {
                    "sent": "Let me see this another way.",
                    "label": 0
                },
                {
                    "sent": "You could take the same learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You could plug in the Pfc which is uniform over all possible decision trees.",
                    "label": 0
                },
                {
                    "sent": "The band would be horrible, but your performance would be exactly the same as here because it's the same learning algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So just because the bound says you have a horrible classifier does not mean that you have a horrible classifier.",
                    "label": 0
                },
                {
                    "sent": "And that's something to always understand about bounds.",
                    "label": 0
                },
                {
                    "sent": "So this particular bound will not apply in a strong way when when your PC is over continuous set.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, the PAC Bayes bound will apply there and second of all there's these VC bounds, so I will not be talking about that.",
                    "label": 0
                },
                {
                    "sent": "Apply there to some extent.",
                    "label": 0
                },
                {
                    "sent": "You start to get into issues of it's very difficult to avoid issues of how tight is the bound we start working with continuous sets of classifiers.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This phone 'cause we are working on the fire system later.",
                    "label": 0
                },
                {
                    "sent": "We constrain these mountains working only with my sisters.",
                    "label": 0
                },
                {
                    "sent": "It is almost like though.",
                    "label": 0
                },
                {
                    "sent": "It's a date.",
                    "label": 0
                },
                {
                    "sent": "Go to play.",
                    "label": 0
                },
                {
                    "sent": "Just trying to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, every argument for a unbiased estimator of this particular boundary by system.",
                    "label": 0
                },
                {
                    "sent": "Right, so in statistics there's this notion of an unbiased estimator, right?",
                    "label": 0
                },
                {
                    "sent": "We do not advise that these are pessimistic, right?",
                    "label": 0
                },
                {
                    "sent": "So these bounds are are supposed to always be a bit worse.",
                    "label": 0
                },
                {
                    "sent": "This test set bound maybe you can prove that you can't do anything better.",
                    "label": 0
                },
                {
                    "sent": "But for the training set bounds, you can often be the case that a different training set bound will actually do much better.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is this example of the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Overall, classifiers versus some reasonable description language.",
                    "label": 0
                },
                {
                    "sent": "The distribution induced by some reasonable description language.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I think it's time for a break.",
                    "label": 0
                },
                {
                    "sent": "Let's take a break for about 10 minutes.",
                    "label": 0
                }
            ]
        }
    }
}