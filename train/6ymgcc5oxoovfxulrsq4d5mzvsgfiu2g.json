{
    "id": "6ymgcc5oxoovfxulrsq4d5mzvsgfiu2g",
    "title": "Improved Geometric Verification for Large Scale Landmark Image Collections",
    "info": {
        "author": [
            "Joseph Tighe, University of North Carolina at Chapel Hill"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_tighe_image_collections/",
    "segmentation": [
        [
            "Good morning as he said, my name is Joseph Thai and this is joint work with overhaul.",
            "Raghuraman Jan, Michael, Frahm, Verheul did do the bulk of the work on this paper, but he started a new job about a week and a half ago and could not attend.",
            "So the title of the paper is improved geometric verification for large scale landmark image collections.",
            "The takeaways that you should get from that is.",
            "That these are going to be large Internet scale image collections that we're dealing with and we're trying to speed up geometric verification so."
        ],
        [
            "Here's an example of the scale of the image collections we're dealing with, so this is our Rome image dataset.",
            "It's downloaded from Flickr.",
            "There's roughly 3 million images, so each pixel you see represents about four images.",
            "What we hope to get from a download like this are images of landmarks throughout the city.",
            "In this case Rome, and we want to match images of the same landmark to eventually create 3D models of them.",
            "When you do a flicker query, however, as many of you probably have done and seen, you get a lot of noise."
        ],
        [
            "So the first step in any one of these systems is going to be removing that noise so that we have just landmark images where we have at least some overlap between."
        ],
        [
            "Images.",
            "This area is quite recent and the 1st paper was in 2006 called photo tourism.",
            "They dealt with noise by manually taking it out and they had a data set of about 3000 images.",
            "And they generated 3D point clouds from that.",
            "Since then there have been a number of techniques that handle more and more images from my group.",
            "In 2010 was building Rome in a cloudless day, which was able to handle the input data set of 3 million images.",
            "And run that on a PC in 24 hours and actually generate 3D models like this.",
            "One thing about this system is almost half the time, about 11 and 1/2 hours was spent just calling out the noise through geometric verification, and so in this paper we look at how to speed that process up.",
            "So there are lots of challenges with any of these systems, and we're going to look at the speed and scalability and also to some extent the model completeness."
        ],
        [
            "So before I go on, let me be specific about what I mean when I say geometric verification, we want to find pairs of images that are of the same landmark, and for any given pair we detect feature points.",
            "We match these feature points.",
            "And then run RANSAC to estimate a fundamental matrix using the seven point algorithm and then find the inlier set which we keep for late."
        ],
        [
            "Processing now because RANSAC is used here, the first area of improvement that's obvious is to improve RANSAC.",
            "I don't know why it's doing that.",
            "And there have been many improvements that have been preemptive ransack, which did a breadth first search walzac, which did sequential decision theory.",
            "Arsac was done by Rahul as well and that combined the first 2 and then Prozac was mentioned earlier as well, and that used a priority ordering and let me."
        ],
        [
            "Go into a little more detail of what that means, so prosak exploits image to image matching scores.",
            "So for these two images it will look at the matching correspondences between them.",
            "And then order the matches based on the score so the score can be either the SIFT score, the civil ratio test.",
            "I believe this if ratio test was used and then sample from a small subset of best points.",
            "Gradually extending that size if no matches."
        ],
        [
            "Found.",
            "So this is the motivation for this work, and thus far in Prosak and Prozac like techniques.",
            "Only image to image matching's information has been considered.",
            "Prosak, however, only requires that you have a priority ordering on these matches, so it doesn't require a specific score.",
            "Now when we look at the images that we're dealing with, we see that there's lots and lots of repetition, and so our key insight for this work is that we can learn a better ordering based on previous matches.",
            "We can learn that for these specific images perhaps certain SIFT feature points are often inliers, an other."
        ],
        [
            "Things are often not.",
            "The way we do this is we first construct a visual vocabulary.",
            "This is a very standard technique and we do it by running K means on a random subset of points ahead of time.",
            "You could also construct a visual vocabulary for all.",
            "Natural images, theoretically we do it per data set, but it could be.",
            "Start one visual vocabulary and use it for all your tests.",
            "Then once you have this visual vocabulary, you carry out standard geometric verification for a pair of images.",
            "You match features and run our stack.",
            "If you have a verified match or a good model, then you update the counts for each visual word corresponding to any point that was an inlier.",
            "We maintain these counts between ransack rounds and we use them as a priority ordering for prosak like sampling."
        ],
        [
            "Give you a better idea of how this works.",
            "As you match as you do your RANSAC steps.",
            "I don't know why it's going so fast as you do your ransack steps.",
            "You will, let's say at 10 images you'll have a visual count like this and then as you match more and more images you have more an more peaked distribution and each one of those visual words along the bottom.",
            "It's an unordered list.",
            "Corresponds to account and hopefully the higher counts will be better features and when you run your ransack, the accounts with the most or the feature points with the most count will be prioritized and sampled from fir."
        ],
        [
            "So to test this, we use the Berlin Dome data set that we have.",
            "It's 10,000 images of the Berlin Dome.",
            "We got it from a Flickr query as well, but it's quite clean.",
            "We randomly choose K image pairs and carry out geometric verification.",
            "Accumulating counts for visual words, visual vocabulary of 20,000 words.",
            "So for a given image we detect feature points and then the process sampling could be thought of as thresholding only feature points where accounts are above some threshold and the threshold is lowered.",
            "If no model is found.",
            "Here is a visualization of which feature points.",
            "Get thresholded the larger the circle, the more counts for that feature point.",
            "The main takeaway is the larger circles are usually in the center of the building and there are areas that are likely to verify against other images.",
            "Also, you'll notice that there are no longer any feature points in the Sky and few on the ground where it's less likely to have inliers for your fundamental metrics estimation."
        ],
        [
            "We ran three experiments to compare our technique on this data set.",
            "The first is just regular ransack.",
            "The second was RR sack with the SIFT based matching scores as the ordering.",
            "You could think of this as close to Prozac, but note we have other optimizations there as well, and the third step, the third test was our sack with the proposed ordering.",
            "As you can see, the mean time for image pair and the mean mean time as per image, barren mean number of hypothesis per image went down as well as the total runtime, indicating that there isn't a large overhead for this as well.",
            "I like to note that there is no dedicated training phase here for our three.",
            "We start with a 0 count and it's just updated as we get more and more matches.",
            "Empirically we found that after about 500.",
            "Valid matches or 500 models that we found.",
            "We start to get performance better than using the SIFT matching scores and it seems that after about 1000 images we've.",
            "Found are optimal.",
            "So the main observation from this smaller test set is that sorting visual words based on the results of previous rounds is desirable.",
            "It gives extra information about which features are useful and speeds up geometric verification."
        ],
        [
            "But we the the goal of this was to speed up on very large scale datasets, so our second experiment was on the Berlin data set.",
            "This is 2.7 million images from Flickr query Ann is very noisy as you can see along the bottom.",
            "Here we use the building Roman, a cloudless day framework.",
            "I'll cover that quickly.",
            "We cluster images into 100,000 clusters using visual descriptor.",
            "In this case it was a binarized just.",
            "And we performed geometric verification only on each cluster separately.",
            "Any image that was in cluster one would not be verified against any image that was in another cluster.",
            "And then clusters with three or more verified images are kept there, called iconic clusters, and the most verified images, called the iconic all unverified images, are discarded.",
            "And for this data set, we accumulate counts for visual cabul airy of 1,000,000 words."
        ],
        [
            "Here we only ran two experiments that we did not run standard ransack, so we ran our sack with the ordering based on SIFT matching scores and our sack with our new proposed ordering based on visual word counts.",
            "Again, we get a speedup for mean time.",
            "For image Parent total runtime, though it's not as significant speedup as the other data set.",
            "Also notice that we have slightly more iconic and registered images.",
            "This is likely due to noise and not the fact that we're actually getting better matches.",
            "But only 5% of the entire data set has been registered.",
            "Now the reason the speedup is not quite as large as in the previous data set is there are many more attempted ransacks that fail, and when RANSAC fails, both of these methods have approximately the same runtime.",
            "But we found that in the cases where there is a very low inlier count, this."
        ],
        [
            "Algorithm excels quite a bit, so for this image we did go back and run RANSAC, and we also had our sack with the matching scores in our sex with our inlier counts, and we get a very significant speedup.",
            "We went from about 200,000.",
            "20,000 matching score, 20,000 hypothesis using the matching score to just under 600.",
            "And that's because the while the matching scores may be high between points, the our algorithm has learned that the center of the building, the points at the center of the building are most likely to be inliers, and so it looks at those first."
        ],
        [
            "But now we'd like to address the completeness problem.",
            "Only 5% of the images have been registered this far, and we have 2.6 million images.",
            "Still not registered in the first step.",
            "Our first attempt to do this was to retrieve the 20 most visually similar iconic images and perform geometric verification on those against each non verified image.",
            "This process alone take took one day and 10 hours were doing.",
            "Roughly, 40,000,000 verification rounds and this is too slow.",
            "But our second key insight is that we have a example of good images thus far in our iconic, so we can learn what a good image looks like and then use this knowledge to only match good images."
        ],
        [
            "We do this by training what we call a landmark classifier.",
            "The positive examples are the iconic images that we've verified and the negative examples are just a random set of rejected images, roughly the same size as the number of Iconix as you see along the top row of images, those are the positive training examples, and even there we have some noise, so there's a band and there were a number of identical images that geometrically verified of the same band, and in addition, in the negative training images there are.",
            "Some images that do look like they could be landmark images, and in fact we hope so because these are the images we hope to verify, but the large majority of them are not landmark images.",
            "So we use our dictionary to compute a bag of words, histogram and so this is already precomputed, so computing this feature is free essentially, and while this feature has a million dimensions, note that each image only has roughly 2000 feature points matches, so it's quite sparse with only about 2000 words.",
            "And we train a linear SVM that takes advantage of the sparsity an IT trains in about 5 minutes, so it's very quick to train."
        ],
        [
            "So our new methodology is to verify only the rejected images that are classified as landmarks and so this reduces the set of images that we're going to verify to about 26%, giving us a trivially close to 4X speedup.",
            "Now, as you can see, there are some images that if we did full reverification we missed because we're only verifying on 26% of them, roughly 70,000, but we reduce our one day 10 hours to just a 10 hours.",
            "We also argue that perhaps it's good that we do not verify those images.",
            "Row along the top is the highest scoring images that do verify, so they verify in both cases and the row along the bottom are lowest scoring images.",
            "The ones that the SVM does reject.",
            "But if we were to do full reverification would actually verify and notice that perhaps there's only one that we would want in their one landmark images.",
            "The rest are images that are probably duplicates or very similar images that end up verifying but are not of landmarks."
        ],
        [
            "So in summary, we found that combining recognition and geometry methodologies enables fast and more complete registration.",
            "Learning these priors based on match history rather than just image to image matching scores does capture useful information.",
            "And using fast pre classification can greatly decrease the runtime while minimally impacting the matching performance.",
            "Thank you questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning as he said, my name is Joseph Thai and this is joint work with overhaul.",
                    "label": 0
                },
                {
                    "sent": "Raghuraman Jan, Michael, Frahm, Verheul did do the bulk of the work on this paper, but he started a new job about a week and a half ago and could not attend.",
                    "label": 0
                },
                {
                    "sent": "So the title of the paper is improved geometric verification for large scale landmark image collections.",
                    "label": 1
                },
                {
                    "sent": "The takeaways that you should get from that is.",
                    "label": 0
                },
                {
                    "sent": "That these are going to be large Internet scale image collections that we're dealing with and we're trying to speed up geometric verification so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of the scale of the image collections we're dealing with, so this is our Rome image dataset.",
                    "label": 0
                },
                {
                    "sent": "It's downloaded from Flickr.",
                    "label": 0
                },
                {
                    "sent": "There's roughly 3 million images, so each pixel you see represents about four images.",
                    "label": 0
                },
                {
                    "sent": "What we hope to get from a download like this are images of landmarks throughout the city.",
                    "label": 0
                },
                {
                    "sent": "In this case Rome, and we want to match images of the same landmark to eventually create 3D models of them.",
                    "label": 0
                },
                {
                    "sent": "When you do a flicker query, however, as many of you probably have done and seen, you get a lot of noise.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first step in any one of these systems is going to be removing that noise so that we have just landmark images where we have at least some overlap between.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images.",
                    "label": 0
                },
                {
                    "sent": "This area is quite recent and the 1st paper was in 2006 called photo tourism.",
                    "label": 0
                },
                {
                    "sent": "They dealt with noise by manually taking it out and they had a data set of about 3000 images.",
                    "label": 0
                },
                {
                    "sent": "And they generated 3D point clouds from that.",
                    "label": 0
                },
                {
                    "sent": "Since then there have been a number of techniques that handle more and more images from my group.",
                    "label": 0
                },
                {
                    "sent": "In 2010 was building Rome in a cloudless day, which was able to handle the input data set of 3 million images.",
                    "label": 0
                },
                {
                    "sent": "And run that on a PC in 24 hours and actually generate 3D models like this.",
                    "label": 0
                },
                {
                    "sent": "One thing about this system is almost half the time, about 11 and 1/2 hours was spent just calling out the noise through geometric verification, and so in this paper we look at how to speed that process up.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of challenges with any of these systems, and we're going to look at the speed and scalability and also to some extent the model completeness.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I go on, let me be specific about what I mean when I say geometric verification, we want to find pairs of images that are of the same landmark, and for any given pair we detect feature points.",
                    "label": 1
                },
                {
                    "sent": "We match these feature points.",
                    "label": 0
                },
                {
                    "sent": "And then run RANSAC to estimate a fundamental matrix using the seven point algorithm and then find the inlier set which we keep for late.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Processing now because RANSAC is used here, the first area of improvement that's obvious is to improve RANSAC.",
                    "label": 0
                },
                {
                    "sent": "I don't know why it's doing that.",
                    "label": 0
                },
                {
                    "sent": "And there have been many improvements that have been preemptive ransack, which did a breadth first search walzac, which did sequential decision theory.",
                    "label": 1
                },
                {
                    "sent": "Arsac was done by Rahul as well and that combined the first 2 and then Prozac was mentioned earlier as well, and that used a priority ordering and let me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go into a little more detail of what that means, so prosak exploits image to image matching scores.",
                    "label": 0
                },
                {
                    "sent": "So for these two images it will look at the matching correspondences between them.",
                    "label": 0
                },
                {
                    "sent": "And then order the matches based on the score so the score can be either the SIFT score, the civil ratio test.",
                    "label": 0
                },
                {
                    "sent": "I believe this if ratio test was used and then sample from a small subset of best points.",
                    "label": 1
                },
                {
                    "sent": "Gradually extending that size if no matches.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Found.",
                    "label": 0
                },
                {
                    "sent": "So this is the motivation for this work, and thus far in Prosak and Prozac like techniques.",
                    "label": 0
                },
                {
                    "sent": "Only image to image matching's information has been considered.",
                    "label": 1
                },
                {
                    "sent": "Prosak, however, only requires that you have a priority ordering on these matches, so it doesn't require a specific score.",
                    "label": 0
                },
                {
                    "sent": "Now when we look at the images that we're dealing with, we see that there's lots and lots of repetition, and so our key insight for this work is that we can learn a better ordering based on previous matches.",
                    "label": 1
                },
                {
                    "sent": "We can learn that for these specific images perhaps certain SIFT feature points are often inliers, an other.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things are often not.",
                    "label": 0
                },
                {
                    "sent": "The way we do this is we first construct a visual vocabulary.",
                    "label": 0
                },
                {
                    "sent": "This is a very standard technique and we do it by running K means on a random subset of points ahead of time.",
                    "label": 1
                },
                {
                    "sent": "You could also construct a visual vocabulary for all.",
                    "label": 0
                },
                {
                    "sent": "Natural images, theoretically we do it per data set, but it could be.",
                    "label": 0
                },
                {
                    "sent": "Start one visual vocabulary and use it for all your tests.",
                    "label": 0
                },
                {
                    "sent": "Then once you have this visual vocabulary, you carry out standard geometric verification for a pair of images.",
                    "label": 1
                },
                {
                    "sent": "You match features and run our stack.",
                    "label": 0
                },
                {
                    "sent": "If you have a verified match or a good model, then you update the counts for each visual word corresponding to any point that was an inlier.",
                    "label": 0
                },
                {
                    "sent": "We maintain these counts between ransack rounds and we use them as a priority ordering for prosak like sampling.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give you a better idea of how this works.",
                    "label": 0
                },
                {
                    "sent": "As you match as you do your RANSAC steps.",
                    "label": 0
                },
                {
                    "sent": "I don't know why it's going so fast as you do your ransack steps.",
                    "label": 0
                },
                {
                    "sent": "You will, let's say at 10 images you'll have a visual count like this and then as you match more and more images you have more an more peaked distribution and each one of those visual words along the bottom.",
                    "label": 0
                },
                {
                    "sent": "It's an unordered list.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to account and hopefully the higher counts will be better features and when you run your ransack, the accounts with the most or the feature points with the most count will be prioritized and sampled from fir.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to test this, we use the Berlin Dome data set that we have.",
                    "label": 0
                },
                {
                    "sent": "It's 10,000 images of the Berlin Dome.",
                    "label": 0
                },
                {
                    "sent": "We got it from a Flickr query as well, but it's quite clean.",
                    "label": 0
                },
                {
                    "sent": "We randomly choose K image pairs and carry out geometric verification.",
                    "label": 1
                },
                {
                    "sent": "Accumulating counts for visual words, visual vocabulary of 20,000 words.",
                    "label": 0
                },
                {
                    "sent": "So for a given image we detect feature points and then the process sampling could be thought of as thresholding only feature points where accounts are above some threshold and the threshold is lowered.",
                    "label": 0
                },
                {
                    "sent": "If no model is found.",
                    "label": 0
                },
                {
                    "sent": "Here is a visualization of which feature points.",
                    "label": 0
                },
                {
                    "sent": "Get thresholded the larger the circle, the more counts for that feature point.",
                    "label": 0
                },
                {
                    "sent": "The main takeaway is the larger circles are usually in the center of the building and there are areas that are likely to verify against other images.",
                    "label": 0
                },
                {
                    "sent": "Also, you'll notice that there are no longer any feature points in the Sky and few on the ground where it's less likely to have inliers for your fundamental metrics estimation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We ran three experiments to compare our technique on this data set.",
                    "label": 0
                },
                {
                    "sent": "The first is just regular ransack.",
                    "label": 0
                },
                {
                    "sent": "The second was RR sack with the SIFT based matching scores as the ordering.",
                    "label": 0
                },
                {
                    "sent": "You could think of this as close to Prozac, but note we have other optimizations there as well, and the third step, the third test was our sack with the proposed ordering.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the mean time for image pair and the mean mean time as per image, barren mean number of hypothesis per image went down as well as the total runtime, indicating that there isn't a large overhead for this as well.",
                    "label": 1
                },
                {
                    "sent": "I like to note that there is no dedicated training phase here for our three.",
                    "label": 0
                },
                {
                    "sent": "We start with a 0 count and it's just updated as we get more and more matches.",
                    "label": 0
                },
                {
                    "sent": "Empirically we found that after about 500.",
                    "label": 0
                },
                {
                    "sent": "Valid matches or 500 models that we found.",
                    "label": 1
                },
                {
                    "sent": "We start to get performance better than using the SIFT matching scores and it seems that after about 1000 images we've.",
                    "label": 0
                },
                {
                    "sent": "Found are optimal.",
                    "label": 1
                },
                {
                    "sent": "So the main observation from this smaller test set is that sorting visual words based on the results of previous rounds is desirable.",
                    "label": 1
                },
                {
                    "sent": "It gives extra information about which features are useful and speeds up geometric verification.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we the the goal of this was to speed up on very large scale datasets, so our second experiment was on the Berlin data set.",
                    "label": 0
                },
                {
                    "sent": "This is 2.7 million images from Flickr query Ann is very noisy as you can see along the bottom.",
                    "label": 0
                },
                {
                    "sent": "Here we use the building Roman, a cloudless day framework.",
                    "label": 1
                },
                {
                    "sent": "I'll cover that quickly.",
                    "label": 0
                },
                {
                    "sent": "We cluster images into 100,000 clusters using visual descriptor.",
                    "label": 1
                },
                {
                    "sent": "In this case it was a binarized just.",
                    "label": 1
                },
                {
                    "sent": "And we performed geometric verification only on each cluster separately.",
                    "label": 1
                },
                {
                    "sent": "Any image that was in cluster one would not be verified against any image that was in another cluster.",
                    "label": 0
                },
                {
                    "sent": "And then clusters with three or more verified images are kept there, called iconic clusters, and the most verified images, called the iconic all unverified images, are discarded.",
                    "label": 1
                },
                {
                    "sent": "And for this data set, we accumulate counts for visual cabul airy of 1,000,000 words.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we only ran two experiments that we did not run standard ransack, so we ran our sack with the ordering based on SIFT matching scores and our sack with our new proposed ordering based on visual word counts.",
                    "label": 1
                },
                {
                    "sent": "Again, we get a speedup for mean time.",
                    "label": 0
                },
                {
                    "sent": "For image Parent total runtime, though it's not as significant speedup as the other data set.",
                    "label": 0
                },
                {
                    "sent": "Also notice that we have slightly more iconic and registered images.",
                    "label": 0
                },
                {
                    "sent": "This is likely due to noise and not the fact that we're actually getting better matches.",
                    "label": 0
                },
                {
                    "sent": "But only 5% of the entire data set has been registered.",
                    "label": 1
                },
                {
                    "sent": "Now the reason the speedup is not quite as large as in the previous data set is there are many more attempted ransacks that fail, and when RANSAC fails, both of these methods have approximately the same runtime.",
                    "label": 0
                },
                {
                    "sent": "But we found that in the cases where there is a very low inlier count, this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm excels quite a bit, so for this image we did go back and run RANSAC, and we also had our sack with the matching scores in our sex with our inlier counts, and we get a very significant speedup.",
                    "label": 0
                },
                {
                    "sent": "We went from about 200,000.",
                    "label": 0
                },
                {
                    "sent": "20,000 matching score, 20,000 hypothesis using the matching score to just under 600.",
                    "label": 0
                },
                {
                    "sent": "And that's because the while the matching scores may be high between points, the our algorithm has learned that the center of the building, the points at the center of the building are most likely to be inliers, and so it looks at those first.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But now we'd like to address the completeness problem.",
                    "label": 0
                },
                {
                    "sent": "Only 5% of the images have been registered this far, and we have 2.6 million images.",
                    "label": 0
                },
                {
                    "sent": "Still not registered in the first step.",
                    "label": 1
                },
                {
                    "sent": "Our first attempt to do this was to retrieve the 20 most visually similar iconic images and perform geometric verification on those against each non verified image.",
                    "label": 1
                },
                {
                    "sent": "This process alone take took one day and 10 hours were doing.",
                    "label": 1
                },
                {
                    "sent": "Roughly, 40,000,000 verification rounds and this is too slow.",
                    "label": 0
                },
                {
                    "sent": "But our second key insight is that we have a example of good images thus far in our iconic, so we can learn what a good image looks like and then use this knowledge to only match good images.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do this by training what we call a landmark classifier.",
                    "label": 0
                },
                {
                    "sent": "The positive examples are the iconic images that we've verified and the negative examples are just a random set of rejected images, roughly the same size as the number of Iconix as you see along the top row of images, those are the positive training examples, and even there we have some noise, so there's a band and there were a number of identical images that geometrically verified of the same band, and in addition, in the negative training images there are.",
                    "label": 0
                },
                {
                    "sent": "Some images that do look like they could be landmark images, and in fact we hope so because these are the images we hope to verify, but the large majority of them are not landmark images.",
                    "label": 0
                },
                {
                    "sent": "So we use our dictionary to compute a bag of words, histogram and so this is already precomputed, so computing this feature is free essentially, and while this feature has a million dimensions, note that each image only has roughly 2000 feature points matches, so it's quite sparse with only about 2000 words.",
                    "label": 1
                },
                {
                    "sent": "And we train a linear SVM that takes advantage of the sparsity an IT trains in about 5 minutes, so it's very quick to train.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our new methodology is to verify only the rejected images that are classified as landmarks and so this reduces the set of images that we're going to verify to about 26%, giving us a trivially close to 4X speedup.",
                    "label": 1
                },
                {
                    "sent": "Now, as you can see, there are some images that if we did full reverification we missed because we're only verifying on 26% of them, roughly 70,000, but we reduce our one day 10 hours to just a 10 hours.",
                    "label": 0
                },
                {
                    "sent": "We also argue that perhaps it's good that we do not verify those images.",
                    "label": 0
                },
                {
                    "sent": "Row along the top is the highest scoring images that do verify, so they verify in both cases and the row along the bottom are lowest scoring images.",
                    "label": 0
                },
                {
                    "sent": "The ones that the SVM does reject.",
                    "label": 0
                },
                {
                    "sent": "But if we were to do full reverification would actually verify and notice that perhaps there's only one that we would want in their one landmark images.",
                    "label": 0
                },
                {
                    "sent": "The rest are images that are probably duplicates or very similar images that end up verifying but are not of landmarks.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, we found that combining recognition and geometry methodologies enables fast and more complete registration.",
                    "label": 1
                },
                {
                    "sent": "Learning these priors based on match history rather than just image to image matching scores does capture useful information.",
                    "label": 1
                },
                {
                    "sent": "And using fast pre classification can greatly decrease the runtime while minimally impacting the matching performance.",
                    "label": 0
                },
                {
                    "sent": "Thank you questions.",
                    "label": 0
                }
            ]
        }
    }
}