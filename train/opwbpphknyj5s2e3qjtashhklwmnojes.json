{
    "id": "opwbpphknyj5s2e3qjtashhklwmnojes",
    "title": "Introduction to data modelling",
    "info": {
        "author": [
            "Tony Dodd, Department of Molecular Biology and Biotechnology, University of Sheffield"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Events"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_dodd_idm/",
    "segmentation": [
        [
            "Right then.",
            "OK, let's make a start again, um.",
            "So this is the first principle sort of proper session oversight to get into.",
            "The technical bits are, So what I'm going to cover?",
            "First thing this morning is what I've called linear models.",
            "But that's perhaps not the best term as you come on to see 'cause actually.",
            "Quite a bit of what's going to come up turns out to be nonlinear OK, but there is.",
            "Again, I will highlight why."
        ],
        [
            "I've called this sort of linear as we go through.",
            "So.",
            "We can talk about number of difference.",
            "Sort of linear models explains I go through what I mean by that.",
            "The first one we look at will be the classic real linear model.",
            "We will look briefly at.",
            "How you estimate the parameters?",
            "That part of it fairly familiar to a lot of you.",
            "We then start getting into.",
            "The nonlinear linear bits OK, and that's where we start talking about what are called linear in the parameters models and where we look at applying well, I've called linear models in classification.",
            "And talk about linear models and classification really is not the way to put it.",
            "Rob will get very angry you angry with me.",
            "And then I'll talk briefly at the end about some real real nonlinear bits.",
            "OK, so although some of this you can do linear, there are bits even in a purely linear model.",
            "There are some things which are still."
        ],
        [
            "Nonlinear and I'll cover those at the end.",
            "OK.",
            "So.",
            "In Kerr fishing.",
            "Um?",
            "This is the probably the simplest model you could ever use.",
            "I suppose the simplest model is that as your data is a constant value.",
            "OK, so if we ignore that, then the simplest possible model you can ever get is a linear fit.",
            "Where?",
            "You have your input variables are just added together by some weighted sum.",
            "So we have a set of weights WI multiplied by the excise.",
            "OK, in the exercise of the components of the input X and then with some those together.",
            "So XI is the ice components of inputs X.",
            "We assume that X nought is equal to 1.",
            "And therefore W nottz is the bias.",
            "So if you only had that one term, if you only had that W noughts and then strictly that'll be the simplest possible model, because it could be that the data is simply a constant value.",
            "But then the natural generalization of that is this linear model that we've got here.",
            "OK, so it's just weighted sum of the input variables.",
            "Is everyone happy with that equation?",
            "And.",
            "I say all we can do with that is represents lines and planes.",
            "OK, so we can do nothing more than represent lines in two dimensions.",
            "So like the examples that I've shown you, or if you start getting into higher input dimensions.",
            "So an input dimension greater than one.",
            "Then we're representing 234.",
            "However many dimensional planes, but they are linear OK, they're flat, and I said.",
            "That's all we can represent.",
            "But actually it may not be as restrictive as you think.",
            "OK, 'cause very often in the real world things can be quite linear and you can do surprisingly well with linear models.",
            "And this always creates a problem when you supervise a PhD student on an application area.",
            "Anything we do some really exciting work and the first thing they do is they try a linear model.",
            "And it turns out to fit extremely well.",
            "I mean, I have to think well, where does the PhD go from here or research go from here?",
            "But that shouldn't stop you.",
            "Fitting linear model and you should always fit.",
            "So if you're looking at curve fitting, first thing you should always do with your data is fit a linear model.",
            "If you're looking at classification, pattern recognition or density estimation, again you should always try the simplest possible model, or a very very simple model.",
            "OK, within data modeling OK, simple, always do the simplest possible model that will describe your data as well as you need it to be described OK, there's no point in going into.",
            "Advanced nonlinear techniques.",
            "If you don't need them.",
            "OK, firstly it's a lot of effort that's not needed and Secondly, you will actually probably end up with a worse model.",
            "Then the linear one.",
            "OK so always try a very very simple models to start off with.",
            "If that works well you can stop.",
            "You can move on to something else if it doesn't work particularly well, at least it's got a bench.",
            "You've got a benchmark to compare against, so again, when you start applying all these fancy kernel methods, support vector machines, Bayesian methods from later on in the week.",
            "You know, if they don't do any better than the linear model.",
            "Again, there's no need for them.",
            "OK so I always have that as a benchmark and it may be that that's all you know."
        ],
        [
            "It.",
            "Um?",
            "Primus restoration and what I'm going to talk about today and the everything that we're going today is about least squares parameter estimation tomorrow and perhaps later on the week that other types of parameter estimation will we talked about, but for the purposes of today.",
            "We're focusing on least squares type estimation methods.",
            "That's not when we come to classification.",
            "That's not strictly true, but it still has that flavor.",
            "So we're going to choose our parameters, so our weights.",
            "So Wis we need to estimate them in some way and we're going to estimate them.",
            "By minimizing the sum of squared errors.",
            "And you sometimes see you know this.",
            "Maybe pre multiplied by 1 / N or one over Sigma squared N where Sigma squared is the expected noise variance.",
            "But it doesn't rain that those terms in front are just constants, don't really affect it, so that the key thing is that we're trying to minimize.",
            "The sum of squared errors.",
            "It's on the data points so.",
            "I've already spotted that my notation is wrong, 'cause that should be.",
            "That shouldn't be why.",
            "Because why is that true?",
            "And we don't even know that.",
            "So pretend that the Y is the output from the model.",
            "Does that mean that my so that's wrong as well, isn't it?",
            "So strictly there, why?",
            "Why should be in F hat?",
            "Sorry bout that.",
            "So.",
            "Y of XI should be F hats of XII is the output from our estimated model.",
            "And the sad eyes are.",
            "Observed noisy data, so we're trying to minimize.",
            "The deviation on our data points between the model output and the measured output, so we're trying to drive that value to 0.",
            "Sum of squared errors to 0.",
            "Does anyone think that's a good or a bad?",
            "Performance measure.",
            "Who thinks it's a good one?",
            "Yeah, those are all valid points, so I mean.",
            "What we're doing is, we're assuming that the noise is Gaussian OK, in terms of this being optimal.",
            "OK, so there is an implied assumption that the noise is Garrison, but it rarely is, and it's still fairly good.",
            "I mean, it's still a reasonable first attempts and there are, you know there are other performance measures you can use, but there tends to be much more complicated to work with.",
            "This is a very, very simple one to work with.",
            "OK, the other point, overfitting is a big big problem.",
            "If all you do.",
            "Is minimize the sum of squared errors?",
            "Then there's a high likelihood that you will overfit your data.",
            "What you'll end up doing is you'll end up modeling not just the underlying true function F. But you will end up modeling the noise and fitting the noise as well, and we don't want to do that.",
            "We want to fix.",
            "We filter out the noise.",
            "OK, so we'll come onto ways of overcoming that later on, but we'll stick with this at the moment.",
            "If your model is not that complicated.",
            "So for a purely linear model.",
            "This is a good thing to do because in a pure linear model is so simple.",
            "That it won't overfit the noise in the data.",
            "OK. Because it's so simple in itself, but if your model is slightly more flexible as the models that will come on so in a minute, then there is the potential that they will overfit the data.",
            "So always be a bit wary.",
            "When you just presented with just minimizing the sum of squared errors.",
            "Not always the best thing to do.",
            "The advantages advantage of it though, is that it has a unique global minimum.",
            "Um?",
            "When we're looking at linear models.",
            "OK.",
            "If you're looking at other more complicated cipher models and multilayer perceptrons that Rob will be talking about later on their reasons.",
            "A single global.",
            "Unique minimum.",
            "OK, you may get multiple men and multiple local minima as well.",
            "Um?",
            "This is optimal when the noise is Gaussian, and in that case it also corresponds to what's known as the maximum likelihood estimate.",
            "So when you start getting into more of the probability, there's an alternative interpretation of this as the maximum likelihood parameters I there the promises that are most likely given the data that you've got.",
            "Again, you learn more about maximum likelihood and stuff later on in the week anyway.",
            "Are there any questions on?",
            "The promised rest, yeah.",
            "Optimum optimum might be options.",
            "Probably the wrong word, isn't it op tits?",
            "Yeah, I mean if if you're yeah if you have the right model, the noise is truly Gaussian.",
            "You know the noise characteristics then this will be the optimum.",
            "Performance measure C. Use OK out of all possible performance measures, or perhaps to be better to think of this would be the best one to use.",
            "OK, but there are a whole host and certainly will learn tomorrow when you do support vector machines.",
            "There are lots of other different types of cost functions that you can use.",
            "They can be considered to be optimum for other types of noise characteristics, but actually they are often used because they are more robust when you don't know what the noises.",
            "OK, I think that the key thing here is.",
            "We normally use the sum of squared errors because it's simple.",
            "It's very well known and as we'll see in a minute, it gives a very neat solution.",
            "OK. Any other questions?",
            "What?",
            "What is?",
            "Scansion on direction is when so when you've got noise.",
            "You noise has a distribution function.",
            "Have you done anything on probability before?",
            "So gation just means that the noise has a Bell shaped.",
            "Probability density function.",
            "Which is centered around the mean, so it has a mean and has a variance associated with it, and what it basically means is that the noise is most likely around the mean, and it's less likely as you get further away from the mean, but it has a very particular shape, so how that trend goes?",
            "The noise.",
            "It's zero, we normally assume it's B0 mean.",
            "Size.",
            "You never, you can't.",
            "You don't minimize the noise.",
            "You filter the noise.",
            "So if you I'll show you an example in a minute which sort of.",
            "You're trying to.",
            "You want to model the true function.",
            "Which is sort of ignoring the noise it's you should think of it.",
            "I, I think of it as being filtering the noise.",
            "The problem with, as someone pointed out the problem with using this performance measure, is that you will end up fit if you got two complicated and model, you will end up fixing the noise and I'll show you an example of that in a minute and Rob will sort much more about this later on.",
            "So will come."
        ],
        [
            "So that in a minute.",
            "That is an example, so our cost function is just a nice bowl shape.",
            "So in that's in.",
            "If you got two weights then you get a nice bowl shape and you see a nice unique global minimum here.",
            "OK, no local minima.",
            "Everything is nice and what will happen, hopefully is you will find the parameters corresponding to that point."
        ],
        [
            "So how do we find the premises?",
            "What we do is we define something called a design matrix.",
            "And design matrix is constructed in this way.",
            "So since we've got a set of biases, we put a zero of one's in the first column.",
            "And then for each input variable, so we got M little M input variables and we have big N. Measurements or data points.",
            "So we populate this matrix in this way so each column.",
            "Is the N big N values of the little NTH?",
            "Input variable.",
            "And we populate the Matrix in that way.",
            "The simplest way to think about what these entries are is in terms of.",
            "Is it a big N or little M associated with that row or column?",
            "That sort tells you.",
            "How will populating it?",
            "Then we can write.",
            "Our observations.",
            "As being equal to the Matrix Phi.",
            "Times are parameter vector W. So that.",
            "Is also noise free output, predicted output, and then we now have a vexor.",
            "Of errors of vector of noise associated with that.",
            "So this becomes a vector equation.",
            "So the these says will have N big N components associated with.",
            "That will be a vector of all of our.",
            "Target observations.",
            "This will be again a vector of N. Model predicted outputs and this will be a vector of all of our noise.",
            "Is everyone happy with Lance?"
        ],
        [
            "Once we've done that.",
            "We can write the sum of squared errors."
        ],
        [
            "Low.",
            "Our equation here."
        ],
        [
            "Can Alternatively be written in the form of the first line.",
            "So we've now written it in matrix vector notation.",
            "But they are completely identical.",
            "What we can then do is we can expand out the sum of squared errors.",
            "So using straightforward matrix notation rules so we get as their transpose AD minus their transpose, fire W, and so on.",
            "We can combine the second and third terms so zed transposed by W&W, transpose, fire, transpose Ed are actually identical terms.",
            "So we can combine them together into the two zed transposed by W. So we now have our sum of squared errors equals to the third line.",
            "So minimize this.",
            "As in standard calculus, you simply.",
            "Calculate the derivative with what you want.",
            "I've driven with specs which in this case is W and set it equal to zero.",
            "I'm not going to go into how you do matrix vector differentiation.",
            "OK, if anyone wants to learn about it, they can look it up on the web, but we haven't got.",
            "We probably need a whole day for me to explain the rules of doing and proofs of how you do matrix vector.",
            "Differentiation, but if you just trust me that if you differentiate with Ed transpose, Ed doesn't even depend on sub W, so we get rid of that one if we differentiate to zed transpose 5 W then it just becomes two zed transfers Fi.",
            "Which in a sense it looks like normal differentiation.",
            "For that one you just get rid of the W. At the site Exchange, one is the W transpose.",
            "Fire transfers find W. If you differentiate that with respect to W. Then you get to W transpose FIE, transpose Phi.",
            "OK, and these are well known rules within linear algebra for doing matrix differential matrix vector differentiation.",
            "If you then rearrange, you get the W transpose fire.",
            "Transverse Phi equals said, transposed by an.",
            "If you then take the transpose of all of this equation and then pre multiply by the French Fire transpose Fi, you end up with.",
            "Your optimum weights in the sense of minimizing the sum of squared errors is equal to what's known as the matrix pseudoinverse, times zed.",
            "So this whole thing here.",
            "Is known as the Matrix pseudoinverse.",
            "OK, and that's equal to five transfers.",
            "5 awesome minus 1 * 5 transpose.",
            "And then you multiply that by your vector of observations of the data and that will give you.",
            "Your parameter estimates.",
            "Are most of you familiar with that equation?",
            "Is everyone happy with that equation?",
            "Good."
        ],
        [
            "OK, so we're going to go back to our example again.",
            "So here we have a set of data points that have been observed.",
            "So input X along.",
            "This direction and why output up.",
            "The screen, so we reserve some data.",
            "I've made an assumption that I think that data can be modeled by a linear model."
        ],
        [
            "So I calculate the price restaurants and that's what I get by minimizing."
        ],
        [
            "Some squared errors compared to the true function.",
            "Which is the dashed line.",
            "And you'll see here that although we've minimizes sum of squared errors, we've certainly not overfitted the noise in the data.",
            "OK, and I would argue that's a pretty good.",
            "Estimate of the true function.",
            "And hope yeah, I'm wondering.",
            "Are you interested in the function that models the data?",
            "Oh, you're not a physicist.",
            "Um?",
            "It depends what you're interested in, and it depends on your reasons for doing data modeling.",
            "Most people that do data modeling are simply interested in getting good predictions.",
            "So.",
            "In a sense, if they don't, if they don't model the true underlying.",
            "Relationship between the inputs and outputs, but still get good predictions.",
            "They will be happy.",
            "Um?",
            "So.",
            "It depends.",
            "I mean you will.",
            "You probably you will never ever, I suspect, find the true function because we don't know what the true function is.",
            "And the chances that you happen to stumble on choosing exactly the correct class of models that correspond to the true function is that probability is 0.",
            "I would say.",
            "And in a lot of cases there is no true underlying function.",
            "You know, if you're trying to model some gas turbine engine, there's not really.",
            "A true function there.",
            "I mean, I suppose this sort of is, but you know, there's no equation that we can write down or anything.",
            "So, and this is, I guess this is one of the problems with data modeling is that there never is a real truth.",
            "For what we're aiming for, where all we're really looking to do is to get good predictions, and that's why some stuff I'll talk about briefly at the end.",
            "But more So what Rob will talk about in terms of how?",
            "How do you assess how good your model is?",
            "That becomes really important because you also have to use tricks.",
            "To try and work out how good your model is, because you never know what the truth is.",
            "Is that answer?",
            "So like, yeah, there's any other questions.",
            "OK.",
            "Yes.",
            "Value.",
            "Maybe?",
            "Do you have?",
            "Do you have variation in the input as you have noise on your inputs as well as that?",
            "Yes.",
            "Over and over again.",
            "Maybe?",
            "Yes.",
            "Point like an average.",
            "Why don't why do you use the average?",
            "Why don't you use?",
            "Well, I mean, I mean something that we're not going to talk too much about his pre processing of data.",
            "I would assume that you would use all of your all of your data in its raw form.",
            "But there are there are cases where it may be advantageous.",
            "To do some form of pre processing so to perhaps average the data beforehand.",
            "I'm going to in effect what you're doing.",
            "If you're averaging the data before you put it into the model you're trying, you're trying to get rid of the noise.",
            "And then there's uncertainty effects before you do the model.",
            "And that may help you when you it may make it easier to do the modeling.",
            "The alternative way of doing it is just give the model the raw data and let the model try and filter out the noise.",
            "Often by doing a bit of intelligent pre processing.",
            "You can help your model an awful lot to rely on the model to do all to do everything can sometimes be a bit too much just by a bit of preprocessing.",
            "The classic things to do are just.",
            "Standardizing your data.",
            "Putting the data into an interval zero to 1.",
            "It can have a huge advantage in some just doing some simple scaling of the data that it just makes the algorithm.",
            "It tends to be that's all associated.",
            "This sort of conversation that's going on that tends to be so perhaps more well conditioned or well, better behaved.",
            "OK, but the point is by doing some preprocessing beforehand you can often help.",
            "Your data model, your model before you start.",
            "I'm not sure I fully answer your quest."
        ],
        [
            "OK, so how can we generalize that very simple linear model that we just had?",
            "Will the simplest way of generalizing it is to take almost the identical equation, but now throw in that Phi I. OK, so instead of WI times XI we take some nonlinear transformation of the X is.",
            "And that knowledge transformation is done via the faize.",
            "OK, I'll show you an illustration what's going on there.",
            "So what we've done we we critically we still have this model being linear in the parameters, so it's still a sum of the Wis times defy eyes, so it has a very similar form to what we have, the only difference being that we formed a nonlinear transformation of the ex is before we do the linear combination.",
            "Is that a typo or is it?",
            "Yes, that is a typo.",
            "It should be fine I X.",
            "That's right, yes.",
            "So each each by each basis function you have a number of basis functions and they all act on X.",
            "Yes.",
            "This is the problem with being videoed.",
            "Is all my errors.",
            "Going to go on the web now.",
            "Um?",
            "Yeah, I'm not obviously you will do this over a number of inputs X as well, so you know there are also putting on, yeah.",
            "So what we do is we're forming a nonlinear transform of the inputs and then we form a linear model.",
            "So in that sense, we can still call this a linear model, but it's still linear in the parameters, but it's starting to get a bit tenuous now to call this a linear model.",
            "OK, 'cause actually it's a nonlinear input output relationship."
        ],
        [
            "The advantage is.",
            "We can still apply the simple estimation that we did before to finding the parameters.",
            "OK, so although this is a non linear model in terms of basis functions, it's nonlinear input output relationship.",
            "It is linear in the."
        ],
        [
            "Analysis, and therefore we can still apply the same.",
            "Sort of cost function that we did before and I'll show you that in a minute.",
            "What I wanted to introduce here's just quickly is.",
            "This is the sort of support vector machine way of thinking about doing these nonlinear transformations.",
            "OK, I put these in now 'cause it may help you a bit for tomorrow.",
            "So what we've got here?",
            "Is a set of data, and that's actually been generated from sort of squared type relationship, so it is nonlinear.",
            "Perhaps not the best example, 'cause it looks quite linear.",
            "What we do and these are our inputs X."
        ],
        [
            "We transform them.",
            "Into a space.",
            "So into the sort of basis function space.",
            "Where they become nice and linear and we can fit a linear model in that space so we can sort of think of here, these are our inputs X.",
            "Here these data points are now are Phis axing on the X is.",
            "OK, so these are sort of basis function data points.",
            "And we can form the linear model in that basis function datapoint space.",
            "We do that format."
        ],
        [
            "The model, but then when we transform back.",
            "We actually get in.",
            "Actually it was a sine wave that is generated from.",
            "We get this sine wave relationship in terms of the input output relationship, so it's linear.",
            "In the basis function space, but it's nonlinear in the inputs output space."
        ],
        [
            "I don't want to dwell.",
            "Too much on that.",
            "'cause you'll learn more about lats tomorrow, where this or nonlinear transformations become far more important in terms of understanding support vector machines."
        ],
        [
            "So if we return to doing our parameter estimation, we gain form up a design matrix and we do it in this way now, so yeah.",
            "Yeah."
        ],
        [
            "Yes.",
            "Comes to that later or I will sort of come to that later.",
            "But there is problem is there is no answer.",
            "OK, but I will.",
            "I will touch on that at the end of this lecture.",
            "OK."
        ],
        [
            "But yes, it's a good question.",
            "And no one has solved it.",
            "Um?",
            "So we now define our design matrix in this way.",
            "So we gain we populate it now with our basis functions.",
            "So we got the five one to five M. So acting on X1 to XN.",
            "OK, so acting over all of our big N data points that we've measured, so we form that matrix and that we, uh, M by N size matrix.",
            "And then we estimate the parameters in exactly the same way.",
            "That we did for the pure linear model.",
            "So this is the beauty of this approach.",
            "So modeling nonlinear input output relationships.",
            "Is that in terms of doing the estimation?",
            "Everything is the same, so that's why I sort of bash this under the linear models.",
            "Because in terms of the important, but it still is linear, yeah?",
            "Number one.",
            "You could in we.",
            "Typically when we start moving into basis function models like this, which typically start ignoring the bias, you could still put in.",
            "You could still treat the bias as another basis function and keep the bias in there, but normally.",
            "Or these models will be flexible enough that they will model the bias themselves?",
            "That is all they will accommodate the bias.",
            "Um?",
            "No, you have an.",
            "No, you've only got you only ever have big N observations.",
            "In both cases, what's happened now is here, I suppose, in the first in the linear model it was dimension little M + 1 by Big N, and now it's just little M. By big N. But actually all of trust me, the the dimensions do fit OK. Yep.",
            "Yes.",
            "Yeah.",
            "Yep."
        ],
        [
            "So an example now is we're going back to our sync function."
        ],
        [
            "So pretty much similar sort of data to what we have before that was the fit.",
            "That I formed by some basis function model."
        ],
        [
            "Which will come on.",
            "So in a minute and there is the true function.",
            "And we just happen to know the true focus.",
            "I've generated it.",
            "In this case an reasonable if it's not particularly good.",
            "In that case, I.",
            "Argue.",
            "Um?",
            "The thing is."
        ],
        [
            "How?",
            "Was that model forms where it is formed because I chose?",
            "A radial basis function neural network with Gaussian Bell shaped basis functions.",
            "And I've got 123458 online, maybe 10 of them.",
            "There is a spread across the input domain."
        ],
        [
            "And what we do is we multiply.",
            "We learn the weights, so we multiply each of those basis functions by its respective wait.",
            "And now these are the weighted basis functions as there are positive and negative weights.",
            "And then what we do is we just."
        ],
        [
            "Has all of those lines together?",
            "And that gives us the final.",
            "Function estimate.",
            "So it's simply always trying to represent is actually simply a weighted linear combination of the basis functions.",
            "Does everyone yeah?",
            "That's another question that's just as awkward as choosing the basis functions in the 1st place.",
            "And again I will mention it, but not answer that question abit later on.",
            "OK, so it comes another yeah.",
            "Another very very important question that no one has answered.",
            "Also, I must have it through this week.",
            "There will probably be more unanswered questions and answer questions.",
            "You will probably find.",
            "This is why there's still so many of you doing research and data modeling.",
            "Um?"
        ],
        [
            "OK, here's an example finally of overfitting.",
            "So here's an example where it's gone horribly wrong, so I've managed to force this one to go really badly.",
            "OK, and I hope you would agree that the solid line there is not a terribly good representation of the true function.",
            "But more on the."
        ],
        [
            "Later.",
            "OK, um this is the bit where the title of the slide is not quite right because I've talked about linear classification and I will need to clarify what I mean by linear classification and what I mean by linear classification.",
            "Certainly initially is that we're getting linear decision boundaries.",
            "OK. We will actually have to use nonlinear techniques to get linear decision boundaries, but when I talk about here linear classification, what I mean is forming linear decision boundaries.",
            "I'm not.",
            "There are a number of different techniques for forming linear decision boundaries are not going to go into all my.",
            "Just going to briefly go through some basic ideas here, but the key point now.",
            "Is our targets are not on the real line.",
            "Our targets are now what is known as categorical, so they belong to class One class two Class 3.",
            "So using the notation I came up with earlier, outputs will be zero or one.",
            "So it's starting to get slightly different now, so you can apply techniques like discriminant analysis, probit analysis, log linear regression, logistic regression, and this is where again, This is why.",
            "When I was talking about the examples in my first lecture and I sort of said the first 2 examples were regression, that wasn't the best term to use their really their curve fitting because actually regression type type techniques can be used or are regularly used within classification.",
            "OK, I'm going to actually what we're doing here is these are regression for categorical outputs.",
            "Yep.",
            "Hello.",
            "I don't know, do you know, Rob?",
            "For regression.",
            "The reason?",
            "Just.",
            "No.",
            "With the kinds of costs.",
            "I think the simple answer is.",
            "As in any area, data modeling is full of abuse of notation.",
            "And terminology and people use regression.",
            "When they don't necessarily mean it and they use.",
            "I mean not only my experiences regression.",
            "Is most often used to refer to Curphey sing on you agree price?",
            "I'm wrong?",
            "Perhaps that maybe me being poor in my notation.",
            "Actually estimating.",
            "Estimated.",
            "Yeah, let's not dwell on it.",
            "So the key thing here is our aim is to get a linear decision boundary, and we'll see that we could generalize that in the same way we did for the curve fitting.",
            "By using basis functions as."
        ],
        [
            "Well, it's getting nonlinear decision boundaries, so.",
            "What we do is we now assume that our data is Bernoulli distributed, so we now.",
            "Because we're only looking at ones and zeros, we can't now have Gaussian noise.",
            "On our targets, so when something is wrong, what it means is that we predicted to be class one, but actually it was Class 0, or we predict it's for class zero and actually it was Class 1.",
            "OK, so the difference is 1 either way, so we were no longer adding in a continuum of Gaussian noise.",
            "So this is known as Bernoulli distributed in the sense that it can only take the data can only take one of two possible values.",
            "And the noise will be associated with with that fact.",
            "What we then do is we do actually form a linear model.",
            "But we form.",
            "The linear model here.",
            "On what is known as.",
            "The probe it or the logic sorry notation.",
            "Also the logic which is the log of P / 1 -- P where P. Is the probability that your output is equal to 1 given X?",
            "So what we're doing is we're now we're not actually modeling.",
            "The targets are all the measured data directly in terms of our linear model.",
            "We're now actually modeling this logic function, he which is defined in this way, so it's it's the probability that y = 1 given X, so we've now had.",
            "Again, we've done a transformation.",
            "Affective our data.",
            "To model something slightly different, but it allows us to still apply a linear model.",
            "Yep.",
            "Color.",
            "Yes.",
            "But this is remember, this is your summing together here I'm.",
            "Looking through as I'm going through this, I like the notation.",
            "I've been slightly sloppy with notation for the exercise in terms of in some cases I think I've used XC to refer to.",
            "The end different sexes and sometimes I've used it to refer to the M variables of X.",
            "Unfortunately, I've.",
            "Not being brilliant with my card, not being perfectly consistent with my notation here.",
            "No little M. So what this is so?",
            "This is for a single input.",
            "It's a way to sum over its variables.",
            "Yep.",
            "Yeah.",
            "I say exactly what we're doing.",
            "Which will become again, slightly clearer when I show you an example in a minute.",
            "So yes, and now.",
            "So what we're doing is we're doing effectively a regression.",
            "On the probabilities rather than.",
            "On the data itself.",
            "Is that fair to say that a fair comment?",
            "Yeah.",
            "And this thing here.",
            "This just comes about if you reverse this equation arounds then P is equal to becomes equal to this thing here.",
            "So that's the that's what the probability is equal to.",
            "Which becomes useful if you want to play."
        ],
        [
            "The surface is we'll see in a minute.",
            "So can we generalize this where we can generalize it in exactly the same way we did for curve fitting instead of having this purely linear model?",
            "Here we can track, we can replace it with a basis function model instead.",
            "So.",
            "Whereas.",
            "In this hot case, you could only get purely linear straight lines or planes.",
            "Decision boundaries in this case by.",
            "Expanding is the basis functions first.",
            "You can then get nonlinear decision boundaries.",
            "I kept this in here because so many were still using an inherently so linear looking model.",
            "Although you know we're now getting quite nonlinear in terms of what we're really doing well, certainly we're now getting nonlinear input output relationships."
        ],
        [
            "So again, we can think about this in the sort of kernel community way in terms of mappings.",
            "So say we've got some classification data, so we've got two clusters of points and Dotson, some X is.",
            "And it looks like there's probably a nonlinear relationship that will separate those two clusters of data points."
        ],
        [
            "What we do?",
            "Is we map it into through the basis functions into a new space where they become linearly separable.",
            "So now our clusters of data points are the basis function.",
            "Data points in affects we form a linear decision boundary and."
        ],
        [
            "That space.",
            "But then when we come back to our standard space.",
            "It becomes a non linear decision boundary again or not as well too much on this.",
            "Yep.",
            "Yes.",
            "Note you can do so, so there are.",
            "I mean the classic theorem is what's known as the stone via Strauss Theorem, which is a universal approximation theorem, and that says that for most linear in the parameters basis function models they can approximate.",
            "Any continuous function to an arbitrary accuracy that's paraphrase slightly, but so if this is there, are you know there are certain types of basis functions that wouldn't be able to do that?",
            "But as long as you're reasonably careful about choosing your beta, most of the common ones that we use.",
            "Can approximate anything you want.",
            "Rob will.",
            "He's going to talk about most layer perceptrons later on which are non linear nonlinear model.",
            "If you want to call it that and there are certain advantages of using those types of models.",
            "OK they made it there more compact in terms of representation.",
            "But to be honest, a lot of this comes down to what is your favorite?",
            "OK, most models.",
            "Given enough effort, most models will do pretty much as well as each other.",
            "There are advantages and disadvantages in other ways.",
            "Any other questions?",
            "Classification boundary golf, linear fire as well if you do it, you sort of go back you.",
            "What we've this is so this is our original space.",
            "OK, here this is sort of some imaginary basis function space.",
            "And then if we were to transform back.",
            "This is what we get, but here we've basically gone back to the original space.",
            "We actually never, you know, you would never show all of this what we actually you know.",
            "You just see this in terms of what you would plot.",
            "You would never plot this thing up here."
        ],
        [
            "So you."
        ],
        [
            "Actually, you just do the solid arrow there within classification are not going to dwell on doing parameter estimation other than to say that what you there are a number of different approaches.",
            "Most common one with sorting maximum likelihood estimation on what you're trying to do here is to maximize the probability of getting the observed results given the parameters, and that's what maximum likelihood does there through the week.",
            "You'll hear more about some other techniques for doing classification estimation.",
            "But here we're not going to.",
            "The main point to take is that although there is a unique minimum, there's now no closed form solution.",
            "So whereas for curve fitting I could write the fire transpose 5 -- 1 * 5 transpose Ed.",
            "There's no longer that equation OK or similar equation.",
            "We now have to use iterative type techniques, sort of gradient type methods to find the solution.",
            "OK, we're not going to."
        ],
        [
            "There's not the time to get too much into that.",
            "So here we've got.",
            "Are two clusters of data points?",
            "From what I recall, I think they're both downstream distributed cloud data points, which has allowed me to, so I know the true or optimum decision boundary.",
            "Overlap."
        ],
        [
            "In them.",
            "That is a linear.",
            "That's the optimum linear decision boundary and you see to me that looks pretty realistic.",
            "Looks like a fairly good fit.",
            "So the decision boundary.",
            "Whoops"
        ],
        [
            "Alternatively class is a non linear decision boundary.",
            "Been pushed in.",
            "OK, by using a fairly simple basis function model.",
            "And actually, if you look at that, I don't know about you, but I would argue that the linear decision boundary probably looks better.",
            "I don't know who who thinks the linear decision boundary is better.",
            "If you have 1/2 of you."
        ],
        [
            "The true is the dashed line, so actually.",
            "The nonlinear decision boundaries probably is better, but this comes back to the issue about.",
            "What's a good model and what in your own do you really want to get the truth?",
            "Do you want to get the true model?",
            "And in this case it may well be that linear decision boundary is more than good enough for the purposes of your application.",
            "And that you don't really know need to know what that is."
        ],
        [
            "The thing is.",
            "This is an example, so you can actually plot the class probabilities as well.",
            "So for that example, actually the decision boundary is formed by cussing.",
            "This goes from zero to one and incision boundaries formed by cutting this surface at .5.",
            "So if the probability is below .5, it belongs to one class, and if it's above .5, it belongs to the Class 1.",
            "So in terms of this is what the curve fitting surface looks like, that you've actually approximated."
        ],
        [
            "And when we look at those decision boundaries, they just formed by."
        ],
        [
            "Crossing that surface at the .5 level.",
            "OK."
        ],
        [
            "This is an example where it can all go wrong.",
            "So whereas in the curve fitting example I showed some overfitting, here by re running this experiment a number of times and re calculating the decision boundaries.",
            "We've got slightly different realizations of the data points, and we'll see that our lines are solid.",
            "Lines do vary quite a bit between the different realizations of data points, and this is a particular problem with doing this, because again, we're doing maximum likelihood, which is very similar to what we're doing for curve fitting when we're minimizing the sum of squared errors.",
            "So there's a possibility.",
            "But although we're not exactly overfitting, we're getting very different results each time we fit the data.",
            "You know, if you look here.",
            "Very different looking curve in that case at the top here the non linear decision boundary is actually curved in the opposite direction, so really doesn't look like a very good example.",
            "So."
        ],
        [
            "Things can go wrong.",
            "Right very quickly before the break, I'm going to touch on a couple of the questions that were asked during the talk.",
            "We also need to estimate or decide on the type of basis function.",
            "The number of basis functions and the positions of those basis functions.",
            "If we're doing basis function.",
            "I'm type modeling data modeling.",
            "And I could just sort of stop and say that last sentence.",
            "These are nonlinear problems and they are difficult.",
            "OK, no one has come up with a solution.",
            "There are ideas on how to do some of this stuff.",
            "But to be honest.",
            "You know these are just guidelines and that's why."
        ],
        [
            "Touch on some guidelines.",
            "Types of basis functions.",
            "Usually choose your favorite.",
            "OK, I think most people within data modeling they latch on to one particular basis function and it's nine times out of 10.",
            "It's the Gaussian radial basis function and they stick with that for their whole career.",
            "OK, if you ask if you ask them why in a few to ask him in a particularly nasty question, the PhD vibe will be so after student.",
            "While I have you chosen a Gaussian basis function, I bet they wouldn't be able to answer you.",
            "OK, I probably just say last one, my supervisor told me to use.",
            "Um?",
            "There are certain properties of some basis functions which make them a bit more suitable, perhaps in some cases than others.",
            "But to be honest, I would you know, and I'm a classic example is I pretty much always use the Garrison basis function.",
            "OK, I find easy to work with.",
            "OK, so that.",
            "Perhaps sort of answers your question on the type of base function, yeah?",
            "Hi, yeah, now you're getting into heavy juicy stuff.",
            "Yes, in theory.",
            "There is.",
            "In practice, I think it's far more important in terms of how many base functions you've got.",
            "But certainly if you were to choose, the Gaussian is quite isn't a nice smooth basis function.",
            "But then again, that depends on the width you choose.",
            "So within even within the Gaussians you've got this parameter Sigma squared, which basically determines how peaks that basis function is.",
            "So there's actually a whole class, so even even if you choose gastron.",
            "You still gotta choose what width of Gaussian.",
            "There's actually an infinite number of Gaussians you can choose.",
            "OK, and we again will touch a bit more on that because that becomes what's known as a hyperparameter.",
            "And again, trying to estimate that becomes really difficult.",
            "So even if you choose a basis function, you've often still got other things that you need to estimate for that basis function.",
            "So yeah.",
            "Choosing the base function can have an effect on the quality in terms of how wiggly your function is or how smooth it is.",
            "Yep.",
            "What's what degree of polynomial?",
            "Are you going to choose?",
            "How many Fourier component, how?",
            "How?",
            "Yeah, because then you're having sweet you having to invert an infinite by big N activation matrix.",
            "This will probably be touched on tomorrow when support vector machines are talked about because there are tricks for working with infinite size basis bases.",
            "But I can pretty much guarantee whatever basis function you give to me.",
            "I will give you a problem with it.",
            "OK, so whether it's the width of it, whether it's the size you know with the polynomial, is the polynomial degree.",
            "There's always something you've got to choose, unfortunately.",
            "Or estimates?"
        ],
        [
            "OK, so that's the type of basis functions, next problem, or how many basis functions are we going to want?",
            "I suppose in this case that, or equivalently the polynomial degree.",
            "Again, who knows?",
            "OK, there is no single answer to this.",
            "There are some ideas, one of them.",
            "Is practice.",
            "You slowly increase the number of basis functions until you start overfitting the data.",
            "Um?",
            "You know, somehow you have to explore it.",
            "OK, there is no there there is.",
            "Although there might in theory be an optimal number of basis functions, you aren't going to find it because it's a nonlinear optimization problem.",
            "So unless you do an exhaustive search.",
            "OK, you can't do this.",
            "OK, so it's difficult.",
            "It's a nonlinear problem.",
            "Often the other thing I often people just put so as well as slowly increasing the number.",
            "The other possibility and what I did in my examples.",
            "Oh, sorry, yes.",
            "That's that's a number come on two way."
        ],
        [
            "We put them now.",
            "Sorry, so now the positions are the basis function.",
            "This becomes really difficult.",
            "And again, there's no definitive answer.",
            "Put some ideas, one is that you just put a basis function on each data points.",
            "So if you go back to the."
        ],
        [
            "Angstrom these see eyes pointing.",
            "You can't see that the CIS are the sensors.",
            "You could put those just locate the sensors on each data points.",
            "That's one possibility, but if your data isn't particularly well distributed, you may end up with loads of basis functions in one area and not many in another area.",
            "I mean."
        ],
        [
            "You know, so an alternative.",
            "Is have a uniform grid, so just uniformly space your basis functions.",
            "That's OK in one 2, maybe 3 dimensions.",
            "If you want to do a uniform distribution of basis functions in 100 dimensional space.",
            "And you want 10 per axis you've got.",
            "Either tends to power 100 or hundreds of hours 10.",
            "Big number number of basis functions and you have to estimate a parameter with each one so that can become a problem.",
            "I will say one of the advantages of using global basis functions, so if you."
        ],
        [
            "Go back.",
            "So the polynomials you don't have to locate these anywhere.",
            "That sort of global.",
            "They're just there.",
            "OK, you don't have.",
            "There's no sense or anything, so advantage of basis functions like polynomials.",
            "Is it you don't need to choose the positions?",
            "That may be one advantage of using those.",
            "My experience often though is using global based functions.",
            "They can be extremely sensitive to noise.",
            "So the reason we don't always you can like well why don't we always use polynomials?",
            "We're actually they have other disadvantages and certainly one of them.",
            "From my experience is they can be very very sensitive to noise for certainly the high order polynomials."
        ],
        [
            "Are there any questions on basis functions?",
            "That would be similar.",
            "Again, you might slowly increase the polynomial.",
            "The degree of the polynomial you mean.",
            "They're just the whites.",
            "With."
        ],
        [
            "If you go back here, the polynomial here, each of those, they're just your basis functions.",
            "So think of those as your faize.",
            "So X1 squared is just a Phi.",
            "Uvex effectively.",
            "And each of those will have a WI in front of it, and those are the same parameters.",
            "Yes.",
            "Oh, I see when I.",
            "You wouldn't know you wouldn't.",
            "You wouldn't do it that way.",
            "You would have.",
            "Your final model would be that complete tense all the polynomial.",
            "Your basis functions would be 1X1X2X1 squared X2 squared up to.",
            "X1 X 2X redirect to X10 plus all of those other combinations of 10th powers of X.",
            "One X2 X.",
            "You know if you have X1 to 10X19 next to.",
            "And you would end up with a model I suspect with millions of basis functions and that way.",
            "Yeah, yeah."
        ],
        [
            "OK, I think this is the final thing.",
            "I've over on Obetts quickly is a note on data.",
            "How much data do we need?",
            "Can anyone answer?",
            "OK, so you have a fairly good answer as much as you can.",
            "Next question.",
            "Do you just need to use all of that data to train your model?",
            "Who thinks you use all of your data to train the model?",
            "Yeah, OK, so that's.",
            "Yeah, so again, how much is this?",
            "Use what you get?",
            "So what about validating and testing the model?",
            "So for those.",
            "Who haven't done much on data modeling before, it's very important that you need three sets of data.",
            "You need a training set, validation sets, and a testing set.",
            "We use the training data to train the parameters.",
            "We use the validation data set normally within hyperparameter estimation, so so to do things like when you are changing the number of basis functions, changing the basis function width.",
            "You check your model on the validation data set, but you should always leave what's known as the testing datasets for the very ends.",
            "OK, just to check that your final model hasn't been biased in any way by the training and validation datasets.",
            "I guess the training and validation datasets are used in doing finding the model.",
            "And then there's a final Saturday to the testing set, which is that final Test that yes, your model is a good model.",
            "Yep.",
            "Yeah, I mean yes, there is an applied assumption that.",
            "Yeah, all of the data that has the same underlying properties, if it's nonstationary then on that's a whole different sort of not really covering nonstationary data modeling here.",
            "OK. Any questions on data?",
            "In terms of how many days points you need in the OR how you divide up your datasets in terms of these, again that's a bit of an open question.",
            "Um?",
            "You often."
        ],
        [
            "Enough in each.",
            "So concluding remarks always try the simplest possible model first, I think that's probably the most important thing to take away from this particular as you go into the rest of the week where you're going to hear about some quite complicated modeling techniques and people who make big grand claims that their technique is the best.",
            "Always try a simple 1 first.",
            "If it works, you don't need to go anymore or acts as a benchmark.",
            "You can make a linear models nonlinear in terms of the input output relationship via straightforward basis function expansions.",
            "But you end up in the ends with nonlinear optimization because you gotta choose things like basis function widths, positions of basis functions becomes a big problem.",
            "The final question that I want to leave you with to think about through the rest of week is least squares or maximum likelihood.",
            "The best way I don't want you answer science that now just think about it.",
            "OK, that's me.",
            "Finished for now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right then.",
                    "label": 0
                },
                {
                    "sent": "OK, let's make a start again, um.",
                    "label": 0
                },
                {
                    "sent": "So this is the first principle sort of proper session oversight to get into.",
                    "label": 0
                },
                {
                    "sent": "The technical bits are, So what I'm going to cover?",
                    "label": 0
                },
                {
                    "sent": "First thing this morning is what I've called linear models.",
                    "label": 0
                },
                {
                    "sent": "But that's perhaps not the best term as you come on to see 'cause actually.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit of what's going to come up turns out to be nonlinear OK, but there is.",
                    "label": 0
                },
                {
                    "sent": "Again, I will highlight why.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've called this sort of linear as we go through.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can talk about number of difference.",
                    "label": 0
                },
                {
                    "sent": "Sort of linear models explains I go through what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "The first one we look at will be the classic real linear model.",
                    "label": 0
                },
                {
                    "sent": "We will look briefly at.",
                    "label": 0
                },
                {
                    "sent": "How you estimate the parameters?",
                    "label": 0
                },
                {
                    "sent": "That part of it fairly familiar to a lot of you.",
                    "label": 0
                },
                {
                    "sent": "We then start getting into.",
                    "label": 0
                },
                {
                    "sent": "The nonlinear linear bits OK, and that's where we start talking about what are called linear in the parameters models and where we look at applying well, I've called linear models in classification.",
                    "label": 1
                },
                {
                    "sent": "And talk about linear models and classification really is not the way to put it.",
                    "label": 0
                },
                {
                    "sent": "Rob will get very angry you angry with me.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk briefly at the end about some real real nonlinear bits.",
                    "label": 0
                },
                {
                    "sent": "OK, so although some of this you can do linear, there are bits even in a purely linear model.",
                    "label": 0
                },
                {
                    "sent": "There are some things which are still.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nonlinear and I'll cover those at the end.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In Kerr fishing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is the probably the simplest model you could ever use.",
                    "label": 1
                },
                {
                    "sent": "I suppose the simplest model is that as your data is a constant value.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we ignore that, then the simplest possible model you can ever get is a linear fit.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "You have your input variables are just added together by some weighted sum.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of weights WI multiplied by the excise.",
                    "label": 0
                },
                {
                    "sent": "OK, in the exercise of the components of the input X and then with some those together.",
                    "label": 1
                },
                {
                    "sent": "So XI is the ice components of inputs X.",
                    "label": 0
                },
                {
                    "sent": "We assume that X nought is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And therefore W nottz is the bias.",
                    "label": 1
                },
                {
                    "sent": "So if you only had that one term, if you only had that W noughts and then strictly that'll be the simplest possible model, because it could be that the data is simply a constant value.",
                    "label": 0
                },
                {
                    "sent": "But then the natural generalization of that is this linear model that we've got here.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just weighted sum of the input variables.",
                    "label": 1
                },
                {
                    "sent": "Is everyone happy with that equation?",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I say all we can do with that is represents lines and planes.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can do nothing more than represent lines in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "So like the examples that I've shown you, or if you start getting into higher input dimensions.",
                    "label": 0
                },
                {
                    "sent": "So an input dimension greater than one.",
                    "label": 1
                },
                {
                    "sent": "Then we're representing 234.",
                    "label": 0
                },
                {
                    "sent": "However many dimensional planes, but they are linear OK, they're flat, and I said.",
                    "label": 0
                },
                {
                    "sent": "That's all we can represent.",
                    "label": 0
                },
                {
                    "sent": "But actually it may not be as restrictive as you think.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause very often in the real world things can be quite linear and you can do surprisingly well with linear models.",
                    "label": 0
                },
                {
                    "sent": "And this always creates a problem when you supervise a PhD student on an application area.",
                    "label": 0
                },
                {
                    "sent": "Anything we do some really exciting work and the first thing they do is they try a linear model.",
                    "label": 0
                },
                {
                    "sent": "And it turns out to fit extremely well.",
                    "label": 0
                },
                {
                    "sent": "I mean, I have to think well, where does the PhD go from here or research go from here?",
                    "label": 0
                },
                {
                    "sent": "But that shouldn't stop you.",
                    "label": 1
                },
                {
                    "sent": "Fitting linear model and you should always fit.",
                    "label": 0
                },
                {
                    "sent": "So if you're looking at curve fitting, first thing you should always do with your data is fit a linear model.",
                    "label": 1
                },
                {
                    "sent": "If you're looking at classification, pattern recognition or density estimation, again you should always try the simplest possible model, or a very very simple model.",
                    "label": 0
                },
                {
                    "sent": "OK, within data modeling OK, simple, always do the simplest possible model that will describe your data as well as you need it to be described OK, there's no point in going into.",
                    "label": 0
                },
                {
                    "sent": "Advanced nonlinear techniques.",
                    "label": 0
                },
                {
                    "sent": "If you don't need them.",
                    "label": 0
                },
                {
                    "sent": "OK, firstly it's a lot of effort that's not needed and Secondly, you will actually probably end up with a worse model.",
                    "label": 1
                },
                {
                    "sent": "Then the linear one.",
                    "label": 0
                },
                {
                    "sent": "OK so always try a very very simple models to start off with.",
                    "label": 0
                },
                {
                    "sent": "If that works well you can stop.",
                    "label": 0
                },
                {
                    "sent": "You can move on to something else if it doesn't work particularly well, at least it's got a bench.",
                    "label": 0
                },
                {
                    "sent": "You've got a benchmark to compare against, so again, when you start applying all these fancy kernel methods, support vector machines, Bayesian methods from later on in the week.",
                    "label": 0
                },
                {
                    "sent": "You know, if they don't do any better than the linear model.",
                    "label": 0
                },
                {
                    "sent": "Again, there's no need for them.",
                    "label": 0
                },
                {
                    "sent": "OK so I always have that as a benchmark and it may be that that's all you know.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Primus restoration and what I'm going to talk about today and the everything that we're going today is about least squares parameter estimation tomorrow and perhaps later on the week that other types of parameter estimation will we talked about, but for the purposes of today.",
                    "label": 0
                },
                {
                    "sent": "We're focusing on least squares type estimation methods.",
                    "label": 0
                },
                {
                    "sent": "That's not when we come to classification.",
                    "label": 0
                },
                {
                    "sent": "That's not strictly true, but it still has that flavor.",
                    "label": 0
                },
                {
                    "sent": "So we're going to choose our parameters, so our weights.",
                    "label": 0
                },
                {
                    "sent": "So Wis we need to estimate them in some way and we're going to estimate them.",
                    "label": 1
                },
                {
                    "sent": "By minimizing the sum of squared errors.",
                    "label": 0
                },
                {
                    "sent": "And you sometimes see you know this.",
                    "label": 0
                },
                {
                    "sent": "Maybe pre multiplied by 1 / N or one over Sigma squared N where Sigma squared is the expected noise variance.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't rain that those terms in front are just constants, don't really affect it, so that the key thing is that we're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "The sum of squared errors.",
                    "label": 0
                },
                {
                    "sent": "It's on the data points so.",
                    "label": 0
                },
                {
                    "sent": "I've already spotted that my notation is wrong, 'cause that should be.",
                    "label": 0
                },
                {
                    "sent": "That shouldn't be why.",
                    "label": 0
                },
                {
                    "sent": "Because why is that true?",
                    "label": 0
                },
                {
                    "sent": "And we don't even know that.",
                    "label": 0
                },
                {
                    "sent": "So pretend that the Y is the output from the model.",
                    "label": 1
                },
                {
                    "sent": "Does that mean that my so that's wrong as well, isn't it?",
                    "label": 0
                },
                {
                    "sent": "So strictly there, why?",
                    "label": 0
                },
                {
                    "sent": "Why should be in F hat?",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Y of XI should be F hats of XII is the output from our estimated model.",
                    "label": 0
                },
                {
                    "sent": "And the sad eyes are.",
                    "label": 1
                },
                {
                    "sent": "Observed noisy data, so we're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "The deviation on our data points between the model output and the measured output, so we're trying to drive that value to 0.",
                    "label": 0
                },
                {
                    "sent": "Sum of squared errors to 0.",
                    "label": 0
                },
                {
                    "sent": "Does anyone think that's a good or a bad?",
                    "label": 0
                },
                {
                    "sent": "Performance measure.",
                    "label": 0
                },
                {
                    "sent": "Who thinks it's a good one?",
                    "label": 0
                },
                {
                    "sent": "Yeah, those are all valid points, so I mean.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is, we're assuming that the noise is Gaussian OK, in terms of this being optimal.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is an implied assumption that the noise is Garrison, but it rarely is, and it's still fairly good.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's still a reasonable first attempts and there are, you know there are other performance measures you can use, but there tends to be much more complicated to work with.",
                    "label": 0
                },
                {
                    "sent": "This is a very, very simple one to work with.",
                    "label": 0
                },
                {
                    "sent": "OK, the other point, overfitting is a big big problem.",
                    "label": 0
                },
                {
                    "sent": "If all you do.",
                    "label": 0
                },
                {
                    "sent": "Is minimize the sum of squared errors?",
                    "label": 0
                },
                {
                    "sent": "Then there's a high likelihood that you will overfit your data.",
                    "label": 0
                },
                {
                    "sent": "What you'll end up doing is you'll end up modeling not just the underlying true function F. But you will end up modeling the noise and fitting the noise as well, and we don't want to do that.",
                    "label": 0
                },
                {
                    "sent": "We want to fix.",
                    "label": 0
                },
                {
                    "sent": "We filter out the noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll come onto ways of overcoming that later on, but we'll stick with this at the moment.",
                    "label": 0
                },
                {
                    "sent": "If your model is not that complicated.",
                    "label": 0
                },
                {
                    "sent": "So for a purely linear model.",
                    "label": 0
                },
                {
                    "sent": "This is a good thing to do because in a pure linear model is so simple.",
                    "label": 0
                },
                {
                    "sent": "That it won't overfit the noise in the data.",
                    "label": 0
                },
                {
                    "sent": "OK. Because it's so simple in itself, but if your model is slightly more flexible as the models that will come on so in a minute, then there is the potential that they will overfit the data.",
                    "label": 0
                },
                {
                    "sent": "So always be a bit wary.",
                    "label": 0
                },
                {
                    "sent": "When you just presented with just minimizing the sum of squared errors.",
                    "label": 0
                },
                {
                    "sent": "Not always the best thing to do.",
                    "label": 0
                },
                {
                    "sent": "The advantages advantage of it though, is that it has a unique global minimum.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "When we're looking at linear models.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at other more complicated cipher models and multilayer perceptrons that Rob will be talking about later on their reasons.",
                    "label": 0
                },
                {
                    "sent": "A single global.",
                    "label": 0
                },
                {
                    "sent": "Unique minimum.",
                    "label": 0
                },
                {
                    "sent": "OK, you may get multiple men and multiple local minima as well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is optimal when the noise is Gaussian, and in that case it also corresponds to what's known as the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "So when you start getting into more of the probability, there's an alternative interpretation of this as the maximum likelihood parameters I there the promises that are most likely given the data that you've got.",
                    "label": 0
                },
                {
                    "sent": "Again, you learn more about maximum likelihood and stuff later on in the week anyway.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions on?",
                    "label": 0
                },
                {
                    "sent": "The promised rest, yeah.",
                    "label": 0
                },
                {
                    "sent": "Optimum optimum might be options.",
                    "label": 0
                },
                {
                    "sent": "Probably the wrong word, isn't it op tits?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean if if you're yeah if you have the right model, the noise is truly Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You know the noise characteristics then this will be the optimum.",
                    "label": 0
                },
                {
                    "sent": "Performance measure C. Use OK out of all possible performance measures, or perhaps to be better to think of this would be the best one to use.",
                    "label": 0
                },
                {
                    "sent": "OK, but there are a whole host and certainly will learn tomorrow when you do support vector machines.",
                    "label": 0
                },
                {
                    "sent": "There are lots of other different types of cost functions that you can use.",
                    "label": 0
                },
                {
                    "sent": "They can be considered to be optimum for other types of noise characteristics, but actually they are often used because they are more robust when you don't know what the noises.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that the key thing here is.",
                    "label": 0
                },
                {
                    "sent": "We normally use the sum of squared errors because it's simple.",
                    "label": 0
                },
                {
                    "sent": "It's very well known and as we'll see in a minute, it gives a very neat solution.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other questions?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What is?",
                    "label": 0
                },
                {
                    "sent": "Scansion on direction is when so when you've got noise.",
                    "label": 0
                },
                {
                    "sent": "You noise has a distribution function.",
                    "label": 0
                },
                {
                    "sent": "Have you done anything on probability before?",
                    "label": 0
                },
                {
                    "sent": "So gation just means that the noise has a Bell shaped.",
                    "label": 0
                },
                {
                    "sent": "Probability density function.",
                    "label": 0
                },
                {
                    "sent": "Which is centered around the mean, so it has a mean and has a variance associated with it, and what it basically means is that the noise is most likely around the mean, and it's less likely as you get further away from the mean, but it has a very particular shape, so how that trend goes?",
                    "label": 0
                },
                {
                    "sent": "The noise.",
                    "label": 0
                },
                {
                    "sent": "It's zero, we normally assume it's B0 mean.",
                    "label": 0
                },
                {
                    "sent": "Size.",
                    "label": 0
                },
                {
                    "sent": "You never, you can't.",
                    "label": 0
                },
                {
                    "sent": "You don't minimize the noise.",
                    "label": 0
                },
                {
                    "sent": "You filter the noise.",
                    "label": 0
                },
                {
                    "sent": "So if you I'll show you an example in a minute which sort of.",
                    "label": 0
                },
                {
                    "sent": "You're trying to.",
                    "label": 0
                },
                {
                    "sent": "You want to model the true function.",
                    "label": 0
                },
                {
                    "sent": "Which is sort of ignoring the noise it's you should think of it.",
                    "label": 0
                },
                {
                    "sent": "I, I think of it as being filtering the noise.",
                    "label": 0
                },
                {
                    "sent": "The problem with, as someone pointed out the problem with using this performance measure, is that you will end up fit if you got two complicated and model, you will end up fixing the noise and I'll show you an example of that in a minute and Rob will sort much more about this later on.",
                    "label": 0
                },
                {
                    "sent": "So will come.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that in a minute.",
                    "label": 0
                },
                {
                    "sent": "That is an example, so our cost function is just a nice bowl shape.",
                    "label": 1
                },
                {
                    "sent": "So in that's in.",
                    "label": 0
                },
                {
                    "sent": "If you got two weights then you get a nice bowl shape and you see a nice unique global minimum here.",
                    "label": 0
                },
                {
                    "sent": "OK, no local minima.",
                    "label": 0
                },
                {
                    "sent": "Everything is nice and what will happen, hopefully is you will find the parameters corresponding to that point.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we find the premises?",
                    "label": 0
                },
                {
                    "sent": "What we do is we define something called a design matrix.",
                    "label": 0
                },
                {
                    "sent": "And design matrix is constructed in this way.",
                    "label": 0
                },
                {
                    "sent": "So since we've got a set of biases, we put a zero of one's in the first column.",
                    "label": 0
                },
                {
                    "sent": "And then for each input variable, so we got M little M input variables and we have big N. Measurements or data points.",
                    "label": 0
                },
                {
                    "sent": "So we populate this matrix in this way so each column.",
                    "label": 0
                },
                {
                    "sent": "Is the N big N values of the little NTH?",
                    "label": 0
                },
                {
                    "sent": "Input variable.",
                    "label": 0
                },
                {
                    "sent": "And we populate the Matrix in that way.",
                    "label": 0
                },
                {
                    "sent": "The simplest way to think about what these entries are is in terms of.",
                    "label": 0
                },
                {
                    "sent": "Is it a big N or little M associated with that row or column?",
                    "label": 0
                },
                {
                    "sent": "That sort tells you.",
                    "label": 0
                },
                {
                    "sent": "How will populating it?",
                    "label": 0
                },
                {
                    "sent": "Then we can write.",
                    "label": 0
                },
                {
                    "sent": "Our observations.",
                    "label": 0
                },
                {
                    "sent": "As being equal to the Matrix Phi.",
                    "label": 0
                },
                {
                    "sent": "Times are parameter vector W. So that.",
                    "label": 0
                },
                {
                    "sent": "Is also noise free output, predicted output, and then we now have a vexor.",
                    "label": 0
                },
                {
                    "sent": "Of errors of vector of noise associated with that.",
                    "label": 0
                },
                {
                    "sent": "So this becomes a vector equation.",
                    "label": 0
                },
                {
                    "sent": "So the these says will have N big N components associated with.",
                    "label": 0
                },
                {
                    "sent": "That will be a vector of all of our.",
                    "label": 0
                },
                {
                    "sent": "Target observations.",
                    "label": 0
                },
                {
                    "sent": "This will be again a vector of N. Model predicted outputs and this will be a vector of all of our noise.",
                    "label": 0
                },
                {
                    "sent": "Is everyone happy with Lance?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we've done that.",
                    "label": 0
                },
                {
                    "sent": "We can write the sum of squared errors.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Low.",
                    "label": 0
                },
                {
                    "sent": "Our equation here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can Alternatively be written in the form of the first line.",
                    "label": 0
                },
                {
                    "sent": "So we've now written it in matrix vector notation.",
                    "label": 0
                },
                {
                    "sent": "But they are completely identical.",
                    "label": 0
                },
                {
                    "sent": "What we can then do is we can expand out the sum of squared errors.",
                    "label": 0
                },
                {
                    "sent": "So using straightforward matrix notation rules so we get as their transpose AD minus their transpose, fire W, and so on.",
                    "label": 0
                },
                {
                    "sent": "We can combine the second and third terms so zed transposed by W&W, transpose, fire, transpose Ed are actually identical terms.",
                    "label": 0
                },
                {
                    "sent": "So we can combine them together into the two zed transposed by W. So we now have our sum of squared errors equals to the third line.",
                    "label": 0
                },
                {
                    "sent": "So minimize this.",
                    "label": 0
                },
                {
                    "sent": "As in standard calculus, you simply.",
                    "label": 0
                },
                {
                    "sent": "Calculate the derivative with what you want.",
                    "label": 0
                },
                {
                    "sent": "I've driven with specs which in this case is W and set it equal to zero.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into how you do matrix vector differentiation.",
                    "label": 0
                },
                {
                    "sent": "OK, if anyone wants to learn about it, they can look it up on the web, but we haven't got.",
                    "label": 0
                },
                {
                    "sent": "We probably need a whole day for me to explain the rules of doing and proofs of how you do matrix vector.",
                    "label": 0
                },
                {
                    "sent": "Differentiation, but if you just trust me that if you differentiate with Ed transpose, Ed doesn't even depend on sub W, so we get rid of that one if we differentiate to zed transpose 5 W then it just becomes two zed transfers Fi.",
                    "label": 0
                },
                {
                    "sent": "Which in a sense it looks like normal differentiation.",
                    "label": 0
                },
                {
                    "sent": "For that one you just get rid of the W. At the site Exchange, one is the W transpose.",
                    "label": 0
                },
                {
                    "sent": "Fire transfers find W. If you differentiate that with respect to W. Then you get to W transpose FIE, transpose Phi.",
                    "label": 0
                },
                {
                    "sent": "OK, and these are well known rules within linear algebra for doing matrix differential matrix vector differentiation.",
                    "label": 0
                },
                {
                    "sent": "If you then rearrange, you get the W transpose fire.",
                    "label": 0
                },
                {
                    "sent": "Transverse Phi equals said, transposed by an.",
                    "label": 0
                },
                {
                    "sent": "If you then take the transpose of all of this equation and then pre multiply by the French Fire transpose Fi, you end up with.",
                    "label": 0
                },
                {
                    "sent": "Your optimum weights in the sense of minimizing the sum of squared errors is equal to what's known as the matrix pseudoinverse, times zed.",
                    "label": 0
                },
                {
                    "sent": "So this whole thing here.",
                    "label": 0
                },
                {
                    "sent": "Is known as the Matrix pseudoinverse.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's equal to five transfers.",
                    "label": 0
                },
                {
                    "sent": "5 awesome minus 1 * 5 transpose.",
                    "label": 0
                },
                {
                    "sent": "And then you multiply that by your vector of observations of the data and that will give you.",
                    "label": 0
                },
                {
                    "sent": "Your parameter estimates.",
                    "label": 0
                },
                {
                    "sent": "Are most of you familiar with that equation?",
                    "label": 0
                },
                {
                    "sent": "Is everyone happy with that equation?",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to go back to our example again.",
                    "label": 0
                },
                {
                    "sent": "So here we have a set of data points that have been observed.",
                    "label": 0
                },
                {
                    "sent": "So input X along.",
                    "label": 0
                },
                {
                    "sent": "This direction and why output up.",
                    "label": 0
                },
                {
                    "sent": "The screen, so we reserve some data.",
                    "label": 0
                },
                {
                    "sent": "I've made an assumption that I think that data can be modeled by a linear model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I calculate the price restaurants and that's what I get by minimizing.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some squared errors compared to the true function.",
                    "label": 0
                },
                {
                    "sent": "Which is the dashed line.",
                    "label": 0
                },
                {
                    "sent": "And you'll see here that although we've minimizes sum of squared errors, we've certainly not overfitted the noise in the data.",
                    "label": 0
                },
                {
                    "sent": "OK, and I would argue that's a pretty good.",
                    "label": 0
                },
                {
                    "sent": "Estimate of the true function.",
                    "label": 0
                },
                {
                    "sent": "And hope yeah, I'm wondering.",
                    "label": 0
                },
                {
                    "sent": "Are you interested in the function that models the data?",
                    "label": 0
                },
                {
                    "sent": "Oh, you're not a physicist.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It depends what you're interested in, and it depends on your reasons for doing data modeling.",
                    "label": 0
                },
                {
                    "sent": "Most people that do data modeling are simply interested in getting good predictions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In a sense, if they don't, if they don't model the true underlying.",
                    "label": 0
                },
                {
                    "sent": "Relationship between the inputs and outputs, but still get good predictions.",
                    "label": 0
                },
                {
                    "sent": "They will be happy.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It depends.",
                    "label": 0
                },
                {
                    "sent": "I mean you will.",
                    "label": 0
                },
                {
                    "sent": "You probably you will never ever, I suspect, find the true function because we don't know what the true function is.",
                    "label": 0
                },
                {
                    "sent": "And the chances that you happen to stumble on choosing exactly the correct class of models that correspond to the true function is that probability is 0.",
                    "label": 0
                },
                {
                    "sent": "I would say.",
                    "label": 0
                },
                {
                    "sent": "And in a lot of cases there is no true underlying function.",
                    "label": 0
                },
                {
                    "sent": "You know, if you're trying to model some gas turbine engine, there's not really.",
                    "label": 0
                },
                {
                    "sent": "A true function there.",
                    "label": 0
                },
                {
                    "sent": "I mean, I suppose this sort of is, but you know, there's no equation that we can write down or anything.",
                    "label": 0
                },
                {
                    "sent": "So, and this is, I guess this is one of the problems with data modeling is that there never is a real truth.",
                    "label": 0
                },
                {
                    "sent": "For what we're aiming for, where all we're really looking to do is to get good predictions, and that's why some stuff I'll talk about briefly at the end.",
                    "label": 0
                },
                {
                    "sent": "But more So what Rob will talk about in terms of how?",
                    "label": 0
                },
                {
                    "sent": "How do you assess how good your model is?",
                    "label": 0
                },
                {
                    "sent": "That becomes really important because you also have to use tricks.",
                    "label": 0
                },
                {
                    "sent": "To try and work out how good your model is, because you never know what the truth is.",
                    "label": 0
                },
                {
                    "sent": "Is that answer?",
                    "label": 0
                },
                {
                    "sent": "So like, yeah, there's any other questions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Value.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Do you have?",
                    "label": 0
                },
                {
                    "sent": "Do you have variation in the input as you have noise on your inputs as well as that?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Over and over again.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Point like an average.",
                    "label": 0
                },
                {
                    "sent": "Why don't why do you use the average?",
                    "label": 0
                },
                {
                    "sent": "Why don't you use?",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, I mean something that we're not going to talk too much about his pre processing of data.",
                    "label": 0
                },
                {
                    "sent": "I would assume that you would use all of your all of your data in its raw form.",
                    "label": 0
                },
                {
                    "sent": "But there are there are cases where it may be advantageous.",
                    "label": 1
                },
                {
                    "sent": "To do some form of pre processing so to perhaps average the data beforehand.",
                    "label": 0
                },
                {
                    "sent": "I'm going to in effect what you're doing.",
                    "label": 0
                },
                {
                    "sent": "If you're averaging the data before you put it into the model you're trying, you're trying to get rid of the noise.",
                    "label": 0
                },
                {
                    "sent": "And then there's uncertainty effects before you do the model.",
                    "label": 1
                },
                {
                    "sent": "And that may help you when you it may make it easier to do the modeling.",
                    "label": 0
                },
                {
                    "sent": "The alternative way of doing it is just give the model the raw data and let the model try and filter out the noise.",
                    "label": 1
                },
                {
                    "sent": "Often by doing a bit of intelligent pre processing.",
                    "label": 0
                },
                {
                    "sent": "You can help your model an awful lot to rely on the model to do all to do everything can sometimes be a bit too much just by a bit of preprocessing.",
                    "label": 0
                },
                {
                    "sent": "The classic things to do are just.",
                    "label": 0
                },
                {
                    "sent": "Standardizing your data.",
                    "label": 0
                },
                {
                    "sent": "Putting the data into an interval zero to 1.",
                    "label": 0
                },
                {
                    "sent": "It can have a huge advantage in some just doing some simple scaling of the data that it just makes the algorithm.",
                    "label": 0
                },
                {
                    "sent": "It tends to be that's all associated.",
                    "label": 0
                },
                {
                    "sent": "This sort of conversation that's going on that tends to be so perhaps more well conditioned or well, better behaved.",
                    "label": 0
                },
                {
                    "sent": "OK, but the point is by doing some preprocessing beforehand you can often help.",
                    "label": 0
                },
                {
                    "sent": "Your data model, your model before you start.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I fully answer your quest.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how can we generalize that very simple linear model that we just had?",
                    "label": 0
                },
                {
                    "sent": "Will the simplest way of generalizing it is to take almost the identical equation, but now throw in that Phi I. OK, so instead of WI times XI we take some nonlinear transformation of the X is.",
                    "label": 0
                },
                {
                    "sent": "And that knowledge transformation is done via the faize.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll show you an illustration what's going on there.",
                    "label": 0
                },
                {
                    "sent": "So what we've done we we critically we still have this model being linear in the parameters, so it's still a sum of the Wis times defy eyes, so it has a very similar form to what we have, the only difference being that we formed a nonlinear transformation of the ex is before we do the linear combination.",
                    "label": 0
                },
                {
                    "sent": "Is that a typo or is it?",
                    "label": 0
                },
                {
                    "sent": "Yes, that is a typo.",
                    "label": 0
                },
                {
                    "sent": "It should be fine I X.",
                    "label": 0
                },
                {
                    "sent": "That's right, yes.",
                    "label": 0
                },
                {
                    "sent": "So each each by each basis function you have a number of basis functions and they all act on X.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This is the problem with being videoed.",
                    "label": 0
                },
                {
                    "sent": "Is all my errors.",
                    "label": 0
                },
                {
                    "sent": "Going to go on the web now.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not obviously you will do this over a number of inputs X as well, so you know there are also putting on, yeah.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we're forming a nonlinear transform of the inputs and then we form a linear model.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, we can still call this a linear model, but it's still linear in the parameters, but it's starting to get a bit tenuous now to call this a linear model.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause actually it's a nonlinear input output relationship.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The advantage is.",
                    "label": 0
                },
                {
                    "sent": "We can still apply the simple estimation that we did before to finding the parameters.",
                    "label": 1
                },
                {
                    "sent": "OK, so although this is a non linear model in terms of basis functions, it's nonlinear input output relationship.",
                    "label": 0
                },
                {
                    "sent": "It is linear in the.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analysis, and therefore we can still apply the same.",
                    "label": 0
                },
                {
                    "sent": "Sort of cost function that we did before and I'll show you that in a minute.",
                    "label": 0
                },
                {
                    "sent": "What I wanted to introduce here's just quickly is.",
                    "label": 0
                },
                {
                    "sent": "This is the sort of support vector machine way of thinking about doing these nonlinear transformations.",
                    "label": 0
                },
                {
                    "sent": "OK, I put these in now 'cause it may help you a bit for tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So what we've got here?",
                    "label": 0
                },
                {
                    "sent": "Is a set of data, and that's actually been generated from sort of squared type relationship, so it is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Perhaps not the best example, 'cause it looks quite linear.",
                    "label": 0
                },
                {
                    "sent": "What we do and these are our inputs X.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We transform them.",
                    "label": 0
                },
                {
                    "sent": "Into a space.",
                    "label": 0
                },
                {
                    "sent": "So into the sort of basis function space.",
                    "label": 0
                },
                {
                    "sent": "Where they become nice and linear and we can fit a linear model in that space so we can sort of think of here, these are our inputs X.",
                    "label": 0
                },
                {
                    "sent": "Here these data points are now are Phis axing on the X is.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are sort of basis function data points.",
                    "label": 0
                },
                {
                    "sent": "And we can form the linear model in that basis function datapoint space.",
                    "label": 0
                },
                {
                    "sent": "We do that format.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model, but then when we transform back.",
                    "label": 0
                },
                {
                    "sent": "We actually get in.",
                    "label": 0
                },
                {
                    "sent": "Actually it was a sine wave that is generated from.",
                    "label": 0
                },
                {
                    "sent": "We get this sine wave relationship in terms of the input output relationship, so it's linear.",
                    "label": 0
                },
                {
                    "sent": "In the basis function space, but it's nonlinear in the inputs output space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't want to dwell.",
                    "label": 0
                },
                {
                    "sent": "Too much on that.",
                    "label": 0
                },
                {
                    "sent": "'cause you'll learn more about lats tomorrow, where this or nonlinear transformations become far more important in terms of understanding support vector machines.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we return to doing our parameter estimation, we gain form up a design matrix and we do it in this way now, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Comes to that later or I will sort of come to that later.",
                    "label": 0
                },
                {
                    "sent": "But there is problem is there is no answer.",
                    "label": 0
                },
                {
                    "sent": "OK, but I will.",
                    "label": 0
                },
                {
                    "sent": "I will touch on that at the end of this lecture.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But yes, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "And no one has solved it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we now define our design matrix in this way.",
                    "label": 0
                },
                {
                    "sent": "So we gain we populate it now with our basis functions.",
                    "label": 0
                },
                {
                    "sent": "So we got the five one to five M. So acting on X1 to XN.",
                    "label": 0
                },
                {
                    "sent": "OK, so acting over all of our big N data points that we've measured, so we form that matrix and that we, uh, M by N size matrix.",
                    "label": 0
                },
                {
                    "sent": "And then we estimate the parameters in exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "That we did for the pure linear model.",
                    "label": 0
                },
                {
                    "sent": "So this is the beauty of this approach.",
                    "label": 0
                },
                {
                    "sent": "So modeling nonlinear input output relationships.",
                    "label": 0
                },
                {
                    "sent": "Is that in terms of doing the estimation?",
                    "label": 0
                },
                {
                    "sent": "Everything is the same, so that's why I sort of bash this under the linear models.",
                    "label": 0
                },
                {
                    "sent": "Because in terms of the important, but it still is linear, yeah?",
                    "label": 0
                },
                {
                    "sent": "Number one.",
                    "label": 0
                },
                {
                    "sent": "You could in we.",
                    "label": 0
                },
                {
                    "sent": "Typically when we start moving into basis function models like this, which typically start ignoring the bias, you could still put in.",
                    "label": 0
                },
                {
                    "sent": "You could still treat the bias as another basis function and keep the bias in there, but normally.",
                    "label": 0
                },
                {
                    "sent": "Or these models will be flexible enough that they will model the bias themselves?",
                    "label": 0
                },
                {
                    "sent": "That is all they will accommodate the bias.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, you have an.",
                    "label": 0
                },
                {
                    "sent": "No, you've only got you only ever have big N observations.",
                    "label": 0
                },
                {
                    "sent": "In both cases, what's happened now is here, I suppose, in the first in the linear model it was dimension little M + 1 by Big N, and now it's just little M. By big N. But actually all of trust me, the the dimensions do fit OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an example now is we're going back to our sync function.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So pretty much similar sort of data to what we have before that was the fit.",
                    "label": 0
                },
                {
                    "sent": "That I formed by some basis function model.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which will come on.",
                    "label": 0
                },
                {
                    "sent": "So in a minute and there is the true function.",
                    "label": 0
                },
                {
                    "sent": "And we just happen to know the true focus.",
                    "label": 0
                },
                {
                    "sent": "I've generated it.",
                    "label": 0
                },
                {
                    "sent": "In this case an reasonable if it's not particularly good.",
                    "label": 0
                },
                {
                    "sent": "In that case, I.",
                    "label": 0
                },
                {
                    "sent": "Argue.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The thing is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Was that model forms where it is formed because I chose?",
                    "label": 0
                },
                {
                    "sent": "A radial basis function neural network with Gaussian Bell shaped basis functions.",
                    "label": 0
                },
                {
                    "sent": "And I've got 123458 online, maybe 10 of them.",
                    "label": 0
                },
                {
                    "sent": "There is a spread across the input domain.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we do is we multiply.",
                    "label": 0
                },
                {
                    "sent": "We learn the weights, so we multiply each of those basis functions by its respective wait.",
                    "label": 0
                },
                {
                    "sent": "And now these are the weighted basis functions as there are positive and negative weights.",
                    "label": 0
                },
                {
                    "sent": "And then what we do is we just.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Has all of those lines together?",
                    "label": 0
                },
                {
                    "sent": "And that gives us the final.",
                    "label": 0
                },
                {
                    "sent": "Function estimate.",
                    "label": 0
                },
                {
                    "sent": "So it's simply always trying to represent is actually simply a weighted linear combination of the basis functions.",
                    "label": 0
                },
                {
                    "sent": "Does everyone yeah?",
                    "label": 0
                },
                {
                    "sent": "That's another question that's just as awkward as choosing the basis functions in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "And again I will mention it, but not answer that question abit later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so it comes another yeah.",
                    "label": 0
                },
                {
                    "sent": "Another very very important question that no one has answered.",
                    "label": 0
                },
                {
                    "sent": "Also, I must have it through this week.",
                    "label": 0
                },
                {
                    "sent": "There will probably be more unanswered questions and answer questions.",
                    "label": 0
                },
                {
                    "sent": "You will probably find.",
                    "label": 0
                },
                {
                    "sent": "This is why there's still so many of you doing research and data modeling.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's an example finally of overfitting.",
                    "label": 0
                },
                {
                    "sent": "So here's an example where it's gone horribly wrong, so I've managed to force this one to go really badly.",
                    "label": 0
                },
                {
                    "sent": "OK, and I hope you would agree that the solid line there is not a terribly good representation of the true function.",
                    "label": 0
                },
                {
                    "sent": "But more on the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "OK, um this is the bit where the title of the slide is not quite right because I've talked about linear classification and I will need to clarify what I mean by linear classification and what I mean by linear classification.",
                    "label": 0
                },
                {
                    "sent": "Certainly initially is that we're getting linear decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "OK. We will actually have to use nonlinear techniques to get linear decision boundaries, but when I talk about here linear classification, what I mean is forming linear decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "I'm not.",
                    "label": 0
                },
                {
                    "sent": "There are a number of different techniques for forming linear decision boundaries are not going to go into all my.",
                    "label": 0
                },
                {
                    "sent": "Just going to briefly go through some basic ideas here, but the key point now.",
                    "label": 0
                },
                {
                    "sent": "Is our targets are not on the real line.",
                    "label": 0
                },
                {
                    "sent": "Our targets are now what is known as categorical, so they belong to class One class two Class 3.",
                    "label": 0
                },
                {
                    "sent": "So using the notation I came up with earlier, outputs will be zero or one.",
                    "label": 0
                },
                {
                    "sent": "So it's starting to get slightly different now, so you can apply techniques like discriminant analysis, probit analysis, log linear regression, logistic regression, and this is where again, This is why.",
                    "label": 0
                },
                {
                    "sent": "When I was talking about the examples in my first lecture and I sort of said the first 2 examples were regression, that wasn't the best term to use their really their curve fitting because actually regression type type techniques can be used or are regularly used within classification.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to actually what we're doing here is these are regression for categorical outputs.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "I don't know, do you know, Rob?",
                    "label": 0
                },
                {
                    "sent": "For regression.",
                    "label": 0
                },
                {
                    "sent": "The reason?",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "With the kinds of costs.",
                    "label": 0
                },
                {
                    "sent": "I think the simple answer is.",
                    "label": 0
                },
                {
                    "sent": "As in any area, data modeling is full of abuse of notation.",
                    "label": 0
                },
                {
                    "sent": "And terminology and people use regression.",
                    "label": 0
                },
                {
                    "sent": "When they don't necessarily mean it and they use.",
                    "label": 0
                },
                {
                    "sent": "I mean not only my experiences regression.",
                    "label": 0
                },
                {
                    "sent": "Is most often used to refer to Curphey sing on you agree price?",
                    "label": 0
                },
                {
                    "sent": "I'm wrong?",
                    "label": 0
                },
                {
                    "sent": "Perhaps that maybe me being poor in my notation.",
                    "label": 0
                },
                {
                    "sent": "Actually estimating.",
                    "label": 0
                },
                {
                    "sent": "Estimated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's not dwell on it.",
                    "label": 0
                },
                {
                    "sent": "So the key thing here is our aim is to get a linear decision boundary, and we'll see that we could generalize that in the same way we did for the curve fitting.",
                    "label": 0
                },
                {
                    "sent": "By using basis functions as.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it's getting nonlinear decision boundaries, so.",
                    "label": 0
                },
                {
                    "sent": "What we do is we now assume that our data is Bernoulli distributed, so we now.",
                    "label": 0
                },
                {
                    "sent": "Because we're only looking at ones and zeros, we can't now have Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "On our targets, so when something is wrong, what it means is that we predicted to be class one, but actually it was Class 0, or we predict it's for class zero and actually it was Class 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so the difference is 1 either way, so we were no longer adding in a continuum of Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So this is known as Bernoulli distributed in the sense that it can only take the data can only take one of two possible values.",
                    "label": 0
                },
                {
                    "sent": "And the noise will be associated with with that fact.",
                    "label": 0
                },
                {
                    "sent": "What we then do is we do actually form a linear model.",
                    "label": 0
                },
                {
                    "sent": "But we form.",
                    "label": 0
                },
                {
                    "sent": "The linear model here.",
                    "label": 0
                },
                {
                    "sent": "On what is known as.",
                    "label": 0
                },
                {
                    "sent": "The probe it or the logic sorry notation.",
                    "label": 0
                },
                {
                    "sent": "Also the logic which is the log of P / 1 -- P where P. Is the probability that your output is equal to 1 given X?",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're now we're not actually modeling.",
                    "label": 0
                },
                {
                    "sent": "The targets are all the measured data directly in terms of our linear model.",
                    "label": 0
                },
                {
                    "sent": "We're now actually modeling this logic function, he which is defined in this way, so it's it's the probability that y = 1 given X, so we've now had.",
                    "label": 0
                },
                {
                    "sent": "Again, we've done a transformation.",
                    "label": 0
                },
                {
                    "sent": "Affective our data.",
                    "label": 0
                },
                {
                    "sent": "To model something slightly different, but it allows us to still apply a linear model.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Color.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "But this is remember, this is your summing together here I'm.",
                    "label": 0
                },
                {
                    "sent": "Looking through as I'm going through this, I like the notation.",
                    "label": 0
                },
                {
                    "sent": "I've been slightly sloppy with notation for the exercise in terms of in some cases I think I've used XC to refer to.",
                    "label": 0
                },
                {
                    "sent": "The end different sexes and sometimes I've used it to refer to the M variables of X.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I've.",
                    "label": 0
                },
                {
                    "sent": "Not being brilliant with my card, not being perfectly consistent with my notation here.",
                    "label": 0
                },
                {
                    "sent": "No little M. So what this is so?",
                    "label": 0
                },
                {
                    "sent": "This is for a single input.",
                    "label": 0
                },
                {
                    "sent": "It's a way to sum over its variables.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I say exactly what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Which will become again, slightly clearer when I show you an example in a minute.",
                    "label": 0
                },
                {
                    "sent": "So yes, and now.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're doing effectively a regression.",
                    "label": 0
                },
                {
                    "sent": "On the probabilities rather than.",
                    "label": 0
                },
                {
                    "sent": "On the data itself.",
                    "label": 0
                },
                {
                    "sent": "Is that fair to say that a fair comment?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And this thing here.",
                    "label": 0
                },
                {
                    "sent": "This just comes about if you reverse this equation arounds then P is equal to becomes equal to this thing here.",
                    "label": 0
                },
                {
                    "sent": "So that's the that's what the probability is equal to.",
                    "label": 0
                },
                {
                    "sent": "Which becomes useful if you want to play.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The surface is we'll see in a minute.",
                    "label": 0
                },
                {
                    "sent": "So can we generalize this where we can generalize it in exactly the same way we did for curve fitting instead of having this purely linear model?",
                    "label": 0
                },
                {
                    "sent": "Here we can track, we can replace it with a basis function model instead.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Whereas.",
                    "label": 0
                },
                {
                    "sent": "In this hot case, you could only get purely linear straight lines or planes.",
                    "label": 0
                },
                {
                    "sent": "Decision boundaries in this case by.",
                    "label": 0
                },
                {
                    "sent": "Expanding is the basis functions first.",
                    "label": 0
                },
                {
                    "sent": "You can then get nonlinear decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "I kept this in here because so many were still using an inherently so linear looking model.",
                    "label": 0
                },
                {
                    "sent": "Although you know we're now getting quite nonlinear in terms of what we're really doing well, certainly we're now getting nonlinear input output relationships.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we can think about this in the sort of kernel community way in terms of mappings.",
                    "label": 0
                },
                {
                    "sent": "So say we've got some classification data, so we've got two clusters of points and Dotson, some X is.",
                    "label": 0
                },
                {
                    "sent": "And it looks like there's probably a nonlinear relationship that will separate those two clusters of data points.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do?",
                    "label": 0
                },
                {
                    "sent": "Is we map it into through the basis functions into a new space where they become linearly separable.",
                    "label": 0
                },
                {
                    "sent": "So now our clusters of data points are the basis function.",
                    "label": 0
                },
                {
                    "sent": "Data points in affects we form a linear decision boundary and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That space.",
                    "label": 0
                },
                {
                    "sent": "But then when we come back to our standard space.",
                    "label": 0
                },
                {
                    "sent": "It becomes a non linear decision boundary again or not as well too much on this.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Note you can do so, so there are.",
                    "label": 0
                },
                {
                    "sent": "I mean the classic theorem is what's known as the stone via Strauss Theorem, which is a universal approximation theorem, and that says that for most linear in the parameters basis function models they can approximate.",
                    "label": 0
                },
                {
                    "sent": "Any continuous function to an arbitrary accuracy that's paraphrase slightly, but so if this is there, are you know there are certain types of basis functions that wouldn't be able to do that?",
                    "label": 0
                },
                {
                    "sent": "But as long as you're reasonably careful about choosing your beta, most of the common ones that we use.",
                    "label": 0
                },
                {
                    "sent": "Can approximate anything you want.",
                    "label": 0
                },
                {
                    "sent": "Rob will.",
                    "label": 0
                },
                {
                    "sent": "He's going to talk about most layer perceptrons later on which are non linear nonlinear model.",
                    "label": 0
                },
                {
                    "sent": "If you want to call it that and there are certain advantages of using those types of models.",
                    "label": 0
                },
                {
                    "sent": "OK they made it there more compact in terms of representation.",
                    "label": 0
                },
                {
                    "sent": "But to be honest, a lot of this comes down to what is your favorite?",
                    "label": 0
                },
                {
                    "sent": "OK, most models.",
                    "label": 0
                },
                {
                    "sent": "Given enough effort, most models will do pretty much as well as each other.",
                    "label": 0
                },
                {
                    "sent": "There are advantages and disadvantages in other ways.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Classification boundary golf, linear fire as well if you do it, you sort of go back you.",
                    "label": 0
                },
                {
                    "sent": "What we've this is so this is our original space.",
                    "label": 0
                },
                {
                    "sent": "OK, here this is sort of some imaginary basis function space.",
                    "label": 0
                },
                {
                    "sent": "And then if we were to transform back.",
                    "label": 0
                },
                {
                    "sent": "This is what we get, but here we've basically gone back to the original space.",
                    "label": 0
                },
                {
                    "sent": "We actually never, you know, you would never show all of this what we actually you know.",
                    "label": 0
                },
                {
                    "sent": "You just see this in terms of what you would plot.",
                    "label": 0
                },
                {
                    "sent": "You would never plot this thing up here.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, you just do the solid arrow there within classification are not going to dwell on doing parameter estimation other than to say that what you there are a number of different approaches.",
                    "label": 0
                },
                {
                    "sent": "Most common one with sorting maximum likelihood estimation on what you're trying to do here is to maximize the probability of getting the observed results given the parameters, and that's what maximum likelihood does there through the week.",
                    "label": 1
                },
                {
                    "sent": "You'll hear more about some other techniques for doing classification estimation.",
                    "label": 0
                },
                {
                    "sent": "But here we're not going to.",
                    "label": 1
                },
                {
                    "sent": "The main point to take is that although there is a unique minimum, there's now no closed form solution.",
                    "label": 0
                },
                {
                    "sent": "So whereas for curve fitting I could write the fire transpose 5 -- 1 * 5 transpose Ed.",
                    "label": 0
                },
                {
                    "sent": "There's no longer that equation OK or similar equation.",
                    "label": 0
                },
                {
                    "sent": "We now have to use iterative type techniques, sort of gradient type methods to find the solution.",
                    "label": 0
                },
                {
                    "sent": "OK, we're not going to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's not the time to get too much into that.",
                    "label": 0
                },
                {
                    "sent": "So here we've got.",
                    "label": 0
                },
                {
                    "sent": "Are two clusters of data points?",
                    "label": 0
                },
                {
                    "sent": "From what I recall, I think they're both downstream distributed cloud data points, which has allowed me to, so I know the true or optimum decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Overlap.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In them.",
                    "label": 0
                },
                {
                    "sent": "That is a linear.",
                    "label": 0
                },
                {
                    "sent": "That's the optimum linear decision boundary and you see to me that looks pretty realistic.",
                    "label": 0
                },
                {
                    "sent": "Looks like a fairly good fit.",
                    "label": 0
                },
                {
                    "sent": "So the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Whoops",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alternatively class is a non linear decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Been pushed in.",
                    "label": 0
                },
                {
                    "sent": "OK, by using a fairly simple basis function model.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you look at that, I don't know about you, but I would argue that the linear decision boundary probably looks better.",
                    "label": 0
                },
                {
                    "sent": "I don't know who who thinks the linear decision boundary is better.",
                    "label": 0
                },
                {
                    "sent": "If you have 1/2 of you.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The true is the dashed line, so actually.",
                    "label": 0
                },
                {
                    "sent": "The nonlinear decision boundaries probably is better, but this comes back to the issue about.",
                    "label": 0
                },
                {
                    "sent": "What's a good model and what in your own do you really want to get the truth?",
                    "label": 0
                },
                {
                    "sent": "Do you want to get the true model?",
                    "label": 0
                },
                {
                    "sent": "And in this case it may well be that linear decision boundary is more than good enough for the purposes of your application.",
                    "label": 0
                },
                {
                    "sent": "And that you don't really know need to know what that is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The thing is.",
                    "label": 0
                },
                {
                    "sent": "This is an example, so you can actually plot the class probabilities as well.",
                    "label": 1
                },
                {
                    "sent": "So for that example, actually the decision boundary is formed by cussing.",
                    "label": 0
                },
                {
                    "sent": "This goes from zero to one and incision boundaries formed by cutting this surface at .5.",
                    "label": 0
                },
                {
                    "sent": "So if the probability is below .5, it belongs to one class, and if it's above .5, it belongs to the Class 1.",
                    "label": 0
                },
                {
                    "sent": "So in terms of this is what the curve fitting surface looks like, that you've actually approximated.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we look at those decision boundaries, they just formed by.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crossing that surface at the .5 level.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example where it can all go wrong.",
                    "label": 0
                },
                {
                    "sent": "So whereas in the curve fitting example I showed some overfitting, here by re running this experiment a number of times and re calculating the decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "We've got slightly different realizations of the data points, and we'll see that our lines are solid.",
                    "label": 0
                },
                {
                    "sent": "Lines do vary quite a bit between the different realizations of data points, and this is a particular problem with doing this, because again, we're doing maximum likelihood, which is very similar to what we're doing for curve fitting when we're minimizing the sum of squared errors.",
                    "label": 0
                },
                {
                    "sent": "So there's a possibility.",
                    "label": 0
                },
                {
                    "sent": "But although we're not exactly overfitting, we're getting very different results each time we fit the data.",
                    "label": 0
                },
                {
                    "sent": "You know, if you look here.",
                    "label": 0
                },
                {
                    "sent": "Very different looking curve in that case at the top here the non linear decision boundary is actually curved in the opposite direction, so really doesn't look like a very good example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things can go wrong.",
                    "label": 0
                },
                {
                    "sent": "Right very quickly before the break, I'm going to touch on a couple of the questions that were asked during the talk.",
                    "label": 0
                },
                {
                    "sent": "We also need to estimate or decide on the type of basis function.",
                    "label": 1
                },
                {
                    "sent": "The number of basis functions and the positions of those basis functions.",
                    "label": 1
                },
                {
                    "sent": "If we're doing basis function.",
                    "label": 0
                },
                {
                    "sent": "I'm type modeling data modeling.",
                    "label": 1
                },
                {
                    "sent": "And I could just sort of stop and say that last sentence.",
                    "label": 0
                },
                {
                    "sent": "These are nonlinear problems and they are difficult.",
                    "label": 0
                },
                {
                    "sent": "OK, no one has come up with a solution.",
                    "label": 0
                },
                {
                    "sent": "There are ideas on how to do some of this stuff.",
                    "label": 0
                },
                {
                    "sent": "But to be honest.",
                    "label": 0
                },
                {
                    "sent": "You know these are just guidelines and that's why.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Touch on some guidelines.",
                    "label": 0
                },
                {
                    "sent": "Types of basis functions.",
                    "label": 0
                },
                {
                    "sent": "Usually choose your favorite.",
                    "label": 0
                },
                {
                    "sent": "OK, I think most people within data modeling they latch on to one particular basis function and it's nine times out of 10.",
                    "label": 0
                },
                {
                    "sent": "It's the Gaussian radial basis function and they stick with that for their whole career.",
                    "label": 0
                },
                {
                    "sent": "OK, if you ask if you ask them why in a few to ask him in a particularly nasty question, the PhD vibe will be so after student.",
                    "label": 0
                },
                {
                    "sent": "While I have you chosen a Gaussian basis function, I bet they wouldn't be able to answer you.",
                    "label": 0
                },
                {
                    "sent": "OK, I probably just say last one, my supervisor told me to use.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There are certain properties of some basis functions which make them a bit more suitable, perhaps in some cases than others.",
                    "label": 0
                },
                {
                    "sent": "But to be honest, I would you know, and I'm a classic example is I pretty much always use the Garrison basis function.",
                    "label": 0
                },
                {
                    "sent": "OK, I find easy to work with.",
                    "label": 0
                },
                {
                    "sent": "OK, so that.",
                    "label": 0
                },
                {
                    "sent": "Perhaps sort of answers your question on the type of base function, yeah?",
                    "label": 0
                },
                {
                    "sent": "Hi, yeah, now you're getting into heavy juicy stuff.",
                    "label": 0
                },
                {
                    "sent": "Yes, in theory.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "In practice, I think it's far more important in terms of how many base functions you've got.",
                    "label": 0
                },
                {
                    "sent": "But certainly if you were to choose, the Gaussian is quite isn't a nice smooth basis function.",
                    "label": 0
                },
                {
                    "sent": "But then again, that depends on the width you choose.",
                    "label": 0
                },
                {
                    "sent": "So within even within the Gaussians you've got this parameter Sigma squared, which basically determines how peaks that basis function is.",
                    "label": 0
                },
                {
                    "sent": "So there's actually a whole class, so even even if you choose gastron.",
                    "label": 0
                },
                {
                    "sent": "You still gotta choose what width of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "There's actually an infinite number of Gaussians you can choose.",
                    "label": 0
                },
                {
                    "sent": "OK, and we again will touch a bit more on that because that becomes what's known as a hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "And again, trying to estimate that becomes really difficult.",
                    "label": 0
                },
                {
                    "sent": "So even if you choose a basis function, you've often still got other things that you need to estimate for that basis function.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "Choosing the base function can have an effect on the quality in terms of how wiggly your function is or how smooth it is.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "What's what degree of polynomial?",
                    "label": 0
                },
                {
                    "sent": "Are you going to choose?",
                    "label": 0
                },
                {
                    "sent": "How many Fourier component, how?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Yeah, because then you're having sweet you having to invert an infinite by big N activation matrix.",
                    "label": 0
                },
                {
                    "sent": "This will probably be touched on tomorrow when support vector machines are talked about because there are tricks for working with infinite size basis bases.",
                    "label": 0
                },
                {
                    "sent": "But I can pretty much guarantee whatever basis function you give to me.",
                    "label": 0
                },
                {
                    "sent": "I will give you a problem with it.",
                    "label": 0
                },
                {
                    "sent": "OK, so whether it's the width of it, whether it's the size you know with the polynomial, is the polynomial degree.",
                    "label": 0
                },
                {
                    "sent": "There's always something you've got to choose, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Or estimates?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the type of basis functions, next problem, or how many basis functions are we going to want?",
                    "label": 1
                },
                {
                    "sent": "I suppose in this case that, or equivalently the polynomial degree.",
                    "label": 0
                },
                {
                    "sent": "Again, who knows?",
                    "label": 0
                },
                {
                    "sent": "OK, there is no single answer to this.",
                    "label": 0
                },
                {
                    "sent": "There are some ideas, one of them.",
                    "label": 0
                },
                {
                    "sent": "Is practice.",
                    "label": 1
                },
                {
                    "sent": "You slowly increase the number of basis functions until you start overfitting the data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You know, somehow you have to explore it.",
                    "label": 0
                },
                {
                    "sent": "OK, there is no there there is.",
                    "label": 0
                },
                {
                    "sent": "Although there might in theory be an optimal number of basis functions, you aren't going to find it because it's a nonlinear optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So unless you do an exhaustive search.",
                    "label": 0
                },
                {
                    "sent": "OK, you can't do this.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's difficult.",
                    "label": 0
                },
                {
                    "sent": "It's a nonlinear problem.",
                    "label": 0
                },
                {
                    "sent": "Often the other thing I often people just put so as well as slowly increasing the number.",
                    "label": 0
                },
                {
                    "sent": "The other possibility and what I did in my examples.",
                    "label": 0
                },
                {
                    "sent": "Oh, sorry, yes.",
                    "label": 0
                },
                {
                    "sent": "That's that's a number come on two way.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We put them now.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so now the positions are the basis function.",
                    "label": 0
                },
                {
                    "sent": "This becomes really difficult.",
                    "label": 0
                },
                {
                    "sent": "And again, there's no definitive answer.",
                    "label": 0
                },
                {
                    "sent": "Put some ideas, one is that you just put a basis function on each data points.",
                    "label": 1
                },
                {
                    "sent": "So if you go back to the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Angstrom these see eyes pointing.",
                    "label": 0
                },
                {
                    "sent": "You can't see that the CIS are the sensors.",
                    "label": 0
                },
                {
                    "sent": "You could put those just locate the sensors on each data points.",
                    "label": 0
                },
                {
                    "sent": "That's one possibility, but if your data isn't particularly well distributed, you may end up with loads of basis functions in one area and not many in another area.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, so an alternative.",
                    "label": 0
                },
                {
                    "sent": "Is have a uniform grid, so just uniformly space your basis functions.",
                    "label": 1
                },
                {
                    "sent": "That's OK in one 2, maybe 3 dimensions.",
                    "label": 1
                },
                {
                    "sent": "If you want to do a uniform distribution of basis functions in 100 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And you want 10 per axis you've got.",
                    "label": 0
                },
                {
                    "sent": "Either tends to power 100 or hundreds of hours 10.",
                    "label": 1
                },
                {
                    "sent": "Big number number of basis functions and you have to estimate a parameter with each one so that can become a problem.",
                    "label": 0
                },
                {
                    "sent": "I will say one of the advantages of using global basis functions, so if you.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back.",
                    "label": 0
                },
                {
                    "sent": "So the polynomials you don't have to locate these anywhere.",
                    "label": 0
                },
                {
                    "sent": "That sort of global.",
                    "label": 0
                },
                {
                    "sent": "They're just there.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't have.",
                    "label": 0
                },
                {
                    "sent": "There's no sense or anything, so advantage of basis functions like polynomials.",
                    "label": 0
                },
                {
                    "sent": "Is it you don't need to choose the positions?",
                    "label": 0
                },
                {
                    "sent": "That may be one advantage of using those.",
                    "label": 0
                },
                {
                    "sent": "My experience often though is using global based functions.",
                    "label": 0
                },
                {
                    "sent": "They can be extremely sensitive to noise.",
                    "label": 0
                },
                {
                    "sent": "So the reason we don't always you can like well why don't we always use polynomials?",
                    "label": 0
                },
                {
                    "sent": "We're actually they have other disadvantages and certainly one of them.",
                    "label": 0
                },
                {
                    "sent": "From my experience is they can be very very sensitive to noise for certainly the high order polynomials.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are there any questions on basis functions?",
                    "label": 0
                },
                {
                    "sent": "That would be similar.",
                    "label": 0
                },
                {
                    "sent": "Again, you might slowly increase the polynomial.",
                    "label": 0
                },
                {
                    "sent": "The degree of the polynomial you mean.",
                    "label": 0
                },
                {
                    "sent": "They're just the whites.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you go back here, the polynomial here, each of those, they're just your basis functions.",
                    "label": 0
                },
                {
                    "sent": "So think of those as your faize.",
                    "label": 0
                },
                {
                    "sent": "So X1 squared is just a Phi.",
                    "label": 0
                },
                {
                    "sent": "Uvex effectively.",
                    "label": 0
                },
                {
                    "sent": "And each of those will have a WI in front of it, and those are the same parameters.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see when I.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't know you wouldn't.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't do it that way.",
                    "label": 0
                },
                {
                    "sent": "You would have.",
                    "label": 0
                },
                {
                    "sent": "Your final model would be that complete tense all the polynomial.",
                    "label": 0
                },
                {
                    "sent": "Your basis functions would be 1X1X2X1 squared X2 squared up to.",
                    "label": 0
                },
                {
                    "sent": "X1 X 2X redirect to X10 plus all of those other combinations of 10th powers of X.",
                    "label": 0
                },
                {
                    "sent": "One X2 X.",
                    "label": 0
                },
                {
                    "sent": "You know if you have X1 to 10X19 next to.",
                    "label": 0
                },
                {
                    "sent": "And you would end up with a model I suspect with millions of basis functions and that way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think this is the final thing.",
                    "label": 0
                },
                {
                    "sent": "I've over on Obetts quickly is a note on data.",
                    "label": 1
                },
                {
                    "sent": "How much data do we need?",
                    "label": 1
                },
                {
                    "sent": "Can anyone answer?",
                    "label": 0
                },
                {
                    "sent": "OK, so you have a fairly good answer as much as you can.",
                    "label": 0
                },
                {
                    "sent": "Next question.",
                    "label": 0
                },
                {
                    "sent": "Do you just need to use all of that data to train your model?",
                    "label": 1
                },
                {
                    "sent": "Who thinks you use all of your data to train the model?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so that's.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so again, how much is this?",
                    "label": 0
                },
                {
                    "sent": "Use what you get?",
                    "label": 0
                },
                {
                    "sent": "So what about validating and testing the model?",
                    "label": 1
                },
                {
                    "sent": "So for those.",
                    "label": 0
                },
                {
                    "sent": "Who haven't done much on data modeling before, it's very important that you need three sets of data.",
                    "label": 0
                },
                {
                    "sent": "You need a training set, validation sets, and a testing set.",
                    "label": 0
                },
                {
                    "sent": "We use the training data to train the parameters.",
                    "label": 0
                },
                {
                    "sent": "We use the validation data set normally within hyperparameter estimation, so so to do things like when you are changing the number of basis functions, changing the basis function width.",
                    "label": 0
                },
                {
                    "sent": "You check your model on the validation data set, but you should always leave what's known as the testing datasets for the very ends.",
                    "label": 0
                },
                {
                    "sent": "OK, just to check that your final model hasn't been biased in any way by the training and validation datasets.",
                    "label": 1
                },
                {
                    "sent": "I guess the training and validation datasets are used in doing finding the model.",
                    "label": 0
                },
                {
                    "sent": "And then there's a final Saturday to the testing set, which is that final Test that yes, your model is a good model.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean yes, there is an applied assumption that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, all of the data that has the same underlying properties, if it's nonstationary then on that's a whole different sort of not really covering nonstationary data modeling here.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions on data?",
                    "label": 0
                },
                {
                    "sent": "In terms of how many days points you need in the OR how you divide up your datasets in terms of these, again that's a bit of an open question.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You often.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enough in each.",
                    "label": 0
                },
                {
                    "sent": "So concluding remarks always try the simplest possible model first, I think that's probably the most important thing to take away from this particular as you go into the rest of the week where you're going to hear about some quite complicated modeling techniques and people who make big grand claims that their technique is the best.",
                    "label": 1
                },
                {
                    "sent": "Always try a simple 1 first.",
                    "label": 0
                },
                {
                    "sent": "If it works, you don't need to go anymore or acts as a benchmark.",
                    "label": 1
                },
                {
                    "sent": "You can make a linear models nonlinear in terms of the input output relationship via straightforward basis function expansions.",
                    "label": 0
                },
                {
                    "sent": "But you end up in the ends with nonlinear optimization because you gotta choose things like basis function widths, positions of basis functions becomes a big problem.",
                    "label": 0
                },
                {
                    "sent": "The final question that I want to leave you with to think about through the rest of week is least squares or maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "The best way I don't want you answer science that now just think about it.",
                    "label": 0
                },
                {
                    "sent": "OK, that's me.",
                    "label": 0
                },
                {
                    "sent": "Finished for now.",
                    "label": 0
                }
            ]
        }
    }
}