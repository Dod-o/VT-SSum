{
    "id": "244f2bnnpbjchoqoal2qmgqvqnd2r6v2",
    "title": "Applications of bandits and recommendation systems",
    "info": {
        "author": [
            "Nicolas Le Roux, Criteo"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_le_roux_recommendation_system/",
    "segmentation": [
        [
            "Can you hear me?",
            "Yes, excellent so thanks for the introduction.",
            "As she said, I was quite intimidated when I saw Lisa speakers who all know.",
            "When they talk about when they talk about reinforcement learning, I came to reinforcement."
        ],
        [
            "Think about two years ago and but the idea is I might present some aspects of our L that are less common in the academic community which which I think are just as interesting.",
            "So just a disclaimer, that's my own experience.",
            "That's how I did things when I was working on actual product.",
            "It turns out that it will be focused on online advertising because that's why I use reinforcement learning.",
            "What I present is not at all limited.",
            "To that industry, and it's really a more general statement on how you can use reinforcement learning in the industry."
        ],
        [
            "So usually when you think about reinforcement learning, we think about two things.",
            "The first one it's multistep episodes, so there's this idea in reinforcement learning that you're going to take an action that's going to influence the state, and then you're going to take another action, and so you're going to take a sequence of actions and you don't want to be too greedy because you don't just want to maximize reward in the next step, you want to maximize reward at the end of the episode, and so I'm assuming that's what most of the talks for events, whether it's Carrie, go.",
            "Helicopters?",
            "There's another aspect to reinforcement learning is the aspect of reward evaluation and maximization, so there's really an important bit in RL, which we usually define which reward we're interested in, and we try to maximize that reward, and it turns out that in this talk it's really this aspect I'm going to focus on.",
            "So just to be clear, it's going to be an RL talk with episodes of length one.",
            "OK, so there's going.",
            "There's not going to be any sequence of actions.",
            "I want to optimize an really what I want to show you is that PRL has much more to offer to the Community and to the industry then just a multistep episodes.",
            "So it's going to be a simple case, but the point of view again that few people have."
        ],
        [
            "So I mentioned online advertising it's going to be a sub part of online advertising called retargeting.",
            "And again that I mean it's advertising.",
            "It's probably boring to most of you.",
            "It's boring to me, but it offers some interesting challenges.",
            "So what's retargeting whenever the user browse the web?",
            "The user lands on a web page and then potentially there will be ads displayed on that webpage, and so how are these ads chosen while the website Contacts what we call an ad exchange, which is basically an Organism that runs an auction, the ad exchange Contacts all the people willing to display ads, and then there's an auction going on.",
            "Its competitor tells the ad exchange how much they're willing to bid for the right to display one specific.",
            "Add and then the highest bidder wins the right to display in AD.",
            "So at my previous company or this work was done, we were participating in about 20 billion auctions today and we're just playing from 2 to 3 billion ads per day.",
            "And again, there are plenty of interesting works on multi multiple auctions, but right now I'm assuming it's a single log auction, so really the state is what you know about the user webpage.",
            "Annual action is going to be how much you're willing to pay to mature, willing to bid to display in ads on that web page."
        ],
        [
            "So just details of the auction, so that's what's called real time bidding, and most of the time, and I'm not going to talk about the more complicated cases.",
            "It's what we call the second price auction.",
            "That means that everyone's bidding at the same time the highest bidder wins the right to display an ad, but the highest bidder does not pay what they bid.",
            "They only pay with the second highest bidder bid, and that has many, many interesting properties, but the one we concern about today is that the optimal strategy for everyone.",
            "Stupid, the expected game.",
            "Which means that when a user lands on a web page, I have to predict how much on average I'm going to earn if I display an ad, and if that's $0.10, that's what should be my optimal bit.",
            "OK, it's a very simple strategy, and that's why we use this auction scheme.",
            "And So what is said on average?",
            "I'm going to earn $0.10.",
            "What is the expected gain?",
            "Its how much I'm being paid every time there's a click on the ad, so I'm going to display in ad, let's say for Walmart.",
            "Walmart is going to pay me whenever there's a click.",
            "So that's contract, and there's another aspect, which is what is the probability that the user will click on the ADD.",
            "Of course is the property is low, they don't average.",
            "I'm going to be paid little, so we want to be less if the property is high.",
            "I'm going to be paid more on average, so I want a bit more and this property is click is something we don't know.",
            "Is that something we want to estimate?",
            "That's very old, and when people realize that that was the optimal strategy, there was a whole lot of literature on how do you estimate the probability of click.",
            "And right now maybe some of you are wondering where is DRL coming?",
            "And right now there's absolutely no reinforcement learning and this whole field wasn't using any reinforcement learning at all.",
            "And I really I want to show you that they were wrong in this."
        ],
        [
            "So again, you won't find a bidding strategy you want to estimate the probability that there's going to be a click.",
            "So what you have is in the past you've been in previous auctions.",
            "Some of these auctions you lossed, so you don't know what happened, but some of these options you want, and so you got to this plan ad and you got to notice if there was a click or not, and so you have access to a labeled datasets.",
            "OK, with some idea X information about the user, the web page and why is a binary label click or no click.",
            "And whenever I showed is the first reaction to people say, oh that's easy, then I have access to data.",
            "I know how many of you were in the deep learning Summer school, but it's really a supervised learning approach, which is, oh, I have access to label data.",
            "I want to predict the probability of y = 1 given X. I'm going to build a classifier to do that.",
            "And you're going to have a lot of research and people very excited and they want to design new models.",
            "And they're going to have deep models to better this particular property of click."
        ],
        [
            "Once you've done this offline, so you've carefully constructed your model, you have a good model, so you have you followed your machine learning classes.",
            "You have your trainer in test, and you compute it on test, and you saw that you model those better on the test that the current model.",
            "Then you're going to go into production that has a cost, so you involving engineers, UAB test.",
            "Your solution.",
            "That means that what you're going to do is we're going to split the users into 2.",
            "Some of them will have the current production system, some of them will have your modification.",
            "We wait for a few days and in the end we check which system brought the most money.",
            "All is good and dandy until."
        ],
        [
            "So you realize that these are the results you get.",
            "So on the X axis.",
            "You have the offline test performance.",
            "On the Y axis you have how much money did your system actually bring?",
            "And you can see that the correlation is actually really really low.",
            "So there is no guarantee that the better system you have will indeed bring more money, and that's extremely important because everything we do in machine learning we the only thing we hear is that we should care about the test error and that the better test error is is actually something we want to achieve.",
            "It turns out that in these situations you can have a better tester that doesn't do anything for the actual problem you're trying to solve.",
            "And now I'm trying to explain how reinforcement learning can help in these situations, and that's really not a random occurrence that happens all the time.",
            "So usually what happens when you start a company is at first, you model is so bad at everything, you do this better, and so you have a good correlation between tester and revenue.",
            "You super happy and you keep going.",
            "And I'm depressed company.",
            "This lasted for a few years and at some point the correlation starts to decrease and people start to get a bit uncomfortable with this.",
            "And the solution most of the companies do is let's try more things, but that has a cost and so I'm going to give you a principled way of doing things again, which I think is really important."
        ],
        [
            "So why do we have this discrepancy?",
            "There are two implicit assumptions we made when we assume that the test error, so I said RMS here, but the log loss would be about the same.",
            "There are tombs assumptions we made when you said the test error as relevant for what we want to do.",
            "The first one is, we said, oh I'm using the log loss so the test log loss that's going to be a good proxy for the revenue.",
            "If I make a better log loss, I'm going to be a better proxy for that means I'm going to make more revenue.",
            "The other assumptions, and that again, that might be something more familiar with, because that's really the topic of this summer school, is that we assume that the input distribution was the same.",
            "OK, that's really again the assumption we make is that the P of X on the train set and P of X or the test set are the same, and so that's why it makes sense to focus on the test error and what we'll see is that in many real life scenario, that's really not the case.",
            "And what you do influences the input distribution, and So what you do in these cases?",
            "And it might seem obvious to you in this particular setting when you're in a company.",
            "People usually discard this problem.",
            "They say machine learning only taught me how to deal with the same input for distribution for training tests, and so I'm going to make assumptions even though it's wrong."
        ],
        [
            "OK, so just to give you a quick idea, I'm going to do something I've never done before and I was told by many people that was a bad thing to try, but I'm going to try.",
            "I'm going to do a quick demo on the Board of why this is a bad idea, so hopefully you can all see the board.",
            "I don't know if there's some light it needs to be turned on, but.",
            "Let's assume that there is a user that on average clicks 20% of the time.",
            "So CPR is called click through rate.",
            "Is the probability that there is going to click.",
            "So on average a user clicks 20% of the time.",
            "Now your model is going to predict a click probability OK, which I'm going to call, let's say P. And what does the log loss say?",
            "Well, the long, let's say, tells you that the loss you incur for P is going to be equal to minus 0 two times.",
            "Log P -- 0.8 times log.",
            "1 -- P. Hopefully I'm not teaching you anything new right now, so if the probability you have here you have zero here you have one here.",
            "You'll loss look something like this.",
            "OK, with a minimum of 0.2.",
            "And there's really a problem that it's not to present in the oil community, but its presence in pretty much everywhere else in machine learning and no one mentions it.",
            "As you can see, many papers with people define the loss function.",
            "And the way they justify this loss function is they say, the optimum of this loss function is what I want.",
            "They only care about the optimum.",
            "They never care about what's outside, so here they say, oh, I actually want to predict.",
            "Join two.",
            "I have a loss for the minimum is at 0.2.",
            "That's fine, I'm happy.",
            "Now let's see what the actual losses.",
            "So what I want to maximize is my revenue, so it's.",
            "How much or how much money I'm making so my gain?",
            "Minus.",
            "How much I'm paying?",
            "OK, the Cdr is 0.2.",
            "So if I'm winning that display, I'm going to earn 0.2 minus what I paid in junction.",
            "If I'm not winning the display, zero will happen.",
            "And now first, there's something you realize here that's really important is that this depends on what the other people in the auction do.",
            "OK, if you're by yourself in the auction, no matter what you predict, you're going to pay a price of 0 because it's a second price auction and you're going to win the display, so it doesn't really matter.",
            "If, on the other hand, the competition is fierce.",
            "Going to make a difference, and so I'm going to make a very simple assumption.",
            "I'm going to see my competitor bid from a uniform distribution between 0.1 and 0.3, so on average they do well and what's going to happen?",
            "I'm not going to do all the math, is that?",
            "Your loss, so I'm going to.",
            "I'm going to put loss here, so I want to minimize that.",
            "Your loss is going to do something like this.",
            "It's going to do that.",
            "Stats.",
            "That's with 0.1 here and 0.3 here.",
            "And your point here?",
            "So really what this says is if you bid lower than 0.1, you're not going to win anything, so you just doesn't matter whether Bijan .1 that's the same if you get more than 0.3.",
            "Doesn't matter in here, it makes a difference, but you can see that this curve is vastly vastly different from that curve.",
            "And again, it might seem like a minor point, but I really want to emphasize that.",
            "You should not define a loss function just by its optimum because there are many many loss functions that have the same optimum and you should really define the loss function by considering how much do I pay if I make the wrong decisions.",
            "And the long loss.",
            "It might have nice theoretical properties.",
            "It's usually completely decorrelated from what you actually pay, should make a wrong decisions.",
            "OK, so it's really important to think about this.",
            "So, and but that's difficult because.",
            "When I told you that I want to predict the CTR, you might know you may know nothing about online advertising.",
            "You may know nothing about how the auctions are run.",
            "You don't care, you train.",
            "Let's say logistic regression and you come up with a model.",
            "Here, that requires to know a lot more.",
            "You need to know how you competitors are bidding.",
            "What's the kind of the auction, and so you need to have more knowledge.",
            "But in the end, that's what really matters.",
            "So really just this small demo.",
            "The point is that.",
            "If you want to have a system that's actually going to be used in real life.",
            "You have to know about what's happening everywhere else and that needs to be included in your loss.",
            "And really, hopefully you see where this is going.",
            "This depends on the rewards.",
            "This depends on how much do I earn if I take the right decision and so at some point we need to talk about rewards and that's what I said on the 1st slides where they really like about reinforcement learning much more than the multistep episodes, which is nice, is that they actually talk about rewards.",
            "Then minimizing if the reward is an amount in dollars.",
            "It's how much money I'm making, why I'm losing if I take this decision, it's not a log loss, so it might be less comfortable, but it's actually much more relevant to what you're actually trying to do."
        ],
        [
            "Now.",
            "The second point, which is another very important point, is is the input distribution the same?",
            "So as I said.",
            "The data you have is on the auctions you want in the past.",
            "OK, so on the auctions ilost I don't know if there was a click or no click, so I can't learn anything on these things, but on the auctions I won I know if there's a click or no click.",
            "The problem is the algorithm I put in production influences which auctions are one.",
            "So I have a past data of auctions I've won and clicks or no clicks.",
            "I'm going to try my new system that's going to influence which input distribution I have.",
            "And now.",
            "How do I pick the system so I pick the one that works best on the current input distribution on the input distribution to come?",
            "That's again to me a complicated question which RL can answer very nicely.",
            "So how many of you are familiar with?"
        ],
        [
            "Simpsons products.",
            "OK.",
            "It's so that's.",
            "I think it's rarely taught that's the source of.",
            "I'd say 90% of the wrong decisions I've seen made by data scientists in industry.",
            "So here's what happens.",
            "Now you're not bidding, you're trying to decide.",
            "Let's say you are Google or Microsoft an you trying to decide if you want to display an ad on the top of the results or underside of the results.",
            "And So what you do is, well, you look at past data and what you see is that, oh, on the top, you reach a clickthrough rate.",
            "Does that actually work?",
            "It doesn't doesn't matter.",
            "So you can see that on the top you reach a clickthrough rate of 0.67%, an on the slide you can reach a click through rate of 0.71%.",
            "And so your natural conclusion will be, oh then, we should actually put more ads on the side because they click more.",
            "OK.",
            "It turns out that that's not the whole of the story, because if you split by users.",
            "Maybe for high value users, so there's a way to detect which users are more likely to click.",
            "Maybe for these users you really wanted to display an ad on top, but since your competitors knew there were high value, they displayed it on top and so the only choice you had was to display them on the side, and so you receive that for high value users.",
            "The top banner clicks more than the side Banner Zero 6% versus zero 2% and for low value users the top banner also works better than the side banner.",
            "1.2% versus zero point.",
            "8% so really.",
            "The conclusion is that.",
            "The top banner is better than the site better, but it turns out that because you weren't winning the top banner enough for the high value users you came to the wrong conclusion.",
            "An alternative maybe?",
            "Let's say it's in medicine.",
            "You want to do cancer treatment and you have either a very simple pill which doesn't work that well, but at least it's not invasive.",
            "Or you can do chemotherapy, which is much more complicated, but tends to be more effective.",
            "And then people could look at the death rates of these two treatments and say, oh, wait, many more people die when they have chemotherapy than when they have the pill.",
            "So chemotherapy doesn't work as well.",
            "Where is the reality?",
            "Is that no, they die more because you use chemotherapy in the hard cases, and that's obvious in that setting in that all the setting that's really not obvious.",
            "So you can very quickly draw wrong conclusions from your past data.",
            "If you don't care.",
            "If you don't do this, what's even worse is that.",
            "Now let's assume you want to build a model to predict the CTR when you're going to build a better model.",
            "So you in that setting you don't know, but high value and low value users.",
            "So you're going to better model.",
            "You're going to build a deep net because that's what you've been told works, so you build the deep Nets.",
            "You predict the CTR better.",
            "That's that's going to make you display even more on the side banner, which is even more of a bad decision.",
            "So that's.",
            "Outside of the realm of Standard supervised learning and standard machine learning that sometimes you really do not want to just improve your test error.",
            "So these are called confounding variables."
        ],
        [
            "The high value users, the low value users is the standard correlation does not imply causation.",
            "So how do you deal with confounding variables?",
            "And that's where RL comes in.",
            "One which is usually done, let's say by economists or in social science.",
            "It's to add as many variables as possible in model.",
            "So you could let controlling for confounding variables.",
            "So you basically want to make sure that you're not forgetting any of these causes.",
            "That's usually complicated.",
            "Another one which is what many companies do do.",
            "Sorry.",
            "Is there an online AB test?",
            "Since the since the two errors, the one you measured?",
            "On the test set and the actual error or Dick related, I'm just going to do all my tests online, so whenever someone has a new idea, I'm going to put it in production, run it for a few days, see what works best and do this.",
            "That's appealing, the problem is if you're a smaller company.",
            "It takes longer for you to get the data than a bigger company, and so your improvements is going to be slower and so eventually you're not going to have the same pace of progress as bigger companies and you'll die.",
            "So it's really important that you can keep the pace of progress, and you can evaluate as much as possible online.",
            "Now there's usually a remark that people do is that it's an exploration issue.",
            "OK here."
        ],
        [
            "You're the reason is Oh well.",
            "If I had displayed high value users on top, bottom or I would have realized that top banner clicks more and so I wouldn't have made that mistake.",
            "So if you just take some of your data to do some exploration, you will converge to the good solution.",
            "That's a typical bandit problem with explore exploit.",
            "I know how that works.",
            "I know how that converges.",
            "Is there something to yes?",
            "And So what I'm going to show now is something that took me quite awhile to realize, and I don't think I ever saw mentioned invented papers.",
            "Which is basically why in most cases bandit algorithms don't work.",
            "So here's the thing.",
            "I told you that the best thing you could do was to predict the CTR and now the question that the Rose is on which input distribution you computer CTR, because maybe you're going to have some input distribution.",
            "You compute the average city.",
            "Are you going to use this to bid?",
            "You're going to end up with another input distribution.",
            "You're going to get another CR.",
            "Where does that converge and what I'm going to show you now is that it converges to a solution which has nothing to do with the actual optimal solution.",
            "It converges to somewhere random no matter how much exploration you put, so it's really not a matter of the amount of exploration.",
            "And now we're going to try to understand why.",
            "In some cases it works, and why in some cases it doesn't.",
            "So I'm going to go again with high value users.",
            "And so let's say high value users represent 50% of my users.",
            "And they click all the time.",
            "They're really nice.",
            "They love clicking.",
            "And there are low value users.",
            "So obviously this is not a judgment on the people, is just for me as a company.",
            "50% of the users as well.",
            "But they only click 10% of the time.",
            "OK.",
            "But now there needs to be a question of which auctions do I win in, which auctions do I need?",
            "So do I lose?",
            "So I need to include competitors?",
            "OK, so I'm going to have my competitors, and since I'm a new company I cannot distinguish between high value users and low value users, but they can, so they going to bid differently for high value users in low value users.",
            "So for high value users my competitors are going to bid uniformly at random between 0.5 and one and for low values are going to be uniformly at random.",
            "Between 0 and 0.5 OK, so they'd be less for low value users than for high value users.",
            "But I have a poor system I cannot distinguish between the two.",
            "I'm going to be the same thing for everyone.",
            "Now.",
            "Here's the thing.",
            "If my bid is less than 0.5, I'm only going to see low value users, so I'm only going to have a see T of zero.",
            "I'm only observe ACTR 0.1.",
            "OK, so observe TTR.",
            "And that's going to be my bed here.",
            "If my bid is less than 0.5.",
            "I'm only going to observe ACTL 0.1.",
            "Now if my bid is more and I'm going to, I'm not going to go into all the details.",
            "If I bid 1 one is super high exploration.",
            "I win all the displays.",
            "I see everything.",
            "Well, if you need one, you see everything, and so you're going to see a mix of these two.",
            "And so you see are the average city order going to observe.",
            "0.55 is the average of 0.1 and one.",
            "So now what do you do when you say oh I'm going to start by do full exploration, so I'm going to be one.",
            "I'm going to win all the displays goods I've seen that my average TR 0.55 theory tells me that I should bid.",
            "My average city are that's what performs the best.",
            "So now given this, I'm going to be 0.55.",
            "I know the average CR you observe is slightly over 0.1, and so you're going to be slightly over 0.1.",
            "And you can observe here oh .1 and you're going to converge to building 0.1.",
            "OK, so if you run a bandit algorithm with full exploration.",
            "And every single time, so you start by buying everything and you bid.",
            "Then the average city.",
            "Are you observed your sister is going to decrease?",
            "In this particular example and converge at 0.1.",
            "Whereas if I actually draw.",
            "How much money you're making?",
            "So that's money, so I want this to go up.",
            "And that's my bed.",
            "What's going to happen is that between 0 and 0.5 you only see low value users and so indeed building 0.1 is the best strategy and you can actually see that you gain is this.",
            "Like this?",
            "With a maximum of 0.1.",
            "But then it should be slightly more than 0.5.",
            "You start winning high value users which bring you money of 1, whereas you only pay 0.5 and what's going to happen?",
            "Is the game is going to be like this?",
            "And this game is going to be much higher than that game.",
            "I really don't want to make this too tedious.",
            "I really wanted to go into the minimal amount of details just to show you that.",
            "Even if the theory tells you, Oh my best bet is to predict my average click through rates on which data sets do I estimate my average clickthrough rate?",
            "The standards belief from people who know that the bandit literature is to say the bigger data set, the better, and so if I buy everything and compute the average CTR over my entire population, this is going to give me the correct answer.",
            "And that's not the case.",
            "That's really, really not the case so.",
            "Huawei.",
            "Is that true?",
            "Is that if you look at the bandit literature, so the benefit response I know originated with the multi armed bandits where you have a finite number of arms and you try to estimate the average reward for each arm.",
            "That is fine, because that means your model is well specified.",
            "Each arm has a different reward and you try to estimate each reward, that's good.",
            "In settings, when you have many many arms, or in my case technically, I have infinitely many arms because I have to be to continuous values.",
            "There are two ways.",
            "Either you treat this as a multi armed bandit where you say let's say I'm going to divide us into hundreds and I'm going to estimate the expected reward over these hundred arms.",
            "That's usually not a suitable solution because you just have too many arms triggers, too many data, and so people invented what's called the contextual bandits, and so they say rather than estimate, and that's Q learning for you when you do.",
            "When you do Q learning if you if you think about Q.",
            "Learning with episodes of size 1, you're going to learn AQ function, which is going to tell you for each action what is my reward.",
            "OK, that is fine, except there's something again that no one talks about.",
            "Which says that if your Q function is, let's say, a deep net, when you try to update the Q function for an action, it updates the Q function for all the other actions as well.",
            "In some setting that you don't necessarily master, and what that means when you have this is that you Q function will try to be better for the actions you take off and then for the actions you don't take off.",
            "In multi arm bandit this never happens because in multi arm bandits you only update the reward of an arm when you draw that arm, so it's not influenced by what you do on the other arms.",
            "When you have contextual bandits or Q learning what you do with one action usually influences the answer you give on the other actions.",
            "When you do this, no matter how much exploration you do, you are no guarantee that you're going to converge to the right solution.",
            "Because the solution you converge to depends on the input distribution you had, and I can give you E greedy with 5% explorations, E greedy with 10% equity with 20%.",
            "These are all going to be different datasets.",
            "If your model is well specified, all these datasets will converge to the same solution.",
            "If your model is not well specified, all these datasets will converge to different solutions.",
            "And your model in real life is never well specified again except in the multi Armed Bandit, which is the simplest case when you have many many actions or continuous set of actions.",
            "Your model is not well specified and you need to be extremely careful and the way you can be careful is by controlling for the input distribution and again will see this.",
            "This is again reinforcement learning."
        ],
        [
            "So when your model is misspecified, it comes back to what I was saying was the cost function.",
            "You have tradeoffs to make.",
            "You can't get a perfect answer and everything, and so you need to make sure that which tradeoffs you're making."
        ],
        [
            "So the last possible thing you can do to try to evaluate the performance of a new system, as we said, exploration doesn't work running online.",
            "Maybe test is very costly and was more likely than not.",
            "Your company will go down under.",
            "You want to ask what's called counterfactual questions.",
            "So really, what you want to ask is what would have happened if we had done something else.",
            "So my current system makes decisions in production and whether it's a translation system, orbiting system, anything.",
            "It doesn't matter your current system makes decision, and now you come up with a new system and you really want one sort of question.",
            "What would my new system have done, and would it have been better or worse than my current system?",
            "And turns out that it looks like a complicated question, but."
        ],
        [
            "It's exactly what important sampling is an what reinforcement learning does.",
            "So your current system in production what it does, it observes States and it takes actions with a certain probability P. And now you come up with a new system which is really just a new distribution over actions given the states which you call queue and you asking how will this perform, and I know you've seen this at least once, but you've probably seen this many times over the last three days.",
            "It is just to know the performance of a policy Q when you have collected data in their policy P, you can just use important sampling and that gives you on average the correct answer.",
            "OK. And compared to what I've said and about exploration, really, what's extremely interesting but important sampling is that it controls for the distribution which which you gather data.",
            "OK, there's a P somewhere in my formula, so I have to know how you gather the data to make a correct to make a correct decision.",
            "So when you do this, so that's what we did at my again previous company rather than say oh I'm going to compute the training error tester and run an AB test and see how that goes.",
            "I'm going to use important sampling and so P is a distribution with my current system.",
            "Q is my distribution with the new system, an R is the reward I observed and since I want to maximize how much money I'm making minor, how much I'm paying, that's the reward."
        ],
        [
            "And it's really so off policy policy."
        ],
        [
            "Valuation, and that's the kind of results we got.",
            "So on the X axis you have the.",
            "Estimated reward was in you system on the Y axis you have the actual reward.",
            "When you run an online AB test and these are rectangles because you have confidence intervals.",
            "And now you can see that the correlation is much, much better.",
            "It's not perfect because you know important sampling has variant issues, and you might also end up with bias issues, but it's incredibly better.",
            "And so again, I really want to emphasize that point.",
            "What was time?",
            "Get.",
            "So I really want to emphasize that point.",
            "In an industry you might care about performance, but at some point you're going to care about evaluation and you need to provide tools to evaluate the new system quickly, because every time you don't do this, that means you have to go to production annually.",
            "The costs of going to production are much, much bigger than the cost of trying something with tensor flow, scikit learn, or any toolbox.",
            "So evaluation is underrated in companies.",
            "Proper evaluation is really underrated.",
            "And there is much more to evaluation than computing the test error.",
            "And again RL offers a proper solution to this issue."
        ],
        [
            "But now that we can.",
            "Evaluate Q well, we can also optimize over Q and again, that's something we've seen several times.",
            "At least in Peter's talk on Monday.",
            "Now you can ask well what is the best new system I can do and you really want to do optimization.",
            "But you have new constraints coming in.",
            "In our case, for instance, when your system is applied to a billion or two billion users, you really don't want to roll out a new policy like this.",
            "You want to have some constraints, and so you've seen many techniques like trust, region, policy optimization, or a bunch of other techniques.",
            "Usually with these techniques do is they have a policy.",
            "They make a small gradient update to that policy, they roll out the new policy, gather samples, and do this over and over again.",
            "There are many cases where you can't do this because just creating a new policy and put it in production can take a day or week, and So what you want to do you come up with another problem, which is really I'm doing this because I'm trying to entice you to work on these problems.",
            "These are undertreated in the literature, which is how do you do policy learning when you have a constraint on how many policies you can try.",
            "Let's say or you can think of this as I want to do optimization.",
            "Usually people care, but.",
            "Optimization at a matter of number of updates.",
            "I'm see optimization versus CPU time, which is which makes a lot of sense, but sometimes optimization like loss as a matter of number of updates is important because doing an update is complicated if you work in the nuclear industry, you can't make a parameter updates every second.",
            "It's going to take a few days, so I think it's really the same when you do direct policy optimization.",
            "There's a whole field, which is how do you optimize this?",
            "Making few policy updates."
        ],
        [
            "There is another very strong benefit of policy policy evaluation, besides being a better predictor of the performance.",
            "Which is, it predicts tangible quantities an at my previous company I was discussing with people in product and you can come to them and say, oh I've reduced the log loss by 10%.",
            "It means absolutely nothing to them.",
            "And if you say, oh, I can put the system in production, it reduces the test log lost by 10%, but it's going to require an extra $50,000 in machines.",
            "No one knows how to make that tradeoff.",
            "When you apply the reinforcement learning language of rewards, now you don't have a log loss.",
            "You have an actual amount of dollars that you can predict here on the X axis.",
            "I have dollars on the Y axis.",
            "I have dollars there, the same quantity.",
            "So now you can answer a lot of important questions, which is well, I can achieve this solution or I can achieve this other solution with twice as many machines.",
            "Is this worth it?",
            "Well now I can.",
            "I know because I know how machines, how many, how much machines cost.",
            "I know how much better my solution is in dollars.",
            "I can make the tradeoff.",
            "There's also other interesting things you can do, and we did, so I don't know how many of you are familiar with optimization, But there's this thing called constraint optimization, so sometimes you want to do optimization, but you want to add a constraint, and usually the constraint is, oh, I want my the norm of my vector to be less than one, which makes little sense, but that's what you usually do in optimization.",
            "Here again, we can apply constraints intangible quantities.",
            "Now you can say I want to do policy optimization.",
            "I want to maximize the number of dollars I make, but I don't want to increase the number of ads I display by more than 10%.",
            "That's something you can evaluate that something and optimize over, and it becomes a very simple language to communicate with other people.",
            "So again, this is underappreciated in the Academy community, because we usually don't necessarily build products or talk to all the people who don't have the same incentives.",
            "But it's really important when you talk about when your company and you talk about log loss.",
            "Very people know what that means.",
            "If you talk about dollars, it's much more.",
            "It's much more tangible, an reinforcement learning the reward, part of reinforcement learning gives you a language to do this, and I think that's absolutely crucial."
        ],
        [
            "Another point which has not been treated much in the academic literature.",
            "I don't have necessarily a solution to this.",
            "Which is that?",
            "There are systems we run optimization quickly, so my policy I want to update my policy, but maybe the landscape is changing.",
            "Some competitors come, some competitors go and so my optimal policy is going to change.",
            "So within my policy optimization, have an optimization procedure and most of the algorithms you saw like TRP, oh, they have step sizes or constraint on the KL.",
            "Anne, how to tune that step size is not necessarily completely obvious.",
            "If you only want to learn a policy once, that's absolutely fine.",
            "You can fine tune this by hand if you have to do this across the entire company many times per day, you can't do this by hand, so we need to come up with robust policy optimization procedure which do not need any hyperparameter or what the hyperparameters are the same across all situations.",
            "And robustness is sometimes talked about in papers, but I think not as much as it should be.",
            "And from experience in optimization usually what makes that an optimization algorithm is used or not.",
            "It's not the raw convergence speed is how easy it is to adapt the parameters to your particular problem, and I think that's something we should focus on."
        ],
        [
            "Anne.",
            "I'm not, it's not going to go into detail, but I think again other unanswered questions that we are really critical in industry and not necessarily very much treated in the academic literature.",
            "Inference time is critical, so in many cases you want to make decisions fast in some other will.",
            "Basically you pay a price for making slower decisions, and so we need to have to come up with algorithms which can make a tradeoff between precise inference or fast inference.",
            "Right now we usually use the same system to do inference in all kinds of cases.",
            "That's suboptimal in many situations.",
            "The other one is rewards comment or multiple form.",
            "As I said in our case we wanted clicks, but maybe we want sales or maybe there are many other things we observe.",
            "How can we include all these kinds of rewards into our system?",
            "That's still an open question.",
            "So."
        ],
        [
            "The quick summary is an industry robustness and efficiency are critical, and when they call about efficiency, it's not necessarily converging quickly.",
            "It's also pipeline efficiency.",
            "You want to be able to try new ideas quickly.",
            "You want to be able to discard bad ideas quickly, and to do this, you need to have a proper evaluation system.",
            "RL in my own experience offered me the best tools to have a good evaluation.",
            "They offered me tools that they hadn't seen in other machine learning setting and so I think that's a really important use of reinforcement learning.",
            "Again, I'm not sure I can call this reinforcement learning since these are episodes of size 1, but I think it still offers the correct vocabulary to deal with these things, especially the idea of rewards of actual rewards, which is usually not mentioned in supervised learning.",
            "Improving the model.",
            "Many people care about improving models and you have many papers on how you make better models.",
            "That's usually I called useless.",
            "That's big of an overstatement that's usually much less important than knowing what you actually want to optimize.",
            "So the last function, I think much more care should be put into the choice of loss function than what it's actually done.",
            "Usually you're going to have either log loss or the mean squared error, because that's what everyone has been doing.",
            "I think we should think a lot more about this, and again, it's not because the loss function has the correct optimum that it's a good loss function.",
            "The loss function gives you plenty of trade offs and you need to make sure that these reflect the actual tradeoffs you're making.",
            "And I said to me last really important aspect of reinforcement learning is that it deals with tangible quantities, physical quantities, you know how to manipulate an.",
            "It makes a lot more sense.",
            "I don't know how to constrain the normal of my parameters.",
            "I don't know what's going to give the best results.",
            "I know how to constraint an amount of dollars or in an SEC that's much more.",
            "That's a lot easier."
        ],
        [
            "So thank you.",
            "And if you have any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you hear me?",
                    "label": 0
                },
                {
                    "sent": "Yes, excellent so thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "As she said, I was quite intimidated when I saw Lisa speakers who all know.",
                    "label": 0
                },
                {
                    "sent": "When they talk about when they talk about reinforcement learning, I came to reinforcement.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about two years ago and but the idea is I might present some aspects of our L that are less common in the academic community which which I think are just as interesting.",
                    "label": 0
                },
                {
                    "sent": "So just a disclaimer, that's my own experience.",
                    "label": 1
                },
                {
                    "sent": "That's how I did things when I was working on actual product.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it will be focused on online advertising because that's why I use reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "What I present is not at all limited.",
                    "label": 0
                },
                {
                    "sent": "To that industry, and it's really a more general statement on how you can use reinforcement learning in the industry.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So usually when you think about reinforcement learning, we think about two things.",
                    "label": 0
                },
                {
                    "sent": "The first one it's multistep episodes, so there's this idea in reinforcement learning that you're going to take an action that's going to influence the state, and then you're going to take another action, and so you're going to take a sequence of actions and you don't want to be too greedy because you don't just want to maximize reward in the next step, you want to maximize reward at the end of the episode, and so I'm assuming that's what most of the talks for events, whether it's Carrie, go.",
                    "label": 0
                },
                {
                    "sent": "Helicopters?",
                    "label": 0
                },
                {
                    "sent": "There's another aspect to reinforcement learning is the aspect of reward evaluation and maximization, so there's really an important bit in RL, which we usually define which reward we're interested in, and we try to maximize that reward, and it turns out that in this talk it's really this aspect I'm going to focus on.",
                    "label": 1
                },
                {
                    "sent": "So just to be clear, it's going to be an RL talk with episodes of length one.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's going.",
                    "label": 0
                },
                {
                    "sent": "There's not going to be any sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "I want to optimize an really what I want to show you is that PRL has much more to offer to the Community and to the industry then just a multistep episodes.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be a simple case, but the point of view again that few people have.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I mentioned online advertising it's going to be a sub part of online advertising called retargeting.",
                    "label": 0
                },
                {
                    "sent": "And again that I mean it's advertising.",
                    "label": 0
                },
                {
                    "sent": "It's probably boring to most of you.",
                    "label": 0
                },
                {
                    "sent": "It's boring to me, but it offers some interesting challenges.",
                    "label": 0
                },
                {
                    "sent": "So what's retargeting whenever the user browse the web?",
                    "label": 0
                },
                {
                    "sent": "The user lands on a web page and then potentially there will be ads displayed on that webpage, and so how are these ads chosen while the website Contacts what we call an ad exchange, which is basically an Organism that runs an auction, the ad exchange Contacts all the people willing to display ads, and then there's an auction going on.",
                    "label": 0
                },
                {
                    "sent": "Its competitor tells the ad exchange how much they're willing to bid for the right to display one specific.",
                    "label": 0
                },
                {
                    "sent": "Add and then the highest bidder wins the right to display in AD.",
                    "label": 1
                },
                {
                    "sent": "So at my previous company or this work was done, we were participating in about 20 billion auctions today and we're just playing from 2 to 3 billion ads per day.",
                    "label": 0
                },
                {
                    "sent": "And again, there are plenty of interesting works on multi multiple auctions, but right now I'm assuming it's a single log auction, so really the state is what you know about the user webpage.",
                    "label": 0
                },
                {
                    "sent": "Annual action is going to be how much you're willing to pay to mature, willing to bid to display in ads on that web page.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just details of the auction, so that's what's called real time bidding, and most of the time, and I'm not going to talk about the more complicated cases.",
                    "label": 1
                },
                {
                    "sent": "It's what we call the second price auction.",
                    "label": 0
                },
                {
                    "sent": "That means that everyone's bidding at the same time the highest bidder wins the right to display an ad, but the highest bidder does not pay what they bid.",
                    "label": 1
                },
                {
                    "sent": "They only pay with the second highest bidder bid, and that has many, many interesting properties, but the one we concern about today is that the optimal strategy for everyone.",
                    "label": 0
                },
                {
                    "sent": "Stupid, the expected game.",
                    "label": 0
                },
                {
                    "sent": "Which means that when a user lands on a web page, I have to predict how much on average I'm going to earn if I display an ad, and if that's $0.10, that's what should be my optimal bit.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a very simple strategy, and that's why we use this auction scheme.",
                    "label": 0
                },
                {
                    "sent": "And So what is said on average?",
                    "label": 0
                },
                {
                    "sent": "I'm going to earn $0.10.",
                    "label": 0
                },
                {
                    "sent": "What is the expected gain?",
                    "label": 1
                },
                {
                    "sent": "Its how much I'm being paid every time there's a click on the ad, so I'm going to display in ad, let's say for Walmart.",
                    "label": 0
                },
                {
                    "sent": "Walmart is going to pay me whenever there's a click.",
                    "label": 0
                },
                {
                    "sent": "So that's contract, and there's another aspect, which is what is the probability that the user will click on the ADD.",
                    "label": 0
                },
                {
                    "sent": "Of course is the property is low, they don't average.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be paid little, so we want to be less if the property is high.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be paid more on average, so I want a bit more and this property is click is something we don't know.",
                    "label": 0
                },
                {
                    "sent": "Is that something we want to estimate?",
                    "label": 0
                },
                {
                    "sent": "That's very old, and when people realize that that was the optimal strategy, there was a whole lot of literature on how do you estimate the probability of click.",
                    "label": 0
                },
                {
                    "sent": "And right now maybe some of you are wondering where is DRL coming?",
                    "label": 0
                },
                {
                    "sent": "And right now there's absolutely no reinforcement learning and this whole field wasn't using any reinforcement learning at all.",
                    "label": 0
                },
                {
                    "sent": "And I really I want to show you that they were wrong in this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, you won't find a bidding strategy you want to estimate the probability that there's going to be a click.",
                    "label": 1
                },
                {
                    "sent": "So what you have is in the past you've been in previous auctions.",
                    "label": 0
                },
                {
                    "sent": "Some of these auctions you lossed, so you don't know what happened, but some of these options you want, and so you got to this plan ad and you got to notice if there was a click or not, and so you have access to a labeled datasets.",
                    "label": 1
                },
                {
                    "sent": "OK, with some idea X information about the user, the web page and why is a binary label click or no click.",
                    "label": 1
                },
                {
                    "sent": "And whenever I showed is the first reaction to people say, oh that's easy, then I have access to data.",
                    "label": 0
                },
                {
                    "sent": "I know how many of you were in the deep learning Summer school, but it's really a supervised learning approach, which is, oh, I have access to label data.",
                    "label": 0
                },
                {
                    "sent": "I want to predict the probability of y = 1 given X. I'm going to build a classifier to do that.",
                    "label": 1
                },
                {
                    "sent": "And you're going to have a lot of research and people very excited and they want to design new models.",
                    "label": 0
                },
                {
                    "sent": "And they're going to have deep models to better this particular property of click.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you've done this offline, so you've carefully constructed your model, you have a good model, so you have you followed your machine learning classes.",
                    "label": 0
                },
                {
                    "sent": "You have your trainer in test, and you compute it on test, and you saw that you model those better on the test that the current model.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to go into production that has a cost, so you involving engineers, UAB test.",
                    "label": 0
                },
                {
                    "sent": "Your solution.",
                    "label": 0
                },
                {
                    "sent": "That means that what you're going to do is we're going to split the users into 2.",
                    "label": 0
                },
                {
                    "sent": "Some of them will have the current production system, some of them will have your modification.",
                    "label": 0
                },
                {
                    "sent": "We wait for a few days and in the end we check which system brought the most money.",
                    "label": 0
                },
                {
                    "sent": "All is good and dandy until.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you realize that these are the results you get.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis.",
                    "label": 0
                },
                {
                    "sent": "You have the offline test performance.",
                    "label": 0
                },
                {
                    "sent": "On the Y axis you have how much money did your system actually bring?",
                    "label": 0
                },
                {
                    "sent": "And you can see that the correlation is actually really really low.",
                    "label": 0
                },
                {
                    "sent": "So there is no guarantee that the better system you have will indeed bring more money, and that's extremely important because everything we do in machine learning we the only thing we hear is that we should care about the test error and that the better test error is is actually something we want to achieve.",
                    "label": 0
                },
                {
                    "sent": "It turns out that in these situations you can have a better tester that doesn't do anything for the actual problem you're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "And now I'm trying to explain how reinforcement learning can help in these situations, and that's really not a random occurrence that happens all the time.",
                    "label": 0
                },
                {
                    "sent": "So usually what happens when you start a company is at first, you model is so bad at everything, you do this better, and so you have a good correlation between tester and revenue.",
                    "label": 0
                },
                {
                    "sent": "You super happy and you keep going.",
                    "label": 0
                },
                {
                    "sent": "And I'm depressed company.",
                    "label": 0
                },
                {
                    "sent": "This lasted for a few years and at some point the correlation starts to decrease and people start to get a bit uncomfortable with this.",
                    "label": 0
                },
                {
                    "sent": "And the solution most of the companies do is let's try more things, but that has a cost and so I'm going to give you a principled way of doing things again, which I think is really important.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why do we have this discrepancy?",
                    "label": 0
                },
                {
                    "sent": "There are two implicit assumptions we made when we assume that the test error, so I said RMS here, but the log loss would be about the same.",
                    "label": 0
                },
                {
                    "sent": "There are tombs assumptions we made when you said the test error as relevant for what we want to do.",
                    "label": 0
                },
                {
                    "sent": "The first one is, we said, oh I'm using the log loss so the test log loss that's going to be a good proxy for the revenue.",
                    "label": 1
                },
                {
                    "sent": "If I make a better log loss, I'm going to be a better proxy for that means I'm going to make more revenue.",
                    "label": 0
                },
                {
                    "sent": "The other assumptions, and that again, that might be something more familiar with, because that's really the topic of this summer school, is that we assume that the input distribution was the same.",
                    "label": 0
                },
                {
                    "sent": "OK, that's really again the assumption we make is that the P of X on the train set and P of X or the test set are the same, and so that's why it makes sense to focus on the test error and what we'll see is that in many real life scenario, that's really not the case.",
                    "label": 1
                },
                {
                    "sent": "And what you do influences the input distribution, and So what you do in these cases?",
                    "label": 0
                },
                {
                    "sent": "And it might seem obvious to you in this particular setting when you're in a company.",
                    "label": 0
                },
                {
                    "sent": "People usually discard this problem.",
                    "label": 0
                },
                {
                    "sent": "They say machine learning only taught me how to deal with the same input for distribution for training tests, and so I'm going to make assumptions even though it's wrong.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to give you a quick idea, I'm going to do something I've never done before and I was told by many people that was a bad thing to try, but I'm going to try.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do a quick demo on the Board of why this is a bad idea, so hopefully you can all see the board.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there's some light it needs to be turned on, but.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that there is a user that on average clicks 20% of the time.",
                    "label": 1
                },
                {
                    "sent": "So CPR is called click through rate.",
                    "label": 0
                },
                {
                    "sent": "Is the probability that there is going to click.",
                    "label": 0
                },
                {
                    "sent": "So on average a user clicks 20% of the time.",
                    "label": 0
                },
                {
                    "sent": "Now your model is going to predict a click probability OK, which I'm going to call, let's say P. And what does the log loss say?",
                    "label": 0
                },
                {
                    "sent": "Well, the long, let's say, tells you that the loss you incur for P is going to be equal to minus 0 two times.",
                    "label": 0
                },
                {
                    "sent": "Log P -- 0.8 times log.",
                    "label": 0
                },
                {
                    "sent": "1 -- P. Hopefully I'm not teaching you anything new right now, so if the probability you have here you have zero here you have one here.",
                    "label": 0
                },
                {
                    "sent": "You'll loss look something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, with a minimum of 0.2.",
                    "label": 0
                },
                {
                    "sent": "And there's really a problem that it's not to present in the oil community, but its presence in pretty much everywhere else in machine learning and no one mentions it.",
                    "label": 1
                },
                {
                    "sent": "As you can see, many papers with people define the loss function.",
                    "label": 0
                },
                {
                    "sent": "And the way they justify this loss function is they say, the optimum of this loss function is what I want.",
                    "label": 0
                },
                {
                    "sent": "They only care about the optimum.",
                    "label": 0
                },
                {
                    "sent": "They never care about what's outside, so here they say, oh, I actually want to predict.",
                    "label": 0
                },
                {
                    "sent": "Join two.",
                    "label": 1
                },
                {
                    "sent": "I have a loss for the minimum is at 0.2.",
                    "label": 0
                },
                {
                    "sent": "That's fine, I'm happy.",
                    "label": 0
                },
                {
                    "sent": "Now let's see what the actual losses.",
                    "label": 0
                },
                {
                    "sent": "So what I want to maximize is my revenue, so it's.",
                    "label": 0
                },
                {
                    "sent": "How much or how much money I'm making so my gain?",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "How much I'm paying?",
                    "label": 0
                },
                {
                    "sent": "OK, the Cdr is 0.2.",
                    "label": 0
                },
                {
                    "sent": "So if I'm winning that display, I'm going to earn 0.2 minus what I paid in junction.",
                    "label": 0
                },
                {
                    "sent": "If I'm not winning the display, zero will happen.",
                    "label": 0
                },
                {
                    "sent": "And now first, there's something you realize here that's really important is that this depends on what the other people in the auction do.",
                    "label": 0
                },
                {
                    "sent": "OK, if you're by yourself in the auction, no matter what you predict, you're going to pay a price of 0 because it's a second price auction and you're going to win the display, so it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "If, on the other hand, the competition is fierce.",
                    "label": 0
                },
                {
                    "sent": "Going to make a difference, and so I'm going to make a very simple assumption.",
                    "label": 0
                },
                {
                    "sent": "I'm going to see my competitor bid from a uniform distribution between 0.1 and 0.3, so on average they do well and what's going to happen?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to do all the math, is that?",
                    "label": 0
                },
                {
                    "sent": "Your loss, so I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to put loss here, so I want to minimize that.",
                    "label": 0
                },
                {
                    "sent": "Your loss is going to do something like this.",
                    "label": 0
                },
                {
                    "sent": "It's going to do that.",
                    "label": 0
                },
                {
                    "sent": "Stats.",
                    "label": 0
                },
                {
                    "sent": "That's with 0.1 here and 0.3 here.",
                    "label": 0
                },
                {
                    "sent": "And your point here?",
                    "label": 0
                },
                {
                    "sent": "So really what this says is if you bid lower than 0.1, you're not going to win anything, so you just doesn't matter whether Bijan .1 that's the same if you get more than 0.3.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter in here, it makes a difference, but you can see that this curve is vastly vastly different from that curve.",
                    "label": 0
                },
                {
                    "sent": "And again, it might seem like a minor point, but I really want to emphasize that.",
                    "label": 0
                },
                {
                    "sent": "You should not define a loss function just by its optimum because there are many many loss functions that have the same optimum and you should really define the loss function by considering how much do I pay if I make the wrong decisions.",
                    "label": 0
                },
                {
                    "sent": "And the long loss.",
                    "label": 0
                },
                {
                    "sent": "It might have nice theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "It's usually completely decorrelated from what you actually pay, should make a wrong decisions.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's really important to think about this.",
                    "label": 0
                },
                {
                    "sent": "So, and but that's difficult because.",
                    "label": 0
                },
                {
                    "sent": "When I told you that I want to predict the CTR, you might know you may know nothing about online advertising.",
                    "label": 0
                },
                {
                    "sent": "You may know nothing about how the auctions are run.",
                    "label": 0
                },
                {
                    "sent": "You don't care, you train.",
                    "label": 0
                },
                {
                    "sent": "Let's say logistic regression and you come up with a model.",
                    "label": 0
                },
                {
                    "sent": "Here, that requires to know a lot more.",
                    "label": 0
                },
                {
                    "sent": "You need to know how you competitors are bidding.",
                    "label": 0
                },
                {
                    "sent": "What's the kind of the auction, and so you need to have more knowledge.",
                    "label": 0
                },
                {
                    "sent": "But in the end, that's what really matters.",
                    "label": 0
                },
                {
                    "sent": "So really just this small demo.",
                    "label": 0
                },
                {
                    "sent": "The point is that.",
                    "label": 0
                },
                {
                    "sent": "If you want to have a system that's actually going to be used in real life.",
                    "label": 0
                },
                {
                    "sent": "You have to know about what's happening everywhere else and that needs to be included in your loss.",
                    "label": 0
                },
                {
                    "sent": "And really, hopefully you see where this is going.",
                    "label": 0
                },
                {
                    "sent": "This depends on the rewards.",
                    "label": 0
                },
                {
                    "sent": "This depends on how much do I earn if I take the right decision and so at some point we need to talk about rewards and that's what I said on the 1st slides where they really like about reinforcement learning much more than the multistep episodes, which is nice, is that they actually talk about rewards.",
                    "label": 0
                },
                {
                    "sent": "Then minimizing if the reward is an amount in dollars.",
                    "label": 0
                },
                {
                    "sent": "It's how much money I'm making, why I'm losing if I take this decision, it's not a log loss, so it might be less comfortable, but it's actually much more relevant to what you're actually trying to do.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The second point, which is another very important point, is is the input distribution the same?",
                    "label": 1
                },
                {
                    "sent": "So as I said.",
                    "label": 1
                },
                {
                    "sent": "The data you have is on the auctions you want in the past.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the auctions ilost I don't know if there was a click or no click, so I can't learn anything on these things, but on the auctions I won I know if there's a click or no click.",
                    "label": 0
                },
                {
                    "sent": "The problem is the algorithm I put in production influences which auctions are one.",
                    "label": 0
                },
                {
                    "sent": "So I have a past data of auctions I've won and clicks or no clicks.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try my new system that's going to influence which input distribution I have.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "How do I pick the system so I pick the one that works best on the current input distribution on the input distribution to come?",
                    "label": 0
                },
                {
                    "sent": "That's again to me a complicated question which RL can answer very nicely.",
                    "label": 0
                },
                {
                    "sent": "So how many of you are familiar with?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simpsons products.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's so that's.",
                    "label": 0
                },
                {
                    "sent": "I think it's rarely taught that's the source of.",
                    "label": 0
                },
                {
                    "sent": "I'd say 90% of the wrong decisions I've seen made by data scientists in industry.",
                    "label": 0
                },
                {
                    "sent": "So here's what happens.",
                    "label": 0
                },
                {
                    "sent": "Now you're not bidding, you're trying to decide.",
                    "label": 0
                },
                {
                    "sent": "Let's say you are Google or Microsoft an you trying to decide if you want to display an ad on the top of the results or underside of the results.",
                    "label": 0
                },
                {
                    "sent": "And So what you do is, well, you look at past data and what you see is that, oh, on the top, you reach a clickthrough rate.",
                    "label": 0
                },
                {
                    "sent": "Does that actually work?",
                    "label": 0
                },
                {
                    "sent": "It doesn't doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So you can see that on the top you reach a clickthrough rate of 0.67%, an on the slide you can reach a click through rate of 0.71%.",
                    "label": 0
                },
                {
                    "sent": "And so your natural conclusion will be, oh then, we should actually put more ads on the side because they click more.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that's not the whole of the story, because if you split by users.",
                    "label": 0
                },
                {
                    "sent": "Maybe for high value users, so there's a way to detect which users are more likely to click.",
                    "label": 0
                },
                {
                    "sent": "Maybe for these users you really wanted to display an ad on top, but since your competitors knew there were high value, they displayed it on top and so the only choice you had was to display them on the side, and so you receive that for high value users.",
                    "label": 0
                },
                {
                    "sent": "The top banner clicks more than the side Banner Zero 6% versus zero 2% and for low value users the top banner also works better than the side banner.",
                    "label": 1
                },
                {
                    "sent": "1.2% versus zero point.",
                    "label": 0
                },
                {
                    "sent": "8% so really.",
                    "label": 0
                },
                {
                    "sent": "The conclusion is that.",
                    "label": 0
                },
                {
                    "sent": "The top banner is better than the site better, but it turns out that because you weren't winning the top banner enough for the high value users you came to the wrong conclusion.",
                    "label": 0
                },
                {
                    "sent": "An alternative maybe?",
                    "label": 0
                },
                {
                    "sent": "Let's say it's in medicine.",
                    "label": 0
                },
                {
                    "sent": "You want to do cancer treatment and you have either a very simple pill which doesn't work that well, but at least it's not invasive.",
                    "label": 0
                },
                {
                    "sent": "Or you can do chemotherapy, which is much more complicated, but tends to be more effective.",
                    "label": 0
                },
                {
                    "sent": "And then people could look at the death rates of these two treatments and say, oh, wait, many more people die when they have chemotherapy than when they have the pill.",
                    "label": 0
                },
                {
                    "sent": "So chemotherapy doesn't work as well.",
                    "label": 0
                },
                {
                    "sent": "Where is the reality?",
                    "label": 0
                },
                {
                    "sent": "Is that no, they die more because you use chemotherapy in the hard cases, and that's obvious in that setting in that all the setting that's really not obvious.",
                    "label": 0
                },
                {
                    "sent": "So you can very quickly draw wrong conclusions from your past data.",
                    "label": 0
                },
                {
                    "sent": "If you don't care.",
                    "label": 0
                },
                {
                    "sent": "If you don't do this, what's even worse is that.",
                    "label": 0
                },
                {
                    "sent": "Now let's assume you want to build a model to predict the CTR when you're going to build a better model.",
                    "label": 0
                },
                {
                    "sent": "So you in that setting you don't know, but high value and low value users.",
                    "label": 0
                },
                {
                    "sent": "So you're going to better model.",
                    "label": 0
                },
                {
                    "sent": "You're going to build a deep net because that's what you've been told works, so you build the deep Nets.",
                    "label": 0
                },
                {
                    "sent": "You predict the CTR better.",
                    "label": 0
                },
                {
                    "sent": "That's that's going to make you display even more on the side banner, which is even more of a bad decision.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Outside of the realm of Standard supervised learning and standard machine learning that sometimes you really do not want to just improve your test error.",
                    "label": 0
                },
                {
                    "sent": "So these are called confounding variables.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The high value users, the low value users is the standard correlation does not imply causation.",
                    "label": 0
                },
                {
                    "sent": "So how do you deal with confounding variables?",
                    "label": 1
                },
                {
                    "sent": "And that's where RL comes in.",
                    "label": 0
                },
                {
                    "sent": "One which is usually done, let's say by economists or in social science.",
                    "label": 0
                },
                {
                    "sent": "It's to add as many variables as possible in model.",
                    "label": 1
                },
                {
                    "sent": "So you could let controlling for confounding variables.",
                    "label": 0
                },
                {
                    "sent": "So you basically want to make sure that you're not forgetting any of these causes.",
                    "label": 0
                },
                {
                    "sent": "That's usually complicated.",
                    "label": 0
                },
                {
                    "sent": "Another one which is what many companies do do.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Is there an online AB test?",
                    "label": 0
                },
                {
                    "sent": "Since the since the two errors, the one you measured?",
                    "label": 0
                },
                {
                    "sent": "On the test set and the actual error or Dick related, I'm just going to do all my tests online, so whenever someone has a new idea, I'm going to put it in production, run it for a few days, see what works best and do this.",
                    "label": 0
                },
                {
                    "sent": "That's appealing, the problem is if you're a smaller company.",
                    "label": 0
                },
                {
                    "sent": "It takes longer for you to get the data than a bigger company, and so your improvements is going to be slower and so eventually you're not going to have the same pace of progress as bigger companies and you'll die.",
                    "label": 0
                },
                {
                    "sent": "So it's really important that you can keep the pace of progress, and you can evaluate as much as possible online.",
                    "label": 0
                },
                {
                    "sent": "Now there's usually a remark that people do is that it's an exploration issue.",
                    "label": 0
                },
                {
                    "sent": "OK here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're the reason is Oh well.",
                    "label": 0
                },
                {
                    "sent": "If I had displayed high value users on top, bottom or I would have realized that top banner clicks more and so I wouldn't have made that mistake.",
                    "label": 0
                },
                {
                    "sent": "So if you just take some of your data to do some exploration, you will converge to the good solution.",
                    "label": 0
                },
                {
                    "sent": "That's a typical bandit problem with explore exploit.",
                    "label": 0
                },
                {
                    "sent": "I know how that works.",
                    "label": 0
                },
                {
                    "sent": "I know how that converges.",
                    "label": 0
                },
                {
                    "sent": "Is there something to yes?",
                    "label": 0
                },
                {
                    "sent": "And So what I'm going to show now is something that took me quite awhile to realize, and I don't think I ever saw mentioned invented papers.",
                    "label": 0
                },
                {
                    "sent": "Which is basically why in most cases bandit algorithms don't work.",
                    "label": 0
                },
                {
                    "sent": "So here's the thing.",
                    "label": 0
                },
                {
                    "sent": "I told you that the best thing you could do was to predict the CTR and now the question that the Rose is on which input distribution you computer CTR, because maybe you're going to have some input distribution.",
                    "label": 0
                },
                {
                    "sent": "You compute the average city.",
                    "label": 0
                },
                {
                    "sent": "Are you going to use this to bid?",
                    "label": 0
                },
                {
                    "sent": "You're going to end up with another input distribution.",
                    "label": 0
                },
                {
                    "sent": "You're going to get another CR.",
                    "label": 0
                },
                {
                    "sent": "Where does that converge and what I'm going to show you now is that it converges to a solution which has nothing to do with the actual optimal solution.",
                    "label": 0
                },
                {
                    "sent": "It converges to somewhere random no matter how much exploration you put, so it's really not a matter of the amount of exploration.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to try to understand why.",
                    "label": 0
                },
                {
                    "sent": "In some cases it works, and why in some cases it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to go again with high value users.",
                    "label": 0
                },
                {
                    "sent": "And so let's say high value users represent 50% of my users.",
                    "label": 0
                },
                {
                    "sent": "And they click all the time.",
                    "label": 0
                },
                {
                    "sent": "They're really nice.",
                    "label": 0
                },
                {
                    "sent": "They love clicking.",
                    "label": 0
                },
                {
                    "sent": "And there are low value users.",
                    "label": 0
                },
                {
                    "sent": "So obviously this is not a judgment on the people, is just for me as a company.",
                    "label": 0
                },
                {
                    "sent": "50% of the users as well.",
                    "label": 0
                },
                {
                    "sent": "But they only click 10% of the time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But now there needs to be a question of which auctions do I win in, which auctions do I need?",
                    "label": 0
                },
                {
                    "sent": "So do I lose?",
                    "label": 0
                },
                {
                    "sent": "So I need to include competitors?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to have my competitors, and since I'm a new company I cannot distinguish between high value users and low value users, but they can, so they going to bid differently for high value users in low value users.",
                    "label": 0
                },
                {
                    "sent": "So for high value users my competitors are going to bid uniformly at random between 0.5 and one and for low values are going to be uniformly at random.",
                    "label": 0
                },
                {
                    "sent": "Between 0 and 0.5 OK, so they'd be less for low value users than for high value users.",
                    "label": 0
                },
                {
                    "sent": "But I have a poor system I cannot distinguish between the two.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be the same thing for everyone.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Here's the thing.",
                    "label": 0
                },
                {
                    "sent": "If my bid is less than 0.5, I'm only going to see low value users, so I'm only going to have a see T of zero.",
                    "label": 0
                },
                {
                    "sent": "I'm only observe ACTR 0.1.",
                    "label": 0
                },
                {
                    "sent": "OK, so observe TTR.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be my bed here.",
                    "label": 0
                },
                {
                    "sent": "If my bid is less than 0.5.",
                    "label": 0
                },
                {
                    "sent": "I'm only going to observe ACTL 0.1.",
                    "label": 0
                },
                {
                    "sent": "Now if my bid is more and I'm going to, I'm not going to go into all the details.",
                    "label": 0
                },
                {
                    "sent": "If I bid 1 one is super high exploration.",
                    "label": 0
                },
                {
                    "sent": "I win all the displays.",
                    "label": 0
                },
                {
                    "sent": "I see everything.",
                    "label": 0
                },
                {
                    "sent": "Well, if you need one, you see everything, and so you're going to see a mix of these two.",
                    "label": 0
                },
                {
                    "sent": "And so you see are the average city order going to observe.",
                    "label": 0
                },
                {
                    "sent": "0.55 is the average of 0.1 and one.",
                    "label": 0
                },
                {
                    "sent": "So now what do you do when you say oh I'm going to start by do full exploration, so I'm going to be one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to win all the displays goods I've seen that my average TR 0.55 theory tells me that I should bid.",
                    "label": 0
                },
                {
                    "sent": "My average city are that's what performs the best.",
                    "label": 0
                },
                {
                    "sent": "So now given this, I'm going to be 0.55.",
                    "label": 0
                },
                {
                    "sent": "I know the average CR you observe is slightly over 0.1, and so you're going to be slightly over 0.1.",
                    "label": 0
                },
                {
                    "sent": "And you can observe here oh .1 and you're going to converge to building 0.1.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you run a bandit algorithm with full exploration.",
                    "label": 0
                },
                {
                    "sent": "And every single time, so you start by buying everything and you bid.",
                    "label": 0
                },
                {
                    "sent": "Then the average city.",
                    "label": 0
                },
                {
                    "sent": "Are you observed your sister is going to decrease?",
                    "label": 0
                },
                {
                    "sent": "In this particular example and converge at 0.1.",
                    "label": 0
                },
                {
                    "sent": "Whereas if I actually draw.",
                    "label": 0
                },
                {
                    "sent": "How much money you're making?",
                    "label": 0
                },
                {
                    "sent": "So that's money, so I want this to go up.",
                    "label": 0
                },
                {
                    "sent": "And that's my bed.",
                    "label": 0
                },
                {
                    "sent": "What's going to happen is that between 0 and 0.5 you only see low value users and so indeed building 0.1 is the best strategy and you can actually see that you gain is this.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "With a maximum of 0.1.",
                    "label": 0
                },
                {
                    "sent": "But then it should be slightly more than 0.5.",
                    "label": 0
                },
                {
                    "sent": "You start winning high value users which bring you money of 1, whereas you only pay 0.5 and what's going to happen?",
                    "label": 0
                },
                {
                    "sent": "Is the game is going to be like this?",
                    "label": 0
                },
                {
                    "sent": "And this game is going to be much higher than that game.",
                    "label": 0
                },
                {
                    "sent": "I really don't want to make this too tedious.",
                    "label": 0
                },
                {
                    "sent": "I really wanted to go into the minimal amount of details just to show you that.",
                    "label": 0
                },
                {
                    "sent": "Even if the theory tells you, Oh my best bet is to predict my average click through rates on which data sets do I estimate my average clickthrough rate?",
                    "label": 0
                },
                {
                    "sent": "The standards belief from people who know that the bandit literature is to say the bigger data set, the better, and so if I buy everything and compute the average CTR over my entire population, this is going to give me the correct answer.",
                    "label": 0
                },
                {
                    "sent": "And that's not the case.",
                    "label": 0
                },
                {
                    "sent": "That's really, really not the case so.",
                    "label": 0
                },
                {
                    "sent": "Huawei.",
                    "label": 0
                },
                {
                    "sent": "Is that true?",
                    "label": 0
                },
                {
                    "sent": "Is that if you look at the bandit literature, so the benefit response I know originated with the multi armed bandits where you have a finite number of arms and you try to estimate the average reward for each arm.",
                    "label": 0
                },
                {
                    "sent": "That is fine, because that means your model is well specified.",
                    "label": 0
                },
                {
                    "sent": "Each arm has a different reward and you try to estimate each reward, that's good.",
                    "label": 0
                },
                {
                    "sent": "In settings, when you have many many arms, or in my case technically, I have infinitely many arms because I have to be to continuous values.",
                    "label": 0
                },
                {
                    "sent": "There are two ways.",
                    "label": 0
                },
                {
                    "sent": "Either you treat this as a multi armed bandit where you say let's say I'm going to divide us into hundreds and I'm going to estimate the expected reward over these hundred arms.",
                    "label": 0
                },
                {
                    "sent": "That's usually not a suitable solution because you just have too many arms triggers, too many data, and so people invented what's called the contextual bandits, and so they say rather than estimate, and that's Q learning for you when you do.",
                    "label": 0
                },
                {
                    "sent": "When you do Q learning if you if you think about Q.",
                    "label": 0
                },
                {
                    "sent": "Learning with episodes of size 1, you're going to learn AQ function, which is going to tell you for each action what is my reward.",
                    "label": 0
                },
                {
                    "sent": "OK, that is fine, except there's something again that no one talks about.",
                    "label": 0
                },
                {
                    "sent": "Which says that if your Q function is, let's say, a deep net, when you try to update the Q function for an action, it updates the Q function for all the other actions as well.",
                    "label": 0
                },
                {
                    "sent": "In some setting that you don't necessarily master, and what that means when you have this is that you Q function will try to be better for the actions you take off and then for the actions you don't take off.",
                    "label": 0
                },
                {
                    "sent": "In multi arm bandit this never happens because in multi arm bandits you only update the reward of an arm when you draw that arm, so it's not influenced by what you do on the other arms.",
                    "label": 0
                },
                {
                    "sent": "When you have contextual bandits or Q learning what you do with one action usually influences the answer you give on the other actions.",
                    "label": 0
                },
                {
                    "sent": "When you do this, no matter how much exploration you do, you are no guarantee that you're going to converge to the right solution.",
                    "label": 0
                },
                {
                    "sent": "Because the solution you converge to depends on the input distribution you had, and I can give you E greedy with 5% explorations, E greedy with 10% equity with 20%.",
                    "label": 0
                },
                {
                    "sent": "These are all going to be different datasets.",
                    "label": 0
                },
                {
                    "sent": "If your model is well specified, all these datasets will converge to the same solution.",
                    "label": 1
                },
                {
                    "sent": "If your model is not well specified, all these datasets will converge to different solutions.",
                    "label": 0
                },
                {
                    "sent": "And your model in real life is never well specified again except in the multi Armed Bandit, which is the simplest case when you have many many actions or continuous set of actions.",
                    "label": 0
                },
                {
                    "sent": "Your model is not well specified and you need to be extremely careful and the way you can be careful is by controlling for the input distribution and again will see this.",
                    "label": 0
                },
                {
                    "sent": "This is again reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when your model is misspecified, it comes back to what I was saying was the cost function.",
                    "label": 0
                },
                {
                    "sent": "You have tradeoffs to make.",
                    "label": 0
                },
                {
                    "sent": "You can't get a perfect answer and everything, and so you need to make sure that which tradeoffs you're making.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the last possible thing you can do to try to evaluate the performance of a new system, as we said, exploration doesn't work running online.",
                    "label": 0
                },
                {
                    "sent": "Maybe test is very costly and was more likely than not.",
                    "label": 0
                },
                {
                    "sent": "Your company will go down under.",
                    "label": 0
                },
                {
                    "sent": "You want to ask what's called counterfactual questions.",
                    "label": 0
                },
                {
                    "sent": "So really, what you want to ask is what would have happened if we had done something else.",
                    "label": 1
                },
                {
                    "sent": "So my current system makes decisions in production and whether it's a translation system, orbiting system, anything.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter your current system makes decision, and now you come up with a new system and you really want one sort of question.",
                    "label": 0
                },
                {
                    "sent": "What would my new system have done, and would it have been better or worse than my current system?",
                    "label": 0
                },
                {
                    "sent": "And turns out that it looks like a complicated question, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's exactly what important sampling is an what reinforcement learning does.",
                    "label": 0
                },
                {
                    "sent": "So your current system in production what it does, it observes States and it takes actions with a certain probability P. And now you come up with a new system which is really just a new distribution over actions given the states which you call queue and you asking how will this perform, and I know you've seen this at least once, but you've probably seen this many times over the last three days.",
                    "label": 1
                },
                {
                    "sent": "It is just to know the performance of a policy Q when you have collected data in their policy P, you can just use important sampling and that gives you on average the correct answer.",
                    "label": 0
                },
                {
                    "sent": "OK. And compared to what I've said and about exploration, really, what's extremely interesting but important sampling is that it controls for the distribution which which you gather data.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a P somewhere in my formula, so I have to know how you gather the data to make a correct to make a correct decision.",
                    "label": 0
                },
                {
                    "sent": "So when you do this, so that's what we did at my again previous company rather than say oh I'm going to compute the training error tester and run an AB test and see how that goes.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use important sampling and so P is a distribution with my current system.",
                    "label": 0
                },
                {
                    "sent": "Q is my distribution with the new system, an R is the reward I observed and since I want to maximize how much money I'm making minor, how much I'm paying, that's the reward.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's really so off policy policy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Valuation, and that's the kind of results we got.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis you have the.",
                    "label": 0
                },
                {
                    "sent": "Estimated reward was in you system on the Y axis you have the actual reward.",
                    "label": 0
                },
                {
                    "sent": "When you run an online AB test and these are rectangles because you have confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "And now you can see that the correlation is much, much better.",
                    "label": 0
                },
                {
                    "sent": "It's not perfect because you know important sampling has variant issues, and you might also end up with bias issues, but it's incredibly better.",
                    "label": 0
                },
                {
                    "sent": "And so again, I really want to emphasize that point.",
                    "label": 0
                },
                {
                    "sent": "What was time?",
                    "label": 0
                },
                {
                    "sent": "Get.",
                    "label": 0
                },
                {
                    "sent": "So I really want to emphasize that point.",
                    "label": 0
                },
                {
                    "sent": "In an industry you might care about performance, but at some point you're going to care about evaluation and you need to provide tools to evaluate the new system quickly, because every time you don't do this, that means you have to go to production annually.",
                    "label": 0
                },
                {
                    "sent": "The costs of going to production are much, much bigger than the cost of trying something with tensor flow, scikit learn, or any toolbox.",
                    "label": 0
                },
                {
                    "sent": "So evaluation is underrated in companies.",
                    "label": 0
                },
                {
                    "sent": "Proper evaluation is really underrated.",
                    "label": 0
                },
                {
                    "sent": "And there is much more to evaluation than computing the test error.",
                    "label": 0
                },
                {
                    "sent": "And again RL offers a proper solution to this issue.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But now that we can.",
                    "label": 0
                },
                {
                    "sent": "Evaluate Q well, we can also optimize over Q and again, that's something we've seen several times.",
                    "label": 1
                },
                {
                    "sent": "At least in Peter's talk on Monday.",
                    "label": 0
                },
                {
                    "sent": "Now you can ask well what is the best new system I can do and you really want to do optimization.",
                    "label": 0
                },
                {
                    "sent": "But you have new constraints coming in.",
                    "label": 0
                },
                {
                    "sent": "In our case, for instance, when your system is applied to a billion or two billion users, you really don't want to roll out a new policy like this.",
                    "label": 0
                },
                {
                    "sent": "You want to have some constraints, and so you've seen many techniques like trust, region, policy optimization, or a bunch of other techniques.",
                    "label": 0
                },
                {
                    "sent": "Usually with these techniques do is they have a policy.",
                    "label": 0
                },
                {
                    "sent": "They make a small gradient update to that policy, they roll out the new policy, gather samples, and do this over and over again.",
                    "label": 0
                },
                {
                    "sent": "There are many cases where you can't do this because just creating a new policy and put it in production can take a day or week, and So what you want to do you come up with another problem, which is really I'm doing this because I'm trying to entice you to work on these problems.",
                    "label": 0
                },
                {
                    "sent": "These are undertreated in the literature, which is how do you do policy learning when you have a constraint on how many policies you can try.",
                    "label": 0
                },
                {
                    "sent": "Let's say or you can think of this as I want to do optimization.",
                    "label": 0
                },
                {
                    "sent": "Usually people care, but.",
                    "label": 0
                },
                {
                    "sent": "Optimization at a matter of number of updates.",
                    "label": 0
                },
                {
                    "sent": "I'm see optimization versus CPU time, which is which makes a lot of sense, but sometimes optimization like loss as a matter of number of updates is important because doing an update is complicated if you work in the nuclear industry, you can't make a parameter updates every second.",
                    "label": 0
                },
                {
                    "sent": "It's going to take a few days, so I think it's really the same when you do direct policy optimization.",
                    "label": 0
                },
                {
                    "sent": "There's a whole field, which is how do you optimize this?",
                    "label": 0
                },
                {
                    "sent": "Making few policy updates.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is another very strong benefit of policy policy evaluation, besides being a better predictor of the performance.",
                    "label": 1
                },
                {
                    "sent": "Which is, it predicts tangible quantities an at my previous company I was discussing with people in product and you can come to them and say, oh I've reduced the log loss by 10%.",
                    "label": 0
                },
                {
                    "sent": "It means absolutely nothing to them.",
                    "label": 0
                },
                {
                    "sent": "And if you say, oh, I can put the system in production, it reduces the test log lost by 10%, but it's going to require an extra $50,000 in machines.",
                    "label": 0
                },
                {
                    "sent": "No one knows how to make that tradeoff.",
                    "label": 0
                },
                {
                    "sent": "When you apply the reinforcement learning language of rewards, now you don't have a log loss.",
                    "label": 0
                },
                {
                    "sent": "You have an actual amount of dollars that you can predict here on the X axis.",
                    "label": 0
                },
                {
                    "sent": "I have dollars on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "I have dollars there, the same quantity.",
                    "label": 0
                },
                {
                    "sent": "So now you can answer a lot of important questions, which is well, I can achieve this solution or I can achieve this other solution with twice as many machines.",
                    "label": 0
                },
                {
                    "sent": "Is this worth it?",
                    "label": 0
                },
                {
                    "sent": "Well now I can.",
                    "label": 0
                },
                {
                    "sent": "I know because I know how machines, how many, how much machines cost.",
                    "label": 0
                },
                {
                    "sent": "I know how much better my solution is in dollars.",
                    "label": 0
                },
                {
                    "sent": "I can make the tradeoff.",
                    "label": 0
                },
                {
                    "sent": "There's also other interesting things you can do, and we did, so I don't know how many of you are familiar with optimization, But there's this thing called constraint optimization, so sometimes you want to do optimization, but you want to add a constraint, and usually the constraint is, oh, I want my the norm of my vector to be less than one, which makes little sense, but that's what you usually do in optimization.",
                    "label": 0
                },
                {
                    "sent": "Here again, we can apply constraints intangible quantities.",
                    "label": 0
                },
                {
                    "sent": "Now you can say I want to do policy optimization.",
                    "label": 0
                },
                {
                    "sent": "I want to maximize the number of dollars I make, but I don't want to increase the number of ads I display by more than 10%.",
                    "label": 0
                },
                {
                    "sent": "That's something you can evaluate that something and optimize over, and it becomes a very simple language to communicate with other people.",
                    "label": 0
                },
                {
                    "sent": "So again, this is underappreciated in the Academy community, because we usually don't necessarily build products or talk to all the people who don't have the same incentives.",
                    "label": 0
                },
                {
                    "sent": "But it's really important when you talk about when your company and you talk about log loss.",
                    "label": 0
                },
                {
                    "sent": "Very people know what that means.",
                    "label": 0
                },
                {
                    "sent": "If you talk about dollars, it's much more.",
                    "label": 0
                },
                {
                    "sent": "It's much more tangible, an reinforcement learning the reward, part of reinforcement learning gives you a language to do this, and I think that's absolutely crucial.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another point which has not been treated much in the academic literature.",
                    "label": 0
                },
                {
                    "sent": "I don't have necessarily a solution to this.",
                    "label": 0
                },
                {
                    "sent": "Which is that?",
                    "label": 0
                },
                {
                    "sent": "There are systems we run optimization quickly, so my policy I want to update my policy, but maybe the landscape is changing.",
                    "label": 0
                },
                {
                    "sent": "Some competitors come, some competitors go and so my optimal policy is going to change.",
                    "label": 0
                },
                {
                    "sent": "So within my policy optimization, have an optimization procedure and most of the algorithms you saw like TRP, oh, they have step sizes or constraint on the KL.",
                    "label": 0
                },
                {
                    "sent": "Anne, how to tune that step size is not necessarily completely obvious.",
                    "label": 0
                },
                {
                    "sent": "If you only want to learn a policy once, that's absolutely fine.",
                    "label": 0
                },
                {
                    "sent": "You can fine tune this by hand if you have to do this across the entire company many times per day, you can't do this by hand, so we need to come up with robust policy optimization procedure which do not need any hyperparameter or what the hyperparameters are the same across all situations.",
                    "label": 0
                },
                {
                    "sent": "And robustness is sometimes talked about in papers, but I think not as much as it should be.",
                    "label": 0
                },
                {
                    "sent": "And from experience in optimization usually what makes that an optimization algorithm is used or not.",
                    "label": 0
                },
                {
                    "sent": "It's not the raw convergence speed is how easy it is to adapt the parameters to your particular problem, and I think that's something we should focus on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I'm not, it's not going to go into detail, but I think again other unanswered questions that we are really critical in industry and not necessarily very much treated in the academic literature.",
                    "label": 0
                },
                {
                    "sent": "Inference time is critical, so in many cases you want to make decisions fast in some other will.",
                    "label": 1
                },
                {
                    "sent": "Basically you pay a price for making slower decisions, and so we need to have to come up with algorithms which can make a tradeoff between precise inference or fast inference.",
                    "label": 0
                },
                {
                    "sent": "Right now we usually use the same system to do inference in all kinds of cases.",
                    "label": 0
                },
                {
                    "sent": "That's suboptimal in many situations.",
                    "label": 0
                },
                {
                    "sent": "The other one is rewards comment or multiple form.",
                    "label": 0
                },
                {
                    "sent": "As I said in our case we wanted clicks, but maybe we want sales or maybe there are many other things we observe.",
                    "label": 0
                },
                {
                    "sent": "How can we include all these kinds of rewards into our system?",
                    "label": 0
                },
                {
                    "sent": "That's still an open question.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The quick summary is an industry robustness and efficiency are critical, and when they call about efficiency, it's not necessarily converging quickly.",
                    "label": 1
                },
                {
                    "sent": "It's also pipeline efficiency.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to try new ideas quickly.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to discard bad ideas quickly, and to do this, you need to have a proper evaluation system.",
                    "label": 0
                },
                {
                    "sent": "RL in my own experience offered me the best tools to have a good evaluation.",
                    "label": 0
                },
                {
                    "sent": "They offered me tools that they hadn't seen in other machine learning setting and so I think that's a really important use of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm not sure I can call this reinforcement learning since these are episodes of size 1, but I think it still offers the correct vocabulary to deal with these things, especially the idea of rewards of actual rewards, which is usually not mentioned in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Improving the model.",
                    "label": 0
                },
                {
                    "sent": "Many people care about improving models and you have many papers on how you make better models.",
                    "label": 0
                },
                {
                    "sent": "That's usually I called useless.",
                    "label": 0
                },
                {
                    "sent": "That's big of an overstatement that's usually much less important than knowing what you actually want to optimize.",
                    "label": 0
                },
                {
                    "sent": "So the last function, I think much more care should be put into the choice of loss function than what it's actually done.",
                    "label": 0
                },
                {
                    "sent": "Usually you're going to have either log loss or the mean squared error, because that's what everyone has been doing.",
                    "label": 0
                },
                {
                    "sent": "I think we should think a lot more about this, and again, it's not because the loss function has the correct optimum that it's a good loss function.",
                    "label": 0
                },
                {
                    "sent": "The loss function gives you plenty of trade offs and you need to make sure that these reflect the actual tradeoffs you're making.",
                    "label": 1
                },
                {
                    "sent": "And I said to me last really important aspect of reinforcement learning is that it deals with tangible quantities, physical quantities, you know how to manipulate an.",
                    "label": 0
                },
                {
                    "sent": "It makes a lot more sense.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to constrain the normal of my parameters.",
                    "label": 0
                },
                {
                    "sent": "I don't know what's going to give the best results.",
                    "label": 0
                },
                {
                    "sent": "I know how to constraint an amount of dollars or in an SEC that's much more.",
                    "label": 0
                },
                {
                    "sent": "That's a lot easier.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions.",
                    "label": 0
                }
            ]
        }
    }
}