{
    "id": "dp2ktfrveodj2g75g3tbieelqeqqcsjg",
    "title": "Localized Complexities for Transductive Learning",
    "info": {
        "author": [
            "Ilya O. Tolstikhin, Russian Academy of Sciences"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_tolstikhin_learning/",
    "segmentation": [
        [
            "My name is realistic and and this is a joint work with Wilburn Shar and Mario's clothes, who is in this room and."
        ],
        [
            "First half of the talk I will be talking about some new concentration inequality's for sampling without replacement, and then I will show you one particular application of this inequality's to the transductive learning setting and let me see."
        ],
        [
            "Earth, with very briefly reminding you what concentration inequality is are, so you have a function depending on large number of random variables.",
            "You want to control it.",
            "Fluctuations around its expected value OK, and it can be done by obtaining high probability upper bounds on these deviations.",
            "And if these random variables are independent?",
            "Things are very well studied and understood by now and a lot of useful results are available and we will need some classical results which all of you of course know."
        ],
        [
            "So if we have some of bounded independent random variables.",
            "The deviations are upper bounded by 1 / sqrt M and this is described by headings inequality.",
            "Moreover, you can tighten the result by accounting for the variance and this is described by Bernstein's inequality and this variance appears in front of 1 / sqrt N term and if the variance is small you get the title result OK."
        ],
        [
            "Now consider you have infinitely many sums and you want to control the uniform deviations.",
            "Then you will have to deal with Soprema orphan prequel processes, which is maximal sum of function values which are also bounded and centered.",
            "OK, and it happens that Holdings and Bernstein's inequality is also hold for this setting their analog style are called Mcdermott's inequality, which is 1 / sqrt N. And telegrams inequality.",
            "And here is the version due to Olivier Busquet.",
            "OK, and this we term it accounts for the maximal variance.",
            "And perhaps one important thing to know to notice here is that if your function class consists of only one function, you're ending up with ordinary, some, and then this inequality gives you have drinks, inequality, and this one gives you Bernstein's inequality.",
            "OK, but then."
        ],
        [
            "Many practical situations, independence or some assumption is breaks OK and you have to deal with dependent random variables.",
            "And in our work we consider one specific situation of dependent random variables, namely when they are sampled uniformly and without replacement from some finite population.",
            "OK, Please note that in this case they are indeed dependent and you need to replace the results I presented previously with some other results.",
            "In this setting is interesting because it naturally appears in various applications, including cross validation procedures where tests and training faults are sampled uniformly from the finite number of examples.",
            "Also transductive learning setting, which I will be talking about later, and randomized sequential algorithms also may use a sampling without replacement in their implementations.",
            "No, so much completion and factorization procedures.",
            "Which are, you know, broadly used in collaborative filtering and other areas so it."
        ],
        [
            "It is quite an interesting setting.",
            "So let me briefly.",
            "Sketch the known results in this field.",
            "First of all, headings and Bernstein's Inequality's also holds hold for this setting OK, but later it was shown that actually you can get get tighter result, and this one is due to serfling.",
            "If you will drop this factor here, you will have just have links inequality and as the sample size N grows this factor decreases and you get an improvement.",
            "OK, actually the same thing can be done to Bernstein's inequality, and it was shown where recently, so the main message here is that if random variables are sampled without replacements, you get the better concentration.",
            "Now."
        ],
        [
            "Magda, if you consider Supreme of empirical processes, Macdiarmid type inequality or is also true.",
            "OK, this one.",
            "So again, if you drop this factor here, you will end up with 1 / sqrt N like Mike McDermott.",
            "But this factor decreases as the sample size grows.",
            "And the problem is that there is no version of telegrams concentration inequality for Supreme of empirical processes and sampling without replacement.",
            "And as we will show later, sometimes it is this refined treatment of variance is very important and it can lead you to the better results."
        ],
        [
            "So this is the main.",
            "We provide 2 new concentration inequality's for Supreme of empirical processes, both of which account for the variance.",
            "This is the first one.",
            "OK, this is sub Gaussian 1 / sqrt N and here is the maximal variance."
        ],
        [
            "Here is the second one if you will look on its upper bound, you will notice that it is exactly the same as in bousquet's version of telegrams inequality.",
            "OK, however, both of these inequalities have their own shortcomings.",
            "So in first one you see this factor here, which can be quite large if you have small sample size and in some regimes this upper bound can even diverge.",
            "And the second inequality, here you're controlling the deviation of your random variable, not from its expected value, but from some other expected value, which is expected value of.",
            "You know, I disapprove of empirical processes and it can be shown that this expected value is always larger than your expected value, so there is a gap and you're obtaining shifted upper bounds which which may, which means that you lose something here.",
            "OK, but we will show later that.",
            "In applications we have in mind, this is not a big problem."
        ],
        [
            "So let me briefly compare 2 new results with the previous Mcdearman style bound.",
            "So first important thing to notice is that two new Inequality's account for the variance while old one did not.",
            "We can, you know.",
            "See we can investigate different regimes of sample size and generation size.",
            "So if sample size is really small compared to the whole sample size, it kind of brings brings you to the IID setting.",
            "And in this case old inequality and second new inequality's can outperform the second one because of this term here.",
            "And if the sample sizes are compareable, have roughly the same order.",
            "Then this new one can outperform old one already.",
            "For variance is smaller than 116th and please know that we are dealing with random variables which leave on 01 interval, which means that their variance cannot be more than 1/4, so it's not not a very restrictive assumption in this in this case.",
            "And as I already told you, there is a gap between two expectations, but it can be actually upper bounded by this term.",
            "So there are several regimes where this gap is.",
            "Not a problem.",
            "And summer."
        ],
        [
            "Using this search, this second new inequality stays informative in order regimes of sample size is OK. At least it's upper bound, while this second one it can be better, at least in the regime where sample sizes are compatible.",
            "Now."
        ],
        [
            "Let me turn to the application to transductive learning setting."
        ],
        [
            "So for simplicity.",
            "I will consider deterministic agnostic setting so.",
            "Let's see what it is.",
            "So you have a finite instance space.",
            "And some output space.",
            "You have fixed your favorite hypothesis class of predictors.",
            "And you have a deterministic non random labeling function.",
            "Which does not necessarily belong to your hypothesis class, which is, which means that it's an agnostic situation.",
            "OK, so you start with sampling an inputs uniformly without replacements from this.",
            "Instance space, then you obtain their labels by applying this labeling function.",
            "And in the end, learning procedure gets access to the labeled training sample, and you unlabeled test points."
        ],
        [
            "And the goal of learning procedure is to find the predictor in hypothesis class.",
            "Having a minimal error on the test set.",
            "Where loss functional is bounded in 01 interval.",
            "OK, we are considering any bounded loss function.",
            "So I also we also define errors on the whole sample and on the training sample, which is empirical error.",
            "And also we define functions, minimizing errors on training sample, test sample and whole sample respectively.",
            "This quantity, which is often called excess loss.",
            "This measures how well does empirical risk minimizer approximate the theoretical optimal one on the test set.",
            "We want to use empirical risk minimization to approximate the optimal solution.",
            "So our goal would be to obtain tight high probability upper bounds on this quantity, which is random becausw.",
            "Both of these functions are random.",
            "OK."
        ],
        [
            "So here is a.",
            "At least of results in this direction of previous results.",
            "So there were some implicit bounds which provided computational procedures, which leads to the values of the bounds.",
            "Then there were bounds for quadratic loss or further one was quite open with empirical risk standing in front of it.",
            "Then there were couple of population inequality's.",
            "Which crucially depends on the prior which you need to set prior to observing the data.",
            "OK, then there were couple of bounds based on algorithmic stability.",
            "Again of the order 1 / sqrt N also global or transductive learning market complexities, which again give you 1 / sqrt N. And finally, it is well known that if we put some strict assumptions on our problem, namely if we will consider assume that this labeling function belongs to our function class, then you can obtain fast rates of 1 / M OK, But the message here is that all these bounds without strict assumptions on the problem at hand, they provide only slow rates 1 / sqrt N."
        ],
        [
            "Let me very briefly remind you of well known results in inductive setting, which assumes that your training sample is IID from some unknown distribution.",
            "So the classic approach.",
            "Suggested to work with uniform deviations over the whole class of functions, and they usually lead to the rates of 1 / sqrt N which are termed slow rates.",
            "And."
        ],
        [
            "Later, it was understood that it is over pessimistic and instead we should.",
            "Take Soprema supremum over some subclass.",
            "Which is likely to contain empirical risk minimizer.",
            "And which consists of functions with small variances.",
            "And if you use this as this technique together with telegrams inequality, it will provide you a way to obtain fast rates faster than one over square root of them.",
            "So these are well known results.",
            "OK, and now we have a version of Telegram's inequality for sampling without replacement, which can be applied to the transductive setting and here is."
        ],
        [
            "The result we get we have so consider this empirical risk measured tones IID sample.",
            "That will be needed for technical reasons.",
            "And consider the local neighborhood of the function, which is optimal on the general set in your hypothesis class, which is measured by this L2 distance between losses of these functions OK. And assume that this nontrivial relation holds between the variance of loss and the expected value of loss.",
            "OK, then what this theorem states is that the difference between.",
            "Errors on the general set is upper bounded.",
            "By the uniform deviations over this local region, and to be more precise by the fixed point of modulus of continuity of this empirical process measured around the optimal function.",
            "OK, and this kind of results were well known in estimation problems.",
            "Then inductive setting.",
            "And now we recover these results in transductive setting as well.",
            "OK, but we wanted to control the performance on the test set right?",
            "And here is the control for the general set and you can obtain the control for the test set which looks."
        ],
        [
            "Were you similar?",
            "And I remind you that this term has order fixed point plus 1 / N. OK, so important thing to notice is that this fixed point.",
            "In some situations it can be over fast rate.",
            "For example it happens for VC classes.",
            "In that case you have 1 / N or for kernel classes for balls and Eric AHS and Lipschitz losses.",
            "In this case the order rates depend on the tail sum of Asian values of the integral operator corresponding to the kernel you chose.",
            "And considering this this assumption, it happens that.",
            "It is not too restrictive and there are situations when it holds.",
            "For example, if you consider quadratic loss with uniformly bounded convex classes.",
            "You will obtain B = 1 here and binary Loss and VC classes if you also assume that the labeling function is contained in your hypothesis class, which is somehow related with the low noise assumptions.",
            "Then you will also obtain this kind of.",
            "As."
        ],
        [
            "Relation OK, so I think that's it.",
            "I have one more minute or so.",
            "OK, so here are a couple of future directions.",
            "First of all, we still have a gap in our inequality and it will be very interesting to close the gap 2nd.",
            "Is it possible to obtain the version of telegrams inequality which improves upon telegrams inequality in the same way as serfling is bound to prove improves Holdings inequality becausw our inequality's do not OK. We can also consider going to local transductive ready market complexities.",
            "And of course, very interesting would be to apply our inequality's two other problems and arguably the most interesting of them would be to apply the normal synthetic analysis of cross validation procedures.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is realistic and and this is a joint work with Wilburn Shar and Mario's clothes, who is in this room and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First half of the talk I will be talking about some new concentration inequality's for sampling without replacement, and then I will show you one particular application of this inequality's to the transductive learning setting and let me see.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Earth, with very briefly reminding you what concentration inequality is are, so you have a function depending on large number of random variables.",
                    "label": 0
                },
                {
                    "sent": "You want to control it.",
                    "label": 1
                },
                {
                    "sent": "Fluctuations around its expected value OK, and it can be done by obtaining high probability upper bounds on these deviations.",
                    "label": 1
                },
                {
                    "sent": "And if these random variables are independent?",
                    "label": 0
                },
                {
                    "sent": "Things are very well studied and understood by now and a lot of useful results are available and we will need some classical results which all of you of course know.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we have some of bounded independent random variables.",
                    "label": 1
                },
                {
                    "sent": "The deviations are upper bounded by 1 / sqrt M and this is described by headings inequality.",
                    "label": 0
                },
                {
                    "sent": "Moreover, you can tighten the result by accounting for the variance and this is described by Bernstein's inequality and this variance appears in front of 1 / sqrt N term and if the variance is small you get the title result OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now consider you have infinitely many sums and you want to control the uniform deviations.",
                    "label": 1
                },
                {
                    "sent": "Then you will have to deal with Soprema orphan prequel processes, which is maximal sum of function values which are also bounded and centered.",
                    "label": 0
                },
                {
                    "sent": "OK, and it happens that Holdings and Bernstein's inequality is also hold for this setting their analog style are called Mcdermott's inequality, which is 1 / sqrt N. And telegrams inequality.",
                    "label": 0
                },
                {
                    "sent": "And here is the version due to Olivier Busquet.",
                    "label": 1
                },
                {
                    "sent": "OK, and this we term it accounts for the maximal variance.",
                    "label": 0
                },
                {
                    "sent": "And perhaps one important thing to know to notice here is that if your function class consists of only one function, you're ending up with ordinary, some, and then this inequality gives you have drinks, inequality, and this one gives you Bernstein's inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, but then.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many practical situations, independence or some assumption is breaks OK and you have to deal with dependent random variables.",
                    "label": 0
                },
                {
                    "sent": "And in our work we consider one specific situation of dependent random variables, namely when they are sampled uniformly and without replacement from some finite population.",
                    "label": 1
                },
                {
                    "sent": "OK, Please note that in this case they are indeed dependent and you need to replace the results I presented previously with some other results.",
                    "label": 0
                },
                {
                    "sent": "In this setting is interesting because it naturally appears in various applications, including cross validation procedures where tests and training faults are sampled uniformly from the finite number of examples.",
                    "label": 0
                },
                {
                    "sent": "Also transductive learning setting, which I will be talking about later, and randomized sequential algorithms also may use a sampling without replacement in their implementations.",
                    "label": 1
                },
                {
                    "sent": "No, so much completion and factorization procedures.",
                    "label": 0
                },
                {
                    "sent": "Which are, you know, broadly used in collaborative filtering and other areas so it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is quite an interesting setting.",
                    "label": 0
                },
                {
                    "sent": "So let me briefly.",
                    "label": 0
                },
                {
                    "sent": "Sketch the known results in this field.",
                    "label": 0
                },
                {
                    "sent": "First of all, headings and Bernstein's Inequality's also holds hold for this setting OK, but later it was shown that actually you can get get tighter result, and this one is due to serfling.",
                    "label": 1
                },
                {
                    "sent": "If you will drop this factor here, you will have just have links inequality and as the sample size N grows this factor decreases and you get an improvement.",
                    "label": 0
                },
                {
                    "sent": "OK, actually the same thing can be done to Bernstein's inequality, and it was shown where recently, so the main message here is that if random variables are sampled without replacements, you get the better concentration.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Magda, if you consider Supreme of empirical processes, Macdiarmid type inequality or is also true.",
                    "label": 0
                },
                {
                    "sent": "OK, this one.",
                    "label": 0
                },
                {
                    "sent": "So again, if you drop this factor here, you will end up with 1 / sqrt N like Mike McDermott.",
                    "label": 0
                },
                {
                    "sent": "But this factor decreases as the sample size grows.",
                    "label": 0
                },
                {
                    "sent": "And the problem is that there is no version of telegrams concentration inequality for Supreme of empirical processes and sampling without replacement.",
                    "label": 1
                },
                {
                    "sent": "And as we will show later, sometimes it is this refined treatment of variance is very important and it can lead you to the better results.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the main.",
                    "label": 0
                },
                {
                    "sent": "We provide 2 new concentration inequality's for Supreme of empirical processes, both of which account for the variance.",
                    "label": 0
                },
                {
                    "sent": "This is the first one.",
                    "label": 0
                },
                {
                    "sent": "OK, this is sub Gaussian 1 / sqrt N and here is the maximal variance.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the second one if you will look on its upper bound, you will notice that it is exactly the same as in bousquet's version of telegrams inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, however, both of these inequalities have their own shortcomings.",
                    "label": 0
                },
                {
                    "sent": "So in first one you see this factor here, which can be quite large if you have small sample size and in some regimes this upper bound can even diverge.",
                    "label": 0
                },
                {
                    "sent": "And the second inequality, here you're controlling the deviation of your random variable, not from its expected value, but from some other expected value, which is expected value of.",
                    "label": 0
                },
                {
                    "sent": "You know, I disapprove of empirical processes and it can be shown that this expected value is always larger than your expected value, so there is a gap and you're obtaining shifted upper bounds which which may, which means that you lose something here.",
                    "label": 0
                },
                {
                    "sent": "OK, but we will show later that.",
                    "label": 0
                },
                {
                    "sent": "In applications we have in mind, this is not a big problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me briefly compare 2 new results with the previous Mcdearman style bound.",
                    "label": 0
                },
                {
                    "sent": "So first important thing to notice is that two new Inequality's account for the variance while old one did not.",
                    "label": 1
                },
                {
                    "sent": "We can, you know.",
                    "label": 1
                },
                {
                    "sent": "See we can investigate different regimes of sample size and generation size.",
                    "label": 0
                },
                {
                    "sent": "So if sample size is really small compared to the whole sample size, it kind of brings brings you to the IID setting.",
                    "label": 0
                },
                {
                    "sent": "And in this case old inequality and second new inequality's can outperform the second one because of this term here.",
                    "label": 0
                },
                {
                    "sent": "And if the sample sizes are compareable, have roughly the same order.",
                    "label": 1
                },
                {
                    "sent": "Then this new one can outperform old one already.",
                    "label": 0
                },
                {
                    "sent": "For variance is smaller than 116th and please know that we are dealing with random variables which leave on 01 interval, which means that their variance cannot be more than 1/4, so it's not not a very restrictive assumption in this in this case.",
                    "label": 0
                },
                {
                    "sent": "And as I already told you, there is a gap between two expectations, but it can be actually upper bounded by this term.",
                    "label": 0
                },
                {
                    "sent": "So there are several regimes where this gap is.",
                    "label": 0
                },
                {
                    "sent": "Not a problem.",
                    "label": 0
                },
                {
                    "sent": "And summer.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this search, this second new inequality stays informative in order regimes of sample size is OK. At least it's upper bound, while this second one it can be better, at least in the regime where sample sizes are compatible.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me turn to the application to transductive learning setting.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for simplicity.",
                    "label": 0
                },
                {
                    "sent": "I will consider deterministic agnostic setting so.",
                    "label": 1
                },
                {
                    "sent": "Let's see what it is.",
                    "label": 1
                },
                {
                    "sent": "So you have a finite instance space.",
                    "label": 1
                },
                {
                    "sent": "And some output space.",
                    "label": 0
                },
                {
                    "sent": "You have fixed your favorite hypothesis class of predictors.",
                    "label": 1
                },
                {
                    "sent": "And you have a deterministic non random labeling function.",
                    "label": 1
                },
                {
                    "sent": "Which does not necessarily belong to your hypothesis class, which is, which means that it's an agnostic situation.",
                    "label": 0
                },
                {
                    "sent": "OK, so you start with sampling an inputs uniformly without replacements from this.",
                    "label": 0
                },
                {
                    "sent": "Instance space, then you obtain their labels by applying this labeling function.",
                    "label": 0
                },
                {
                    "sent": "And in the end, learning procedure gets access to the labeled training sample, and you unlabeled test points.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the goal of learning procedure is to find the predictor in hypothesis class.",
                    "label": 1
                },
                {
                    "sent": "Having a minimal error on the test set.",
                    "label": 0
                },
                {
                    "sent": "Where loss functional is bounded in 01 interval.",
                    "label": 1
                },
                {
                    "sent": "OK, we are considering any bounded loss function.",
                    "label": 0
                },
                {
                    "sent": "So I also we also define errors on the whole sample and on the training sample, which is empirical error.",
                    "label": 1
                },
                {
                    "sent": "And also we define functions, minimizing errors on training sample, test sample and whole sample respectively.",
                    "label": 0
                },
                {
                    "sent": "This quantity, which is often called excess loss.",
                    "label": 0
                },
                {
                    "sent": "This measures how well does empirical risk minimizer approximate the theoretical optimal one on the test set.",
                    "label": 0
                },
                {
                    "sent": "We want to use empirical risk minimization to approximate the optimal solution.",
                    "label": 1
                },
                {
                    "sent": "So our goal would be to obtain tight high probability upper bounds on this quantity, which is random becausw.",
                    "label": 0
                },
                {
                    "sent": "Both of these functions are random.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a.",
                    "label": 0
                },
                {
                    "sent": "At least of results in this direction of previous results.",
                    "label": 1
                },
                {
                    "sent": "So there were some implicit bounds which provided computational procedures, which leads to the values of the bounds.",
                    "label": 1
                },
                {
                    "sent": "Then there were bounds for quadratic loss or further one was quite open with empirical risk standing in front of it.",
                    "label": 0
                },
                {
                    "sent": "Then there were couple of population inequality's.",
                    "label": 0
                },
                {
                    "sent": "Which crucially depends on the prior which you need to set prior to observing the data.",
                    "label": 0
                },
                {
                    "sent": "OK, then there were couple of bounds based on algorithmic stability.",
                    "label": 1
                },
                {
                    "sent": "Again of the order 1 / sqrt N also global or transductive learning market complexities, which again give you 1 / sqrt N. And finally, it is well known that if we put some strict assumptions on our problem, namely if we will consider assume that this labeling function belongs to our function class, then you can obtain fast rates of 1 / M OK, But the message here is that all these bounds without strict assumptions on the problem at hand, they provide only slow rates 1 / sqrt N.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me very briefly remind you of well known results in inductive setting, which assumes that your training sample is IID from some unknown distribution.",
                    "label": 1
                },
                {
                    "sent": "So the classic approach.",
                    "label": 1
                },
                {
                    "sent": "Suggested to work with uniform deviations over the whole class of functions, and they usually lead to the rates of 1 / sqrt N which are termed slow rates.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Later, it was understood that it is over pessimistic and instead we should.",
                    "label": 1
                },
                {
                    "sent": "Take Soprema supremum over some subclass.",
                    "label": 0
                },
                {
                    "sent": "Which is likely to contain empirical risk minimizer.",
                    "label": 0
                },
                {
                    "sent": "And which consists of functions with small variances.",
                    "label": 1
                },
                {
                    "sent": "And if you use this as this technique together with telegrams inequality, it will provide you a way to obtain fast rates faster than one over square root of them.",
                    "label": 1
                },
                {
                    "sent": "So these are well known results.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we have a version of Telegram's inequality for sampling without replacement, which can be applied to the transductive setting and here is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The result we get we have so consider this empirical risk measured tones IID sample.",
                    "label": 0
                },
                {
                    "sent": "That will be needed for technical reasons.",
                    "label": 0
                },
                {
                    "sent": "And consider the local neighborhood of the function, which is optimal on the general set in your hypothesis class, which is measured by this L2 distance between losses of these functions OK. And assume that this nontrivial relation holds between the variance of loss and the expected value of loss.",
                    "label": 0
                },
                {
                    "sent": "OK, then what this theorem states is that the difference between.",
                    "label": 0
                },
                {
                    "sent": "Errors on the general set is upper bounded.",
                    "label": 0
                },
                {
                    "sent": "By the uniform deviations over this local region, and to be more precise by the fixed point of modulus of continuity of this empirical process measured around the optimal function.",
                    "label": 0
                },
                {
                    "sent": "OK, and this kind of results were well known in estimation problems.",
                    "label": 0
                },
                {
                    "sent": "Then inductive setting.",
                    "label": 0
                },
                {
                    "sent": "And now we recover these results in transductive setting as well.",
                    "label": 0
                },
                {
                    "sent": "OK, but we wanted to control the performance on the test set right?",
                    "label": 0
                },
                {
                    "sent": "And here is the control for the general set and you can obtain the control for the test set which looks.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Were you similar?",
                    "label": 0
                },
                {
                    "sent": "And I remind you that this term has order fixed point plus 1 / N. OK, so important thing to notice is that this fixed point.",
                    "label": 0
                },
                {
                    "sent": "In some situations it can be over fast rate.",
                    "label": 0
                },
                {
                    "sent": "For example it happens for VC classes.",
                    "label": 0
                },
                {
                    "sent": "In that case you have 1 / N or for kernel classes for balls and Eric AHS and Lipschitz losses.",
                    "label": 1
                },
                {
                    "sent": "In this case the order rates depend on the tail sum of Asian values of the integral operator corresponding to the kernel you chose.",
                    "label": 0
                },
                {
                    "sent": "And considering this this assumption, it happens that.",
                    "label": 0
                },
                {
                    "sent": "It is not too restrictive and there are situations when it holds.",
                    "label": 0
                },
                {
                    "sent": "For example, if you consider quadratic loss with uniformly bounded convex classes.",
                    "label": 1
                },
                {
                    "sent": "You will obtain B = 1 here and binary Loss and VC classes if you also assume that the labeling function is contained in your hypothesis class, which is somehow related with the low noise assumptions.",
                    "label": 0
                },
                {
                    "sent": "Then you will also obtain this kind of.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relation OK, so I think that's it.",
                    "label": 0
                },
                {
                    "sent": "I have one more minute or so.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are a couple of future directions.",
                    "label": 0
                },
                {
                    "sent": "First of all, we still have a gap in our inequality and it will be very interesting to close the gap 2nd.",
                    "label": 1
                },
                {
                    "sent": "Is it possible to obtain the version of telegrams inequality which improves upon telegrams inequality in the same way as serfling is bound to prove improves Holdings inequality becausw our inequality's do not OK. We can also consider going to local transductive ready market complexities.",
                    "label": 1
                },
                {
                    "sent": "And of course, very interesting would be to apply our inequality's two other problems and arguably the most interesting of them would be to apply the normal synthetic analysis of cross validation procedures.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}