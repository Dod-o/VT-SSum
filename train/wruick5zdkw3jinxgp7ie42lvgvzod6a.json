{
    "id": "wruick5zdkw3jinxgp7ie42lvgvzod6a",
    "title": "Pointwise Tracking the Optimal Regression Function",
    "info": {
        "author": [
            "Yair Wiener, Computer Science Department, Technion - Israel Institute of Technology"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/machine_wiener_pointwise_tracking/",
    "segmentation": [
        [
            "For most of us."
        ],
        [
            "It's only a dream to have unlimited number of examples for every learning problem.",
            "In fact, in this case it has been proven that even simple learning algorithm like KNN approach the Bazel.",
            "However, in most practical problems in the the number of training example is limited and in many cases it's even small.",
            "So the question is, given hypothesis class, can we get arbitrary close to the best hypothesis in the class using only small number of training examples?",
            "Unfortunately, there is no such thing as free lunch.",
            "Something needs to be compromised."
        ],
        [
            "Mark Twain once said that it ends what we don't know that get us into trouble.",
            "But what we know for sure that just end.",
            "So if this rifle we propose to compromise the coverage, we allow the classifier or regressor in our case, to abstain from prediction on part of the domain.",
            "And by doing that we guarantee that on the accepted domain are regress or will be pointwise closed to the best regressor in the class.",
            "When I say pointwise close, I mean that.",
            "For every single test point in the accepted domain, the difference between the prediction of our regressor and the best regressor in the class will not be more than epsilon.",
            "So how do we achieve that?",
            "We start with the small training set depicted in the blue dots and we calculate the set of hypothesis with low empirical error.",
            "That's the greenish set on the right side.",
            "Then whenever we are presented with a new test point, we check if there are at least two hypothesis in the green set that predicts values for that test point with a difference of more than epsilon.",
            "If this is the case, we reject the point.",
            "If this is not the case, we accept the point we were able to show that with high probability the best regressor in the class reside in the green set.",
            "Therefore they regress or or the selective regress.",
            "So if you want that we get a is actually pointwise closed to the best regressor in the class.",
            "Interesting Lee, we were able also to prove that under reasonable conditions the volume of the rejected domain diminished quickly with the number of training examples."
        ],
        [
            "So in the paper we were able to show theoretical results, both coverage bound on the accepted domain as well as well as a pointwise regret bound for the entire domain.",
            "We were also able to show how we can implement this algorithm efficiently for linear regression, and we conclude with some empirical results demonstrating the advantage of our technique compared to other rejection based technique for a. Regression I invite all of you on Thursday to our poster.",
            "It's TH 56 you are all welcome.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For most of us.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's only a dream to have unlimited number of examples for every learning problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, in this case it has been proven that even simple learning algorithm like KNN approach the Bazel.",
                    "label": 0
                },
                {
                    "sent": "However, in most practical problems in the the number of training example is limited and in many cases it's even small.",
                    "label": 0
                },
                {
                    "sent": "So the question is, given hypothesis class, can we get arbitrary close to the best hypothesis in the class using only small number of training examples?",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there is no such thing as free lunch.",
                    "label": 0
                },
                {
                    "sent": "Something needs to be compromised.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mark Twain once said that it ends what we don't know that get us into trouble.",
                    "label": 0
                },
                {
                    "sent": "But what we know for sure that just end.",
                    "label": 0
                },
                {
                    "sent": "So if this rifle we propose to compromise the coverage, we allow the classifier or regressor in our case, to abstain from prediction on part of the domain.",
                    "label": 0
                },
                {
                    "sent": "And by doing that we guarantee that on the accepted domain are regress or will be pointwise closed to the best regressor in the class.",
                    "label": 0
                },
                {
                    "sent": "When I say pointwise close, I mean that.",
                    "label": 0
                },
                {
                    "sent": "For every single test point in the accepted domain, the difference between the prediction of our regressor and the best regressor in the class will not be more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So how do we achieve that?",
                    "label": 0
                },
                {
                    "sent": "We start with the small training set depicted in the blue dots and we calculate the set of hypothesis with low empirical error.",
                    "label": 0
                },
                {
                    "sent": "That's the greenish set on the right side.",
                    "label": 0
                },
                {
                    "sent": "Then whenever we are presented with a new test point, we check if there are at least two hypothesis in the green set that predicts values for that test point with a difference of more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "If this is the case, we reject the point.",
                    "label": 0
                },
                {
                    "sent": "If this is not the case, we accept the point we were able to show that with high probability the best regressor in the class reside in the green set.",
                    "label": 0
                },
                {
                    "sent": "Therefore they regress or or the selective regress.",
                    "label": 0
                },
                {
                    "sent": "So if you want that we get a is actually pointwise closed to the best regressor in the class.",
                    "label": 0
                },
                {
                    "sent": "Interesting Lee, we were able also to prove that under reasonable conditions the volume of the rejected domain diminished quickly with the number of training examples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the paper we were able to show theoretical results, both coverage bound on the accepted domain as well as well as a pointwise regret bound for the entire domain.",
                    "label": 1
                },
                {
                    "sent": "We were also able to show how we can implement this algorithm efficiently for linear regression, and we conclude with some empirical results demonstrating the advantage of our technique compared to other rejection based technique for a. Regression I invite all of you on Thursday to our poster.",
                    "label": 0
                },
                {
                    "sent": "It's TH 56 you are all welcome.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}