{
    "id": "lm2spe2am6krqgwlbhxgttq5pk7fwqi6",
    "title": "Metric Embedding for Kernel Classification Rules",
    "info": {
        "author": [
            "Bharath K. Sriperumbudur, Department of Electrical and Computer Engineering, UC San Diego"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Classification"
        ]
    },
    "url": "http://videolectures.net/icml08_sriperumbudur_me/",
    "segmentation": [
        [
            "Ambrette so this is joint work with Homer and get lancret."
        ],
        [
            "This is San Diego.",
            "So the idea of this work is basically to.",
            "So basically, the comparative metric embedding scheme for this puzzle window based methods, which we know the positive methods are basically popular in density estimation and kernel regression, and in this work basically we consider these rules."
        ],
        [
            "Or classification.",
            "So the setup is as follows where so we all know about this binder classification where we have these set of training points down from distribution D. And then we need to classify a given point which belongs study.",
            "So the current classification rules that have been studied by Deborah and others where the rule is as follows.",
            "So this can be actually seen as.",
            "OK so so.",
            "Basically this is the rule where you are given point X.",
            "You have a weighting function which is centered at this point and this weighting function is basically called the smoothing kernel function which is usually non negative and you can actually see that if this this function is a positive definite function then this rule can be thought of as a arcade just based kernel code.",
            "Oh so so.",
            "One way to think about this rule is basically you have a weighting function.",
            "You have a function which is sitting at this point X, which weights each of these points.",
            "XI An H is the bandwidth that is associated with this.",
            "With this smoothing kernel.",
            "So what this rule does is basically if the waiting that is associated the points which are which belong to the Class 0.",
            "If it is more than the waiting that is associated, the points which are available one then this point is classified as zero, otherwise it's classified as one."
        ],
        [
            "So some other examples of these kernels include the Gaussian kernel and coffee kernel, which are actually positive definite functions.",
            "So when you use these kernels, actually you can treat these rules as in our kitchens based kernel rule, whereas the other two kernels are this.",
            "Now you gonna land on the oppositional kernel, which is which is much more famous in statistics.",
            "You can you can actually show that these two kernels are not positive definite kernels."
        ],
        [
            "And when the when the kernels are you Colonel, then this rule actually what it is doing is something very similar to K nearest neighbor classification, except that here it is doing something like the hedge ball nearest neighbor classification.",
            "So when you use occur when you use this.",
            "This type of a kernel which means.",
            "This Colonel.",
            "It's one over over this this.",
            "Of radius edge.",
            "So this rule is equivalent to 8 weighted majority rule.",
            "So now the only parameter that needs to be determined in this rule is this parameter H. There are some asymptotic guarantees about how to choose this hedge in the infinite sample case, but in the finite sample case we don't know how to choose this.",
            "Choose this parameter H since we are in a classification setting, maybe one can deal with one can actually estimate this parameter using cross validation, but in this work we come up with a different scheme which we tie into this metric embedding and then jointly compute this embedding function and then."
        ],
        [
            "Bandwidth parameter.",
            "A quick intro on this metric learning for this kernel.",
            "Nearest neighbor rules OK.",
            "Nearest neighbor rules Snap-on.",
            "Venkatesh has shown that the final sample risk of AK nearest neighbor rule it can be reduced by using a weighted Euclidean metric, even though the infinite sample disk is independent of the metric used so."
        ],
        [
            "So.",
            "The final sample setting this result has been experimentally verified by by your.",
            "These works, which actually appeared quite recently, and so the idea in most of the most of these works actually the idea behind is to come up with an embedding function L. So basically every point gets mapped in to Alex and then you compute the Euclidean distance between the between these embedded points.",
            "In such a way that it improves the classification accuracy of the K nearest neighbor rule."
        ],
        [
            "So one of the motivation for work is is as follows, where.",
            "In some applications, probably 1 needs to have natural distance measures which reflect the structure of the data.",
            "So if you want to measure the distance between the image is probably a good measure.",
            "Might be the tangent distance, or if you're if you're measuring the points between the distance between the points on a manifold, better distance would be a jealousy rather than directly using the Canadian distance.",
            "But since I mean most of the time you don't know what is the right metric to use, so usually we end up using the kleidion or the weighted Euclidean distance in computing the distance between the points.",
            "For most of these distance based rules.",
            "Like this can use neighbor rules."
        ],
        [
            "So.",
            "These are the two questions that were addressed in this work where we need to find the bandwidth parameter of these.",
            "The parzen window based rules and we need to find this embedding function Phi.",
            "There actually tempered this metric Space Hero is a metric that is associated with this space.",
            "We need him, but this metric spaces into space where the metric is equal."
        ],
        [
            "Alien.",
            "So the problem formulation is as follows.",
            "We generalize the binder classification rule into multi multi class classification rule where actually you can see I've introduced this symbol as an indicator function and.",
            "This rule this rule is just.",
            "It's a simple generalization of the binder rule where what I'm doing is just the weighted majority.",
            "And OK, so I need to explain this, so here I'm not talking about any isometric embedding.",
            "I mean I don't want an embedding function fee which isometrically embeds this metric.",
            "SpaceX row into Y, L2, becausw.",
            "If the distances are preserved in both spaces, then it does not make any sense to work work in the other space.",
            "You might as well work in the input space, so here the idea is.",
            "You need to basically find this function fee which kind of Maps you into the Delta spares, and I'll be measuring the distance the L2 norm of the difference of these embedded points."
        ],
        [
            "So the goal is.",
            "So we need to learn Fi and H by minimizing the probability of error associated with this multiclass classification rule.",
            "So this is what I want to do and we don't know the distribution from which data points."
        ],
        [
            "Sample so.",
            "Simple ways.",
            "OK, let us minimize the empirical risk which is this.",
            "And actually in the paper I have given example where you can actually come up.",
            "You can always come up with a function fee for which you can.",
            "You can have this error to be 0, whereas your generalization error can be arbitrary."
        ],
        [
            "So the regular set up where we basically regularize this problem by introducing a by basically restricting a function to a smaller set of functions, which is equally which is equally imposing the penalty on the on your setup embedding functions which you want to learn.",
            "OK, so.",
            "Here if you see so.",
            "I'm basically minimizing the number of instances on which number of instances which get misclassified by this rule, and since it's basically a non convex function which is hard to solve you, basically you can replace.",
            "You can replace this function using this hinge Ristic.",
            "But if you notice, the Gian itself has a lot of actually nonconvex functions.",
            "It has like 2 indicator functions and this has it has this maximization, so you need to do a lot of.",
            "But tedious algebra to actually put into any."
        ],
        [
            "Form so that you can apply invoke convex relaxations.",
            "So I'm basically I'm not.",
            "I'm not doing all this stuff here.",
            "You can actually refer to that in the paper.",
            "So what we do is we minimize an upper bound on on the empirical classification error and then we use this hinge relaxation to.",
            "To arrive at this form so this form is, this form is actually interesting and I'll show you I'll show you a few images in the next slides, so.",
            "So in this form, what needs to be determined is we need to know what kind of functions we're going to restrict ourselves so that we can solve this problem in a. Polly"
        ],
        [
            "Polynomial time, so one choice of this function classes.",
            "Choose your, choose your fee to be the set of all Mercer kernel Maps.",
            "So these muscle kernel Maps have this property that it always embeds your data space into negative space, which is basically which has an associated this kernel care here, here, this Kripke is basically the reproducing kernel associated with some Mark ages.",
            "Then if you choose it."
        ],
        [
            "Is your function fee of that form then?",
            "This distance is a function of this descriptor alone, so which means.",
            "Which means this quantity actually depends only on the entries of the kernel matrix, and usually the regular since it depends on the metrics and we need to regularize this function Phi.",
            "This thing is usually chosen to be some functional of the kernel matrix.",
            "Usually it can be traced the kernel or or the business number the kernel.",
            "So you can clearly see that now you have an optimization problem which depends.",
            "On only the kernel matrix, and then your your hedge.",
            "And since it only depends on the kernel matrix, so this problem is equal to doing the kernel learning.",
            "And once you learn the kernel this does not.",
            "This does not provide you an option to extend it out of samples, so which means you can do kind of an unsupervised or a transit 2 type of learning with this setup, but you cannot do a supervised learning by by this choice or the function class.",
            "So I'll talk later by restricting fetuin architectures, which basically helps in solving the problem.",
            "But before that, let's get back to this equation where here you can see that you have two different hinge loss is there isn't hinge loss inside and there is an angel's outside, so we will see what these laws are.",
            "What these functions."
        ],
        [
            "Actually doing.",
            "So.",
            "Graph."
        ],
        [
            "Way of explaining this object to this first part of the objective."
        ],
        [
            "Is basically this where?",
            "Let's choose let's fix a point XI and then let us consider a ball of Delta ball of radius H. And let us consider like two other different balls of radius X + 1 and H -- 1.",
            "Now the objective what it is trying to do is basically pulling the points which are the same classes XI into the ball of radius X -- 1 at the same time it tries to push the dip points which are of different classes XA into outside the ball H + 1.",
            "So basically this word.",
            "The objective function in the previous."
        ],
        [
            "Patient is doing so.",
            "So once OK.",
            "So this is what I've shown where it tries to adjust your embedding function fee and then the radius of the ball hitch in such a way that more number of points of the same class actually it enters inside this ball of radius X -- 1."
        ],
        [
            "Now at some point this.",
            "What it does is.",
            "Once this point XI, it gets the required number of points such that it can be classified correctly.",
            "It doesn't.",
            "It doesn't try to bring anymore points of the same class inside to the inside.",
            "This ball of radius such minus one at the same time, it doesn't try to push the points of opposite class outside the ball of H + 1, so this is what they."
        ],
        [
            "The outer hinge allows the outer hinge function that without a hitch function comes in."
        ],
        [
            "OK, so now we.",
            "So we saw this situation where if I choose my embedding function to be the class of most of Colonel Maps, then it does not help you in solving this supervised setting problem.",
            "So we consider fee to be in architectures where I consider fee to be a multivariate function where we have like 3 one to 3D which are which are real valued functions and each fear you belong to an architecture such a who's reproducing kernel scale and choosing a regularizer of this form.",
            "By invoking the standard represent a theorem will get the representation of each of these functions to be of this form, and then you have a constraint on the coefficients of this of this function.",
            "Earn intuitively one can see how these can."
        ],
        [
            "It's come up.",
            "So in the objective here you can see that this distance since these are distance between Phoenix and Phoenix J, which is invariant to translation."
        ],
        [
            "So which actually it ends up in a constraint on the coefficients of these of the kernel functions."
        ],
        [
            "And if we choose these, these different kernel functions to be the single function, then you can show the distance between these embeddings is equivalent to computing the mob is distance between the empirical kernel Maps."
        ],
        [
            "And if I choose my domain to be already, and if I choose my kernel to be a linear kernel, then this distance is equivalent to computing the model of this distance between the points in the original space.",
            "So so this result is interesting.",
            "'cause now let's say I choose my Spartan window rule where I'm not using this naive kernel.",
            "But let's say I'm using a Gaussian kernel.",
            "This means already.",
            "I'm my rule is something like an arcade, just basic rule, and probably you don't want to do again again, the second level of embedding into our cages.",
            "So in such a case you can.",
            "You can simply use.",
            "A linear kernel, which is actually amounts to computing the somehow capturing the covariance structure of the data which lies in your input space."
        ],
        [
            "Is equal to D. So once we have this representation of fee, we basically use that and plug it back into the into this into into our program and then we have.",
            "We have this program.",
            "I mean at the first instance, if you see I mean this program doesn't seem to be.",
            "Actually it seems to be convex, so this part of the program is for sure it's convex.",
            "But here this parameter to AJ.",
            "Actually it makes the problem nonconvex, because Tower J can take values.",
            "Plus or minus one.",
            "So since since when it is plus this function is convex in C, But when it is minus, so you have, you can write your, then you're writing this as like a difference of two convex functions, which is hard problem to solve."
        ],
        [
            "However, we can do semidefinite relaxation by writing or Sigma as C, transpose C, and here and then neglecting the neglecting the rank constraint on our Sigma, which results in this semidefinite program.",
            "OK, so they the algorithm is as follows.",
            "We don't want to use.",
            "We don't want to use any standard solvers to solve this problem, so we use this trick proposed by Weinberger and solve in their large margin nearest neighbor paper.",
            "So the idea is.",
            "We basically find.",
            "When are these hinge functions active?",
            "Which means when is this function greater than zero and when is this whole thing greater than zero?",
            "So you need to find the set of all AJ for which you're this part of the object to is actually greater than 0."
        ],
        [
            "So which we?"
        ],
        [
            "I denote by script A, so once I find what script I is then the program basically reduces to.",
            "Reduces to this form, where now I can alternately minimize between Sigma and H."
        ],
        [
            "And when you're minimizing or Sigma, so you have if you don't have this constraint and when you're minimizing or Sigma then.",
            "OK.",
            "So here you have, you have basically an intersection of two convex sets, and the idea is you need to minimize this objective over the intersection of these two convex sets, and you don't.",
            "You don't get a straightforward rule, so you need to alternately projected you need to solve that objective and then projected onto this convex sets, and then you need to do it iteratively together."
        ],
        [
            "Solution.",
            "So the algorithm just follows where this part of the algorithm basically finds which are which.",
            "Is find the script here, which is your active set and once you know the active said we do a, we do a gradient descent and then we do this projection operation onto this onto the intersection of these two convex sets, which gives the next update of our Sigma.",
            "And then we do the same thing for this parameter H and you keep iterating until until.",
            "Until this two parameter."
        ],
        [
            "Converge.",
            "OK, before I get to the results, I'll show a small demo.",
            "OK, so this is the wine data set from the user database.",
            "So this data set is 178 points which are in 13 dimensional space and this picture is basically the that is basically projecting on to the first 2 eigenvectors of the covariance matrix of this data.",
            "So basically the PCA basis.",
            "And you can see basically there are.",
            "I mean if I'm doing the classification on that on the training data itself.",
            "So this is the error that is associated with the with this classification and this and this calculation is done in 30 dimensions.",
            "OK.",
            "So now this is your active set matrix where you can see that if you.",
            "Each of these active rows.",
            "Basically these are the ones where the outer hinge function is active.",
            "And in each row, if there are empty blocks, which means in those functions, the inner hinge function is inactive.",
            "So which means those points do not come into picture.",
            "And so you can actually see the.",
            "The points which are almost like near the near the boundary of each of these clusters.",
            "Basically they are the points which are.",
            "Yep, so which are basically sufficient to describe the clusters.",
            "OK.",
            "So we ran these experiments on five USA datasets and we compared it to the simple K nearest neighbor and these are the.",
            "And and these are the other key nearest neighbor metric learning algorithms, and this is our method with the Gaussian kernel and the linear kernel.",
            "And here you can see."
        ],
        [
            "That our method works better than the other methods.",
            "And.",
            "Yeah, in some cases, in fact the linear kernel it does better than even the Gaussian kernel."
        ],
        [
            "So to quickly summarize, basically we proposed a method to embed the metric space into space by for this kernel classification rule, and we also learn the."
        ],
        [
            "Deposit window and the advantage of our method over over some of them.",
            "Some other metric learning methods is that this method, this element and actually requires the target neighbors to be defined our priority so that now you can somehow find the distance between the points to learn your metric.",
            "So such."
        ],
        [
            "Definition is not required for our method and our method involves fewer tuning parameters compared to other meta."
        ],
        [
            "Learning methods.",
            "We feel this.",
            "This work actually provides a unified and a formal treatment for solving these metric learning problems compared to other heuristics."
        ],
        [
            "One main issue of not only our method, but all these metric learning methods is that they solve some different program and these programs are very expensive, very intensive computation intensive.",
            "So the future work.",
            "Basically it involves speeding up these methods.",
            "Thank you.",
            "While it's taking questions, the next speaker could come up.",
            "Oh, it's sorry it's large margin.",
            "Nearest neighbor.",
            "What is it?",
            "No, I mean.",
            "No, I mean you you basically you.",
            "Basically you need to predefine what your neighbors are going to be, and then you want to learn a metric such that the points you want to bring the points of same class close to those neighbors so that points get slightly classified."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ambrette so this is joint work with Homer and get lancret.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is San Diego.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this work is basically to.",
                    "label": 0
                },
                {
                    "sent": "So basically, the comparative metric embedding scheme for this puzzle window based methods, which we know the positive methods are basically popular in density estimation and kernel regression, and in this work basically we consider these rules.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or classification.",
                    "label": 0
                },
                {
                    "sent": "So the setup is as follows where so we all know about this binder classification where we have these set of training points down from distribution D. And then we need to classify a given point which belongs study.",
                    "label": 0
                },
                {
                    "sent": "So the current classification rules that have been studied by Deborah and others where the rule is as follows.",
                    "label": 0
                },
                {
                    "sent": "So this can be actually seen as.",
                    "label": 0
                },
                {
                    "sent": "OK so so.",
                    "label": 0
                },
                {
                    "sent": "Basically this is the rule where you are given point X.",
                    "label": 0
                },
                {
                    "sent": "You have a weighting function which is centered at this point and this weighting function is basically called the smoothing kernel function which is usually non negative and you can actually see that if this this function is a positive definite function then this rule can be thought of as a arcade just based kernel code.",
                    "label": 1
                },
                {
                    "sent": "Oh so so.",
                    "label": 0
                },
                {
                    "sent": "One way to think about this rule is basically you have a weighting function.",
                    "label": 0
                },
                {
                    "sent": "You have a function which is sitting at this point X, which weights each of these points.",
                    "label": 0
                },
                {
                    "sent": "XI An H is the bandwidth that is associated with this.",
                    "label": 0
                },
                {
                    "sent": "With this smoothing kernel.",
                    "label": 0
                },
                {
                    "sent": "So what this rule does is basically if the waiting that is associated the points which are which belong to the Class 0.",
                    "label": 0
                },
                {
                    "sent": "If it is more than the waiting that is associated, the points which are available one then this point is classified as zero, otherwise it's classified as one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some other examples of these kernels include the Gaussian kernel and coffee kernel, which are actually positive definite functions.",
                    "label": 0
                },
                {
                    "sent": "So when you use these kernels, actually you can treat these rules as in our kitchens based kernel rule, whereas the other two kernels are this.",
                    "label": 0
                },
                {
                    "sent": "Now you gonna land on the oppositional kernel, which is which is much more famous in statistics.",
                    "label": 0
                },
                {
                    "sent": "You can you can actually show that these two kernels are not positive definite kernels.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when the when the kernels are you Colonel, then this rule actually what it is doing is something very similar to K nearest neighbor classification, except that here it is doing something like the hedge ball nearest neighbor classification.",
                    "label": 1
                },
                {
                    "sent": "So when you use occur when you use this.",
                    "label": 0
                },
                {
                    "sent": "This type of a kernel which means.",
                    "label": 0
                },
                {
                    "sent": "This Colonel.",
                    "label": 0
                },
                {
                    "sent": "It's one over over this this.",
                    "label": 0
                },
                {
                    "sent": "Of radius edge.",
                    "label": 0
                },
                {
                    "sent": "So this rule is equivalent to 8 weighted majority rule.",
                    "label": 0
                },
                {
                    "sent": "So now the only parameter that needs to be determined in this rule is this parameter H. There are some asymptotic guarantees about how to choose this hedge in the infinite sample case, but in the finite sample case we don't know how to choose this.",
                    "label": 1
                },
                {
                    "sent": "Choose this parameter H since we are in a classification setting, maybe one can deal with one can actually estimate this parameter using cross validation, but in this work we come up with a different scheme which we tie into this metric embedding and then jointly compute this embedding function and then.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "A quick intro on this metric learning for this kernel.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor rules OK.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor rules Snap-on.",
                    "label": 0
                },
                {
                    "sent": "Venkatesh has shown that the final sample risk of AK nearest neighbor rule it can be reduced by using a weighted Euclidean metric, even though the infinite sample disk is independent of the metric used so.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The final sample setting this result has been experimentally verified by by your.",
                    "label": 1
                },
                {
                    "sent": "These works, which actually appeared quite recently, and so the idea in most of the most of these works actually the idea behind is to come up with an embedding function L. So basically every point gets mapped in to Alex and then you compute the Euclidean distance between the between these embedded points.",
                    "label": 1
                },
                {
                    "sent": "In such a way that it improves the classification accuracy of the K nearest neighbor rule.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the motivation for work is is as follows, where.",
                    "label": 0
                },
                {
                    "sent": "In some applications, probably 1 needs to have natural distance measures which reflect the structure of the data.",
                    "label": 1
                },
                {
                    "sent": "So if you want to measure the distance between the image is probably a good measure.",
                    "label": 1
                },
                {
                    "sent": "Might be the tangent distance, or if you're if you're measuring the points between the distance between the points on a manifold, better distance would be a jealousy rather than directly using the Canadian distance.",
                    "label": 0
                },
                {
                    "sent": "But since I mean most of the time you don't know what is the right metric to use, so usually we end up using the kleidion or the weighted Euclidean distance in computing the distance between the points.",
                    "label": 0
                },
                {
                    "sent": "For most of these distance based rules.",
                    "label": 0
                },
                {
                    "sent": "Like this can use neighbor rules.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These are the two questions that were addressed in this work where we need to find the bandwidth parameter of these.",
                    "label": 0
                },
                {
                    "sent": "The parzen window based rules and we need to find this embedding function Phi.",
                    "label": 0
                },
                {
                    "sent": "There actually tempered this metric Space Hero is a metric that is associated with this space.",
                    "label": 0
                },
                {
                    "sent": "We need him, but this metric spaces into space where the metric is equal.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alien.",
                    "label": 0
                },
                {
                    "sent": "So the problem formulation is as follows.",
                    "label": 0
                },
                {
                    "sent": "We generalize the binder classification rule into multi multi class classification rule where actually you can see I've introduced this symbol as an indicator function and.",
                    "label": 0
                },
                {
                    "sent": "This rule this rule is just.",
                    "label": 0
                },
                {
                    "sent": "It's a simple generalization of the binder rule where what I'm doing is just the weighted majority.",
                    "label": 0
                },
                {
                    "sent": "And OK, so I need to explain this, so here I'm not talking about any isometric embedding.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't want an embedding function fee which isometrically embeds this metric.",
                    "label": 0
                },
                {
                    "sent": "SpaceX row into Y, L2, becausw.",
                    "label": 0
                },
                {
                    "sent": "If the distances are preserved in both spaces, then it does not make any sense to work work in the other space.",
                    "label": 0
                },
                {
                    "sent": "You might as well work in the input space, so here the idea is.",
                    "label": 0
                },
                {
                    "sent": "You need to basically find this function fee which kind of Maps you into the Delta spares, and I'll be measuring the distance the L2 norm of the difference of these embedded points.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal is.",
                    "label": 0
                },
                {
                    "sent": "So we need to learn Fi and H by minimizing the probability of error associated with this multiclass classification rule.",
                    "label": 1
                },
                {
                    "sent": "So this is what I want to do and we don't know the distribution from which data points.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample so.",
                    "label": 0
                },
                {
                    "sent": "Simple ways.",
                    "label": 0
                },
                {
                    "sent": "OK, let us minimize the empirical risk which is this.",
                    "label": 0
                },
                {
                    "sent": "And actually in the paper I have given example where you can actually come up.",
                    "label": 0
                },
                {
                    "sent": "You can always come up with a function fee for which you can.",
                    "label": 0
                },
                {
                    "sent": "You can have this error to be 0, whereas your generalization error can be arbitrary.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the regular set up where we basically regularize this problem by introducing a by basically restricting a function to a smaller set of functions, which is equally which is equally imposing the penalty on the on your setup embedding functions which you want to learn.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here if you see so.",
                    "label": 0
                },
                {
                    "sent": "I'm basically minimizing the number of instances on which number of instances which get misclassified by this rule, and since it's basically a non convex function which is hard to solve you, basically you can replace.",
                    "label": 0
                },
                {
                    "sent": "You can replace this function using this hinge Ristic.",
                    "label": 0
                },
                {
                    "sent": "But if you notice, the Gian itself has a lot of actually nonconvex functions.",
                    "label": 0
                },
                {
                    "sent": "It has like 2 indicator functions and this has it has this maximization, so you need to do a lot of.",
                    "label": 0
                },
                {
                    "sent": "But tedious algebra to actually put into any.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Form so that you can apply invoke convex relaxations.",
                    "label": 0
                },
                {
                    "sent": "So I'm basically I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not doing all this stuff here.",
                    "label": 0
                },
                {
                    "sent": "You can actually refer to that in the paper.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we minimize an upper bound on on the empirical classification error and then we use this hinge relaxation to.",
                    "label": 1
                },
                {
                    "sent": "To arrive at this form so this form is, this form is actually interesting and I'll show you I'll show you a few images in the next slides, so.",
                    "label": 0
                },
                {
                    "sent": "So in this form, what needs to be determined is we need to know what kind of functions we're going to restrict ourselves so that we can solve this problem in a. Polly",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Polynomial time, so one choice of this function classes.",
                    "label": 0
                },
                {
                    "sent": "Choose your, choose your fee to be the set of all Mercer kernel Maps.",
                    "label": 0
                },
                {
                    "sent": "So these muscle kernel Maps have this property that it always embeds your data space into negative space, which is basically which has an associated this kernel care here, here, this Kripke is basically the reproducing kernel associated with some Mark ages.",
                    "label": 0
                },
                {
                    "sent": "Then if you choose it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is your function fee of that form then?",
                    "label": 0
                },
                {
                    "sent": "This distance is a function of this descriptor alone, so which means.",
                    "label": 1
                },
                {
                    "sent": "Which means this quantity actually depends only on the entries of the kernel matrix, and usually the regular since it depends on the metrics and we need to regularize this function Phi.",
                    "label": 1
                },
                {
                    "sent": "This thing is usually chosen to be some functional of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Usually it can be traced the kernel or or the business number the kernel.",
                    "label": 0
                },
                {
                    "sent": "So you can clearly see that now you have an optimization problem which depends.",
                    "label": 0
                },
                {
                    "sent": "On only the kernel matrix, and then your your hedge.",
                    "label": 0
                },
                {
                    "sent": "And since it only depends on the kernel matrix, so this problem is equal to doing the kernel learning.",
                    "label": 1
                },
                {
                    "sent": "And once you learn the kernel this does not.",
                    "label": 0
                },
                {
                    "sent": "This does not provide you an option to extend it out of samples, so which means you can do kind of an unsupervised or a transit 2 type of learning with this setup, but you cannot do a supervised learning by by this choice or the function class.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk later by restricting fetuin architectures, which basically helps in solving the problem.",
                    "label": 0
                },
                {
                    "sent": "But before that, let's get back to this equation where here you can see that you have two different hinge loss is there isn't hinge loss inside and there is an angel's outside, so we will see what these laws are.",
                    "label": 0
                },
                {
                    "sent": "What these functions.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually doing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Graph.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way of explaining this object to this first part of the objective.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is basically this where?",
                    "label": 0
                },
                {
                    "sent": "Let's choose let's fix a point XI and then let us consider a ball of Delta ball of radius H. And let us consider like two other different balls of radius X + 1 and H -- 1.",
                    "label": 0
                },
                {
                    "sent": "Now the objective what it is trying to do is basically pulling the points which are the same classes XI into the ball of radius X -- 1 at the same time it tries to push the dip points which are of different classes XA into outside the ball H + 1.",
                    "label": 0
                },
                {
                    "sent": "So basically this word.",
                    "label": 0
                },
                {
                    "sent": "The objective function in the previous.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient is doing so.",
                    "label": 0
                },
                {
                    "sent": "So once OK.",
                    "label": 0
                },
                {
                    "sent": "So this is what I've shown where it tries to adjust your embedding function fee and then the radius of the ball hitch in such a way that more number of points of the same class actually it enters inside this ball of radius X -- 1.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now at some point this.",
                    "label": 0
                },
                {
                    "sent": "What it does is.",
                    "label": 0
                },
                {
                    "sent": "Once this point XI, it gets the required number of points such that it can be classified correctly.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't try to bring anymore points of the same class inside to the inside.",
                    "label": 0
                },
                {
                    "sent": "This ball of radius such minus one at the same time, it doesn't try to push the points of opposite class outside the ball of H + 1, so this is what they.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The outer hinge allows the outer hinge function that without a hitch function comes in.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we.",
                    "label": 0
                },
                {
                    "sent": "So we saw this situation where if I choose my embedding function to be the class of most of Colonel Maps, then it does not help you in solving this supervised setting problem.",
                    "label": 0
                },
                {
                    "sent": "So we consider fee to be in architectures where I consider fee to be a multivariate function where we have like 3 one to 3D which are which are real valued functions and each fear you belong to an architecture such a who's reproducing kernel scale and choosing a regularizer of this form.",
                    "label": 0
                },
                {
                    "sent": "By invoking the standard represent a theorem will get the representation of each of these functions to be of this form, and then you have a constraint on the coefficients of this of this function.",
                    "label": 0
                },
                {
                    "sent": "Earn intuitively one can see how these can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's come up.",
                    "label": 0
                },
                {
                    "sent": "So in the objective here you can see that this distance since these are distance between Phoenix and Phoenix J, which is invariant to translation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So which actually it ends up in a constraint on the coefficients of these of the kernel functions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we choose these, these different kernel functions to be the single function, then you can show the distance between these embeddings is equivalent to computing the mob is distance between the empirical kernel Maps.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if I choose my domain to be already, and if I choose my kernel to be a linear kernel, then this distance is equivalent to computing the model of this distance between the points in the original space.",
                    "label": 0
                },
                {
                    "sent": "So so this result is interesting.",
                    "label": 0
                },
                {
                    "sent": "'cause now let's say I choose my Spartan window rule where I'm not using this naive kernel.",
                    "label": 0
                },
                {
                    "sent": "But let's say I'm using a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "This means already.",
                    "label": 0
                },
                {
                    "sent": "I'm my rule is something like an arcade, just basic rule, and probably you don't want to do again again, the second level of embedding into our cages.",
                    "label": 0
                },
                {
                    "sent": "So in such a case you can.",
                    "label": 0
                },
                {
                    "sent": "You can simply use.",
                    "label": 0
                },
                {
                    "sent": "A linear kernel, which is actually amounts to computing the somehow capturing the covariance structure of the data which lies in your input space.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is equal to D. So once we have this representation of fee, we basically use that and plug it back into the into this into into our program and then we have.",
                    "label": 0
                },
                {
                    "sent": "We have this program.",
                    "label": 0
                },
                {
                    "sent": "I mean at the first instance, if you see I mean this program doesn't seem to be.",
                    "label": 0
                },
                {
                    "sent": "Actually it seems to be convex, so this part of the program is for sure it's convex.",
                    "label": 0
                },
                {
                    "sent": "But here this parameter to AJ.",
                    "label": 0
                },
                {
                    "sent": "Actually it makes the problem nonconvex, because Tower J can take values.",
                    "label": 0
                },
                {
                    "sent": "Plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So since since when it is plus this function is convex in C, But when it is minus, so you have, you can write your, then you're writing this as like a difference of two convex functions, which is hard problem to solve.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, we can do semidefinite relaxation by writing or Sigma as C, transpose C, and here and then neglecting the neglecting the rank constraint on our Sigma, which results in this semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "OK, so they the algorithm is as follows.",
                    "label": 0
                },
                {
                    "sent": "We don't want to use.",
                    "label": 0
                },
                {
                    "sent": "We don't want to use any standard solvers to solve this problem, so we use this trick proposed by Weinberger and solve in their large margin nearest neighbor paper.",
                    "label": 0
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "We basically find.",
                    "label": 0
                },
                {
                    "sent": "When are these hinge functions active?",
                    "label": 0
                },
                {
                    "sent": "Which means when is this function greater than zero and when is this whole thing greater than zero?",
                    "label": 0
                },
                {
                    "sent": "So you need to find the set of all AJ for which you're this part of the object to is actually greater than 0.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So which we?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I denote by script A, so once I find what script I is then the program basically reduces to.",
                    "label": 0
                },
                {
                    "sent": "Reduces to this form, where now I can alternately minimize between Sigma and H.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you're minimizing or Sigma, so you have if you don't have this constraint and when you're minimizing or Sigma then.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here you have, you have basically an intersection of two convex sets, and the idea is you need to minimize this objective over the intersection of these two convex sets, and you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't get a straightforward rule, so you need to alternately projected you need to solve that objective and then projected onto this convex sets, and then you need to do it iteratively together.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm just follows where this part of the algorithm basically finds which are which.",
                    "label": 0
                },
                {
                    "sent": "Is find the script here, which is your active set and once you know the active said we do a, we do a gradient descent and then we do this projection operation onto this onto the intersection of these two convex sets, which gives the next update of our Sigma.",
                    "label": 0
                },
                {
                    "sent": "And then we do the same thing for this parameter H and you keep iterating until until.",
                    "label": 0
                },
                {
                    "sent": "Until this two parameter.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Converge.",
                    "label": 0
                },
                {
                    "sent": "OK, before I get to the results, I'll show a small demo.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the wine data set from the user database.",
                    "label": 0
                },
                {
                    "sent": "So this data set is 178 points which are in 13 dimensional space and this picture is basically the that is basically projecting on to the first 2 eigenvectors of the covariance matrix of this data.",
                    "label": 0
                },
                {
                    "sent": "So basically the PCA basis.",
                    "label": 0
                },
                {
                    "sent": "And you can see basically there are.",
                    "label": 0
                },
                {
                    "sent": "I mean if I'm doing the classification on that on the training data itself.",
                    "label": 0
                },
                {
                    "sent": "So this is the error that is associated with the with this classification and this and this calculation is done in 30 dimensions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now this is your active set matrix where you can see that if you.",
                    "label": 0
                },
                {
                    "sent": "Each of these active rows.",
                    "label": 0
                },
                {
                    "sent": "Basically these are the ones where the outer hinge function is active.",
                    "label": 0
                },
                {
                    "sent": "And in each row, if there are empty blocks, which means in those functions, the inner hinge function is inactive.",
                    "label": 0
                },
                {
                    "sent": "So which means those points do not come into picture.",
                    "label": 0
                },
                {
                    "sent": "And so you can actually see the.",
                    "label": 0
                },
                {
                    "sent": "The points which are almost like near the near the boundary of each of these clusters.",
                    "label": 0
                },
                {
                    "sent": "Basically they are the points which are.",
                    "label": 0
                },
                {
                    "sent": "Yep, so which are basically sufficient to describe the clusters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we ran these experiments on five USA datasets and we compared it to the simple K nearest neighbor and these are the.",
                    "label": 0
                },
                {
                    "sent": "And and these are the other key nearest neighbor metric learning algorithms, and this is our method with the Gaussian kernel and the linear kernel.",
                    "label": 0
                },
                {
                    "sent": "And here you can see.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That our method works better than the other methods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in some cases, in fact the linear kernel it does better than even the Gaussian kernel.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to quickly summarize, basically we proposed a method to embed the metric space into space by for this kernel classification rule, and we also learn the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deposit window and the advantage of our method over over some of them.",
                    "label": 1
                },
                {
                    "sent": "Some other metric learning methods is that this method, this element and actually requires the target neighbors to be defined our priority so that now you can somehow find the distance between the points to learn your metric.",
                    "label": 1
                },
                {
                    "sent": "So such.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definition is not required for our method and our method involves fewer tuning parameters compared to other meta.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning methods.",
                    "label": 0
                },
                {
                    "sent": "We feel this.",
                    "label": 0
                },
                {
                    "sent": "This work actually provides a unified and a formal treatment for solving these metric learning problems compared to other heuristics.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One main issue of not only our method, but all these metric learning methods is that they solve some different program and these programs are very expensive, very intensive computation intensive.",
                    "label": 0
                },
                {
                    "sent": "So the future work.",
                    "label": 0
                },
                {
                    "sent": "Basically it involves speeding up these methods.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "While it's taking questions, the next speaker could come up.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's sorry it's large margin.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "No, I mean.",
                    "label": 0
                },
                {
                    "sent": "No, I mean you you basically you.",
                    "label": 0
                },
                {
                    "sent": "Basically you need to predefine what your neighbors are going to be, and then you want to learn a metric such that the points you want to bring the points of same class close to those neighbors so that points get slightly classified.",
                    "label": 0
                }
            ]
        }
    }
}