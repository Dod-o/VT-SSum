{
    "id": "3dcwsamztozgcedxuo6loei75uitiqiq",
    "title": "Fast joint segmentation of multiple array CGH profiles for detecting frequent copy number variations",
    "info": {
        "author": [
            "Kevin Bleakley, MINES ParisTech"
        ],
        "published": "Oct. 11, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Biology->Genetics",
            "Top->Medicine"
        ]
    },
    "url": "http://videolectures.net/cancerbioinformatics2010_bleakley_fsma/",
    "segmentation": [
        [
            "Project is to take a group of patients with the same cancer and to find regions in which commonly their copy number profile is as amplifications or deletions.",
            "So that's the idea behind."
        ],
        [
            "So I'll go briefly through the biology because I think if you want an expert on copy numbers before this conference, you probably are now, then I'll do some math, show some simulations, and a little bit on some real data."
        ],
        [
            "So I think we can fairly quickly pass through that slide."
        ],
        [
            "DNA.",
            "So I guess what we're kind of thinking of the biological hypothesis is that we in regions of amplification we expect to hopefully find oncogenes and then regions where this deletion we expect that we hope that there will be some relationship between genes that could have suppressed the tumor."
        ],
        [
            "Anyway, so in theory, in a perfect world, what we would hope to expect was that a copy number profile would look like this.",
            "You would have a normal region and then original amplification, normal deletion, so very nice and piecewise constant.",
            "And of course every."
        ],
        [
            "Ality it looks more like that and our goal is to take that which is the same data.",
            "I just added some noise to it and read."
        ],
        [
            "Instructs hopefully what was there, but."
        ],
        [
            "So.",
            "Trying to reconstruct a piecewise constant signal to identify gains and losses, and this is not exactly a new topic.",
            "A lot of people have tried to do this and various different ways you have.",
            "Hmm, Hmm's, you have methods which use dynamic programming and what I work in more and was mentioned slightly in the last talk is sparse methods."
        ],
        [
            "So quickly explain what you can do with dynamic programming.",
            "For example, you might want to say, let's assume that there are KK breakpoints and where should we put them along a genome with P probes so that they optimize some criteria.",
            "So if you want to do this, you have P choose K possibilities for placing these these breakpoints and so originally there's too hard, but you can use dynamic programming to to break this down to.",
            "P P2P squared.",
            "Now the problem is that today we have very high dimensional data and so B squared.",
            "When you're dealing with 1 million probes is is a little bit too much."
        ],
        [
            "So another class of methods, which is what I work in is like sparsity based methods.",
            "So trying if you don't know these, hopefully this will actually help you to understand a little bit.",
            "It's not too high tech.",
            "So one of these methods is called the Lasso, so you're trying to.",
            "Minimize yet, so your vector of responses.",
            "This is a matrix with your variables and you have some some parameter beta.",
            "And so you want to do this minimization and you put a constraint on the beaters so you make sure that their sum is is less than some Samuel.",
            "And if you start new at zero and you slowly make community bigger and bigger and bigger, what does actually turns out to do this formulation is to bring in the variables, so debaters the coefficients into the problem one at a time.",
            "They will start at zero and the lesser would bring one in.",
            "Then bring the second line and bring a third one and sometimes it takes one out as well.",
            "And so it's a way of solving your minimization problem in a sense, but it's also a way of selecting variables a small number of variables, which is useful if we have a million and we want to find 10, for example."
        ],
        [
            "So another method is called loud that's least angle regression.",
            "So you have the same objects and what you do with laws is that you just start and you find the variable which correlates most with your response.",
            "Why and all you do then with that variable variable you take beta I which was zero and you raise it up.",
            "You keep raising it up until Y minus XI beta.",
            "I becomes equally correlated with your original XI Victor available variable and a new variable.",
            "At this moment you bring in beta J up from zero so you bring in the variables one at a time, up from zero.",
            "So that's interesting."
        ],
        [
            "About the laws within geometric kind of impression of what we're doing.",
            "So what's nice is that if you do a tiny modification to Lars, it turns out and it was quite a surprise to the authors when they."
        ],
        [
            "Did it was that it turns out that you can solve the lesser this minimization problem."
        ],
        [
            "Using almost just this last technique, which is very fast and."
        ],
        [
            "Simple.",
            "So one strategy people use to solve to solve the lasso is to just use this last modification, which is very fast to program."
        ],
        [
            "So let's try and show how we can move from just this basic, lesser inliers to do stuff with copy numbers.",
            "So if we say why is just the value of all the probes along the genome?",
            "And if we say X is just the identity, then what we end up doing is minimizing differences between the road values and the beaters.",
            "With this constraint on, the sum of the beaters.",
            "The problem with this is that brings in variables one at a time and it brings in for example variable 7 and then it brings in variable 30 and then variable 6 million and it doesn't in the end generate what we're looking for behind this, which is an approximation of the signal which is piecewise constant.",
            "So it's in its native form.",
            "It's not very useful."
        ],
        [
            "So what you can do is scored the fuse letter, which is where you add a second constraint.",
            "Into the formulation and what it does is it penalizes the difference between sub between consecutive betas along the genome.",
            "So what it turns out this does is it forces.",
            "Most coefficient, most consecutive beaters to be the same, so that the sum is often 0.",
            "And it turns out that this has the effect of actually generating a piecewise constant approximation of the signal, because you have whole run of beaches which are the same number, and then it jumps, I'll run the same number then it.",
            "The trouble is that this takes a lot longer to run on a computer then then we would like."
        ],
        [
            "So what chalion living addicted was they said, let's just get rid of this.",
            "Some constraint just keep the keep the second constraint and it was interesting to do this because if you then just do a very very simple change of variable, you are into the beta.",
            "You end up with something which if you've been following so far, it's actually it turns it back into a lasso.",
            "Simple, normal, lesser just with the X becomes a special form.",
            "So we still doing a few things we can still use it for copy number of stuff, but because it's now in the lasso formulation we can use laws which is fast with a slight modification to run lesson.",
            "So we've got back to something which is fast and easy."
        ],
        [
            "At the same time, for example, laws which the R package which exists.",
            "For our it has this door.",
            "Normally this P by P matrix.",
            "So if P is like a million, you still still not going to happen, but they were able to show how Joey that because of this very special.",
            "Shape of eggs.",
            "It's very simple that you don't have to store this matrix and you can just work with can cumulative sums and it's very very fast so it moves from P squared down to PK.",
            "If you're finding K breakpoints."
        ],
        [
            "So maybe tomorrow, but towards what I've done is done for you there.",
            "So our motivation is that we have a whole set of patients with the same cancer, and we expect that at least to some extent they will share similar regions of amplification or deletion.",
            "At the same time, yeah, so they really all share this.",
            "For example, is a deletion, deletion, deletion, but there doesn't seem to be a deletion.",
            "And.",
            "Various things like that so to to stop the first question, which I always get when I show this project.",
            "I'm not going to assume that in reality everyone has the same breakpoints in the same regions, but by formalizing it in that way we can get some good results and practically it doesn't."
        ],
        [
            "Did too much.",
            "So without question is just a segment profiles into piecewise constant objects.",
            "If we have a lot of profiles, we can either do them one at a time and then look where the intersections of amplification regions are or deletion regions are.",
            "Or we can try and segment them all at the same time jointly.",
            "So as I said, this will force them all to have breakpoints in the same places, which biologically speaking, is an extreme exaggeration.",
            "But in practice this would actually be."
        ],
        [
            "So it's a simple generalization of the work of of how cherry and living in Duke's.",
            "That just why now becomes a matrix where each each column is 1 profile along the genome and beta becomes a matrix as well.",
            "And all we do is we penalize the columns of data with this kind of formulation.",
            "What this ends up doing is making the difference between consecutive columns, sparse and So what in practice does that means that all the profiles have certain value along here and then they all jump?",
            "And then they will jump altogether."
        ],
        [
            "And if we do the same change of variable as before.",
            "Turns into something which looks about horrible, but which is exactly called a group lasso.",
            "It's a known object, and it's an object which is used to select variables in groups and it just turns out that what we've ended up doing is to select variables and groups which are shared breakpoint, a place where all the profiles break at the same time."
        ],
        [
            "So I was introduced the group class over in 2006 at the same time as the group Flowers, which is kind of helpful because the group lasso in order to make it run its water faster.",
            "Methods now, but it tends to be fairly slow with using convex optimization and so forth, whereas the group laws, which is kind of a generalization of the Flowers.",
            "Can still be made to run very, very fast, so we can.",
            "We can make it run in KNP number of number of breakpoints, number of profiles, number of probes.",
            "And we can we can do it faster than most.",
            "Now this is about about a year ago, I think, but you can have 20 profiles.",
            "The million probes, excuse me.",
            "And you can find the first break point in six seconds, a second one in six seconds at one, 6 seconds.",
            "So it's just a couple of minutes."
        ],
        [
            "So the question is, does this formulation actually do what we wanted to do?",
            "So here is just some simulation stuff where you have low to high noise, 10 break points.",
            "You know where the breakpoints are and you just ask the algorithm.",
            "Give it 1 profile.",
            "Does it find the 10 first breakpoints first?",
            "If it does, we stop.",
            "Otherwise add a second second simulated profile.",
            "Ask it to jointly segment them.",
            "If it finds a 1010 correct breakpoints, stop.",
            "Otherwise, at a third and a fourth and 5th, so you keep adding new profiles until the algorithm finds the 10 real breakpoints so."
        ],
        [
            "These are histograms of the results starting from low noise up to higher noise, so it's a histogram of the number of profiles you needed to add so that the algorithm found the 10 real breakpoints so you can see for the low noise conditions you really need more than 20 or 30 when you get to more realistic noise levels.",
            "Here, the more realistic numbers of profiles that you'll need, some more like 50 or 100 or so forth."
        ],
        [
            "So we have a theorem theoretical results on that as well, which show that as you increase the.",
            "As you increase the number of profiles but keep the length of the profiles fixed, then with probability one the algorithm will find the correct breakpoint.",
            "So it's different from the previous kind of theoretical results where what you do is you add more and more probes.",
            "So it's just a practical example of what it can do.",
            "So here is we did joint segmentation of about 50 blood profiles and this is the result of taking all of the results and throwing the piecewise constant approximations all onto the same axis.",
            "So visually at least this gives you a start where you can see regions which are commonly amplified, for example, and commonly commonly deleted.",
            "And just to show you that the fact that we force all of them to break in the same places doesn't mean that they look the same.",
            "So these are two different single individual profiles with the original data and yellow and their smoothings.",
            "So they all have the same breakpoints.",
            "But you see that if one of the profiles doesn't really have a break point there, it just makes a tiny break point which doesn't doesn't affect us in any way."
        ],
        [
            "Something conclusions and future work, so it's this is a very fast type of algorithm.",
            "We've got faster ways to do it now, but it's already just takes a few minutes for practical, practical studies.",
            "It does what it should.",
            "Theoretically it does find the real breakpoints.",
            "And we're working on next generation stuff for finding breakpoints and regions, as was a bit mentioned yesterday with a different algorithm.",
            "And hopefully yes, make it faster and faster.",
            "So that's all, thanks.",
            "Questions I gotta question.",
            "Is it possible to not just get discrete values but maybe get a posterior probability for the segmented value?",
            "So I just want to probe 2500 is a copy number of four with 80% chance and three with a 15% translates to mean.",
            "Somewhere in your.",
            "I'm talking about individual profiles that you just look at an individual copy number variation profile.",
            "If you can get posterior probabilities of being in certain amplification state at a certain point on the on the ship, I guess if the segmentation is work correctly the algorithm then it should look fairly visually.",
            "See it already and I guess the higher it is amplified with the lower it is deleted.",
            "You know that there's more copies, but you've got factors like the inherent noise in the technology and various the.",
            "That's why I'm saying that it would be nice to not have it as quick value.",
            "The unseen this is.",
            "Copy #5 or something so it would be nice, but I think I'd be lying if I could say that that's definitely copy #5 or.",
            "There's like, for example, you're dealing with tumor cells as well, so it's 30% probably coming from normal cells, and so the levels do not nicely, but not.",
            "It's not clear in any way whether that's one, that's why I'm asking, but it's possible to get the probabilities smooth and an absolute value perhaps, but it's I guess it's not exactly the.",
            "The point of what I'm trying to do here with the message people do that, yeah.",
            "Anymore questions.",
            "Yes, test.",
            "Methods.",
            "So it's there's not really any method which there are methods.",
            "For example, this method that does joint work and it finds regions but not break points.",
            "So there's a method that I know that jointly finds the breakpoints like that, so it's difficult to make direct comparisons, but in terms of what you can do at this point is create a statistic based on these values to find regions which are commonly higher, commonly low, and then compare with the other methods.",
            "So that tends to work fairly similarly.",
            "I thought there was something out there, but.",
            "Laugh.",
            "So not exactly know and the original article they said without trying to show it.",
            "They said it's very similar solution path but they just used it on the very small small data set and so in practice what we found in the last couple of months is that John Philip is gone and program the group Lasso anyway, which is much much longer and it turns out that the parser relatively different in certain moments they're much less close than Mars and letter.",
            "Good, so thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Project is to take a group of patients with the same cancer and to find regions in which commonly their copy number profile is as amplifications or deletions.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea behind.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll go briefly through the biology because I think if you want an expert on copy numbers before this conference, you probably are now, then I'll do some math, show some simulations, and a little bit on some real data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think we can fairly quickly pass through that slide.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "DNA.",
                    "label": 0
                },
                {
                    "sent": "So I guess what we're kind of thinking of the biological hypothesis is that we in regions of amplification we expect to hopefully find oncogenes and then regions where this deletion we expect that we hope that there will be some relationship between genes that could have suppressed the tumor.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so in theory, in a perfect world, what we would hope to expect was that a copy number profile would look like this.",
                    "label": 0
                },
                {
                    "sent": "You would have a normal region and then original amplification, normal deletion, so very nice and piecewise constant.",
                    "label": 0
                },
                {
                    "sent": "And of course every.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ality it looks more like that and our goal is to take that which is the same data.",
                    "label": 0
                },
                {
                    "sent": "I just added some noise to it and read.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instructs hopefully what was there, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Trying to reconstruct a piecewise constant signal to identify gains and losses, and this is not exactly a new topic.",
                    "label": 1
                },
                {
                    "sent": "A lot of people have tried to do this and various different ways you have.",
                    "label": 0
                },
                {
                    "sent": "Hmm, Hmm's, you have methods which use dynamic programming and what I work in more and was mentioned slightly in the last talk is sparse methods.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So quickly explain what you can do with dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "For example, you might want to say, let's assume that there are KK breakpoints and where should we put them along a genome with P probes so that they optimize some criteria.",
                    "label": 1
                },
                {
                    "sent": "So if you want to do this, you have P choose K possibilities for placing these these breakpoints and so originally there's too hard, but you can use dynamic programming to to break this down to.",
                    "label": 0
                },
                {
                    "sent": "P P2P squared.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that today we have very high dimensional data and so B squared.",
                    "label": 0
                },
                {
                    "sent": "When you're dealing with 1 million probes is is a little bit too much.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another class of methods, which is what I work in is like sparsity based methods.",
                    "label": 0
                },
                {
                    "sent": "So trying if you don't know these, hopefully this will actually help you to understand a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's not too high tech.",
                    "label": 0
                },
                {
                    "sent": "So one of these methods is called the Lasso, so you're trying to.",
                    "label": 0
                },
                {
                    "sent": "Minimize yet, so your vector of responses.",
                    "label": 0
                },
                {
                    "sent": "This is a matrix with your variables and you have some some parameter beta.",
                    "label": 0
                },
                {
                    "sent": "And so you want to do this minimization and you put a constraint on the beaters so you make sure that their sum is is less than some Samuel.",
                    "label": 0
                },
                {
                    "sent": "And if you start new at zero and you slowly make community bigger and bigger and bigger, what does actually turns out to do this formulation is to bring in the variables, so debaters the coefficients into the problem one at a time.",
                    "label": 0
                },
                {
                    "sent": "They will start at zero and the lesser would bring one in.",
                    "label": 0
                },
                {
                    "sent": "Then bring the second line and bring a third one and sometimes it takes one out as well.",
                    "label": 0
                },
                {
                    "sent": "And so it's a way of solving your minimization problem in a sense, but it's also a way of selecting variables a small number of variables, which is useful if we have a million and we want to find 10, for example.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another method is called loud that's least angle regression.",
                    "label": 1
                },
                {
                    "sent": "So you have the same objects and what you do with laws is that you just start and you find the variable which correlates most with your response.",
                    "label": 0
                },
                {
                    "sent": "Why and all you do then with that variable variable you take beta I which was zero and you raise it up.",
                    "label": 0
                },
                {
                    "sent": "You keep raising it up until Y minus XI beta.",
                    "label": 1
                },
                {
                    "sent": "I becomes equally correlated with your original XI Victor available variable and a new variable.",
                    "label": 0
                },
                {
                    "sent": "At this moment you bring in beta J up from zero so you bring in the variables one at a time, up from zero.",
                    "label": 0
                },
                {
                    "sent": "So that's interesting.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the laws within geometric kind of impression of what we're doing.",
                    "label": 0
                },
                {
                    "sent": "So what's nice is that if you do a tiny modification to Lars, it turns out and it was quite a surprise to the authors when they.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did it was that it turns out that you can solve the lesser this minimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using almost just this last technique, which is very fast and.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple.",
                    "label": 0
                },
                {
                    "sent": "So one strategy people use to solve to solve the lasso is to just use this last modification, which is very fast to program.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's try and show how we can move from just this basic, lesser inliers to do stuff with copy numbers.",
                    "label": 0
                },
                {
                    "sent": "So if we say why is just the value of all the probes along the genome?",
                    "label": 1
                },
                {
                    "sent": "And if we say X is just the identity, then what we end up doing is minimizing differences between the road values and the beaters.",
                    "label": 0
                },
                {
                    "sent": "With this constraint on, the sum of the beaters.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is that brings in variables one at a time and it brings in for example variable 7 and then it brings in variable 30 and then variable 6 million and it doesn't in the end generate what we're looking for behind this, which is an approximation of the signal which is piecewise constant.",
                    "label": 0
                },
                {
                    "sent": "So it's in its native form.",
                    "label": 0
                },
                {
                    "sent": "It's not very useful.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what you can do is scored the fuse letter, which is where you add a second constraint.",
                    "label": 0
                },
                {
                    "sent": "Into the formulation and what it does is it penalizes the difference between sub between consecutive betas along the genome.",
                    "label": 1
                },
                {
                    "sent": "So what it turns out this does is it forces.",
                    "label": 0
                },
                {
                    "sent": "Most coefficient, most consecutive beaters to be the same, so that the sum is often 0.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this has the effect of actually generating a piecewise constant approximation of the signal, because you have whole run of beaches which are the same number, and then it jumps, I'll run the same number then it.",
                    "label": 0
                },
                {
                    "sent": "The trouble is that this takes a lot longer to run on a computer then then we would like.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what chalion living addicted was they said, let's just get rid of this.",
                    "label": 0
                },
                {
                    "sent": "Some constraint just keep the keep the second constraint and it was interesting to do this because if you then just do a very very simple change of variable, you are into the beta.",
                    "label": 0
                },
                {
                    "sent": "You end up with something which if you've been following so far, it's actually it turns it back into a lasso.",
                    "label": 0
                },
                {
                    "sent": "Simple, normal, lesser just with the X becomes a special form.",
                    "label": 0
                },
                {
                    "sent": "So we still doing a few things we can still use it for copy number of stuff, but because it's now in the lasso formulation we can use laws which is fast with a slight modification to run lesson.",
                    "label": 0
                },
                {
                    "sent": "So we've got back to something which is fast and easy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the same time, for example, laws which the R package which exists.",
                    "label": 0
                },
                {
                    "sent": "For our it has this door.",
                    "label": 0
                },
                {
                    "sent": "Normally this P by P matrix.",
                    "label": 0
                },
                {
                    "sent": "So if P is like a million, you still still not going to happen, but they were able to show how Joey that because of this very special.",
                    "label": 0
                },
                {
                    "sent": "Shape of eggs.",
                    "label": 0
                },
                {
                    "sent": "It's very simple that you don't have to store this matrix and you can just work with can cumulative sums and it's very very fast so it moves from P squared down to PK.",
                    "label": 0
                },
                {
                    "sent": "If you're finding K breakpoints.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe tomorrow, but towards what I've done is done for you there.",
                    "label": 0
                },
                {
                    "sent": "So our motivation is that we have a whole set of patients with the same cancer, and we expect that at least to some extent they will share similar regions of amplification or deletion.",
                    "label": 1
                },
                {
                    "sent": "At the same time, yeah, so they really all share this.",
                    "label": 0
                },
                {
                    "sent": "For example, is a deletion, deletion, deletion, but there doesn't seem to be a deletion.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Various things like that so to to stop the first question, which I always get when I show this project.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to assume that in reality everyone has the same breakpoints in the same regions, but by formalizing it in that way we can get some good results and practically it doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did too much.",
                    "label": 0
                },
                {
                    "sent": "So without question is just a segment profiles into piecewise constant objects.",
                    "label": 0
                },
                {
                    "sent": "If we have a lot of profiles, we can either do them one at a time and then look where the intersections of amplification regions are or deletion regions are.",
                    "label": 0
                },
                {
                    "sent": "Or we can try and segment them all at the same time jointly.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this will force them all to have breakpoints in the same places, which biologically speaking, is an extreme exaggeration.",
                    "label": 0
                },
                {
                    "sent": "But in practice this would actually be.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a simple generalization of the work of of how cherry and living in Duke's.",
                    "label": 0
                },
                {
                    "sent": "That just why now becomes a matrix where each each column is 1 profile along the genome and beta becomes a matrix as well.",
                    "label": 0
                },
                {
                    "sent": "And all we do is we penalize the columns of data with this kind of formulation.",
                    "label": 0
                },
                {
                    "sent": "What this ends up doing is making the difference between consecutive columns, sparse and So what in practice does that means that all the profiles have certain value along here and then they all jump?",
                    "label": 0
                },
                {
                    "sent": "And then they will jump altogether.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we do the same change of variable as before.",
                    "label": 1
                },
                {
                    "sent": "Turns into something which looks about horrible, but which is exactly called a group lasso.",
                    "label": 1
                },
                {
                    "sent": "It's a known object, and it's an object which is used to select variables in groups and it just turns out that what we've ended up doing is to select variables and groups which are shared breakpoint, a place where all the profiles break at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I was introduced the group class over in 2006 at the same time as the group Flowers, which is kind of helpful because the group lasso in order to make it run its water faster.",
                    "label": 0
                },
                {
                    "sent": "Methods now, but it tends to be fairly slow with using convex optimization and so forth, whereas the group laws, which is kind of a generalization of the Flowers.",
                    "label": 0
                },
                {
                    "sent": "Can still be made to run very, very fast, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can make it run in KNP number of number of breakpoints, number of profiles, number of probes.",
                    "label": 0
                },
                {
                    "sent": "And we can we can do it faster than most.",
                    "label": 0
                },
                {
                    "sent": "Now this is about about a year ago, I think, but you can have 20 profiles.",
                    "label": 0
                },
                {
                    "sent": "The million probes, excuse me.",
                    "label": 0
                },
                {
                    "sent": "And you can find the first break point in six seconds, a second one in six seconds at one, 6 seconds.",
                    "label": 0
                },
                {
                    "sent": "So it's just a couple of minutes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, does this formulation actually do what we wanted to do?",
                    "label": 0
                },
                {
                    "sent": "So here is just some simulation stuff where you have low to high noise, 10 break points.",
                    "label": 0
                },
                {
                    "sent": "You know where the breakpoints are and you just ask the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Give it 1 profile.",
                    "label": 0
                },
                {
                    "sent": "Does it find the 10 first breakpoints first?",
                    "label": 0
                },
                {
                    "sent": "If it does, we stop.",
                    "label": 0
                },
                {
                    "sent": "Otherwise add a second second simulated profile.",
                    "label": 0
                },
                {
                    "sent": "Ask it to jointly segment them.",
                    "label": 0
                },
                {
                    "sent": "If it finds a 1010 correct breakpoints, stop.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, at a third and a fourth and 5th, so you keep adding new profiles until the algorithm finds the 10 real breakpoints so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are histograms of the results starting from low noise up to higher noise, so it's a histogram of the number of profiles you needed to add so that the algorithm found the 10 real breakpoints so you can see for the low noise conditions you really need more than 20 or 30 when you get to more realistic noise levels.",
                    "label": 0
                },
                {
                    "sent": "Here, the more realistic numbers of profiles that you'll need, some more like 50 or 100 or so forth.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a theorem theoretical results on that as well, which show that as you increase the.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of profiles but keep the length of the profiles fixed, then with probability one the algorithm will find the correct breakpoint.",
                    "label": 0
                },
                {
                    "sent": "So it's different from the previous kind of theoretical results where what you do is you add more and more probes.",
                    "label": 0
                },
                {
                    "sent": "So it's just a practical example of what it can do.",
                    "label": 0
                },
                {
                    "sent": "So here is we did joint segmentation of about 50 blood profiles and this is the result of taking all of the results and throwing the piecewise constant approximations all onto the same axis.",
                    "label": 0
                },
                {
                    "sent": "So visually at least this gives you a start where you can see regions which are commonly amplified, for example, and commonly commonly deleted.",
                    "label": 0
                },
                {
                    "sent": "And just to show you that the fact that we force all of them to break in the same places doesn't mean that they look the same.",
                    "label": 0
                },
                {
                    "sent": "So these are two different single individual profiles with the original data and yellow and their smoothings.",
                    "label": 0
                },
                {
                    "sent": "So they all have the same breakpoints.",
                    "label": 0
                },
                {
                    "sent": "But you see that if one of the profiles doesn't really have a break point there, it just makes a tiny break point which doesn't doesn't affect us in any way.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something conclusions and future work, so it's this is a very fast type of algorithm.",
                    "label": 1
                },
                {
                    "sent": "We've got faster ways to do it now, but it's already just takes a few minutes for practical, practical studies.",
                    "label": 0
                },
                {
                    "sent": "It does what it should.",
                    "label": 1
                },
                {
                    "sent": "Theoretically it does find the real breakpoints.",
                    "label": 0
                },
                {
                    "sent": "And we're working on next generation stuff for finding breakpoints and regions, as was a bit mentioned yesterday with a different algorithm.",
                    "label": 0
                },
                {
                    "sent": "And hopefully yes, make it faster and faster.",
                    "label": 0
                },
                {
                    "sent": "So that's all, thanks.",
                    "label": 0
                },
                {
                    "sent": "Questions I gotta question.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to not just get discrete values but maybe get a posterior probability for the segmented value?",
                    "label": 0
                },
                {
                    "sent": "So I just want to probe 2500 is a copy number of four with 80% chance and three with a 15% translates to mean.",
                    "label": 0
                },
                {
                    "sent": "Somewhere in your.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about individual profiles that you just look at an individual copy number variation profile.",
                    "label": 0
                },
                {
                    "sent": "If you can get posterior probabilities of being in certain amplification state at a certain point on the on the ship, I guess if the segmentation is work correctly the algorithm then it should look fairly visually.",
                    "label": 0
                },
                {
                    "sent": "See it already and I guess the higher it is amplified with the lower it is deleted.",
                    "label": 0
                },
                {
                    "sent": "You know that there's more copies, but you've got factors like the inherent noise in the technology and various the.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm saying that it would be nice to not have it as quick value.",
                    "label": 0
                },
                {
                    "sent": "The unseen this is.",
                    "label": 0
                },
                {
                    "sent": "Copy #5 or something so it would be nice, but I think I'd be lying if I could say that that's definitely copy #5 or.",
                    "label": 0
                },
                {
                    "sent": "There's like, for example, you're dealing with tumor cells as well, so it's 30% probably coming from normal cells, and so the levels do not nicely, but not.",
                    "label": 0
                },
                {
                    "sent": "It's not clear in any way whether that's one, that's why I'm asking, but it's possible to get the probabilities smooth and an absolute value perhaps, but it's I guess it's not exactly the.",
                    "label": 0
                },
                {
                    "sent": "The point of what I'm trying to do here with the message people do that, yeah.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, test.",
                    "label": 0
                },
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "So it's there's not really any method which there are methods.",
                    "label": 0
                },
                {
                    "sent": "For example, this method that does joint work and it finds regions but not break points.",
                    "label": 0
                },
                {
                    "sent": "So there's a method that I know that jointly finds the breakpoints like that, so it's difficult to make direct comparisons, but in terms of what you can do at this point is create a statistic based on these values to find regions which are commonly higher, commonly low, and then compare with the other methods.",
                    "label": 0
                },
                {
                    "sent": "So that tends to work fairly similarly.",
                    "label": 0
                },
                {
                    "sent": "I thought there was something out there, but.",
                    "label": 0
                },
                {
                    "sent": "Laugh.",
                    "label": 0
                },
                {
                    "sent": "So not exactly know and the original article they said without trying to show it.",
                    "label": 0
                },
                {
                    "sent": "They said it's very similar solution path but they just used it on the very small small data set and so in practice what we found in the last couple of months is that John Philip is gone and program the group Lasso anyway, which is much much longer and it turns out that the parser relatively different in certain moments they're much less close than Mars and letter.",
                    "label": 0
                },
                {
                    "sent": "Good, so thank you again.",
                    "label": 0
                }
            ]
        }
    }
}