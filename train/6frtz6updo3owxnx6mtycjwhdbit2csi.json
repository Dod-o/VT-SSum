{
    "id": "6frtz6updo3owxnx6mtycjwhdbit2csi",
    "title": "PAC-Bayes, Sample Compress and Kernel Methods",
    "info": {
        "author": [
            "Pascal Germain, GRAAL, D\u00e9partement d'informatique et de g\u00e9nie logiciel, Universit\u00e9 Laval"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_germain_sckm/",
    "segmentation": [
        [
            "General framework that allows us to use the back based method, the Back Bay.",
            "So we worked with kernel methods Ann for doing this.",
            "I will use a simple converse Terry.",
            "It's a word that I did with crossword idea that which is my supervisor but also several colleagues in the University where I confirm so Alex on the gas tax on that coast.",
            "Magnum Action Saga, Shannon.",
            "So here's my outline.",
            "I will be."
        ],
        [
            "And by by reviewing some elements of the simple compressed air and after I will show you how we can describe the standard support vector machine as a majority vote of sample compress classifiers which I will call the SC class SVM.",
            "And once we have the SVM described as a majority vote, we can use the back based are we to upper bound the risk of our SVM?",
            "And finally once we have that?",
            "I will present you some empirical results that we obtained by minimizing those bounds."
        ],
        [
            "So let's make it.",
            "I mean, I'm interested by the classical binary classification problem, so we will just say that we have that training set S that contains an example.",
            "Each example is denoted by said where there is an input output pair.",
            "XYX is simply vector of three attributes.",
            "And why is the label you doing minus one and plus one?",
            "And we will.",
            "We will consider that each example Z.",
            "Is there an IID according to the data?"
        ],
        [
            "Generating distribution.",
            "So the important thing to know about the simple compulsory is that it considers data dependent classifiers.",
            "Each data dependent classifier that I with all ASI classifier just for the sake of shortness is described by two important things.",
            "First, compression set as I that contains a subset of the training data.",
            "In that notation I bold means only a vector of indices that refer to.",
            "And this is the example on the training set and the SC classifier will be also described by a message string mu that contains additional information that we need to construct the classifier and it will be important that knew would be chosen amount of predefined set of our messages that we can supply with a particular compression set an in their simple compressed air.",
            "We would say that we have a reconstruction function R. When we can recover the SC classifiers with only those two important things.",
            "So there are thanks for input.",
            "The compression set at the message ring and we build the classifier."
        ],
        [
            "Now I define the risk and the empirical risk of the simple compressed classifier.",
            "In fact, the risk is the user usual definition of any kinds of risk, so it's only the probability probability that classifier edge make a mistake.",
            "On an example drawn according to the data generating distribution, the empirical risk will be mostly the rate of error on the training set, but with a slight subtility.",
            "Here we consider that a C classifier are there for.",
            "Does any error on an example belonging to is 2 compression set, so it's the single that we see here and since we have this we obtain the result that the random variable M multiplied by their paper risk with the following binomial distribution of probability.",
            "Of success are deep, which is the generalization error and this will allow us to recover the package."
        ],
        [
            "Boats.",
            "OK, now."
        ],
        [
            "I will show you how we can describe the SVM as a majority vote of SC classifiers.",
            "I will denote HS the set of all the possible SC classifiers and it just will contain all the classifiers with the following compression set and message string.",
            "So all the possible combination the compression set will contains only one example of the training set, so we will have a compression set with the with the first example of the.",
            "Of the training set, the second example, and so on, and the message string will be formed by two things.",
            "Real number between minus one and plus one and assign either plus or minus an.",
            "So as we can see we will have an infinite number of voters in our majority vote because the interval between minus one and plus one is continuous.",
            "I will come back to that later, but it will be important in our setting that we will also always have in our majority vote pairs of Boolean complement classifiers.",
            "So the SC classifiers with the minors on its message ring will always return the inverse than the classifier with a plus its message ring.",
            "So the two class virus."
        ],
        [
            "Or we disagree.",
            "And now I define the probability distribution Q among the set of our voters and more precisely I will define.",
            "I will say that UI is the probability of choosing a particular compression set in Q and given a particular compression set, I will say that 2 SI is the probability of choosing a particular message string.",
            "Therefore the probability of choosing.",
            "A particular C classifiers will be simply equal to the multiplication of the two things here and then.",
            "Finally, we will obtain that the output of the majority vote will be simply design of the expectation here.",
            "So the expectation of a few of our classifiers."
        ],
        [
            "Since our goal is intimately to use the Parkway story, we will also define a prior distribution over all the possible classifiers.",
            "Akainu pickle you're here because normally a prior distribution must be defined before seeing the data and in this simple compress framework we are dealing with data dependent classifiers, but in fact we are deafening.",
            "The priority solution over the compression set and the message string before seeing the data, and once we see the data diverse.",
            "Uh, this fire distribution over the classifiers.",
            "So here that little picture I represent.",
            "Yeah, you can see it on each line that we have the same probability of choosing any compression set, but and given a confession set, we have the same probability of choosing a message showing our classifier with a plus on its message win and a classifier with a minus on its message ring."
        ],
        [
            "Now I define two new concept and five crossword talk about this.",
            "Quickly.",
            "This morning we will define the concepts of Apostle align on a prior and posterior strongly align on a prior.",
            "So we will say that the posterior Q is aligned on the prior P when for all the pairs of the Boolean complement classifiers and the majority vote.",
            "There's some of the weight on the posterior will be equal to the sum of the weight of the two same complement classifiers in the fire.",
            "Moreover, we will see that this string, this along posterior will be also strongly align when we have this property.",
            "So for all the message, all the compression set I the difference of the weight of the two Boolean complementary classifiers and the posterior will be equal to a constant WI.",
            "And this is for all message doing so we have only N values of WI.",
            "And it would."
        ],
        [
            "Useful to restrict ourselves to this setting because all the posterior distribution will be defined by N values of WI."
        ],
        [
            "So we will obtain a posterior that looks like this, again, like in the prior.",
            "We can see that we have the same probability of choosing any compression set, but given a compression set, the probability of choosing classifiers with a plus on its message ring and a minus on its message ring will vary according to the value I and so the value of W is valuable.",
            "WI will allow us to play with the banderi."
        ],
        [
            "Now that define all this.",
            "I can explain new.",
            "What is the reconstruction function that we use in our setting?",
            "In other words, what are the outputs of the sea classifiers?",
            "So first consider similarity function key which takes 2 example and return a number between minus one or plus one chicken be Mercer kernel like we use on the SVM, but it can be any kind of function.",
            "So once we have this function, we will decide that the output of the SC classifier age with the example I on its compression set and the message Ring Sigma Plus will return either one or plus 1 + 1 if the Sigma of the message ring is lower than the value of the kernel between XI.",
            "The example on the conversion set an X the example that we want to classify.",
            "An If Sigma is greater or equal than K it the C classifier will return minus one.",
            "With this definition, we obtain finally that we can recover and the majority vote the value of the kernel, so we can see here that the WI multiplied by key is only equal to the part of the majority vote that.",
            "That implies this example I."
        ],
        [
            "Finally, we are obtained at the the majority vote of how the C classifiers given the distribution will be the same as the classification function of the SVM.",
            "So.",
            "We will have that when the WI that we have in that define our majority vote is only equal to Yi multiplied by Alpha.",
            "I divide by your normalization constant."
        ],
        [
            "So since we reach our goal to define the SVM as a majority vote of simple compressed classifiers, we will now use the Back Bay story to obtain upper bound on the risk of those SES."
        ],
        [
            "PM like you know now the stereo.",
            "Otherwise to bound the risk of the majority vote classifier by bonding what we called the Gibbs risk.",
            "So the risk of the Gibbs classifier and usually with bound on the risk of the majority vote just by using this property that the base risk the risk of the majority vote is bounded by two times the risk of the Gibbs classifier.",
            "This is a really bad choice in our setting because the Gibbs risk of our majority vote will likely be there one off, even if the majority majority vote is really good.",
            "This is like this.",
            "Because remember that each classifier that we have in all majority vote, rely only on one training example.",
            "So each base classifier is really weak."
        ],
        [
            "We want them to bound more relevant risk and for doing this we will use a method that we develop on a previous work so we can see the reference.",
            "It's a NIPS 2006 paper Francois talk about it a little bit this otherwise to bound any kind of general loss, provided that this Journal, this loss that we called Cheetah.",
            "Can be expanded by a Taylor series.",
            "I want the zero margin.",
            "So in that work we decide to which choose to bound the quadratic risk that we can see in red."
        ],
        [
            "In this in this graphic.",
            "Here the gamma is only the minimum of the parabolas, so it's an operator of the loss, and it's useful because it allow.",
            "The majority vote to have a little little risk, even if the majority vote is close to the margin in control of the Gibbs risk that we see you in.",
            "In fact, it's two times that gives you."
        ],
        [
            "That we see you in.",
            "OK, so.",
            "Giving this we obtain two telegrams that gives back based bound on the on the risk of this SVM, the first one is inspired by the theorem of katani that Francois talk about earlier and we adapt this to rent to to end our general loss functions into an set of data dependent classifiers.",
            "Size lesser equal then L. So I I I'm not showing you the proof but we can see that we have a lot of constraints that appearing that depends on the loss that we kind of lost that we use and the size of the.",
            "And the compression set.",
            "But what is interesting is that finally, since all these are constant, we obtain that minimizing the bound is equivalent to minimizing only the little equation that we see in the bottom.",
            "So see Constant C, which is another parameter multiplied by the empirical risk plus the KL regularizer."
        ],
        [
            "We also adapt the Seeger bound to obtain a newer term on the risk of the majority vote of SC classifiers, so this bound used the function this markiel, which is the bear need, diverge.",
            "The difference between two burglary distributions of probability of success, Q1P and this function care have this interesting for protein that the keel between 2:00 and P. It's equal to their kill between 1 -- 2 and 1 -- P. And more, Moreover, we specialize this diagram for the kind the case of online posterior.",
            "So remember that when we have a line posterior the the sum of the complement classifiers and the posterior and prior or equal and when we are doing this, the usual regularizer KL disappear for the from the diagram.",
            "This is a quite a picture.",
            "Result and I will explain you in the next few slides.",
            "While we're doing that.",
            "But this is interesting because in strange because it suggests that in the case and posterior we only we only need to minimize the empirical risk."
        ],
        [
            "So now I try to give you the big idea why we are the KL disappear of the in the case of a line posterior.",
            "So remember that in the usual back based motivation the KL appear when we want to bound the random variable that we see on the top.",
            "So the expectation of RP in terms of something that depends on two.",
            "So when we went.",
            "We transform the expectation of our Q over into an expectation over Q by adding the diffraction P / Q and this becomes eventually decaire after against an inequality.",
            "But in the case of a line posterior again see here that we had this operation by free will transform the expectation of RP until next mutation over Q without introducing any new new constant."
        ],
        [
            "And here I explain you why it's like this, so remember that in our majority vote we have always compliment classifiers, so if the majority vote contain the classifier age, it also contains the classifier minus age.",
            "And this gives us the the equality between the 20 ground that we send it up and.",
            "So we can see that two time the expectation that over P is equal to the sum of the twenty goal.",
            "Here we can say you mean that we use the fact that the risk of minors age is only the risk of 1 minus the risk of age, and after that we use the property of the function KL, which is that the KL between two LP gives the Cal between one minus P 1 -- Q.",
            "In one minute, speak that allow us to be.",
            "Group the two part so.",
            "We are, we use the property of online posterior after that to transform the term.",
            "PP of H + P of minus age into the term Q of H + 2 of my message and from then we are saying that the expectation of two times expectation of working.",
            "So the two things are completely quit."
        ],
        [
            "Finally, I will explain you, and in fact I will not explain you the algorithms, but I just want to show you that we design tutoring algorithms to minimize the two bounds.",
            "So remember that in the Stonia line setting or our posterior distribution is defined by N values of the value I.",
            "So the minimizing algorithm will will only play with vector.",
            "WN component varies as similarly.",
            "Then there is VM."
        ],
        [
            "And the other interesting thing is that we are pre compute the margin into a matrix that looks really like the matrix.",
            "The gram matrix that we use with Windows VM.",
            "The only thing is that we have a one Internet signal which comes from the fact that we always presume that see classifier doesn't made an error on an example belonging to his compression set."
        ],
        [
            "And the two algorithms minimize those three questions.",
            "The first one minimize the that I did the algorithm with the KL minimize the bound based on the first pack pack based around I present.",
            "So this to remap 2 paramaters C, which is the values of the catoni bound.",
            "The tradeoff between the minimalization of the empirical risk and the minimization of the regularizer.",
            "And gamma, which is the minimum of the Parable of the risk that we use.",
            "And we also had the algorithm without the KL based on the second version of the bound and this.",
            "This algorithm only minimize the quadratic quadratic risk with only one hyperparameter.",
            "To tune the minimum of the parable.",
            "And this isn't everything because we have here 2 convex function.",
            "So it's really simple to design A minimization algorithm that find global minimum of."
        ],
        [
            "Who's equations?",
            "We did the experiments to compare the two algorithms with DD SVM using the RBF kernel and we use the cross validation.",
            "In our case is to find the best hyperparameters.",
            "It's interesting to see that there's no clear winner winner between the three algorithms.",
            "But it seems that we have this system is obtained.",
            "Best result that the classic SVM but the results are there for significance and the other interesting thing to say is that we attend to be resolved with the two version of the algorithm, the one without the regularizer and with the regularizer.",
            "And it's very particular because.",
            "We don't have the regret and we have one eye Parameterless studio."
        ],
        [
            "So finally I will just just conclude by saying that I."
        ],
        [
            "Present a general framework that otherwise to apply the PAC Bayes bounds Dippach based are we to the kernel methods and for now I just compare the result of this framework with this VM.",
            "But this framework is more general and we have many other ideas.",
            "The.",
            "Twice so among them there is the idea of doing experimentation with nonpositive, some indefinite kernel, because because we are not limited to.",
            "We can also think to consider a majority vote of SC classifiers with more than one example on the compression set, and we are beginning to think also to consider prior that are not strong line, so we will have something much more general, that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "General framework that allows us to use the back based method, the Back Bay.",
                    "label": 0
                },
                {
                    "sent": "So we worked with kernel methods Ann for doing this.",
                    "label": 0
                },
                {
                    "sent": "I will use a simple converse Terry.",
                    "label": 0
                },
                {
                    "sent": "It's a word that I did with crossword idea that which is my supervisor but also several colleagues in the University where I confirm so Alex on the gas tax on that coast.",
                    "label": 0
                },
                {
                    "sent": "Magnum Action Saga, Shannon.",
                    "label": 0
                },
                {
                    "sent": "So here's my outline.",
                    "label": 0
                },
                {
                    "sent": "I will be.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And by by reviewing some elements of the simple compressed air and after I will show you how we can describe the standard support vector machine as a majority vote of sample compress classifiers which I will call the SC class SVM.",
                    "label": 1
                },
                {
                    "sent": "And once we have the SVM described as a majority vote, we can use the back based are we to upper bound the risk of our SVM?",
                    "label": 0
                },
                {
                    "sent": "And finally once we have that?",
                    "label": 0
                },
                {
                    "sent": "I will present you some empirical results that we obtained by minimizing those bounds.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's make it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm interested by the classical binary classification problem, so we will just say that we have that training set S that contains an example.",
                    "label": 1
                },
                {
                    "sent": "Each example is denoted by said where there is an input output pair.",
                    "label": 0
                },
                {
                    "sent": "XYX is simply vector of three attributes.",
                    "label": 0
                },
                {
                    "sent": "And why is the label you doing minus one and plus one?",
                    "label": 0
                },
                {
                    "sent": "And we will.",
                    "label": 0
                },
                {
                    "sent": "We will consider that each example Z.",
                    "label": 1
                },
                {
                    "sent": "Is there an IID according to the data?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So the important thing to know about the simple compulsory is that it considers data dependent classifiers.",
                    "label": 0
                },
                {
                    "sent": "Each data dependent classifier that I with all ASI classifier just for the sake of shortness is described by two important things.",
                    "label": 0
                },
                {
                    "sent": "First, compression set as I that contains a subset of the training data.",
                    "label": 1
                },
                {
                    "sent": "In that notation I bold means only a vector of indices that refer to.",
                    "label": 1
                },
                {
                    "sent": "And this is the example on the training set and the SC classifier will be also described by a message string mu that contains additional information that we need to construct the classifier and it will be important that knew would be chosen amount of predefined set of our messages that we can supply with a particular compression set an in their simple compressed air.",
                    "label": 1
                },
                {
                    "sent": "We would say that we have a reconstruction function R. When we can recover the SC classifiers with only those two important things.",
                    "label": 0
                },
                {
                    "sent": "So there are thanks for input.",
                    "label": 0
                },
                {
                    "sent": "The compression set at the message ring and we build the classifier.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I define the risk and the empirical risk of the simple compressed classifier.",
                    "label": 1
                },
                {
                    "sent": "In fact, the risk is the user usual definition of any kinds of risk, so it's only the probability probability that classifier edge make a mistake.",
                    "label": 0
                },
                {
                    "sent": "On an example drawn according to the data generating distribution, the empirical risk will be mostly the rate of error on the training set, but with a slight subtility.",
                    "label": 1
                },
                {
                    "sent": "Here we consider that a C classifier are there for.",
                    "label": 1
                },
                {
                    "sent": "Does any error on an example belonging to is 2 compression set, so it's the single that we see here and since we have this we obtain the result that the random variable M multiplied by their paper risk with the following binomial distribution of probability.",
                    "label": 0
                },
                {
                    "sent": "Of success are deep, which is the generalization error and this will allow us to recover the package.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Boats.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will show you how we can describe the SVM as a majority vote of SC classifiers.",
                    "label": 1
                },
                {
                    "sent": "I will denote HS the set of all the possible SC classifiers and it just will contain all the classifiers with the following compression set and message string.",
                    "label": 0
                },
                {
                    "sent": "So all the possible combination the compression set will contains only one example of the training set, so we will have a compression set with the with the first example of the.",
                    "label": 0
                },
                {
                    "sent": "Of the training set, the second example, and so on, and the message string will be formed by two things.",
                    "label": 0
                },
                {
                    "sent": "Real number between minus one and plus one and assign either plus or minus an.",
                    "label": 0
                },
                {
                    "sent": "So as we can see we will have an infinite number of voters in our majority vote because the interval between minus one and plus one is continuous.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that later, but it will be important in our setting that we will also always have in our majority vote pairs of Boolean complement classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the SC classifiers with the minors on its message ring will always return the inverse than the classifier with a plus its message ring.",
                    "label": 0
                },
                {
                    "sent": "So the two class virus.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or we disagree.",
                    "label": 0
                },
                {
                    "sent": "And now I define the probability distribution Q among the set of our voters and more precisely I will define.",
                    "label": 1
                },
                {
                    "sent": "I will say that UI is the probability of choosing a particular compression set in Q and given a particular compression set, I will say that 2 SI is the probability of choosing a particular message string.",
                    "label": 0
                },
                {
                    "sent": "Therefore the probability of choosing.",
                    "label": 1
                },
                {
                    "sent": "A particular C classifiers will be simply equal to the multiplication of the two things here and then.",
                    "label": 0
                },
                {
                    "sent": "Finally, we will obtain that the output of the majority vote will be simply design of the expectation here.",
                    "label": 1
                },
                {
                    "sent": "So the expectation of a few of our classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since our goal is intimately to use the Parkway story, we will also define a prior distribution over all the possible classifiers.",
                    "label": 1
                },
                {
                    "sent": "Akainu pickle you're here because normally a prior distribution must be defined before seeing the data and in this simple compress framework we are dealing with data dependent classifiers, but in fact we are deafening.",
                    "label": 1
                },
                {
                    "sent": "The priority solution over the compression set and the message string before seeing the data, and once we see the data diverse.",
                    "label": 0
                },
                {
                    "sent": "Uh, this fire distribution over the classifiers.",
                    "label": 0
                },
                {
                    "sent": "So here that little picture I represent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can see it on each line that we have the same probability of choosing any compression set, but and given a confession set, we have the same probability of choosing a message showing our classifier with a plus on its message win and a classifier with a minus on its message ring.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I define two new concept and five crossword talk about this.",
                    "label": 0
                },
                {
                    "sent": "Quickly.",
                    "label": 0
                },
                {
                    "sent": "This morning we will define the concepts of Apostle align on a prior and posterior strongly align on a prior.",
                    "label": 0
                },
                {
                    "sent": "So we will say that the posterior Q is aligned on the prior P when for all the pairs of the Boolean complement classifiers and the majority vote.",
                    "label": 1
                },
                {
                    "sent": "There's some of the weight on the posterior will be equal to the sum of the weight of the two same complement classifiers in the fire.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we will see that this string, this along posterior will be also strongly align when we have this property.",
                    "label": 0
                },
                {
                    "sent": "So for all the message, all the compression set I the difference of the weight of the two Boolean complementary classifiers and the posterior will be equal to a constant WI.",
                    "label": 0
                },
                {
                    "sent": "And this is for all message doing so we have only N values of WI.",
                    "label": 0
                },
                {
                    "sent": "And it would.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Useful to restrict ourselves to this setting because all the posterior distribution will be defined by N values of WI.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we will obtain a posterior that looks like this, again, like in the prior.",
                    "label": 0
                },
                {
                    "sent": "We can see that we have the same probability of choosing any compression set, but given a compression set, the probability of choosing classifiers with a plus on its message ring and a minus on its message ring will vary according to the value I and so the value of W is valuable.",
                    "label": 0
                },
                {
                    "sent": "WI will allow us to play with the banderi.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that define all this.",
                    "label": 0
                },
                {
                    "sent": "I can explain new.",
                    "label": 0
                },
                {
                    "sent": "What is the reconstruction function that we use in our setting?",
                    "label": 0
                },
                {
                    "sent": "In other words, what are the outputs of the sea classifiers?",
                    "label": 0
                },
                {
                    "sent": "So first consider similarity function key which takes 2 example and return a number between minus one or plus one chicken be Mercer kernel like we use on the SVM, but it can be any kind of function.",
                    "label": 0
                },
                {
                    "sent": "So once we have this function, we will decide that the output of the SC classifier age with the example I on its compression set and the message Ring Sigma Plus will return either one or plus 1 + 1 if the Sigma of the message ring is lower than the value of the kernel between XI.",
                    "label": 0
                },
                {
                    "sent": "The example on the conversion set an X the example that we want to classify.",
                    "label": 0
                },
                {
                    "sent": "An If Sigma is greater or equal than K it the C classifier will return minus one.",
                    "label": 0
                },
                {
                    "sent": "With this definition, we obtain finally that we can recover and the majority vote the value of the kernel, so we can see here that the WI multiplied by key is only equal to the part of the majority vote that.",
                    "label": 0
                },
                {
                    "sent": "That implies this example I.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we are obtained at the the majority vote of how the C classifiers given the distribution will be the same as the classification function of the SVM.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We will have that when the WI that we have in that define our majority vote is only equal to Yi multiplied by Alpha.",
                    "label": 0
                },
                {
                    "sent": "I divide by your normalization constant.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So since we reach our goal to define the SVM as a majority vote of simple compressed classifiers, we will now use the Back Bay story to obtain upper bound on the risk of those SES.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "PM like you know now the stereo.",
                    "label": 0
                },
                {
                    "sent": "Otherwise to bound the risk of the majority vote classifier by bonding what we called the Gibbs risk.",
                    "label": 1
                },
                {
                    "sent": "So the risk of the Gibbs classifier and usually with bound on the risk of the majority vote just by using this property that the base risk the risk of the majority vote is bounded by two times the risk of the Gibbs classifier.",
                    "label": 1
                },
                {
                    "sent": "This is a really bad choice in our setting because the Gibbs risk of our majority vote will likely be there one off, even if the majority majority vote is really good.",
                    "label": 0
                },
                {
                    "sent": "This is like this.",
                    "label": 0
                },
                {
                    "sent": "Because remember that each classifier that we have in all majority vote, rely only on one training example.",
                    "label": 0
                },
                {
                    "sent": "So each base classifier is really weak.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want them to bound more relevant risk and for doing this we will use a method that we develop on a previous work so we can see the reference.",
                    "label": 1
                },
                {
                    "sent": "It's a NIPS 2006 paper Francois talk about it a little bit this otherwise to bound any kind of general loss, provided that this Journal, this loss that we called Cheetah.",
                    "label": 1
                },
                {
                    "sent": "Can be expanded by a Taylor series.",
                    "label": 0
                },
                {
                    "sent": "I want the zero margin.",
                    "label": 0
                },
                {
                    "sent": "So in that work we decide to which choose to bound the quadratic risk that we can see in red.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this in this graphic.",
                    "label": 0
                },
                {
                    "sent": "Here the gamma is only the minimum of the parabolas, so it's an operator of the loss, and it's useful because it allow.",
                    "label": 0
                },
                {
                    "sent": "The majority vote to have a little little risk, even if the majority vote is close to the margin in control of the Gibbs risk that we see you in.",
                    "label": 1
                },
                {
                    "sent": "In fact, it's two times that gives you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we see you in.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Giving this we obtain two telegrams that gives back based bound on the on the risk of this SVM, the first one is inspired by the theorem of katani that Francois talk about earlier and we adapt this to rent to to end our general loss functions into an set of data dependent classifiers.",
                    "label": 0
                },
                {
                    "sent": "Size lesser equal then L. So I I I'm not showing you the proof but we can see that we have a lot of constraints that appearing that depends on the loss that we kind of lost that we use and the size of the.",
                    "label": 0
                },
                {
                    "sent": "And the compression set.",
                    "label": 0
                },
                {
                    "sent": "But what is interesting is that finally, since all these are constant, we obtain that minimizing the bound is equivalent to minimizing only the little equation that we see in the bottom.",
                    "label": 0
                },
                {
                    "sent": "So see Constant C, which is another parameter multiplied by the empirical risk plus the KL regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also adapt the Seeger bound to obtain a newer term on the risk of the majority vote of SC classifiers, so this bound used the function this markiel, which is the bear need, diverge.",
                    "label": 0
                },
                {
                    "sent": "The difference between two burglary distributions of probability of success, Q1P and this function care have this interesting for protein that the keel between 2:00 and P. It's equal to their kill between 1 -- 2 and 1 -- P. And more, Moreover, we specialize this diagram for the kind the case of online posterior.",
                    "label": 1
                },
                {
                    "sent": "So remember that when we have a line posterior the the sum of the complement classifiers and the posterior and prior or equal and when we are doing this, the usual regularizer KL disappear for the from the diagram.",
                    "label": 0
                },
                {
                    "sent": "This is a quite a picture.",
                    "label": 0
                },
                {
                    "sent": "Result and I will explain you in the next few slides.",
                    "label": 0
                },
                {
                    "sent": "While we're doing that.",
                    "label": 1
                },
                {
                    "sent": "But this is interesting because in strange because it suggests that in the case and posterior we only we only need to minimize the empirical risk.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I try to give you the big idea why we are the KL disappear of the in the case of a line posterior.",
                    "label": 0
                },
                {
                    "sent": "So remember that in the usual back based motivation the KL appear when we want to bound the random variable that we see on the top.",
                    "label": 1
                },
                {
                    "sent": "So the expectation of RP in terms of something that depends on two.",
                    "label": 0
                },
                {
                    "sent": "So when we went.",
                    "label": 1
                },
                {
                    "sent": "We transform the expectation of our Q over into an expectation over Q by adding the diffraction P / Q and this becomes eventually decaire after against an inequality.",
                    "label": 0
                },
                {
                    "sent": "But in the case of a line posterior again see here that we had this operation by free will transform the expectation of RP until next mutation over Q without introducing any new new constant.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here I explain you why it's like this, so remember that in our majority vote we have always compliment classifiers, so if the majority vote contain the classifier age, it also contains the classifier minus age.",
                    "label": 0
                },
                {
                    "sent": "And this gives us the the equality between the 20 ground that we send it up and.",
                    "label": 0
                },
                {
                    "sent": "So we can see that two time the expectation that over P is equal to the sum of the twenty goal.",
                    "label": 0
                },
                {
                    "sent": "Here we can say you mean that we use the fact that the risk of minors age is only the risk of 1 minus the risk of age, and after that we use the property of the function KL, which is that the KL between two LP gives the Cal between one minus P 1 -- Q.",
                    "label": 0
                },
                {
                    "sent": "In one minute, speak that allow us to be.",
                    "label": 0
                },
                {
                    "sent": "Group the two part so.",
                    "label": 0
                },
                {
                    "sent": "We are, we use the property of online posterior after that to transform the term.",
                    "label": 0
                },
                {
                    "sent": "PP of H + P of minus age into the term Q of H + 2 of my message and from then we are saying that the expectation of two times expectation of working.",
                    "label": 0
                },
                {
                    "sent": "So the two things are completely quit.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, I will explain you, and in fact I will not explain you the algorithms, but I just want to show you that we design tutoring algorithms to minimize the two bounds.",
                    "label": 1
                },
                {
                    "sent": "So remember that in the Stonia line setting or our posterior distribution is defined by N values of the value I.",
                    "label": 1
                },
                {
                    "sent": "So the minimizing algorithm will will only play with vector.",
                    "label": 0
                },
                {
                    "sent": "WN component varies as similarly.",
                    "label": 0
                },
                {
                    "sent": "Then there is VM.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the other interesting thing is that we are pre compute the margin into a matrix that looks really like the matrix.",
                    "label": 0
                },
                {
                    "sent": "The gram matrix that we use with Windows VM.",
                    "label": 0
                },
                {
                    "sent": "The only thing is that we have a one Internet signal which comes from the fact that we always presume that see classifier doesn't made an error on an example belonging to his compression set.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the two algorithms minimize those three questions.",
                    "label": 0
                },
                {
                    "sent": "The first one minimize the that I did the algorithm with the KL minimize the bound based on the first pack pack based around I present.",
                    "label": 0
                },
                {
                    "sent": "So this to remap 2 paramaters C, which is the values of the catoni bound.",
                    "label": 0
                },
                {
                    "sent": "The tradeoff between the minimalization of the empirical risk and the minimization of the regularizer.",
                    "label": 1
                },
                {
                    "sent": "And gamma, which is the minimum of the Parable of the risk that we use.",
                    "label": 1
                },
                {
                    "sent": "And we also had the algorithm without the KL based on the second version of the bound and this.",
                    "label": 1
                },
                {
                    "sent": "This algorithm only minimize the quadratic quadratic risk with only one hyperparameter.",
                    "label": 1
                },
                {
                    "sent": "To tune the minimum of the parable.",
                    "label": 0
                },
                {
                    "sent": "And this isn't everything because we have here 2 convex function.",
                    "label": 0
                },
                {
                    "sent": "So it's really simple to design A minimization algorithm that find global minimum of.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who's equations?",
                    "label": 0
                },
                {
                    "sent": "We did the experiments to compare the two algorithms with DD SVM using the RBF kernel and we use the cross validation.",
                    "label": 0
                },
                {
                    "sent": "In our case is to find the best hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to see that there's no clear winner winner between the three algorithms.",
                    "label": 0
                },
                {
                    "sent": "But it seems that we have this system is obtained.",
                    "label": 0
                },
                {
                    "sent": "Best result that the classic SVM but the results are there for significance and the other interesting thing to say is that we attend to be resolved with the two version of the algorithm, the one without the regularizer and with the regularizer.",
                    "label": 0
                },
                {
                    "sent": "And it's very particular because.",
                    "label": 0
                },
                {
                    "sent": "We don't have the regret and we have one eye Parameterless studio.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally I will just just conclude by saying that I.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Present a general framework that otherwise to apply the PAC Bayes bounds Dippach based are we to the kernel methods and for now I just compare the result of this framework with this VM.",
                    "label": 1
                },
                {
                    "sent": "But this framework is more general and we have many other ideas.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Twice so among them there is the idea of doing experimentation with nonpositive, some indefinite kernel, because because we are not limited to.",
                    "label": 1
                },
                {
                    "sent": "We can also think to consider a majority vote of SC classifiers with more than one example on the compression set, and we are beginning to think also to consider prior that are not strong line, so we will have something much more general, that's it.",
                    "label": 0
                }
            ]
        }
    }
}