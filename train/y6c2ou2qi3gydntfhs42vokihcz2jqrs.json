{
    "id": "y6c2ou2qi3gydntfhs42vokihcz2jqrs",
    "title": "Approximate Bayesian computation: a simulation based approach to inference",
    "info": {
        "author": [
            "Richard Wilkinson, Department of Molecular Biology and Biotechnology, University of Sheffield"
        ],
        "published": "Sept. 9, 2008",
        "recorded": "May 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/aispds08_wilkinson_abc/",
    "segmentation": [
        [
            "So yes, as the title says, I'm going to talk about approximate Bayesian computation or.",
            "ABC is it is often known or likelihood free inference an now obviously in the past few days I've realized that most people here are doing approximate Bayesian computation in some sense.",
            "So what I mean is a class of Monte Carlo algorithms that are used to do inference in stochastic models, and in particular they don't use when the function model is not available.",
            "Likelihood free inference.",
            "So what I'll do, I'll highlight the basic idea behind these ABC methods an and then give a couple of extensions and illustrate them.",
            "With reference to a particular kind of problem I worked on from evolutionary biology, in that while the general idea is a very generic, some of the extensions are best kind of illustrated with specific reference to a problem I should just mention.",
            "This is work that was done with Simon Cabaret while doing my PhD in Cambridge."
        ],
        [
            "OK, so I guess there's two main types of statistical model that we tend to build.",
            "The first type is kind of your prescribed model, so you get your data and you decide you're going to fit a directionally process or a normal distribution to it or whatever.",
            "OK, they might be very complicated about hierarchical parts to it and so on, but ultimately you can write down the likelihood function at the end so you can calculate the likelihood function or some like version likelihood function up to a normalizing constant.",
            "So another type of model that we use, our implicit models, so these are mechanisms to.",
            "Simulate the observations from the model.",
            "So you put some parameters.",
            "Computer works away and produces some sample output.",
            "Now, this implicit models often give scientists much more freedom to more accurately model the phenomena they are interested in.",
            "Until recently, with increasing computer power, there wasn't really very many ways to do inference in these models, so these approximate Bayesian computation algorithms are way through inference in implicit models.",
            "So this is by implicit model.",
            "This is a branching process and lying here in my problem represents different species of primate, but it could equally be a different cell in a cancer tumor or a different human in a population.",
            "And these red dots or some kind of discrete sampling.",
            "So my problem again, these are primate species, so the red dots represent the fossil finds, so fossils here, here, here and so on.",
            "Or they could be mutations in a bit of DNA."
        ],
        [
            "So what we're interested in doing an is fittingly multidata so very aware talking to machine people from machine learning background that we use different terminology and statistics.",
            "We're talking about the same thing all the time, but we don't understand each other.",
            "Sometimes we talking across purposes so.",
            "Put in the data calibration data assimilation, kind of fitting is an inverse problem, so most builds are forwards models.",
            "We specify parameter values, initial conditions, and it turns out output and it's the reverse procedure we want to do.",
            "So here is just a representation of the primate.",
            "So we've got some kind of phylogeny here.",
            "And the fossils represent the data we have available at aim.",
            "Here is to estimate the depth of this tree.",
            "So where this route is?",
            "So being very slack with my terminology, this is the time the first primate appeared, the time they first involved.",
            "If you like, change the divergance time."
        ],
        [
            "OK, so it's just some notation.",
            "Everyone knows this of course, but things will be parameter throughout the talk and this is going to be data and we want to draw samples from the posterior distribution.",
            "The distribution of theater given B.",
            "And let me just say for many of these kind of tree based."
        ],
        [
            "Models that appear in genetics.",
            "Population biology.",
            "Evolutionary Epidemiology.",
            "The likelihood function is in."
        ],
        [
            "Actable OK, so if we don't know the likelihood function property data given Theta, we can't use MCMC.",
            "We can't use game sampler or many other Monte Carlo algorithms through some other approach."
        ],
        [
            "OK, so the basic algorithm is based upon the rejection algorithm which was first proposed by Von Neumann.",
            "I believe back in the 50s and it's very simple.",
            "If this works as follows, we pick a parameter value feature for my prior distribution and then we accept that parameter value with probability proportional to the likelihood.",
            "So we can divide by some constant here as long as everything is less than one.",
            "Button that works fine.",
            "That gives us independent draws in the posterior distribution of the parameter, so that works.",
            "But he said we don't know the likelihood function."
        ],
        [
            "So that's what David Balding calls that kind of mechanical version of this so kind of mechanical version of Bayes theorem.",
            "So we draw theater from the prior.",
            "We simulate the data from the Model D prime, and we accept the property value if the simulated data D prime exactly matches the real data D. OK, there's a mechanical version, but your computer bowl turning away in the background.",
            "Traditionally simulated datasets.",
            "We only accept the ones that exactly match our real data set.",
            "Now obviously for any model of any complexity here, this is just never going to happen.",
            "I mean effectively continuous data or anything like that.",
            "You're never going to exactly simulate your observation."
        ],
        [
            "So the weather approximation comes in.",
            "So what we're going to do instead is going to pick up parameters from our prior simulation data.",
            "We're going to accept the parameter if the simulated data is close to the real data.",
            "By close I mean we're going to have a metric rho and tolerance epsilon."
        ],
        [
            "And this works in the following sense.",
            "OK, so first of all, this is an approximation, obviously.",
            "This is what we're sampling from with this and it makes sense in the following way.",
            "If epsilon is Infinity here, we're going to accept every single value of Theta, so we're going left with this supplier distribution.",
            "Well, we've had silent O here.",
            "We're only going to accept data if the simulated data matches a real data symmetric.",
            "We get the posterior distribution.",
            "So epsilon controls accuracy.",
            "At same time, it controls the efficiency of the compatibility.",
            "So with that sideline very small, we're going to accept very few fee to wear them very large wet lots of data.",
            "So this is kind of a trade off.",
            "It represents a tension between compute ability and accuracy."
        ],
        [
            "OK, well there's another kind of just another layer of complexity to add to that.",
            "Even using that approximation, this kind of models we're talking about typically have very high dimensional output, so if you're talking about Gina types or kind of gene expression data, very high dimensional, an even that would be good so.",
            "And statistically, always summarize the data to some lower dimensional kind of summary.",
            "What we're going to do is compare the summary of the real data with a summary of the simulation output, and if there close, we gotta check the parameters.",
            "Now, if S is a sufficient statistic here.",
            "I I mean sufficient to mystical sense of representing all the information about data that there is in the data is sufficient.",
            "Then this algorithm is equivalent to one of the other, the previous slide.",
            "Of course I should just say if we don't know the likelihood function, there's no way we can never know whether any statistics sufficient or not.",
            "Funny, we think it's difficult though."
        ],
        [
            "County OK, so just the simplest example.",
            "Well, so let me just say that the accuracy of these algorithms is unknown.",
            "These are very new.",
            "The first proposal within 99 we think by pretty hard at all and then Mark Beaumont today building in 2001 made them really well known in the genetics community.",
            "And at the moment it's not really known how the error scales.",
            "So for simple examples spoke about Gaussian distribution here.",
            "So data comes from a normal distribution with known variance, unknown mean, and we're going to give a flap improper prior to the mean.",
            "And with that little generality, will assume that we observed data that has a sample mean of zero.",
            "That's how real data observation.",
            "So the algorithm which follows would pick a parameter from our prior distribution simulation data using that parameter value and accept that parameter if the simulated data is close to 0, which is our observation.",
            "And in this case we can actually calculate what the approximation is analytically.",
            "The key thing to note is that the variance of the approximation.",
            "Is the variance of the true posterior plus epsilon squared over 3?",
            "And so this is an overdispersion.",
            "The approximation is an overdispersion of the true cost area, and we'd only true in general that these estimates are conservative.",
            "They over disperse what the true answer should be.",
            "Here the pictures you got the solid lines.",
            "If you can see this other troop osteria, the dotted lines of the approximation and we've got applied on decreasing again, the approximation gets better, but we have to do more work to do at each time."
        ],
        [
            "OK. Well, the problem with the algorithm have presented so far.",
            "It's up there very inefficient, so I'm sure you probably spotted with sampling repeatedly from the prior distribution.",
            "So if you've got a highly multivariate parameter or a multivariate parameter, 10 dimensions or whatever you're looking for original high probability in kind of a multi dimensional haystack will be hard to find where you're looking for.",
            "So the idea behind MCMC of course, is that by correlating successive observations, we can spend more time in regions of high likelihood."
        ],
        [
            "Well, we can't do MCMC because we don't have a likelihood, but there's an approximate version of it.",
            "So we have our usual proposal density Q an we do the following for currently at theater we propose a moved to theater prime from Q and then we simulate some data using that parameter.",
            "If the simulated data is close again, we then calculate the Metropolis Hastings exception ratio.",
            "So usually this has a ratio of likelihoods in here, but that term has been approximated by this acceptance step.",
            "And just a couple implementational details.",
            "If you try to apply this and start off with the required tolerance level, the one you're aiming for, it generally takes an age to get started.",
            "So there's kind of adaptive tolerances schemes.",
            "We start off with large values here and slowly decrease the value we're we're interested in to get warmed up.",
            "There's also an approximate sequential importance sampling algorithm proposed by sitting at L, but it seems to be a few problems with that.",
            "As pointed out a couple of weeks ago by Robert Christine, Robert Ann Company.",
            "OK, well let's albums so so good.",
            "But there are problems still very slow to converge.",
            "We've got this acceptance rate here and this acceptance step here.",
            "We've mixed very slowly and so on."
        ],
        [
            "So off."
        ],
        [
            "And we have more help available to us, so I'll illustrate this with reference to the branching process problems in a moment.",
            "But suppose we've got a parameter Theta, and we can split it into several blocks.",
            "It can be arbitrarily many.",
            "The Gibbs sampler, as I'm sure you know, then, which follows.",
            "We draw the first value, the first block theater, one from its conditional density.",
            "Given DM's are the blocks and so on 32.",
            "Now, after the autumn turns out, although we don't know the complete likelihood, we can calculate part of the likelihood that if we know some of these parameters, then we can calculate some of the conditional distributions.",
            "So suppose for a moment that we know this conditional to the distributor one given Peter two in the data.",
            "But we don't know this one.",
            "So we could then do a ABC within Gibbs kind of."
        ],
        [
            "Templar or an ABC or Metropolis.",
            "Approximate Acropolis Hastings within Gibbs so we do the following.",
            "We update the first block from its standard Gibbs proposal density, so this of course is an acceptance rate of 1.",
            "And for the second block we do our ABC step.",
            "We draw it from its parameter.",
            "We simulate some data and we accept if it.",
            "If the data is good.",
            "OK.",
            "So this way we've got this automatic acceptance.",
            "Here we got our standard ABC here and we can explore the the posterior space quicker than would ever be the case.",
            "So you keep doing Step 2 until you get.",
            "An yes, you want to draw from it.",
            "Yep, if you're doing the Metropolis Hastings version of it.",
            "So from the previous slide, then you wouldn't.",
            "OK, you, did you do it till you got an acceptance and then you would calculate this.",
            "And if you rejected here you go back to step one."
        ],
        [
            "OK. OK, so back to the example for before we call this.",
            "This is a branching process that's generating the data in some sense, and these red bits of the discrete observations we observe.",
            "Turns out that if we condition unknown this tree or the tree parameters.",
            "Then the distribution of the sampling points are often tractable.",
            "This likelihood might just pass on mutations along these branch lengths, or binomial fossil finds.",
            "So so on.",
            "So concentrating on the primate divergent problem for a moment.",
            "Because what we're trying to estimate was the depth of this point here.",
            "How far back in the past do we have to go to find the divergent time of the primates when they first evolved the data we have are the fossil finds that we know of.",
            "So they play intelligence.",
            "Discover Fossil fans is linear.",
            "All they get is this snapshot.",
            "I want to reconstruct something about history."
        ],
        [
            "OK. Journal today we need to think about how to choose good summary statistics.",
            "And this is generally I an unknown problem and we don't have to do this in general.",
            "So what we want is somewhere summary summarizing our output S tomorrow, same output data.",
            "Which is sensitive to changes in the parameter, but robust to variations indeed.",
            "So recall this is a stochastic model, so over here this is 2 dimensional data.",
            "The green might represent output from a particular parameter value and this from another parameter value.",
            "OK, so we're going to cloud data for each value of the parameter.",
            "We want a summary that actually is kind of separates these two clouds, except the parameters to be continuous, as much more difficult app.",
            "So obviously the location.",
            "Then kind of mean something like a starting point.",
            "Generalize.",
            "Nontrivial how to come up with good summaries takes a great deal of intuition.",
            "It seems about the problem you're dealing with.",
            "And there's been counterintuitive results as well so.",
            "I know it's been found that sometimes you can add information to your summary, so increase your summary.",
            "Adding more information you expect to do better and your inferences and the exact opposite happens.",
            "Everything falls down, and that's probably happening because your model is wrong.",
            "All models are wrong.",
            "I guess an while you might have to fit them all to one summary or another summary fit into both at the same time is difficult.",
            "Perhaps we need to model some kind of model discrepancy or some kind of error model error there, But what we would like ideally.",
            "It's some kind of systematic approach for finding good summaries.",
            "Of highly motivated data."
        ],
        [
            "OK, so talking about primates fossils, the observed number of fossils I should have said Anne."
        ],
        [
            "The picture isn't actually this simple in that time is divided into geological epoch, so we can't date the age of fossils that precisely, so all we can do is count the number of fossils were found in the Miocene.",
            "The number we found the Essene, and so on."
        ],
        [
            "So our data is actually a discrete number of counts of the different number of primary species we know from each.",
            "So the metric one of the metrics we tried was the following.",
            "Just kind of measure the distance from simulated data in the real data.",
            "An nice thing about the ABC methodology is that you run your simulator as many times as you want.",
            "Then you can try as many different metrics as you want.",
            "You have to rerun everything if you keep all your data."
        ],
        [
            "OK, so when we first did this, it went wrong.",
            "This represents the tracer R. MCMC output and this here is the prediction of the number of modern primate species.",
            "So we know that the number of modern private speeches about ferns and 50 where their model is predicting about 50.",
            "While"
        ],
        [
            "This is very simple to solve using ABC methodology or we do is change the metric we determine to the metric that takes account of the predicted modern population size.",
            "So we know that there's 376 modern species, roughly.",
            "And So what we do, we compare our simulated output with three, 7, six, and if it's far away, we penalize the model.",
            "And then this gives us approximation from this posterior distribution.",
            "So the posterior distributor given the data and values 376.",
            "But we wanted, I just think about what you have to do if you're doing likelihood based inference here you have to go away and calculate this distribution so the data and there's something about the endpoint of the tree are very highly correlated.",
            "This calculation will be nontrivial, so be certain amount of work, presumably to calculate that thing if you could.",
            "So with the ABC approach you just change your metric.",
            "And you get a different answer.",
            "And as I say, there's lots wrong with it, which I'm coming to.",
            "OK.",
            "Thinking about it."
        ],
        [
            "OK, so just to give you the punch line from the science story an reason these problems interesting is, there's a big controversy of surrounding this.",
            "So the paleontologists are all certain that primates diverged after the demise of the dinosaurs 65 million years ago.",
            "My certain of that, or a large proportion of them, are certain of it thinking, whereas the geneticists using DNA from modern primates are fairly sure that the divergent time is back here somewhere.",
            "So what our work is done as looked at a reasonable model of evolution from the paleontology literature and actually done inference of what we expect the diverse time to be given the data we have available.",
            "And although we can't say for sure is in the Cretaceous and primates and dinosaurs alongside, it seems we can't rule it out either.",
            "There's a reasonable amount of certainty, so the dinosaurs died about here, and so we put most down mass over here.",
            "So this depends on the distribution, but it's actually fairly robust this prior OK.",
            "So using the fossil evidence alone, you can't constrain the primary evidence time to the Cretaceous."
        ],
        [
            "OK, so back to the Monte Carlo and ABC.",
            "There's a couple of extensions that May or may not be possible.",
            "So the first one that has been suggested is that we can use these algorithms to do model selection.",
            "So in theory the acceptance rate from these algorithms approximates the normalizing constant.",
            "From your post area, so no magic constant partition, function evidence, whatever you call it.",
            "OK, so this is the probability of the data given the model.",
            "So in theory we can approximate that with an acceptance rate from our ABC algorithm.",
            "However, in practice I think we've shown complete it.",
            "I mean, it's fairly hopeless.",
            "It's so non robust a choice of epsilon rho and so on.",
            "There's a few other methods.",
            "I mean this is fairly out of date, but I know these things are difficult to calculate.",
            "'cause I think someone saying earlier.",
            "And of course, these ABC albums are only ever any good if you can.",
            "Your simulator is cheap.",
            "If it takes you 10 seconds to simulate a sample output from your model, that's probably no good.",
            "Never mind if you have to take two weeks like Peter had.",
            "So what option might be to emulate the stochastic model so we know something about emulating deterministic models?",
            "And I believe Dan and a couple of other people have done stuff on emulating stochastic models, but it's still still work to be done there.",
            "And there's a paper by Richard Boysen Darren Wilkinson submission where they've.",
            "Approximated the output by some parametric family and emulated those parameter values using the Gaussian process.",
            "So they've emulated built a meta model for the stochastic simulator."
        ],
        [
            "OK, so just to summarize I guess.",
            "Yeah, well in advantage of this is that it allows us to do inference in models for which we would not otherwise be able to, and we can.",
            "We can let the scientist whatever model they want, as long as we can simulate observations from it reasonably cheaply.",
            "We can do some sort of inference for them.",
            "Need a code related to give it to us, they can do it in such without any problem.",
            "OK and it's very easy to adapt if you change your model anyway.",
            "If you change your priors.",
            "If you want to add in more data.",
            "The inference works exactly the same way with MCMC.",
            "If you change your model, you often find you need to go through the whole process of re tuning everything.",
            "Finding new proposal distributions, checking, mixing convergence and so on.",
            "So it's very simple in that sense.",
            "The big drawbacks are, first of all it is approximate and it can be a pawn approximation in certain cases and we don't really know yet how good an approximation is.",
            "We have no way of bounding that error yet.",
            "Secondly, we have counterintuitive results, so the summary statistics as I said are hard to choose, and it's hard to anticipate what effect they're going to have.",
            "And just a couple of issues as I say how to choose good summary statistics and have gotten approximation, we get and so I shall leave you with."
        ],
        [
            "A few references.",
            "Thank you very much.",
            "Question.",
            "Natural thing to do would be to do this sort of sequential importance under, because then your clothes are coming from ascent effectively or posterior from the previous steps.",
            "In some populations.",
            "Carlo idea, since Christian Rebecca, who came up with that idea, he's saying it doesn't work.",
            "Can you give an intuition why that doesn't work or what?",
            "So it's Scott Sitton and Group who came up with the idea.",
            "And then Mark Beaumont and Christine Robert I've.",
            "Just published a paper which I only found about a week or two ago on the archive and that suggested there's a serious flaw in this.",
            "I haven't really had time to understand why they didn't work, but they missed some step out.",
            "Basically they missed their sampling from the wrong thing.",
            "There's someone in my head at the back, so maybe they know more than I know, but.",
            "It applied to Codell and yes.",
            "Well.",
            "I also tried to code it up.",
            "Yeah, I also tried to get up and get it to work, but I didn't email them or put an yeah.",
            "I think you're right.",
            "I think that probably isn't going to be the most fruitful.",
            "By doing it, but there's still issues to be resolved, I think.",
            "Do you think they can be resolved?",
            "I mean, yeah, I think I think they have been resolved.",
            "I will mark my suggestion we have.",
            "I haven't tried them.",
            "I haven't really passed the paper properly, so.",
            "I don't know.",
            "So use this kind of 0 one thing right on the discrepancy, yes.",
            "I need something that's coming through there and have some probability of getting rejected so they do so.",
            "As you say, for each simulation output we get a.",
            "The value how good it is, how close it is to the simulated data.",
            "So 701 cut off we can wait everything instead.",
            "So then we can do kind of weighted kernel density estimate of the posterior and that works much better in general I meant by that end, but I thought I got another time so I cut it last minute.",
            "But yeah, you're absolutely right.",
            "This reference here Beaumont and Bolding OK, and that does work better and that can be applied uniformly across the proxy.",
            "Empty all the approximate sequentially important sampling just the same.",
            "OK. Serum approximating them prior as well.",
            "It's like that, like prior free and likelihood, so why?",
            "Why we approximating the prior?",
            "So the emulation?",
            "Oh yeah, oh God, yeah.",
            "Seems like fairly scary world, yeah?",
            "Oil.",
            "I know Richard boys talk about this in Durham and he May Skepticality presenting saying he didn't know if it worked or not and he wasn't going to get accepted or not.",
            "But if you have an expensive stochastic simulator, I mean you want to do some sort of assimilation.",
            "What are the other options available to you?",
            "I mean it.",
            "No, it's not.",
            "It's not the prior, it's the.",
            "But it's not likely.",
            "No OK. Ha ha ha.",
            "OK, thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, as the title says, I'm going to talk about approximate Bayesian computation or.",
                    "label": 1
                },
                {
                    "sent": "ABC is it is often known or likelihood free inference an now obviously in the past few days I've realized that most people here are doing approximate Bayesian computation in some sense.",
                    "label": 0
                },
                {
                    "sent": "So what I mean is a class of Monte Carlo algorithms that are used to do inference in stochastic models, and in particular they don't use when the function model is not available.",
                    "label": 0
                },
                {
                    "sent": "Likelihood free inference.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do, I'll highlight the basic idea behind these ABC methods an and then give a couple of extensions and illustrate them.",
                    "label": 0
                },
                {
                    "sent": "With reference to a particular kind of problem I worked on from evolutionary biology, in that while the general idea is a very generic, some of the extensions are best kind of illustrated with specific reference to a problem I should just mention.",
                    "label": 0
                },
                {
                    "sent": "This is work that was done with Simon Cabaret while doing my PhD in Cambridge.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I guess there's two main types of statistical model that we tend to build.",
                    "label": 1
                },
                {
                    "sent": "The first type is kind of your prescribed model, so you get your data and you decide you're going to fit a directionally process or a normal distribution to it or whatever.",
                    "label": 0
                },
                {
                    "sent": "OK, they might be very complicated about hierarchical parts to it and so on, but ultimately you can write down the likelihood function at the end so you can calculate the likelihood function or some like version likelihood function up to a normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "So another type of model that we use, our implicit models, so these are mechanisms to.",
                    "label": 0
                },
                {
                    "sent": "Simulate the observations from the model.",
                    "label": 0
                },
                {
                    "sent": "So you put some parameters.",
                    "label": 0
                },
                {
                    "sent": "Computer works away and produces some sample output.",
                    "label": 1
                },
                {
                    "sent": "Now, this implicit models often give scientists much more freedom to more accurately model the phenomena they are interested in.",
                    "label": 1
                },
                {
                    "sent": "Until recently, with increasing computer power, there wasn't really very many ways to do inference in these models, so these approximate Bayesian computation algorithms are way through inference in implicit models.",
                    "label": 0
                },
                {
                    "sent": "So this is by implicit model.",
                    "label": 0
                },
                {
                    "sent": "This is a branching process and lying here in my problem represents different species of primate, but it could equally be a different cell in a cancer tumor or a different human in a population.",
                    "label": 0
                },
                {
                    "sent": "And these red dots or some kind of discrete sampling.",
                    "label": 0
                },
                {
                    "sent": "So my problem again, these are primate species, so the red dots represent the fossil finds, so fossils here, here, here and so on.",
                    "label": 0
                },
                {
                    "sent": "Or they could be mutations in a bit of DNA.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're interested in doing an is fittingly multidata so very aware talking to machine people from machine learning background that we use different terminology and statistics.",
                    "label": 0
                },
                {
                    "sent": "We're talking about the same thing all the time, but we don't understand each other.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we talking across purposes so.",
                    "label": 0
                },
                {
                    "sent": "Put in the data calibration data assimilation, kind of fitting is an inverse problem, so most builds are forwards models.",
                    "label": 1
                },
                {
                    "sent": "We specify parameter values, initial conditions, and it turns out output and it's the reverse procedure we want to do.",
                    "label": 0
                },
                {
                    "sent": "So here is just a representation of the primate.",
                    "label": 1
                },
                {
                    "sent": "So we've got some kind of phylogeny here.",
                    "label": 1
                },
                {
                    "sent": "And the fossils represent the data we have available at aim.",
                    "label": 0
                },
                {
                    "sent": "Here is to estimate the depth of this tree.",
                    "label": 0
                },
                {
                    "sent": "So where this route is?",
                    "label": 0
                },
                {
                    "sent": "So being very slack with my terminology, this is the time the first primate appeared, the time they first involved.",
                    "label": 0
                },
                {
                    "sent": "If you like, change the divergance time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's just some notation.",
                    "label": 0
                },
                {
                    "sent": "Everyone knows this of course, but things will be parameter throughout the talk and this is going to be data and we want to draw samples from the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "The distribution of theater given B.",
                    "label": 1
                },
                {
                    "sent": "And let me just say for many of these kind of tree based.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models that appear in genetics.",
                    "label": 0
                },
                {
                    "sent": "Population biology.",
                    "label": 0
                },
                {
                    "sent": "Evolutionary Epidemiology.",
                    "label": 0
                },
                {
                    "sent": "The likelihood function is in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actable OK, so if we don't know the likelihood function property data given Theta, we can't use MCMC.",
                    "label": 0
                },
                {
                    "sent": "We can't use game sampler or many other Monte Carlo algorithms through some other approach.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the basic algorithm is based upon the rejection algorithm which was first proposed by Von Neumann.",
                    "label": 0
                },
                {
                    "sent": "I believe back in the 50s and it's very simple.",
                    "label": 0
                },
                {
                    "sent": "If this works as follows, we pick a parameter value feature for my prior distribution and then we accept that parameter value with probability proportional to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we can divide by some constant here as long as everything is less than one.",
                    "label": 0
                },
                {
                    "sent": "Button that works fine.",
                    "label": 0
                },
                {
                    "sent": "That gives us independent draws in the posterior distribution of the parameter, so that works.",
                    "label": 1
                },
                {
                    "sent": "But he said we don't know the likelihood function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's what David Balding calls that kind of mechanical version of this so kind of mechanical version of Bayes theorem.",
                    "label": 0
                },
                {
                    "sent": "So we draw theater from the prior.",
                    "label": 0
                },
                {
                    "sent": "We simulate the data from the Model D prime, and we accept the property value if the simulated data D prime exactly matches the real data D. OK, there's a mechanical version, but your computer bowl turning away in the background.",
                    "label": 0
                },
                {
                    "sent": "Traditionally simulated datasets.",
                    "label": 0
                },
                {
                    "sent": "We only accept the ones that exactly match our real data set.",
                    "label": 0
                },
                {
                    "sent": "Now obviously for any model of any complexity here, this is just never going to happen.",
                    "label": 0
                },
                {
                    "sent": "I mean effectively continuous data or anything like that.",
                    "label": 0
                },
                {
                    "sent": "You're never going to exactly simulate your observation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the weather approximation comes in.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do instead is going to pick up parameters from our prior simulation data.",
                    "label": 0
                },
                {
                    "sent": "We're going to accept the parameter if the simulated data is close to the real data.",
                    "label": 0
                },
                {
                    "sent": "By close I mean we're going to have a metric rho and tolerance epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this works in the following sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, this is an approximation, obviously.",
                    "label": 0
                },
                {
                    "sent": "This is what we're sampling from with this and it makes sense in the following way.",
                    "label": 0
                },
                {
                    "sent": "If epsilon is Infinity here, we're going to accept every single value of Theta, so we're going left with this supplier distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, we've had silent O here.",
                    "label": 0
                },
                {
                    "sent": "We're only going to accept data if the simulated data matches a real data symmetric.",
                    "label": 0
                },
                {
                    "sent": "We get the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "So epsilon controls accuracy.",
                    "label": 0
                },
                {
                    "sent": "At same time, it controls the efficiency of the compatibility.",
                    "label": 0
                },
                {
                    "sent": "So with that sideline very small, we're going to accept very few fee to wear them very large wet lots of data.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a trade off.",
                    "label": 0
                },
                {
                    "sent": "It represents a tension between compute ability and accuracy.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well there's another kind of just another layer of complexity to add to that.",
                    "label": 0
                },
                {
                    "sent": "Even using that approximation, this kind of models we're talking about typically have very high dimensional output, so if you're talking about Gina types or kind of gene expression data, very high dimensional, an even that would be good so.",
                    "label": 0
                },
                {
                    "sent": "And statistically, always summarize the data to some lower dimensional kind of summary.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is compare the summary of the real data with a summary of the simulation output, and if there close, we gotta check the parameters.",
                    "label": 0
                },
                {
                    "sent": "Now, if S is a sufficient statistic here.",
                    "label": 1
                },
                {
                    "sent": "I I mean sufficient to mystical sense of representing all the information about data that there is in the data is sufficient.",
                    "label": 0
                },
                {
                    "sent": "Then this algorithm is equivalent to one of the other, the previous slide.",
                    "label": 1
                },
                {
                    "sent": "Of course I should just say if we don't know the likelihood function, there's no way we can never know whether any statistics sufficient or not.",
                    "label": 0
                },
                {
                    "sent": "Funny, we think it's difficult though.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "County OK, so just the simplest example.",
                    "label": 0
                },
                {
                    "sent": "Well, so let me just say that the accuracy of these algorithms is unknown.",
                    "label": 0
                },
                {
                    "sent": "These are very new.",
                    "label": 0
                },
                {
                    "sent": "The first proposal within 99 we think by pretty hard at all and then Mark Beaumont today building in 2001 made them really well known in the genetics community.",
                    "label": 0
                },
                {
                    "sent": "And at the moment it's not really known how the error scales.",
                    "label": 0
                },
                {
                    "sent": "So for simple examples spoke about Gaussian distribution here.",
                    "label": 0
                },
                {
                    "sent": "So data comes from a normal distribution with known variance, unknown mean, and we're going to give a flap improper prior to the mean.",
                    "label": 0
                },
                {
                    "sent": "And with that little generality, will assume that we observed data that has a sample mean of zero.",
                    "label": 0
                },
                {
                    "sent": "That's how real data observation.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm which follows would pick a parameter from our prior distribution simulation data using that parameter value and accept that parameter if the simulated data is close to 0, which is our observation.",
                    "label": 0
                },
                {
                    "sent": "And in this case we can actually calculate what the approximation is analytically.",
                    "label": 0
                },
                {
                    "sent": "The key thing to note is that the variance of the approximation.",
                    "label": 0
                },
                {
                    "sent": "Is the variance of the true posterior plus epsilon squared over 3?",
                    "label": 0
                },
                {
                    "sent": "And so this is an overdispersion.",
                    "label": 0
                },
                {
                    "sent": "The approximation is an overdispersion of the true cost area, and we'd only true in general that these estimates are conservative.",
                    "label": 0
                },
                {
                    "sent": "They over disperse what the true answer should be.",
                    "label": 0
                },
                {
                    "sent": "Here the pictures you got the solid lines.",
                    "label": 0
                },
                {
                    "sent": "If you can see this other troop osteria, the dotted lines of the approximation and we've got applied on decreasing again, the approximation gets better, but we have to do more work to do at each time.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Well, the problem with the algorithm have presented so far.",
                    "label": 0
                },
                {
                    "sent": "It's up there very inefficient, so I'm sure you probably spotted with sampling repeatedly from the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you've got a highly multivariate parameter or a multivariate parameter, 10 dimensions or whatever you're looking for original high probability in kind of a multi dimensional haystack will be hard to find where you're looking for.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind MCMC of course, is that by correlating successive observations, we can spend more time in regions of high likelihood.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we can't do MCMC because we don't have a likelihood, but there's an approximate version of it.",
                    "label": 0
                },
                {
                    "sent": "So we have our usual proposal density Q an we do the following for currently at theater we propose a moved to theater prime from Q and then we simulate some data using that parameter.",
                    "label": 0
                },
                {
                    "sent": "If the simulated data is close again, we then calculate the Metropolis Hastings exception ratio.",
                    "label": 0
                },
                {
                    "sent": "So usually this has a ratio of likelihoods in here, but that term has been approximated by this acceptance step.",
                    "label": 0
                },
                {
                    "sent": "And just a couple implementational details.",
                    "label": 0
                },
                {
                    "sent": "If you try to apply this and start off with the required tolerance level, the one you're aiming for, it generally takes an age to get started.",
                    "label": 0
                },
                {
                    "sent": "So there's kind of adaptive tolerances schemes.",
                    "label": 0
                },
                {
                    "sent": "We start off with large values here and slowly decrease the value we're we're interested in to get warmed up.",
                    "label": 0
                },
                {
                    "sent": "There's also an approximate sequential importance sampling algorithm proposed by sitting at L, but it seems to be a few problems with that.",
                    "label": 1
                },
                {
                    "sent": "As pointed out a couple of weeks ago by Robert Christine, Robert Ann Company.",
                    "label": 0
                },
                {
                    "sent": "OK, well let's albums so so good.",
                    "label": 0
                },
                {
                    "sent": "But there are problems still very slow to converge.",
                    "label": 0
                },
                {
                    "sent": "We've got this acceptance rate here and this acceptance step here.",
                    "label": 0
                },
                {
                    "sent": "We've mixed very slowly and so on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So off.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have more help available to us, so I'll illustrate this with reference to the branching process problems in a moment.",
                    "label": 0
                },
                {
                    "sent": "But suppose we've got a parameter Theta, and we can split it into several blocks.",
                    "label": 0
                },
                {
                    "sent": "It can be arbitrarily many.",
                    "label": 0
                },
                {
                    "sent": "The Gibbs sampler, as I'm sure you know, then, which follows.",
                    "label": 0
                },
                {
                    "sent": "We draw the first value, the first block theater, one from its conditional density.",
                    "label": 0
                },
                {
                    "sent": "Given DM's are the blocks and so on 32.",
                    "label": 0
                },
                {
                    "sent": "Now, after the autumn turns out, although we don't know the complete likelihood, we can calculate part of the likelihood that if we know some of these parameters, then we can calculate some of the conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "So suppose for a moment that we know this conditional to the distributor one given Peter two in the data.",
                    "label": 0
                },
                {
                    "sent": "But we don't know this one.",
                    "label": 0
                },
                {
                    "sent": "So we could then do a ABC within Gibbs kind of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Templar or an ABC or Metropolis.",
                    "label": 0
                },
                {
                    "sent": "Approximate Acropolis Hastings within Gibbs so we do the following.",
                    "label": 0
                },
                {
                    "sent": "We update the first block from its standard Gibbs proposal density, so this of course is an acceptance rate of 1.",
                    "label": 0
                },
                {
                    "sent": "And for the second block we do our ABC step.",
                    "label": 0
                },
                {
                    "sent": "We draw it from its parameter.",
                    "label": 0
                },
                {
                    "sent": "We simulate some data and we accept if it.",
                    "label": 0
                },
                {
                    "sent": "If the data is good.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this way we've got this automatic acceptance.",
                    "label": 0
                },
                {
                    "sent": "Here we got our standard ABC here and we can explore the the posterior space quicker than would ever be the case.",
                    "label": 0
                },
                {
                    "sent": "So you keep doing Step 2 until you get.",
                    "label": 0
                },
                {
                    "sent": "An yes, you want to draw from it.",
                    "label": 0
                },
                {
                    "sent": "Yep, if you're doing the Metropolis Hastings version of it.",
                    "label": 0
                },
                {
                    "sent": "So from the previous slide, then you wouldn't.",
                    "label": 0
                },
                {
                    "sent": "OK, you, did you do it till you got an acceptance and then you would calculate this.",
                    "label": 0
                },
                {
                    "sent": "And if you rejected here you go back to step one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, so back to the example for before we call this.",
                    "label": 0
                },
                {
                    "sent": "This is a branching process that's generating the data in some sense, and these red bits of the discrete observations we observe.",
                    "label": 0
                },
                {
                    "sent": "Turns out that if we condition unknown this tree or the tree parameters.",
                    "label": 0
                },
                {
                    "sent": "Then the distribution of the sampling points are often tractable.",
                    "label": 0
                },
                {
                    "sent": "This likelihood might just pass on mutations along these branch lengths, or binomial fossil finds.",
                    "label": 0
                },
                {
                    "sent": "So so on.",
                    "label": 0
                },
                {
                    "sent": "So concentrating on the primate divergent problem for a moment.",
                    "label": 0
                },
                {
                    "sent": "Because what we're trying to estimate was the depth of this point here.",
                    "label": 0
                },
                {
                    "sent": "How far back in the past do we have to go to find the divergent time of the primates when they first evolved the data we have are the fossil finds that we know of.",
                    "label": 0
                },
                {
                    "sent": "So they play intelligence.",
                    "label": 0
                },
                {
                    "sent": "Discover Fossil fans is linear.",
                    "label": 0
                },
                {
                    "sent": "All they get is this snapshot.",
                    "label": 0
                },
                {
                    "sent": "I want to reconstruct something about history.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Journal today we need to think about how to choose good summary statistics.",
                    "label": 1
                },
                {
                    "sent": "And this is generally I an unknown problem and we don't have to do this in general.",
                    "label": 0
                },
                {
                    "sent": "So what we want is somewhere summary summarizing our output S tomorrow, same output data.",
                    "label": 0
                },
                {
                    "sent": "Which is sensitive to changes in the parameter, but robust to variations indeed.",
                    "label": 1
                },
                {
                    "sent": "So recall this is a stochastic model, so over here this is 2 dimensional data.",
                    "label": 0
                },
                {
                    "sent": "The green might represent output from a particular parameter value and this from another parameter value.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to cloud data for each value of the parameter.",
                    "label": 0
                },
                {
                    "sent": "We want a summary that actually is kind of separates these two clouds, except the parameters to be continuous, as much more difficult app.",
                    "label": 0
                },
                {
                    "sent": "So obviously the location.",
                    "label": 0
                },
                {
                    "sent": "Then kind of mean something like a starting point.",
                    "label": 0
                },
                {
                    "sent": "Generalize.",
                    "label": 0
                },
                {
                    "sent": "Nontrivial how to come up with good summaries takes a great deal of intuition.",
                    "label": 1
                },
                {
                    "sent": "It seems about the problem you're dealing with.",
                    "label": 0
                },
                {
                    "sent": "And there's been counterintuitive results as well so.",
                    "label": 0
                },
                {
                    "sent": "I know it's been found that sometimes you can add information to your summary, so increase your summary.",
                    "label": 0
                },
                {
                    "sent": "Adding more information you expect to do better and your inferences and the exact opposite happens.",
                    "label": 0
                },
                {
                    "sent": "Everything falls down, and that's probably happening because your model is wrong.",
                    "label": 0
                },
                {
                    "sent": "All models are wrong.",
                    "label": 0
                },
                {
                    "sent": "I guess an while you might have to fit them all to one summary or another summary fit into both at the same time is difficult.",
                    "label": 0
                },
                {
                    "sent": "Perhaps we need to model some kind of model discrepancy or some kind of error model error there, But what we would like ideally.",
                    "label": 1
                },
                {
                    "sent": "It's some kind of systematic approach for finding good summaries.",
                    "label": 0
                },
                {
                    "sent": "Of highly motivated data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so talking about primates fossils, the observed number of fossils I should have said Anne.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The picture isn't actually this simple in that time is divided into geological epoch, so we can't date the age of fossils that precisely, so all we can do is count the number of fossils were found in the Miocene.",
                    "label": 0
                },
                {
                    "sent": "The number we found the Essene, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our data is actually a discrete number of counts of the different number of primary species we know from each.",
                    "label": 1
                },
                {
                    "sent": "So the metric one of the metrics we tried was the following.",
                    "label": 1
                },
                {
                    "sent": "Just kind of measure the distance from simulated data in the real data.",
                    "label": 0
                },
                {
                    "sent": "An nice thing about the ABC methodology is that you run your simulator as many times as you want.",
                    "label": 0
                },
                {
                    "sent": "Then you can try as many different metrics as you want.",
                    "label": 0
                },
                {
                    "sent": "You have to rerun everything if you keep all your data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so when we first did this, it went wrong.",
                    "label": 0
                },
                {
                    "sent": "This represents the tracer R. MCMC output and this here is the prediction of the number of modern primate species.",
                    "label": 0
                },
                {
                    "sent": "So we know that the number of modern private speeches about ferns and 50 where their model is predicting about 50.",
                    "label": 0
                },
                {
                    "sent": "While",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is very simple to solve using ABC methodology or we do is change the metric we determine to the metric that takes account of the predicted modern population size.",
                    "label": 1
                },
                {
                    "sent": "So we know that there's 376 modern species, roughly.",
                    "label": 1
                },
                {
                    "sent": "And So what we do, we compare our simulated output with three, 7, six, and if it's far away, we penalize the model.",
                    "label": 0
                },
                {
                    "sent": "And then this gives us approximation from this posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So the posterior distributor given the data and values 376.",
                    "label": 0
                },
                {
                    "sent": "But we wanted, I just think about what you have to do if you're doing likelihood based inference here you have to go away and calculate this distribution so the data and there's something about the endpoint of the tree are very highly correlated.",
                    "label": 0
                },
                {
                    "sent": "This calculation will be nontrivial, so be certain amount of work, presumably to calculate that thing if you could.",
                    "label": 0
                },
                {
                    "sent": "So with the ABC approach you just change your metric.",
                    "label": 0
                },
                {
                    "sent": "And you get a different answer.",
                    "label": 0
                },
                {
                    "sent": "And as I say, there's lots wrong with it, which I'm coming to.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thinking about it.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to give you the punch line from the science story an reason these problems interesting is, there's a big controversy of surrounding this.",
                    "label": 0
                },
                {
                    "sent": "So the paleontologists are all certain that primates diverged after the demise of the dinosaurs 65 million years ago.",
                    "label": 0
                },
                {
                    "sent": "My certain of that, or a large proportion of them, are certain of it thinking, whereas the geneticists using DNA from modern primates are fairly sure that the divergent time is back here somewhere.",
                    "label": 0
                },
                {
                    "sent": "So what our work is done as looked at a reasonable model of evolution from the paleontology literature and actually done inference of what we expect the diverse time to be given the data we have available.",
                    "label": 0
                },
                {
                    "sent": "And although we can't say for sure is in the Cretaceous and primates and dinosaurs alongside, it seems we can't rule it out either.",
                    "label": 0
                },
                {
                    "sent": "There's a reasonable amount of certainty, so the dinosaurs died about here, and so we put most down mass over here.",
                    "label": 0
                },
                {
                    "sent": "So this depends on the distribution, but it's actually fairly robust this prior OK.",
                    "label": 0
                },
                {
                    "sent": "So using the fossil evidence alone, you can't constrain the primary evidence time to the Cretaceous.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so back to the Monte Carlo and ABC.",
                    "label": 1
                },
                {
                    "sent": "There's a couple of extensions that May or may not be possible.",
                    "label": 0
                },
                {
                    "sent": "So the first one that has been suggested is that we can use these algorithms to do model selection.",
                    "label": 0
                },
                {
                    "sent": "So in theory the acceptance rate from these algorithms approximates the normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "From your post area, so no magic constant partition, function evidence, whatever you call it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the probability of the data given the model.",
                    "label": 0
                },
                {
                    "sent": "So in theory we can approximate that with an acceptance rate from our ABC algorithm.",
                    "label": 1
                },
                {
                    "sent": "However, in practice I think we've shown complete it.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's fairly hopeless.",
                    "label": 0
                },
                {
                    "sent": "It's so non robust a choice of epsilon rho and so on.",
                    "label": 0
                },
                {
                    "sent": "There's a few other methods.",
                    "label": 0
                },
                {
                    "sent": "I mean this is fairly out of date, but I know these things are difficult to calculate.",
                    "label": 0
                },
                {
                    "sent": "'cause I think someone saying earlier.",
                    "label": 0
                },
                {
                    "sent": "And of course, these ABC albums are only ever any good if you can.",
                    "label": 0
                },
                {
                    "sent": "Your simulator is cheap.",
                    "label": 0
                },
                {
                    "sent": "If it takes you 10 seconds to simulate a sample output from your model, that's probably no good.",
                    "label": 0
                },
                {
                    "sent": "Never mind if you have to take two weeks like Peter had.",
                    "label": 1
                },
                {
                    "sent": "So what option might be to emulate the stochastic model so we know something about emulating deterministic models?",
                    "label": 1
                },
                {
                    "sent": "And I believe Dan and a couple of other people have done stuff on emulating stochastic models, but it's still still work to be done there.",
                    "label": 0
                },
                {
                    "sent": "And there's a paper by Richard Boysen Darren Wilkinson submission where they've.",
                    "label": 0
                },
                {
                    "sent": "Approximated the output by some parametric family and emulated those parameter values using the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So they've emulated built a meta model for the stochastic simulator.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to summarize I guess.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well in advantage of this is that it allows us to do inference in models for which we would not otherwise be able to, and we can.",
                    "label": 0
                },
                {
                    "sent": "We can let the scientist whatever model they want, as long as we can simulate observations from it reasonably cheaply.",
                    "label": 0
                },
                {
                    "sent": "We can do some sort of inference for them.",
                    "label": 0
                },
                {
                    "sent": "Need a code related to give it to us, they can do it in such without any problem.",
                    "label": 0
                },
                {
                    "sent": "OK and it's very easy to adapt if you change your model anyway.",
                    "label": 1
                },
                {
                    "sent": "If you change your priors.",
                    "label": 0
                },
                {
                    "sent": "If you want to add in more data.",
                    "label": 0
                },
                {
                    "sent": "The inference works exactly the same way with MCMC.",
                    "label": 0
                },
                {
                    "sent": "If you change your model, you often find you need to go through the whole process of re tuning everything.",
                    "label": 0
                },
                {
                    "sent": "Finding new proposal distributions, checking, mixing convergence and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple in that sense.",
                    "label": 1
                },
                {
                    "sent": "The big drawbacks are, first of all it is approximate and it can be a pawn approximation in certain cases and we don't really know yet how good an approximation is.",
                    "label": 0
                },
                {
                    "sent": "We have no way of bounding that error yet.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we have counterintuitive results, so the summary statistics as I said are hard to choose, and it's hard to anticipate what effect they're going to have.",
                    "label": 1
                },
                {
                    "sent": "And just a couple of issues as I say how to choose good summary statistics and have gotten approximation, we get and so I shall leave you with.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A few references.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Natural thing to do would be to do this sort of sequential importance under, because then your clothes are coming from ascent effectively or posterior from the previous steps.",
                    "label": 0
                },
                {
                    "sent": "In some populations.",
                    "label": 0
                },
                {
                    "sent": "Carlo idea, since Christian Rebecca, who came up with that idea, he's saying it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Can you give an intuition why that doesn't work or what?",
                    "label": 0
                },
                {
                    "sent": "So it's Scott Sitton and Group who came up with the idea.",
                    "label": 0
                },
                {
                    "sent": "And then Mark Beaumont and Christine Robert I've.",
                    "label": 0
                },
                {
                    "sent": "Just published a paper which I only found about a week or two ago on the archive and that suggested there's a serious flaw in this.",
                    "label": 0
                },
                {
                    "sent": "I haven't really had time to understand why they didn't work, but they missed some step out.",
                    "label": 0
                },
                {
                    "sent": "Basically they missed their sampling from the wrong thing.",
                    "label": 0
                },
                {
                    "sent": "There's someone in my head at the back, so maybe they know more than I know, but.",
                    "label": 0
                },
                {
                    "sent": "It applied to Codell and yes.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I also tried to code it up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I also tried to get up and get it to work, but I didn't email them or put an yeah.",
                    "label": 0
                },
                {
                    "sent": "I think you're right.",
                    "label": 0
                },
                {
                    "sent": "I think that probably isn't going to be the most fruitful.",
                    "label": 0
                },
                {
                    "sent": "By doing it, but there's still issues to be resolved, I think.",
                    "label": 0
                },
                {
                    "sent": "Do you think they can be resolved?",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, I think I think they have been resolved.",
                    "label": 0
                },
                {
                    "sent": "I will mark my suggestion we have.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried them.",
                    "label": 0
                },
                {
                    "sent": "I haven't really passed the paper properly, so.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "So use this kind of 0 one thing right on the discrepancy, yes.",
                    "label": 0
                },
                {
                    "sent": "I need something that's coming through there and have some probability of getting rejected so they do so.",
                    "label": 0
                },
                {
                    "sent": "As you say, for each simulation output we get a.",
                    "label": 0
                },
                {
                    "sent": "The value how good it is, how close it is to the simulated data.",
                    "label": 0
                },
                {
                    "sent": "So 701 cut off we can wait everything instead.",
                    "label": 0
                },
                {
                    "sent": "So then we can do kind of weighted kernel density estimate of the posterior and that works much better in general I meant by that end, but I thought I got another time so I cut it last minute.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're absolutely right.",
                    "label": 0
                },
                {
                    "sent": "This reference here Beaumont and Bolding OK, and that does work better and that can be applied uniformly across the proxy.",
                    "label": 0
                },
                {
                    "sent": "Empty all the approximate sequentially important sampling just the same.",
                    "label": 0
                },
                {
                    "sent": "OK. Serum approximating them prior as well.",
                    "label": 0
                },
                {
                    "sent": "It's like that, like prior free and likelihood, so why?",
                    "label": 0
                },
                {
                    "sent": "Why we approximating the prior?",
                    "label": 0
                },
                {
                    "sent": "So the emulation?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, oh God, yeah.",
                    "label": 0
                },
                {
                    "sent": "Seems like fairly scary world, yeah?",
                    "label": 0
                },
                {
                    "sent": "Oil.",
                    "label": 0
                },
                {
                    "sent": "I know Richard boys talk about this in Durham and he May Skepticality presenting saying he didn't know if it worked or not and he wasn't going to get accepted or not.",
                    "label": 0
                },
                {
                    "sent": "But if you have an expensive stochastic simulator, I mean you want to do some sort of assimilation.",
                    "label": 0
                },
                {
                    "sent": "What are the other options available to you?",
                    "label": 0
                },
                {
                    "sent": "I mean it.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not the prior, it's the.",
                    "label": 0
                },
                {
                    "sent": "But it's not likely.",
                    "label": 0
                },
                {
                    "sent": "No OK. Ha ha ha.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks again.",
                    "label": 0
                }
            ]
        }
    }
}