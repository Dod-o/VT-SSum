{
    "id": "xa6tddenp6nq2vytv3x6nhtthtng52yv",
    "title": "Szemer\u00e9di's Regularity Lemma and PairwiseClustering",
    "info": {
        "author": [
            "Marcello Pelillo, University Ca' Foscari"
        ],
        "published": "July 19, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/gbr07_pelillo_srl/",
    "segmentation": [
        [
            "OK thanks, let's talk.",
            "I'm going to describe a work than with my former student, Anna Sparato is now is at University of Twente, the Netherlands.",
            "The aim of this work was to try and see whether an extremely interesting result from extremal graph theory, which is the so called them already regularity lemma, can be imported into the field of pattern recognition and machine learning.",
            "This theorem, which was proved in the mid 70s by Android them ready.",
            "It's quite remarkable in that it stays.",
            "That's essentially all.",
            "Large graphs can be partitioned in such a way that however you take 2 classes, the edges between these two classes behave in a quiet, uniform way.",
            "So when we became aware of this interesting theorem, actually it was a lemma.",
            "It was an auxiliary result that them already.",
            "Used to prove a famous conjecture by Erdos, and to run in number theory.",
            "But since its introduction it has become a very important tool in extremely graph theory.",
            "A theoretical tool for proving theoretical results in extremely graph theory.",
            "So our our attempt was our question was was actually quite simple.",
            "Can we use this result?",
            "Can we import this result in the field of pattern recognition and computer vision and machine learning?",
            "And since we are working on clustering so our our our question was can we use this result for?",
            "Essay for solving for helping clustering algorithms."
        ],
        [
            "OK.",
            "So this is the outline of the talk.",
            "First of all, of course I'm going to describe them already, regularity lemma and it is quite technical, but I hope I will provide the main intuition behind this result.",
            "Then another interesting aspect of this story is that it is possible to find the various partitions in polynomial time, and so this allows us to have fast algorithm for partitioning large graphs.",
            "And then I'm going to describe very roughly in the second part of the talk a polynomial time algorithm for finding these partitions.",
            "Then in the third part of the talk I'm going to describe how we did use the regularity lemma.",
            "Together with another interesting result from extremely graph theory, in order to essentially to compress graphs in order to accelerate to speed up clustering processes.",
            "Then I'm going to introduce a few experimental results which are just preliminary."
        ],
        [
            "In nature."
        ],
        [
            "So now comes the first part.",
            "In order to be able to describe generated dilemma, we need some notations.",
            "First of all very simple, one the notion of an edge density.",
            "Let me first say one simple thing.",
            "From now on all our graphs will be undirected and they will have no self loops.",
            "So I give you a graph G undirected graph with no self loops and I give you 2 subsets of vertices X&Y which are supposed to be disjoint.",
            "Then the edge density between X&Y is just the final destination is the ratio between the number of edges crossing X&Y, which have one endpoint next and the other end point in Y divided by the cardinality of X times the cardinality of Y, which is just total number of edges.",
            "The maximum number of edges you can have between these two subsets, so in a sense it represents the density of the edges between the two sets and of course.",
            "Number would be a number between a real number between zero and one."
        ],
        [
            "Now introduce the notion of a regularity.",
            "I give you a positive constant.",
            "The positive real number epsilon and I give you a pair of vertices A&B.",
            "We say that the pair A&B.",
            "Of course we are assuming that they are disjoint, as in the previous case.",
            "We say that this pair of vertices is epsilon regular or just regular if the role of epsilon is understood.",
            "If let me tell you in a very intuitive way.",
            "If however I pick a subset is sufficiently large subset of vertices X of.",
            "A and a sufficiently large subset of vertices Y of B, then the density between X&Y is almost the same as the density of A&B.",
            "So technically speaking this means that, however I take Capital X, this is a subset of a capital Y subset to be sufficiently large.",
            "These are the conditions for the for the sets X&Y to be large enough, then the difference between the density between X.",
            "And Y and the density between A&B is less than epsilon.",
            "So there in one sense this means that the two subsets of vertices look like a random bipartite graph.",
            "This is not the case that you have choose subset of vertices which where you can find all the edges between them and the rest are with no head, so they are distributed fairly uniformly."
        ],
        [
            "OK, now we have a notion of equitable partition which is quite simple.",
            "I give you a partition of the vertex set in 2K plus one class is the class C1 to say QCC are assumed to have exactly the same number of vertice is.",
            "If this happens then the partition is called equitable.",
            "There is only one exception which is this vertex set.",
            "This vertex set C0 which is called exceptional set.",
            "Which is just a technical purpose.",
            "It just allows the other subsystem have precisely the same number of vertice is.",
            "So this is the only set since 0 is the only set which is allowed to have different number of vertices, so an equitable partition C0C1 CK with C0 being the exceptional set is called epsilon regular.",
            "If the following happens first of all we have to be sure that the exceptional set is small enough.",
            "So technically speaking, this means that the cardinality of C0 has to be less than epsilon times the cardinality of E. The second condition is that almost all pairs of classes are to be epsilon regular, so almost means in the definition of equal partition that all but at most epsilon times K square of the pairs CI, CJ.",
            "Are epsilon regular?",
            "A straightforward note for any epsilon and a single partition where the classes contain only one element will always be a regular partition."
        ],
        [
            "Now it comes in Redis lemma.",
            "Actually, the form I'm going to describe generators lemma is not the original 1.",
            "The original generated lemmas, slightly different.",
            "This one was taken by a more recent paper by alone and other colleagues, but essentially they say more or less the same thing.",
            "Suppose I give you an epsilon a real number epsilon an positive integer T. Then the lemma says that there exists an integer Q which depends on both epsilon and T such that every graph which is at least which is more than Q viruses, admits an epsilon regular partition in 2K plus one classes and K, which is the number of K plus one which which denotes the number of partition.",
            "Is constrained to be between T, which is our parameter.",
            "The parameter we give as input and the value of Q which depends on epsilon and T. So let me this is a theorem which holds for all graphs for whole sufficiently large graphs.",
            "In particular, it is trivial for sparse graphs for a simple reason, because if you have a large and sparse graph then the density between pairs of vertices pair of classes will be almost zero, so it will be quite.",
            "Easy to get an epsilon regular partition, but it is not so for dense one, so it's a remarkable result which is specifically interesting in case of dense graphs, we can play with the parameter, in particular the parameter epsilon says something about the regularity and the parameter T actually is a lower bound on the number of classes and the upper bound Q instead.",
            "How can I say guarantees that for large graphs the partition sets are large too?",
            "So as I said that at the begin."
        ],
        [
            "In the this lemma was used by.",
            "Them are ready to prove the orders to run conjecture in the mid 70s, but soon people working extremely graph theory recognize the importance of this result, and so there's a lot.",
            "Now it has become a sort of standard tool in extremely graph theory.",
            "For from our computational perspective, the interesting point is that in 1992 alone, an other colleagues develop that polynomial time algorithm for finding regular partitions in graphs, and I'm going to briefly."
        ],
        [
            "Describe this.",
            "This algorithm, essentially the algorithm, uses 2 procedures.",
            "The first one is needed in order to check regularity between pairs of vertices, pairs of classes, and which is stated very briefly here.",
            "Actually.",
            "Actually this is the main theorem.",
            "Sorry, I just give you the main theorem says the following.",
            "For every fixed epsilon, and for every integer T at the maready partition can be found in almost quadratic time.",
            "I mean that I'm actually is N to the power of 2, three, 2.376, which is the time we need for multiplying 2 N by N matrices with 01 entries.",
            "Actually, if you have a parallel computer parallel processor and area Pier Am, then you can use log time with the polynomial time of processors.",
            "So this is the theorem actually.",
            "Alone and the colleagues provided such an algorithm.",
            "So you run going now to describe this algorithm."
        ],
        [
            "As I was saying before, the first thing we need is a procedure to check regularity between pairs of classes and actually what alone and colleagues need was to provide such an algorithm.",
            "Essentially, this lemma in the proof of this lemma we have the algorithm indeed.",
            "Of course, I'm not going to show this here, but this lemma says that if I give you a bipartite graph H with equal number of classes which which called.",
            "IME.",
            "Then there is an, let's say, an almost quadratic algorithm that verifies that H is regular, is epsilon regular, or finds two subsets which violate the regularity condition.",
            "So this is the core of the algorithm I'm going to describe."
        ],
        [
            "We also need another piece of work.",
            "Actually this was done by generating himself in his original paper in his original paper.",
            "Them already introduced the so-called Index of partition, which is this number here, which in one sense provides a measure of how regular are the payers in a given partition.",
            "It's a quantitative measure, so given a partition P of a graph G in two classes C0C1 CK.",
            "The initial partition is just the sum of all the densities and then divided by KK Square.",
            "This is a number between zero and 1 / 2 and essentially the larger this number, the more likely is the partition to be regular."
        ],
        [
            "So them already in self in his original paper prove this lemma, and in the proof of the lemma he also provide an algorithm for exhibiting what it was stated in the lemma.",
            "I'm not going to go into the details, but let me just give you the intuition behind this.",
            "Actually the idea is that if I give you a partition which is not absolute regular, then high have a way to refine this partition so very briefly.",
            "If I give you an equitable partition P into classes C0C1 and CK with the C 0 being the exception class.",
            "If K is the least positive integer such that forward to the power K is greater than this number 600 gamma to the minus five, then what is written here is essentially the following.",
            "If the partition P is not regular, which means that more than gamma K squared pairs are gamma irregular, then is it possible to find an equitable partition of V into 1 + K * 4 to the power of K classes?",
            "Actually, what the lemma says that?",
            "I can split each set, each class C1C2C K into an exponential number of classes and the new equitable partition will be better than the previous one in the sense that the index of Q which remember it measure in a way the degree of regularity of a partition is greater than the index of P. And this at the cost of increasing the exceptional set only slightly OK.",
            "So the main is in his in his original paper, actually."
        ],
        [
            "Provided an algorithm for doing this, and this is.",
            "Actually used in the algorithm by alone and these colleagues.",
            "Now I'm going to describe very briefly the algorithm.",
            "First of all, the algorithm has to compute a constant.",
            "The console which appeared in the generated alignment before.",
            "So today I give an epsilon a real positive real positive number epsilon as input, and a positive integer T as input, and the algorithm calculates this B, which is the least positive integer such that.",
            "This happens is taken directly from from the LMI showed before and then the following algorithm, which is the complexity I said before."
        ],
        [
            "Computer generated partition for an input graph.",
            "So very briefly, the algorithm is can be considered a five step.",
            "Algorithm.",
            "Is an iterative process first of all, in the first step it creates an initial arbitrary partition.",
            "It creates an arbitrary equitable partition.",
            "The number B is just the number we computed before, and then using the lemma over land that I showed before we check regularity for every possible pair, CR and CS.",
            "This awkward condition here is just to check whether the paper is regular or not.",
            "Then we count regular players and if we have at most epsilon times KKI actually Ki, choose two.",
            "Regular partition of sorry, regular partition, then the algorithm halts and P1 is declared as a national regular partition.",
            "Otherwise we use the lemma badezimmer Radian the associated procedure to refine the out to refine the current partition.",
            "So what happens is that each subset C0C1C2C K split into an exponential number of classes and then we get another partition with repeat until convergence.",
            "So this is the basic algorithm.",
            "They prove that this algorithm is almost quadratic."
        ],
        [
            "Flexity"
        ],
        [
            "So what we did next was, well, we have this very interesting result which asserts a property for all large graphs.",
            "How can we use it in a graph based clustering process?",
            "Well, it's not easy to answer because although the generated lamb actually provides you with a partition, that partitions are not useful in a clustering context because it is a strong assumption that all.",
            "The classes have exactly the same number of vertices, so it makes no sense supplied generators lemma to cluster eyes given input graph.",
            "But we became aware of another interesting result from external graph theory, which is quite useful, which allows us to combine with the regularity lemma to compress a graph while preserving important structural information.",
            "So our idea was to combine the regularity lemma and what is called the key lemma that I'm now going to describe in order to have a quite powerful compression mechanism that allows us to run pairwise clustering algorithms.",
            "In order to speed up process, so let me introduce the notion of a reduced graph.",
            "I give you a graph G and the partition P of the vertex set into C1C2C K. We construct the reduced graph is as follows.",
            "The verticies of are are just the clusters, so if you have K classes you will have K viruses here and then you decide to put an edge between CI and CJ if and only if the CI CJ is excellent regular with density more than D. So we need a parameter D. Of course on the parameter D. So for example, here we have the original graph which has been partitioned.",
            "Then some of these pairs are epsilon regular.",
            "Some are not.",
            "We take for example C&BR epsilon regular with the density more than D. The same happens between A&B between D&D and so on.",
            "So that reduces graph.",
            "Is this one so we have a means to compress."
        ],
        [
            "After this way, Interestingly, we can think the other way around.",
            "If I give you a graph, RI can devise a procedure to expand the vertices of the graph we have what we call the expanded graph.",
            "So consider now a graph R and an integer T. We construct the expanded graph out of T as follows as follows.",
            "Each vertex of our.",
            "Is replaced by a set of T vertices, so T is a parameter that you have to be given, and then we decide to put an edge between two classes if and only if there is an edge between the vertices in the original graph.",
            "So very simply, RT is a graph in which every edge of our is replaced by a copy of the complete bipartite graph KTT so.",
            "These are old Bieber, complete bipartite graphs with exactly 2 vertices."
        ],
        [
            "In a in a nice review of the regularity lemma published in 1996, Russian Simanovich proved this following the following lemma, which is usually called the key lemma.",
            "I'm not going again to bore you with the details.",
            "Let me give you the intuition behind this result.",
            "Suppose I give you a graph GA larger graph.",
            "Probably then I partition the graph G for example using generated.",
            "Emma and I construct the reduced graph are then from RI, can construct the expanded graph.",
            "So one sense I have the original graph which is large with the reduced graph I get a smaller graph.",
            "We expanded graph, I get another larger graph.",
            "Now G, the original graph and R of T are different of course, but this nice result says that if I have a sufficiently small parameter epsilon.",
            "And if I choose T properly, then every sub graph of RT is also a subgraph of H. So in this sense I can work on your graph.",
            "So what usually graph theorists do with this with this result is I take a graph I ran, or I use generators lemma to get a regular partition, and then that purified the graph by getting rid of the exceptional set, which is usually small.",
            "I and get rid of few leftover edges and then I work on a pure graph.",
            "In some sense, this lemma assures us that, however, I take a graph H in the in the expanded in the expander graph out of T. This will also be a sub graph of the original graph, so in one sense this means that the reduced graph contains very useful structural information about the original graph in the sense that I can reconstruct structure from the original graph.",
            "By the reduced graph, so our."
        ],
        [
            "Idea, actually this is a picture which shows sketch away how people in graph theory usually use these results.",
            "So as I was saying before, you have a graph, then you construct a reduced graph thanks to the key lemma, we can study the property of the reduced graph.",
            "For example, I can look for subgraphs here and then the key lemma ensures us that this graph, the sub graph will also be a subgraph of the original graph.",
            "From our perspective, it was remarkable that the Reducer graph contains a lot of structural information about the original graph, so our idea was quite simple indeed.",
            "Why don't we use in a combined way the regularity lemma and the key lemma to compress a graph?",
            "The key lemma assures us that this will contain structural information about the graph and we perform clustering on the reduced graph.",
            "We find clusters under reducing graph, and then we expand.",
            "The vertices of the reducer graph in order to get a clustering of the."
        ],
        [
            "This is essentially the idea."
        ],
        [
            "Let me briefly say this is the idea of our approach.",
            "Of course you can use any pairwise or graph based clustering algorithm.",
            "We decide in our experiments to use dominant sets, which was a framework we introduced a few years ago, which is a sort of generalization of the notion of maximum Click to edge weighted graphs, and I would like to just point out that this approach is substantially different from what other people have done for, say, for working with more manageable graphs.",
            "Typically people use out of sample methods.",
            "I'm thinking of the work by focus at all recently published in Palmy, where they used Onestream method.",
            "Also, we did some work on out of sample method combined with the dominant set.",
            "This is different because usually this out of sample methods perform a sampling of the vertices of the graph, then perform clustering on this sample and then in some way they attach the XD out of sample vertices to the clusters that have been found.",
            "Here we do not perform any sampling of the original graph.",
            "We're just compressing the graph and then we expand back of course."
        ],
        [
            "So.",
            "In a classroom process, we need weights on the edges which reflect the similarity between vertis is the the IT is straightforward to generalize all the notions we have seen to edge weighted graphs.",
            "The key point is just to replace the notion of a density of edge density we have introduced before, which just counts the number of edges between pairs of classes.",
            "We replace that with sort of.",
            "Weighted degree or weighted density.",
            "So given two subsets A&B instead of counting the number of edges were just some of the weights on the edges between a vertex and a an vertex in me.",
            "So we obtain a weighted measure which is the weighted counterpart of the original edge density, and it can be seen that all we have said applies with this.",
            "Using this new measure.",
            "Actually this was done by Zig Reno and Riddle in the recent.",
            "Paper apart on same general computing where they were also interested in applying sort of regularity lemma to hypergraphs."
        ],
        [
            "One thing that generated that's already lemma does is to account for OK this interclass edges.",
            "It doesn't care about intraclass edges, so in order to adapt this framework with pairwise classroom process, we need also to take somehow into account interact, class similarities and so our idea was just to introduce to bias the choice of the vertices too.",
            "Put into these subsets into the algorithm, so our choice, but it's just the first that came to our mind, was just the following.",
            "If I have a set SI just order the vertices of S according to this measure here, which is just says how similar is the vertex with all the other vertices in the subset.",
            "And then we create the subpartitions taking account.",
            "This formula for this is one way, but we can also do different thing."
        ],
        [
            "Um, OK, so where?",
            "Next applied this framework to practical clustering problems?",
            "The results I'm going to show our preliminary.",
            "So the first thing we did was to take a few subsets taken from the UCI repository, and we ran the algorithm over this datasets and then these datasets are one that we chose were network.",
            "We're not that big.",
            "Then we apply the framework also to image segmentation problems.",
            "First of all, let me say a few things about the actual implementation of the alone algorithm.",
            "First of all, remember that the alone algorithm stops when a regular partition have been found.",
            "In practice, that would mean in practical application that will mean that probably the algorithm runs too much, then our choice was to stop the algorithm either when a regular partition has been found or when the subset size becomes smaller than.",
            "For determining threshold, which is a parameter of the algorithm, also remember that according to the original algorithm, every in every step of the algorithm we have to split the subset into an exponential number of subsets, which is just impractical for real world applications unless we have a very huge graph.",
            "So our choice was very crude one.",
            "We just add parameter and we split each subset in a fixed according to this parameter, which is typical is more two or three.",
            "So it's a very crude approximation of the original algorithm.",
            "Also, in a clustering process, you have to take care of the exceptional set, because of course we always have this set of spurious verticies, but the regularity lemma ensures us that this set is quite small, so our choice was just after the clustering is taking place all the elements of the exceptional set were attached to the predetermined clusters, so sort of."
        ],
        [
            "Post processing step.",
            "OK, so we took as I said before.",
            "We took a few datasets from the UCI database.",
            "Actually the Unisphere database, the Aberman's data based in the Pima Indians diabetes database.",
            "All these database contain attribute vectors.",
            "So we transform this attribute vectors into similarities using the standard kernel function."
        ],
        [
            "Our our idea was let's see what happens when we run on one side.",
            "Plain dominant set approach without any preprocessing.",
            "Let's see what the dominant set framework does and on the other end we tried this two phase approach.",
            "So first we partition the graph using them radius lemma.",
            "Then we apply dominant sets on the reduced graphene.",
            "Then we expand back.",
            "So here we report the results.",
            "Here the other results obtained by playing dominant sets here are the results obtained by two face procedure and of course we were interested in both speed and accuracy, so we wanted to measure how fast we can get with this two phase procedure.",
            "And of course we are interested in not here in worsening our performance.",
            "So for example, on the UNISPHERE data set we got, these are small data set though because of course generated lemma makes sense for very large graphs, but.",
            "Just wear and an idea of what the algorithm does, so we were able.",
            "This is pretty interesting because we were able to get reduced graph with only four vertices.",
            "This this is the size of the reduced graph.",
            "So where the dominant said actually were run an this number which shows in manual in order to file.",
            "Can I say to get a good compromise between speed and accuracy?",
            "So we found that using a very small regular graph with four vertices we were able to be substantially the plane dominant set.",
            "So 72% against 62% and obtain some speedup.",
            "So we had a compression rate of about 9098.9%.",
            "On the other data set, we were not that lucky because we got a regular graph which was not that small with respect to the original one.",
            "We got a compression rate of about 60% for the other man's database and 67% for the Pima database.",
            "And we got similar performance in terms in terms of accuracy."
        ],
        [
            "Um, of course it makes sense.",
            "Makes a lot of sense to try this approach to larger graph.",
            "So we decided to run this algorithm over image segmentation problems which few images from the Berkeley database.",
            "Each pixel was a vertex in the graph, and then similarity between pixels was just a measure of how similar are the intensity values of the picture we use.",
            "Again, the standard kernel function."
        ],
        [
            "These are the qualitative qualitative results that we got.",
            "These are the four images.",
            "This is what we got with the two phase procedure.",
            "This is what we got with by running the plane replicator dynamics directly on the image.",
            "Qualitatively, you can see that results are quite similar.",
            "This one are almost identical.",
            "This ones two, probably here we lose some details and he's this.",
            "One is also better than this one.",
            "This is just.",
            "A qualitative measure, but of course we were interested also in the speed."
        ],
        [
            "App achieved in the in the compression rate.",
            "It was quite surprising those results have been obtained with a substantial compression.",
            "All graphs were compressed at least 9080%.",
            "So for example, all images contain 9600 pixels.",
            "That this is the sides of the of the regular of the reduced graph in parenthesis.",
            "Here we have the size of the subsets, so here we perform.",
            "A plane replicator dynamics playing dominant sets on a graph having just 128 while the original graph was 9600 and so on so forth.",
            "So we had a substantial compression rate and then speed up from 4 point, something to 20 point something."
        ],
        [
            "So, so we think that this preliminary results are pretty encouraging.",
            "This means that it makes sense to use the regularity lemma and our approach of compressing graphs via via the key lemma.",
            "So what we did essentially was an attempt to import into the machine, learning the pattern recognition the computer vision domain resolved, which is quite remarkable, and with we think we have succeeded in doing this.",
            "Of course there's a lot of work to do."
        ],
        [
            "Example, as far as the experiments are concerned, we need to compare what we did with other algorithms.",
            "Of course we ran dominant set, but we can run normalized cut of your favorite graph based clustering algorithm.",
            "This is what an approach is currently doing.",
            "We could try different partitioning algorithms, for example freezy and canal in 1999 proposed different algorithm based on SVD.",
            "The decomposition to find regular partitions of graphs.",
            "Interestingly, we were talking a few days ago with Horst and other people.",
            "There is an interesting in our community to work on hypergraphs because high Holder relations are especially important in many practical applications and it is interesting to know that indeed the regularity lemma is being generalized to hypergraphs also.",
            "And of course our main question is, is there any other interesting application to in computer vision and pattern recognition where the same radial Emma can be applied?",
            "We think, for example, that graph matching could be one such application.",
            "OK, thanks a lot for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK thanks, let's talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe a work than with my former student, Anna Sparato is now is at University of Twente, the Netherlands.",
                    "label": 1
                },
                {
                    "sent": "The aim of this work was to try and see whether an extremely interesting result from extremal graph theory, which is the so called them already regularity lemma, can be imported into the field of pattern recognition and machine learning.",
                    "label": 0
                },
                {
                    "sent": "This theorem, which was proved in the mid 70s by Android them ready.",
                    "label": 0
                },
                {
                    "sent": "It's quite remarkable in that it stays.",
                    "label": 0
                },
                {
                    "sent": "That's essentially all.",
                    "label": 0
                },
                {
                    "sent": "Large graphs can be partitioned in such a way that however you take 2 classes, the edges between these two classes behave in a quiet, uniform way.",
                    "label": 0
                },
                {
                    "sent": "So when we became aware of this interesting theorem, actually it was a lemma.",
                    "label": 0
                },
                {
                    "sent": "It was an auxiliary result that them already.",
                    "label": 0
                },
                {
                    "sent": "Used to prove a famous conjecture by Erdos, and to run in number theory.",
                    "label": 0
                },
                {
                    "sent": "But since its introduction it has become a very important tool in extremely graph theory.",
                    "label": 0
                },
                {
                    "sent": "A theoretical tool for proving theoretical results in extremely graph theory.",
                    "label": 0
                },
                {
                    "sent": "So our our attempt was our question was was actually quite simple.",
                    "label": 0
                },
                {
                    "sent": "Can we use this result?",
                    "label": 0
                },
                {
                    "sent": "Can we import this result in the field of pattern recognition and computer vision and machine learning?",
                    "label": 0
                },
                {
                    "sent": "And since we are working on clustering so our our our question was can we use this result for?",
                    "label": 0
                },
                {
                    "sent": "Essay for solving for helping clustering algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "First of all, of course I'm going to describe them already, regularity lemma and it is quite technical, but I hope I will provide the main intuition behind this result.",
                    "label": 0
                },
                {
                    "sent": "Then another interesting aspect of this story is that it is possible to find the various partitions in polynomial time, and so this allows us to have fast algorithm for partitioning large graphs.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to describe very roughly in the second part of the talk a polynomial time algorithm for finding these partitions.",
                    "label": 1
                },
                {
                    "sent": "Then in the third part of the talk I'm going to describe how we did use the regularity lemma.",
                    "label": 1
                },
                {
                    "sent": "Together with another interesting result from extremely graph theory, in order to essentially to compress graphs in order to accelerate to speed up clustering processes.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to introduce a few experimental results which are just preliminary.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In nature.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now comes the first part.",
                    "label": 0
                },
                {
                    "sent": "In order to be able to describe generated dilemma, we need some notations.",
                    "label": 0
                },
                {
                    "sent": "First of all very simple, one the notion of an edge density.",
                    "label": 0
                },
                {
                    "sent": "Let me first say one simple thing.",
                    "label": 0
                },
                {
                    "sent": "From now on all our graphs will be undirected and they will have no self loops.",
                    "label": 0
                },
                {
                    "sent": "So I give you a graph G undirected graph with no self loops and I give you 2 subsets of vertices X&Y which are supposed to be disjoint.",
                    "label": 1
                },
                {
                    "sent": "Then the edge density between X&Y is just the final destination is the ratio between the number of edges crossing X&Y, which have one endpoint next and the other end point in Y divided by the cardinality of X times the cardinality of Y, which is just total number of edges.",
                    "label": 1
                },
                {
                    "sent": "The maximum number of edges you can have between these two subsets, so in a sense it represents the density of the edges between the two sets and of course.",
                    "label": 0
                },
                {
                    "sent": "Number would be a number between a real number between zero and one.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now introduce the notion of a regularity.",
                    "label": 0
                },
                {
                    "sent": "I give you a positive constant.",
                    "label": 1
                },
                {
                    "sent": "The positive real number epsilon and I give you a pair of vertices A&B.",
                    "label": 0
                },
                {
                    "sent": "We say that the pair A&B.",
                    "label": 1
                },
                {
                    "sent": "Of course we are assuming that they are disjoint, as in the previous case.",
                    "label": 0
                },
                {
                    "sent": "We say that this pair of vertices is epsilon regular or just regular if the role of epsilon is understood.",
                    "label": 0
                },
                {
                    "sent": "If let me tell you in a very intuitive way.",
                    "label": 0
                },
                {
                    "sent": "If however I pick a subset is sufficiently large subset of vertices X of.",
                    "label": 0
                },
                {
                    "sent": "A and a sufficiently large subset of vertices Y of B, then the density between X&Y is almost the same as the density of A&B.",
                    "label": 0
                },
                {
                    "sent": "So technically speaking this means that, however I take Capital X, this is a subset of a capital Y subset to be sufficiently large.",
                    "label": 1
                },
                {
                    "sent": "These are the conditions for the for the sets X&Y to be large enough, then the difference between the density between X.",
                    "label": 0
                },
                {
                    "sent": "And Y and the density between A&B is less than epsilon.",
                    "label": 1
                },
                {
                    "sent": "So there in one sense this means that the two subsets of vertices look like a random bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "This is not the case that you have choose subset of vertices which where you can find all the edges between them and the rest are with no head, so they are distributed fairly uniformly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we have a notion of equitable partition which is quite simple.",
                    "label": 0
                },
                {
                    "sent": "I give you a partition of the vertex set in 2K plus one class is the class C1 to say QCC are assumed to have exactly the same number of vertice is.",
                    "label": 1
                },
                {
                    "sent": "If this happens then the partition is called equitable.",
                    "label": 0
                },
                {
                    "sent": "There is only one exception which is this vertex set.",
                    "label": 1
                },
                {
                    "sent": "This vertex set C0 which is called exceptional set.",
                    "label": 0
                },
                {
                    "sent": "Which is just a technical purpose.",
                    "label": 0
                },
                {
                    "sent": "It just allows the other subsystem have precisely the same number of vertice is.",
                    "label": 0
                },
                {
                    "sent": "So this is the only set since 0 is the only set which is allowed to have different number of vertices, so an equitable partition C0C1 CK with C0 being the exceptional set is called epsilon regular.",
                    "label": 1
                },
                {
                    "sent": "If the following happens first of all we have to be sure that the exceptional set is small enough.",
                    "label": 0
                },
                {
                    "sent": "So technically speaking, this means that the cardinality of C0 has to be less than epsilon times the cardinality of E. The second condition is that almost all pairs of classes are to be epsilon regular, so almost means in the definition of equal partition that all but at most epsilon times K square of the pairs CI, CJ.",
                    "label": 0
                },
                {
                    "sent": "Are epsilon regular?",
                    "label": 0
                },
                {
                    "sent": "A straightforward note for any epsilon and a single partition where the classes contain only one element will always be a regular partition.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it comes in Redis lemma.",
                    "label": 0
                },
                {
                    "sent": "Actually, the form I'm going to describe generators lemma is not the original 1.",
                    "label": 0
                },
                {
                    "sent": "The original generated lemmas, slightly different.",
                    "label": 0
                },
                {
                    "sent": "This one was taken by a more recent paper by alone and other colleagues, but essentially they say more or less the same thing.",
                    "label": 0
                },
                {
                    "sent": "Suppose I give you an epsilon a real number epsilon an positive integer T. Then the lemma says that there exists an integer Q which depends on both epsilon and T such that every graph which is at least which is more than Q viruses, admits an epsilon regular partition in 2K plus one classes and K, which is the number of K plus one which which denotes the number of partition.",
                    "label": 1
                },
                {
                    "sent": "Is constrained to be between T, which is our parameter.",
                    "label": 0
                },
                {
                    "sent": "The parameter we give as input and the value of Q which depends on epsilon and T. So let me this is a theorem which holds for all graphs for whole sufficiently large graphs.",
                    "label": 0
                },
                {
                    "sent": "In particular, it is trivial for sparse graphs for a simple reason, because if you have a large and sparse graph then the density between pairs of vertices pair of classes will be almost zero, so it will be quite.",
                    "label": 0
                },
                {
                    "sent": "Easy to get an epsilon regular partition, but it is not so for dense one, so it's a remarkable result which is specifically interesting in case of dense graphs, we can play with the parameter, in particular the parameter epsilon says something about the regularity and the parameter T actually is a lower bound on the number of classes and the upper bound Q instead.",
                    "label": 0
                },
                {
                    "sent": "How can I say guarantees that for large graphs the partition sets are large too?",
                    "label": 1
                },
                {
                    "sent": "So as I said that at the begin.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the this lemma was used by.",
                    "label": 0
                },
                {
                    "sent": "Them are ready to prove the orders to run conjecture in the mid 70s, but soon people working extremely graph theory recognize the importance of this result, and so there's a lot.",
                    "label": 0
                },
                {
                    "sent": "Now it has become a sort of standard tool in extremely graph theory.",
                    "label": 0
                },
                {
                    "sent": "For from our computational perspective, the interesting point is that in 1992 alone, an other colleagues develop that polynomial time algorithm for finding regular partitions in graphs, and I'm going to briefly.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Describe this.",
                    "label": 0
                },
                {
                    "sent": "This algorithm, essentially the algorithm, uses 2 procedures.",
                    "label": 0
                },
                {
                    "sent": "The first one is needed in order to check regularity between pairs of vertices, pairs of classes, and which is stated very briefly here.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Actually this is the main theorem.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I just give you the main theorem says the following.",
                    "label": 0
                },
                {
                    "sent": "For every fixed epsilon, and for every integer T at the maready partition can be found in almost quadratic time.",
                    "label": 1
                },
                {
                    "sent": "I mean that I'm actually is N to the power of 2, three, 2.376, which is the time we need for multiplying 2 N by N matrices with 01 entries.",
                    "label": 1
                },
                {
                    "sent": "Actually, if you have a parallel computer parallel processor and area Pier Am, then you can use log time with the polynomial time of processors.",
                    "label": 0
                },
                {
                    "sent": "So this is the theorem actually.",
                    "label": 0
                },
                {
                    "sent": "Alone and the colleagues provided such an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you run going now to describe this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I was saying before, the first thing we need is a procedure to check regularity between pairs of classes and actually what alone and colleagues need was to provide such an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Essentially, this lemma in the proof of this lemma we have the algorithm indeed.",
                    "label": 0
                },
                {
                    "sent": "Of course, I'm not going to show this here, but this lemma says that if I give you a bipartite graph H with equal number of classes which which called.",
                    "label": 0
                },
                {
                    "sent": "IME.",
                    "label": 0
                },
                {
                    "sent": "Then there is an, let's say, an almost quadratic algorithm that verifies that H is regular, is epsilon regular, or finds two subsets which violate the regularity condition.",
                    "label": 1
                },
                {
                    "sent": "So this is the core of the algorithm I'm going to describe.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also need another piece of work.",
                    "label": 0
                },
                {
                    "sent": "Actually this was done by generating himself in his original paper in his original paper.",
                    "label": 0
                },
                {
                    "sent": "Them already introduced the so-called Index of partition, which is this number here, which in one sense provides a measure of how regular are the payers in a given partition.",
                    "label": 0
                },
                {
                    "sent": "It's a quantitative measure, so given a partition P of a graph G in two classes C0C1 CK.",
                    "label": 1
                },
                {
                    "sent": "The initial partition is just the sum of all the densities and then divided by KK Square.",
                    "label": 0
                },
                {
                    "sent": "This is a number between zero and 1 / 2 and essentially the larger this number, the more likely is the partition to be regular.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So them already in self in his original paper prove this lemma, and in the proof of the lemma he also provide an algorithm for exhibiting what it was stated in the lemma.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details, but let me just give you the intuition behind this.",
                    "label": 0
                },
                {
                    "sent": "Actually the idea is that if I give you a partition which is not absolute regular, then high have a way to refine this partition so very briefly.",
                    "label": 0
                },
                {
                    "sent": "If I give you an equitable partition P into classes C0C1 and CK with the C 0 being the exception class.",
                    "label": 1
                },
                {
                    "sent": "If K is the least positive integer such that forward to the power K is greater than this number 600 gamma to the minus five, then what is written here is essentially the following.",
                    "label": 0
                },
                {
                    "sent": "If the partition P is not regular, which means that more than gamma K squared pairs are gamma irregular, then is it possible to find an equitable partition of V into 1 + K * 4 to the power of K classes?",
                    "label": 1
                },
                {
                    "sent": "Actually, what the lemma says that?",
                    "label": 0
                },
                {
                    "sent": "I can split each set, each class C1C2C K into an exponential number of classes and the new equitable partition will be better than the previous one in the sense that the index of Q which remember it measure in a way the degree of regularity of a partition is greater than the index of P. And this at the cost of increasing the exceptional set only slightly OK.",
                    "label": 0
                },
                {
                    "sent": "So the main is in his in his original paper, actually.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Provided an algorithm for doing this, and this is.",
                    "label": 0
                },
                {
                    "sent": "Actually used in the algorithm by alone and these colleagues.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to describe very briefly the algorithm.",
                    "label": 0
                },
                {
                    "sent": "First of all, the algorithm has to compute a constant.",
                    "label": 0
                },
                {
                    "sent": "The console which appeared in the generated alignment before.",
                    "label": 0
                },
                {
                    "sent": "So today I give an epsilon a real positive real positive number epsilon as input, and a positive integer T as input, and the algorithm calculates this B, which is the least positive integer such that.",
                    "label": 1
                },
                {
                    "sent": "This happens is taken directly from from the LMI showed before and then the following algorithm, which is the complexity I said before.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Computer generated partition for an input graph.",
                    "label": 0
                },
                {
                    "sent": "So very briefly, the algorithm is can be considered a five step.",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is an iterative process first of all, in the first step it creates an initial arbitrary partition.",
                    "label": 1
                },
                {
                    "sent": "It creates an arbitrary equitable partition.",
                    "label": 1
                },
                {
                    "sent": "The number B is just the number we computed before, and then using the lemma over land that I showed before we check regularity for every possible pair, CR and CS.",
                    "label": 1
                },
                {
                    "sent": "This awkward condition here is just to check whether the paper is regular or not.",
                    "label": 1
                },
                {
                    "sent": "Then we count regular players and if we have at most epsilon times KKI actually Ki, choose two.",
                    "label": 0
                },
                {
                    "sent": "Regular partition of sorry, regular partition, then the algorithm halts and P1 is declared as a national regular partition.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we use the lemma badezimmer Radian the associated procedure to refine the out to refine the current partition.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that each subset C0C1C2C K split into an exponential number of classes and then we get another partition with repeat until convergence.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic algorithm.",
                    "label": 0
                },
                {
                    "sent": "They prove that this algorithm is almost quadratic.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flexity",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we did next was, well, we have this very interesting result which asserts a property for all large graphs.",
                    "label": 0
                },
                {
                    "sent": "How can we use it in a graph based clustering process?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not easy to answer because although the generated lamb actually provides you with a partition, that partitions are not useful in a clustering context because it is a strong assumption that all.",
                    "label": 0
                },
                {
                    "sent": "The classes have exactly the same number of vertices, so it makes no sense supplied generators lemma to cluster eyes given input graph.",
                    "label": 0
                },
                {
                    "sent": "But we became aware of another interesting result from external graph theory, which is quite useful, which allows us to combine with the regularity lemma to compress a graph while preserving important structural information.",
                    "label": 0
                },
                {
                    "sent": "So our idea was to combine the regularity lemma and what is called the key lemma that I'm now going to describe in order to have a quite powerful compression mechanism that allows us to run pairwise clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "In order to speed up process, so let me introduce the notion of a reduced graph.",
                    "label": 0
                },
                {
                    "sent": "I give you a graph G and the partition P of the vertex set into C1C2C K. We construct the reduced graph is as follows.",
                    "label": 1
                },
                {
                    "sent": "The verticies of are are just the clusters, so if you have K classes you will have K viruses here and then you decide to put an edge between CI and CJ if and only if the CI CJ is excellent regular with density more than D. So we need a parameter D. Of course on the parameter D. So for example, here we have the original graph which has been partitioned.",
                    "label": 0
                },
                {
                    "sent": "Then some of these pairs are epsilon regular.",
                    "label": 0
                },
                {
                    "sent": "Some are not.",
                    "label": 0
                },
                {
                    "sent": "We take for example C&BR epsilon regular with the density more than D. The same happens between A&B between D&D and so on.",
                    "label": 0
                },
                {
                    "sent": "So that reduces graph.",
                    "label": 0
                },
                {
                    "sent": "Is this one so we have a means to compress.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After this way, Interestingly, we can think the other way around.",
                    "label": 0
                },
                {
                    "sent": "If I give you a graph, RI can devise a procedure to expand the vertices of the graph we have what we call the expanded graph.",
                    "label": 0
                },
                {
                    "sent": "So consider now a graph R and an integer T. We construct the expanded graph out of T as follows as follows.",
                    "label": 1
                },
                {
                    "sent": "Each vertex of our.",
                    "label": 0
                },
                {
                    "sent": "Is replaced by a set of T vertices, so T is a parameter that you have to be given, and then we decide to put an edge between two classes if and only if there is an edge between the vertices in the original graph.",
                    "label": 0
                },
                {
                    "sent": "So very simply, RT is a graph in which every edge of our is replaced by a copy of the complete bipartite graph KTT so.",
                    "label": 1
                },
                {
                    "sent": "These are old Bieber, complete bipartite graphs with exactly 2 vertices.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a in a nice review of the regularity lemma published in 1996, Russian Simanovich proved this following the following lemma, which is usually called the key lemma.",
                    "label": 1
                },
                {
                    "sent": "I'm not going again to bore you with the details.",
                    "label": 0
                },
                {
                    "sent": "Let me give you the intuition behind this result.",
                    "label": 1
                },
                {
                    "sent": "Suppose I give you a graph GA larger graph.",
                    "label": 0
                },
                {
                    "sent": "Probably then I partition the graph G for example using generated.",
                    "label": 0
                },
                {
                    "sent": "Emma and I construct the reduced graph are then from RI, can construct the expanded graph.",
                    "label": 0
                },
                {
                    "sent": "So one sense I have the original graph which is large with the reduced graph I get a smaller graph.",
                    "label": 0
                },
                {
                    "sent": "We expanded graph, I get another larger graph.",
                    "label": 0
                },
                {
                    "sent": "Now G, the original graph and R of T are different of course, but this nice result says that if I have a sufficiently small parameter epsilon.",
                    "label": 0
                },
                {
                    "sent": "And if I choose T properly, then every sub graph of RT is also a subgraph of H. So in this sense I can work on your graph.",
                    "label": 1
                },
                {
                    "sent": "So what usually graph theorists do with this with this result is I take a graph I ran, or I use generators lemma to get a regular partition, and then that purified the graph by getting rid of the exceptional set, which is usually small.",
                    "label": 0
                },
                {
                    "sent": "I and get rid of few leftover edges and then I work on a pure graph.",
                    "label": 0
                },
                {
                    "sent": "In some sense, this lemma assures us that, however, I take a graph H in the in the expanded in the expander graph out of T. This will also be a sub graph of the original graph, so in one sense this means that the reduced graph contains very useful structural information about the original graph in the sense that I can reconstruct structure from the original graph.",
                    "label": 0
                },
                {
                    "sent": "By the reduced graph, so our.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Idea, actually this is a picture which shows sketch away how people in graph theory usually use these results.",
                    "label": 0
                },
                {
                    "sent": "So as I was saying before, you have a graph, then you construct a reduced graph thanks to the key lemma, we can study the property of the reduced graph.",
                    "label": 1
                },
                {
                    "sent": "For example, I can look for subgraphs here and then the key lemma ensures us that this graph, the sub graph will also be a subgraph of the original graph.",
                    "label": 0
                },
                {
                    "sent": "From our perspective, it was remarkable that the Reducer graph contains a lot of structural information about the original graph, so our idea was quite simple indeed.",
                    "label": 1
                },
                {
                    "sent": "Why don't we use in a combined way the regularity lemma and the key lemma to compress a graph?",
                    "label": 0
                },
                {
                    "sent": "The key lemma assures us that this will contain structural information about the graph and we perform clustering on the reduced graph.",
                    "label": 1
                },
                {
                    "sent": "We find clusters under reducing graph, and then we expand.",
                    "label": 0
                },
                {
                    "sent": "The vertices of the reducer graph in order to get a clustering of the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is essentially the idea.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me briefly say this is the idea of our approach.",
                    "label": 1
                },
                {
                    "sent": "Of course you can use any pairwise or graph based clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "We decide in our experiments to use dominant sets, which was a framework we introduced a few years ago, which is a sort of generalization of the notion of maximum Click to edge weighted graphs, and I would like to just point out that this approach is substantially different from what other people have done for, say, for working with more manageable graphs.",
                    "label": 0
                },
                {
                    "sent": "Typically people use out of sample methods.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking of the work by focus at all recently published in Palmy, where they used Onestream method.",
                    "label": 0
                },
                {
                    "sent": "Also, we did some work on out of sample method combined with the dominant set.",
                    "label": 1
                },
                {
                    "sent": "This is different because usually this out of sample methods perform a sampling of the vertices of the graph, then perform clustering on this sample and then in some way they attach the XD out of sample vertices to the clusters that have been found.",
                    "label": 0
                },
                {
                    "sent": "Here we do not perform any sampling of the original graph.",
                    "label": 1
                },
                {
                    "sent": "We're just compressing the graph and then we expand back of course.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In a classroom process, we need weights on the edges which reflect the similarity between vertis is the the IT is straightforward to generalize all the notions we have seen to edge weighted graphs.",
                    "label": 1
                },
                {
                    "sent": "The key point is just to replace the notion of a density of edge density we have introduced before, which just counts the number of edges between pairs of classes.",
                    "label": 0
                },
                {
                    "sent": "We replace that with sort of.",
                    "label": 0
                },
                {
                    "sent": "Weighted degree or weighted density.",
                    "label": 0
                },
                {
                    "sent": "So given two subsets A&B instead of counting the number of edges were just some of the weights on the edges between a vertex and a an vertex in me.",
                    "label": 0
                },
                {
                    "sent": "So we obtain a weighted measure which is the weighted counterpart of the original edge density, and it can be seen that all we have said applies with this.",
                    "label": 1
                },
                {
                    "sent": "Using this new measure.",
                    "label": 0
                },
                {
                    "sent": "Actually this was done by Zig Reno and Riddle in the recent.",
                    "label": 0
                },
                {
                    "sent": "Paper apart on same general computing where they were also interested in applying sort of regularity lemma to hypergraphs.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing that generated that's already lemma does is to account for OK this interclass edges.",
                    "label": 0
                },
                {
                    "sent": "It doesn't care about intraclass edges, so in order to adapt this framework with pairwise classroom process, we need also to take somehow into account interact, class similarities and so our idea was just to introduce to bias the choice of the vertices too.",
                    "label": 0
                },
                {
                    "sent": "Put into these subsets into the algorithm, so our choice, but it's just the first that came to our mind, was just the following.",
                    "label": 0
                },
                {
                    "sent": "If I have a set SI just order the vertices of S according to this measure here, which is just says how similar is the vertex with all the other vertices in the subset.",
                    "label": 1
                },
                {
                    "sent": "And then we create the subpartitions taking account.",
                    "label": 0
                },
                {
                    "sent": "This formula for this is one way, but we can also do different thing.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, OK, so where?",
                    "label": 0
                },
                {
                    "sent": "Next applied this framework to practical clustering problems?",
                    "label": 0
                },
                {
                    "sent": "The results I'm going to show our preliminary.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we did was to take a few subsets taken from the UCI repository, and we ran the algorithm over this datasets and then these datasets are one that we chose were network.",
                    "label": 0
                },
                {
                    "sent": "We're not that big.",
                    "label": 0
                },
                {
                    "sent": "Then we apply the framework also to image segmentation problems.",
                    "label": 0
                },
                {
                    "sent": "First of all, let me say a few things about the actual implementation of the alone algorithm.",
                    "label": 0
                },
                {
                    "sent": "First of all, remember that the alone algorithm stops when a regular partition have been found.",
                    "label": 0
                },
                {
                    "sent": "In practice, that would mean in practical application that will mean that probably the algorithm runs too much, then our choice was to stop the algorithm either when a regular partition has been found or when the subset size becomes smaller than.",
                    "label": 1
                },
                {
                    "sent": "For determining threshold, which is a parameter of the algorithm, also remember that according to the original algorithm, every in every step of the algorithm we have to split the subset into an exponential number of subsets, which is just impractical for real world applications unless we have a very huge graph.",
                    "label": 0
                },
                {
                    "sent": "So our choice was very crude one.",
                    "label": 0
                },
                {
                    "sent": "We just add parameter and we split each subset in a fixed according to this parameter, which is typical is more two or three.",
                    "label": 1
                },
                {
                    "sent": "So it's a very crude approximation of the original algorithm.",
                    "label": 0
                },
                {
                    "sent": "Also, in a clustering process, you have to take care of the exceptional set, because of course we always have this set of spurious verticies, but the regularity lemma ensures us that this set is quite small, so our choice was just after the clustering is taking place all the elements of the exceptional set were attached to the predetermined clusters, so sort of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Post processing step.",
                    "label": 0
                },
                {
                    "sent": "OK, so we took as I said before.",
                    "label": 0
                },
                {
                    "sent": "We took a few datasets from the UCI database.",
                    "label": 0
                },
                {
                    "sent": "Actually the Unisphere database, the Aberman's data based in the Pima Indians diabetes database.",
                    "label": 1
                },
                {
                    "sent": "All these database contain attribute vectors.",
                    "label": 0
                },
                {
                    "sent": "So we transform this attribute vectors into similarities using the standard kernel function.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our our idea was let's see what happens when we run on one side.",
                    "label": 0
                },
                {
                    "sent": "Plain dominant set approach without any preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Let's see what the dominant set framework does and on the other end we tried this two phase approach.",
                    "label": 0
                },
                {
                    "sent": "So first we partition the graph using them radius lemma.",
                    "label": 0
                },
                {
                    "sent": "Then we apply dominant sets on the reduced graphene.",
                    "label": 0
                },
                {
                    "sent": "Then we expand back.",
                    "label": 0
                },
                {
                    "sent": "So here we report the results.",
                    "label": 0
                },
                {
                    "sent": "Here the other results obtained by playing dominant sets here are the results obtained by two face procedure and of course we were interested in both speed and accuracy, so we wanted to measure how fast we can get with this two phase procedure.",
                    "label": 0
                },
                {
                    "sent": "And of course we are interested in not here in worsening our performance.",
                    "label": 0
                },
                {
                    "sent": "So for example, on the UNISPHERE data set we got, these are small data set though because of course generated lemma makes sense for very large graphs, but.",
                    "label": 0
                },
                {
                    "sent": "Just wear and an idea of what the algorithm does, so we were able.",
                    "label": 0
                },
                {
                    "sent": "This is pretty interesting because we were able to get reduced graph with only four vertices.",
                    "label": 0
                },
                {
                    "sent": "This this is the size of the reduced graph.",
                    "label": 0
                },
                {
                    "sent": "So where the dominant said actually were run an this number which shows in manual in order to file.",
                    "label": 0
                },
                {
                    "sent": "Can I say to get a good compromise between speed and accuracy?",
                    "label": 0
                },
                {
                    "sent": "So we found that using a very small regular graph with four vertices we were able to be substantially the plane dominant set.",
                    "label": 0
                },
                {
                    "sent": "So 72% against 62% and obtain some speedup.",
                    "label": 0
                },
                {
                    "sent": "So we had a compression rate of about 9098.9%.",
                    "label": 0
                },
                {
                    "sent": "On the other data set, we were not that lucky because we got a regular graph which was not that small with respect to the original one.",
                    "label": 0
                },
                {
                    "sent": "We got a compression rate of about 60% for the other man's database and 67% for the Pima database.",
                    "label": 0
                },
                {
                    "sent": "And we got similar performance in terms in terms of accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, of course it makes sense.",
                    "label": 0
                },
                {
                    "sent": "Makes a lot of sense to try this approach to larger graph.",
                    "label": 0
                },
                {
                    "sent": "So we decided to run this algorithm over image segmentation problems which few images from the Berkeley database.",
                    "label": 1
                },
                {
                    "sent": "Each pixel was a vertex in the graph, and then similarity between pixels was just a measure of how similar are the intensity values of the picture we use.",
                    "label": 1
                },
                {
                    "sent": "Again, the standard kernel function.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the qualitative qualitative results that we got.",
                    "label": 0
                },
                {
                    "sent": "These are the four images.",
                    "label": 0
                },
                {
                    "sent": "This is what we got with the two phase procedure.",
                    "label": 0
                },
                {
                    "sent": "This is what we got with by running the plane replicator dynamics directly on the image.",
                    "label": 0
                },
                {
                    "sent": "Qualitatively, you can see that results are quite similar.",
                    "label": 0
                },
                {
                    "sent": "This one are almost identical.",
                    "label": 0
                },
                {
                    "sent": "This ones two, probably here we lose some details and he's this.",
                    "label": 0
                },
                {
                    "sent": "One is also better than this one.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "A qualitative measure, but of course we were interested also in the speed.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "App achieved in the in the compression rate.",
                    "label": 1
                },
                {
                    "sent": "It was quite surprising those results have been obtained with a substantial compression.",
                    "label": 0
                },
                {
                    "sent": "All graphs were compressed at least 9080%.",
                    "label": 0
                },
                {
                    "sent": "So for example, all images contain 9600 pixels.",
                    "label": 1
                },
                {
                    "sent": "That this is the sides of the of the regular of the reduced graph in parenthesis.",
                    "label": 1
                },
                {
                    "sent": "Here we have the size of the subsets, so here we perform.",
                    "label": 1
                },
                {
                    "sent": "A plane replicator dynamics playing dominant sets on a graph having just 128 while the original graph was 9600 and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So we had a substantial compression rate and then speed up from 4 point, something to 20 point something.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we think that this preliminary results are pretty encouraging.",
                    "label": 1
                },
                {
                    "sent": "This means that it makes sense to use the regularity lemma and our approach of compressing graphs via via the key lemma.",
                    "label": 0
                },
                {
                    "sent": "So what we did essentially was an attempt to import into the machine, learning the pattern recognition the computer vision domain resolved, which is quite remarkable, and with we think we have succeeded in doing this.",
                    "label": 0
                },
                {
                    "sent": "Of course there's a lot of work to do.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, as far as the experiments are concerned, we need to compare what we did with other algorithms.",
                    "label": 0
                },
                {
                    "sent": "Of course we ran dominant set, but we can run normalized cut of your favorite graph based clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is what an approach is currently doing.",
                    "label": 0
                },
                {
                    "sent": "We could try different partitioning algorithms, for example freezy and canal in 1999 proposed different algorithm based on SVD.",
                    "label": 0
                },
                {
                    "sent": "The decomposition to find regular partitions of graphs.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, we were talking a few days ago with Horst and other people.",
                    "label": 0
                },
                {
                    "sent": "There is an interesting in our community to work on hypergraphs because high Holder relations are especially important in many practical applications and it is interesting to know that indeed the regularity lemma is being generalized to hypergraphs also.",
                    "label": 0
                },
                {
                    "sent": "And of course our main question is, is there any other interesting application to in computer vision and pattern recognition where the same radial Emma can be applied?",
                    "label": 0
                },
                {
                    "sent": "We think, for example, that graph matching could be one such application.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks a lot for attention.",
                    "label": 0
                }
            ]
        }
    }
}