{
    "id": "cj6jitiv2eus3cwpodspfltkzxgwobot",
    "title": "Learning Structure and Symmetry",
    "info": {
        "author": [
            "Andrea Torsello, Department of Computer Science, Ca' Foscari University of Venice"
        ],
        "published": "July 2, 2012",
        "recorded": "May 2012",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures",
            "Top->Mathematics->Geometry"
        ]
    },
    "url": "http://videolectures.net/complexnetworks2012_torsello_structure_symmetry/",
    "segmentation": [
        [
            "I've been talking about her work I've been doing.",
            "I'm trying to learn structure and the related problem of learning symmetry will see that there are interesting relationship within this.",
            "These two problems, the last one, will actually be working progress.",
            "I mean, I took the suggestion that it gave us to present something you're actively working on right now, so actively that I'm still waiting for the big results in the big data, but."
        ],
        [
            "Anyway, just to start, just to give some Metallica telling me, but I'm telling telling you about my background, I'm not a complex networks, a person or whatever that means, but I come from a pattern analysis and computer vision background and but I do we do use a lot graph structure too as a way to abstract and represent elements that we get from the images.",
            "So to me a graph is just a configuration and a particular observation.",
            "Of something that I want to learn, I want to do some inference from.",
            "So I do.",
            "We do use graphs for several reasons.",
            "They provide a lot of important contextual information they need.",
            "Most importantly, they provide a set of invariants that are within the representation that are very important to do that particular inference.",
            "So the problem we're looking into is to see.",
            "To try to see how this representation behaves, to try to sort of model this representation with respect to the underlying process that generated it.",
            "So there is a two level process, one at the optical process in the vision, and this is in division problem, the original, the optical process that goes from the object anlat interaction with lies to the image and to the particular features we return detecting on the image.",
            "And then there is another process that is the process of creating the representation which is not.",
            "An inherent in.",
            "In the problem, but it's actually something we construct on with some algorithm inside my book means.",
            "In this case we're looking at baloney triangulations on feature points.",
            "So we have two steps to go from the underlying physical property problem today from the underlying physical process to the graphical representation and.",
            "But there is a relation between these two steps and there is a relation between the underlying problem we wanted to do inference.",
            "On an actual representation, and So what we want to do is try to lift the sum.",
            "Statistical analysis on the levels representation.",
            "So in essence, what I want to do is try to learn how graphs that represent the same objects in the general term, or to present the same underlying physical property.",
            "That process will actually appear.",
            "So what is the variability, the common variability, and the modes of variability that you would expect from a single object so that you can then do recognition on that particular class of graphs?"
        ],
        [
            "So problem with that is that there is this virus problem, so we have sort of various processes to go from the physical to the representation.",
            "And of course we have problems in the detection.",
            "Bitten the problems in the way we construct graph, so that so that we have missing nodes variation in edge structure.",
            "But then they are not all equal.",
            "They are actually strongly dependent on both the underline object.",
            "Properties over the underlying object and the properties with which we are creating the representation.",
            "And so.",
            "The common approach, at least in vision, is just too well.",
            "Ignore that to keep the feature.",
            "You don't think you're thinking matter a set and try to really influence on the set, and there is very little work on the classification of the structure procedure there is.",
            "And not much work done on trying to learn the structure and the little work has been done, but it's mostly based on the on graph matching approaches where we've tried to look how similar the structures are, but.",
            "So we'll see that there's actually."
        ],
        [
            "Problem with that so.",
            "What are the underlying diffing problems with graph learning?",
            "Then?",
            "Why can we not use normal learning and inference process in this context?",
            "Well, one the first problem is of course the problem correspondence.",
            "There is no natural order within nodes and edges, so you have some how to extract those correspondences from the context.",
            "But even then there is no.",
            "In fixed number of nodes, so we not only have variation in the edge structure, but also have a variation in the dimensionality of the representation and that has to be taken into account.",
            "So in the end you cannot have a simple vector representation and you cannot summarize easily the other.",
            "This graphs as you all know interesting with a single vector, so you cannot use normal statistical inference approaches to try to characterize the distribution of structures.",
            "I mean there is.",
            "We were lacking very basic concept like a the idea of a random vector in this space precisely because we don't have.",
            "We don't have a vector Victor representation.",
            "So.",
            "Well, it's it's so."
        ],
        [
            "Approaches that have been used in the literature well, the use of graphs and computer vision that goes back for 40 years now an for there have been several attempts.",
            "One of the earliest approaches wrote for the user user prototype learning so that you actually try try to find some central structures that are very similar to all the structures within the class and use those prototypes to characterize the variation.",
            "And are you just the problem that this is actually requires this distance and the distance?",
            "There is no clear way in which we get lessons.",
            "We've got back to a matching problem, But anyway, probably one of the most well known approaches.",
            "The use of the graph median boom cattle worked a lot on that.",
            "There are other approaches to do try to do learning with graphs is just to use similarity again.",
            "Again matching problem.",
            "You compute how similar structures are and then you can do.",
            "You can do pairwise clustering or any pairwise learning approach or kernel methods to try and do some.",
            "Form of either unsupervised or discriminative analysis on the distribution.",
            "But you're not really correct with this approach or not really characterizing the distribution process.",
            "Then you can have.",
            "You can have embedding approaches where you embed and graphs in low battery low dimensional space, but we want to do that.",
            "You can characterize distribution, but you cannot put it back to the structure, so you do not have any idea how that variations those motivational you can see in the embedding space will actually project back on variation on the structure.",
            "Well, what we've trended, we've been trying to do for the last years.",
            "Try to to have approaches that can actually learn the most structural variation, how the structure varies within a particular class, and then we do that actually by creating generative models or.",
            "Position of generative models that tries to represent structure graph structure."
        ],
        [
            "Uh, well."
        ],
        [
            "Let's talk about probably the first example it with the early work I've done on this is was dates in its back on the time I was doing my PhD in York.",
            "Work with having a Coke in 2006, and is the three union model where we're trying to characterize the distribution distribution of trees, how the variation of a distribution of treated of three observational arises from the data, and we have a very simple model which is a sort of the two part.",
            "Structural and stochastic bits.",
            "In a sense, it's all this is can be seen as generalization of our grocery graphs.",
            "In the sense that we do have a sample probability associated with notes, that is a probability of seeing the particular elements of the structure.",
            "The big difference is that this probability is not unique, but is no dependent, and so we have an observation properly and observation process that is apparently process on each node there all independent.",
            "So we have this on independence independent assumption of the observation and we won't need to characterize the probability of observing each element in the structure.",
            "When we when we are able to characterize all this, we actually have a model that can be used to re simulate structure from the same class distribution.",
            "And so the idea we have a structure, a bit which is in the tree case, was just a partial order relation and this this probability or non probability observation.",
            "And then we have the observation processes with process which is just remote removes a particular node from the original structure with the probability of thigh.",
            "OK, so."
        ],
        [
            "The we given the generative model we needed to learn it and the idea is that we.",
            "We had agreed emerge approach where we do matching and merging nodes and that was driven by.",
            "Minimum message language approach where or minimum description description length approach where we're trying to merge elements together only just as long as we the merge model can plus account describes that describes the data better than.",
            "The separate models, or better better, weather the costs of describing the merged model plus signature because of describing the data given the merge model is less than the cost of describing the two origonal models posture data.",
            "And so.",
            "We do this and we have an interesting.",
            "Interesting property that we can, once you formulate it.",
            "This way we can cast the learning processes into an editing distance where the added costs are actually related to the to the entropy of the node distribution over the node observation.",
            "So OK, we did this in 2006."
        ],
        [
            "Well, it just some results here.",
            "The results as we have much better classification rate with our then most methods.",
            "The reason for this is mostly because by having my characterizing the way in which structure changes based which characterizing the way in which I think each node can be seen is saying or not within the particular class we're deconstructing.",
            "An idea of similarity of distance that is not as a Tropic and then an is ought to be is actually related to the observation probability."
        ],
        [
            "Two OK, that was then well, then since then, I've tried to generalize that and generalize that to graphs, and we wanted to.",
            "We had wanted to solve a few problems.",
            "One is we had this very nested coupling of structure and.",
            "Observation, probability and cost membership so that we had to do that heuristic greedy merging to do the do the inference, which of course is suboptimal and well, we did we actually solve this by.",
            "They are very simple and obvious, probably extension of the error error screen model, but my observation is just a graph where instead of having a single of the node and edge observe observation probability I have.",
            "A note of sufficient numbers of Salvation, probability for each node, and then a conditional observation probability for each edge where the observation become.",
            "The observation is conditioned to the observation of the two nodes.",
            "So it's a very simple graph, but the.",
            "But we are actually putting in all the information about structure within the observation probabilities, so it's it's not a random graph, but say it's random and that is we would put in some randomness.",
            "But we also put in some some stochastic structure in it.",
            "And of course there is that it's this stochastic part that includes the variability of the observation, while of course the structural part in this case is just a click.",
            "The full and dimensional click node."
        ],
        [
            "Nick and so we were missing bits.",
            "So the big parts here are just.",
            "Probability of observing nodes in Bolivia observing an edges and then of course we can add.",
            "Distributions over any attributes that we can add on edge or nodes so we can actually add information on the elements and we want to learn.",
            "For metric models of the behavior of the observation that we placed on edges or or nodes.",
            "So just for the inference part, we assume that.",
            "Node and edge observation are independent, although the model per say does not require that.",
            "And then we we definitely do not require that this.",
            "Merge attribute observation and no observation.",
            "Be independent, if anything, there will never be fully independent, the always be tightly coupled.",
            "So."
        ],
        [
            "Now we have an approach we have somehow a way of generating graphs with from this model.",
            "So we have generated at this parametric model have a graph, so we can then generate a graph.",
            "But we work in a situation where we have we don't have the information about the correspondence is so this is equivalent to assuming that we add a random permutation random unknown permutation to observation before giving it before seeing it.",
            "So the the problem is, once we have added this random permutation, the probability of a particular graph being observed from the model depends on this unknown quantity in this unknown permutation quantity.",
            "But once we do have discovered this quantity, that is, if you knew the correspondences, then the probability of observing a particular graph will be very easy to calculate model.",
            "I mean it's very obvious everything all the information required, although hidden information is within that matching their correspondences.",
            "Hotwire OK, so there are some formulas are not showing up for some reason anyway, so the point is in when working at least in structural pattern recognition there is a very common thing to do.",
            "A common theme that is to just this new one you need the correspondences.",
            "You can just do some matching and pick the the best matching correspondences and say that that would actually provide.",
            "The best.",
            "Use that to do the inference and that would provide the maximum likelihood estimate of the inference.",
            "But that is just equivalent to say that the probability of observing is equivalent is equal to the maximum.",
            "The probability of observing with the maximum correspondence, and that and that is a problem."
        ],
        [
            "And this is actually linked with the problem of symmetry that will, that is the other team over talking about.",
            "And the problem with this it would be trying to do inference.",
            "With that it can be shown in this particular.",
            "Example here and again we're missing a bit.",
            "OK, so assume that we have a model that is composed of three nodes and three edges, and all the three nodes, the all the three the.",
            "Let's say the the model is very similar to this one here.",
            "That is, the probability of observing this node is 1.",
            "This at two edges is 1.",
            "Probability of observing that jazero except the probability of observing those two nodes is oh point 5.",
            "OK, now if you try to start and look at the nodes in the graph distribution of the distribution of the graphs are generated by that model.",
            "We have this distribution.",
            "OK, now if you start if you try and do inference starting from this distribution.",
            "And you try to do inference using the best possible correspondences that you just take do matching between all these graphs and then construct the model based on the on the maximum likelihood.",
            "Correspondence is then your construct.",
            "This particular model here, which is different from the original model we are introducing a bias in your estimate every time you have a symmetry in the underlying model.",
            "OK, now find is in at least in butter analysis.",
            "There is a very common wisdom where you just you can you can improve.",
            "Sorry, ignore the symmetries.",
            "This is actually due to the resolved by edition Rooney that says that all that as an gross Infinity the proportion of.",
            "Graphs that exhibit of fundamental gas event nodes that exhibit nontrivial symmetries goes to 0.",
            "So in essence, if you're observing randomly observing a granite graph of an end nodes, it's very very unlikely that you will find any symmetries in it.",
            "But there is actually.",
            "Dual through that, an interesting tool to that where that it that tells you that it's not true that that symmetries are not important now that this result is telling you is that asymmetry is having very important characteristic observation.",
            "I mean, if you do, the point is the observation of the runner, the random observation that auto show we were talking about where observation on the completely random address Tony graph with with no OS edge observation probability of 1/2.",
            "So there we have underlying model that is absolutely this is maximally symmetric.",
            "Has all the possible symmetries you can look for, but then it's actually very high likelihood generating.",
            "Various symmetric observation and it is.",
            "This is the symmetries in the model that are very that is actually very important.",
            "One try to do inference and in their line in the.",
            "Underlying problem that it's the the symmetries that introduce will actually introduce.",
            "By us if not taking account, taking into account, and of course the symmetry in the model.",
            "Actually I actually a good measure of the amount of lack of symmetry is actually a good measure of the complexity of the underlying structure and the line model structure, not so much of the observation.",
            "So you have to take into account is duality where the observation is almost always nonsymmetric, but you are very likely to have some symmetries in the underline process generating that.",
            "And you have to take that into account.",
            "So how are you?"
        ],
        [
            "Taking that into account will just look instead of.",
            "Looking for all for only the maximum like correspondence with marginalizing over all the correspondence is which?",
            "In which we can just do that.",
            "They were just.",
            "Summing the probability of generating a graph time times the probability of the of our particular permutation over all the possible permutation of correspondences.",
            "Which is just.",
            "Equal to that term, since we're assuming that we have uniform distribution over all the permutations that they have, that idea that we randomly generated perturb the data, the data.",
            "So this can actually be rewritten, as this formula here, which is just a generalization of the permanent to the quadratic assignment problem, just like you have a permanent problem that that counts, the end characterizes the number of solutions on the on the.",
            "Um?",
            "Um?",
            "For the bipartite matching problem, this actually counts.",
            "An average is over the number of solutions over on the quadratic assignment problem.",
            "Now of course we have a problem here that it's a double solution to try to marginalized that, but it's unfeasible because we're actually summing over an exponentially large.",
            "In the exponential Guard said of all the possible permutation but but we're trying to do just an approximation of that by taking the important sampling approach to try to have a fast converging who my homework going late have fast converging estimate.",
            "Now I'm not."
        ],
        [
            "Really going in."
        ],
        [
            "And how I'm doing this sampling with?"
        ],
        [
            "I have a approach, a sampling approach that tries to sample close to the.",
            "To the sample posterior and it's.",
            "It's actually interesting that.",
            "It's when you try to understand how well and how close it it'll match.",
            "That is, for that we can characterize at least the extremal behaviour of the sample.",
            "One is that for a very high entropy models, the distribution will be approximately uniform, so it doesn't will not change the solution very much, but.",
            "The interesting thing is that as you as models move toward it more and more deterministic models for that is where the entropy that although dental observation entropies go to zoo, go to zero, then the sampling approach becomes very equivalent to the labeling procedure used by by addition selkoe to push to show the graph isomorphism is expected polynomial time.",
            "So it is again expected polynomial time in the extreme deterministic case.",
            "And we have interesting properties that the convergence, at least in this case, is actually dependent on the degree distribution.",
            "That is, it will be faster the more heterogeneous the degree distribution is.",
            "If you have a lot of graphs that are lumped in with the same degree distribution than it actually be slower in convergence.",
            "And then if all no degree distribution will be our very well separate then.",
            "Or that it will converge much faster.",
            "Boo OK, just."
        ],
        [
            "2.",
            "If you observe just an example here, the grid of the relative variance of a simple Monte Carlo approach without doing just sampling uniformly under distribution, and this is what we have in red and the.",
            "Sampling variance we have with this particular sampler, it's actually very close to the orders of magnitude differences in logs.",
            "This is in linear scale."
        ],
        [
            "And again, this is very much expanded.",
            "The expected behavior of the sample.",
            "Which has very much smaller magnitude than than just random sampling, so."
        ],
        [
            "Just how we do to learn the model.",
            "Again, we have this minimum description length approach where interesting.",
            "Again, we have a similar symmetric symmetric complexity term that arises when you try to do the description.",
            "The model description term right there, which again is a telling you that highly symmetric models are more similar than highly asymmetric model.",
            "It just arises immediately from trying.",
            "To create a description of the model so."
        ],
        [
            "Just few results on them again in comparison on a few OS.",
            "Vision problem on graphs and look close.",
            "It performs at least one a good chunk better than her good bit better than part of prototype methods and this and again one of the reasons for this and where the symmetry bit comes in is that when you do have a symmetries in the observational and in the actual.",
            "Attributes that we have on nodes like here on the legs of the horses, then buy it by doing this.",
            "Averaging over all possible maybe permutations?",
            "You're generalizing better of the violent version of your Salvation of the attributes from one leg to all the other, while if you're using a maximum maximum entropy, you'd have to observe all the same levels of detail to be projected on all the limbs.",
            "The fact that you are averaging over over the symmetries that you have observed then allows you to generalize better where in the presence of actual symmetries in the model.",
            "OK, so this is."
        ],
        [
            "Very data problems, just a bit better and I just want to."
        ],
        [
            "Few minutes I have to talk about working progress and I release a work in progress is something that that my PC student Lucas has been doing in the last few months in York Theater with Edwin Cook and the idea is here is to try to characterize symmetries from a structure at least partial symmetries.",
            "And this is of course important because scimitars as I said is are central to the characterization of model complexity but also are central to the to have a good generalization.",
            "Of the.",
            "Inference we can do on a particular structure, and so the idea is try to work around symmetries in various graphs and just want to show quick results."
        ],
        [
            "And we do this by.",
            "The applying there is adopting the results at it on quantum walks at dividends.",
            "Did in New York for a few years ago.",
            "So in his worker dividends use the inference pattern of continuous time random walks to do too much graphs and they do a.",
            "He did that by constructing as exhilarating nodes corresponding to matches and then starting the walk, and in such a way that if the two graphs are isomorphic then the distributor have the structure inference.",
            "On the keeping the amplitude at exactly at zero at correct matches, which users actually similar approach to try to cross the character's actual symmetries in graphs where."
        ],
        [
            "Would we have this continuous time random or can we start the work from 2 nodes like here I like like like those two where we have the initial state, has opposite amplitudes on the two nodes that we're looking at and zero completely everywhere else.",
            "And we let let this evolve.",
            "What it what happens is that you would have if you have an actual symmetry where the two these two nodes are in opposite side of this symmetry, then all the nodes within the symmetric axis will remain to 0 due to the structure of the inference interference, while all the all the others were actually very so.",
            "With the terms we just learned yesterday, we can actually say that the the Axis will will will separate the graph into two nodal regions, where the access will perform will be nodes of these two regions and the two will be separated like that, but not only we do not only characterize perfect symmetries, but hopefully at least experimentally we've seen that that was reasonable.",
            "And that partial symmetries will have very small amplitude, so will have completely exactly 0 on perfect symmetries, but still have some destructive interference on partial symmetries when reducing the amplitude where."
        ],
        [
            "But quite a bit.",
            "So we've just tried to characterize.",
            "We just have different thresholds on the amplitudes that observed at different times to see.",
            "To try and characterize how different classes of graphs are characterized in terms of the of the symmetries and so this is the result for very very low threshold, so that we only build too much, only see.",
            "Perfect symmetry is an this is.",
            "Shown against the length of the symmetry of the axis with respect to the total number of nodes.",
            "So the larger the axis.",
            "That means that it's smaller, the symmetry we're looking to add more local symmetry.",
            "We're looking at.",
            "So when do you have access equal to 1, we're just flipping to nodes.",
            "And this is typical on our.",
            "Well, anyway, when you when the well when the axis is very small then you have more global shifts in the whole structure that performs.",
            "That has the more global symmetries.",
            "So when we actually get fact.",
            "This strongly regular graphs will have a lot of, uh, perfect symmetries.",
            "And what did I find it interesting here is that all is perfect symmetries are around the average span of possible symmetry, so there are in between global and local symmetries.",
            "I always thought you had your observe the most of them while all the others are very low.",
            "The error shown in graphs do have slightly higher.",
            "Global a blue number of global symmetries, then the other models.",
            "While we can see that our body Albert or Sophie graphs will have a lot of large number of very very small.",
            "Symmetries very large access and that is due to the fact that if you have to leave and you'll have a lot of leaf nodes connected to a single hub for the same hub, then they can just be swapped over and that that is a very local symmetry.",
            "Actual symmetry.",
            "But as interesting things happen as you have tried to look for partial."
        ],
        [
            "Symmetries and you immediately see that as you increase the threshold, you will have an increase.",
            "In well here here we are looking for strongly regular error.",
            "Shawnee Barbaza Albert will start guard and this acrostic connector which this last class has both the both scale free and small world properties and this it's interesting, especially the behavior of this last one which has behavior very close to scale free networks as we go as we have.",
            "Very low threshold, so that has very similar properties.",
            "Property properties on perfect axis, but as we increase."
        ],
        [
            "The threshold, so we as we get more and more partial symmetries.",
            "Really looking into then the behavior will actually smooth over and then be married.",
            "Very much closer and closer to the elders or any graphs are initials to always be characterized by.",
            "Peak on very very global symmetries, while the Albert Barbagia skill free will tend to have a more wider distribution as you get looser and looser on more partial, more and more PowerShell in your view of the symmetries.",
            "And again we get."
        ],
        [
            "Larger and larger.",
            "It's interesting.",
            "Lee the What store card will always have very few symmetry, so graphs with the small with only small world property will have very few symmetries in general.",
            "And diffuse images that we do have will be very global as you'd expect."
        ],
        [
            "And that that again, that's it.",
            "Well, I did.",
            "Another interesting thing is that the strongly regular graphs which had a lot of symmetry from the get go will not get.",
            "Will do not have many.",
            "Partial symmetries at all.",
            "They just kept have keep their their regional perfect symmetries, and that's and that's about it.",
            "They will not add anything to it.",
            "OK, but that's about it.",
            "Sorry.",
            "State for this works OK, then initial state."
        ],
        [
            "Is the amplitude is minus 1 / sqrt 2 on one of the two nodes 1 / sqrt 2 on the other one and zero everywhere else.",
            "So I for each pair of nodes that we're testing where they are within an actual symmetry.",
            "But there were there all opposing edges within an opposing nodes within a National Cemetery.",
            "Kara, Pastor, when baptism do this work?",
            "Well, depends if it's a.",
            "If it's an odd length chain and you start from the exchange, the central, the node will always be have an amplitude of zero while the others will, and that will just split into two nodal regions where the alternating within being positive and negative, the two sides.",
            "Well if it's even you will not have a node belonging to the axis, so you'll have the other elements alternating into.",
            "They will store.",
            "They will alternate between positive and negative.",
            "The behavior is always an absolution.",
            "Here it of course we are only looking at the probabilities in this in this animation here, but the amplitude will be always be negative, opposite in the sides and they will.",
            "In both this case, that is just the.",
            "The cycle or in the string, they'll just be suicidal behavior, but that we have observed in general suicidal behavior.",
            "Uh understood this suicidal behavior on when you have a lot of symmetries, well, destructive inference, reducing the periodicity when you have more irregular graphs."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've been talking about her work I've been doing.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to learn structure and the related problem of learning symmetry will see that there are interesting relationship within this.",
                    "label": 0
                },
                {
                    "sent": "These two problems, the last one, will actually be working progress.",
                    "label": 0
                },
                {
                    "sent": "I mean, I took the suggestion that it gave us to present something you're actively working on right now, so actively that I'm still waiting for the big results in the big data, but.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, just to start, just to give some Metallica telling me, but I'm telling telling you about my background, I'm not a complex networks, a person or whatever that means, but I come from a pattern analysis and computer vision background and but I do we do use a lot graph structure too as a way to abstract and represent elements that we get from the images.",
                    "label": 0
                },
                {
                    "sent": "So to me a graph is just a configuration and a particular observation.",
                    "label": 0
                },
                {
                    "sent": "Of something that I want to learn, I want to do some inference from.",
                    "label": 0
                },
                {
                    "sent": "So I do.",
                    "label": 0
                },
                {
                    "sent": "We do use graphs for several reasons.",
                    "label": 0
                },
                {
                    "sent": "They provide a lot of important contextual information they need.",
                    "label": 1
                },
                {
                    "sent": "Most importantly, they provide a set of invariants that are within the representation that are very important to do that particular inference.",
                    "label": 0
                },
                {
                    "sent": "So the problem we're looking into is to see.",
                    "label": 0
                },
                {
                    "sent": "To try to see how this representation behaves, to try to sort of model this representation with respect to the underlying process that generated it.",
                    "label": 0
                },
                {
                    "sent": "So there is a two level process, one at the optical process in the vision, and this is in division problem, the original, the optical process that goes from the object anlat interaction with lies to the image and to the particular features we return detecting on the image.",
                    "label": 0
                },
                {
                    "sent": "And then there is another process that is the process of creating the representation which is not.",
                    "label": 0
                },
                {
                    "sent": "An inherent in.",
                    "label": 0
                },
                {
                    "sent": "In the problem, but it's actually something we construct on with some algorithm inside my book means.",
                    "label": 0
                },
                {
                    "sent": "In this case we're looking at baloney triangulations on feature points.",
                    "label": 0
                },
                {
                    "sent": "So we have two steps to go from the underlying physical property problem today from the underlying physical process to the graphical representation and.",
                    "label": 0
                },
                {
                    "sent": "But there is a relation between these two steps and there is a relation between the underlying problem we wanted to do inference.",
                    "label": 0
                },
                {
                    "sent": "On an actual representation, and So what we want to do is try to lift the sum.",
                    "label": 0
                },
                {
                    "sent": "Statistical analysis on the levels representation.",
                    "label": 0
                },
                {
                    "sent": "So in essence, what I want to do is try to learn how graphs that represent the same objects in the general term, or to present the same underlying physical property.",
                    "label": 0
                },
                {
                    "sent": "That process will actually appear.",
                    "label": 0
                },
                {
                    "sent": "So what is the variability, the common variability, and the modes of variability that you would expect from a single object so that you can then do recognition on that particular class of graphs?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So problem with that is that there is this virus problem, so we have sort of various processes to go from the physical to the representation.",
                    "label": 0
                },
                {
                    "sent": "And of course we have problems in the detection.",
                    "label": 0
                },
                {
                    "sent": "Bitten the problems in the way we construct graph, so that so that we have missing nodes variation in edge structure.",
                    "label": 0
                },
                {
                    "sent": "But then they are not all equal.",
                    "label": 0
                },
                {
                    "sent": "They are actually strongly dependent on both the underline object.",
                    "label": 0
                },
                {
                    "sent": "Properties over the underlying object and the properties with which we are creating the representation.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "The common approach, at least in vision, is just too well.",
                    "label": 0
                },
                {
                    "sent": "Ignore that to keep the feature.",
                    "label": 0
                },
                {
                    "sent": "You don't think you're thinking matter a set and try to really influence on the set, and there is very little work on the classification of the structure procedure there is.",
                    "label": 0
                },
                {
                    "sent": "And not much work done on trying to learn the structure and the little work has been done, but it's mostly based on the on graph matching approaches where we've tried to look how similar the structures are, but.",
                    "label": 0
                },
                {
                    "sent": "So we'll see that there's actually.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem with that so.",
                    "label": 0
                },
                {
                    "sent": "What are the underlying diffing problems with graph learning?",
                    "label": 1
                },
                {
                    "sent": "Then?",
                    "label": 0
                },
                {
                    "sent": "Why can we not use normal learning and inference process in this context?",
                    "label": 0
                },
                {
                    "sent": "Well, one the first problem is of course the problem correspondence.",
                    "label": 0
                },
                {
                    "sent": "There is no natural order within nodes and edges, so you have some how to extract those correspondences from the context.",
                    "label": 1
                },
                {
                    "sent": "But even then there is no.",
                    "label": 1
                },
                {
                    "sent": "In fixed number of nodes, so we not only have variation in the edge structure, but also have a variation in the dimensionality of the representation and that has to be taken into account.",
                    "label": 0
                },
                {
                    "sent": "So in the end you cannot have a simple vector representation and you cannot summarize easily the other.",
                    "label": 0
                },
                {
                    "sent": "This graphs as you all know interesting with a single vector, so you cannot use normal statistical inference approaches to try to characterize the distribution of structures.",
                    "label": 1
                },
                {
                    "sent": "I mean there is.",
                    "label": 0
                },
                {
                    "sent": "We were lacking very basic concept like a the idea of a random vector in this space precisely because we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have a vector Victor representation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, it's it's so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approaches that have been used in the literature well, the use of graphs and computer vision that goes back for 40 years now an for there have been several attempts.",
                    "label": 0
                },
                {
                    "sent": "One of the earliest approaches wrote for the user user prototype learning so that you actually try try to find some central structures that are very similar to all the structures within the class and use those prototypes to characterize the variation.",
                    "label": 0
                },
                {
                    "sent": "And are you just the problem that this is actually requires this distance and the distance?",
                    "label": 0
                },
                {
                    "sent": "There is no clear way in which we get lessons.",
                    "label": 0
                },
                {
                    "sent": "We've got back to a matching problem, But anyway, probably one of the most well known approaches.",
                    "label": 0
                },
                {
                    "sent": "The use of the graph median boom cattle worked a lot on that.",
                    "label": 1
                },
                {
                    "sent": "There are other approaches to do try to do learning with graphs is just to use similarity again.",
                    "label": 1
                },
                {
                    "sent": "Again matching problem.",
                    "label": 1
                },
                {
                    "sent": "You compute how similar structures are and then you can do.",
                    "label": 0
                },
                {
                    "sent": "You can do pairwise clustering or any pairwise learning approach or kernel methods to try and do some.",
                    "label": 0
                },
                {
                    "sent": "Form of either unsupervised or discriminative analysis on the distribution.",
                    "label": 0
                },
                {
                    "sent": "But you're not really correct with this approach or not really characterizing the distribution process.",
                    "label": 0
                },
                {
                    "sent": "Then you can have.",
                    "label": 0
                },
                {
                    "sent": "You can have embedding approaches where you embed and graphs in low battery low dimensional space, but we want to do that.",
                    "label": 1
                },
                {
                    "sent": "You can characterize distribution, but you cannot put it back to the structure, so you do not have any idea how that variations those motivational you can see in the embedding space will actually project back on variation on the structure.",
                    "label": 1
                },
                {
                    "sent": "Well, what we've trended, we've been trying to do for the last years.",
                    "label": 0
                },
                {
                    "sent": "Try to to have approaches that can actually learn the most structural variation, how the structure varies within a particular class, and then we do that actually by creating generative models or.",
                    "label": 0
                },
                {
                    "sent": "Position of generative models that tries to represent structure graph structure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's talk about probably the first example it with the early work I've done on this is was dates in its back on the time I was doing my PhD in York.",
                    "label": 0
                },
                {
                    "sent": "Work with having a Coke in 2006, and is the three union model where we're trying to characterize the distribution distribution of trees, how the variation of a distribution of treated of three observational arises from the data, and we have a very simple model which is a sort of the two part.",
                    "label": 0
                },
                {
                    "sent": "Structural and stochastic bits.",
                    "label": 0
                },
                {
                    "sent": "In a sense, it's all this is can be seen as generalization of our grocery graphs.",
                    "label": 0
                },
                {
                    "sent": "In the sense that we do have a sample probability associated with notes, that is a probability of seeing the particular elements of the structure.",
                    "label": 1
                },
                {
                    "sent": "The big difference is that this probability is not unique, but is no dependent, and so we have an observation properly and observation process that is apparently process on each node there all independent.",
                    "label": 0
                },
                {
                    "sent": "So we have this on independence independent assumption of the observation and we won't need to characterize the probability of observing each element in the structure.",
                    "label": 0
                },
                {
                    "sent": "When we when we are able to characterize all this, we actually have a model that can be used to re simulate structure from the same class distribution.",
                    "label": 0
                },
                {
                    "sent": "And so the idea we have a structure, a bit which is in the tree case, was just a partial order relation and this this probability or non probability observation.",
                    "label": 1
                },
                {
                    "sent": "And then we have the observation processes with process which is just remote removes a particular node from the original structure with the probability of thigh.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The we given the generative model we needed to learn it and the idea is that we.",
                    "label": 0
                },
                {
                    "sent": "We had agreed emerge approach where we do matching and merging nodes and that was driven by.",
                    "label": 0
                },
                {
                    "sent": "Minimum message language approach where or minimum description description length approach where we're trying to merge elements together only just as long as we the merge model can plus account describes that describes the data better than.",
                    "label": 0
                },
                {
                    "sent": "The separate models, or better better, weather the costs of describing the merged model plus signature because of describing the data given the merge model is less than the cost of describing the two origonal models posture data.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We do this and we have an interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting property that we can, once you formulate it.",
                    "label": 0
                },
                {
                    "sent": "This way we can cast the learning processes into an editing distance where the added costs are actually related to the to the entropy of the node distribution over the node observation.",
                    "label": 0
                },
                {
                    "sent": "So OK, we did this in 2006.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it just some results here.",
                    "label": 0
                },
                {
                    "sent": "The results as we have much better classification rate with our then most methods.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is mostly because by having my characterizing the way in which structure changes based which characterizing the way in which I think each node can be seen is saying or not within the particular class we're deconstructing.",
                    "label": 0
                },
                {
                    "sent": "An idea of similarity of distance that is not as a Tropic and then an is ought to be is actually related to the observation probability.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two OK, that was then well, then since then, I've tried to generalize that and generalize that to graphs, and we wanted to.",
                    "label": 1
                },
                {
                    "sent": "We had wanted to solve a few problems.",
                    "label": 0
                },
                {
                    "sent": "One is we had this very nested coupling of structure and.",
                    "label": 0
                },
                {
                    "sent": "Observation, probability and cost membership so that we had to do that heuristic greedy merging to do the do the inference, which of course is suboptimal and well, we did we actually solve this by.",
                    "label": 0
                },
                {
                    "sent": "They are very simple and obvious, probably extension of the error error screen model, but my observation is just a graph where instead of having a single of the node and edge observe observation probability I have.",
                    "label": 0
                },
                {
                    "sent": "A note of sufficient numbers of Salvation, probability for each node, and then a conditional observation probability for each edge where the observation become.",
                    "label": 1
                },
                {
                    "sent": "The observation is conditioned to the observation of the two nodes.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple graph, but the.",
                    "label": 0
                },
                {
                    "sent": "But we are actually putting in all the information about structure within the observation probabilities, so it's it's not a random graph, but say it's random and that is we would put in some randomness.",
                    "label": 0
                },
                {
                    "sent": "But we also put in some some stochastic structure in it.",
                    "label": 0
                },
                {
                    "sent": "And of course there is that it's this stochastic part that includes the variability of the observation, while of course the structural part in this case is just a click.",
                    "label": 1
                },
                {
                    "sent": "The full and dimensional click node.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nick and so we were missing bits.",
                    "label": 0
                },
                {
                    "sent": "So the big parts here are just.",
                    "label": 0
                },
                {
                    "sent": "Probability of observing nodes in Bolivia observing an edges and then of course we can add.",
                    "label": 0
                },
                {
                    "sent": "Distributions over any attributes that we can add on edge or nodes so we can actually add information on the elements and we want to learn.",
                    "label": 1
                },
                {
                    "sent": "For metric models of the behavior of the observation that we placed on edges or or nodes.",
                    "label": 1
                },
                {
                    "sent": "So just for the inference part, we assume that.",
                    "label": 0
                },
                {
                    "sent": "Node and edge observation are independent, although the model per say does not require that.",
                    "label": 0
                },
                {
                    "sent": "And then we we definitely do not require that this.",
                    "label": 1
                },
                {
                    "sent": "Merge attribute observation and no observation.",
                    "label": 0
                },
                {
                    "sent": "Be independent, if anything, there will never be fully independent, the always be tightly coupled.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we have an approach we have somehow a way of generating graphs with from this model.",
                    "label": 0
                },
                {
                    "sent": "So we have generated at this parametric model have a graph, so we can then generate a graph.",
                    "label": 1
                },
                {
                    "sent": "But we work in a situation where we have we don't have the information about the correspondence is so this is equivalent to assuming that we add a random permutation random unknown permutation to observation before giving it before seeing it.",
                    "label": 0
                },
                {
                    "sent": "So the the problem is, once we have added this random permutation, the probability of a particular graph being observed from the model depends on this unknown quantity in this unknown permutation quantity.",
                    "label": 1
                },
                {
                    "sent": "But once we do have discovered this quantity, that is, if you knew the correspondences, then the probability of observing a particular graph will be very easy to calculate model.",
                    "label": 0
                },
                {
                    "sent": "I mean it's very obvious everything all the information required, although hidden information is within that matching their correspondences.",
                    "label": 0
                },
                {
                    "sent": "Hotwire OK, so there are some formulas are not showing up for some reason anyway, so the point is in when working at least in structural pattern recognition there is a very common thing to do.",
                    "label": 0
                },
                {
                    "sent": "A common theme that is to just this new one you need the correspondences.",
                    "label": 0
                },
                {
                    "sent": "You can just do some matching and pick the the best matching correspondences and say that that would actually provide.",
                    "label": 0
                },
                {
                    "sent": "The best.",
                    "label": 0
                },
                {
                    "sent": "Use that to do the inference and that would provide the maximum likelihood estimate of the inference.",
                    "label": 0
                },
                {
                    "sent": "But that is just equivalent to say that the probability of observing is equivalent is equal to the maximum.",
                    "label": 1
                },
                {
                    "sent": "The probability of observing with the maximum correspondence, and that and that is a problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is actually linked with the problem of symmetry that will, that is the other team over talking about.",
                    "label": 0
                },
                {
                    "sent": "And the problem with this it would be trying to do inference.",
                    "label": 0
                },
                {
                    "sent": "With that it can be shown in this particular.",
                    "label": 1
                },
                {
                    "sent": "Example here and again we're missing a bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so assume that we have a model that is composed of three nodes and three edges, and all the three nodes, the all the three the.",
                    "label": 0
                },
                {
                    "sent": "Let's say the the model is very similar to this one here.",
                    "label": 1
                },
                {
                    "sent": "That is, the probability of observing this node is 1.",
                    "label": 0
                },
                {
                    "sent": "This at two edges is 1.",
                    "label": 0
                },
                {
                    "sent": "Probability of observing that jazero except the probability of observing those two nodes is oh point 5.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you try to start and look at the nodes in the graph distribution of the distribution of the graphs are generated by that model.",
                    "label": 0
                },
                {
                    "sent": "We have this distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you start if you try and do inference starting from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And you try to do inference using the best possible correspondences that you just take do matching between all these graphs and then construct the model based on the on the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Correspondence is then your construct.",
                    "label": 0
                },
                {
                    "sent": "This particular model here, which is different from the original model we are introducing a bias in your estimate every time you have a symmetry in the underlying model.",
                    "label": 0
                },
                {
                    "sent": "OK, now find is in at least in butter analysis.",
                    "label": 1
                },
                {
                    "sent": "There is a very common wisdom where you just you can you can improve.",
                    "label": 0
                },
                {
                    "sent": "Sorry, ignore the symmetries.",
                    "label": 0
                },
                {
                    "sent": "This is actually due to the resolved by edition Rooney that says that all that as an gross Infinity the proportion of.",
                    "label": 0
                },
                {
                    "sent": "Graphs that exhibit of fundamental gas event nodes that exhibit nontrivial symmetries goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So in essence, if you're observing randomly observing a granite graph of an end nodes, it's very very unlikely that you will find any symmetries in it.",
                    "label": 0
                },
                {
                    "sent": "But there is actually.",
                    "label": 1
                },
                {
                    "sent": "Dual through that, an interesting tool to that where that it that tells you that it's not true that that symmetries are not important now that this result is telling you is that asymmetry is having very important characteristic observation.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you do, the point is the observation of the runner, the random observation that auto show we were talking about where observation on the completely random address Tony graph with with no OS edge observation probability of 1/2.",
                    "label": 0
                },
                {
                    "sent": "So there we have underlying model that is absolutely this is maximally symmetric.",
                    "label": 0
                },
                {
                    "sent": "Has all the possible symmetries you can look for, but then it's actually very high likelihood generating.",
                    "label": 0
                },
                {
                    "sent": "Various symmetric observation and it is.",
                    "label": 0
                },
                {
                    "sent": "This is the symmetries in the model that are very that is actually very important.",
                    "label": 1
                },
                {
                    "sent": "One try to do inference and in their line in the.",
                    "label": 0
                },
                {
                    "sent": "Underlying problem that it's the the symmetries that introduce will actually introduce.",
                    "label": 0
                },
                {
                    "sent": "By us if not taking account, taking into account, and of course the symmetry in the model.",
                    "label": 1
                },
                {
                    "sent": "Actually I actually a good measure of the amount of lack of symmetry is actually a good measure of the complexity of the underlying structure and the line model structure, not so much of the observation.",
                    "label": 0
                },
                {
                    "sent": "So you have to take into account is duality where the observation is almost always nonsymmetric, but you are very likely to have some symmetries in the underline process generating that.",
                    "label": 0
                },
                {
                    "sent": "And you have to take that into account.",
                    "label": 0
                },
                {
                    "sent": "So how are you?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Taking that into account will just look instead of.",
                    "label": 1
                },
                {
                    "sent": "Looking for all for only the maximum like correspondence with marginalizing over all the correspondence is which?",
                    "label": 0
                },
                {
                    "sent": "In which we can just do that.",
                    "label": 0
                },
                {
                    "sent": "They were just.",
                    "label": 0
                },
                {
                    "sent": "Summing the probability of generating a graph time times the probability of the of our particular permutation over all the possible permutation of correspondences.",
                    "label": 1
                },
                {
                    "sent": "Which is just.",
                    "label": 0
                },
                {
                    "sent": "Equal to that term, since we're assuming that we have uniform distribution over all the permutations that they have, that idea that we randomly generated perturb the data, the data.",
                    "label": 0
                },
                {
                    "sent": "So this can actually be rewritten, as this formula here, which is just a generalization of the permanent to the quadratic assignment problem, just like you have a permanent problem that that counts, the end characterizes the number of solutions on the on the.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For the bipartite matching problem, this actually counts.",
                    "label": 0
                },
                {
                    "sent": "An average is over the number of solutions over on the quadratic assignment problem.",
                    "label": 0
                },
                {
                    "sent": "Now of course we have a problem here that it's a double solution to try to marginalized that, but it's unfeasible because we're actually summing over an exponentially large.",
                    "label": 0
                },
                {
                    "sent": "In the exponential Guard said of all the possible permutation but but we're trying to do just an approximation of that by taking the important sampling approach to try to have a fast converging who my homework going late have fast converging estimate.",
                    "label": 0
                },
                {
                    "sent": "Now I'm not.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really going in.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how I'm doing this sampling with?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a approach, a sampling approach that tries to sample close to the.",
                    "label": 0
                },
                {
                    "sent": "To the sample posterior and it's.",
                    "label": 1
                },
                {
                    "sent": "It's actually interesting that.",
                    "label": 0
                },
                {
                    "sent": "It's when you try to understand how well and how close it it'll match.",
                    "label": 0
                },
                {
                    "sent": "That is, for that we can characterize at least the extremal behaviour of the sample.",
                    "label": 0
                },
                {
                    "sent": "One is that for a very high entropy models, the distribution will be approximately uniform, so it doesn't will not change the solution very much, but.",
                    "label": 1
                },
                {
                    "sent": "The interesting thing is that as you as models move toward it more and more deterministic models for that is where the entropy that although dental observation entropies go to zoo, go to zero, then the sampling approach becomes very equivalent to the labeling procedure used by by addition selkoe to push to show the graph isomorphism is expected polynomial time.",
                    "label": 1
                },
                {
                    "sent": "So it is again expected polynomial time in the extreme deterministic case.",
                    "label": 0
                },
                {
                    "sent": "And we have interesting properties that the convergence, at least in this case, is actually dependent on the degree distribution.",
                    "label": 0
                },
                {
                    "sent": "That is, it will be faster the more heterogeneous the degree distribution is.",
                    "label": 0
                },
                {
                    "sent": "If you have a lot of graphs that are lumped in with the same degree distribution than it actually be slower in convergence.",
                    "label": 0
                },
                {
                    "sent": "And then if all no degree distribution will be our very well separate then.",
                    "label": 0
                },
                {
                    "sent": "Or that it will converge much faster.",
                    "label": 0
                },
                {
                    "sent": "Boo OK, just.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "If you observe just an example here, the grid of the relative variance of a simple Monte Carlo approach without doing just sampling uniformly under distribution, and this is what we have in red and the.",
                    "label": 1
                },
                {
                    "sent": "Sampling variance we have with this particular sampler, it's actually very close to the orders of magnitude differences in logs.",
                    "label": 0
                },
                {
                    "sent": "This is in linear scale.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, this is very much expanded.",
                    "label": 0
                },
                {
                    "sent": "The expected behavior of the sample.",
                    "label": 0
                },
                {
                    "sent": "Which has very much smaller magnitude than than just random sampling, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just how we do to learn the model.",
                    "label": 1
                },
                {
                    "sent": "Again, we have this minimum description length approach where interesting.",
                    "label": 0
                },
                {
                    "sent": "Again, we have a similar symmetric symmetric complexity term that arises when you try to do the description.",
                    "label": 1
                },
                {
                    "sent": "The model description term right there, which again is a telling you that highly symmetric models are more similar than highly asymmetric model.",
                    "label": 0
                },
                {
                    "sent": "It just arises immediately from trying.",
                    "label": 0
                },
                {
                    "sent": "To create a description of the model so.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just few results on them again in comparison on a few OS.",
                    "label": 0
                },
                {
                    "sent": "Vision problem on graphs and look close.",
                    "label": 0
                },
                {
                    "sent": "It performs at least one a good chunk better than her good bit better than part of prototype methods and this and again one of the reasons for this and where the symmetry bit comes in is that when you do have a symmetries in the observational and in the actual.",
                    "label": 0
                },
                {
                    "sent": "Attributes that we have on nodes like here on the legs of the horses, then buy it by doing this.",
                    "label": 0
                },
                {
                    "sent": "Averaging over all possible maybe permutations?",
                    "label": 0
                },
                {
                    "sent": "You're generalizing better of the violent version of your Salvation of the attributes from one leg to all the other, while if you're using a maximum maximum entropy, you'd have to observe all the same levels of detail to be projected on all the limbs.",
                    "label": 0
                },
                {
                    "sent": "The fact that you are averaging over over the symmetries that you have observed then allows you to generalize better where in the presence of actual symmetries in the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very data problems, just a bit better and I just want to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Few minutes I have to talk about working progress and I release a work in progress is something that that my PC student Lucas has been doing in the last few months in York Theater with Edwin Cook and the idea is here is to try to characterize symmetries from a structure at least partial symmetries.",
                    "label": 0
                },
                {
                    "sent": "And this is of course important because scimitars as I said is are central to the characterization of model complexity but also are central to the to have a good generalization.",
                    "label": 1
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "Inference we can do on a particular structure, and so the idea is try to work around symmetries in various graphs and just want to show quick results.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we do this by.",
                    "label": 0
                },
                {
                    "sent": "The applying there is adopting the results at it on quantum walks at dividends.",
                    "label": 0
                },
                {
                    "sent": "Did in New York for a few years ago.",
                    "label": 0
                },
                {
                    "sent": "So in his worker dividends use the inference pattern of continuous time random walks to do too much graphs and they do a.",
                    "label": 1
                },
                {
                    "sent": "He did that by constructing as exhilarating nodes corresponding to matches and then starting the walk, and in such a way that if the two graphs are isomorphic then the distributor have the structure inference.",
                    "label": 1
                },
                {
                    "sent": "On the keeping the amplitude at exactly at zero at correct matches, which users actually similar approach to try to cross the character's actual symmetries in graphs where.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would we have this continuous time random or can we start the work from 2 nodes like here I like like like those two where we have the initial state, has opposite amplitudes on the two nodes that we're looking at and zero completely everywhere else.",
                    "label": 1
                },
                {
                    "sent": "And we let let this evolve.",
                    "label": 0
                },
                {
                    "sent": "What it what happens is that you would have if you have an actual symmetry where the two these two nodes are in opposite side of this symmetry, then all the nodes within the symmetric axis will remain to 0 due to the structure of the inference interference, while all the all the others were actually very so.",
                    "label": 0
                },
                {
                    "sent": "With the terms we just learned yesterday, we can actually say that the the Axis will will will separate the graph into two nodal regions, where the access will perform will be nodes of these two regions and the two will be separated like that, but not only we do not only characterize perfect symmetries, but hopefully at least experimentally we've seen that that was reasonable.",
                    "label": 0
                },
                {
                    "sent": "And that partial symmetries will have very small amplitude, so will have completely exactly 0 on perfect symmetries, but still have some destructive interference on partial symmetries when reducing the amplitude where.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But quite a bit.",
                    "label": 0
                },
                {
                    "sent": "So we've just tried to characterize.",
                    "label": 0
                },
                {
                    "sent": "We just have different thresholds on the amplitudes that observed at different times to see.",
                    "label": 0
                },
                {
                    "sent": "To try and characterize how different classes of graphs are characterized in terms of the of the symmetries and so this is the result for very very low threshold, so that we only build too much, only see.",
                    "label": 0
                },
                {
                    "sent": "Perfect symmetry is an this is.",
                    "label": 0
                },
                {
                    "sent": "Shown against the length of the symmetry of the axis with respect to the total number of nodes.",
                    "label": 0
                },
                {
                    "sent": "So the larger the axis.",
                    "label": 0
                },
                {
                    "sent": "That means that it's smaller, the symmetry we're looking to add more local symmetry.",
                    "label": 0
                },
                {
                    "sent": "We're looking at.",
                    "label": 0
                },
                {
                    "sent": "So when do you have access equal to 1, we're just flipping to nodes.",
                    "label": 0
                },
                {
                    "sent": "And this is typical on our.",
                    "label": 0
                },
                {
                    "sent": "Well, anyway, when you when the well when the axis is very small then you have more global shifts in the whole structure that performs.",
                    "label": 0
                },
                {
                    "sent": "That has the more global symmetries.",
                    "label": 0
                },
                {
                    "sent": "So when we actually get fact.",
                    "label": 0
                },
                {
                    "sent": "This strongly regular graphs will have a lot of, uh, perfect symmetries.",
                    "label": 0
                },
                {
                    "sent": "And what did I find it interesting here is that all is perfect symmetries are around the average span of possible symmetry, so there are in between global and local symmetries.",
                    "label": 0
                },
                {
                    "sent": "I always thought you had your observe the most of them while all the others are very low.",
                    "label": 0
                },
                {
                    "sent": "The error shown in graphs do have slightly higher.",
                    "label": 0
                },
                {
                    "sent": "Global a blue number of global symmetries, then the other models.",
                    "label": 0
                },
                {
                    "sent": "While we can see that our body Albert or Sophie graphs will have a lot of large number of very very small.",
                    "label": 0
                },
                {
                    "sent": "Symmetries very large access and that is due to the fact that if you have to leave and you'll have a lot of leaf nodes connected to a single hub for the same hub, then they can just be swapped over and that that is a very local symmetry.",
                    "label": 0
                },
                {
                    "sent": "Actual symmetry.",
                    "label": 0
                },
                {
                    "sent": "But as interesting things happen as you have tried to look for partial.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Symmetries and you immediately see that as you increase the threshold, you will have an increase.",
                    "label": 0
                },
                {
                    "sent": "In well here here we are looking for strongly regular error.",
                    "label": 0
                },
                {
                    "sent": "Shawnee Barbaza Albert will start guard and this acrostic connector which this last class has both the both scale free and small world properties and this it's interesting, especially the behavior of this last one which has behavior very close to scale free networks as we go as we have.",
                    "label": 0
                },
                {
                    "sent": "Very low threshold, so that has very similar properties.",
                    "label": 0
                },
                {
                    "sent": "Property properties on perfect axis, but as we increase.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The threshold, so we as we get more and more partial symmetries.",
                    "label": 0
                },
                {
                    "sent": "Really looking into then the behavior will actually smooth over and then be married.",
                    "label": 0
                },
                {
                    "sent": "Very much closer and closer to the elders or any graphs are initials to always be characterized by.",
                    "label": 0
                },
                {
                    "sent": "Peak on very very global symmetries, while the Albert Barbagia skill free will tend to have a more wider distribution as you get looser and looser on more partial, more and more PowerShell in your view of the symmetries.",
                    "label": 0
                },
                {
                    "sent": "And again we get.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Larger and larger.",
                    "label": 0
                },
                {
                    "sent": "It's interesting.",
                    "label": 0
                },
                {
                    "sent": "Lee the What store card will always have very few symmetry, so graphs with the small with only small world property will have very few symmetries in general.",
                    "label": 0
                },
                {
                    "sent": "And diffuse images that we do have will be very global as you'd expect.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that that again, that's it.",
                    "label": 0
                },
                {
                    "sent": "Well, I did.",
                    "label": 0
                },
                {
                    "sent": "Another interesting thing is that the strongly regular graphs which had a lot of symmetry from the get go will not get.",
                    "label": 0
                },
                {
                    "sent": "Will do not have many.",
                    "label": 0
                },
                {
                    "sent": "Partial symmetries at all.",
                    "label": 0
                },
                {
                    "sent": "They just kept have keep their their regional perfect symmetries, and that's and that's about it.",
                    "label": 0
                },
                {
                    "sent": "They will not add anything to it.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's about it.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "State for this works OK, then initial state.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the amplitude is minus 1 / sqrt 2 on one of the two nodes 1 / sqrt 2 on the other one and zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So I for each pair of nodes that we're testing where they are within an actual symmetry.",
                    "label": 0
                },
                {
                    "sent": "But there were there all opposing edges within an opposing nodes within a National Cemetery.",
                    "label": 0
                },
                {
                    "sent": "Kara, Pastor, when baptism do this work?",
                    "label": 0
                },
                {
                    "sent": "Well, depends if it's a.",
                    "label": 0
                },
                {
                    "sent": "If it's an odd length chain and you start from the exchange, the central, the node will always be have an amplitude of zero while the others will, and that will just split into two nodal regions where the alternating within being positive and negative, the two sides.",
                    "label": 0
                },
                {
                    "sent": "Well if it's even you will not have a node belonging to the axis, so you'll have the other elements alternating into.",
                    "label": 0
                },
                {
                    "sent": "They will store.",
                    "label": 0
                },
                {
                    "sent": "They will alternate between positive and negative.",
                    "label": 0
                },
                {
                    "sent": "The behavior is always an absolution.",
                    "label": 0
                },
                {
                    "sent": "Here it of course we are only looking at the probabilities in this in this animation here, but the amplitude will be always be negative, opposite in the sides and they will.",
                    "label": 0
                },
                {
                    "sent": "In both this case, that is just the.",
                    "label": 0
                },
                {
                    "sent": "The cycle or in the string, they'll just be suicidal behavior, but that we have observed in general suicidal behavior.",
                    "label": 0
                },
                {
                    "sent": "Uh understood this suicidal behavior on when you have a lot of symmetries, well, destructive inference, reducing the periodicity when you have more irregular graphs.",
                    "label": 0
                }
            ]
        }
    }
}